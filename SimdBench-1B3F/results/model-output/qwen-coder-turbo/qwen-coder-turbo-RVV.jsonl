{"task_id": "SimdBench_0_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vbool64_t vmask = vle8_v_b64(mask, vl);\n    vint64m1_t vsrc = vle64_v_i64m1(src, vl);\n    vint64m1_t vdst = vle64_v_i64m1(dst, vl);\n\n    vdst = vselect_b64(vmask, vsrc, vdst, vl);\n\n    vse64_v_i64m1(dst, vdst, vl);\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vbool64_t vmask = vle8_v_b64(mask, vl);\n    vint64m1_t vsrc = vle64_v_i64m1(src, vl);\n    vint64m1_t vdst = vle64_v_i64m1(dst, vl);\n\n    vdst = vcond_mask_m(vmask, vsrc, vdst, vl);\n\n    vse64_v_i64m1(dst, vdst, vl);\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vbool64_t vmask = vle8_v_b64(mask, vl);\n    vint64m1_t vsrc = vle64_v_i64m1(src, vl);\n    vint64m1_t vdst = vle64_v_i64m1(dst, vl);\n\n    vdst = vmselect_b64(vdst, vsrc, vmask);\n\n    vse64_v_i64m1(dst, vdst, vl);\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vbool64_t vmask = vle8_v_b64(mask, vl);\n    vint64m1_t vsrc = vle64_v_i64m1(src, vl);\n    vint64m1_t vdst = vle64_v_i64m1(dst, vl);\n\n    vdst = vmselect_b64(vdst, vsrc, vmask);\n\n    vse64_v_i64m1(dst, vdst, vl);\n}\n```"}
{"task_id": "SimdBench_0_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vbool64_t vmask = vle8_v_b64(mask, vl);\n    vint64m1_t vsrc = vle64_v_i64m1(src, vl);\n    vint64m1_t vdst = vle64_v_i64m1(dst, vl);\n\n    vdst = vcond_mask_m(vmask, vsrc, vdst, vl);\n\n    vse64_v_i64m1(dst, vdst, vl);\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vint32m8_t v_src = vle32_v_i32m8(src, vl);\n    vint32m8_t v_scale = vmul_vx_i32m8(v_src, scale, vl);\n    vint32m8_t v_tmp1 = vsra_vx_i32m8(v_scale, 3, vl);\n    vint32m8_t v_tmp2 = vsll_vx_i32m8(v_scale, 3, vl);\n    vint64m1_t v_result = vwredsum_vs_i32m8_i64m1_m(v_add_vv_i32m8(v_scale, vmul_vv_i32m8(v_tmp1, v_tmp2, vl), vl), 0, vl);\n    vse32_v_i32m8(dst, v_divu_vx_i32m8(v_result, 7, vl), vl);\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    vint32m8_t v_src = vle32_v_i32m8(src, vl);\n    vint32m8_t v_scale = vmul_vx_i32m8(v_src, scale, vl);\n    vint32m8_t v_tmp1 = vsra_vx_i32m8(v_scale, 3, vl);\n    vint32m8_t v_tmp2 = vsll_vx_i32m8(v_scale, 3, vl);\n    vint64m1_t v_result = vwredsum_vs_i32m8_i64m1_m(v_add_vv_i32m8(v_scale, vmul_vv_i32m8(v_tmp1, v_tmp2, vl), vl), vzero_v_i64m1(), vl);\n    vse32_v_i32m8(dst, v_divu_vx_i32m8(v_result, 7, vl), vl);\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vint32m8_t v_src = vle32_v_i32m8(src, vl);\n    vint32m8_t v_scale = vmul_vx_i32m8(v_src, scale, vl);\n    vint32m8_t v_tmp1 = vsra_vx_i32m8(v_scale, 3, vl);\n    vint32m8_t v_tmp2 = vsll_vx_i32m8(v_scale, 3, vl);\n    vint64m1_t v_result = vwredsum_vs_i32m8_i64m1_m(v_add_vv_i32m8(v_scale, vmul_vv_i32m8(v_tmp1, v_tmp2, vl), vl), 0, vl);\n    vse32_v_i32m8(dst, v_divu_vx_i32m8(v_result, 7, vl), vl);\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vint32m8_t v_src = vle32_v_i32m8(src, vl);\n    vint32m8_t v_scale = vmul_vx_i32m8(v_src, scale, vl);\n    vint32m8_t v_tmp1 = vsra_vx_i32m8(v_scale, 3, vl);\n    vint32m8_t v_tmp2 = vsll_vx_i32m8(v_scale, 3, vl);\n    vint64m1_t v_result = vwredsum_vs_i32m8_i64m1_m(v_add_vv_i32m8(v_scale, v_mul_vv_i32m8(v_tmp1, v_tmp2, vl), vl), 0, vl);\n    vse32_v_i32m8(dst, v_divu_vx_i32m8(v_result, 7, vl), vl);\n}\n```"}
{"task_id": "SimdBench_1_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vint32m8_t v_src = vle32_v_i32m8(src, vl);\n    vint32m8_t v_scale = vmul_vx_i32m8(v_src, scale, vl);\n    vint32m8_t v_tmp1 = vsra_vx_i32m8(v_scale, 3, vl);\n    vint32m8_t v_tmp2 = vsll_vx_i32m8(v_scale, 3, vl);\n    vint64m1_t v_result = vwredsum_vs_i32m8_i64m1_m(v_add_vv_i32m8(v_scale, vmul_vv_i32m8(v_tmp1, v_tmp2, vl), vl), vzero_v_i64m1(), vl);\n    vse32_v_i32m8(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t vlen = __riscv_vsetvl_e64m1();\n    size_t vlmax = __riscv_vsetvlmax_e64m1();\n\n    for (size_t r = 0; r < rows; r++) {\n        size_t idx = r * cols + stride;\n        if (idx >= rows * cols) break;\n\n        size_t i = 0;\n        while (i < rows) {\n            size_t vl = __riscv_vsetvl_e64m1(vlmax);\n            vfloat64m1_t v_src = __riscv_vle64_v_f64m1(src + idx + i, vl);\n            __riscv_vse64_v_f64m1(dst + i, v_src, vl);\n\n            i += vl;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t vlen = __riscv_vsetvl_e64m1();\n    size_t vlmax = __riscv_vsetvlmax_e64m1();\n\n    for (size_t r = 0; r < rows; r++) {\n        size_t offset = r * cols + stride;\n        if (offset >= cols) {\n            continue;\n        }\n\n        size_t i = 0;\n        while (i < rows) {\n            size_t vl = __riscv_vsetvl_e64m1(offset);\n            vfloat64m1_t v_src = __riscv_vle64_v_f64m1(src + offset, vl);\n            __riscv_vse64_v_f64m1(dst + i, v_src, vl);\n\n            i += vl;\n            offset += vl * cols;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t vlen = __riscv_vsetvl_e64m1();\n    size_t vlmax = __riscv_vsetvlmax_e64m1();\n\n    for (size_t r = 0; r < rows; r++) {\n        size_t idx = r * cols + stride;\n        if (idx >= cols) continue;\n\n        size_t vl = __riscv_vsetvl_e64m1(idx);\n        vfloat64m1_t v_src = __riscv_vle64_v_f64m1(src + idx, vl);\n        __riscv_vse64_v_f64m1(dst + r, v_src, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t vlen = __riscv_vsetvl_e64m1();\n    size_t vlmax = __riscv_vsetvlmax_e64m1();\n\n    for (size_t r = 0; r < rows; r++) {\n        size_t offset = r * cols + stride;\n        if (offset >= cols) continue;\n\n        size_t i = 0;\n        while (i < rows) {\n            size_t vl = __riscv_vsetvl_e64m1(i);\n            vfloat64m1_t vsrc = __riscv_vle64_v_f64m1(src + offset + i * vlen, vl);\n            __riscv_vse64_v_f64m1(dst + i * vlen, vsrc, vl);\n            i += vl;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t vlen = __riscv_vsetvl_e64m1();\n    size_t vlmax = __riscv_vsetvlmax_e64m1();\n\n    for (size_t r = 0; r < rows; r++) {\n        size_t idx = r * cols + stride;\n        if (idx >= rows * cols) break;\n\n        size_t vl = vlen;\n        while (vl > 0 && idx + vl <= rows * cols) {\n            vfloat64m1_t v_src = __riscv_vle64_v_f64m1(src + idx, vl);\n            __riscv_vse64_v_f64m1(dst + r, v_src, vl);\n            idx += vl;\n            vl = __riscv_vsetvl_e64m1();\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vfloat32m1_t v_src = vle32_v_f32m1(src, vl);\n    vint32m1_t v_indices = vle32_v_i32m1(indices, vl);\n\n    // Gather operation\n    vfloat32m1_t v_gathered = vgatherv_f32m1(v_indices, v_src, vl);\n\n    // Scatter operation\n    vse32_v_f32m1(dst, v_gathered, vl);\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vfloat32m1_t v_src = vle32_v_f32m1(src, vl);\n    vint32m1_t v_indices = vle32_v_i32m1(indices, vl);\n\n    // Gather operation\n    vfloat32m1_t v_gathered = vgatherv_f32m1(v_indices, v_src, vl);\n\n    // Scatter operation\n    vse32_v_f32m1(dst, v_gathered, vl);\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vfloat32m1_t v_src = vle32_v_f32m1(src, vl);\n    vint32m1_t v_indices = vle32_v_i32m1(indices, vl);\n\n    // Gather operation\n    vfloat32m1_t v_gathered = vgathese32_v_f32m1(v_indices, v_src, vl);\n\n    // Scatter operation\n    vsseg32_v_f32m1(dst, v_gathered, v_indices, vl);\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vfloat32m1_t v_src = vle32_v_f32m1(src, vl);\n    vint32m1_t v_indices = vle32_v_i32m1(indices, vl);\n\n    // Gather operation\n    vfloat32m1_t v_gathered = vgatherv_f32m1(v_indices, v_src, vl);\n\n    // Scatter operation\n    vse32_v_f32m1(dst, v_gathered, vl);\n\n    // Repeat gather and scatter for remaining elements if necessary\n    while (length > vl) {\n        src += vl;\n        indices += vl;\n        dst += vl;\n        vl = vsetvl_e32m1(length);\n        v_src = vle32_v_f32m1(src, vl);\n        v_indices = vle32_v_i32m1(indices, vl);\n        v_gathered = vgatherv_f32m1(v_indices, v_src, vl);\n        vse32_v_f32m1(dst, v_gathered, vl);\n        length -= vl;\n    }\n}\n```"}
{"task_id": "SimdBench_3_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vfloat32m1_t v_src = vle32_v_f32m1(src, vl);\n    vint32m1_t v_indices = vle32_v_i32m1(indices, vl);\n\n    // Gather operation\n    vfloat32m1_t v_gathered = vgathese32_v_f32m1(v_indices, v_src, vl);\n\n    // Scatter operation\n    vsseg32_v_f32m1(dst, v_gathered, v_indices, vl);\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    size_t len = end - start + 1;\n    vint16m8_t vec;\n    int16_t temp;\n\n    if (len <= 0) return;\n\n    for (size_t i = 0; i < len / 2; i++) {\n        size_t idx1 = start + i;\n        size_t idx2 = start + len - i - 1;\n\n        vec = vle16_v_i16m8(arr + idx1, vlmax_v_ei16m8());\n        temp = arr[idx2];\n        vse16_v_i16m8(arr + idx2, vec, vlmax_v_ei16m8());\n        arr[idx1] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    size_t len = end - start + 1;\n    if (len <= 1) return;\n\n    vint16m8_t vec;\n    int16_t temp;\n    size_t i, j;\n\n    for (i = start; i < start + len / 2; i++) {\n        j = end - (i - start);\n\n        vec = vle16_v_i16m8(arr + i, vlmax_vv_i(vlmax_e16m8(), len));\n        temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n\n        vse16_v_i16m8(arr + i, vec, vlmax_vv_i(vlmax_e16m8(), len));\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    size_t len = end - start + 1;\n    size_t vec_len = vsetvl_e16m8(len);\n\n    int16_t *vec_ptr = arr + start;\n    int16_t *vec_end = arr + end;\n\n    while (vec_ptr < vec_end) {\n        vint16m8_t vec1 = vle16_v_i16m8(vec_ptr, vec_len);\n        vint16m8_t vec2 = vle16_v_i16m8(vec_end - vec_len, vec_len);\n\n        vint16m8_t temp = vec1;\n        vec1 = vec2;\n        vec2 = temp;\n\n        vse16_v_i16m8(vec_ptr, vec1, vec_len);\n        vse16_v_i16m8(vec_end - vec_len, vec2, vec_len);\n\n        vec_ptr += vec_len;\n        vec_end -= vec_len;\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    size_t len = end - start + 1;\n    size_t vec_len = vsetvl_e16m8(len);\n\n    int16_t *vec_ptr_start = arr + start;\n    int16_t *vec_ptr_end = arr + end;\n\n    while (len > 0) {\n        vint16m8_t vec_start = vle16_v_i16m8(vec_ptr_start, vec_len);\n        vint16m8_t vec_end = vle16_v_i16m8(vec_ptr_end, vec_len);\n\n        vint16m8_t reversed_vec = vmul_vx_i16m8(vsub_vv_i16m8(vlseg2_v_i16m8(&vec_ptr_end[-1], &vec_ptr_start[1], vec_len), vec_ptr_start), 2);\n\n        vse16_v_i16m8(vec_ptr_start, reversed_vec, vec_len);\n        vse16_v_i16m8(vec_ptr_end, vec_start, vec_len);\n\n        vec_ptr_start += vec_len;\n        vec_ptr_end -= vec_len;\n        len -= vec_len;\n    }\n}\n```"}
{"task_id": "SimdBench_4_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    size_t len = end - start + 1;\n    if (len <= 1) return;\n\n    vint16m8_t vec;\n    int16_t temp;\n    size_t i, j;\n\n    for (i = start; i < start + len / 2; i++) {\n        j = start + len - i - 1;\n        vec = vle16_v_i16m8(arr + i, vlmax_v_e16m8(len));\n        temp = arr[j];\n        arr[j] = vfirst_m_i16m8(vec);\n        vse16_v_i16m8(arr + i, temp, vlmax_v_e16m8(len));\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_size = dim1 * dim2;\n    size_t start_index = slice_idx * slice_size;\n\n    vuint8m1_t v_src = vle8_v_u8m1(tensor + start_index, slice_size, vlmax_v_e8m1(slice_size));\n    vse8_v_u8m1(slice, v_src, slice_size, vlmax_v_e8m1(slice_size));\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    vuint8m1_t v_in;\n    vuint8m1_t v_out = vle8_v_u8m1(slice, dim2 * sizeof(uint8_t), vlmax_v_e8m1());\n    \n    for (size_t i = 0; i < dim1; i++) {\n        v_in = vle8_v_u8m1(tensor + i * dim2 * dim3 + slice_idx * dim2, dim2 * sizeof(uint8_t), vlmax_v_e8m1());\n        vse8_v_u8m1(slice + i * dim2, v_in, dim2 * sizeof(uint8_t), vlmax_v_e8m1());\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    vuint8m1_t v_src;\n    vuint8m1_t v_dst;\n    size_t offset = slice_idx * dim1 * dim2;\n\n    for (size_t i = 0; i < dim1 * dim2; i += VLMAX) {\n        v_src = vl_vle8_v_u8m1(tensor + offset + i * dim3, VLMAX);\n        v_dst = vl_vse8_v_u8m1(slice + i, v_src, VLMAX);\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    vuint8m1_t v_in;\n    vuint8m1_t v_out;\n    size_t offset = slice_idx * dim1 * dim2;\n\n    for (size_t i = 0; i < dim1 * dim2; i += VL) {\n        v_in = vl_vle8_v_u8m1(tensor + offset + i * dim3, VL);\n        v_out = vl_vse8_v_u8m1(slice + i * dim3, v_in, VL);\n    }\n}\n```"}
{"task_id": "SimdBench_5_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    vuint8m1_t v_in;\n    vuint8m1_t v_out;\n    size_t offset = slice_idx * dim1 * dim2;\n\n    for (size_t i = 0; i < dim1 * dim2; i += VLMAX) {\n        v_in = vle8_v_u8m1(tensor + offset + i * dim3, VLMAX);\n        vse8_v_u8m1(slice + i, v_in, VLMAX);\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    vint16m8_t v_src, v_dst;\n    size_t i, j;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (size_t k = 0; k < block_size; k++) {\n                v_src = vle16_v_i16m8(src + (i + k) * cols + j, block_size);\n                v_dst = vse16_v_i16m8(dst + (j + k) * rows + i, v_src, block_size);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    vint16m8_t v_src, v_dst;\n    size_t i, j, k;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; k++) {\n                v_src = vle16_v_i16m8(src + (i + k) * cols + j, vlmax_e16m8());\n                v_dst = vse16_v_i16m8(dst + (j + k) * rows + i, v_src, vlmax_e16m8());\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j;\n    vint16m8_t v_src, v_dst;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (size_t bi = 0; bi < block_size; bi++) {\n                for (size_t bj = 0; bj < block_size; bj++) {\n                    v_src = vle16_v_i16m8(src + (i + bi) * cols + j + bj, vlmax_v_e16m8());\n                    v_dst = vse16_v_i16m8(dst + (j + bj) * rows + i + bi, v_src, vlmax_v_e16m8());\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k;\n    vint16m8_t v_src, v_dst;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; k++) {\n                v_src = vle16_v_i16m8(src + (i + k) * cols + j, block_size);\n                v_dst = vse16_v_i16m8(dst + (j + k) * rows + i, v_src, block_size);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j;\n    vint16m8_t v_src, v_dst;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (size_t k = 0; k < block_size; k++) {\n                v_src = vle16_v_i16m8(src + (i + k) * cols + j, block_size);\n                v_dst = vse16_v_i16m8(dst + (j + k) * rows + i, v_src, block_size);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m8_t vsum = vfmv_v_f_f32m8(0.0f);\n    size_t num_elements = dim * dim * dim;\n    size_t vector_length = vsetvl_e32m8(num_elements);\n\n    for (size_t i = 0; i < num_elements; i += vector_length) {\n        vfloat32m8_t vdata = vlw_v_f32m8(array + i, vector_length);\n        vfloat32m8_t vdiag = vfadd_vv_f32m8(vdata, vdata, vector_length);\n        vsum = vfadd_vv_f32m8(vsum, vdiag, vector_length);\n    }\n\n    return vfwredosum_vs_f32m8_f32m1(vsum, 0.0f);\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m8_t vsum = vfmv_v_f_f32m8(0.0f);\n    size_t vec_len = vsetvl_e32m8(dim);\n\n    for (size_t i = 0; i < dim; i += vec_len) {\n        vfloat32m8_t vdata = vlw_v_f32m8(array + i * dim * dim + i * dim + i, vec_len);\n        vsum = vfadd_vv_f32m8(vsum, vdata, vec_len);\n    }\n\n    return vfwredosum_vs_f32m8_f32m1(vsum, 0.0f);\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m8_t vsum = vfmv_v_f_f32m8(0.0f);\n    size_t i = 0;\n\n    for (; i <= dim - 8; i += 8) {\n        vfloat32m8_t vdata = vlse32_v_f32m8(array + i * dim * dim + i * dim + i, dim * dim * sizeof(float));\n        vsum = vfadd_vv_f32m8(vsum, vdata, vdata);\n    }\n\n    for (; i < dim; ++i) {\n        vsum = vfadd_vf_f32m8(vsum, vle32_v_f32m8(array + i * dim * dim + i * dim + i, sizeof(float)), vle32_v_f32m8(array + i * dim * dim + i * dim + i, sizeof(float)));\n    }\n\n    return vfwredosum_vs_f32m8_f32m1(vsum, vsum);\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m8_t vsum = vfmv_v_f_f32m8(0.0f);\n    size_t vec_len = vsetvl_e32m8(dim);\n    for (size_t i = 0; i < dim; i += vec_len) {\n        vfloat32m8_t vdata = vlse32_v_f32m8(array + i * dim * dim + i * dim + i, dim * dim * sizeof(float), vec_len);\n        vsum = vfadd_vv_f32m8(vsum, vdata, vec_len);\n    }\n    return vfwredosum_vs_f32m8_f32m1(vsum, 0.0f);\n}\n```"}
{"task_id": "SimdBench_7_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    vfloat32m8_t vsum = vfmv_v_f_f32m8(0.0f);\n    size_t vec_len = vsetvl_e32m8(dim);\n    for (size_t i = 0; i < dim; i += vec_len) {\n        vfloat32m8_t vdata = vlse32_v_f32m8(array + i * dim * dim + i * dim + i, dim * dim * sizeof(float), vec_len);\n        vsum = vfadd_vv_f32m8(vsum, vdata, vec_len);\n    }\n    return vfwredosum_vs_f32m8_f32m1(vsum, 0.0f);\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    vfloat64m1_t v_threshold = vle64_v_f64m1((const f64 *)(&threshold), 1);\n    vfloat64m1_t v_scale = vle64_v_f64m1((const f64 *)(&scale), 1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_src = vle64_v_f64m1(src + i * cols, cols);\n        vbool64_t mask = vf64_gt_m_b_mask(v_threshold, v_src);\n        vfloat64m1_t v_scaled = vf64_mul_m_b(v_scale, v_src, mask);\n        vsst64_v_f64m1(dst + i * cols, cols, v_scaled);\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    vfloat64m1_t v_threshold = vle64_v_f64m1((const f64 *)(&threshold), 1);\n    vfloat64m1_t v_scale = vle64_v_f64m1((const f64 *)(&scale), 1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_src = vle64_v_f64m1(src + i * cols, cols);\n        vbool64_t mask = vf64_gt_m_b_mask(v_threshold, v_src);\n        vfloat64m1_t v_scaled = vf64_mul_m_b_mask(mask, v_src, v_scale);\n        vfloat64m1_t v_result = vf64_blend_m_b_mask(mask, v_scaled, v_src);\n        vs64_v_f64m1(dst + i * cols, v_result, cols);\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    vfloat64m1_t v_threshold = vle64_v_f64m1((const f64 *) &threshold, 1);\n    vfloat64m1_t v_scale = vle64_v_f64m1((const f64 *) &scale, 1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_src = vle64_v_f64m1(src + i * cols, cols);\n        vbool64_t mask = vf64_gt_m_b_mask(v_threshold, v_src);\n        vfloat64m1_t v_scaled = vf64_mul_m_b(v_scale, v_src, mask);\n        vsst64_v_f64m1(dst + i * cols, cols, v_scaled);\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    vfloat64m1_t v_threshold = vle64_v_f64m1((const f64 *) &threshold, 1);\n    vfloat64m1_t v_scale = vle64_v_f64m1((const f64 *) &scale, 1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_src = vle64_v_f64m1(src + i * cols, cols);\n        vbool64_t mask = vf64_gt_m_b64(v_sltu_vx_f64m1_mask(cols, v_src, v_threshold), 0);\n        vfloat64m1_t v_scaled = vf64_mul_vf64m1(v_src, v_scale);\n        vst64_v_f64m1(dst + i * cols, vf64_blend_vv_f64m1(mask, v_scaled, v_src), cols);\n    }\n}\n```"}
{"task_id": "SimdBench_8_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    vfloat64m1_t v_threshold = vle64_v_f64m1((const f64 *)(&threshold), 1);\n    vfloat64m1_t v_scale = vle64_v_f64m1((const f64 *)(&scale), 1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_src = vle64_v_f64m1(src + i * cols, cols);\n        vbool64_t mask = vf64_gt_m_b64(v_slt_vf_f64m1(v_src, v_threshold), v_true_v_f64m1());\n        vfloat64m1_t v_scaled = vf64_mul_m_b64(v_src, v_scale, mask);\n        vsst64_v_f64m1(dst + i * cols, cols, v_scaled);\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t vlen = __riscv_vsetvl_e64m1(cols);\n    vfloat64m1_t v_src, v_dst;\n    vuint32m1_t v_indices;\n\n    for (size_t i = 0; i < rows; i += vlen) {\n        v_indices = __riscv_vle32_v_u32m1(indices + i, vlen);\n        for (size_t j = 0; j < cols; j += vlen) {\n            v_src = __riscv_vle64_v_f64m1(src + i * cols + j, vlen);\n            v_dst = __riscv_vsra_vx_f64m1(v_src, v_indices, vlen);\n            __riscv_vse64_v_f64m1(dst + i * cols + j, v_dst, vlen);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t vlen = __riscv_vsetvl_e64m1(cols);\n    vfloat64m1_t v_src, v_dst;\n    vuint32m1_t v_indices;\n\n    for (size_t i = 0; i < rows; i += vlen) {\n        v_indices = __riscv_vle32_v_u32m1(indices + i, vlen);\n        for (size_t j = 0; j < cols; j += vlen) {\n            v_src = __riscv_vle64_v_f64m1(src + i * cols + j, vlen);\n            v_dst = __riscv_vsra_vx_f64m1(v_src, v_indices, vlen);\n            __riscv_vse64_v_f64m1(dst + i * cols + j, v_dst, vlen);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    vfloat64m8_t v_src, v_dst;\n    vuint32m8_t v_indices;\n\n    for (size_t i = 0; i < rows; i += VLMAX) {\n        size_t vl = vsetvl_e64m8(rows - i);\n        v_indices = vle32_v_u32m8(indices + i, vl);\n\n        for (size_t j = 0; j < cols; j += VLMAX) {\n            size_t vl = vsetvl_e64m8(cols - j);\n            v_src = vle64_v_f64m8(src + (i * cols + j), vl);\n            v_dst = vse64_v_f64m8(dst + (v_indices * cols + j), v_src, vl);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    vfloat64m8_t v_src;\n    vfloat64m8_t v_dst;\n    vuint32m8_t v_indices;\n\n    for (size_t i = 0; i < rows; i += vl) {\n        size_t vl = vsetvl_e64m8(rows - i);\n        v_indices = vle32_v_u32m8(indices + i, vl);\n\n        for (size_t j = 0; j < cols; j += vl) {\n            v_src = vle64_v_f64m8(src + (i * cols + j), vl);\n            v_dst = vse64_v_f64m8(dst + (v_indices * cols + j), v_src, vl);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    vfloat64m8_t v_src;\n    vfloat64m8_t v_dst;\n    vuint32m8_t v_indices;\n\n    for (size_t i = 0; i < rows; i += vl) {\n        size_t vl = vsetvl_e64m8(rows - i);\n        v_indices = vle32_v_u32m8(indices + i, vl);\n\n        for (size_t j = 0; j < cols; j += vl) {\n            v_src = vle64_v_f64m8(src + (i * cols + j), vl);\n            v_dst = vse64_v_f64m8(dst + (v_indices * cols + j), v_src, vl);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t v_src1 = vle64_v_i64m1(src1, vl);\n    vint64m1_t v_src2 = vle64_v_i64m1(src2, vl);\n    vint64m1_t v_scalar = vmul_vx_i64m1(v_src1, scalar, vl);\n    vint64m1_t v_result = vadd_vv_i64m1(v_src1, v_src2, vl);\n    vresult = vadd_vv_i64m1(vresult, v_scalar, vl);\n    vresult = vsra_vx_i64m1(vresult, 1, vl);\n    vse64_v_i64m1(dst, vresult, vl);\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t v_src1 = vle64_v_i64m1(src1, vl);\n    vint64m1_t v_src2 = vle64_v_i64m1(src2, vl);\n    vint64m1_t v_scalar = vmul_vx_i64m1(v_src1, scalar, vl);\n    vint64m1_t v_result = vfadd_vv_i64m1(v_src1, v_src2, vl);\n    vresult = vfadd_vv_i64m1(vresult, v_scalar, vl);\n    vresult = vdiv_vv_i64m1(vresult, v_src1, vl);\n    vse64_v_i64m1(dst, vresult, vl);\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t v_src1 = vle64_v_i64m1(src1, vl);\n    vint64m1_t v_src2 = vle64_v_i64m1(src2, vl);\n    vint64m1_t v_scalar = vmul_vx_i64m1(v_src1, scalar, vl);\n    vint64m1_t v_result = vfadd_vv_i64m1(v_src1, v_src2, vl);\n    vresult = vfadd_vv_i64m1(vresult, v_scalar, vl);\n    vresult = vdiv_vv_i64m1(vresult, v_src1, vl);\n    vse64_v_i64m1(dst, vresult, vl);\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t v_src1 = vle64_v_i64m1(src1, vl);\n    vint64m1_t v_src2 = vle64_v_i64m1(src2, vl);\n    vint64m1_t v_scalar = vmul_vx_i64m1(v_src1, scalar, vl);\n    vint64m1_t v_result = vfadd_vv_i64m1(v_src1, v_src2, vl);\n    vresult = vfadd_vv_i64m1(vresult, v_scalar, vl);\n    vresult = vdiv_vv_i64m1(vresult, v_src1, vl);\n    vse64_v_i64m1(dst, vresult, vl);\n}\n```"}
{"task_id": "SimdBench_10_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t v_src1 = vle64_v_i64m1(src1, vl);\n    vint64m1_t v_src2 = vle64_v_i64m1(src2, vl);\n    vint64m1_t v_scalar = vmul_vx_i64m1(v_src1, scalar, vl);\n    vint64m1_t v_result = vadd_vv_i64m1(v_src1, v_src2, vl);\n    vresult = vadd_vv_i64m1(vresult, v_scalar, vl);\n    vresult = vsra_vx_i64m1(vresult, 1, vl);\n    vse64_v_i64m1(dst, vresult, vl);\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    vint32m8_t v_src = vle32_v_i32m8(src, length, vlmax_vv_i32m8(length));\n    vbool8_t v_mask = vmseq_vi_b8(v_src, 0, 1); // Mask for even numbers\n    vint32m8_t v_even = vand_vvm_i32m8(v_src, v_mask, vlmax_vv_i32m8(length));\n\n    int min_value = vredmin_vs_i32m8_i32m1(v_even, INT_MAX, vlmax_vv_i32m8(length));\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    vint32m8_t v_src = vle32_v_i32m8(src, length, vlmax_vv_i32m8(length));\n    vbool8_t mask = vmseq_vi_b8(v_src, vsext_vx_i32m8(0, vand_vi_x_i32m8(v_src, vle32_v_i32m8(src, length, vlmax_vv_i32m8(length)), vlmax_vv_i32m8(length)), vlmax_vv_i32m8(length)));\n    vint32m8_t v_min = vminv_vv_i32m8(v_src, mask);\n\n    return vfirst_m_i32m8(v_min);\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    vint32m8_t v_src = vle32_v_i32m8(src, length, vlmax_vv_i32m8(length));\n    vbool8_t mask = vmseq_vi_b8(v_src, vadd_vx_i32m8(v_src, 1), vlmax_vv_i32m8(length));\n    vint32m8_t v_even = vselect_vvm_i32m8(mask, v_src, vsetzero_v_i32m8(), vlmax_vv_i32m8(length));\n    int min_value = vminv_v_i32m8(v_even, vlmax_vv_i32m8(length));\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    vint32m8_t v_src = vle32_v_i32m8(src, length, vlmax_e32m8(length));\n    vbool8_t v_mask = vmseq_vi8_m(v_src, vadd_vx_i32m8(v_src, 1), vlmax_e32m8(length));\n    vint32m8_t v_even = vcompress_vm_i32m8(v_mask, v_src, vlmax_e32m8(length));\n    int min_value = vminv_v_i32m8(v_even, vlmax_e32m8(length));\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    vint32m8_t v_src = vle32_v_i32m8(src, length, vlmax_vv_i32m8(length));\n    vbool8_t mask = vmseq_vi_b8(v_src, 0, 1); // Create a mask for even numbers\n    vint32m8_t v_even = vand_vvm_i32m8(v_src, v_src, mask, vlmax_vv_i32m8(length));\n\n    int min_value = vredmin_vs_i32m8_i32m1(v_even, INT_MAX, vlmax_vv_i32m8(length));\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vint32m1_t v_src = vle32_v_i32m1(src, vl);\n    vuint32m1_t v_src_uint = vreinterpret_v_u32m1_i32m1(v_src);\n    vint32m1_t v_arith_shift = vsra_vx_i32m1(v_src, shift, vl);\n    vint32m1_t v_logic_shift = vsrl_vx_i32m1(v_src_uint, shift, vl);\n    vint32m1_t v_result = vwredsum_vs_i32m1_i32m1_m(v_arith_shift, v_logic_shift, vtrue_v_i1m1(vl), vl);\n    vse32_v_i32m1(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vint32m1_t v_src = vle32_v_i32m1(src, vl);\n    vuint32m1_t v_src_uint = vreinterpret_v_u32m1_i32m1(v_src);\n    vint32m1_t v_arith_shift = vsra_vx_i32m1(v_src, shift, vl);\n    vint32m1_t v_logic_shift = vsrl_vx_u32m1(v_src_uint, shift, vl);\n    vint32m1_t v_result = vwredsum_vs_i32m1_i32m1_m(v_arith_shift, v_logic_shift, vbool_true_m1(), vl);\n    vse32_v_i32m1(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vint32m1_t v_src = vle32_v_i32m1(src, vl);\n    vuint32m1_t v_src_uint = vreinterpret_v_u32m1_i32m1(v_src);\n    vint32m1_t v_arith_shift = vsra_vx_i32m1(v_src, shift, vl);\n    vint32m1_t v_logic_shift = vsrl_vx_u32m1(v_src_uint, shift, vl);\n    vint32m1_t v_result = vwredsum_vs_i32m1_i32m1_m(v_arith_shift, v_logic_shift, vfalse_v_i32m1(), vl);\n    vse32_v_i32m1(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vint32m1_t v_src = vle32_v_i32m1(src, vl);\n    vuint32m1_t v_src_uint = vreinterpret_v_u32m1_i32m1(v_src);\n    vint32m1_t v_arith_shift = vsra_vx_i32m1(v_src, shift, vl);\n    vint32m1_t v_logic_shift = vsrl_vx_i32m1(v_src_uint, shift, vl);\n    vint32m1_t v_result = vwredsum_vs_i32m1_i32m1_m(v_arith_shift, v_logic_shift, vtrue_v_i1m1(vl), vl);\n    vse32_v_i32m1(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_12_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    vint32m1_t v_src = vle32_v_i32m1(src, vl);\n    vuint32m1_t v_src_uint = vreinterpret_v_u32m1_i32m1(v_src);\n    vint32m1_t v_arith_shift = vsra_vx_i32m1(v_src, shift, vl);\n    vint32m1_t v_logic_shift = vsrl_vx_u32m1(v_src_uint, shift, vl);\n    vint32m1_t v_result = vwredsum_vs_i32m1_i32m1_m(v_arith_shift, v_logic_shift, vbool_true_m1(), vl);\n    vse32_v_i32m1(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    vint32m8_t v_src = vle32_v_i32m8(matrix, total_elements / 8, vlmax_vv_e32m8(total_elements));\n    int64_t sum = vredsum_vs_i32m8_i64m1(v_src, 0, vlmax_vv_e32m8(total_elements));\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t vlen = __riscv_vsetvl_e32m8(total_elements);\n    vint32m8_t vsum = __riscv_vmv_v_x_i32m8(0, vlen);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint32m8_t vdata = __riscv_vle32_v_i32m8(matrix + i, vlen);\n        vsum = __riscv_vsadd_vv_i32m8(vsum, vdata, vlen);\n    }\n\n    int32_t sum[2];\n    __riscv_vse32_v_i32m8(sum, vsum, 2);\n    return (int64_t)sum[0] + sum[1];\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    vint32m8_t v_src = vle32_v_i32m8(matrix, total_elements, vlmax_vv_i32m8(total_elements));\n    int64_t sum = vredsum_vs_i32m8_i64m1(v_src, 0, vlmax_vv_i32m8(total_elements));\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    vint32m8_t vsum = vmv_v_x_i32m8(0, vlmax_v_e32m8(total_elements));\n    size_t i = 0;\n\n    while (i + vlmax_v_e32m8(total_elements) <= total_elements) {\n        vint32m8_t vdata = vle32_v_i32m8(matrix + i, vlmax_v_e32m8(total_elements));\n        vsum = vwredsum_vs_i32m8_i64m1(vsum, vdata, 0, vlmax_v_e32m8(total_elements));\n        i += vlmax_v_e32m8(total_elements);\n    }\n\n    if (i < total_elements) {\n        vint32m1_t vdata = vle32_v_i32m1(matrix + i, total_elements - i);\n        vint64m1_t vsum_m1 = vwredsum_vs_i32m1_i64m1(0, vdata, 0, total_elements - i);\n        vsum = vadd_vx_i64m1(vsum, vsum_m1, 0);\n    }\n\n    return vfirst_m_i64m1(vsum);\n}\n```"}
{"task_id": "SimdBench_13_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    vint32m8_t vsum = vle32_v_i32m8(matrix, total_elements, vlmax_vv_i32m8(total_elements));\n    int32_t sum = vredsum_vs_i32m8_i32m1(vsum, 0, vsetvl_e32m1(1));\n    return (int64_t)sum;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t vlen = __riscv_vsetvl_e8m1(length);\n    vint8m1_t v_src = __riscv_vle8_v_i8m1(src, vlen);\n    vbool1_t mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n\n    for (size_t i = vlen; i < length; i += vlen) {\n        v_src = __riscv_vle8_v_i8m1(src + i, vlen);\n        vbool1_t new_mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n        mask |= new_mask;\n\n        if (__riscv_vmv_x_s_b1(mask)) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t vlen = __riscv_vsetvl_e8m1(length);\n    vint8m1_t v_src = __riscv_vle8_v_i8m1(src, vlen);\n    vbool1_t mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n\n    for (size_t i = vlen; i < length; i += vlen) {\n        v_src = __riscv_vle8_v_i8m1(src + i, vlen);\n        vbool1_t new_mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n        mask |= new_mask;\n\n        if (__riscv_vfirst_m_b1(mask)) {\n            max_index = i + __riscv_vfirst_m_b1(mask);\n            max_value = src[max_index];\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t vlen = __riscv_vsetvl_e8m1(length);\n    vint8m1_t v_src = __riscv_vle8_v_i8m1(src, vlen);\n    vbool1_t mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n\n    for (size_t i = vlen; i < length; i += vlen) {\n        v_src = __riscv_vle8_v_i8m1(src + i, vlen);\n        vbool1_t new_mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n        mask |= new_mask;\n\n        vint8m1_t v_max = __riscv_vmv_v_x_i8m1(max_value);\n        v_max = __riscv_vmax_vv_i8m1(v_max, v_src);\n        max_value = __riscv_vfirst_m_b1(mask);\n\n        vint32m1_t v_indices = __riscv_vle32_v_i32m1((const int32_t *)src + i / sizeof(int32_t), vlen / sizeof(int32_t));\n        vint32m1_t v_max_indices = __riscv_vmv_v_x_i32m1(0);\n        v_max_indices = __riscv_vmax_vv_i32m1(v_max_indices, v_indices);\n        size_t max_index_in_block = __riscv_vfirst_m_b1(mask);\n        max_index_in_block *= sizeof(int32_t) / sizeof(int8_t);\n        max_index += max_index_in_block;\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t vlen = __riscv_vsetvl_e8m1(length);\n    vint8m1_t v_src = __riscv_vle8_v_i8m1(src, vlen);\n    vbool1_t mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n\n    for (size_t i = vlen; i < length; i += vlen) {\n        v_src = __riscv_vle8_v_i8m1(src + i, vlen);\n        vbool1_t new_mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n        mask |= new_mask;\n\n        if (__riscv_vmv_x_s_b1(mask)) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t vlen = __riscv_vsetvl_e8m1(length);\n    vint8m1_t v_src = __riscv_vle8_v_i8m1(src, vlen);\n    vboolm1_t v_mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n\n    for (size_t i = vlen; i < length; i += vlen) {\n        v_src = __riscv_vle8_v_i8m1(src + i, vlen);\n        v_mask = __riscv_vmseq_vx_i8m1_b1(v_src, max_value);\n        vboolm1_t v_not_mask = __riscv_vnot_m_b1(v_mask);\n        vint8m1_t v_max = __riscv_vsadd_vv_i8m1(v_src, v_src, v_not_mask);\n        max_value = __riscv_vfirst_m_b1(v_mask);\n        max_index = i + __riscv_vfind_first_m_b1(v_mask);\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_mask = vmseq_vx_u16m8(vl, vle16_v_u16m8(src1, vl), mask);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vor_vv_u16m8(vl, vand_vv_u16m8(vl, v_src1, v_mask), vandnot_vv_u16m8(vl, v_src2, v_mask));\n    vse16_v_u16m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_mask = vmseq_vx_u16m8(vl, vle16_v_u16m8(src1, vl), mask);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n\n    vse16_v_u16m8(dst, vselect_vvm_u16m8(vl, v_mask, v_src1, v_src2), vl);\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_mask = vmseq_vx_u16m8(vl, vle16_v_u16m8(src1, vl), mask);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vor_vv_u16m8(vl, vand_vv_u16m8(vl, v_src1, v_mask), vandnot_vv_u16m8(vl, v_src2, v_mask));\n    vse16_v_u16m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_mask = vmseq_vx_u16m8(vl, vle16_v_u16m8(src1, vl), mask);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vor_vv_u16m8(vl, vand_vv_u16m8(vl, v_src1, v_mask), vandnot_vv_u16m8(vl, v_src2, v_mask));\n    vse16_v_u16m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_15_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_mask = vmseq_vx_u16m8(vl, vle16_v_u16m8(src1, vl), mask);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n\n    vuint16m8_t v_result = vor_vv_u16m8(v_and_vv_u16m8(v_mask, v_src1, vl), v_andnot_vv_u16m8(v_mask, v_src2, vl), vl);\n\n    vse16_v_u16m8(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m8_t v_src = vle32_v_u32m8(src + i, vl);\n        vuint32m8_t v_popcount = vpopc_v_u32m8(v_src, vl);\n        vse32_v_u32m8(dst + i, v_popcount, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m8_t v_src = vle32_v_u32m8(src + i, vl);\n        vuint32m8_t v_popcount = vpopc_v_u32m8(v_src, vl);\n        vse32_v_u32m8(dst + i, v_popcount, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m8_t v_src = vle32_v_u32m8(src + i, vl);\n        vuint32m8_t v_popcount = vpopc_v_u32m8(v_src, vl);\n        vse32_v_u32m8(dst + i, v_popcount, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m8_t v_src = vle32_v_u32m8(src + i, vl);\n        vuint32m8_t v_popcount = vpopc_v_u32m8(v_src, vl);\n        vse32_v_u32m8(dst + i, v_popcount, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_16_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m8_t v_src = vle32_v_u32m8(src + i, vl);\n        vuint32m8_t v_popcount = vpopc_v_u32m8(v_src, vl);\n        vse32_v_u32m8(dst + i, v_popcount, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n    vse16_v_u16m8(dst, v_dst, vl);\n\n    while (length > vl) {\n        length -= vl;\n        src1 += vl;\n        src2 += vl;\n        dst += vl;\n        vl = vsetvl_e16m8(length);\n        v_src1 = vle16_v_u16m8(src1, vl);\n        v_src2 = vle16_v_u16m8(src2, vl);\n        v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n        vse16_v_u16m8(dst, v_dst, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n    vse16_v_u16m8(dst, v_dst, vl);\n\n    while (length > vl) {\n        length -= vl;\n        src1 += vl;\n        src2 += vl;\n        dst += vl;\n        vl = vsetvl_e16m8(length);\n        v_src1 = vle16_v_u16m8(src1, vl);\n        v_src2 = vle16_v_u16m8(src2, vl);\n        v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n        vse16_v_u16m8(dst, v_dst, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n    vse16_v_u16m8(dst, v_dst, vl);\n\n    while (length > vl) {\n        length -= vl;\n        src1 += vl;\n        src2 += vl;\n        dst += vl;\n\n        vl = vsetvl_e16m8(length);\n        v_src1 = vle16_v_u16m8(src1, vl);\n        v_src2 = vle16_v_u16m8(src2, vl);\n        v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n        vse16_v_u16m8(dst, v_dst, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n    vse16_v_u16m8(dst, v_dst, vl);\n\n    while (length > vl) {\n        length -= vl;\n        src1 += vl;\n        src2 += vl;\n        dst += vl;\n        vl = vsetvl_e16m8(length);\n        v_src1 = vle16_v_u16m8(src1, vl);\n        v_src2 = vle16_v_u16m8(src2, vl);\n        v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n        vse16_v_u16m8(dst, v_dst, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_17_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t v_src1 = vle16_v_u16m8(src1, vl);\n    vuint16m8_t v_src2 = vle16_v_u16m8(src2, vl);\n    vuint16m8_t v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n    vse16_v_u16m8(dst, v_dst, vl);\n\n    while (length > vl) {\n        length -= vl;\n        src1 += vl;\n        src2 += vl;\n        dst += vl;\n\n        vl = vsetvl_e16m8(length);\n        v_src1 = vle16_v_u16m8(src1, vl);\n        v_src2 = vle16_v_u16m8(src2, vl);\n        v_dst = vsaddu_vv_u16m8(v_src1, v_src2, vl);\n        vse16_v_u16m8(dst, v_dst, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t va, vb, vc;\n    vbool64_t mask;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vle64_v_f64m8(&C[i * p + j], p);\n            for (k = 0; k < n; k += 8) {\n                va = vle64_v_f64m8(&A[i * n + k], 8);\n                vb = vle64_v_f64m8(&B[k * p + j], 8);\n                mask = vmseq_vv_f64m8(va, -100.0);\n                va = vmin_vf64m8(va, 100.0, mask);\n                mask = vmseq_vv_f64m8(vb, -100.0);\n                vb = vmin_vf64m8(vb, 100.0, mask);\n                vc = vfma_vvfm_f64m8(vc, va, vb, 0.0);\n            }\n            vse64_v_f64m8(&C[i * p + j], vc, p);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    vfloat64m8_t va, vb, vc;\n    vbool64_t mask;\n    size_t i, j, k;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vfmv_v_f_f64m8(0.0);\n\n            for (k = 0; k < n; k += 8) {\n                va = vlw_v_f64m8(A + i * n + k, 8);\n                vb = vlw_v_f64m8(B + k * p + j, 8);\n\n                mask = vmseq_vv_f64m8(va, -100.0);\n                va = vmin_vf_f64m8(va, 100.0, mask);\n                mask = vmseq_vv_f64m8(vb, -100.0);\n                vb = vmin_vf_f64m8(vb, 100.0, mask);\n\n                vc = vfma_vv_f64m8(vc, va, vb);\n            }\n\n            stw_v_f64m8(C + i * p + j, vc, 8);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    vfloat64m8_t va, vb, vc;\n    vbool64_t mask;\n    size_t i, j, k;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vfmv_v_f_f64m8(0.0);\n\n            for (k = 0; k < n; k += 8) {\n                va = vlw_v_f64m8(A + i * n + k, 8);\n                vb = vlw_v_f64m8(B + k * p + j, 8);\n\n                mask = vmseq_vv_f64m8(va, -100.0);\n                va = vfcvt_x_f_f64m8(va, mask);\n                mask = vmslt_vv_f64m8(va, 100.0);\n                va = vfcvt_x_f_f64m8(va, mask);\n\n                mask = vmseq_vv_f64m8(vb, -100.0);\n                vb = vfcvt_x_f_f64m8(vb, mask);\n                mask = vmslt_vv_f64m8(vb, 100.0);\n                vb = vfcvt_x_f_f64m8(vb, mask);\n\n                vc = vfadd_vv_f64m8(vc, vfmul_vv_f64m8(va, vb));\n            }\n\n            stw_v_f64m8(C + i * p + j, vc, 8);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t va, vb, vc;\n    vbool64_t mask;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vfmv_v_f_f64m8(0.0);\n\n            for (k = 0; k < n; k += 8) {\n                va = vlw_v_f64m8(A + i * n + k, 8);\n                vb = vlw_v_f64m8(B + k * p + j, 8);\n\n                mask = vmseq_vv_f64m8(va, -100.0);\n                va = vfcvt_x_f_f64m8(va, mask);\n                mask = vmslt_vv_f64m8(va, 100.0);\n                va = vfcvt_x_f_f64m8(va, mask);\n\n                mask = vmseq_vv_f64m8(vb, -100.0);\n                vb = vfcvt_x_f_f64m8(vb, mask);\n                mask = vmslt_vv_f64m8(vb, 100.0);\n                vb = vfcvt_x_f_f64m8(vb, mask);\n\n                vc = vfadd_vv_f64m8(vc, vfmul_vv_f64m8(va, vb));\n            }\n\n            stw_v_f64m8(C + i * p + j, vc, 8);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    vfloat64m8_t va, vb, vc;\n    vbool64_t mask;\n    size_t i, j, k;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            vc = vfmv_v_f_f64m8(0.0);\n\n            for (k = 0; k < n; k += VLMAX) {\n                va = vlw_v_f64m8(A + i * n + k, VLMAX);\n                vb = vlw_v_f64m8(B + k * p + j, VLMAX);\n\n                mask = vmseq_vv_f64m8(va, -100.0, VLMAX);\n                va = vfcvt_x_f_f64m8(va, mask, VLMAX);\n\n                mask = vmslt_vv_f64m8(va, 100.0, VLMAX);\n                va = vfcvt_x_f_f64m8(va, mask, VLMAX);\n\n                mask = vmseq_vv_f64m8(vb, -100.0, VLMAX);\n                vb = vfcvt_x_f_f64m8(vb, mask, VLMAX);\n\n                mask = vmslt_vv_f64m8(vb, 100.0, VLMAX);\n                vb = vfcvt_x_f_f64m8(vb, mask, VLMAX);\n\n                vc = vfadd_vv_f64m8(vc, vfmul_vv_f64m8(va, vb, VLMAX), VLMAX);\n            }\n\n            stw_v_f64m8(C + i * p + j, vc, VLMAX);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vector_size = vsetvl_e64m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vector_size) {\n        vint64m1_t va = vle64_v_i64m1(A + i, vector_size);\n        vint64m1_t vb = vle64_v_i64m1(B + i, vector_size);\n        vint64m1_t vc = vfadd_vv_i64m1(va, vb, vector_size);\n        vse64_v_i64m1(C + i, vc, vector_size);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e64m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint64m1_t va = __riscv_vle64_v_i64m1(A + i, vlen);\n        vint64m1_t vb = __riscv_vle64_v_i64m1(B + i, vlen);\n        vint64m1_t vc = __riscv_vsadd_vv_i64m1(va, vb, vlen);\n        __riscv_vse64_v_i64m1(C + i, vc, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e64m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint64m1_t va = __riscv_vle64_v_i64m1(A + i, vlen);\n        vint64m1_t vb = __riscv_vle64_v_i64m1(B + i, vlen);\n        vint64m1_t vc = __riscv_vsadd_vv_i64m1(va, vb, vlen);\n        __riscv_vse64_v_i64m1(C + i, vc, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e64m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint64m1_t va = __riscv_vle64_v_i64m1(A + i, vlen);\n        vint64m1_t vb = __riscv_vle64_v_i64m1(B + i, vlen);\n        vint64m1_t vc = __riscv_vsadd_vv_i64m1(va, vb, vlen);\n        __riscv_vse64_v_i64m1(C + i, vc, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_19_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e64m8(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint64m8_t va = __riscv_vle64_v_i64m8(A + i, vlen);\n        vint64m8_t vb = __riscv_vle64_v_i64m8(B + i, vlen);\n        vint64m8_t vc = __riscv_vsadd_vv_i64m8(va, vb, vlen);\n        __riscv_vse64_v_i64m8(C + i, vc, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_result = vfadd_vv_f32m8(v_src1, v_src2, vl);\n    vint32m8_t v_int_result = vrnd_rtz_v_f32m8_i32m8(v_result, vl);\n    vse32_v_f32m8(dst, v_float32m8(v_int_result), vl);\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vfadd_vv_f32m8(v_src1, v_src2, vl);\n    vfloat32m8_t v_rounded = vrnd_rtz_v_f32m8(v_dst, vl);\n    vse32_v_f32m8(dst, v_rounded, vl);\n\n    while (length > vl) {\n        length -= vl;\n        vl = vsetvl_e32m8(length);\n        v_src1 = vle32_v_f32m8(src1 + vl * 8, vl);\n        v_src2 = vle32_v_f32m8(src2 + vl * 8, vl);\n        v_dst = vfadd_vv_f32m8(v_src1, v_src2, vl);\n        v_rounded = vrnd_rtz_v_f32m8(v_dst, vl);\n        vse32_v_f32m8(dst + vl * 8, v_rounded, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vfadd_vv_f32m8(v_src1, v_src2, vl);\n    vint32m8_t v_int_dst = vrnd_rtz_v_f32m8_i32m8(v_dst, vl);\n    vse32_v_f32m8(dst, v_float_cast_i32m8_f32m8(v_int_dst), vl);\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vfadd_vv_f32m8(v_src1, v_src2, vl);\n    vfloat32m8_t v_rounded = vrnd_rtz_f32m8(v_dst, vl);\n    vse32_v_f32m8(dst, v_rounded, vl);\n\n    while (length > vl) {\n        length -= vl;\n        vl = vsetvl_e32m8(length);\n        v_src1 = vle32_v_f32m8(src1 + vl * 8, vl);\n        v_src2 = vle32_v_f32m8(src2 + vl * 8, vl);\n        v_dst = vfadd_vv_f32m8(v_src1, v_src2, vl);\n        v_rounded = vrnd_rtz_f32m8(v_dst, vl);\n        vse32_v_f32m8(dst + vl * 8, v_rounded, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_20_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_result = vfadd_vv_f32m8(v_src1, v_src2, vl);\n    vint32m8_t v_int_result = vrndra_rtz_v_f32m8_i32m8(v_result, vl);\n    vse32_v_f32m8(dst, v_float32m8(v_int_result), vl);\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vmul_vv_f32m8(v_src1, v_src2, vl);\n\n    vbool8_t mask = vand_vv_b8(vmseq_vf_f32m8(vabs_vf_f32m8(vsub_vv_f32m8(v_src1, v_src2, vl)), 50.0f, vl), vmseq_vx_b8(vrem_vx_f32m8(vadd_vx_f32m8(vlmul_vv_f32m8(v_src1, v_src2, vl), 0.5f, vl), vl), 2, vl), vl);\n    v_dst = vor_vv_f32m8(v_dst, vfne_vx_f32m8(v_dst, -1.0f, vl), vl);\n    v_dst = vselect(mask, v_dst, vfne_vx_f32m8(v_dst, -1.0f, vl), vl);\n\n    vse32_v_f32m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vmul_vv_f32m8(v_src1, v_src2, vl);\n\n    for (size_t i = 0; i < vl; i++) {\n        if (fabsf(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            v_dst[i] = ceilf(v_dst[i]);\n        } else {\n            v_dst[i] = -1.0f;\n        }\n    }\n\n    vse32_v_f32m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vmul_vv_f32m8(v_src1, v_src2, vl);\n\n    for (size_t i = 0; i < vl; i++) {\n        if (fabsf(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            v_dst[i] = ceilf(v_dst[i]);\n        } else {\n            v_dst[i] = -1.0f;\n        }\n    }\n\n    vse32_v_f32m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vmul_vv_f32m8(v_src1, v_src2, vl);\n\n    vbool8_t mask = vand_vv_b8(vmseq_vf_f32m8(vabs_vf_f32m8(vsub_vv_f32m8(v_src1, v_src2, vl)), 50.0f, vl), vmseq_vx_i32m8(vle32_v_i32m8((const int32_t*)src1, vl), 0, vl));\n    v_dst = vor_vv_f32m8(v_dst, vfne_vx_f32m8(v_dst, -1.0f, vl), vl);\n    v_dst = vor_vv_f32m8(v_dst, vselect_vvm_f32m8(mask, v_dst, vfne_vx_f32m8(v_dst, -1.0f, vl), vl), vl);\n\n    vse32_v_f32m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_21_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t vl = vsetvl_e32m8(length);\n    vfloat32m8_t v_src1 = vle32_v_f32m8(src1, vl);\n    vfloat32m8_t v_src2 = vle32_v_f32m8(src2, vl);\n    vfloat32m8_t v_dst = vmul_vv_f32m8(v_src1, v_src2, vl);\n\n    vbool8_t mask = vand_vv_b8(vmseq_vf_f32m8(v_abs_v_f32m8(v_sub_vv_f32m8(v_src1, v_src2, vl)), 50.0f, vl), vmseq_vx_b8(vrem_vf_f32m8(v_src1, 2.0f, vl), 0.0f, vl), vl);\n    v_dst = vor_vv_f32m8(v_dst, vfsub_vf_f32m8(v_dst, 1.0f, vl), mask);\n    v_dst = vrndau_v_f32m8(v_dst, vl);\n\n    vse32_v_f32m8(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m1_t v_mat = vle64_v_f64m1(mat + i, vsetvl_e64m1(vlen_d()));\n        vfloat64m1_t v_scalar = vfmul_vf_f64m1(v_mat, scalar, vsetvl_e64m1(vlen_d()));\n        vse64_v_f64m1(dst + i, v_scalar, vsetvl_e64m1(vlen_d()));\n    }\n    for (; i < m * n; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m1_t vec_mat = vle64_v_f64m1(mat + i, vsetvl_e64m1(vlen_d()));\n        vfloat64m1_t vec_scalar = vfmul_vf_f64m1(vec_mat, scalar, vsetvl_e64m1(vlen_d()));\n        vse64_v_f64m1(dst + i, vec_scalar, vsetvl_e64m1(vlen_d()));\n    }\n    for (; i < m * n; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m1_t v_mat = vle64_v_f64m1(mat + i, vsetvl_e64m1(vlen_d()));\n        vfloat64m1_t v_scalar = vfmul_vf_f64m1(v_mat, scalar, vsetvl_e64m1(vlen_d()));\n        vse64_v_f64m1(dst + i, v_scalar, vsetvl_e64m1(vlen_d()));\n    }\n    for (; i < m * n; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m1_t vec_mat = vle64_v_f64m1(mat + i, vsetvl_e64m1(vlen_d()));\n        vfloat64m1_t vec_scalar = vfmul_vf_f64m1(vec_mat, scalar, vsetvl_e64m1(vlen_d()));\n        vse64_v_f64m1(dst + i, vec_scalar, vsetvl_e64m1(vlen_d()));\n    }\n    for (; i < m * n; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m1_t vec_mat = vle64_v_f64m1(mat + i, vsetvl_e64m1(vlen_d()));\n        vfloat64m1_t vec_scalar = vfmul_vf_f64m1(vec_mat, scalar, vsetvl_e64m1(vlen_d()));\n        vse64_v_f64m1(dst + i, vec_scalar, vsetvl_e64m1(vlen_d()));\n    }\n    for (; i < m * n; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    size_t vlen = __riscv_vsetvl_e32m8(n);\n    float32x8_t vzero = __riscv_vfmv_v_f_f32m8(0.0f, vlen);\n    float32x8_t vsqrt = __riscv_vfmv_v_f_f32m8(1.0f / sqrt(2), vlen);\n\n    for (size_t i = 0; i < m; ++i) {\n        float32x8_t vsum = vzero;\n        for (size_t j = 0; j < n; j += vlen) {\n            float32x8_t vrow = __riscv_vle32_v_f32m8(mat + i * n + j, vlen);\n            vsum = __riscv_vfredosum_vs_f32m8_f32m8(vsum, vrow * vrow, vlen);\n        }\n        vsum = __riscv_vsqrt_v_f32m8(vsum, vlen);\n        vsum = __riscv_vmul_vv_f32m8(vsum, vsqrt, vlen);\n        for (size_t j = 0; j < n; j += vlen) {\n            float32x8_t vrow = __riscv_vle32_v_f32m8(mat + i * n + j, vlen);\n            vrow = __riscv_vdiv_vv_f32m8(vrow, vsum, vlen);\n            __riscv_vse32_v_f32m8(mat + i * n + j, vrow, vlen);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    vfloat32m8_t vzero = vfmv_v_f_f32m8(0.0f, vlmax_vv_i64m1(vlmax_vx_i64m1(n, 1), 1));\n    vfloat32m8_t vsqrt = vfredosum_vs_f32m8_f32m8_m(vsqrt_v_f32m8(vfmul_vv_f32m8(mat, mat, vlmax_vv_i64m1(n, 1)), vlmax_vv_i64m1(n, 1)), vzero, vlmax_vv_i64m1(m, 1));\n\n    for (size_t i = 0; i < m; ++i) {\n        vfloat32m8_t vrow = vle32_v_f32m8(mat + i * n, vlmax_vv_i64m1(n, 1));\n        vfloat32m8_t vnorm = vdiv_vv_f32m8(vrow, vsqrt[i], vlmax_vv_i64m1(n, 1));\n        vse32_v_f32m8(mat + i * n, vnorm, vlmax_vv_i64m1(n, 1));\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    size_t vlen = __riscv_vsetvl_e32m8(n);\n    float32x8_t vzero = __riscv_vfmv_v_f_f32m8(0.0f, vlen);\n    float32x8_t vsqrt = __riscv_vfsgnjx_vv_f32m8(vzero, vzero, vlen);\n\n    for (size_t i = 0; i < m; ++i) {\n        float32x8_t vsum = vzero;\n        for (size_t j = 0; j < n; j += vlen) {\n            size_t vl = __riscv_vsetvl_e32m8(n - j);\n            float32x8_t vrow = __riscv_vle32_v_f32m8(mat + i * n + j, vl);\n            vsum = __riscv_vfredosum_vs_f32m8_f32m8(vsum, vrow * vrow, vl);\n        }\n        float32_t sum = __riscv_vfmv_s_f_f32(vsum);\n        float norm = sqrt(sum);\n        if (norm == 0.0f) {\n            norm = 1.0f;\n        }\n        for (size_t j = 0; j < n; j += vlen) {\n            size_t vl = __riscv_vsetvl_e32m8(n - j);\n            float32x8_t vrow = __riscv_vle32_v_f32m8(mat + i * n + j, vl);\n            vrow = __riscv_vfdiv_vv_f32m8(vrow, __riscv_vfmv_v_f_f32m8(norm, vl));\n            __riscv_vse32_v_f32m8(mat + i * n + j, vrow, vl);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    size_t vlen = __riscv_vsetvl_e32m8();\n    size_t vlmax = __riscv_vsetvlmax_e32m8();\n\n    for (size_t i = 0; i < m; ++i) {\n        float* row_start = &mat[i * n];\n        float sum_of_squares = 0.0f;\n\n        for (size_t j = 0; j < n; j += vlmax) {\n            vfloat32m8_t vrow = __riscv_vle32_v_f32m8(row_start + j, vlmax);\n            vfloat32m8_t vrow_squared = __riscv_vmul_vv_f32m8(vrow, vrow, vlmax);\n            vfloat32m1_t vsum_of_squares = __riscv_vsredsum_vs_f32m8_f32m1(vrow_squared, vsum_of_squares, vlmax);\n            sum_of_squares += __riscv_vfmacc_f32m1_f32m1_f32m1(0.0f, vsum_of_squares, 0.0f);\n        }\n\n        float l2_norm = sqrt(sum_of_squares);\n\n        for (size_t j = 0; j < n; j += vlen) {\n            vfloat32m8_t vrow = __riscv_vle32_v_f32m8(row_start + j, vlen);\n            vfloat32m8_t vnormed_row = __riscv_vdiv_vf_f32m8(vrow, l2_norm, vlen);\n            __riscv_vse32_v_f32m8(row_start + j, vnormed_row, vlen);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    vfloat32m8_t vzero = vle32_v_f32m8(mat, n * sizeof(float));\n    vfloat32m8_t vsqrsum;\n    vfloat32m8_t vnorm;\n    vfloat32m8_t vinvnorm;\n\n    for (size_t i = 0; i < m; ++i) {\n        vsqrsum = vfmul_vv_f32m8(vzero, vzero);\n        for (size_t j = 0; j < n; j += 8) {\n            vfloat32m8_t vrow = vle32_v_f32m8(&mat[i * n + j], 8 * sizeof(float));\n            vsqrsum = vfadd_vv_f32m8(vsqrsum, vfmul_vv_f32m8(vrow, vrow));\n        }\n        vnorm = vsqrt_v_f32m8(vsqrsum);\n        vinvnorm = vfdiv_vf_f32m8(vone, vnorm);\n        for (size_t j = 0; j < n; j += 8) {\n            vfloat32m8_t vrow = vle32_v_f32m8(&mat[i * n + j], 8 * sizeof(float));\n            vfloat32m8_t vnormalized = vfmul_vv_f32m8(vrow, vinvnorm);\n            vse32_v_f32m8(&mat[i * n + j], 8 * sizeof(float), vnormalized);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    for (i = 0; i < m; i += vsetvl_e64m1(m)) {\n        size_t vl = vsetvl_e64m1(m - i);\n        vfloat64m1_t v_dst = vmul_vv_f64(vle64_v_f64m1(mat1 + i * n, vl), vle64_v_f64m1(mat2, vl), vl);\n        for (j = 1; j < n; j++) {\n            v_dst = vfadd_vv_f64(v_dst, vmul_vv_f64(vle64_v_f64m1(mat1 + i * n + j * p, vl), vle64_v_f64m1(mat2 + j * p, vl), vl), vl);\n        }\n        vse64_v_f64m1(dst + i * n, vrnd_rtz_f64m1(v_dst, vl), vl);\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t v_mat1, v_mat2, v_res;\n    vint64m8_t v_dst;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j += 8) {\n            v_res = vle64_v_f64m8(mat1 + i * n, n);\n            for (k = 0; k < n; k++) {\n                v_mat2 = vle64_v_f64m8(mat2 + k * p + j, p);\n                v_res = vfma_vv_f64m8(v_res, v_mat2, *(mat1 + i * n + k));\n            }\n            v_dst = vrnd_v_f64m8(v_res);\n            vse64_v_f64m8(dst + i * p + j, v_dst, 8);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    vfloat64m8_t v_a, v_b, v_c;\n    vint64m8_t v_result;\n    size_t i, j, k;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j += 8) {\n            v_c = vle64_v_f64m8(&dst[i * p + j], 8);\n\n            for (k = 0; k < n; k++) {\n                v_a = vle64_v_f64m8(&mat1[i * n + k], 8);\n                v_b = vle64_v_f64m8(&mat2[k * p + j], 8);\n                v_c = vfadd_vv_f64m8(v_c, vmul_vv_f64m8(v_a, v_b));\n            }\n\n            v_result = vrndra_v_f64m8(v_c);\n            vse64_v_i64m8(&dst[i * p + j], v_result, 8);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    vfloat64m8_t v_a, v_b, v_c;\n    vint64m8_t v_result;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < p; j++) {\n            v_c = vle64_v_f64m8(&dst[i * p + j], p);\n            for (k = 0; k < n; k += 8) {\n                v_a = vle64_v_f64m8(&mat1[i * n + k], 8);\n                v_b = vle64_v_f64m8(&mat2[k * p + j], 8);\n                v_c = vfadd_vv_f64m8(v_c, vmul_vv_f64m8(v_a, v_b));\n            }\n            v_result = vrndra_v_f64m8(v_c);\n            vse64_v_i64m8((long long*)&dst[i * p + j], v_result, p);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    size_t i, j, k;\n    for (i = 0; i < m; i += vsetvl_e64m8(1)) {\n        size_t vl = vsetvl_e64m8(1);\n        vfloat64m8_t v_dst = vmul_vv_f64m8(vle64_v_f64m8(mat1 + i * n, vl), vle64_v_f64m8(mat2, vl), vl);\n        for (j = 1; j < n; j++) {\n            v_dst = vfadd_vv_f64m8(v_dst, vmul_vv_f64m8(vle64_v_f64m8(mat1 + i * n + j * n, vl), vle64_v_f64m8(mat2 + j * p, vl), vl), vl);\n        }\n        vint64m8_t v_dst_rnd = vrnd_v_f64m8(v_dst, vl);\n        vse64_v_i64m8(dst + i * p, v_dst_rnd, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t vlen = __riscv_vsetvl_e32m1(cols);\n    size_t vlmax = __riscv_vsetvlmax_e32m1();\n    size_t i, j;\n\n    for (i = 0; i < rows; i += vlmax) {\n        size_t row_vl = __riscv_vsetvl_e32m1(rows - i);\n        vfloat32m1_t v_src = __riscv_vle32_v_f32m1(src + i * cols, row_vl);\n\n        for (j = 0; j < cols; j += vlen) {\n            vfloat32m1_t v_dst = __riscv_vsra_vx_f32m1(v_src, vfloat32m1_set_s(2), row_vl);\n            v_dst = __riscv_vfmul_vf_f32m1(v_dst, vfloat32m1_set_s(4), row_vl);\n            v_dst = __riscv_vfcvt_rtz_x_f_f32m1(v_dst, row_vl);\n            v_dst = __riscv_vfmul_vf_f32m1(v_dst, vfloat32m1_set_s(0.25), row_vl);\n            __riscv_vse32_v_f32m1(dst + j * rows + i, v_dst, row_vl);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t vlen = __riscv_vsetvl_e32m1(cols);\n    vfloat32m1_t v_src, v_dst;\n    size_t i, j;\n\n    for (i = 0; i < rows; i += vlen) {\n        v_src = __riscv_vle32_v_f32m1(src + i * cols, vlen);\n        for (j = 0; j < cols; j++) {\n            v_dst = __riscv_vfadd_vf_f32m1(__riscv_vsra_vx_f32m1(v_src, 2), 0.5f, vlen);\n            __riscv_vse32_v_f32m1(dst + j * rows + i, v_dst, vlen);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t vlen = __riscv_vsetvl_e32m1(cols);\n    size_t vlmax = __riscv_vsetvlmax_e32m1();\n    size_t i, j;\n\n    for (i = 0; i < rows; i += vlmax) {\n        size_t row_vl = __riscv_vsetvl_e32m1(rows - i);\n        vfloat32m1_t v_src = __riscv_vle32_v_f32m1(src + i * cols, row_vl);\n\n        for (j = 0; j < cols; j += vlen) {\n            vfloat32m1_t v_dst = __riscv_vsra_vx_f32m1(v_src, 2, row_vl);\n            __riscv_vse32_v_f32m1(dst + j * rows + i, v_dst, row_vl);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t vlen = __riscv_vsetvl_e32m1(cols);\n    vfloat32m1_t v_src, v_dst;\n    size_t i, j;\n\n    for (i = 0; i < rows; i += vlen) {\n        v_src = __riscv_vle32_v_f32m1(src + i * cols, vlen);\n        for (j = 0; j < cols; j++) {\n            v_dst = __riscv_vsra_vx_f32m1(v_src, v_src, 2);\n            __riscv_vse32_v_f32m1(dst + j * rows + i, v_dst, vlen);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t vlen = __riscv_vsetvl_e32m1(cols);\n    size_t vlmax = __riscv_vsetvlmax_e32m1();\n    size_t i, j;\n\n    for (i = 0; i < rows; i += vlmax) {\n        size_t row_vl = __riscv_vsetvl_e32m1(rows - i);\n        vfloat32m1_t v_src = __riscv_vle32_v_f32m1(src + i * cols, row_vl);\n\n        for (j = 0; j < cols; j += vlen) {\n            vfloat32m1_t v_dst = __riscv_vfmul_vf_f32m1(v_src, 4.0f, vlen);\n            vfloat32m1_t v_rounded = __riscv_vfrint_n_f32m1(v_dst, 0, vlen);\n            vfloat32m1_t v_scaled = __riscv_vfmul_vf_f32m1(v_rounded, 0.25f, vlen);\n            __riscv_vse32_v_f32m1(dst + j * rows + i, v_scaled, vlen);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e32m8(1);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vfloat32m8_t v_tensor1 = __riscv_vle32_v_f32m8(tensor1 + i, vlen);\n        vfloat32m8_t v_tensor2 = __riscv_vle32_v_f32m8(tensor2 + i, vlen);\n        vfloat32m8_t v_result = __riscv_vsra_vf_f32m8(v_tensor1 - v_tensor2, 0, vlen);\n        __riscv_vse32_v_f32m8(dst + i, v_result, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e32m8(1); // Set vector length for 32-bit elements\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vfloat32m8_t vec1 = __riscv_vle32_v_f32m8(tensor1 + i, vlen);\n        vfloat32m8_t vec2 = __riscv_vle32_v_f32m8(tensor2 + i, vlen);\n        vfloat32m8_t sub_result = __riscv_vsra_vf_f32m8(vec1, vec2, 0); // Subtract and round towards zero\n        __riscv_vse32_v_f32m8(dst + i, sub_result, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e32m8(1); // Set vector length for 32-bit elements\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vfloat32m8_t vec1 = __riscv_vle32_v_f32m8(tensor1 + i, vlen);\n        vfloat32m8_t vec2 = __riscv_vle32_v_f32m8(tensor2 + i, vlen);\n        vfloat32m8_t sub_result = __riscv_vsra_vf_f32m8(vec1, vec2, 0); // Subtract and round towards zero\n        __riscv_vse32_v_f32m8(dst + i, sub_result, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e32m8(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vfloat32m8_t v_tensor1 = __riscv_vle32_v_f32m8(tensor1 + i, vlen);\n        vfloat32m8_t v_tensor2 = __riscv_vle32_v_f32m8(tensor2 + i, vlen);\n        vfloat32m8_t v_result = __riscv_vsra_vf_f32m8(v_tensor1 - v_tensor2, 0, vlen);\n        __riscv_vse32_v_f32m8(dst + i, v_result, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_26_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e32m8(1); // Set vector length for 32-bit elements\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vfloat32m8_t v_tensor1 = __riscv_vle32_v_f32m8(tensor1 + i, vlen);\n        vfloat32m8_t v_tensor2 = __riscv_vle32_v_f32m8(tensor2 + i, vlen);\n        vfloat32m8_t v_result = __riscv_vsra_vf_f32m8(v_tensor1 - v_tensor2, 0, vlen);\n        __riscv_vse32_v_f32m8(dst + i, v_result, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m8_t v_mat1 = vle64_v_f64m8(mat1 + i, vlen_d());\n        vfloat64m8_t v_mat2 = vle64_v_f64m8(mat2 + i, vlen_d());\n        vfloat64m8_t v_dst = vfma_vv_f64(vzero_v_f64m8(), v_mat1, v_mat2);\n        vse64_v_f64m8(dst + i, v_dst, vlen_d());\n    }\n    for (; i < m * n; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m8_t v_mat1 = vle64_v_f64m8(mat1 + i, vlen_d());\n        vfloat64m8_t v_mat2 = vle64_v_f64m8(mat2 + i, vlen_d());\n        vfloat64m8_t v_dst = vfma_vv_f64(vfsub_vv_f64(v_mat1, v_mat1), v_mat2, v_mat1);\n        vse64_v_f64(dst + i, v_dst, vlen_d());\n    }\n    for (; i < m * n; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m8_t v_mat1 = vle64_v_f64m8(mat1 + i, vsetvl_e64m8(vlen_d()));\n        vfloat64m8_t v_mat2 = vle64_v_f64m8(mat2 + i, vsetvl_e64m8(vlen_d()));\n        vfloat64m8_t v_dst = vfma_vv_f64m8(vzero_v_f64m8(), v_mat1, v_mat2, vsetvl_e64m8(vlen_d()));\n        vse64_v_f64m8(dst + i, v_dst, vsetvl_e64m8(vlen_d()));\n    }\n    for (; i < m * n; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m8_t v_mat1 = vle64_v_f64m8(mat1 + i, vlen_d());\n        vfloat64m8_t v_mat2 = vle64_v_f64m8(mat2 + i, vlen_d());\n        vfloat64m8_t v_dst = vfma_vv_f64(vfsub_vv_f64(v_mat1, v_mat1), v_mat2, v_mat1);\n        vse64_v_f64(dst + i, v_dst, vlen_d());\n    }\n    for (; i < m * n; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i + vlen_d() <= m * n; i += vlen_d()) {\n        vfloat64m8_t vec_mat1 = vle64_v_f64m8(mat1 + i, vsetvl_e64m8(vlen_d()));\n        vfloat64m8_t vec_mat2 = vle64_v_f64m8(mat2 + i, vsetvl_e64m8(vlen_d()));\n        vfloat64m8_t vec_dst = vmul_vv_f64m8(vec_mat1, vec_mat2, vsetvl_e64m8(vlen_d()));\n        vse64_v_f64m8(dst + i, vec_dst, vsetvl_e64m8(vlen_d()));\n    }\n    for (; i < m * n; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    vfloat32m8_t v_vec = vle32_v_f32m8(vec, n * sizeof(float));\n    for (size_t i = 0; i < m; i++) {\n        vfloat32m8_t v_row = vle32_v_f32m8(mat + i * n, n * sizeof(float));\n        vfloat32m8_t v_result = vfma_vv_f32m8(vzero_v_f32m8(), v_row, v_vec);\n        vse32_v_f32m8(dst + i, v_result, n * sizeof(float));\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t vlen = __riscv_vsetvl_e32m8(n);\n    vfloat32m8_t v_vec = __riscv_vle32_v_f32m8(vec, vlen);\n\n    for (size_t i = 0; i < m; ++i) {\n        vfloat32m8_t v_mat_row = __riscv_vle32_v_f32m8(mat + i * n, vlen);\n        vfloat32m8_t v_result = __riscv_vfmul_vv_f32m8(v_mat_row, v_vec, vlen);\n        float32m1_t v_sum = __riscv_vsredsum_vs_f32m8_f32m1(v_result, v_sum, vlen);\n        __riscv_vse32_v_f32m1(dst + i, v_sum, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i;\n    vfloat32m8_t v_vec = vle32_v_f32m8(vec, n * sizeof(float));\n    for (i = 0; i < m; i += 8) {\n        vfloat32m8_t v_row = vle32_v_f32m8(mat + i * n, n * sizeof(float));\n        vfloat32m8_t v_result = vfma_vv_f32m8(vzero_v_f32m8(), v_row, v_vec);\n        vse32_v_f32m8(dst + i, v_result, 8 * sizeof(float));\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t vlen = __riscv_vsetvl_e32m8(n); // Set vector length for 32-bit elements\n\n    for (size_t i = 0; i < m; ++i) {\n        vfloat32m8_t row = __riscv_vle32_v_f32m8(mat + i * n, vlen); // Load row from matrix\n        vfloat32m8_t vec_vec = __riscv_vle32_v_f32m8(vec, vlen);     // Load vector\n        vfloat32m8_t prod = __riscv_vmul_vv_f32m8(row, vec_vec, vlen); // Multiply row by vector\n        vfloat32m8_t sum = __riscv_vsredsum_vs_f32m8_f32m1(prod, 0.0f, vlen); // Sum up the products\n        __riscv_vse32_v_f32m1(dst + i, sum, vlen); // Store the result in destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t vlen = __riscv_vsetvl_e32m8(n);\n    vfloat32m8_t v_vec = __riscv_vle32_v_f32m8(vec, vlen);\n\n    for (size_t i = 0; i < m; ++i) {\n        vfloat32m8_t v_mat = __riscv_vle32_v_f32m8(mat + i * n, vlen);\n        vfloat32m8_t v_result = __riscv_vfmul_vv_f32m8(v_mat, v_vec, vlen);\n        vfloat32m1_t v_sum = __riscv_vsredsum_vs_f32m8_f32m1(v_result, v_sum, vlen);\n        __riscv_vse32_v_f32m1(dst + i, v_sum, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t vector_width = vsetvl_e64m1(total_elements);\n    vfloat64m1_t sum_vec = vfmv_v_f_f64m1(0.0, vector_width);\n\n    for (size_t i = 0; i < total_elements; i += vector_width) {\n        vfloat64m1_t vec = vlw_v_f64m1(mat + i, vector_width);\n        sum_vec = vfadd_vv_f64m1(sum_vec, vec, vector_width);\n    }\n\n    double sum = vfredsum_vs_f64m1_f64m1(0.0, sum_vec, vector_width);\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    vfloat64m1_t vec;\n    vfloat64m1_t sum = vfmv_v_f_f64m1(0.0, vlmax_v_f64m1(n));\n    size_t i;\n\n    for (i = 0; i < m * n; i += vlmax_v_f64m1(n)) {\n        vec = vle64_v_f64m1(mat + i, vlmax_v_f64m1(n));\n        sum = vfadd_vv_f64m1(sum, vec, vlmax_v_f64m1(n));\n    }\n\n    double total_sum = vfredsum_vs_f64m1_f64m1(0.0, sum, vlmax_v_f64m1(n));\n    return total_sum / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```c\n#include <riscv_vector.h>\n#include <math.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t vec_size = vsetvl_e64m1(total_elements);\n    vfloat64m1_t sum_vec = vfmv_v_f_f64m1(0.0, vec_size);\n\n    for (size_t i = 0; i < total_elements; i += vec_size) {\n        vfloat64m1_t vec = vlse64_v_f64m1(mat + i, vec_size);\n        sum_vec = vfadd_vv_f64m1(sum_vec, vec, vec_size);\n    }\n\n    double sum = vfredsum_vs_f64m1_f64m1(0.0, sum_vec, vec_size);\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```c\n#include <riscv_vector.h>\n#include <math.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t vector_width = vsetvl_e64m1(total_elements);\n    vfloat64m1_t vec_sum = vfmv_v_f_f64m1(0.0, vector_width);\n\n    for (size_t i = 0; i < total_elements; i += vector_width) {\n        vfloat64m1_t vec_data = vlw_v_f64m1(mat + i, vector_width);\n        vec_sum = vfadd_vv_f64m1(vec_sum, vec_data, vector_width);\n    }\n\n    double sum = vfredsum_vs_f64m1_f64m1(0.0, vec_sum, vector_width);\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t vector_width = vsetvl_e64m1(total_elements);\n    vfloat64m1_t sum = vfmv_v_f_f64m1(0.0, vector_width);\n\n    for (size_t i = 0; i < total_elements; i += vector_width) {\n        vfloat64m1_t vec = vlw_v_f64m1(mat + i, vector_width);\n        sum = vfadd_vv_f64m1(sum, vec, vector_width);\n    }\n\n    double total_sum = vfredsum_vs_f64m1_f64m1(0.0, sum, vector_width);\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = &matrix[i * cols];\n        vint32m1_t vec_row = vle32_v_i32m1(row_start, cols);\n\n        vbool8m1_t asc_mask = vmseq_vx_i32m1_b8(vec_row, vfirst_m_b8(vcmpgt_vv_i32m1(vec_row, vle32_v_i32m1(&row_start[1], cols - 1))));\n        vbool8m1_t desc_mask = vmseq_vx_i32m1_b8(vec_row, vfirst_m_b8(vcmplt_vv_i32m1(vec_row, vle32_v_i32m1(&row_start[1], cols - 1))));\n\n        if (directions[i]) {\n            if (!vand_vv_b8(asc_mask, desc_mask)) return false;\n        } else {\n            if (!vand_not_vv_b8(asc_mask, desc_mask)) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t row_offset = i * cols;\n        vint32m1_t vec_a = vle32_v_i32m1(matrix + row_offset, cols);\n        vint32m1_t vec_b = vle32_v_i32m1(matrix + row_offset + 1, cols);\n\n        vbool8m1_t mask = vmseq_vv_i32m1_b8(vec_a, vec_b);\n        vbool8m1_t dir_mask = vle8_v_u8m1(directions + i, 1) == 0;\n\n        if (!vmfirst_m_b8(dir_mask)) {\n            mask = vmseq_vv_i32m1_b8(vec_b, vec_a);\n        }\n\n        if (!vmand_vv_b8(mask, mask)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t offset = i * cols;\n        vint32m1_t vec = vle32_v_i32m1(matrix + offset, cols);\n        vbool8_t mask = vle8_v_b8(directions + i, 1);\n\n        if (mask[0]) {\n            vint32m1_t next_vec = vle32_v_i32m1(matrix + offset + cols, cols);\n            vint32m1_t diff = vsub_vv_i32m1(next_vec, vec);\n            vbool8_t sorted = vgts_vv_i32m1(diff, vzero_v_i32m1(), cols);\n            if (!vredand_vs_u_b8(sorted, sorted, vone_v_b8())) return false;\n        } else {\n            vint32m1_t next_vec = vle32_v_i32m1(matrix + offset + cols, cols);\n            vint32m1_t diff = vsub_vv_i32m1(vec, next_vec);\n            vbool8_t sorted = vgts_vv_i32m1(diff, vzero_v_i32m1(), cols);\n            if (!vredand_vs_u_b8(sorted, sorted, vone_v_b8())) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    bool result = true;\n    size_t vector_size = vsetvl_e32m1(cols);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = matrix + i * cols;\n        const bool direction = directions[i];\n\n        vint32m1_t vec_a = vle32_v_i32m1(row_start, vector_size);\n        vint32m1_t vec_b = vle32_v_i32m1(row_start + vector_size, vector_size);\n\n        for (size_t j = 2 * vector_size; j < cols; j += 2 * vector_size) {\n            vint32m1_t vec_c = vle32_v_i32m1(row_start + j, vector_size);\n            vint32m1_t vec_d = vle32_v_i32m1(row_start + j + vector_size, vector_size);\n\n            if (direction) {\n                if (!vall_gt_vv_i32m1(vec_a, vec_b)) {\n                    result = false;\n                    break;\n                }\n                if (!vall_gt_vv_i32m1(vec_c, vec_d)) {\n                    result = false;\n                    break;\n                }\n            } else {\n                if (!vall_lt_vv_i32m1(vec_a, vec_b)) {\n                    result = false;\n                    break;\n                }\n                if (!vall_lt_vv_i32m1(vec_c, vec_d)) {\n                    result = false;\n                    break;\n                }\n            }\n\n            vec_a = vec_c;\n            vec_b = vec_d;\n        }\n\n        if (!result) {\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_30_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        size_t offset = i * cols;\n        vint32m1_t vec = vle32_v_i32m1(matrix + offset, cols);\n\n        vbool8_t mask = vmseq_vx_i32m1_b8(vec, vec, 0); // Initialize mask\n\n        for (size_t j = 1; j < cols; ++j) {\n            vint32m1_t next_vec = vle32_v_i32m1(matrix + offset + j * cols, cols);\n            vbool8_t dir_mask = vle8_v_u8(directions + i, 1) == 0 ? vmgt_vv_i32m1_b8(next_vec, vec) : vmslt_vv_i32m1_b8(next_vec, vec);\n            mask = vand_vv_b8(mask, dir_mask);\n        }\n\n        if (!vmfirst_m_b8(mask)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    vint16m8_t v_src = vle16_v_i16m8(src, vl);\n    vuint8m1_t v_base = vmv_v_x_u8m1(base, vl);\n    vint16m8_t v_result = vfsub_vf_i16m8(v_src, vrem_vx_i16m8(v_src, v_base), vl);\n    vst16_v_i16m8(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t vl = vsetvl_e16m4(length);\n    vint16m4_t v_src = vle16_v_i16m4(src, vl);\n    vuint8m1_t v_base = vmv_v_x_u8m1(base, vl);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint16m4_t v_quotient = vdiv_vx_i16m4(v_src, v_base, vl);\n        vint16m4_t v_floor = vsub_vx_i16m4(v_quotient, vrem_vx_i16m4(v_src, v_base, vl), vl);\n        vint16m4_t v_ceil = vadd_vx_i16m4(v_quotient, vvmul_vx_i16m4(vbool1_m1(vrem_vx_i16m4(v_src, v_base, vl) != 0), 1, vl), vl);\n        vint16m4_t v_nearest = vmin_vv_i16m4(vmacc_vx_i16m4(vfloor, vabs_vx_i16m4(vsub_vx_i16m4(v_src, vfloor, vl)), vl), vmacc_vx_i16m4(vceil, vabs_vx_i16m4(vsub_vx_i16m4(v_src, vceil, vl)), vl), vl);\n        vse16_v_i16m4(dst + i, v_nearest, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    vint16m4_t v_src = vle16_v_i16m4(src, vl);\n    vuint8m1_t v_base = vmv_v_x_u8m1(base, vl);\n    vint16m4_t v_result = vfsub_vv_i16m4(v_src, vmul_vx_i16m4(v_src, v_rdiv_vf_f16m8(v_src, v_base, vl), vl), vl);\n    vse16_v_i16m4(dst, v_result, vl);\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t vl = vsetvl_e16m4(length);\n    vint16m4_t v_src = vle16_v_i16m4(src, vl);\n    vuint8m1_t v_base = vmv_v_x_u8m1(base, vl);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint16m4_t v_quotient = vdiv_vx_i16m4(v_src, v_base, vl);\n        vint16m4_t v_floor = vsub_vx_i16m4(v_quotient, vmul_vx_i16m4(v_quotient, v_base, vl), vl);\n        vint16m4_t v_ceil = vadd_vx_i16m4(v_quotient, vmul_vx_i16m4(v_quotient, v_base, vl), vl);\n        vint16m4_t v_diff_floor = vsub_vv_i16m4(v_src, v_floor, vl);\n        vint16m4_t v_diff_ceil = vsub_vv_i16m4(v_ceil, v_src, vl);\n        vboolm4_t v_mask = vmin_vv_b16(v_diff_floor, v_diff_ceil, vl);\n        vint16m4_t v_result = vselect_b16(v_mask, v_floor, v_ceil, vl);\n        vse16_v_i16m4(dst + i, v_result, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_31_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t vl = vsetvl_e16m4(length);\n    vint16m4_t v_src = vle16_v_i16m4(src, vl);\n    vuint8m1_t v_base = vmv_v_x_u8m1(base, vl);\n\n    for (size_t i = 0; i < length; i += vl) {\n        vint16m4_t v_quotient = vdiv_vx_i16m4(v_src, v_base, vl);\n        vint16m4_t v_floor = vsub_vx_i16m4(v_quotient, vmul_vx_i16m4(v_quotient, v_base, vl), vl);\n        vint16m4_t v_ceil = vadd_vx_i16m4(v_quotient, vone_v_i16m4(), vl);\n        vint16m4_t v_nearest = vmin_vv_i16m4(vmacc_vx_i16m4(v_floor, v_base, v_base, vl), vmacc_vx_i16m4(v_ceil, v_base, v_base, vl), vl);\n        vse16_v_i16m4(dst + i, v_nearest, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    vfloat64m1_t v_vec = vle64_v_f64m1(vector, 1);\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_row = vle64_v_f64m1(matrix + i * cols, cols);\n        bool match = vmseq_m_b1(v_row, v_vec, cols);\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    vfloat64m1_t v_vec = vle64_v_f64m1(vector, 1);\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_row = vle64_v_f64m1(matrix + i * cols, cols);\n        bool match = vmseq_vv_f64m1_b1(v_row, v_vec, cols);\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    vfloat64m1_t v_vec = vle64_v_f64m1(vector, 1);\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_row = vle64_v_f64m1(matrix + i * cols, cols);\n        bool match = vmseq_vv_f64m1_b1(v_row, v_vec, cols);\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    vfloat64m1_t v_vec = vle64_v_f64m1(vector, 1);\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_row = vle64_v_f64m1(matrix + i * cols, cols);\n        bool match = vmseq_vv_f64m1_b1(v_row, v_vec, cols);\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    vfloat64m1_t v_vec = vle64_v_f64m1(vector, 1);\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m1_t v_row = vle64_v_f64m1(matrix + i * cols, cols);\n        bool match = vmseq_vv_f64m1_b1(v_row, v_vec, cols);\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t num_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t vlen = __riscv_vsetvl_e8m1(num_elements);\n\n    for (size_t i = 0; i < num_elements; i += vlen) {\n        vuint8m1_t va = __riscv_vle8_v_u8m1(A + i, vlen);\n        vuint8m1_t vb = __riscv_vle8_v_u8m1(B + i, vlen);\n        vboolm1_t vm = __riscv_vmseq_vv_b1m1(va, vb, vlen);\n        size_t count = __riscv_vfredosum_vs_i1m1_i64m1(vm, 0, vlen);\n        total_count += count;\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t vector_size = vsetvl_e8m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vector_size) {\n        vuint8m1_t va = vlw_v_u8m1(A + i, vector_size);\n        vuint8m1_t vb = vlw_v_u8m1(B + i, vector_size);\n        vboolm1_t eq = veq_vv_b8(va, vb, vector_size);\n        size_t true_count = vpopcnt_m_b(eq, vector_size);\n        count += true_count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t num_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t vector_size = vsetvl_e8m1(16); // Set vector length to 16\n\n    for (size_t i = 0; i < num_elements; i += vector_size) {\n        vuint8m1_t va = vlse8_v_u8m1(A + i, vector_size, vector_size);\n        vuint8m1_t vb = vlse8_v_u8m1(B + i, vector_size, vector_size);\n\n        vbool1_t mask = vmseq_vv_b1(va, vb, vector_size);\n        size_t count = vpopcnt_m_b(mask, vector_size);\n        total_count += count;\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t num_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t vlen = __riscv_vsetvl_e8m1(num_elements);\n\n    for (size_t i = 0; i < num_elements; i += vlen) {\n        vuint8m1_t va = __riscv_vle8_v_u8m1(A + i, vlen);\n        vuint8m1_t vb = __riscv_vle8_v_u8m1(B + i, vlen);\n        vboolm1_t vm = __riscv_vmseq_vv_b1m1(va, vb, vlen);\n        size_t count = __riscv_vfredosum_vs_i1m1_i64(vm, 0, vlen);\n        total_count += count;\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t num_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t vec_size = vsetvl_e8m1(64);\n\n    for (size_t i = 0; i < num_elements; i += vec_size) {\n        vuint8m1_t va = vlw_v_u8m1(A + i, vec_size);\n        vuint8m1_t vb = vlw_v_u8m1(B + i, vec_size);\n        vboolm1_t mask = vmseq_vv_b8(va, vb, vec_size);\n        size_t count = vpopcnt_m_b(mask, vec_size);\n        total_count += count;\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t va = vle64_v_i64m1(x, vl);\n    vint64m1_t vb = vmul_vx_i64m1(va, a, vl);\n    vint64m1_t vc = vabs_v_i64m1(vb, vl);\n    vse64_v_i64m1(y, vc, vl);\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t va = vle64_v_i64m1(x, vl);\n    vint64m1_t vb = vmul_vx_i64m1(va, a, vl);\n    vint64m1_t vc = vabs_v_i64m1(vb, vl);\n    vse64_v_i64m1(y, vc, vl);\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t va = vmul_vx_i64m1(vle64_v_i64m1(x, vl), a, vl);\n    vint64m1_t vy = vabs_v_i64m1(va, vl);\n    vse64_v_i64m1(y, vy, vl);\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t vx = vle64_v_i64m1(x, vl);\n    vint64m1_t va = vmul_vx_i64m1(vx, a, vl);\n    vint64m1_t vy = vabs_v_i64m1(va, vl);\n    vse64_v_i64m1(y, vy, vl);\n}\n```"}
{"task_id": "SimdBench_34_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t vl = vsetvl_e64m1(length);\n    vint64m1_t va = vle64_v_i64m1(x, vl);\n    vint64m1_t vb = vmul_vx_i64m1(va, a, vl);\n    vint64m1_t vc = vabs_v_i64m1(vb, vl);\n    vse64_v_i64m1(y, vc, vl);\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    vint16m8_t v = vle16_v_i16m8(vec, length);\n    vint16m8_t vmin = vfirst_m_i16m8(v);\n    vint16m8_t vmax = vfirst_m_i16m8(v);\n\n    for (size_t i = 0; i < length; i += 8) {\n        vint16m8_t vtemp = vle16_v_i16m8(&vec[i], 8);\n        vmin = vmin_vv_i16m8(vmin, vtemp);\n        vmax = vmax_vv_i16m8(vmax, vtemp);\n    }\n\n    MinMaxPair result;\n    result.min_num = vmv_s_x_i16m8_i16(vmin);\n    result.max_num = vmv_s_x_i16m8_i16(vmax);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    vint16m8_t v = vlseg16_v_i16m8(vec, length);\n    vint16m8_t vmin = vfredmin_vs_i16m8_i16m8(v, v, vsew_v_i16m8(16));\n    vint16m8_t vmax = vfredmax_vs_i16m8_i16m8(v, v, vsew_v_i16m8(16));\n    int16_t min_num = vmfirst_m_i16m8_mask(vmin, vle16_v_i16m8(&vmin[0], 1));\n    int16_t max_num = vmfirst_m_i16m8_mask(vmax, vle16_v_i16m8(&vmax[0], 1));\n    MinMaxPair result = {min_num, max_num};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    vint16m8_t v = vle16_v_i16m8(vec, length);\n    vint16m8_t vmin = vfirst_m_i16m8(v);\n    vint16m8_t vmax = vfirst_m_i16m8(v);\n\n    for (size_t i = vsetvl_e16m8(length); i > 0; i--) {\n        vmin = vmin_vv_i16m8(vmin, v, i);\n        vmax = vmax_vv_i16m8(vmax, v, i);\n    }\n\n    MinMaxPair result = {vle16_v_i16m8(&vmin, 1)[0], vle16_v_i16m8(&vmax, 1)[0]};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    vint16m8_t v = vle16_v_i16m8(vec, length);\n    vint16m8_t vmin = vfirst_m_i16m8(v);\n    vint16m8_t vmax = vfirst_m_i16m8(v);\n\n    for (size_t i = vsetvl_e16m8(length); i > 0; i--) {\n        vmin = vmin_vv_i16m8(vmin, v, eew_i16m8());\n        vmax = vmax_vv_i16m8(vmax, v, eew_i16m8());\n        v = vsll_vx_i16m8(v, 1, eew_i16m8());\n    }\n\n    MinMaxPair result = {vmin[0], vmax[0]};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_RVV", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = {INT16_MAX, INT16_MIN};\n    size_t vlen = __riscv_vsetvl_e16m8(length);\n\n    for (size_t i = 0; i < length; i += vlen) {\n        vint16m8_t v = __riscv_vle16_v_i16m8(vec + i, vlen);\n        vint16m8_t vmin = __riscv_vmin_vv_i16m8(v, v, vlen);\n        vint16m8_t vmax = __riscv_vmax_vv_i16m8(v, v, vlen);\n\n        int16_t min_val = __riscv_vfirst_m_i16m8(__riscv_vmseq_vx_i16m8(vmin, __riscv_vmv_v_x_i16m8(result.min_num), vlen));\n        int16_t max_val = __riscv_vfirst_m_i16m8(__riscv_vmseq_vx_i16m8(vmax, __riscv_vmv_v_x_i16m8(result.max_num), vlen));\n\n        if (min_val != -1) {\n            result.min_num = min_val;\n        }\n        if (max_val != -1) {\n            result.max_num = max_val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    size_t vec_len = vsetvl_e32m1(cols);\n    vint32m1_t prev_row = vle32_v_i32m1(matrix, vec_len);\n\n    for (size_t i = 1; i < rows; ++i) {\n        vint32m1_t curr_row = vle32_v_i32m1(matrix + i * cols, vec_len);\n        vbool32_t mask = vmul_vv_b32(vsub_vv_i32m1(curr_row, prev_row), vle32_v_i32m1(matrix + i * cols - 1, vec_len));\n        if (!vmfirst_m_b32(mask)) {\n            return false;\n        }\n        prev_row = curr_row;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    size_t vec_len = vsetvl_e32m1(cols);\n    vint32m1_t prev_row = vle32_v_i32m1(matrix, vec_len);\n\n    for (size_t i = 1; i < rows; ++i) {\n        vint32m1_t curr_row = vle32_v_i32m1(matrix + i * cols, vec_len);\n        vbool32_t mask = vmul_vv_v_b32(vsub_vv_v_i32m1(curr_row, prev_row), vle32_v_i32m1(matrix + i * cols - 1, vec_len));\n        if (!vmfirst_m_b32(mask)) {\n            return false;\n        }\n        prev_row = curr_row;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    size_t vec_len = vsetvl_e32m1(cols);\n    vint32m1_t prev_row = vle32_v_i32m1(matrix, vec_len);\n\n    for (size_t i = 1; i < rows; ++i) {\n        vint32m1_t curr_row = vle32_v_i32m1(matrix + i * cols, vec_len);\n        vbool32_t mask = vmul_vv_b32(vsub_vv_i32m1(curr_row, prev_row), vle32_v_i32m1(matrix + i * cols - 1, vec_len));\n        if (!vmfirst_m_b32(mask)) {\n            return false;\n        }\n        prev_row = curr_row;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    size_t vec_len = vsetvl_e32m1(cols);\n    vint32m1_t prev_row = vle32_v_i32m1(matrix, vec_len);\n\n    for (size_t i = 1; i < rows; ++i) {\n        vint32m1_t curr_row = vle32_v_i32m1(matrix + i * cols, vec_len);\n        vbool32_t mask = vmul_vv_v_b32(vsub_vv_v_i32m1(curr_row, prev_row), vle32_v_i32m1(matrix + i * cols - 1, vec_len));\n        if (!vmfirst_m_b32(mask)) {\n            return false;\n        }\n        prev_row = curr_row;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    size_t vec_len = vsetvl_e32m1(cols);\n    vint32m1_t prev_row = vle32_v_i32m1(matrix, vec_len);\n\n    for (size_t i = 1; i < rows; ++i) {\n        vint32m1_t curr_row = vle32_v_i32m1(matrix + i * cols, vec_len);\n        vbool32_t mask = vmul_vv_b32(vsub_vv_i32m1(curr_row, prev_row), vle32_v_i32m1(matrix + i * cols - 1, vec_len));\n        if (!vmfirst_m_b32(mask)) {\n            return false;\n        }\n        prev_row = curr_row;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n\n    vint32m1_t ref_block = vle32_v_i32m1(matrix, block_size * block_size);\n\n    for (size_t i = 1; i < num_blocks_row; ++i) {\n        const int* row_start = matrix + i * block_size * cols;\n        for (size_t j = 0; j < num_blocks_col; ++j) {\n            vint32m1_t current_block = vle32_v_i32m1(row_start + j * block_size, block_size * block_size);\n            bool equal = veq_vv_v_b_i32m1(current_block, ref_block, block_size * block_size);\n            if (!equal) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n\n    vint32m1_t ref_block = vle32_v_i32m1(matrix, block_size * block_size);\n\n    for (size_t i = 1; i < num_blocks_row; ++i) {\n        const int* row_start = matrix + i * block_size * cols;\n        for (size_t j = 0; j < num_blocks_col; ++j) {\n            vint32m1_t current_block = vle32_v_i32m1(row_start + j * block_size, block_size * block_size);\n            if (!veq_vv_v_b_i32m1(current_block, ref_block)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    vint32m1_t v_block = vle32_v_i32m1(matrix, block_size * block_size);\n    vbool16_t v_mask = vmseq_vx_i32m1_b16(v_block, vle32_v_i32m1(matrix, block_size * block_size), 0);\n\n    for (size_t i = block_size; i < rows; i += block_size) {\n        for (size_t j = block_size; j < cols; j += block_size) {\n            vint32m1_t v_current_block = vle32_v_i32m1(matrix + i * cols + j, block_size * block_size);\n            vbool16_t v_current_mask = vmseq_vx_i32m1_b16(v_current_block, v_block, 0);\n            v_mask = vbland_vv_b16(v_mask, v_current_mask);\n        }\n    }\n\n    return vfirst_m_b16(v_mask) == -1;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n\n    vint32m1_t ref_block = vle32_v_i32m1(matrix, block_size * block_size);\n\n    for (size_t i = 1; i < num_blocks_row; ++i) {\n        const int* row_start = matrix + i * block_size * cols;\n        for (size_t j = 0; j < num_blocks_col; ++j) {\n            vint32m1_t current_block = vle32_v_i32m1(row_start + j * block_size, block_size * block_size);\n            bool equal = veq_vv_v_b_i32m1(current_block, ref_block, block_size * block_size);\n            if (!equal) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n\n    vint32m1_t ref_block = vle32_v_i32m1(matrix, block_size * block_size);\n\n    for (size_t i = 1; i < num_blocks_row; ++i) {\n        const int* row_start = matrix + i * block_size * cols;\n        for (size_t j = 0; j < num_blocks_col; ++j) {\n            vint32m1_t current_block = vle32_v_i32m1(row_start + j * block_size, block_size * block_size);\n            bool equal = vmseq_vv_v_b1(current_block, ref_block, block_size * block_size);\n            if (!equal) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n\n    vfloat64m1_t v_tolerance = vle64_v_f64m1(&tolerance, 1, vlmax_v_f64m1());\n    vfloat64m1_t v_first_block = vle64_v_f64m1(vec, block_size, vlmax_v_f64m1());\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        vfloat64m1_t v_current_block = vle64_v_f64m1(vec + i * block_size, block_size, vlmax_v_f64m1());\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_current_block, v_first_block, vlmax_v_f64m1());\n        vfloat64m1_t v_abs_diff = vfsgnjx_vv_f64m1(v_diff, v_diff, vlmax_v_f64m1());\n        vbool64m1_t v_mask = vflt_m_v_f64m1(v_abs_diff, v_tolerance, vlmax_v_f64m1());\n        if (!vmseq_mask_z_b_f64m1(v_mask, vlmax_v_f64m1())) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n\n    vfloat64m1_t v_tolerance = vle64_v_f64m1(&tolerance, 1, vlmax_v_f64m1());\n    vfloat64m1_t v_first_block = vle64_v_f64m1(vec, block_size, vlmax_v_f64m1());\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        vfloat64m1_t v_current_block = vle64_v_f64m1(vec + i * block_size, block_size, vlmax_v_f64m1());\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_current_block, v_first_block, vlmax_v_f64m1());\n        vfloat64m1_t v_abs_diff = vfsgnjx_vv_f64m1(v_diff, v_diff, vlmax_v_f64m1());\n        vbool64m1_t v_mask = vflt_m_v_f64m1(v_abs_diff, v_tolerance, vlmax_v_f64m1());\n        if (!vmfirst_x_v_b64m1(v_mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n\n    vfloat64m1_t v_tolerance = vle64_v_f64m1(&tolerance, 1, vlmax_v_f64m1());\n    vfloat64m1_t v_first_block = vle64_v_f64m1(vec, block_size, vlmax_v_f64m1());\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        vfloat64m1_t v_current_block = vle64_v_f64m1(vec + i * block_size, block_size, vlmax_v_f64m1());\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_current_block, v_first_block, vlmax_v_f64m1());\n        vfloat64m1_t v_abs_diff = vfsgnjx_vv_f64m1(v_diff, v_diff, vlmax_v_f64m1());\n        vfloat64m1_t v_max_diff = vmax_vv_f64m1(v_abs_diff, v_abs_diff, vlmax_v_f64m1());\n        bool max_diff_less_than_tolerance = vmul_vf_f64m1(v_max_diff, v_tolerance, vlmax_v_f64m1()) < 1.0;\n        if (!max_diff_less_than_tolerance) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    vfloat64m1_t v_tolerance = vle64_v_f64m1(&tolerance, 1);\n    vfloat64m1_t v_first_block = vle64_v_f64m1(vec, block_size);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        vfloat64m1_t v_current_block = vle64_v_f64m1(vec + i * block_size, block_size);\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_current_block, v_first_block);\n        vfloat64m1_t v_abs_diff = vfsgnjn_vv_f64m1(v_diff, v_diff);\n        vbool64m1_t v_mask = vflt_m_b64m1(v_abs_diff, v_tolerance);\n        if (!vmfirst_m_b64m1(v_mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n\n    vfloat64m1_t v_tolerance = vle64_v_f64m1(&tolerance, 1, vlmax_v_f64m1());\n    vfloat64m1_t v_first_block = vle64_v_f64m1(vec, block_size, vlmax_v_f64m1());\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        vfloat64m1_t v_current_block = vle64_v_f64m1(vec + i * block_size, block_size, vlmax_v_f64m1());\n        vfloat64m1_t v_diff = vfsub_vv_f64m1(v_current_block, v_first_block, vlmax_v_f64m1());\n        vfloat64m1_t v_abs_diff = vfsgnjx_vv_f64m1(v_diff, v_diff, vlmax_v_f64m1());\n        vbool64m1_t v_mask = vflt_m_v_f64m1(v_abs_diff, v_tolerance, vlmax_v_f64m1());\n        if (!vmseq_mask_z_b_f64m1(v_mask, vlmax_v_f64m1())) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t vlen = __riscv_vsetvl_e64m8(1);\n    vfloat64m8_t v_thresholds;\n    vbool64m8_t v_result;\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m8_t v_sum = __riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8"}
{"task_id": "SimdBench_39_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t vlen = __riscv_vsetvl_e64m8(1);\n    size_t vlmax = __riscv_vsetvlmax_e64m8();\n    size_t i = 0;\n\n    while (i < rows) {\n        size_t vl = __riscv_vsetvl_e64m8(cols);\n        vfloat64m8_t v_matrix = __riscv_vle64_v_f64m8(matrix + i * cols, vl);\n        vfloat64m8_t v_thresholds = __riscv_vle64_v_f64m8(thresholds + i, vl);\n\n        vfloat64m8_t v_sum_squares = __riscv_vfmul_vv_f64m8(v_matrix, v_matrix, vl);\n        vfloat64m8_t v_norm = __riscv_vsqrt_v_f64m8(__riscv_vfredosum_vs_f64m8_f64m8(v_sum_squares, 0.0, vl), vl);\n\n        vbool64m8_t v_mask = __riscv_vfsgt_vv_f64m8(v_norm, v_thresholds, vl);\n        vuint32m8_t v_result = __riscv_vmseq_vx_u32m8(v_mask, 1, vl);\n\n        __riscv_vse32_v_u32m8((uint32_t*)(result + i), v_result, vl);\n\n        i += vl;\n    }\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t vlen = __riscv_vsetvl_e64m8();\n    vfloat64m8_t v_thresholds;\n    vbool64m8_t v_result;\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m8_t v_sum = __riscv_vfmv_s_f_f64m8(0.0);\n        for (size_t j = 0; j < cols; j += vlen) {\n            size_t vl = __riscv_vsetvl_e64m8(cols - j);\n            vfloat64m8_t v_matrix = __riscv_vle64_v_f64m8(matrix + i * cols + j, vl);\n            v_sum = __riscv_vfadd_vv_f64m8(v_sum, v_matrix, v_matrix, vl);\n        }\n        v_sum = __riscv_vfsqrt_v_f64m8(v_sum);\n        v_thresholds = __riscv_vle64_v_f64m8(thresholds + i, vlen);\n        v_result = __riscv_vfsgt_vv_f64m8(v_sum, v_thresholds, vlen);\n        __riscv_vse64_v_b64(result + i, v_result, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_39_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t vlen = __riscv_vsetvl_e64m8(cols);\n    vfloat64m8_t v_thresholds = __riscv_vle64_v_f64m8(thresholds, vlen);\n    vbool64_t v_result = __riscv_vmseq_vx_f64m8(false, 0.0, vlen);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_start = matrix + i * cols;\n        vfloat64m8_t v_sum = __riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__r"}
{"task_id": "SimdBench_39_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t vlen = __riscv_vsetvl_e64m8();\n    vfloat64m8_t v_thresholds;\n    vbool64_t v_result;\n\n    for (size_t i = 0; i < rows; ++i) {\n        vfloat64m8_t v_sum = __riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv_vfmacc_vf_f64m8(__riscv"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(A + i, vl);\n        vuint32m1_t vb = vle32_v_u32m1(B + i, vl);\n        vuint32m1_t vc = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(C + i, vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(A + i, vl);\n        vuint32m1_t vb = vle32_v_u32m1(B + i, vl);\n        vuint32m1_t vc = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(C + i, vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(A + i, vl);\n        vuint32m1_t vb = vle32_v_u32m1(B + i, vl);\n        vuint32m1_t vc = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(C + i, vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(A + i, vl);\n        vuint32m1_t vb = vle32_v_u32m1(B + i, vl);\n        vuint32m1_t vc = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(C + i, vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_40_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t vl = vsetvl_e32m1(length);\n    for (size_t i = 0; i < length; i += vl) {\n        vuint32m1_t va = vle32_v_u32m1(A + i, vl);\n        vuint32m1_t vb = vle32_v_u32m1(B + i, vl);\n        vuint32m1_t vc = vor_vv_u32m1(va, vb, vl);\n        vse32_v_u32m1(C + i, vc, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint16m1_t vsa2 = vwredsum_vs_i8m1_i16m1(vmul_vv_i16m1(va, va, vl), vl);\n    vint16m1_t vsb2 = vwredsum_vs_i8m1_i16m1(vmul_vv_i16m1(vb, vb, vl), vl);\n    vint32m1_t vsab = vwredsum_vs_i16m1_i32m1(vmul_vv_i32m1(va, vb, vl), vl);\n    int64_t result = (int64_t)vle32_v_i32m1(&vsab, 1)[0] - ((int64_t)vle16_v_i16m1(&vsa2, 1)[0] + (int64_t)vle16_v_i16m1(&vsb2, 1)[0]);\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint16m1_t vsa2 = vwredsum_vs_i8m1_i16m1(vmul_vv_i16m1(va, va, vl), vl);\n    vint16m1_t vsb2 = vwredsum_vs_i8m1_i16m1(vmul_vv_i16m1(vb, vb, vl), vl);\n    vint32m1_t vab = vmul_vv_i32m1(va, vb, vl);\n    vint32m1_t vsab = vwredsum_vs_i32m1_i32m1(vab, vl);\n    int64_t result = vle64_v_i64m1(&result, vl);\n    return ((int64_t)vle64_v_i64m1(&result, vl)) - ((int64_t)vle64_v_i64m1(&result, vl));\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    vint8m1_t va = vle8_v_i8m1(A, vlmax_e8m1(length));\n    vint8m1_t vb = vle8_v_i8m1(B, vlmax_e8m1(length));\n\n    vint32m1_t sum_squares_a = vwredsum_vs_i8m1_i32m1(vwmul_vv_i8m1(va, va, vlmax_e8m1(length)), 0, vlmax_e8m1(length));\n    vint32m1_t sum_squares_b = vwredsum_vs_i8m1_i32m1(vwmul_vv_i8m1(vb, vb, vlmax_e8m1(length)), 0, vlmax_e8m1(length));\n\n    vint32m1_t sum_products = vwredsum_vs_i8m1_i32m1(vwmul_vv_i8m1(va, vb, vlmax_e8m1(length)), 0, vlmax_e8m1(length));\n\n    int64_t result = (int64_t)vle32_v_i32m1(&sum_squares_a, 1)[0] - (int64_t)vle32_v_i32m1(&sum_squares_b, 1)[0] - (int64_t)vle32_v_i32m1(&sum_products, 1)[0];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint16m1_t vsum_squares = vwredsum_vs_i8m1_i16m1(vwredsum_vs_i8m1_i16m1(vmul_vv_i16m1(va, va, vl), vl), vl);\n    vint16m1_t vsum_products = vwredsum_vs_i8m1_i16m1(vmul_vv_i16m1(va, vb, vl), vl);\n    int32_t sum_squares = vmv_x_s_i16m1_i32(vsum_squares);\n    int32_t sum_products = vmv_x_s_i16m1_i32(vsum_products);\n    return (int64_t)sum_squares - (int64_t)sum_products;\n}\n```"}
{"task_id": "SimdBench_41_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    vint8m1_t va = vle8_v_i8m1(A, length);\n    vint8m1_t vb = vle8_v_i8m1(B, length);\n\n    vint16m1_t va_sq = vmul_vv_i16m1(va, va);\n    vint16m1_t vb_sq = vmul_vv_i16m1(vb, vb);\n    vint32m1_t va_sq_sum = vwredsum_vs_i16m1_i32m1_m(0, va_sq, vlmax_e16m1(length));\n    vint32m1_t vb_sq_sum = vwredsum_vs_i16m1_i32m1_m(0, vb_sq, vlmax_e16m1(length));\n\n    vint16m1_t va_b = vmul_vv_i16m1(va, vb);\n    vint32m1_t va_b_sum = vwredsum_vs_i16m1_i32m1_m(0, va_b, vlmax_e16m1(length));\n\n    int64_t result = (int64_t)(va_sq_sum - vb_sq_sum + 2 * va_b_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint64m1_t va = vle64_v_u64m1(A, vl);\n    vuint64m1_t vb = vle64_v_u64m1(B, vl);\n    vuint64m1_t vn = vmseq_vx_u64m1(vb, 0, vl); // Bitwise NOT of B\n    vuint64m1_t vc = vand_vv_u64m1(va, vn, vl); // Element-wise AND\n    vse64_v_u64m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint64m1_t va = vle64_v_u64m1(A, vl);\n    vuint64m1_t vb = vle64_v_u64m1(B, vl);\n    vuint64m1_t vc = vnot_v_u64m1(vb, vl);\n    vsst64_v_u64m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint64m1_t va = vle64_v_u64m1(A, vl);\n    vuint64m1_t vb = vle64_v_u64m1(B, vl);\n    vuint64m1_t vc = vnot_v_u64m1(vb, vl);\n    vsst64_v_u64m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint64m1_t va = vle64_v_u64m1(A, vl);\n    vuint64m1_t vb = vle64_v_u64m1(B, vl);\n    vuint64m1_t vc = vnand_vv_u64m1(va, vb, vl);\n    vse64_v_u64m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_42_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint64m1_t va = vle64_v_u64m1(A, vl);\n    vuint64m1_t vb = vle64_v_u64m1(B, vl);\n    vuint64m1_t vn = vmseq_vx_u64m1(vb, 0, vl);\n    vuint64m1_t vc = vand_vv_u64m1(va, vn, vl);\n    vse64_v_u64m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(length);\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint8m1_t vc = vmul_vv_i8m1(va, vb, vl);\n    vboolm1_t mask_pos = vmsltu_vx_i8m1_m(vand_vv_i8m1(vc, va, vl), 0, vl);\n    vboolm1_t mask_neg = vmsltu_vx_i8m1_m(vand_vv_i8m1(vc, vb, vl), 0, vl);\n    vint8m1_t result = vsext_vf_fi_i8m1(vmseq_vv_i16m1(mask_pos, mask_neg, vl), vl);\n    vint8m1_t zero = vmv_v_x_i8m1(0, vl);\n    vint8m1_t one = vmv_v_x_i8m1(1, vl);\n    vint8m1_t minus_one = vmv_v_x_i8m1(-1, vl);\n    vint8m1_t final_result = vselect_vvm_i8m1(result, one, zero, vl);\n    final_result = vselect_vvm_i8m1(final_result, minus_one, final_result, vl);\n    vse8_v_i8m1(C, final_result, vl);\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(length);\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint8m1_t vc = vmul_vv_i8m1(va, vb, vl);\n\n    vbool8_t mask_pos = vand_vv_b8(vgtz_v_i8m1(va, vl), vgtz_v_i8m1(vb, vl));\n    vbool8_t mask_neg = vand_vv_b8(vltz_v_i8m1(va, vl), vltz_v_i8m1(vb, vl));\n\n    vc = vselect_vvm_i8m1(mask_pos, vsext_vf8_i8m1(1, vl), vc, vl);\n    vc = vselect_vvm_i8m1(mask_neg, vsext_vf8_i8m1(-1, vl), vc, vl);\n\n    vse8_v_i8m1(C, vc, vl);\n\n    if (length % vl != 0) {\n        size_t remainder = length % vl;\n        for (size_t i = 0; i < remainder; ++i) {\n            if (A[length - remainder + i] > 0 && B[length - remainder + i] > 0) {\n                C[length - remainder + i] = 1;\n            } else if (A[length - remainder + i] < 0 && B[length - remainder + i] < 0) {\n                C[length - remainder + i] = -1;\n            } else {\n                C[length - remainder + i] = 0;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(length);\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint8m1_t vc = vmul_vv_i8m1(va, vb, vl);\n    vboolm1_t mask_pos = vgts_vx_i8m1_mask(vc, 0, vl);\n    vboolm1_t mask_neg = vltu_vx_i8m1_mask(vc, 0, vl);\n    vint8m1_t result = vsext_vf_fi_m(vmv_v_x_i8m1(0, vl), mask_pos, vl);\n    result = vmerge_vvm_i8m1(result, vsext_vf_fi_m(vmv_v_x_i8m1(-1, vl), mask_neg, vl), vl);\n    vse8_v_i8m1(C, result, vl);\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(length);\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint8m1_t vc = vmul_vv_i8m1(va, vb, vl);\n\n    vbool8_t mask_pos = vand_vv_b8(vgtu_vx_i8m1(va, 0, vl), vgtu_vx_i8m1(vb, 0, vl));\n    vbool8_t mask_neg = vand_vv_b8(vltu_vx_i8m1(va, 0, vl), vltu_vx_i8m1(vb, 0, vl));\n\n    vc = vselect(mask_pos, vsext_vf8_i8m1(1, vl), vc, vl);\n    vc = vselect(mask_neg, vsext_vf8_i8m1(-1, vl), vc, vl);\n\n    vse8_v_i8m1(C, vc, vl);\n\n    if (length % vl != 0) {\n        tensor_three_valued_logic_simd(A + vl * (length / vl), B + vl * (length / vl), C + vl * (length / vl), length % vl);\n    }\n}\n```"}
{"task_id": "SimdBench_43_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t vl = vsetvl_e8m1(length);\n    vint8m1_t va = vle8_v_i8m1(A, vl);\n    vint8m1_t vb = vle8_v_i8m1(B, vl);\n    vint8m1_t vc = vmul_vv_i8m1(va, vb, vl);\n    vboolm1_t mask_pos = vgts_vx_i8m1_m(vand_vv_i8m1(va, vb, vl), 0, vl);\n    vboolm1_t mask_neg = vltu_vx_i8m1_m(vand_vv_i8m1(va, vb, vl), 0, vl);\n    vc = vselect_b_mask(vc, vsext_vf8m1_i8m1(1, vl), mask_pos);\n    vc = vselect_b_mask(vc, vsext_vf8m1_i8m1(-1, vl), mask_neg);\n    vse8_v_i8m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t va = vle16_v_u16m8(A, vl);\n    vuint16m8_t vb = vle16_v_u16m8(B, vl);\n    vuint16m8_t vc = vle16_v_u16m8(C, vl);\n\n    size_t mid = length / 2;\n    if (mid > 0) {\n        vuint16m8_t mask = vmseq_vx_u16m8(vl, vsew_v_i16m8(), mid);\n        va = vor_vv_u16m8(va, vb, vl);\n        vb = vand_vv_u16m8(va, vb, vl);\n        va = vselect_vvm_u16m8(mask, va, vb, vl);\n    }\n\n    vse16_v_u16m8(C, va, vl);\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t va = vle16_v_u16m8(A, vl);\n    vuint16m8_t vb = vle16_v_u16m8(B, vl);\n    vuint16m8_t vc = vle16_v_u16m8(C, vl);\n\n    size_t mid = length / 2;\n\n    if (mid >= vl) {\n        mid = vl;\n    }\n\n    vbool8_t mask = vmseq_mask_b8(vl, vsew_e16(), mid);\n\n    vc = vor_vv_u16m8(va, vb, mask);\n    vc = vand_vv_u16m8(va, vb, vmnot(mask));\n\n    vse16_v_u16m8(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t va = vle16_v_u16m8(A, vl);\n    vuint16m8_t vb = vle16_v_u16m8(B, vl);\n    vuint16m8_t vc = vle16_v_u16m8(C, vl);\n\n    size_t mid = length / 2;\n    if (mid > 0) {\n        vuint16m8_t mask = vmseq_vx_u16m8(vl, vsew_v_i16m8(), mid);\n        va = vor_vv_u16m8(va, vb, vl);\n        vb = vand_vv_u16m8(va, vb, vl);\n        va = vselect_vvm_u16m8(mask, va, vb, vl);\n    }\n\n    vse16_v_u16m8(C, va, vl);\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t va = vle16_v_u16m8(A, vl);\n    vuint16m8_t vb = vle16_v_u16m8(B, vl);\n    vuint16m8_t vc = vle16_v_u16m8(C, vl);\n\n    vbool8_t mask = vmseq_vx_i8(vl, vlmul_vx_i8(vl, vl / 2), 0);\n\n    vc = vbitwise_and_m_b8(mask, va, vb, vl);\n    vc = vbitwise_or_m_b8(vmnot_m_b8(mask), vc, va, vl);\n\n    vse16_v_u16m8(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_44_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t vl = vsetvl_e16m8(length);\n    vuint16m8_t va = vle16_v_u16m8(A, vl);\n    vuint16m8_t vb = vle16_v_u16m8(B, vl);\n    vuint16m8_t vc = vle16_v_u16m8(C, vl);\n\n    size_t mid = length / 2;\n\n    if (mid >= vl) {\n        vmv_v_x_u16m8(vc, 0, vl);\n    } else {\n        vuint16m8_t mask = vmseq_vx_u16m8(vlmask_b16(vl), mid - 1);\n        vuint16m8_t xor_result = vxor_vv_u16m8(va, vb, vl);\n        vuint16m8_t and_result = vand_vv_u16m8(va, vb, vl);\n        vuint16m8_t select_result = vselect_vvm_u16m8(mask, xor_result, and_result, vl);\n        vmv_v_x_u16m8(vc, 0, vl);\n        vse16_v_u16m8(C + mid, select_result, vl);\n    }\n\n    vse16_v_u16m8(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    vfloat32m8_t va, vb, vc, vd, vout, vcond1, vcond2;\n    size_t i = 0;\n\n    while (i < total_elements) {\n        size_t vl = vsetvl_e32m8(total_elements - i);\n\n        va = vle32_v_f32m8(A + i, vl);\n        vb = vle32_v_f32m8(B + i, vl);\n        vc = vle32_v_f32m8(C + i, vl);\n        vd = vle32_v_f32m8(D + i, vl);\n        vcond1 = vle8_v_u8m8((const uint8_t*)(cond1 + i), vl);\n        vcond2 = vle8_v_u8m8((const uint8_t*)(cond2 + i), vl);\n\n        vcond1 = vfclass_vx_u8m8(vcond1, 0x01, vl); // Extract the least significant bit\n        vcond2 = vfclass_vx_u8m8(vcond2, 0x01, vl); // Extract the least significant bit\n\n        vcond1 = vmul_vx_u8m8(vcond1, 0x01, vl); // Convert to boolean\n        vcond2 = vmul_vx_u8m8(vcond2, 0x01, vl); // Convert to boolean\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); // !(cond1 && cond2)\n\n        vcond1 = vor_vv_u8m8(vcond1, vcond2, vl); // cond1 || cond2\n\n        vcond1 = vand_vv_u8m8(vcond1, vcond2, vl); // cond1 && cond2\n        vcond2 = vsub_vx_u8m8(0xFF, vcond1, vl); //"}
{"task_id": "SimdBench_45_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vector_size = vsetvl_e32m8(1); // Set vector length for 32-bit floats\n\n    for (size_t i = 0; i < total_elements; i += vector_size) {\n        vfloat32m8_t va = vle32_v_f32m8(A + i, vector_size);\n        vfloat32m8_t vb = vle32_v_f32m8(B + i, vector_size);\n        vfloat32m8_t vc = vle32_v_f32m8(C + i, vector_size);\n        vfloat32m8_t vd = vle32_v_f32m8(D + i, vector_size);\n\n        vbool8_t vbcond1 = vle8_v_b8(cond1 + i, vector_size);\n        vbool8_t vbcond2 = vle8_v_b8(cond2 + i, vector_size);\n\n        vfloat32m8_t vout = vselect_vvm_f32m8(va, vb, vbcond1, vbcond2);\n        vfloat32m8_t vout_not_cond1 = vselect_vvm_f32m8(vb, vc, vbcond1, vbcond2);\n        vfloat32m8_t vout_not_cond2 = vselect_vvm_f32m8(vc, vd, vbcond1, vbcond2);\n\n        vfloat32m8_t vresult = vselect_vvm_f32m8(vout, vout_not_cond1, vbcond1, vbcond2);\n        vfloat32m8_t vfinal_result = vselect_vvm_f32m8(vresult, vout_not_cond2, vbcond1, vbcond2);\n\n        vse32_v_f32m8(out + i, vfinal_result, vector_size);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    vfloat32m8_t va, vb, vc, vd, vout;\n    vbool8_t vcond1, vcond2;\n\n    for (size_t i = 0; i < total_elements; i += vl) {\n        size_t vl = vsetvl_e32m8(total_elements - i);\n\n        va = vle32_v_f32m8(A + i, vl);\n        vb = vle32_v_f32m8(B + i, vl);\n        vc = vle32_v_f32m8(C + i, vl);\n        vd = vle32_v_f32m8(D + i, vl);\n        vcond1 = vle8_v_b8(cond1 + i, vl);\n        vcond2 = vle8_v_b8(cond2 + i, vl);\n\n        vout = vselect_b8(va, vb, vand_vv_b8(vcond1, vcond2), vl);\n        vout = vselect_b8(vout, vc, vand_vv_b8(vcond1, vnot_vb8(vcond2)), vl);\n        vout = vselect_b8(vout, vd, vand_vv_b8(vnot_vb8(vcond1), vcond2), vl);\n\n        vse32_v_f32m8(out + i, vout, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e32m8(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vfloat32m8_t va = __riscv_vle32_v_f32m8(A + i, vlen);\n        vfloat32m8_t vb = __riscv_vle32_v_f32m8(B + i, vlen);\n        vfloat32m8_t vc = __riscv_vle32_v_f32m8(C + i, vlen);\n        vfloat32m8_t vd = __riscv_vle32_v_f32m8(D + i, vlen);\n        vbool8m8_t vcond1 = __riscv_vle8_v_b8m8((const uint8_t*)(cond1 + i), vlen);\n        vbool8m8_t vcond2 = __riscv_vle8_v_b8m8((const uint8_t*)(cond2 + i), vlen);\n\n        vfloat32m8_t vout = __riscv_vsle8_mask_v_f32m8(va, vb, vcond1, vcond2);\n        vout = __riscv_vsle8_mask_v_f32m8(vout, vc, vcond1, __riscv_vmseq_v_b8m8(vcond2, vfalse_b8m8()));\n        vout = __riscv_vsle8_mask_v_f32m8(vout, vd, __riscv_vmseq_v_b8m8(vcond1, vfalse_b8m8()), vcond2);\n\n        __riscv_vse32_v_f32m8(out + i, vout, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_45_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vector_size = vsetvl_e32m8(1); // Set vector length to 8 elements\n\n    for (size_t i = 0; i < total_elements; i += vector_size) {\n        vfloat32m8_t va = vle32_v_f32m8(A + i, vector_size);\n        vfloat32m8_t vb = vle32_v_f32m8(B + i, vector_size);\n        vfloat32m8_t vc = vle32_v_f32m8(C + i, vector_size);\n        vfloat32m8_t vd = vle32_v_f32m8(D + i, vector_size);\n\n        vbool8_t vbcond1 = vle8_v_b8(cond1 + i, vector_size);\n        vbool8_t vbcond2 = vle8_v_b8(cond2 + i, vector_size);\n\n        vfloat32m8_t vout = vselect_vvm_f32m8(va, vb, vbcond1, vbcond2);\n        vfloat32m8_t vout_not_cond1 = vselect_vvm_f32m8(vout, vc, vbcond1, vbcond2 ^ vbcond1);\n        vfloat32m8_t vout_final = vselect_vvm_f32m8(vout_not_cond1, vd, vbcond1 ^ vbcond2, vbcond2);\n\n        vse32_v_f32m8(out + i, vout_final, vector_size);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e8m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vuint32m1_t va = __riscv_vle32_v_u32m1(A + i, vlen);\n        vuint8m1_t vb = __riscv_vcnt_u8m1(va, vlen);\n        __riscv_vse8_v_u8m1(out + i, vb, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e8m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vuint32m1_t va = __riscv_vle32_v_u32m1(A + i, vlen);\n        vuint8m1_t vb = __riscv_vcnt_u8m1(va, vlen);\n        __riscv_vse8_v_u8m1(out + i, vb, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    vuint32m1_t va;\n    vuint8m1_t vb;\n    size_t i;\n\n    for (i = 0; i < total_elements; i += vl) {\n        size_t vl = vsetvl_e32m1(total_elements - i);\n        va = vle32_v_u32m1(A + i, vl);\n        vb = vmv_v_x_u8m1(0, vl);\n        vb = vor_vv_u8m1(vb, vctpopcnt_v_u8m1(va, vl), vl);\n        vse8_v_u8m1(out + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e8m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vuint32m1_t va = __riscv_vle32_v_u32m1(A + i, vlen);\n        vuint8m1_t vb = __riscv_vcnt_u8m1(va, vlen);\n        __riscv_vse8_v_u8m1(out + i, vb, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_46_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vlen = __riscv_vsetvl_e8m1(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vuint32m1_t va = __riscv_vle32_v_u32m1(A + i, vlen);\n        vuint8m1_t vb = __riscv_vcnt_u8m1(va, vlen);\n        __riscv_vse8_v_u8m1(out + i, vb, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t vlen = __riscv_vsetvl_e32m8(1);\n    size_t vlmax = __riscv_vsetvlmax_e32m8();\n\n    for (size_t i = 0; i < total_elements; i += vlmax) {\n        vint32m8_t va = __riscv_vle32_v_i32m8(A + i, vlmax);\n        vbool8_t vm = __riscv_vmseq_vx_i32m8_mask(vlmax, va, 3);\n        vint32m8_t vb = __riscv_vabs_v_i32m8(va);\n        vb = __riscv_vmask_vvm_i32m8(vm, vb, va);\n        __riscv_vse32_v_i32m8(B + i, vb, vlmax);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t vlen = __riscv_vsetvl_e32m8(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint32m8_t va = __riscv_vle32_v_i32m8(A + i, vlen);\n        vbool8_t vb = __riscv_vmseq_vx_i32m8_b8(va, 3);\n        vint32m8_t vc = __riscv_vabs_v_i32m8(va);\n        vint32m8_t vd = __riscv_vcond_mask_vvm_i32m8(vb, vc, va, vlen);\n        __riscv_vse32_v_i32m8(B + i, vd, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t vlen = __riscv_vsetvl_e32m8(1); // Set vector length for 32-bit elements\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint32m8_t va = __riscv_vle32_v_i32m8(A + i, vlen); // Load vector from A\n        vbool8_t vm = __riscv_vmseq_vx_i32m8_b8(va, 3);      // Check if elements are divisible by 3\n        vint32m8_t vb = __riscv_vabs_v_i32m8(va);          // Compute absolute values\n        vint32m8_t vc = __riscv_vselect_vvm_i32m8(vb, va, vm); // Select between abs and original values\n        __riscv_vse32_v_i32m8(B + i, vc, vlen);            // Store result back to B\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t vlen = __riscv_vsetvl_e32m8(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint32m8_t va = __riscv_vle32_v_i32m8(A + i, vlen);\n        vbool8_t vb = __riscv_vmseq_vx_i32m8_b8(va, 3);\n        vint32m8_t vc = __riscv_vabs_v_i32m8(va);\n        vint32m8_t vd = __riscv_vblend_vvm_i32m8(vb, vc, va);\n        __riscv_vse32_v_i32m8(B + i, vd, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_47_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    size_t vlen = __riscv_vsetvl_e32m8(total_elements);\n\n    for (size_t i = 0; i < total_elements; i += vlen) {\n        vint32m8_t va = __riscv_vle32_v_i32m8(A + i, vlen);\n        vbool8_t vb = __riscv_vmseq_vx_i32m8_b8(va, 3);\n        vint32m8_t vc = __riscv_vabs_v_i32m8(va);\n        vint32m8_t vd = __riscv_vblend_vvm_i32m8(vb, vc, va);\n        __riscv_vse32_v_i32m8(B + i, vd, vlen);\n    }\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    vuint32m1_t v_src = vle32_v_u32m1(src, length);\n    vbool8_t v_mask = vmseq_vx_u8_b8(v_src, 0xF); // Check if low 4 bits are all 1\n    vuint32m1_t v_rotated = vrrot_r_v_i32m1_u8(v_src, rotate_bits, v_mask); // Rotate if condition is true\n    vuint32m1_t v_negated = vnot_v_u32m1(v_src); // Negate otherwise\n    vuint32m1_t v_result = vselect_vvm_u32m1(v_mask, v_rotated, v_negated); // Select based on mask\n    vse32_v_u32m1(dst, v_result, length); // Store result\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vbool8_t v_mask = vmseq_vx_u8m1_b8(v_src, 0xF, vl);\n    vuint32m1_t v_rotated = vrrot_r_v_u32m1_u8m1(v_src, rotate_bits, v_mask, vl);\n    vuint32m1_t v_negated = vnot_v_u32m1(v_src, vl);\n    vsel_vvm_u32m1(dst, v_rotated, v_negated, v_mask, vl);\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    vuint32m1_t v_src = vle32_v_u32m1(src, length);\n    vbool8_t v_mask = vmseq_vx_u8_b8(v_src, 0xF);\n    vuint32m1_t v_rotated = vrotr_vx_u32m1(v_src, rotate_bits);\n    vuint32m1_t v_negated = ~v_src;\n    vuint32m1_t v_dst = vselect_vvm_u32m1(v_mask, v_rotated, v_negated);\n    vsse32_v_u32m1(dst, length, v_dst);\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vbool16_t v_mask = vmseq_vx_u32m1_b16(v_src, 0xF, vl);\n    vuint32m1_t v_rotated = vrrot_r_v_i32m1_u32m1(v_src, rotate_bits, vl);\n    vuint32m1_t v_negated = vnot_v_u32m1(v_src, vl);\n    vuint32m1_t v_dst = vselect_vvm_u32m1(v_mask, v_rotated, v_negated, vl);\n    vsse32_v_u32m1(dst, vl, v_dst);\n}\n```"}
{"task_id": "SimdBench_48_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vbool8_t v_mask = vmseq_vx_u8m1_b8(v_src, 0xF, vl);\n    vuint32m1_t v_rotated = vrrot_r_v_u32m1_u32m1(v_src, rotate_bits, vl);\n    vuint32m1_t v_negated = vnot_v_u32m1(v_src, vl);\n    vuint32m1_t v_dst = vselect_vvm_u32m1(v_mask, v_rotated, v_negated, vl);\n    vse32_v_u32m1(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e8m4(length);\n    vfloat32m4_t va = vle32_v_f32m4(A, vl);\n    vfloat32m4_t vb = vle32_v_f32m4(B, vl);\n    vfloat32m4_t veps = vmul_vf_f32m4(vfsgnjx_f32m4(va, vb), epsilon, vl);\n    vint8m4_t vcmp_gt = vfcmpgt_m_b8m4(vl, va, vsub_vv_f32m4(va, vb, vl));\n    vint8m4_t vcmp_lt = vflt_m_b8m4(vl, vsub_vv_f32m4(va, vb, vl), veps);\n    vint8m4_t vresult = vand_vv_i8m4(vcmp_gt, vneg_vx_i8m4(vcmp_lt), vl);\n    vse8_v_i8m4(out, vresult, vl);\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e8m4(length);\n    vfloat32m4_t va = vle32_v_f32m4(A, vl);\n    vfloat32m4_t vb = vle32_v_f32m4(B, vl);\n    vfloat32m4_t veps = vmul_vf_f32m4(vfsgnjx_f32m4(va, vb), epsilon, vl);\n    vint8m4_t vcmp_gt = vfsgnjx_f32m4(va, vfsub_vv_f32m4(va, vb, vl)), vcmp_lt = vfsgnjx_f32m4(vb, vfsub_vv_f32m4(va, vb, vl));\n    vint8m4_t vresult = vand_vv_i8m4(vcmp_gt, vcmp_lt, vl);\n    vresult = vor_vv_i8m4(vresult, vcmpltu_vv_u8m4(vabs_vf_f32m4(vfsub_vv_f32m4(va, vb, vl)), veps, vl), vl);\n    vresult = vreinterpret_v_i8m4_b8m1(vresult);\n    vse8_v_i8m4(out, vresult, vl);\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e8m4(length);\n    vfloat32m4_t va = vle32_v_f32m4(A, vl);\n    vfloat32m4_t vb = vle32_v_f32m4(B, vl);\n    vfloat32m4_t veps = vmul_vf_f32m4(vfmv_s_s_f32m1_f32(epsilon), vfmv_s_s_f32m1_f32(-1.0f), vl);\n    vint8m4_t vout = vsub_vv_i8m4(vcmpgt_vv_f32m4(va, vb, vl), vcmpgt_vv_f32m4(vb, va, vl), vl);\n    vout = vand_vx_i8m4(vout, vle32_v_f32m4(&veps, vl), vl);\n    vsse_i8m4(out, vl, vout);\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e8m4(length);\n    vfloat32m4_t va = vle32_v_f32m4(A, vl);\n    vfloat32m4_t vb = vle32_v_f32m4(B, vl);\n    vfloat32m4_t veps = vmul_vf_f32m4(vfmv_s_s_f32m1_f32(epsilon), vfmv_s_s_f32m1_f32(-1.0f), vl);\n    vint8m4_t vres = vsub_vv_i8m4(vcmpgt_vv_f32m4(va, vadd_vv_f32m4(vb, veps, vl), vl),\n                                 vcmpgt_vv_f32m4(vsub_vv_f32m4(va, vb, vl), veps, vl));\n    vse8_v_i8m4(out, vres, vl);\n}\n```"}
{"task_id": "SimdBench_49_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t vl = vsetvl_e8m4(length);\n    vfloat32m4_t va = vle32_v_f32m4(A, vl);\n    vfloat32m4_t vb = vle32_v_f32m4(B, vl);\n    vfloat32m4_t veps = vmul_vf_f32m4(vfmv_s_s_f32m1_f32(epsilon), vfmv_s_s_f32m1_f32(-1.0f), vl);\n    vint8m4_t vout = vsub_vv_i8m4(vcmpgt_vv_f32m4(va, vb, vl), vcmpgt_vv_f32m4(vb, va, vl), vl);\n    vse8_v_i8m4(out, vout, vl);\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vint32m1_t vb = vfclassif_v_f32m1_mask(va, vmul_vx_u8m1(vfcmpgt_vf_f32m1(va, vfmacc_vf_f32m1(va, vffloat32m1(0), vffloat32m1(INT32_MAX)), vl), vl));\n    vb = vfsub_vx_i32m1(vb, vffloat32m1(1), vl);\n    vse32_v_i32m1(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vint32m1_t vb = vfclassif_v_f32m1_mask(vfcmpgt_vf_f32m1(va, vfmacc_vf_f32m1(va, vffloat32m1(0), vffloat32m1(INT32_MAX)), vl));\n    vb = vfsub_vx_i32m1(vb, vffloat32m1(INT32_MIN), vl);\n    vse32_v_i32m1(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vint32m1_t vb = vfcsrr_ew(va, vfmacc_vf_f32m1(vfsub_vf_f32m1(va, vfmul_vf_f32m1(va, vffs_f32m1(va)), vffs_f32m1(va)), vffs_f32m1(va), vl));\n    vse32_v_i32m1(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_50_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vint32m1_t vb = vfcsrr_ew(va, vfmacc_vf_f32m1(vfsub_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmax_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va, vfredmin_vs_f32m1(va, vfmul_vf_f32m1(va, vffsra_vf_f32m1(va,"}
{"task_id": "SimdBench_50_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vint32m1_t vb = vfclassif_v_f32m1_mask(va, vmul_vx_u8m1(vfsgnjx_v_f32m1(va, va), 0x7F, vl));\n    vb = vmin_vv_i32m1(vb, vle32_v_i32m1((const int32_t*)&INT32_MAX, vl), vl);\n    vb = vmax_vv_i32m1(vb, vle32_v_i32m1((const int32_t*)&INT32_MIN, vl), vl);\n    vse32_v_i32m1(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t vb;\n    vint32m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_i32m1(A + i, vl);\n        vb = vf32_v_f32m1(va, vl);\n        vb = vmul_vf_f32m1(vb, scale, vl);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t vb;\n    vint32m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_i32m1(A + i, vl);\n        vb = vf32m1_v_f32m1(va, vl);\n        vb = vmul_vf_f32m1(vb, scale, vl);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t vb;\n    vint32m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_i32m1(A + i, vl);\n        vb = vf32m1_v_f32m1(va, vl);\n        vb = vmul_vf_f32m1(vb, scale, vl);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t vb;\n    vint32m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_i32m1(A + i, vl);\n        vb = vf32m1_v_f32m1(va, vl);\n        vb = vmul_vf_f32m1(vb, scale, vl);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_51_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t vb;\n    vint32m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle32_v_i32m1(A + i, vl);\n        vb = vf32m1_v_f32m1(va, vl);\n        vb = vmul_vf_f32m1(vb, scale, vl);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vfloat32m1_t vb = vle32_v_f32m1(B, vl);\n    vfloat32m1_t vr = vfdiv_vv_f32m1(va, vb, vl);\n    vint32m1_t vc = vfwredsum_vs_f32_i32m1(C, vr, vfmul_vf_f32m1(vr, 0.5f, vl), vl);\n    vse32_v_i32m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vfloat32m1_t vb = vle32_v_f32m1(B, vl);\n    vfloat32m1_t vr = vfdiv_vv_f32m1(va, vb, vl);\n    vint32m1_t vc = vfcvt_x_f_v_i32m1(vr, vl);\n    vse32_v_i32m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vfloat32m1_t vb = vle32_v_f32m1(B, vl);\n    vfloat32m1_t vr = vfdiv_vv_f32m1(va, vb, vl);\n    vint32m1_t vc = vfcvt_x_f_w(vround_rtz_f32m1(vr, vl), vl);\n    vse32_v_i32m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vfloat32m1_t vb = vle32_v_f32m1(B, vl);\n    vfloat32m1_t vr = vfdiv_vv_f32m1(va, vb, vl);\n    vint32m1_t vc = vfcvt_x_f_w(vround_rtz_f32m1(vr, vl), vl);\n    vse32_v_i32m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_52_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vfloat32m1_t va = vle32_v_f32m1(A, vl);\n    vfloat32m1_t vb = vle32_v_f32m1(B, vl);\n    vfloat32m1_t vr = vfdiv_vv_f32m1(va, vb, vl);\n    vint32m1_t vc = vfcvt_x_f_w(vround_rtz_f32m1(vr, vl), vl);\n    vse32_v_i32m1(C, vc, vl);\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```c\n#include <riscv_vector.h>\n#include <math.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size * sizeof(double));\n    vfloat32m1_t vb;\n    vfloat64m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle64_v_f64m1(A + i, vl);\n        vb = vfmax_vf_f32m1(vfmin_vf_f32m1(va, f32max), f32min);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```c\n#include <riscv_vector.h>\n#include <math.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size * sizeof(double));\n    vfloat32m1_t vb;\n    vfloat64m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle64_v_f64m1(A + i, vl);\n        vb = vfmax_vf_f32m1(va, -FLT_MAX, vl);\n        vb = vfmin_vf_f32m1(vb, FLT_MAX, vl);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```c\n#include <riscv_vector.h>\n#include <limits.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size * sizeof(double));\n    vfloat32m1_t vb;\n    vfloat64m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle64_v_f64m1(A + i, vl);\n        vb = vfmax_vf_f32m1(va, f32m1_set(FLT_MAX), vl);\n        vb = vfmin_vf_f32m1(vb, f32m1_set(-FLT_MAX), vl);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```c\n#include <riscv_vector.h>\n#include <math.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size * sizeof(double));\n    vfloat32m1_t vb;\n    vfloat64m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle64_v_f64m1(A + i, vl);\n        vb = vfmax_vf_f32m1(vfmin_vf_f32m1(va, f32max), f32min);\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_53_RVV", "completion": "```c\n#include <riscv_vector.h>\n#include <limits.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size * sizeof(double));\n    vfloat32m1_t vb;\n    vfloat64m1_t va;\n\n    for (size_t i = 0; i < size; i += vl) {\n        va = vle64_v_f64m1(A + i, vl);\n        vb = vfmax_vf_f32m1(vfmin_vf_f32m1(va, f32m1_set(FLT_MAX)), f32m1_set(-FLT_MAX));\n        vse32_v_f32m1(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size * sizeof(int16_t));\n    vint16m8_t va = vle16_v_i16m8(A, vl);\n    vint32m8_t vb = vsext_vf32_i32m8(va, vl);\n    vse32_v_i32m8(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size * sizeof(int16_t));\n    vint16m8_t va = vle16_v_i16m8(A, vl);\n    vint32m8_t vb = vsext_vf32_i32m8(va, vl);\n    vse32_v_i32m8(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size * sizeof(int16_t));\n    vint16m8_t va = vle16_v_i16m8(A, vl);\n    vint32m8_t vb = vsext_vf_f_i32m8(va, 0, vl);\n    vse32_v_i32m8(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size * sizeof(int16_t));\n    vint16m8_t va = vle16_v_i16m8(A, vl);\n    vint32m8_t vb = vsext_vf32_i32m8(va, vl);\n    vse32_v_i32m8(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_54_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size * sizeof(int16_t));\n    vint16m8_t va = vle16_v_i16m8(A, vl);\n    vint32m8_t vb = vsext_vf32_i32m8(va, vl);\n    vse32_v_i32m8(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t va = vle32_v_f32m8(A, vl);\n    vint32m8_t vcontrol = vle32_v_i32m8(control, vl);\n    vfloat32m8_t vmin = vfmv_s_f_f32m8(min_val, vfmv_v_f_f32m8(0.0f));\n    vfloat32m8_t vmax = vfmv_s_f_f32m8(max_val, vfmv_v_f_f32m8(1.0f));\n\n    for (size_t i = 0; i < size; i += vl) {\n        vbool8_t vmask = vgts_vx_i32m8_b8(vcontrol, 0, vl);\n        vfloat32m8_t vdiff = vfsub_vv_f32m8(va, vmin, vl);\n        vfloat32m8_t vrange = vfsub_vv_f32m8(vmax, vmin, vl);\n        vfloat32m8_t vnrm = vfdiv_vv_f32m8(vdiff, vrange, vl);\n        vnrm = vmin_vv_f32m8(vnrm, vmax, vl);\n        vnrm = vmax_vv_f32m8(vnrm, vmin, vl);\n\n        vfloat32m8_t vresult = vselect_m_vv_f32m8(vmask, vnrm, va, vl);\n        vse32_v_f32m8(B + i, vresult, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t va = vle32_v_f32m8(A, vl);\n    vint32m8_t vcontrol = vle32_v_i32m8(control, vl);\n    vfloat32m8_t vmin = vfmv_s_f_f32m8(min_val, vfmv_v_f_f32m8(0.0f, vl));\n    vfloat32m8_t vmax = vfmv_s_f_f32m8(max_val, vfmv_v_f_f32m8(0.0f, vl));\n    vfloat32m8_t vzero = vfmv_v_f_f32m8(0.0f, vl);\n    vfloat32m8_t vone = vfmv_v_f_f32m8(1.0f, vl);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vbool8_t vmask = vgts_vx_i32m8_b8(vcontrol, 0, vl);\n        vfloat32m8_t vdiff = vfsub_vv_f32m8(va, vmin, vl);\n        vfloat32m8_t vrange = vfsub_vv_f32m8(vmax, vmin, vl);\n        vfloat32m8_t vnorm = vfdiv_vv_f32m8(vdiff, vrange, vl);\n        vfloat32m8_t vclamp = vmin_fmv(vnorm, vone, vl);\n        vfloat32m8_t vresult = vmin_fmv(vclamp, vzero, vl);\n        vse32_v_f32m8(B + i, vresult, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t va = vle32_v_f32m8(A, vl);\n    vint32m8_t vcontrol = vle32_v_i32m8(control, vl);\n    vfloat32m8_t vmin = vfmv_s_f_f32m8(min_val, vfmv_v_f_f32m8(0.0f, vl));\n    vfloat32m8_t vmax = vfmv_s_f_f32m8(max_val, vfmv_v_f_f32m8(0.0f, vl));\n    vfloat32m8_t vzero = vfmv_v_f_f32m8(0.0f, vl);\n    vfloat32m8_t vone = vfmv_v_f_f32m8(1.0f, vl);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vbool8_t vmask = vgts_vx_i32m8_b8(vcontrol, 0, vl);\n        vfloat32m8_t vdiff = vfsub_vv_f32m8(va, vmin, vl);\n        vfloat32m8_t vrange = vfsub_vv_f32m8(vmax, vmin, vl);\n        vfloat32m8_t vnorm = vfdiv_vv_f32m8(vdiff, vrange, vl);\n        vfloat32m8_t vclamp = vmin_fmv(vnorm, vzero, vone, vl);\n        vfloat32m8_t vb = vselect_m_b8(vmask, vclamp, va, vl);\n        vse32_v_f32m8(B + i, vb, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t va = vle32_v_f32m8(A, vl);\n    vint32m8_t vcontrol = vle32_v_i32m8(control, vl);\n    vfloat32m8_t vmin = vfmv_s_f_f32m8(min_val, vfmv_v_f_f32m8(0.0f, vl));\n    vfloat32m8_t vmax = vfmv_s_f_f32m8(max_val, vfmv_v_f_f32m8(0.0f, vl));\n    vfloat32m8_t vone = vfmv_s_f_f32m8(1.0f, vfmv_v_f_f32m8(0.0f, vl));\n\n    for (size_t i = 0; i < size; i += vl) {\n        vbool8_t vmask = vgts_vx_i32m8_b8(vcontrol, 0, vl);\n        vfloat32m8_t val = vfsub_vv_f32m8(va, vmin, vl);\n        vfloat32m8_t vrange = vfsub_vv_f32m8(vmax, vmin, vl);\n        vfloat32m8_t vnorm = vfdiv_vv_f32m8(val, vrange, vl);\n        vfloat32m8_t vclamp = vfmin_vv_f32m8(vnorm, vone, vl);\n        vfloat32m8_t vclamp2 = vfmax_vv_f32m8(vclamp, vmin, vl);\n\n        vse32_v_f32m8(B + i, vselect_m_b_f32m8(vmask, vclamp2, va, vl), vl);\n    }\n}\n```"}
{"task_id": "SimdBench_55_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t va = vle32_v_f32m8(A, vl);\n    vint32m8_t vcontrol = vle32_v_i32m8(control, vl);\n    vfloat32m8_t vmin = vmv_v_f32m8(min_val);\n    vfloat32m8_t vmax = vmv_v_f32m8(max_val);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vbool8_t mask = vgts_vx_i32m8_b8(vcontrol, 0, vl);\n        vfloat32m8_t val = vfsub_vf_f32m8(va, vmin, vl);\n        vfloat32m8_t denom = vfsub_vf_f32m8(vmax, vmin, vl);\n        vfloat32m8_t norm = vfdiv_vv_f32m8(val, denom, vl);\n        vfloat32m8_t clamp_min = vfmul_vs_f32m8(norm, 0.0f, vl);\n        vfloat32m8_t clamp_max = vfmul_vs_f32m8(norm, 1.0f, vl);\n        vfloat32m8_t result = vfmin_vv_f32m8(clamp_max, clamp_min, vl);\n        vfloat32m8_t select = vselect_vvm_f32m8(mask, result, va, vl);\n        vse32_v_f32m8(B + i, select, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vuint32m8_t va = vle32_v_u32m8(A, vl);\n    vf32m8_t vb = vfcvt_f_f_v_f32m8(va, vl);\n    vf32m8_t vc = vsqrt_v_f_f32m8(vb, vl);\n    vse32_v_f32m8(B, vc, vl);\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vuint32m8_t va = vle32_v_u32m8(A, vl);\n    vf32m8_t vb = vfcvt_f_f_v_f32m8(va, vl);\n    vf32m8_t vc = vsqrt_v_f_f32m8(vb, vl);\n    vse32_v_f32m8(B, vc, vl);\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vuint32m8_t va = vle32_v_u32m8(A, vl);\n    vf32m8_t vb = vfcvt_f_f_v_f32m8(va, vl);\n    vf32m8_t vc = vsqrt_v_f_f32m8(vb, vl);\n    vse32_v_f32m8(B, vc, vl);\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vuint32m8_t va = vle32_v_u32m8(A, vl);\n    vf32m8_t vb = vfcvt_f_f_v_f32m8(va, vl);\n    vf32m8_t vc = vsqrt_v_f_f32m8(vb, vl);\n    vse32_v_f32m8(B, vc, vl);\n}\n```"}
{"task_id": "SimdBench_56_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vuint32m8_t va = vle32_v_u32m8(A, vl);\n    vf32m8_t vb = vfcvt_f_f_v_f32m8(va, vl);\n    vf32m8_t vc = vsqrt_v_f_f32m8(vb, vl);\n    vse32_v_f32m8(B, vc, vl);\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    vuint32m8_t va = vle32_v_u32m8(A, size / sizeof(uint32_t));\n    vfloat32m8_t vb = vfncvt_f_f_v_f32m8(va);\n    vse32_v_f32m8(B, vb, size / sizeof(float));\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    vuint32m8_t va = vle32_v_u32m8(A, size / sizeof(uint32_t));\n    vfloat32m8_t vb = vfncvt_f_f_v_f32m8(va);\n    vse32_v_f32m8(B, vb, size / sizeof(float));\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t vl = vsetvl_e32m1(size);\n    vuint32m1_t va = vle32_v_u32m1(A, vl);\n    vfloat32m1_t vb = vfclass_s_f_f(va, vl);\n    vse32_v_f32m1(B, vb, vl);\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    vuint32m8_t va = vle32_v_u32m8(A, size / sizeof(uint32_t));\n    vfloat32m8_t vb = vfncvt_f_f_v_f32m8(va);\n    vse32_v_f32m8(B, vb, size / sizeof(float));\n}\n```"}
{"task_id": "SimdBench_57_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    vuint32m8_t va = vle32_v_u32m8(A, size / sizeof(uint32_t));\n    vfloat32m8_t vb = vfncvt_f_f_v_f32m8(va);\n    vse32_v_f32m8(B, vb, size / sizeof(float));\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t v_diff = vle32_v_f32m8(diff, vl);\n    vbool8_t v_cond = vle8_v_b8(cond, vl);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint32m8_t v_A = vle32_v_i32m8(A + i, vl);\n        vint32m8_t v_A_prev = vsew_w_v_i32m8(v_A, vl);\n        vint32m8_t v_diff_int = vsub_vv_i32m8(v_A, v_A_prev, vl);\n        vfloat32m8_t v_diff_float = vfsub_vv_f32m8(v_cast_f32m8_i32m8(v_diff_int), v_fmv_v_x_f32m8(0.0f), vl);\n        v_diff = vselect_vvm_f32m8(v_cond, v_diff_float, v_fmv_v_x_f32m8(0.0f), vl);\n        vse32_v_f32m8(diff + i, v_diff, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t v_diff = vle32_v_f32m8(diff, vl);\n    vbool8_t v_cond = vle8_v_b8(cond, vl);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint32m8_t v_A = vle32_v_i32m8(A + i, vl);\n        vint32m8_t v_prev_A = vsew_w_v_i32m8(v_A, vsew_w_i32m8(v_A, vl), vl);\n        vint32m8_t v_diff_int = vsub_vv_i32m8(v_A, v_prev_A, vl);\n        vfloat32m8_t v_diff_float = vfsub_vv_f32m8(v_cast_f32m8_i32m8(v_diff_int), v_fmv_v_f_f32m8(0.0f), vl);\n        v_diff = vselect_vvm_f32m8(v_cond, v_diff_float, v_fmv_v_f_f32m8(0.0f), vl);\n        vse32_v_f32m8(diff + i, v_diff, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t vl = vsetvl_e32m8(size);\n    vfloat32m8_t v_diff = vfmv_v_f_f32m8(0.0f, vl);\n    vbool8_t v_cond = vle8_v_b8(cond, vl);\n\n    for (size_t i = 0; i < size; i += vl) {\n        vint32m8_t v_A = vle32_v_i32m8(A + i, vl);\n        if (i > 0) {\n            vint32m8_t v_prev_A = vle32_v_i32m8(A + i - vl, vl);\n            vint32m8_t v_diff_int = vsub_vv_i32m8(v_A, v_prev_A, vl);\n            v_diff = vfcti_v_f_f32m8(v_diff_int, vl);\n        }\n        v_diff = vselect_vvm_f_f32m8(v_cond, v_diff, vfmv_v_f_f32m8(0.0f, vl), vl);\n        vse32_v_f(diff + i, v_diff, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    vuint32m8_t v_size = vle32_v_u32m8(size, A, size);\n    vbool8_t v_cond = vle8_v_b8(size, (const uint8_t*)cond, size);\n\n    vfloat32m8_t v_diff = vmul_vx_f32m8(v_flt_f32m8(0.0f), v_size, size);\n    v_diff = vfsub_vv_f32m8(v_diff, v_fmul_vx_f32m8(v_flt_f32m8(1.0f), v_size, size), size);\n\n    for (size_t i = 1; i < size; i++) {\n        vuint32m8_t v_index = vmul_vx_u32m8(v_flt_f32m8(i), v_size, size);\n        vuint32m8_t v_prev_index = vmul_vx_u32m8(v_flt_f32m8(i - 1), v_size, size);\n        vint32m8_t v_diff_int = vsub_vv_i32m8(vle32_v_i32m8(v_index, A, size), vle32_v_i32m8(v_prev_index, A, size), size);\n        vfloat32m8_t v_diff_float = vfwredsum_vs_f32m8_f32m8_m(v_diff_int, v_diff, v_cond, size);\n        v_diff = vfmerge_vvm_f32m8(v_diff, v_diff_float, v_cond, size);\n    }\n\n    vse32_v_f32m8(diff, v_diff, size);\n}\n```"}
{"task_id": "SimdBench_58_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    vint32m8_t va = vle32_v_i32m8(A, size * sizeof(int32_t));\n    vboolm8_t vb = vle8_v_b8(cond, size * sizeof(bool));\n    vfloat32m8_t vd = vfmv_v_f_f32m8(0.0f);\n\n    for (size_t i = 1; i < size; i++) {\n        vint32m8_t va_prev = vle32_v_i32m8(A + i - 1, sizeof(int32_t));\n        vint32m8_t va_diff = vsub_vv_i32m8(va, va_prev);\n        vfloat32m8_t vf_diff = vfrcvt_s_w_f_f32m8(va_diff);\n        vd = vselect_vvm_f_f32m8(vb, vf_diff, vd);\n    }\n\n    vse32_v_f(diff, vd, size * sizeof(float));\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vuint64m1_t v_dst = vmseq_vx_u64m1(v_src, 0xFFFFFFFF, vl);\n    vse64_v_u64m1(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vuint64m1_t v_dst = vmseq_vx_u64m1(v_src, 0xFFFFFFFF, vl);\n    vse64_v_u64m1(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vuint64m1_t v_dst = vmseq_vx_u64m1(v_src, 0xFFFFFFFFu, vl);\n    vse64_v_u64m1(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vuint64m1_t v_dst = vmseq_vx_u64m1(v_src, 0xFFFFFFFFU, vl);\n    vse64_v_u64m1(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_59_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vuint32m1_t v_src = vle32_v_u32m1(src, vl);\n    vuint64m1_t v_dst = vmseq_vx_u64m1(v_src, 0xFFFFFFFFu, vl);\n    vse64_v_u64m1(dst, v_dst, vl);\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vfloat64m1_t vsum = vfmv_v_f_f64m1(0.0, vl);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t vl = vsetvl_e64m1(length - i);\n        vfloat64m1_t vvec = vle64_v_f64m1(vec + i, vl);\n        vfloat64m1_t vidx = vle16_v_i16m1(index + i, vl);\n        vfloat64m1_t vtmp = vle64_v_f64m1(vec + vvidx, vl);\n        vsum = vfadd_vv_f64m1(vsum, vtmp, vl);\n    }\n\n    return vfredosum_vs_f64m1_f64m1(0.0, vsum, vl);\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vfloat64m1_t v_vec = vle64_v_f64m1(vec, vl);\n    vint16m1_t v_index = vle16_v_i16m1(index, vl);\n    vfloat64m1_t v_result = vmul_vf_f64m1(v_vec, v_index, vl);\n    vresult = vfredsum_vs_f64m1_f64m1(0.0, v_result, v_result, vl);\n    return vresult;\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vfloat64m1_t vsum = vfmv_v_f_f64m1(0.0, vl);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t chunk_length = vsetvl_e64m1(length - i);\n        vint16m1_t vidx = vle16_v_i16m1(index + i, chunk_length);\n        vfloat64m1_t vval = vle64_v_f64m1(vec + vle16_v_i32m1(vreinterpret_v_i16m1_i32m1(vidx), chunk_length), chunk_length);\n        vsum = vfadd_vv_f64m1(vsum, vval, chunk_length);\n    }\n\n    return vfredosum_vs_f64m1_f64m1(0.0, vsum, vgetvl());\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vfloat64m1_t vsum = vfmv_v_f_f64m1(0.0, vl);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t vl_i = vsetvl_e64m1(length - i);\n        vfloat64m1_t vvec = vle64_v_f64m1(vec + i, vl_i);\n        vint16m1_t vidx = vle16_v_i16m1(index + i, vl_i);\n        vfloat64m1_t vindexed = vfwredosum_vs_f64m1_f64m1(vsum, vvec, vvidx, vl_i);\n        vsum = vfadd_vf_f64m1(vsum, vindexed, vl_i);\n    }\n\n    return vfirst_m_f64m1(vsum);\n}\n```"}
{"task_id": "SimdBench_60_RVV", "completion": "```c\n#include <riscv_vector.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    size_t vl = vsetvl_e64m1(length);\n    vfloat64m1_t vsum = vfmv_v_f_f64m1(0.0, vl);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t vl = vsetvl_e64m1(length - i);\n        vfloat64m1_t vidx = vle16_v_i16m1(index + i, vl);\n        vfloat64m1_t vval = vle64_v_f64m1(vec + vvidx.v[0], vl);\n        vsum = vfadd_vv_f64m1(vsum, vval, vl);\n    }\n\n    return vfredosum_vs_f64m1_f64m1(0.0, vsum, vsum, vgetvl());\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    vfloat64m8_t v_input, v_kernel, v_output;\n    vint32m8_t v_idx;\n\n    for (size_t i = 0; i < output_size; i += vl) {\n        v_idx = vle32_v_i32m8(i, vl);\n        v_output = vmul_vf_f64m8(vzero_v_f64m8(), 0.0, vl);\n\n        for (size_t j = 0; j < kernel_size; j++) {\n            v_input = vle64_v_f64m8(input + i + j * input_size, vl);\n            v_kernel = vle64_v_f64m8(kernel + j * kernel_size, vl);\n            v_output = vfadd_vv_f64m8(v_output, vmul_vv_f64m8(v_input, v_kernel, vl), vl);\n        }\n\n        v_output = vfmax_vf_f64m8(v_output, 0.0, vl);\n        vse64_v_f64m8(output + i, v_output, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    vfloat64m8_t v_input, v_kernel, v_output;\n    vint32m8_t v_idx;\n\n    for (size_t i = 0; i < output_size; i += vl) {\n        v_idx = vmul_vx_i32m8(vl, vle32_v_i32m8(i, (const int32_t*)input, vl), kernel_size);\n        v_input = vle64_v_f64m8(vl, &input[v_idx], vl);\n        v_kernel = vle64_v_f64m8(vl, kernel, vl);\n\n        v_output = vfma_vv_f64m8(vl, v_input, v_kernel, vfmacc_vv_f64m8(vl, v_input, v_kernel, 0.0));\n\n        v_output = vmax_vv_f64m8(vl, v_output, vfsub_vf_f64m8(vl, vzero_v_f64m8(), v_output));\n\n        vse64_v_f64m8(&output[i], v_output, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    vfloat64m8_t v_input, v_kernel, v_output;\n    vint32m8_t v_idx;\n\n    for (size_t i = 0; i < output_size; i += vl) {\n        v_idx = vmul_vx_i32m8(vl, i, kernel_size);\n        v_output = vfsub_vf_f64m8(zero_vf64m8(), 0.0);\n\n        for (size_t j = 0; j < kernel_size; j++) {\n            v_input = vle64_v_f64m8(input + i * input_size + v_idx, vl);\n            v_kernel = vle64_v_f64m8(kernel + j * kernel_size, vl);\n            v_output = vfadd_vv_f64m8(v_output, vfmul_vv_f64m8(v_input, v_kernel), vl);\n        }\n\n        v_output = vfmax_vf_f64m8(v_output, zero_vf64m8());\n        vse64_v_f64m8(output + i, v_output, vl);\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    vfloat64m8_t v_input, v_kernel, v_output;\n    vint32m8_t v_idx;\n\n    for (size_t i = 0; i < output_size; i++) {\n        v_idx = vlset_v_i32m8(i, 1, vsetvl_e32m8(1));\n        v_output = vfmacc_vf_f64m8(vzero_v_f64m8(), vld_v_f64m8(input + i * input_size, v_idx), vld_v_f64m8(kernel, v_idx));\n\n        for (size_t j = 1; j < kernel_size; j++) {\n            v_idx = vlset_v_i32m8(i + j * input_size, 1, vsetvl_e32m8(1));\n            v_output = vfmacc_vf_f64m8(v_output, vld_v_f64m8(input + i * input_size + j, v_idx), vld_v_f64m8(kernel + j, v_idx));\n        }\n\n        v_output = vfmax_vv_f64m8(v_output, vzero_v_f64m8());\n        vst_v_f64m8(output + i, v_output, vsetvl_e32m8(1));\n    }\n}\n```"}
{"task_id": "SimdBench_61_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    vfloat64m8_t v_input, v_kernel, v_output;\n    vint32m8_t v_idx;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        v_idx = vlset_v_i32m8(i, 1, vsetvl_e32m8(1));\n        v_output = vfsub_vf_f64m8(vfmul_vv_f64m8(vle64_v_f64m8(input + i * input_size, vsetvl_e64m8(kernel_size)), vle64_v_f64m8(kernel, vsetvl_e64m8(kernel_size))), 0.0);\n\n        for (size_t j = 1; j < kernel_size; ++j) {\n            v_idx = vlset_v_i32m8(i + j * output_size, 1, vsetvl_e32m8(1));\n            v_output = vfadd_vv_f64m8(v_output, vfmul_vv_f64m8(vle64_v_f64m8(input + (i + j * output_size) * input_size, vsetvl_e64m8(kernel_size)), vle64_v_f64m8(kernel, vsetvl_e64m8(kernel_size))));\n        }\n\n        v_output = vfmax_vf_f64m8(v_output, 0.0);\n        vse64_v_f64m8(output + i, v_output, vsetvl_e64m8(1));\n    }\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    vfloat64m1_t v_threshold = vle64_v_f64m1(threshold, &threshold, 1);\n    vfloat64m1_t v_numbers = vle64_v_f64m1(numbers.data(), n, 1);\n\n    for (size_t i = 0; i + 7 < n; i += 8) {\n        vfloat64m1_t v_diffs = vfsub_vv_f64m1(v_numbers[i:i+8], v_numbers[i:i+8]);\n        vfloat64m1_t v_abs_diffs = vfabs_v_f64m1(v_diffs);\n        vbool64m1_t v_mask = vflt_m_b64(v_abs_diffs, v_threshold);\n        if (vmfirst_m_b64(v_mask)) return true;\n    }\n\n    // Handle remaining elements\n    for (size_t i = n - n % 8; i < n; ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            if (std::fabs(numbers[i] - numbers[j]) < threshold) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    vfloat64m1_t v_threshold = vle64_v_f64m1(threshold, &threshold, 1);\n    vfloat64m1_t v_numbers = vle64_v_f64m1(numbers.data(), n, 1);\n\n    for (size_t i = 0; i + 7 < n; i += 8) {\n        vfloat64m1_t v_diffs = vfsub_vv_f64m1(v_numbers[i:i+8], v_numbers[i:i+8]);\n        vfloat64m1_t v_abs_diffs = vfabs_v_f64m1(v_diffs);\n        vbool64m1_t v_mask = vflt_m_b64(v_abs_diffs, v_threshold);\n        if (vmfirst_m_b64(v_mask)) return true;\n    }\n\n    // Handle remaining elements\n    for (size_t i = n - n % 8; i < n; ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            if (std::fabs(numbers[i] - numbers[j]) < threshold) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    vfloat64m1_t v_threshold = vle64_v_f64m1(numbers.data(), n);\n    vfloat64m1_t v_diff;\n    bool result = false;\n\n    for (size_t i = 0; i + 7 < n; i += 8) {\n        vfloat64m1_t v_a = vle64_v_f64m1(numbers.data() + i, 8);\n        for (int j = 0; j < 8; ++j) {\n            v_diff = vfsub_vf_f64m1(v_a, v_a[j], 8);\n            v_diff = vfadd_vv_f64m1(v_diff, v_threshold, 8);\n            v_diff = vmin_vv_f64m1(v_diff, v_neg_v_f64m1(v_diff), 8);\n            if (vmfirst_m_b1(vnez_vv_mask_f64m1(v_diff, vzero_v_f64m1(8)))) {\n                result = true;\n                break;\n            }\n        }\n        if (result) break;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    vfloat64m1_t v_threshold = vle64_v_f64m1(numbers.data(), n);\n    vfloat64m1_t v_diffs;\n    bool result = false;\n\n    for (size_t i = 0; i + 7 < n; i += 8) {\n        vfloat64m1_t v_a = vle64_v_f64m1(numbers.data() + i, 8);\n        for (int j = 0; j < 8; ++j) {\n            vfloat64m1_t v_b = vle64_v_f64m1(numbers.data() + i + j + 1, 8);\n            v_diffs = vfsub_vv_f64m1(v_a, v_b);\n            v_diffs = vfabs_v_f64m1(v_diffs);\n            v_diffs = vmin_vv_f64m1(v_diffs, v_threshold);\n            result |= vmfirst_m_b1(v_diffs != v_threshold);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_62_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t n = numbers.size();\n    if (n < 2) return false;\n\n    vfloat64m1_t v_threshold = vle64_v_f64m1(threshold, &threshold, 1);\n    vfloat64m1_t v_numbers = vle64_v_f64m1(numbers.data(), n, 1);\n\n    for (size_t i = 0; i < n - 1; i += 8) {\n        vfloat64m1_t v_current = vle64_v_f64m1(&numbers[i], 8, 1);\n        vfloat64m1_t v_diffs = vfsub_vv_f64m1(v_current, v_numbers, 8);\n        vfloat64m1_t v_abs_diffs = vfsgnjx_vv_f64m1(v_diffs, v_diffs, 8);\n        vbool64m1_t v_mask = vflt_m_v_f64m1(v_abs_diffs, v_threshold, 8);\n        if (vmfirst_m_b64(v_mask)) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    size_t n = operations.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* ptr = const_cast<int32_t*>(operations.data());\n    vint32m1_t v_balance = vmv_v_x_i32m1(v_balance, 0, vlmax);\n    vbool1_t v_mask = vmseq_vx_i32m1_b1(v_balance, 0, vlmax);\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        vint32m1_t v_operations = vle32_v_i32m1(ptr + i, vlmax);\n        v_balance = vwredsum_vs_i32m1_i32m1(v_balance, v_operations, v_balance, vlmax);\n        v_mask = vor_vv_b1(v_mask, vmseq_vx_i32m1_b1(v_balance, 0, vlmax), vlmax);\n    }\n\n    return vmfirst_m_b1(v_mask);\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    size_t n = operations.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* op_ptr = const_cast<int32_t*>(operations.data());\n    vint32m1_t ops = vle32_v_i32m1(op_ptr, vlmax);\n\n    vint32m1_t balance = vmv_v_x_i32m1(0, vlmax);\n    vboolm1_t below_zero = vmv_v_x_b1m1(false, vlmax);\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        vint32m1_t current_ops = vsub_vv_i32m1(balance, ops, vlmax);\n        balance = vadd_vv_i32m1(balance, ops, vlmax);\n        below_zero |= vmslt_vx_b1m1(current_ops, 0, vlmax);\n    }\n\n    return vbtest_m(below_zero, vlmax);\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    size_t n = operations.size();\n    vint32m8_t balances = vle32_v_i32m8(operations.data(), n);\n    vbool8_t mask = vmul_vx_i32m8_mask(balances, balances, 0);\n    bool result = vmfirst_m_b8(mask);\n    return result;\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    size_t n = operations.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* ops_ptr = const_cast<int32_t*>(operations.data());\n    vint32m1_t ops_v = vle32_v_i32m1(ops_ptr, vlmax);\n\n    vint32m1_t balance_v = vmv_v_x_i32m1(0, vlmax);\n    vboolm1_t below_zero_v = vmul_vx_v_i32m1(below_zero_v, balance_v, 0, vlmax);\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        vint32m1_t ops_slice_v = vsub_vv_v_i32m1(balance_v, ops_v, vlmax);\n        balance_v = ops_slice_v;\n        below_zero_v = vor_vv_v_b1m1(below_zero_v, vlt_vx_v_b1m1(ops_slice_v, 0, vlmax), vlmax);\n    }\n\n    return vbpopcnt_m_b1m1(below_zero_v, vlmax) > 0;\n}\n```"}
{"task_id": "SimdBench_63_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    size_t n = operations.size();\n    vint32m8_t balances = vle32_v_i32m8(operations.data(), n);\n    vbool8_t below_zero_mask = vbsetone_m_b8(n);\n    for (size_t i = 0; i < n; ++i) {\n        balances = vadd_vx_i32m8(balances, operations[i]);\n        below_zero_mask = vor_vv_b8(below_zero_mask, vslt_vx_i32m8(balances, 0));\n        if (vfirst_m_b8(below_zero_mask)) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    vfloat64m8_t v_numbers = vle64_v_f64m8(numbers.data(), n);\n    vfloat64m8_t v_sum = vfredsum_vs_f64m8_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmv_s_s_f64m8_f64(1.0), v_numbers), vfmv_s_s_f64m8_f64(0.0));\n    double sum = vfirst_m_f64m8(v_sum);\n\n    vfloat64m8_t v_mean = vfdiv_vf_f64m8(v_sum, vfmv_s_s_f64m8_f64(n));\n\n    vfloat64m8_t v_diff = vsub_vv_f64m8(v_numbers, v_mean);\n    vfloat64m8_t v_abs_diff = vfsgnjx_vv_f64m8(v_diff, v_diff);\n    vfloat64m8_t v_abs_sum = vfredsum_vs_f64m8_f64m8(v_abs_diff, vabs_v_f64m8(v_abs_diff), vfmv_s_s_f64m8_f64(0.0));\n    double abs_sum = vfirst_m_f64m8(v_abs_sum);\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    vfloat64m8_t v_numbers = vle64_v_f64m8(numbers.data(), n);\n    vfloat64m8_t v_sum = vfredsum_vs_f64m8_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmul_vf_f64"}
{"task_id": "SimdBench_64_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    vfloat64m8_t v_numbers = vle64_v_f64m8(numbers.data(), n);\n    vfloat64m8_t v_sum = vfredsum_vs_f64m8_f64m8(v_numbers, vfmv_s_s_f64m8(0.0), v_setvl_e64m8(n));\n    double sum = vfirst_m_x_f64m8(v_sum);\n\n    vfloat64m8_t v_mean = vfdiv_vf_f64m8(v_sum, v_setvl_e64m8(n));\n\n    vfloat64m8_t v_diff = vfsub_vv_f64m8(v_numbers, v_mean);\n    vfloat64m8_t v_abs_diff = vfsgnjx_vv_f64m8(v_diff, v_diff);\n    vfloat64m8_t v_abs_sum = vfredsum_vs_f64m8_f64m8(v_abs_diff, vfmv_s_s_f64m8(0.0), v_setvl_e64m8(n));\n    double abs_sum = vfirst_m_x_f64m8(v_abs_sum);\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    vfloat64m8_t v_numbers = vle64_v_f64m8(numbers.data(), n);\n    vfloat64m8_t v_sum = vfredsum_vs_f64m8_f64m8(v_numbers, vfmul_vf_f64m8(v_numbers, vfmv_s_s_f64m8_f64(1.0), v_numbers), vfmv_s_s_f64m8_f64(0.0));\n    double sum = vfirst_m_f64m8(v_sum);\n\n    vfloat64m8_t v_mean = vfdiv_vf_f64m8(v_sum, vfmv_s_s_f64m8_f64(n));\n\n    vfloat64m8_t v_diff = vsub_vv_f64m8(v_numbers, v_mean);\n    vfloat64m8_t v_abs_diff = vfsgnjx_vv_f64m8(v_diff, v_diff);\n    vfloat64m8_t v_abs_sum = vfredsum_vs_f64m8_f64m8(v_abs_diff, vabs_v_f64m8(v_abs_diff), vfmv_s_s_f64m8_f64(0.0));\n    double abs_sum = vfirst_m_f64m8(v_abs_sum);\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    vfloat64m8_t v_numbers = vle64_v_f64m8(numbers.data(), n);\n    vfloat64m8_t v_sum = vfredsum_vs_f64m8_f64m8(v_numbers, vfmv_s_s_f64m8_f64(0.0), vsetvl_e64m8(n));\n    double sum = vfirst_m_x_f64m8(v_sum);\n\n    vfloat64m8_t v_mean = vfdiv_vf_f64m8(v_setvl_e64m8(n), v_sum, v_setvl_e64m8(n));\n\n    vfloat64m8_t v_diff = vfsub_vv_f64m8(v_numbers, v_mean, v_setvl_e64m8(n));\n    vfloat64m8_t v_abs_diff = vfsgnjx_vv_f64m8(v_diff, v_diff, v_setvl_e64m8(n));\n    vfloat64m8_t v_abs_sum = vfredsum_vs_f64m8_f64m8(v_abs_diff, vfmv_s_s_f64m8_f64(0.0), v_setvl_e64m8(n));\n    double abs_sum = vfirst_m_x_f64m8(v_abs_sum);\n\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t total_size = n * 2 - 1;\n    std::vector<int> result(total_size);\n\n    vint32m8_t v_numbers = vle32_v_i32m8(numbers.data(), n);\n    vint32m8_t v_delimiter = vmv_v_x_i32m8(delimiter, n);\n\n    size_t i = 0;\n    for (; i + 7 < n; i += 8) {\n        vse32_v_i32m8(result.data() + i * 2, v_numbers, 8);\n        vse32_v_i32m8(result.data() + i * 2 + 8, v_delimiter, 8);\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n        result[i * 2 + 1] = delimiter;\n    }\n\n    // Remove the last delimiter\n    result.pop_back();\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t total_size = n * 2 - 1;\n    std::vector<int> result(total_size);\n\n    vint32m8_t v_numbers = vle32_v_i32m8(numbers.data(), n);\n    vint32m8_t v_delimiter = vmv_v_x_i32m8(delimiter, n);\n\n    size_t i = 0;\n    for (; i < n - 1; i += 8) {\n        vse32_v_i32m8(result.data() + i * 2, v_numbers, 8);\n        vse32_v_i32m8(result.data() + i * 2 + 8, v_delimiter, 8);\n    }\n\n    if (i < n) {\n        vse32_v_i32m8(result.data() + i * 2, v_numbers, n - i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t total_size = n * 2 - 1;\n    std::vector<int> result(total_size);\n\n    vint32m8_t v_numbers = vle32_v_i32m8(numbers.data(), n);\n    vint32m8_t v_delimiter = vmv_v_x_i32m8(delimiter, n);\n\n    size_t i = 0;\n    for (; i < n - 1; i += 8) {\n        vse32_v_i32m8(result.data() + i * 2, v_numbers, 8);\n        vse32_v_i32m8(result.data() + i * 2 + 1, v_delimiter, 8);\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t total_size = n * 2 - 1;\n    std::vector<int> result(total_size);\n\n    vint32m8_t v_numbers = vle32_v_i32m8(numbers.data(), n);\n    vint32m8_t v_delimiter = vmv_v_x_i32m8(delimiter, n);\n\n    size_t i = 0;\n    for (; i < n - 1; i += 8) {\n        vse32_v_i32m8(result.data() + i * 2, v_numbers, 8);\n        vse32_v_i32m8(result.data() + i * 2 + 8, v_delimiter, 8);\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t total_size = n * 2 - 1;\n    std::vector<int> result(total_size);\n\n    vint32m8_t v_numbers = vle32_v_i32m8(numbers.data(), n);\n    vint32m8_t v_delimiter = vmv_v_x_i32m8(delimiter, n);\n\n    size_t i = 0;\n    for (; i + 7 < n; i += 8) {\n        vse32_v_i32m8(result.data() + i * 2, v_numbers, 8);\n        vse32_v_i32m8(result.data() + i * 2 + 8, v_delimiter, 8);\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != n - 1) {\n            result[i * 2 + 1] = delimiter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    int16_t* data = numbers.data();\n    vint16m8_t vdata = vle16_v_i16m8(data, n);\n\n    vint64m1_t vsum = vredsum_vs_i16m8_i64m1(vdata, vzero_i64m1(), vl);\n    vint64m1_t vproduct = vfredprod_vs_i16m8_i64m1(vdata, vone_i64m1(), vl);\n\n    int64_t sum = vfirst_m_i64m1(vsum);\n    int64_t product = vfirst_m_i64m1(vproduct);\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    int16_t v_sum = 0;\n    int32_t v_prod = 1;\n\n    size_t vl = __riscv_vsetvl_e16m8(n);\n    vint16m8_t v_numbers = __riscv_vle16_v_i16m8(numbers.data(), vl);\n\n    while (vl > 0) {\n        v_sum = __riscv_vsadd_vx_i16m8(v_sum, __riscv_vfirst_m_i16m8(v_numbers), vl);\n        v_prod = __riscv_vmul_vx_i32m4(v_prod, __riscv_vfirst_m_i16m8(v_numbers), vl);\n        vl = __riscv_vsetvl_e16m8(n - vl);\n        v_numbers = __riscv_vle16_v_i16m8(numbers.data() + n - vl, vl);\n    }\n\n    return {(int64_t)v_sum, (int64_t)v_prod};\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    int16_t v_sum = 0;\n    int64_t v_prod = 1;\n\n    size_t vl = __riscv_vsetvl_e16m8(n);\n    vint16m8_t v_numbers = __riscv_vle16_v_i16m8(numbers.data(), vl);\n\n    v_sum = __riscv_vsadd_vx_i16m8(v_sum, __riscv_vredsum_vs_i16m8_i16m1(v_numbers, 0, vl), vl);\n    v_prod = __riscv_vmul_vx_i64m1(v_prod, __riscv_vredprod_vs_i16m8_i64m1(v_numbers, 1, vl), vl);\n\n    return {v_sum, v_prod};\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    int16_t v_numbers[16];\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    size_t i = 0;\n    for (; i + 15 < n; i += 16) {\n        for (int j = 0; j < 16; ++j) {\n            v_numbers[j] = numbers[i + j];\n        }\n        vint16m1_t v_num = vle16_v_i16m1(v_numbers, 16);\n        sum += vredsum_vs_i16m1_i64m1(sum, v_num, 0);\n        product *= vfredprod_vs_i16m1_i64m1(product, v_num, 1);\n    }\n\n    for (; i < n; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    int16_t v_sum = 0;\n    int64_t v_prod = 1;\n\n    size_t vl = __riscv_vsetvl_e16m8(n);\n    vint16m8_t v_numbers = __riscv_vle16_v_i16m8(numbers.data(), vl);\n\n    v_sum = __riscv_vsadd_vx_i16m8(v_sum, v_numbers, vl);\n    v_prod = __riscv_vmul_vx_i64m1(v_prod, v_numbers, vl);\n\n    int16_t sum = __riscv_vredsum_vs_i16m8_i16m1(v_sum, 0, vl);\n    int64_t prod = __riscv_vfredprod_vs_i64m1_i64m1(v_prod, 1, vl);\n\n    return {static_cast<int64_t>(sum), prod};\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    vint32m8_t v_result = vle32_v_i32m8(numbers.data(), n / 8 + (n % 8 != 0));\n    int max_val = numbers[0];\n    for (size_t i = 0; i < n; ++i) {\n        if (numbers[i] > max_val) {\n            max_val = numbers[i];\n        }\n        result[i] = max_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    vint32m8_t v_result = vle32_v_i32m8(numbers.data(), n, vlmax_e32m8(n));\n    int max_val = vfirst_m_b8(v_cmpgt_vx_i32m8(v_result, numbers[0], vlmax_e32m8(1)));\n\n    for (size_t i = 1; i < n; ++i) {\n        vint32m8_t v_numbers = vle32_v_i32m8(&numbers[i], n - i, vlmax_e32m8(n - i));\n        v_result = v_max_vv_i32m8(v_result, v_numbers, vlmax_e32m8(n - i));\n        max_val = vfirst_m_b8(v_cmpgt_vx_i32m8(v_result, numbers[max_val], vlmax_e32m8(1)));\n        result[i] = numbers[max_val];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    vint32m8_t v_result = vle32_v_i32m8(numbers.data(), n, vlmax_e32m8(n));\n    vint32m8_t v_numbers = vle32_v_i32m8(numbers.data(), n, vlmax_e32m8(n));\n\n    for (size_t i = 0; i < n; ++i) {\n        vint32m8_t v_current = vle32_v_i32m8(&numbers[i], 1, vlmax_e32m8(1));\n        v_result = vmax_vv_i32m8(v_result, v_current, vlmax_e32m8(1));\n        vse32_v_i32m8(&result[i], v_result, 1, vlmax_e32m8(1));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    vint32m8_t v_result = vle32_v_i32m8(numbers.data(), n, vlmax_e32m8(n));\n    int max_value = vfirst_m_b8(v_cmpgt_vx_i32m8(v_result, numbers[0], vlmax_e32m8(n)));\n\n    for (size_t i = 1; i < n; ++i) {\n        vint32m8_t v_numbers = vle32_v_i32m8(&numbers[i], n - i, vlmax_e32m8(n - i));\n        v_result = v_max_vv_i32m8(v_result, v_numbers, vlmax_e32m8(n - i));\n        max_value = vfirst_m_b8(v_cmpgt_vx_i32m8(v_result, numbers[max_value], vlmax_e32m8(n - i)));\n        result[i] = numbers[max_value];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    vint32m8_t v_result = vle32_v_i32m8(numbers.data(), n, vlmax_e32m8(n));\n    vint32m8_t v_numbers = vle32_v_i32m8(numbers.data(), n, vlmax_e32m8(n));\n\n    for (size_t i = 0; i < n; ++i) {\n        vint32m8_t v_current = vle32_v_i32m8(&numbers[i], 1, vlmax_e32m8(1));\n        v_result = vmax_vv_i32m8(v_result, v_current, vlmax_e32m8(1));\n        vse32_v_i32m8(&result[i], v_result, 1, vlmax_e32m8(1));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    vuint8m1_t va = vle8_v_u8m1(a.data(), len);\n    vuint8m1_t vb = vle8_v_u8m1(b.data(), len);\n\n    vuint8m1_t vresult = vxor_vv_u8m1(va, vb, len);\n\n    vse8_v_u8m1(result.data(), vresult, len);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must have the same length\");\n    }\n\n    std::string result(len, '\\0');\n    size_t i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        vuint8m1_t va = vle8_v_u8m1(a.data() + i, 32);\n        vuint8m1_t vb = vle8_v_u8m1(b.data() + i, 32);\n        vuint8m1_t vr = vxor_vv_u8m1(va, vb, 32);\n        vse8_v_u8m1(result.data() + i, vr, 32);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    size_t vlen = __riscv_vsetvl_e8m1(len);\n    uint8_t* va = reinterpret_cast<uint8_t*>(const_cast<char*>(a.c_str()));\n    uint8_t* vb = reinterpret_cast<uint8_t*>(const_cast<char*>(b.c_str()));\n    uint8_t* vr = reinterpret_cast<uint8_t*>(result.data());\n\n    for (size_t i = 0; i < len; i += vlen) {\n        vuint8m1_t va_vec = __riscv_vle8_v_u8m1(va + i, vlen);\n        vuint8m1_t vb_vec = __riscv_vle8_v_u8m1(vb + i, vlen);\n        vuint8m1_t vr_vec = __riscv_vsra_vx_u8m1(va_vec ^ vb_vec, 0, vlen);\n        __riscv_vse8_v_u8m1(vr + i, vr_vec, vlen);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must have the same length\");\n    }\n\n    std::string result(len, '\\0');\n    size_t i = 0;\n\n    for (; i + 32 <= len; i += 32) {\n        vuint8m1_t va = vle8_v_u8m1(a.data() + i, 32);\n        vuint8m1_t vb = vle8_v_u8m1(b.data() + i, 32);\n        vuint8m1_t vr = vxor_vv_u8m1(va, vb, 32);\n        vse8_v_u8m1(result.data() + i, vr, 32);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    vuint8m1_t va = vle8_v_u8m1(a.data(), len);\n    vuint8m1_t vb = vle8_v_u8m1(b.data(), len);\n\n    vuint8m1_t xor_result = vxor_vv_u8m1(va, vb);\n\n    vse8_v_u8m1(result.data(), xor_result, len);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n#include <optional>\n\nstd::optional<std::string> longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return std::nullopt;\n    }\n\n    size_t max_len = 0;\n    const char* longest_str = nullptr;\n\n    size_t num_strings = strings.size();\n    size_t vec_size = vsetvl_e8m1(num_strings);\n\n    vuint32m1_t lengths = vmul_vx_u32m1(vle8_v_i8m1((const int8_t*)strings.data(), vec_size), vle8_v_i8m1((const int8_t*)strings.data() + vec_size, vec_size));\n\n    for (size_t i = 0; i < num_strings; i += vec_size) {\n        vbool1_t mask = vmseq_vx_b1(i, lengths);\n        vuint32m1_t current_max = vmax_vv_u32m1(lengths, lengths);\n        vuint32m1_t current_longest = vselect(mask, lengths, vle8_v_i8m1((const int8_t*)longest_str, vec_size));\n        max_len = vmfirst_m_b1(mask, current_max);\n        longest_str = (const char*)vfirst_m_b1(mask, current_longest);\n    }\n\n    return std::string(longest_str, max_len);\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <vector>\n#include <optional>\n\nstd::optional<std::string> longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return std::nullopt;\n    }\n\n    size_t max_len = 0;\n    const char* longest_str = nullptr;\n\n    size_t num_strings = strings.size();\n    size_t vec_size = vsetvl_e8m1(num_strings);\n\n    vuint32m1_t lengths = vmul_vx_u32m1(vle8_v_i8m1(strings.data(), vec_size), vle8_v_i8m1(strings.data() + vec_size, vec_size), vec_size);\n    vbool1_t mask = vmslt_vx_b1(lengths, max_len, vec_size);\n\n    while (vec_size > 0) {\n        vuint32m1_t lengths = vmul_vx_u32m1(vle8_v_i8m1(strings.data(), vec_size), vle8_v_i8m1(strings.data() + vec_size, vec_size), vec_size);\n        vbool1_t mask = vmslt_vx_b1(lengths, max_len, vec_size);\n\n        if (vmfirst_m_b1(mask)) {\n            max_len = vfirst_m_b1(mask);\n            longest_str = strings.data() + vmfirst_m_b1(mask);\n        }\n\n        vec_size = vsetvl_e8m1(vec_size - vec_size);\n    }\n\n    return std::string(longest_str, max_len);\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <iostream>\n#include <vector>\n#include <optional>\n#include <algorithm>\n\nstd::optional<std::string> longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return std::nullopt;\n    }\n\n    size_t max_len = 0;\n    const char* longest_str = nullptr;\n\n    for (const auto& str : strings) {\n        if (str.size() > max_len) {\n            max_len = str.size();\n            longest_str = str.c_str();\n        }\n    }\n\n    return std::string(longest_str);\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <vector>\n#include <optional>\n\nstd::optional<std::string> longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return std::nullopt;\n    }\n\n    size_t max_len = 0;\n    const char* longest_str = nullptr;\n\n    size_t num_strings = strings.size();\n    size_t vec_size = vsetvl_e8m1(num_strings);\n\n    vuint32m1_t lengths = vmul_vx_u32m1(vle8_v_i8m1(strings.data(), vec_size), vle8_v_i8m1(&strings[vec_size], vec_size), vec_size);\n    vbool1_t mask = vmsltu_vx_b1(vle8_v_i8m1(strings.data(), vec_size), vle8_v_i8m1(&strings[vec_size], vec_size), vec_size);\n\n    for (size_t i = 0; i < num_strings; i += vec_size) {\n        vuint32m1_t current_lengths = vmul_vx_u32m1(vle8_v_i8m1(strings.data() + i, vec_size), vle8_v_i8m1(strings.data() + i, vec_size), vec_size);\n        vbool1_t current_mask = vmsltu_vx_b1(vle8_v_i8m1(strings.data() + i, vec_size), vle8_v_i8m1(strings.data() + i, vec_size), vec_size);\n\n        lengths = vor_m_b1(lengths, lengths, current_lengths, current_mask);\n        mask = vor_m_b1(mask, mask, current_mask, current_mask);\n    }\n\n    for (size_t i = 0; i < num_strings; ++i) {\n        if (mask[i]) {\n            if (strings[i].length() > max_len) {\n                max_len = strings[i].length();\n                longest_str = strings[i].data();\n            }\n        }\n    }\n\n    return longest_str ? std::string(longest_str, max_len) : std::nullopt;\n}\n```"}
{"task_id": "SimdBench_69_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <vector>\n#include <optional>\n\nstd::optional<std::string> longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return std::nullopt;\n    }\n\n    size_t max_len = 0;\n    const char* longest_str = nullptr;\n\n    size_t num_strings = strings.size();\n    size_t vec_size = vsetvl_e8m1(num_strings);\n\n    vuint32m1_t lengths = vmul_vx_u32m1(vle8_v_i8m1((const int8_t*)strings.data(), vec_size), vle8_v_i8m1((const int8_t*)strings.data() + vec_size, vec_size));\n\n    for (size_t i = 0; i < num_strings; i += vec_size) {\n        vuint32m1_t current_lengths = vle32_v_u32m1(lengths, vec_size);\n        vbool16_t mask = vmseq_vx_b16(current_lengths, vmov_v_x_u32m1(max_len, vec_size));\n        vuint32m1_t new_max_len = vmax_vv_u32m1(current_lengths, vmov_v_x_u32m1(max_len, vec_size), mask);\n        max_len = vmfirst_m_b16(mask);\n\n        if (max_len > 0) {\n            longest_str = strings[i];\n        }\n    }\n\n    return longest_str ? std::string(longest_str, max_len) : std::nullopt;\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    vuint64m8_t va = __vdup_v_u64(a, vlmax_v_u64());\n    vuint64m8_t vb = __vdup_v_u64(b, vlmax_v_u64());\n    while (__builtin_rvv_vredand_vs_u64_m(vlmax_v_u64(), va, vb)) {\n        vuint64m8_t temp = va;\n        va = vb;\n        vb = __vrem_vv_u64_m(vlmax_v_u64(), temp, vb);\n    }\n    return va[0];\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    vuint64m1_t va = __rvm_vle64_v_u64m1(&a, 1);\n    vuint64m1_t vb = __rvm_vle64_v_u64m1(&b, 1);\n\n    while (__rvm_vmseq_vx_u64m1(va, vb)) {\n        va = __rvm_vsrl_vx_u64m1(va, vb);\n    }\n\n    return __rvm_vmv_x_s_u64m1_u64(va);\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    vuint64m1_t va = __riscv_vle64_v_u64m1(&a, 1);\n    vuint64m1_t vb = __riscv_vle64_v_u64m1(&b, 1);\n\n    while (__riscv_vredand_vs_u64m1_u64(va, vb, 0)) {\n        vuint64m1_t temp = va;\n        va = vb;\n        vb = __riscv_vremu_vv_u64m1(vb, temp);\n    }\n\n    return __riscv_vmfirst_m_u64m1(va);\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    vuint64m8_t va = __vdup_v_u64(a, vlmax_v_u64());\n    vuint64m8_t vb = __vdup_v_u64(b, vlmax_v_u64());\n\n    while (__builtin_rvv_vredand_vs_u64m8(va, vb, va)) {\n        vuint64m8_t temp = va;\n        va = vb;\n        vb = __vrotr_vv_u64m8(vb, temp, vlmax_v_u64());\n    }\n\n    return va[0];\n}\n```"}
{"task_id": "SimdBench_70_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    vuint64m8_t va = vle64_v_u64m8(&a, 1);\n    vuint64m8_t vb = vle64_v_u64m8(&b, 1);\n\n    while (!vand_vv_u64m8(va, vb)) {\n        vuint64m8_t temp = va;\n        va = vb;\n        vb = vor_vv_u64m8(vsub_vv_u64m8(temp, vb), vrem_vv_u64m8(temp, vb));\n    }\n\n    return vfirst_m_b64(va);\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_diffs = vsub_vv_f32m8(v_numbers, v_numbers);\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        vfloat32m8_t v_diff = vfsub_vf_f32m8(v_numbers[i], v_numbers[i + 1]);\n        v_diffs = vmin_vv_f32m8(v_diffs, v_abs_v_f32m8(v_diff));\n    }\n\n    float min_diff = vfirst_m_i32m1(vle32_v_f32m1(&min_diff), vle32_v_f32m1(&min_diff));\n\n    float closest1 = std::numeric_limits<float>::max();\n    float closest2 = std::numeric_limits<float>::lowest();\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        float diff = std::abs(numbers[i] - numbers[i + 1]);\n        if (diff == min_diff) {\n            closest1 = std::min(numbers[i], numbers[i + 1]);\n            closest2 = std::max(numbers[i], numbers[i + 1]);\n            break;\n        }\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_diffs = vsub_vv_f32m8(v_numbers, v_numbers);\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        vfloat32m8_t v_diff = vfsub_vf_f32m8(v_numbers[i], v_numbers);\n        v_diffs = vmin_vv_f32m8(v_diffs, v_abs_v_f32m8(v_diff));\n    }\n\n    float min_diff = vfirst_m_f32m8_mask(vle32_v_f32m8(&min_diff, 1), vle32_v_f32m8(&min_diff, 1));\n\n    float closest1 = std::numeric_limits<float>::max();\n    float closest2 = std::numeric_limits<float>::lowest();\n\n    for (size_t i = 0; i < n; ++i) {\n        for (size_t j = i + 1; j < n; ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff == min_diff && numbers[i] < numbers[j]) {\n                closest1 = numbers[i];\n                closest2 = numbers[j];\n            }\n        }\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_diffs = vsub_vv_f32m8(v_numbers, v_numbers);\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        vfloat32m8_t v_diff = vfsub_vf_f32m8(v_numbers[i], v_numbers);\n        v_diffs = vmin_vv_f32m8(v_diffs, v_abs_v_f32m8(v_diff));\n    }\n\n    float min_diff = vfirst_m_f32m8_mask(vle32_v_f32m8(&min_diff, 1), vseq_vv_f32m8(vdiffs, vabs_v_f32m8(vdiffs)));\n\n    float closest1 = std::numeric_limits<float>::max();\n    float closest2 = std::numeric_limits<float>::lowest();\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        if (fabs(numbers[i] - numbers[i + 1]) == min_diff) {\n            closest1 = std::min(numbers[i], numbers[i + 1]);\n            closest2 = std::max(numbers[i], numbers[i + 1]);\n            break;\n        }\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n#include <limits>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_diffs = vsub_vv_f32m8(v_numbers, v_numbers);\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        vfloat32m8_t v_i = vfmul_vf_f32m8(v_numbers, v_fmv_s_s_f32m8(i), vrs1);\n        vfloat32m8_t v_j = vfmul_vf_f32m8(v_numbers, v_fmv_s_s_f32m8(j), vrs1);\n        vfloat32m8_t v_diff = vsub_vv_f32m8(v_i, v_j);\n        v_diffs = vmin_vv_f32m8(v_diffs, v_abs_v_f32m8(v_diff));\n    }\n\n    float min_diff = vfirst_m_f32m8_mask(vle32_v_f32m8(&min_diff, 1), vle32_v_f32m8(&min_diff, 1));\n\n    float closest1 = std::numeric_limits<float>::max();\n    float closest2 = std::numeric_limits<float>::lowest();\n\n    for (size_t i = 0; i < n; ++i) {\n        for (size_t j = i + 1; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) == min_diff) {\n                closest1 = std::min(numbers[i], numbers[j]);\n                closest2 = std::max(numbers[i], numbers[j]);\n                break;\n            }\n        }\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_diffs = vsub_vv_f32m8(v_numbers, v_numbers);\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        vfloat32m8_t v_diff = vfsub_vf_f32m8(v_numbers[i], v_numbers[i + 1]);\n        v_diffs = vmin_vv_f32m8(v_diffs, v_abs_v_f32m8(v_diff));\n    }\n\n    float min_diff = vfirst_m_i32m1(vle32_v_f32m8(&min_diff, 1));\n\n    vbool8_t mask = veq_vv_f32m8(vdiffs, vle32_v_f32m8(&min_diff, 1));\n    vfloat32m8_t v_indices = vmul_vx_f32m8(vmseq_vx_u8m8(mask, 1), vle32_v_f32m8(&i, 1));\n\n    size_t idx1 = vfirst_m_i32m1(vle32_v_f32m8(&idx1, 1));\n    size_t idx2 = vfirst_m_i32m1(vadd_vv_f32m8(v_indices, vle32_v_f32m8(&1, 1)));\n\n    return {numbers[idx1], numbers[idx2]};\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return numbers;\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_min = vfredmin_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 0.0f, 0.0f));\n    vfloat32m8_t v_max = vfredmax_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 0.0f, 0.0f));\n\n    vfloat32m8_t v_range = vsub_vv_f32m8(v_max, v_min);\n    vfloat32m8_t v_result = vfdiv_vv_f32m8(vsub_vv_f32m8(v_numbers, v_min), v_range);\n\n    std::vector<float> result(n);\n    vse32_v_f32m8(result.data(), v_result, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return numbers;\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_min = vfredmin_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 0.0f, 0.0f));\n    vfloat32m8_t v_max = vfredmax_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 0.0f, 0.0f));\n\n    vfloat32m8_t v_range = vsub_vv_f32m8(v_max, v_min);\n    vfloat32m8_t v_result = vfdiv_vv_f32m8(vsub_vv_f32m8(v_numbers, v_min), v_range);\n\n    std::vector<float> result(n);\n    vse32_v_f32m8(result.data(), v_result, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return numbers;\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_min = vfredmin_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 1.0f, 0.0f));\n    vfloat32m8_t v_max = vfredmax_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 1.0f, 0.0f));\n\n    vfloat32m8_t v_range = vsub_vv_f32m8(v_max, v_min);\n    vfloat32m8_t v_result = vfdiv_vv_f32m8(vsub_vv_f32m8(v_numbers, v_min), v_range);\n\n    std::vector<float> result(n);\n    vse32_v_f32m8(result.data(), v_result, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) return numbers;\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_min = vfredmin_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 1.0f, v_numbers));\n    vfloat32m8_t v_max = vfredmax_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 1.0f, v_numbers));\n\n    vfloat32m8_t v_range = vsub_vv_f32m8(v_max, v_min);\n    vfloat32m8_t v_result = vfdiv_vv_f32m8(vsub_vv_f32m8(v_numbers, v_min), v_range);\n\n    std::vector<float> result(n);\n    vse32_v_f32m8(result.data(), v_result, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    size_t n = numbers.size();\n    if (n < 2) {\n        return numbers;\n    }\n\n    vfloat32m8_t v_numbers = vle32_v_f32m8(numbers.data(), n);\n    vfloat32m8_t v_min = vfredmin_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 0.0f, 0.0f));\n    vfloat32m8_t v_max = vfredmax_vs_f32m8_f32m8(v_numbers, v_numbers, vfmacc_vf_f32m8(0.0f, 0.0f, 0.0f));\n\n    vfloat32m8_t v_range = vsub_vv_f32m8(v_max, v_min);\n    vfloat32m8_t v_result = vfdiv_vv_f32m8(vsub_vv_f32m8(v_numbers, v_min), v_range);\n\n    std::vector<float> result(n);\n    vse32_v_f32m8(result.data(), v_result, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    uint64_t result = 0;\n    vuint64m8_t v_n = vle64_v_u64m8(&n, 1);\n    vuint64m8_t v_i = vmul_vx_u64m8(v_n, 2);\n    vbool8_t mask = vmseq_vx_b8(v_i, v_n, 1);\n    while (vmfirst_m_b8(mask)) {\n        v_i = vmul_vx_u64m8(v_n, 2);\n        mask = vmseq_vx_b8(v_i, v_n, 1);\n        result = viota_v_u64m8(1, 1, 1);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    vuint64m8_t v_n = vle64_v_u64m8(&n, 1);\n    vuint64m8_t v_divisors = vmul_vx_u64m8(v_n, 2);\n    vbool8_t mask = vmseq_vx_b8(v_divisors, v_n, 1);\n    vuint64m8_t v_max_divisor = vmax_vv_u64m8(v_divisors, v_zero_v_u64m8(), mask);\n    uint64_t max_divisor = vfirst_m_b8(mask);\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    vuint64m8_t v_n = vle64_v_u64m8(&n, 1);\n    vuint64m8_t v_divisors = vmul_vx_u64m8(v_n, 2);\n    vbool8_t mask = vmseq_vx_b8(v_divisors, v_n);\n    v_divisors = vmin_vv_u64m8(v_divisors, v_n, mask);\n    uint64_t result = vfirst_m_b8(mask);\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    uint64_t result = 0;\n    vuint64m8_t v_n = vle64_v_u64m8(&n, 1);\n    vuint64m8_t v_divisors = vmul_vx_u64m8(v_n, 2);\n    vbool8_t mask = vmseq_vx_b8(v_divisors, v_n);\n    vuint64m8_t v_result = vmin_vv_u64m8(v_divisors, mask);\n    vse64_v_u64m8(&result, v_result, 1);\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    vuint64m8_t v_n = vle64_v_u64m8(&n, 1);\n    vuint64m8_t v_divisors = vmul_vx_u64m8(v_n, 2);\n    vbool8_t mask = vmseq_vx_b8(v_divisors, v_n, 1);\n    vuint64m8_t v_result = vmin_vv_u64m8(v_divisors, v_result, mask);\n    return vfirst_m_b8(mask);\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    vint32m1_t v_n = {n};\n    vbool8_t mask = true;\n\n    while (mask.any_true()) {\n        vint32m1_t v_factors = {factors.empty() ? 2 : factors.back()};\n        vint32m1_t v_remainders = v_n % v_factors;\n        mask = v_remainders == 0;\n        if (mask.any_true()) {\n            v_n /= v_factors;\n            factors.push_back(v_factors[0]);\n        } else {\n            v_factors += 1;\n            v_remainders = v_n % v_factors;\n            mask = v_remainders == 0;\n            if (mask.any_true()) {\n                v_n /= v_factors;\n                factors.push_back(v_factors[0]);\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    vint32m1_t v_n = {n};\n    vbool8_t mask = true;\n\n    while (mask.any_true()) {\n        vint32m1_t v_factors = {factors.empty() ? 2 : factors.back()};\n        vint32m1_t v_divisors = v_and(v_n, v_factors);\n        mask = v_eq(v_n, v_divisors);\n\n        if (mask.any_true()) {\n            factors.push_back(factors.empty() ? 2 : factors.back());\n            v_n = v_div(v_n, v_factors);\n        } else {\n            v_factors = v_add(v_factors, {1});\n            v_divisors = v_and(v_n, v_factors);\n            mask = v_eq(v_n, v_divisors);\n\n            if (mask.any_true()) {\n                factors.push_back(factors.empty() ? 2 : factors.back());\n                v_n = v_div(v_n, v_factors);\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    int i = 2;\n    while (n > 1) {\n        if (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i++;\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    int v = n;\n    int mask = 1;\n    while (v > 1) {\n        v = __riscv_vle32_mask(v, &mask);\n        int factor = 2;\n        while (factor * factor <= v) {\n            if (v % factor == 0) {\n                factors.push_back(factor);\n                v /= factor;\n            } else {\n                factor++;\n            }\n        }\n        if (v > 1) {\n            factors.push_back(v);\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    vint32m1_t v_n = {n};\n    vbool8_t mask = true;\n\n    while (mask.any_true()) {\n        vint32m1_t v_factors = {factors.empty() ? 2 : factors.back()};\n        vint32m1_t v_divisors = v_and(v_n, v_factors);\n        mask = v_eq(v_n, v_divisors);\n\n        if (mask.any_true()) {\n            factors.push_back(factors.empty() ? 2 : factors.back());\n            v_n = v_div(v_n, v_factors);\n        } else {\n            v_factors = v_add(v_factors, {1});\n            v_divisors = v_and(v_n, v_factors);\n            mask = v_eq(v_n, v_divisors);\n\n            if (mask.any_true()) {\n                factors.push_back(factors.empty() ? 2 : factors.back());\n                v_n = v_div(v_n, v_factors);\n            }\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n    size_t i = 0;\n\n    for (; i + vfloat32m8_t::vlmax() <= n; i += vfloat32m8_t::vlmax()) {\n        vfloat32m8_t vec = vle32_v_f32m8(l.data() + i, vfloat32m8_t::vlmax());\n        vbool32_t mask = vgts_vf_f32m8(vec, 0.0f, vfloat32m8_t::vlmax());\n        vfclass_v_f32m8_t res = vfclass_v_f32m8(mask, vfloat32m8_t::vlmax());\n        vuint32m8_t indices = vmseq_v_u32m8(res, 0x00000001u, vuint32m8_t::vlmax());\n        vfloat32m8_t pos_vec = vcompress_v_f32m8(indices, vec, vuint32m8_t::vlmax());\n        vse32_v_f32m8(result.data() + i, pos_vec, vuint32m8_t::vlmax());\n    }\n\n    if (i < n) {\n        vfloat32m1_t vec = vle32_v_f32m1(l.data() + i, n - i);\n        vbool32_t mask = vgts_vf_f32m1(vec, 0.0f, vfloat32m1_t::vlmax());\n        vfclass_v_f32m1_t res = vfclass_v_f32m1(mask, vfloat32m1_t::vlmax());\n        vuint32m1_t indices = vmseq_v_u32m1(res, 0x00000001u, vuint32m1_t::vlmax());\n        vfloat32m1_t pos_vec = vcompress_v_f32m1(indices, vec, vuint32m1_t::vlmax());\n        vse32_v_f32m1(result.data() + i, pos_vec, vuint32m1_t::vlmax());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n    vfloat32m8_t va = vle32_v_f32m8(l.data(), n / 8 * 8);\n    vbool8_t vb = vgts_vf_f32m8_b8(va, 0.0f);\n    vfloat32m8_t vc = vmul_vx_f32m8(va, vb, 1.0f);\n    vse32_v_f32m8(result.data(), vc, n / 8 * 8);\n    for (size_t i = n / 8 * 8; i < n; ++i) {\n        if (l[i] > 0.0f) {\n            result[i] = l[i];\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m8(n);\n    std::vector<float> result;\n    float *in = const_cast<float*>(l.data());\n    float *out = new float[n];\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m8(n - i);\n        vfloat32m8_t vin = vle32_v_f32m8(in + i, vl);\n        vbool32_t mask = vgts_vf_f32m8_b32(vin, 0, vl);\n        vfloat32m8_t vout = vcompress_v_f32m8(mask, vin, vl);\n        vse32_v_f32m8(out + i, vout, vl);\n    }\n\n    for (size_t i = 0; i < n; ++i) {\n        if (out[i] > 0) {\n            result.push_back(out[i]);\n        }\n    }\n\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m8(n);\n    std::vector<float> result;\n    float *in = const_cast<float*>(l.data());\n    float *out = new float[n];\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m8(n - i);\n        vfloat32m8_t vin = vle32_v_f32m8(in + i, vl);\n        vbool32_t mask = vgts_vf_f32m8_b32(vin, 0, vl);\n        vfloat32m8_t vout = vcompress_v_f32m8(mask, vin, vl);\n        vse32_v_f32m8(out + i, vout, vl);\n    }\n\n    for (size_t i = 0; i < n; ++i) {\n        if (out[i] > 0) {\n            result.push_back(out[i]);\n        }\n    }\n\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n    size_t i = 0;\n\n    for (; i + vfloat32m8_t::vlmax() <= n; i += vfloat32m8_t::vlmax()) {\n        vfloat32m8_t vec = vle32_v_f32m8(l.data() + i, vfloat32m8_t::vlmax());\n        vbool32_t mask = vgts_vf_b32(vfloat32m8_t::vlmax(), vec, 0.0f);\n        vfclass_v_f32m8_t res = vfclass_v_f32m8_t(vec, mask);\n        vse32_v_f32m8(result.data() + i, vec, vfloat32m8_t::vlmax());\n    }\n\n    if (i < n) {\n        size_t vl = vsetvl_e32m1(1);\n        vfloat32m1_t vec = vle32_v_f32m1(l.data() + i, vl);\n        vbool1_t mask = vgts_vf_b1(vl, vec, 0.0f);\n        vfclass_v_f32m1_t res = vfclass_v_f32m1_t(vec, mask);\n        vse32_v_f32m1(result.data() + i, vec, vl);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    vint64m8_t v_start = vle64_v_i64m8(&start_values[0], 5);\n    vint64m8_t v_step = vle64_v_i64m8(&step_values[0], 5);\n    vint64m8_t v_limit = vmul_vx_i64m8(limit, 2);\n\n    for (int i = 0; i < 5; i++) {\n        vint64m8_t v_divisor = vadd_vx_i64m8(v_start[i], v_step[i] * (n / 6));\n        vint64m8_t v_remainder = vrem_vx_i64m8(n, v_divisor);\n        vboolm8_t v_is_zero = veq_vx_i64m8(v_remainder, 0);\n        if (vfredor_vs_b_i64m8(v_is_zero)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n\nconst int64_t start_values[] = {5, 7, 11, 13, 17};\nconst int64_t step_values[] = {2, 4, 2, 4, 2};\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    vint64m8_t v_start = vle64_v_i64m8(&start_values[0], 16);\n    vint64m8_t v_step = vle64_v_i64m8(&step_values[0], 16);\n\n    for (int i = 5; i <= limit; i += 6) {\n        vint64m8_t v_i = vmul_vx_i64m8(v_start, i);\n        vint64m8_t v_mod = vrem_vx_i64m8(n, v_i);\n        vboolm8_t v_mask = vmseq_vx_b1m8(v_mod, 0);\n        if (vlmax_v_b1m8(v_mask)) return false;\n    }\n\n    return true;\n}\n\nconst int64_t start_values[] = {5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35};\nconst int64_t step_values[] = {2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4};\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <stdbool.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    vint64m8_t v_start = vle64_v_i64m8(&start_values[0], 16);\n    vint64m8_t v_step = vle64_v_i64m8(&step_values[0], 16);\n\n    for (int i = 5; i <= limit; i += 6) {\n        vint64m8_t v_divisor = vmul_vx_i64m8(v_start, i);\n        vint64m8_t v_remainder = vrem_vv_i64m8(n, v_divisor);\n        vboolm8_t v_is_zero = veq_vx_i64m8(v_remainder, 0);\n        if (vfirst_m_b8(v_is_zero)) return false;\n        v_start = vadd_vx_i64m8(v_start, v_step);\n    }\n\n    return true;\n}\n\nconst int64_t start_values[] = {5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35};\nconst int64_t step_values[] = {2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4};\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    vint64m8_t v_limit = vle64_v_i64m8(&limit, 1);\n    vint64m8_t v_step = vmul_vx_i64m8(v_limit, 2);\n    vint64m8_t v_start = vadd_vx_i64m8(v_step, 5);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        vint64m8_t v_divisor = vle64_v_i64m8(&i, 1);\n        vint64m8_t v_remainder1 = vrem_vx_i64m8(n, v_divisor);\n        vint64m8_t v_remainder2 = vrem_vx_i64m8(n, v_add_vx_i64m8(v_divisor, 2));\n        vboolm8_t v_is_not_prime = vor_vm_vm_b8(vne_vx_i64m8(v_remainder1, 0), vne_vx_i64m8(v_remainder2, 0));\n        if (vlmax_v_b8(v_is_not_prime)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    vint64m8_t v_start = vle64_v_i64m8(&start_values[0], 16);\n    vint64m8_t v_step = vle64_v_i64m8(&step_values[0], 16);\n\n    for (int i = 5; i <= limit; i += 6) {\n        vint64m8_t v_i = vmul_vx_i64m8(v_start, i);\n        vint64m8_t v_i_plus_2 = vmul_vx_i64m8(v_start, i + 2);\n        vint64m8_t v_remainder = vrem_vx_i64m8(n, v_i);\n        vint64m8_t v_remainder_plus_2 = vrem_vx_i64m8(n, v_i_plus_2);\n\n        vboolm8_t v_is_zero = veq_vx_i64m8(v_remainder, 0);\n        vboolm8_t v_is_zero_plus_2 = veq_vx_i64m8(v_remainder_plus_2, 0);\n\n        if (vlmax_v_b8m8(v_is_zero) || vlmax_v_b8m8(v_is_zero_plus_2)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n\nconst int64_t start_values[] = {5, 7};\nconst int64_t step_values[] = {6, 6};\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len == 0) return s;\n\n    size_t vec_len = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vec_len);\n\n    vbool8_t is_alpha = __riscv_vmseq_vx_b8(vs, 'a', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'b', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'c', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'd', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'e', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'f', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'g', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'h', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'i', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'j', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'k', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'l', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'm', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'n', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'o', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'p', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'q', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'r', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 's', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 't', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'u', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'v', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'w', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'x', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'y', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'z', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'A', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'B', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'C', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'D', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'E', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'F', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'G', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'H', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'I', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'J', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'K', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'L', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'M', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'N', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'O', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'P', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'Q', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'R', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'S', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'T', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'U', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'V', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'W', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'X', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'Y', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'Z', vec_len);\n\n    vuint8m1_t vmask = __riscv_vmv_v_x_u8m1(0xFF, vec_len);\n    vuint8m1_t vresult = __riscv_vsub_vvm_u8m1(vmask, vs, vec_len);\n\n    vuint8m1_t vnot_alpha = __riscv_vnot_v_b8(is_alpha, vec_len);\n    vuint8m1_t vfinal = __riscv_vselect_vvm_u8m1(vnot_alpha, vresult, vs, vec_len);\n\n    std::string result(len, '\\0');\n    __riscv_vse8_v_u8m1(result.data(), vfinal, vec_len);\n\n    for (size_t i = 0; i < len; ++i) {\n        if (!isalpha(s[i])) {\n            std::swap(result[i], result[len - i - 1]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len == 0) return s;\n\n    size_t vec_len = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vec_len);\n\n    vbool8_t vm = __riscv_vmseq_vx_b8(vs, 'a', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'b', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'c', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'd', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'e', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'f', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'g', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'h', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'i', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'j', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'k', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'l', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'm', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'n', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'o', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'p', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'q', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'r', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 's', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 't', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'u', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'v', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'w', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'x', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'y', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'z', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'A', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'B', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'C', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'D', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'E', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'F', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'G', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'H', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'I', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'J', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'K', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'L', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'M', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'N', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'O', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'P', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'Q', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'R', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'S', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'T', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'U', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'V', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'W', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'X', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'Y', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'Z', vec_len);\n\n    vuint8m1_t vr = __riscv_vsub_vv_u8m1(vs, vm, vec_len);\n    vr = __riscv_vadd_vv_u8m1(vr, vm, vec_len);\n\n    std::string result;\n    result.resize(len);\n    __riscv_vse8_v_u8m1(result.data(), vr, vec_len);\n\n    if (__builtin_popcount(vm) == 0) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return s;\n\n    size_t vec_len = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vec_len);\n\n    vbool8_t is_alpha = __riscv_vmseq_vx_b8(vs, 'a', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'b', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'c', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'd', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'e', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'f', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'g', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'h', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'i', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'j', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'k', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'l', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'm', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'n', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'o', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'p', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'q', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'r', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 's', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 't', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'u', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'v', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'w', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'x', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'y', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'z', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'A', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'B', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'C', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'D', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'E', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'F', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'G', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'H', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'I', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'J', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'K', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'L', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'M', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'N', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'O', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'P', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'Q', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'R', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'S', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'T', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'U', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'V', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'W', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'X', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'Y', vec_len) |\n                        __riscv_vmseq_vx_b8(vs, 'Z', vec_len);\n\n    vuint8m1_t vmask = __riscv_vmv_v_x_u8m1(0xFF, vec_len);\n    vuint8m1_t vresult = __riscv_vsub_vvm_u8m1(vmask, vs, vec_len);\n\n    vuint8m1_t vnot_alpha = __riscv_vnot_v_b8(is_alpha, vec_len);\n    vresult = __riscv_vselect_vvm_u8m1(vnot_alpha, vs, vresult, vec_len);\n\n    std::string result(len, '\\0');\n    __riscv_vse8_v_u8m1(result.data(), vresult, vec_len);\n\n    for (size_t i = 0; i < len; ++i) {\n        if (!isalpha(s[i])) {\n            std::swap(result[i], result[len - i - 1]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return s;\n\n    size_t vec_len = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vec_len);\n\n    vbool8_t vm = __riscv_vmseq_vx_b8(vs, 'a', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'b', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'c', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'd', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'e', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'f', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'g', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'h', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'i', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'j', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'k', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'l', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'm', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'n', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'o', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'p', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'q', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'r', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 's', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 't', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'u', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'v', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'w', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'x', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'y', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'z', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'A', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'B', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'C', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'D', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'E', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'F', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'G', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'H', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'I', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'J', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'K', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'L', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'M', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'N', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'O', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'P', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'Q', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'R', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'S', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'T', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'U', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'V', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'W', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'X', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'Y', vec_len);\n    vm |= __riscv_vmseq_vx_b8(vs, 'Z', vec_len);\n\n    vuint8m1_t vt = __riscv_vsrai_vx_u8(vs, vm, 32);\n    vt = __riscv_vsub_vv_u8(vt, vs, vec_len);\n\n    char *result = new char[len + 1];\n    __riscv_vse8_v_u8(result, vt, vec_len);\n    result[len] = '\\0';\n\n    std::string res(result);\n    delete[] result;\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len == 0) return s;\n\n    size_t vec_len = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vec_len);\n\n    vbool8_t is_alpha = __riscv_vmseq_vx_b8(vs, 'a', vec_len) | __riscv_vmseq_vx_b8(vs, 'A', vec_len);\n    vint8m1_t sign = __riscv_vsrai_vx_i8m1(__riscv_vsub_vv_i8m1('a' - 'A', __riscv_vand_vvm_i8m1(vs, is_alpha, vec_len)), 5, vec_len);\n    vs = __riscv_vadd_vvm_i8m1(vs, sign, vec_len);\n\n    std::string result;\n    for (size_t i = 0; i < len; i += vec_len) {\n        __riscv_vse8_v_u8m1(result.data() + i, vs, vec_len);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* vec_l = l.data();\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t v_l = vle32_v_i32m1(vec_l + i, vl);\n\n        vbool8_t mask = vmseq_mask_b8(viota_v_u8m1(0, 1, vl), vmul_vx_u8m1(viota_v_u8m1(0, 1, vl), 3, vl));\n        vint32m1_t v_sorted = vlsseg2ouv_v_i32m1(v_l, v_l, mask, vl);\n\n        vse32_v_i32m1(vec_l + i, v_sorted, vl);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* vec_l = l.data();\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t v_l = vle32_v_i32m1(vec_l + i, vl);\n\n        vbool8_t mask = vmseq_mask_b8(viota_v_i8m1(0, 1, vl), vi32_v_i32m1(i / 3, vl));\n        vint32m1_t v_sorted = vlsseg32_v_i32m1(&vec_l[i], &vec_l[i], vl, mask);\n\n        vse32_v_i32m1(vec_l + i, v_sorted, vl);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* vec_l = l.data();\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t v_l = vle32_v_i32m1(vec_l + i, vl);\n\n        vbool8_t mask = vmseq_mask_b8(viota_v_i8m1(0, 1, vl), viwadd_vx_i8m1(viota_v_i8m1(0, 1, vl), 2, vl));\n        vint32m1_t v_sorted = vlsseg2ouv_v_i32m1(v_l, v_l, mask, vl);\n\n        vse32_v_i32m1(vec_l + i, v_sorted, vl);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    vint32m1_t vl = vsetvl_e32m1(n);\n    vint32m1_t v_l = vle32_v_i32m1(l.data(), vl);\n\n    vbool1_t mask = vmseq_mask_b1(vl, v_l, vfmul_vf_s32m1_i32m1(vl, v_l, 3));\n\n    vint32m1_t indices = vle32_v_i32m1((int32_t*)l.data(), vl);\n    vint32m1_t sorted_indices = vsort_v_i32m1(indices, vl);\n\n    vint32m1_t sorted_values = vle32_v_i32m1(l.data(), vl);\n    vsort_v_i32m1(sorted_values, vl);\n\n    vint32m1_t result = vselect_vvm_i32m1(mask, sorted_values, v_l, vl);\n\n    std::vector<int> result_vec(n);\n    vse32_v_i32m1(result_vec.data(), result, vl);\n\n    return result_vec;\n}\n```"}
{"task_id": "SimdBench_78_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    size_t n = l.size();\n    vint32m1_t vl = vsetvl_e32m1(n);\n    vint32m1_t v_l = vle32_v_i32m1(l.data(), vl);\n\n    vbool1_t mask = vmseq_mask_b1(vl, v_l, vfmul_vf_s32m1(vl, v_l, 3));\n\n    vint32m1_t v_thirds = vcompress_vx_i32m1(v_l, mask, 0);\n    vint32m1_t v_sorted_thirds = vqsort_v_i32m1(v_thirds, vl);\n\n    vst32_v_i32m1(l.data() + (n / 3), v_sorted_thirds, vl);\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    float* data = const_cast<float*>(l.data());\n    \n    vfloat32m8_t v_data = vle32_v_f32m8(data, n / 8);\n    vfloat32m8_t v_max = vmax_vv_f32m8(v_data, v_data);\n    \n    for (size_t i = 8; i < n; i += 8) {\n        v_data = vle32_v_f32m8(data + i, 1);\n        v_max = vmax_vv_f32m8(v_max, v_data);\n    }\n    \n    float max_value;\n    vse32_v_f32m1(&max_value, v_max, 1);\n    \n    return max_value;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    float* data = const_cast<float*>(l.data());\n    \n    vfloat32m8_t v_data = vle32_v_f32m8(data, n);\n    vfloat32m8_t v_max = vmax_vv_f32m8(v_data, v_data);\n    \n    while (n > 8) {\n        v_data = vle32_v_f32m8(data + n - 8, 8);\n        v_max = vmax_vv_f32m8(v_max, v_data);\n        n -= 8;\n    }\n    \n    float result[8];\n    vse32_v_f32m8(result, v_max, 8);\n    \n    return *std::max_element(result, result + 8);\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    float* data = const_cast<float*>(l.data());\n    \n    vfloat32m8_t v_data = vle32_v_f32m8(data, n);\n    vfloat32m8_t v_max = vmax_vv_f32m8(v_data, v_data);\n    \n    while (n > 8) {\n        v_data = vle32_v_f32m8(data + n - 8, 8);\n        v_max = vmax_vv_f32m8(v_max, v_data);\n        n -= 8;\n    }\n    \n    float max_val = vfirst_m_b8(v_cmpge_vf_f32m8(v_max, v_max));\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    float* data = const_cast<float*>(l.data());\n    \n    vfloat32m8_t v_data = vle32_v_f32m8(data, n);\n    vfloat32m8_t v_max = vmax_vv_f32m8(v_data, v_data);\n    \n    while (n > 8) {\n        v_data = vle32_v_f32m8(data + n - 8, 8);\n        v_max = vmax_vv_f32m8(v_max, v_data);\n        n -= 8;\n    }\n    \n    float result[8];\n    vse32_v_f32m8(result, v_max, 8);\n    \n    return *std::max_element(result, result + 8);\n}\n```"}
{"task_id": "SimdBench_79_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    float* data = const_cast<float*>(l.data());\n    \n    vfloat32m8_t v_data = vle32_v_f32m8(data, n);\n    vfloat32m8_t v_max = vmax_vv_f32m8(v_data, v_data);\n    \n    while (n > 8) {\n        v_data = vle32_v_f32m8(data + n - 8, 8);\n        v_max = vmax_vv_f32m8(v_max, v_data);\n        n -= 8;\n    }\n    \n    float result[8];\n    vse32_v_f32m8(result, v_max, 8);\n    \n    return *std::max_element(result, result + 8);\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    vint32m1_t v_n = vle32_v_i32m1((const int32_t*)&n, 1);\n    vbool8_t mask = vmseq_vx_i8m1(v_n, 0);\n    vint32m1_t v_count = vfmacc_vvm_i32m1(count, mask, 0);\n    return vfirst_m_i32m1(v_count);\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    vint32m1_t v_n = vle32_v_i32m1((const int32_t*)&n, 1);\n    vbool8_t mask = vmseq_vx_i8m1(v_n, 0);\n    vint32m1_t v_count = vfmacc_vvm_i32m1(count, v_n, mask);\n    return vfirst_m_i32m1(v_count);\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    vint32m1_t v_n = vle32_v_i32m1((const int32_t*)&n, 1);\n    vbool8_t mask = vmseq_vx_i8m1(v_n, 0);\n    vuint32m1_t v_count = vsetvl_e32m1(1);\n    v_count = vsub_vv_u32m1(v_count, v_and_vvmask_u32m1(mask, v_setzero_u32m1()));\n    return vfirst_m_b8(mask) ? 0 : vle32_v_i32m1((const int32_t*)&count, 1)[0];\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    vint32m1_t v_n = vle32_v_i32m1((const int32_t*)&n, 1);\n    vbool8_t mask = vmseq_vx_i8m1(v_n, 0);\n    vint32m1_t v_count = vzext_vf_f_i32m1(vmfcvt_x_f_f32(vfredosum_vs_i32m1_f32m1(vmv_v_x_f32m1(0.0f), vmseq_vx_i8m1(mask, 1), vfloat32m1_men(v_n, mask)), v_n));\n    return vfirst_m_i32m1(vcount);\n}\n```"}
{"task_id": "SimdBench_80_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    vint32m1_t v_n = vle32_v_i32m1((const int32_t*)&n, 1);\n    vbool8_t mask = vmseq_vx_i8m1(v_n, 0);\n    vint32m1_t v_count = vsetvl_e32m1(1);\n    vcount_v_i32m1(&count, v_count, mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return l;\n\n    // Extract even-indexed elements\n    vfloat32m8_t evens = vle32_v_f32m8(l.data(), n / 2);\n    vbool8_t mask = vmseq_mask_b8(vsetvl_e8m1(1), vuint8m1_t(), vuint8m1_t());\n    vfloat32m8_t sorted_evens = vsort_v_f32m8(evens, mask);\n\n    // Merge sorted even-indexed elements back into the original vector\n    vst32_v_f32m8(sorted_evens, l.data(), n / 2);\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0 || n == 1) return l;\n\n    // Extract even-indexed elements\n    vfloat32m8_t evens = vle32_v_f32m8(l.data(), n / 2);\n    vbool8_t mask = vmseq_mask_b8(vsetvl_e8m1(1), vsew_e32m8(), vle8_v_u8m1(\"0x0102030405060708\"));\n    evens = vselect_vvm_f32m8(evens, vle32_v_f32m8(l.data() + n / 2, n / 2), mask);\n\n    // Sort even-indexed elements\n    vuint32m1_t indices = vle32_v_u32m1(\"0x00010203\");\n    vuint32m1_t sorted_indices = vmsort_bu_w(indices, vreinterpret_v_f32m1_f32m8(evens));\n\n    // Reconstruct the vector with sorted even-indexed elements\n    std::vector<float> result(n);\n    for (size_t i = 0; i < n / 2; ++i) {\n        result[sorted_indices[i] * 2] = vget_v_f32m8(evens, i);\n    }\n\n    // Copy odd-indexed elements as they are\n    for (size_t i = 0; i < n / 2; ++i) {\n        result[sorted_indices[i] * 2 + 1] = l[n / 2 + i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return l;\n\n    // Extract even-indexed elements\n    vfloat32m8_t evens = vle32_v_f32m8(l.data(), n / 2);\n    vbool8_t mask = vmseq_mask_b8(vsetvl_e8m1(1), vle8_v_u8m1((uint8_t*)\"\\x0\\x2\\x4\\x6\\x8\\xa\\xc\\xe\", 8));\n\n    // Sort even-indexed elements\n    vfloat32m8_t sorted_evens = vqsort_v_f32m8(evens, vsetvl_e32m8(n / 2));\n\n    // Merge sorted even-indexed elements back into original vector\n    vst32_v_f32m8(sorted_evens, l.data() + 1, n / 2);\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0 || n == 1) return l;\n\n    // Extract even-indexed elements\n    vfloat32m8_t evens = vle32_v_f32m8(l.data(), n / 2);\n    vbool8_t mask = vmseq_mask_b8(vsetvl_e8m1(1), vle8_v_u8m1((uint8_t*)\"\\x0\\x2\\x4\\x6\\x8\\xa\\xc\\xe\", 8));\n\n    // Sort even-indexed elements\n    vfloat32m8_t sorted_evens = vqsort_v_f32m8(evens, vsetvl_e32m8(n / 2));\n\n    // Merge sorted even-indexed elements back into original vector\n    vst32_v_f32m8(sorted_evens, l.data() + 1, n / 2);\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0 || n == 1) return l;\n\n    // Extract even-indexed elements\n    vfloat32m8_t evens = vle32_v_f32m8(l.data(), n / 2);\n    vbool8_t mask = vmseq_mask_b8(vsetvl_e8m1(1), vsext_wx_i8m1(0));\n\n    // Sort the extracted elements\n    vfloat32m8_t sorted_evens = vqsort_v_f32m8(evens, n / 2);\n\n    // Merge sorted even-indexed elements back into the original vector\n    vst32_v_f32m8(sorted_evens, l.data() + n - n / 2, n / 2);\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <iostream>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    vbool8_t mask = vmseq_vx_i32m8(vec_l, 0);\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        vint32m8_t vec_a = vele32_v_i32m8(vec_l, i);\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            vint32m8_t vec_b = vele32_v_i32m8(vec_l, j);\n            vint32m8_t vec_c = vadd_vv_i32m8(vec_a, vec_b);\n            vint32m8_t vec_d = vsub_vx_i32m8(vec_c, l[j + 1]);\n            vbool8_t result = vmseq_vv_i32m8(vec_d, 0);\n            if (vmfirst_m_b8(mask & result)) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <iostream>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    bool result = false;\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        vint32m8_t vec_a = vle32_v_i32m8(&l[i], 1);\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            vint32m8_t vec_b = vle32_v_i32m8(&l[j], 1);\n            vint32m8_t vec_c = vsub_vv_i32m8(vec_a, vec_b);\n            vint32m8_t vec_d = vadd_vx_i32m8(vec_c, l[j + 1]);\n            vbool8_t mask = vmseq_vx_i32m8_m(vand_vv_i32m8(vec_d, vec_d), 0, n - 2 - j);\n            if (vmfirst_x_mask(mask)) {\n                result = true;\n                break;\n            }\n        }\n        if (result) break;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    bool result = false;\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        vint32m8_t vec_a = vle32_v_i32m8(l.data() + i, n - i);\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            vint32m8_t vec_b = vle32_v_i32m8(l.data() + j, n - j);\n            vint32m8_t vec_c = vadd_vv_i32m8(vec_a, vec_b);\n            vint32m8_t vec_res = vsub_vv_i32m8(vec_c, vec_l);\n            if (vmseq_vx_i32m8_m(vand_vv_v_i32m8(vmseq_vx_i32m8(vec_res, 0), vmseq_vx_i32m8(vec_res, 0)), 1)) {\n                result = true;\n                break;\n            }\n        }\n        if (result) break;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    bool result = false;\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        vint32m8_t vec_a = vle32_v_i32m8(l.data() + i, n - i);\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            vint32m8_t vec_b = vle32_v_i32m8(l.data() + j, n - j);\n            vint32m8_t vec_c = vadd_vv_i32m8(vec_a, vec_b);\n            vint32m8_t vec_res = vsub_vv_i32m8(vec_c, vec_l);\n            if (vmseq_vx_i32m8_m(vand_vv_v_i8m8(vmseq_vx_i32m8(vec_res, 0), vmseq_vx_i32m8(vec_res, 0)), 0xFF)) {\n                result = true;\n                break;\n            }\n        }\n        if (result) break;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    vbool8_t mask = vmseq_vx_i32m8(vec_l, 0);\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        vint32m8_t vec_a = vele32_v_i32m8(vec_l, i);\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            vint32m8_t vec_b = vele32_v_i32m8(vec_l, j);\n            vint32m8_t vec_c = vsub_vv_i32m8(vneg_v_i32m8(vec_a), vec_b);\n            vbool8_t found = vmfirst_m_b8(mask, vec_c == vec_l);\n            if (found) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    vint32m8_t vec_one = vmv_v_x_i32m8(1);\n    vint32m8_t vec_result = vadd_vv_i32m8(vec_l, vec_one);\n    std::vector<int> result(n);\n    vse32_v_i32m8(result.data(), vec_result, n);\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* in_ptr = l.data();\n    int32_t* out_ptr = new int32_t[n];\n    \n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t vin = vle32_v_i32m1(in_ptr + i, vl);\n        vint32m1_t vout = vadd_vx_i32m1(vin, 1, vl);\n        vse32_v_i32m1(out_ptr + i, vout, vl);\n    }\n    \n    std::vector<int> result(out_ptr, out_ptr + n);\n    delete[] out_ptr;\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* input = l.data();\n    int32_t* output = new int32_t[n];\n    \n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t vec_input = vle32_v_i32m1(input + i, vl);\n        vint32m1_t vec_output = vadd_vx_i32m1(vec_input, 1, vl);\n        vse32_v_i32m1(output + i, vec_output, vl);\n    }\n    \n    std::vector<int> result(output, output + n);\n    delete[] output;\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m8(n);\n    int32_t* in_ptr = l.data();\n    int32_t* out_ptr = new int32_t[n];\n    \n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m8(n - i);\n        vint32m8_t vec_in = vle32_v_i32m8(in_ptr + i, vl);\n        vint32m8_t vec_out = vadd_vx_i32m8(vec_in, 1, vl);\n        vse32_v_i32m8(out_ptr + i, vec_out, vl);\n    }\n    \n    std::vector<int> result(out_ptr, out_ptr + n);\n    delete[] out_ptr;\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    int32_t* in_ptr = l.data();\n    int32_t* out_ptr = new int32_t[n];\n    \n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t vin = vle32_v_i32m1(in_ptr + i, vl);\n        vint32m1_t vout = vadd_vx_i32m1(vin, 1, vl);\n        vse32_v_i32m1(out_ptr + i, vout, vl);\n    }\n    \n    std::vector<int> result(out_ptr, out_ptr + n);\n    delete[] out_ptr;\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    vint32m8_t vec = vle32_v_i32m8(l.data(), n);\n    vbool8_t mask = vmul_vv_m_b8(vec, vec, n);\n\n    return vredany_vs_u_b8(mask, n);\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <iostream>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    vbool8_t mask = vmseq_vx_i32m8(vec_l, 0);\n\n    for (size_t i = 0; i < n; ++i) {\n        int target = -l[i];\n        vint32m8_t vec_target = vle32_v_i32m8(&target, 1);\n        vbool8_t found = vmseq_vv_i32m8(vec_l, vec_target);\n        found = vmor_vv_b8(found, mask);\n        if (vmfirst_m_b8(found)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <iostream>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    vint32m8_t vec = vle32_v_i32m8(l.data(), n);\n    vint32m8_t neg_vec = vneg_v_i32m8(vec);\n\n    bool result = false;\n    for (size_t i = 0; i < n / 8; ++i) {\n        vbool8_t mask = vmseq_vv_i32m8(neg_vec, vec, vlmax_e32m8(n));\n        if (vmfirst_m_b8(mask)) {\n            result = true;\n            break;\n        }\n        vec = vsll_vx_i32m8(vec, 1, vlmax_e32m8(n));\n        neg_vec = vsll_vx_i32m8(neg_vec, 1, vlmax_e32m8(n));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    vint32m8_t vec = vle32_v_i32m8(l.data(), n);\n    vint32m8_t neg_vec = vneg_v_i32m8(vec);\n\n    bool result = false;\n    for (size_t i = 0; i + 7 < n; i += 8) {\n        vint32m8_t mask = vmseq_vv_i32m8(neg_vec, vec, 8);\n        if (vmfirst_m_b8(mask)) {\n            result = true;\n            break;\n        }\n        vec = vlse32_v_i32m8(l.data() + i + 8, 8);\n        neg_vec = vneg_v_i32m8(vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    size_t n = l.size();\n    if (n < 2) return false;\n\n    vint32m8_t vec_l = vle32_v_i32m8(l.data(), n);\n    vbool8_t mask = vmseq_vv_i32m8(vec_l, vzero_v_i32m8(), n);\n\n    for (size_t i = 0; i < n; ++i) {\n        int target = -l[i];\n        vint32m8_t vec_target = vle32_v_i32m8(&target, 1);\n        vbool8_t found = vmseq_vv_i32m8(vec_l, vec_target, n);\n        if (vmfirst_m_b8(found)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result = \"\";\n    vint64m1_t vx = {x};\n    vuint8m1_t vbase = {static_cast<uint8_t>(base)};\n    vbool1_t vb = true;\n\n    while (vx[0] != 0) {\n        vint64m1_t vremainder = vx % vbase;\n        vx /= vbase;\n        vuint8m1_t vchar = vremainder + '0';\n        result.insert(result.begin(), vchar[0]);\n    }\n\n    return result.empty() ? \"0\" : result;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <iostream>\n#include <vector>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::vector<int8_t> digits;\n    while (x > 0) {\n        digits.push_back(x % base);\n        x /= base;\n    }\n    std::reverse(digits.begin(), digits.end());\n    std::string result;\n    for (int8_t digit : digits) {\n        result += std::to_string(digit);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <iostream>\n#include <vector>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::vector<int8_t> digits;\n    while (x > 0) {\n        digits.push_back(x % base);\n        x /= base;\n    }\n    std::reverse(digits.begin(), digits.end());\n    std::string result = \"\";\n    for (int8_t digit : digits) {\n        result += std::to_string(digit);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <iostream>\n#include <vector>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::vector<int8_t> digits;\n    while (x > 0) {\n        digits.push_back(x % base);\n        x /= base;\n    }\n    std::reverse(digits.begin(), digits.end());\n    std::string result = \"\";\n    for (int8_t digit : digits) {\n        result += std::to_string(digit);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result = \"\";\n    if (x == 0) return \"0\";\n    while (x != 0) {\n        result = std::to_string(x % base) + result;\n        x /= base;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    size_t half_len = len / 2;\n    vuint8m1_t vec_text = vle8_v_u8m1(text.data(), half_len);\n    vuint8m1_t vec_reversed = vlre8_v_u8m1(text.data() + len - half_len - 1, half_len);\n\n    bool result = true;\n    for (size_t i = 0; i < half_len; ++i) {\n        if (vec_text[i] != vec_reversed[half_len - 1 - i]) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    size_t half_len = len / 2;\n    vuint8m1_t vec1, vec2;\n    int i = 0;\n\n    while (i + 16 <= half_len) {\n        vec1 = vle8_v_u8m1(text.data() + i, 16);\n        vec2 = vle8_v_u8m1(text.data() + len - i - 16, 16);\n        if (!veq_vv_b_u8m1(vec1, vec2)) return false;\n        i += 16;\n    }\n\n    while (i + 4 <= half_len) {\n        vec1 = vle8_v_u8m1(text.data() + i, 4);\n        vec2 = vle8_v_u8m1(text.data() + len - i - 4, 4);\n        if (!veq_vv_b_u8m1(vec1, vec2)) return false;\n        i += 4;\n    }\n\n    while (i < half_len) {\n        if (text[i] != text[len - i - 1]) return false;\n        i++;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    size_t vec_len = vsetvl_e8m1(len);\n    vuint8m1_t vec_text = vlse8_v_u8m1(text.data(), vec_len);\n\n    for (size_t i = 0; i < len / 2; ++i) {\n        vuint8m1_t vec_start = vle8_v_u8m1(text.data() + i, vec_len);\n        vuint8m1_t vec_end = vle8_v_u8m1(text.data() + len - i - 1, vec_len);\n\n        vboolm1_t mask = vmseq_vv_b8(vec_start, vec_end);\n        if (!vmfirst_m_b1(mask)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cstring>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    size_t vec_len = vsetvl_e8m1(len);\n    vuint8m1_t vec_text = vlse8_v_u8m1(text.data(), vec_len);\n\n    for (size_t i = 0; i < len / 2; ++i) {\n        vuint8m1_t vec_start = vle8_v_u8m1(text.data() + i, vec_len);\n        vuint8m1_t vec_end = vle8_v_u8m1(text.data() + len - i - 1, vec_len);\n\n        vboolm1_t mask = vmseq_vv_b8(vec_start, vec_end);\n        if (!vmfirst_m_b1(mask)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cstring>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    size_t vec_len = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t v_text = __riscv_vle8_v_u8m1(text.data(), vec_len);\n\n    for (size_t i = 0; i < len / 2; ++i) {\n        vuint8m1_t v_first = __riscv_vsra_vx_u8m1(v_text, i * 8, 0);\n        vuint8m1_t v_last = __riscv_vsra_vx_u8m1(v_text, (len - i - 1) * 8, 0);\n        vuint8m1_t v_eq = __riscv_veq_vv_u8m1(v_first, v_last);\n        if (__riscv_vmfirst_m_b8(v_eq) != -1) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cstring>\n\nstd::string decode_shift_simd(const std::string& s) {\n    size_t len = s.size();\n    if (len == 0) return s;\n\n    size_t vlmax = vsetvl_e8m1(len);\n    size_t vl;\n    char* out = new char[len + 1];\n    const char* in = s.c_str();\n\n    while (len > 0) {\n        vl = vsetvl_e8m1(len);\n        vuint8m1_t vin = vsload_v_u8m1(in, vl, 0);\n        vin = vsub_vx_u8m1(vin, 5);\n        vin = vmul_vx_u8m1(vin, 26);\n        vin = vrem_vx_u8m1(vin, 26);\n        vin = vadd_vx_u8m1(vin, 'a');\n        vsstore_v_u8m1(out, vin, vl, 0);\n\n        in += vl;\n        out += vl;\n        len -= vl;\n    }\n\n    out[len] = '\\0';\n    std::string result(out);\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cstring>\n\nstd::string decode_shift_simd(const std::string& s) {\n    size_t len = s.length();\n    size_t vec_len = vsetvl_e8m1(len);\n    char* out = new char[len + 1];\n    const char* in = s.c_str();\n\n    for (size_t i = 0; i < len; i += vec_len) {\n        vfloat32m1_t vin = vlse32_v_f32m1((const float32_t*)(in + i), vec_len, 4);\n        vfloat32m1_t vout = vfsub_vf_f32m1(vin, 5.0f, vec_len);\n        vfloat32m1_t vmod = vfrem_vf_f32m1(vout, 26.0f, vec_len);\n        vfloat32m1_t vadd = vfadd_vf_f32m1(vmod, 'a', vec_len);\n        vsse32_v_f32m1((float32_t*)(out + i), vec_len, 4, vadd);\n    }\n\n    out[len] = '\\0';\n    std::string result(out);\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cstring>\n\nstd::string decode_shift_simd(const std::string& s) {\n    size_t len = s.length();\n    size_t vlmax = vsetvl_e8m1(len);\n    char* out = new char[len + 1];\n    const char* in = s.c_str();\n\n    for (size_t i = 0; i < len; i += vlmax) {\n        vuint8m1_t vin = vle8_v_u8m1(in + i, vlmax);\n        vuint8m1_t vout = vmul_vx_u8m1(vin, 5, vlmax);\n        vuint8m1_t vshift = vsub_vx_u8m1(vout, 'a', vlmax);\n        vuint8m1_t vmod = vrem_vx_u8m1(vshift, 26, vlmax);\n        vuint8m1_t vadd = vadd_vx_u8m1(vmod, 'a', vlmax);\n        vse8_v_u8m1(out + i, vadd, vlmax);\n    }\n\n    out[len] = '\\0';\n    std::string result(out);\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cstring>\n\nstd::string decode_shift_simd(const std::string& s) {\n    size_t len = s.size();\n    size_t vec_len = vsetvl_e8m1(len);\n    char* out = new char[len + 1];\n    const char* in = s.c_str();\n\n    for (size_t i = 0; i < len; i += vec_len) {\n        vfloat32m1_t vin = vlse32_v_f32m1((const float32_t*)(in + i), vec_len, 4);\n        vfloat32m1_t vout = vfsub_vf_f32m1(vin, 5.0f, vec_len);\n        vfloat32m1_t vmod = vfrem_vf_f32m1(vout, 26.0f, vec_len);\n        vfloat32m1_t vadd = vfadd_vf_f32m1(vmod, 'a', vec_len);\n        vsse32_v_f32m1((float32_t*)(out + i), vec_len, 4, vadd);\n    }\n\n    out[len] = '\\0';\n    std::string result(out);\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_87_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cstring>\n\nstd::string decode_shift_simd(const std::string& s) {\n    size_t len = s.length();\n    size_t vec_len = vsetvl_e8m1(len);\n    size_t i = 0;\n\n    std::string out(len, '\\0');\n\n    while (i < len) {\n        vuint8m1_t vec_in = vlseg8_v_u8m1(s.data() + i, vec_len);\n        vuint8m1_t vec_out = vwredsum_vs_u8m1_i32m1_m(vec_in, 0, vmul_vx_u8m1(vsub_vx_u8m1(vec_in, 'a' - 5), 26), vle8_v_u8m1(\"abcdefghijklmnopqrstuvwxyz\", vec_len));\n        vsseg8_v_u8m1(out.data() + i, vec_out, vec_len);\n        i += vec_len;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    vint32m1_t vl = vle32_v_i32m1(l.data(), vlmax);\n    vint32m1_t vt = vmul_vx_i32m1(vl, vle32_v_i32m1(&t, 1), vlmax);\n    bool result = vredand_vs_u_b_i32m1(true, vl, vt, vlmax);\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m8(n);\n    vint32m8_t vl = vle32_v_i32m8(l.data(), vlmax);\n    vint32m8_t vt = vmul_vx_i32m8(vl, vle32_v_i32m8(&t, 1), vlmax);\n    bool result = vredand_vs_u_b_i32m8(true, vl, vt, vlmax);\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m8(n);\n    vint32m8_t vl = vle32_v_i32m8(l.data(), vlmax);\n    vint32m8_t vt = vmul_vx_i32m8(vl, vle32_v_i32m8(&t, 1), vlmax);\n    bool result = vredand_vs_u_b_i32m8(true, vl, vt, vlmax);\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    vint32m1_t vl = vle32_v_i32m1(l.data(), vlmax);\n    vint32m1_t vt = vmul_vx_i32m1(vl, vle32_v_i32m1(&t, 1), vlmax);\n    bool result = vall_gt_vx_i32m1(vt, vl, vlmax);\n    return !result;\n}\n```"}
{"task_id": "SimdBench_88_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    size_t n = l.size();\n    size_t vlmax = vsetvl_e32m8(n);\n    vint32m8_t vl = vle32_v_i32m8(l.data(), vlmax);\n    vint32m8_t vt = vmul_vx_i32m8(vl, vle32_v_i32m8(&t, 1), vlmax);\n    bool result = vall_gt_vx_i32m8(vt, vle32_v_i32m8(&t, 1), vlmax) == 0;\n    return result;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cstring>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    vuint8m1_t vec_brackets = vle8_v_u8m1(brackets.data(), len);\n    vint32m1_t count = vmul_vx_i32m1(vreinterpret_v_f32m1(vec_brackets), 1);\n\n    int sum = vredsum_vs_i32m1_i32m1_m(0, count, nullptr, vsew_e32m1());\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cstring>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.length();\n    if (len % 2 != 0) return false;\n\n    vuint8m1_t vec_brackets = vle8_v_u8m1(brackets.data(), len);\n    vint32m1_t count = vmul_vx_i32m1(vreinterpret_v_f32m1(vec_brackets), 1);\n\n    int sum = vredsum_vs_i32m1_i32m1_m(0, count, nullptr, vsew_e32m1());\n\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cstring>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    vuint8m1_t vec_brackets = vle8_v_u8m1(brackets.data(), len);\n    vint32m1_t count = vmul_vx_i32m1(vreinterpret_v_f32m1_u8m1(vec_brackets), 1);\n\n    int sum = vredsum_vs_i32m1_i32m1(0, count, vsetvl_e32m1(len / 4));\n\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cstring>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    vuint8m1_t vec_brackets = vle8_v_u8m1(brackets.data(), len);\n    vint32m1_t count = vmul_vx_i32m1(vreinterpret_v_f32m1(vec_brackets), 1);\n\n    int sum = vredsum_vs_i32m1_i32m1_m(0, count, nullptr, vsew_e32m1());\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_89_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cstring>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.length();\n    if (len % 2 != 0) return false;\n\n    vint8m1_t stack = vle8_v_i8m1((const int8_t*)brackets.data(), len);\n    vint8m1_t mask = vmseq_vx_i8m1(stack, (int8_t)'<', vl);\n\n    size_t count = vfredosum_vs_i32m1_i32m1(0, mask, vl);\n\n    return count == len / 2;\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.length();\n    if (len % 2 != 0) return false;\n\n    vint8m1_t vec_open = vle8_v_i8m1((const int8_t*)brackets.data(), len / 8);\n    vint8m1_t vec_close = vle8_v_i8m1((const int8_t*)brackets.data() + len / 4, len / 8);\n\n    vint8m1_t mask_open = vmseq_vx_i8m1(vec_open, '(', vl);\n    vint8m1_t mask_close = vmseq_vx_i8m1(vec_close, ')', vl);\n\n    vint32m1_t count_open = vredsum_vs_i8m1_i32m1(vpopcnt_v_i8m1(mask_open, vl), 0, vl);\n    vint32m1_t count_close = vredsum_vs_i8m1_i32m1(vpopcnt_v_i8m1(mask_close, vl), 0, vl);\n\n    return vle32_v_i32m1(&count_open, 1)[0] == vle32_v_i32m1(&count_close, 1)[0];\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.length();\n    if (len % 2 != 0) return false;\n\n    vint8m1_t vec_open = vle8_v_i8m1((const int8_t*)brackets.data(), len / 8);\n    vint8m1_t vec_close = vle8_v_i8m1((const int8_t*)(brackets.data() + len / 4), len / 8);\n\n    vint8m1_t mask_open = vmseq_vx_i8m1(vec_open, '(', vl);\n    vint8m1_t mask_close = vmseq_vx_i8m1(vec_close, ')', vl);\n\n    vint32m1_t count_open = vredsum_vs_i8m1_i32m1(0, mask_open, vl);\n    vint32m1_t count_close = vredsum_vs_i8m1_i32m1(0, mask_close, vl);\n\n    return vle32_v_i32m1(count_open, vl)[0] == vle32_v_i32m1(count_close, vl)[0];\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.length();\n    if (len % 2 != 0) return false;\n\n    vuint8m1_t vec_open = vle8_v_u8m1(brackets.data(), len / 2);\n    vuint8m1_t vec_close = vle8_v_u8m1(brackets.data() + len / 2, len / 2);\n\n    vint32m1_t diff = vsub_vv_i32m1(vreinterpret_v_f32m1(vec_open),vreinterpret_v_f32m1(vec_close));\n    vint32m1_t sum = vredsum_vs_i32m1_i32m1_i32m1_m(diff, 0, vsetvl_e32m1(len / 4));\n\n    return vfirst_m_b32(sum) == -1;\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.length();\n    if (len % 2 != 0) return false;\n\n    vint8m1_t vec_open = vle8_v_i8m1((const int8_t*)brackets.data(), len / 8);\n    vint8m1_t vec_close = vle8_v_i8m1((const int8_t*)brackets.data() + len / 4, len / 8);\n\n    vint8m1_t mask_open = vmseq_vx_i8m1(vec_open, '(', vl);\n    vint8m1_t mask_close = vmseq_vx_i8m1(vec_close, ')', vl);\n\n    vint32m1_t sum_open = vwredsum_vs_i8m1_i32m1(vmv_v_x_i32m1(0, vl), mask_open, vl);\n    vint32m1_t sum_close = vwredsum_vs_i8m1_i32m1(vmv_v_x_i32m1(0, vl), mask_close, vl);\n\n    return vsadd_vx_i32m1(sum_open, -sum_close, vl) == 0;\n}\n```"}
{"task_id": "SimdBench_90_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <vector>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.size();\n    if (len % 2 != 0) return false;\n\n    vint8m1_t stack = vle8_v_i8m1(brackets.data(), len);\n    vint8m1_t top = vmul_vx_i8m1(stack, -1);\n    vboolm1_t match = veq_vv_b8m1(top, stack);\n\n    return vredand_vs_u8m1(match, 1, vtrue_v_u8m1());\n}\n```"}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    size_t vlmax = vsetvl_e32m8(1);\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < n - 1;) {\n        size_t vl = std::min(vlmax, n - 1 - i);\n        vfloat32m8_t vx = vle32_v_f32m8(xs.data() + i, vl);\n        vfloat32m8_t vy = vfsub_vf_f32m8(vx, vfmul_vf_f32m8(vx, vffloat32m8(vl)), vl);\n        vse32_v_f32m8(result.data() + i, vy, vl);\n        i += vl;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    size_t vlmax = vsetvl_e32m8(1);\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < n - 1; i += vlmax) {\n        size_t vl = vsetvl_e32m8((n - 1 - i));\n        vfloat32m8_t vx = vle32_v_f32m8(xs.data() + i, vl);\n        vfloat32m8_t vy = vfsub_vf_f32m8(vx, vfmul_vf_f32m8(vx, vffloat32m8(1)), vl);\n        vse32_v_f32m8(result.data() + i, vy, vl);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    size_t vlmax = vsetvl_e32m8(1);\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < n - 1;) {\n        size_t vl = vsetvl_e32m8((n - 1 - i) / vlmax);\n        vfloat32m8_t vx = vle32_v_f32m8(xs.data() + i, vl);\n        vfloat32m8_t vy = vfsub_vf_f32m8(vx, vfmul_vf_f32m8(vx, vffloat32m8(vl)), vl);\n        vse32_v_f32m8(result.data() + i, vy, vl);\n        i += vl;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    size_t vlmax = vsetvl_e32m8(1);\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < n - 1; i += vlmax) {\n        size_t vl = vsetvl_e32m8((n - 1 - i));\n        vfloat32m8_t vx = vle32_v_f32m8(xs.data() + i, vl);\n        vfloat32m8_t vy = vfsub_vf_f32m8(vx, vfmul_vf_f32m8(vx, vffloat32m8(1)), vl);\n        vse32_v_f32m8(result.data() + i, vy, vl);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    size_t vlmax = vsetvl_e32m8(1);\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < n - 1;) {\n        size_t vl = std::min(vlmax, n - 1 - i);\n        vfloat32m8_t vx = vle32_v_f32m8(xs.data() + i, vl);\n        vfloat32m8_t vy = vfsub_vf_f32m8(vx, vfmul_vf_f32m8(vx, vffloat32m8(vl)), vl);\n        vse32_v_f32m8(result.data() + i, vy, vl);\n        i += vl;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    size_t len = s.length();\n    vuint8m1_t vs = vle8_v_u8m1(s.data(), len);\n    vbool8_t vm = vmseq_vx_b8(vs, 'a') | vmseq_vx_b8(vs, 'e') | vmseq_vx_b8(vs, 'i') | vmseq_vx_b8(vs, 'o') | vmseq_vx_b8(vs, 'u');\n    if (len > 0 && s[len - 1] == 'y') {\n        vm = vmor_vm_b8(vm, vmseq_vx_b8(vs, 'y'));\n    }\n    count += vfredosum_vs_i32m1_i32(vmv_v_x_i32m1(0), vm, vs);\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    size_t len = s.length();\n    vuint8m1_t vs = vle8_v_u8m1(s.data(), len);\n    vbool8_t vm = vmsb_b_u8(vs, 'a') | vmsb_b_u8(vs, 'e') | vmsb_b_u8(vs, 'i') | vmsb_b_u8(vs, 'o') | vmsb_b_u8(vs, 'u');\n    if (len > 0 && s[len - 1] == 'y') {\n        vm = vm | vslide1up_mask_b_u8(vm, 1);\n    }\n    count += vpopcnt_m_i32(VM, vm);\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    size_t len = s.length();\n    vuint8m1_t vs = vle8_v_u8m1(s.data(), len);\n    vbool8_t vm = vmseq_vx_b8(vs, 'a') | vmseq_vx_b8(vs, 'e') | vmseq_vx_b8(vs, 'i') | vmseq_vx_b8(vs, 'o') | vmseq_vx_b8(vs, 'u');\n    if (len > 0 && s[len - 1] == 'y') {\n        vm = vmor_vx_b8(vm, vmseq_vx_b8(vs, 'y'));\n    }\n    count += vfredosum_vs_i32m1_i32(vmv_v_x_i32m1(0), vm, vs);\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    size_t len = s.length();\n    vuint8m1_t vs = vle8_v_u8m1(s.data(), len);\n    vbool8_t vm = vmsb_b_u8(vs, 'a') | vmsb_b_u8(vs, 'e') | vmsb_b_u8(vs, 'i') | vmsb_b_u8(vs, 'o') | vmsb_b_u8(vs, 'u');\n    if (len > 0 && s[len - 1] == 'y') {\n        vm = vm | vslide1down_mask_b_u8(vm, true);\n    }\n    count += vpopcnt_m_i32(VM, vm);\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    size_t len = s.length();\n    vuint8m1_t vs = vle8_v_u8m1(s.data(), len);\n    vbool8_t vm = vmseq_vx_b8(vs, 'a') | vmseq_vx_b8(vs, 'e') | vmseq_vx_b8(vs, 'i') | vmseq_vx_b8(vs, 'o') | vmseq_vx_b8(vs, 'u');\n    if (len > 0 && s[len - 1] == 'y') {\n        vm = vmor_vx_b8(vm, vmseq_vx_b8(vs, 'y'));\n    }\n    count += vfredosum_vs_i32m1_i32(vmv_v_x_i32m1(0), vm, vs);\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    size_t len = s.size();\n    size_t i = 0;\n\n    vuint8m1_t vstr;\n    vboolm1_t vmask;\n    uint8_t mask[16];\n\n    while (i < len) {\n        size_t chunk_size = std::min(len - i, 16U);\n        vstr = vle8_v_u8m1(&s[i], chunk_size);\n\n        // Create a mask for uppercase letters\n        for (size_t j = 0; j < chunk_size; ++j) {\n            mask[j] = (vstr.v[j] >= 'A' && vstr.v[j] <= 'Z');\n        }\n        vmask = vlmask_b1(16, mask);\n\n        // Sum ASCII values of uppercase letters\n        vuint8m1_t vupper = vand_vx_u8m1(vstr, 0x5F); // Convert to lowercase\n        vuint16m1_t vsum = vwredsum_vs_u8_u16m1(0, vupper, vmask, 0);\n\n        sum += vfirst_m_i16m1(vsum);\n\n        i += chunk_size;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    size_t len = s.size();\n    size_t i = 0;\n\n    vuint8m1_t vstr;\n    vboolm1_t vmask;\n    uint8_t* ptr = const_cast<uint8_t*>(s.c_str());\n\n    while (i + 16 <= len) {\n        vstr = vl16_v_u8m1(ptr + i, 16);\n        vmask = vmseq_vx_u8m1_b1(vstr, 'A', 16);\n        vmask &= vmslt_vx_u8m1_b1(vstr, 'Z' + 1, 16);\n        vuint8m1_t vupper = vand_vv_u8m1(vstr, vmask, 16);\n        sum += vredsum_vs_u8_i32(vupper, 0, nullptr);\n        i += 16;\n    }\n\n    while (i < len) {\n        if (ptr[i] >= 'A' && ptr[i] <= 'Z') {\n            sum += ptr[i];\n        }\n        i++;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    size_t len = s.size();\n    size_t i = 0;\n\n    vuint8m1_t vec_s;\n    vboolm1_t mask_upper;\n    uint8_t result[16];\n\n    while (i + 16 <= len) {\n        vec_s = vle8_v_u8m1(&s[i], 16);\n        mask_upper = vmseq_vx_u8m1_b8(vec_s, 'A', 16) & vmslt_vx_u8m1_b8(vec_s, 'Z', 16);\n        vse8_v_u8m1(result, vadd_vvm_u8m1(vand_vvm_u8m1(vec_s, mask_upper), 0, 16));\n        sum += vredsum_vs_u8_i32(result, 0, 16);\n        i += 16;\n    }\n\n    while (i < len) {\n        if (isupper(s[i])) {\n            sum += static_cast<int>(s[i]);\n        }\n        i++;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    vuint8m1_t vstr;\n    vboolm1_t vmask;\n    uint8_t result[VL];\n\n    while (i < len) {\n        size_t chunk_size = std::min(len - i, VL);\n        vstr = vl_vle8_v_u8m1(&s[i], &chunk_size);\n\n        vmask = vl_vmseq_vx_u8m1_b8m1(vstr, 'A');\n        vmask = vl_vmand_vv_u8m1_b8m1(vmask, vl_vmseq_vx_u8m1_b8m1(vstr, 'Z'));\n\n        vuint8m1_t vupper = vl_vand_vv_u8m1_u8m1(vstr, vmask);\n        vuint32m1_t vsum = vl_vsadu_vv_u32m1_u8m1(vupper, vl_vmv_v_x_u8m1(0), VL);\n\n        vl_vse32_v_i32m1(result, vsum, VL);\n\n        sum += result[0];\n        i += chunk_size;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    size_t len = s.size();\n    size_t i = 0;\n\n    vuint8m1_t vstr = vle8_v_u8m1(s.data(), len);\n    vboolm1_t vmask = vmseq_vx_u8m1_b1(vstr, 'A', len);\n    vuint8m1_t vupper = vsadd_vx_u8m1(vstr, -'A' + 'a', vmask);\n\n    while (i < len) {\n        size_t vl = vsetvl_e8m1(len - i);\n        vuint8m1_t vstr = vle8_v_u8m1(&s[i], vl);\n        vboolm1_t vmask = vmseq_vx_u8m1_b1(vstr, 'A', vl);\n        vuint8m1_t vupper = vsadd_vx_u8m1(vstr, -'A' + 'a', vmask);\n        vuint32m1_t vsum = vwredsum_vs_i32m1_i8m1(0, vupper, vmask, vl);\n        sum += vfirst_m_i32m1(vsum);\n        i += vl;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    size_t len = arr.size();\n    if (len == 0) {\n        return {};\n    }\n\n    vint32m8_t v_arr = vle32_v_i32m8(arr.data(), len);\n    vbool8_t v_is_even = vmseq_vx_i32m8(v_arr, 0, 1);\n    vint32m8_t v_min_val = vfredmin_vs_i32m8_i32m8(v_is_even, v_arr, vfmv_s_s_i32m8(0));\n    vuint32m8_t v_min_idx = vfind_first_m_i32m8(v_is_even);\n\n    int min_val = vfirst_m_i32m8(v_is_even, v_min_val);\n    int min_idx = vfirst_m_i32m8(v_is_even, v_min_idx);\n\n    if (min_val == 0) {\n        return {min_val, min_idx};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    size_t len = arr.size();\n    vint32m8_t v_arr = vle32_v_i32m8(arr.data(), len);\n    vbool8_t v_is_even = vmseq_vx_i32m8(v_arr, 0, 1);\n    vint32m8_t v_min_val = vfredmin_vs_i32m8_i32m8(v_is_even, v_arr, vfmv_s_s_i32m8(0));\n    vint32m8_t v_indices = vmsbf_m_i32m8(len, v_is_even);\n    vint32m8_t v_min_index = vfredmin_vs_i32m8_i32m8(v_is_even, v_indices, vfmv_s_s_i32m8(0));\n\n    int min_val = vfirst_m_i32m8(v_eq_vx_i32m8(v_min_val, vfmv_s_s_i32m8(0)));\n    int min_index = vfirst_m_i32m8(v_eq_vx_i32m8(v_min_index, vfmv_s_s_i32m8(0)));\n\n    if (min_val == -1 || min_index == -1) {\n        return {};\n    }\n\n    return {min_val, min_index};\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    size_t len = arr.size();\n    vint32m8_t v_arr = vle32_v_i32m8(arr.data(), len);\n    vbool8_t v_is_even = vmseq_vx_i32m8(v_arr, 0, 1);\n    vint32m8_t v_min_val = vfredmin_vs_i32m8_i32m8(v_is_even, v_arr, vundefined_i32m8());\n    vuint32m8_t v_min_idx = vfind_first_m_i32m8(v_is_even);\n\n    int min_val = vfirst_i32m8(v_min_val);\n    int min_idx = vfirst_u32m8(v_min_idx);\n\n    if (min_val == INT_MAX) {\n        return {};\n    }\n\n    return {min_val, min_idx};\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    size_t len = arr.size();\n    if (len == 0) {\n        return {};\n    }\n\n    vint32m8_t v_arr = vle32_v_i32m8(arr.data(), len);\n    vbool8_t v_is_even = vmseq_vx_i32m8(v_arr, 0, 1);\n    vint32m8_t v_min_val = vfredmin_vs_i32m8_i32m8(v_is_even, v_arr, v_setvl_e32m8(len));\n    vint32m8_t v_indices = vfmul_vx_i32m8(v_is_even, vsetvl_e32m8(len), 1);\n    vint32m8_t v_min_index = vfredmin_vs_i32m8_i32m8(v_is_even, v_indices, v_setvl_e32m8(len));\n\n    int min_val = vfirst_m_b8(v_is_even);\n    int min_index = vfirst_m_b8(v_is_even);\n\n    if (min_val == -1) {\n        return {};\n    } else {\n        return {min_val, min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_94_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    size_t len = arr.size();\n    if (len == 0) {\n        return {};\n    }\n\n    vint32m8_t v_arr = vle32_v_i32m8(arr.data(), len);\n    vbool8_t v_is_even = vmseq_vx_i32m8(v_arr, 0, 1);\n    vint32m8_t v_min_val = vfredmin_vs_i32m8_i32m8(v_is_even, v_arr, v_setvl_e32m8(len));\n    vint32m8_t v_indices = vfmul_vx_i32m8(v_is_even, v_iota_v_i32m8(0, 1, v_setvl_e32m8(len)), v_setvl_e32m8(len));\n    vint32m8_t v_min_index = vfredmin_vs_i32m8_i32m8(v_is_even, v_indices, v_setvl_e32m8(len));\n\n    int min_val = vfirst_m_b8(v_is_even);\n    int min_index = vfirst_m_b8(v_is_even);\n\n    if (min_val == -1) {\n        return {};\n    } else {\n        return {min_val, min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n == 0) return -1;\n\n    // Sort the list to group identical elements together\n    std::vector<int> sorted_lst(lst);\n    std::sort(sorted_lst.begin(), sorted_lst.end());\n\n    // Create a vector to store frequencies\n    std::vector<int> freq(n);\n    vint32m8_t v_freq = vle32_v_i32m8(freq.data(), n, vl);\n\n    // Iterate through the sorted list and count frequencies\n    int i = 0;\n    while (i < n) {\n        int val = sorted_lst[i];\n        int count = 1;\n        while (i + 1 < n && sorted_lst[i + 1] == val) {\n            ++count;\n            ++i;\n        }\n        v_freq = vslli_vx_i32m8(v_freq, i, vl);\n        v_freq = vadd_vx_i32m8(v_freq, count, vl);\n        ++i;\n    }\n\n    // Find the greatest integer with frequency >= its value\n    int result = -1;\n    for (int j = 0; j < n; ++j) {\n        if (sorted_lst[j] <= freq[j]) {\n            result = std::max(result, sorted_lst[j]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n == 0) return -1;\n\n    // Sort the list to group identical elements together\n    std::vector<int> sorted_lst(lst);\n    std::sort(sorted_lst.begin(), sorted_lst.end());\n\n    // Create a vector to store frequencies\n    std::vector<int> freq(n, 0);\n\n    // Calculate frequencies using RVV intrinsics\n    size_t vl = vsetvl_e32m8(n);\n    vint32m8_t v_freq = vmv_v_x_i32m8(0, vl);\n    vint32m8_t v_sorted_lst = vle32_v_i32m8(sorted_lst.data(), vl);\n    for (size_t i = 0; i < n; i += vl) {\n        vint32m8_t v_mask = veq_vx_i32m8(v_sorted_lst, sorted_lst[i], vl);\n        v_freq = vadd_vv_i32m8(v_freq, v_mask, vl);\n    }\n\n    // Find the greatest integer with frequency >= its value\n    int result = -1;\n    for (int i = 0; i < n; ++i) {\n        if (sorted_lst[i] <= freq[i]) {\n            result = std::max(result, sorted_lst[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n == 0) return -1;\n\n    // Sort the list to group identical elements together\n    std::vector<int> sorted_lst(lst);\n    std::sort(sorted_lst.begin(), sorted_lst.end());\n\n    // Create a vector to store frequencies\n    std::vector<int> freq(n);\n    vint32m8_t v_freq = vle32_v_i32m8(freq.data(), n, vl);\n\n    // Iterate through the sorted list and count frequencies\n    int i = 0;\n    while (i < n) {\n        int val = sorted_lst[i];\n        int count = 1;\n        while (i + 1 < n && sorted_lst[i + 1] == val) {\n            i++;\n            count++;\n        }\n        freq[i] = count;\n        i++;\n    }\n\n    // Find the greatest integer with a frequency >= its value\n    int result = -1;\n    for (int j = 0; j < n; j++) {\n        if (sorted_lst[j] <= freq[j]) {\n            result = std::max(result, sorted_lst[j]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n == 0) return -1;\n\n    // Sort the list to group identical elements together\n    std::vector<int> sorted_lst(lst);\n    std::sort(sorted_lst.begin(), sorted_lst.end());\n\n    // Create a vector to store the frequencies\n    std::vector<int> freq(n);\n\n    // Calculate the frequency of each element using RVV intrinsics\n    size_t vl = vsetvl_e32m8(n);\n    vint32m8_t v_freq = vmul_vx_i32m8(vle32_v_i32m8(sorted_lst.data(), vl), 1, vl);\n    for (size_t i = vl; i < n; i += vl) {\n        vint32m8_t v_next = vle32_v_i32m8(sorted_lst.data() + i, vl);\n        vint32m8_t v_mask = veq_vv_i32m8(v_freq, v_next, vl);\n        v_freq = vadd_vv_i32m8(v_freq, v_mask, vl);\n    }\n\n    // Find the greatest integer with a frequency >= its value\n    int result = -1;\n    for (size_t i = 0; i < n; i++) {\n        if (sorted_lst[i] <= freq[i]) {\n            result = std::max(result, sorted_lst[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    if (n == 0) return -1;\n\n    // Sort the list to group identical elements together\n    std::vector<int> sorted_lst(lst);\n    std::sort(sorted_lst.begin(), sorted_lst.end());\n\n    // Initialize variables for the result and the current element being checked\n    int result = -1;\n    int current_element = sorted_lst[0];\n    int count = 1;\n\n    // Iterate through the sorted list using RVV intrinsics for parallel processing\n    size_t vl = vsetvl_e32m8(n);\n    vint32m8_t v_sorted = vle32_v_i32m8(sorted_lst.data(), vl);\n    vbool8_t mask = vmseq_vx_i32m8(v_sorted, current_element, vl);\n\n    while (vl > 0) {\n        int sum = vfredosum_vs_i32m8_i32m1(mask, v_sorted, vl);\n        count += sum;\n\n        if (count >= current_element) {\n            result = current_element;\n        }\n\n        vl = vsetvl_e32m8(n - vl);\n        v_sorted = vle32_v_i32m8(sorted_lst.data() + vl, vl);\n        mask = vmseq_vx_i32m8(v_sorted, current_element, vl);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is balanced (palindromic)\n    vint32m1_t vec = vle32_v_i32m1(q.data(), n);\n    vint32m1_t reversed_vec = vrev32_v_i32m1(vec, n);\n    bool is_balanced = vand_vv_m_b(vbool8m1_t(), vec, reversed_vec, n);\n\n    // Calculate the sum of the elements\n    vint32m1_t sum_vec = vredsum_vs_i32m1_i32m1(0, vec, n, vsetvl_e32m1(n));\n\n    // Compare the sum with the maximum possible weight\n    bool is_sum_less_than_or_equal_w = vle32_v_i32m1(&w, 1)[0] >= vfirst_m_b(sum_vec, n);\n\n    return is_balanced && is_sum_less_than_or_equal_w;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is balanced (palindromic)\n    vint32m8_t vec = vle32_v_i32m8(q.data(), n);\n    vint32m8_t reversed_vec = vlre32_v_i32m8(q.data() + n - 1, n);\n    bool is_balanced = vand_vv_m_b(vbool8m1_t(), vec, reversed_vec);\n\n    // Calculate the sum of the elements\n    vint32m8_t sum_vec = vredsum_vs_i32m8_i32m8(vec, 0, vbool8m1_t());\n\n    // Compare the sum with the maximum possible weight\n    bool is_sum_valid = vle32_v_i32m8(&w, 1)[0] >= vfirst_m_i32(sum_vec);\n\n    return is_balanced && is_sum_valid;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is balanced (palindromic)\n    vint32m1_t vec_q = vle32_v_i32m1(q.data(), n);\n    vint32m1_t vec_q_rev = vle32_v_i32m1(q.data() + n - 1, n);\n    vbool8m1_t mask = vmseq_vv_b8m1(vec_q, vec_q_rev, n);\n    bool is_balanced = vredand_vs_u_b8m1(mask, mask, vfalse_b8m1());\n\n    // Calculate the sum of the elements\n    vint32m1_t vec_sum = vfredosum_vs_i32m1_i32m1(vi32_m1(0), vec_q, n);\n\n    // Compare the sum with the maximum possible weight\n    bool is_within_weight = vle32_v_i32m1(&w, 1)[0] >= vfirst_m_i32m1(vec_sum);\n\n    return is_balanced && is_within_weight;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is balanced (palindromic)\n    vint32m1_t vec = vle32_v_i32m1(q.data(), n);\n    vint32m1_t reversed_vec = vrev32_v_i32m1(vec, n);\n    bool is_balanced = vand_vv_m_b(vbool8m1_t(), vec, reversed_vec, n);\n\n    // Calculate the sum of the elements\n    vint32m1_t sum_vec = vredsum_vs_i32m1_i32m1(0, vec, n, nullptr);\n    int sum = vfirst_m_i32(sum_vec);\n\n    // Check if the sum is less than or equal to the maximum possible weight\n    bool is_sum_valid = sum <= w;\n\n    return is_balanced && is_sum_valid;\n}\n```"}
{"task_id": "SimdBench_96_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is balanced (palindromic)\n    vint32m1_t vec = vle32_v_i32m1(q.data(), n);\n    vint32m1_t rev_vec = vrev32_v_i32m1(vec, n);\n    bool is_balanced = vand_vv_m_b(vbool8m1_t(), vec, rev_vec, n);\n\n    // Calculate the sum of the elements\n    vint32m1_t sum_vec = vredsum_vs_i32m1_i32m1(0, vec, n);\n    int sum = vfirst_m(sum_vec, n);\n\n    // Check if the sum is less than or equal to the maximum possible weight\n    bool is_light_enough = sum <= w;\n\n    return is_balanced && is_light_enough;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n    size_t vlmax = vsetvl_e32m8(n);\n    vint32m8_t va = vle32_v_i32m8(arr.data(), vlmax);\n    vint32m8_t vb = vle32_v_i32m8(arr.data() + n - vlmax, vlmax);\n\n    for (size_t i = 0; i < vlmax; ++i) {\n        if (va[i] != vb[i]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n    size_t vlmax = vsetvl_e32m1(n);\n    vint32m1_t va = vle32_v_i32m1(arr.data(), vlmax);\n    vint32m1_t vb = vle32_v_i32m1(arr.data() + n - vlmax, vlmax);\n\n    for (size_t i = 0; i < vlmax; ++i) {\n        if (vfirst_m_b32(vne_vv_i32m1(va[i], vb[vlmax - 1 - i]))) {\n            changes += vpopcnt_m_b32(vne_vv_i32m1(va[i], vb[vlmax - 1 - i]));\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n    vint32m8_t va, vb;\n    vbool8_t mask;\n\n    for (int i = 0; i < n / 2; i += 8) {\n        va = vle32_v_i32m8(arr.data() + i, vlmax_e32m8(n - i));\n        vb = vle32_v_i32m8(arr.data() + n - i - 8, vlmax_e32m8(8));\n\n        mask = vmseq_vv_v_b8(va, vb);\n        changes += vlcnt_m_b(mask, vlmax_e32m8(8));\n    }\n\n    if (n % 2 != 0) {\n        int mid = n / 2;\n        if (arr[mid] != arr[n - mid - 1]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n    size_t vlmax = vsetvl_e32m8(n);\n    vint32m8_t va = vle32_v_i32m8(arr.data(), vlmax);\n    vint32m8_t vb = vle32_v_i32m8(arr.data() + n - vlmax, vlmax);\n\n    for (size_t i = 0; i < vlmax; ++i) {\n        if (va[i] != vb[i]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n    vint32m8_t va, vb;\n    vbool8_t mask;\n\n    for (int i = 0; i < n / 2; i += 8) {\n        va = vle32_v_i32m8(arr.data() + i, vlmax_e32m8(n - i));\n        vb = vle32_v_i32m8(arr.data() + n - i - 8, vlmax_e32m8(8));\n\n        mask = vmseq_vv_i8m8(va, vb, vlmax_e32m8(8));\n        changes += vlcvt_s_f_f32m1(vfredosum_vs_f32m1_f32m1_f32m1(0.0f, vmseq_vv_i8m8(va, vb, vlmax_e32m8(8)), vlmax_e32m1()), vlmax_e32m1());\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    size_t len1 = lst1.size(), len2 = lst2.size();\n    \n    if (len1 == 0 && len2 == 0) return {};\n    \n    vuint8m1_t vec1, vec2;\n    size_t i = 0;\n    \n    while (i + 64 <= len1) {\n        vec1 = vle8_v_u8m1(lst1.data() + i, 64);\n        sum1 += vredsum_vs_i32m1_i32(0, vlenb_v_u8m1(vec1), vec1, 0);\n        i += 64;\n    }\n    \n    while (i < len1) {\n        sum1 += lst1[i].size();\n        i++;\n    }\n    \n    i = 0;\n    \n    while (i + 64 <= len2) {\n        vec2 = vle8_v_u8m1(lst2.data() + i, 64);\n        sum2 += vredsum_vs_i32m1_i32(0, vlenb_v_u8m1(vec2), vec2, 0);\n        i += 64;\n    }\n    \n    while (i < len2) {\n        sum2 += lst2[i].size();\n        i++;\n    }\n    \n    if (sum1 < sum2) return lst1;\n    else if (sum2 < sum1) return lst2;\n    else return lst1;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    size_t len1 = lst1.size(), len2 = lst2.size();\n    \n    if (len1 == 0 || len2 == 0) {\n        return len1 == 0 ? lst2 : lst1;\n    }\n    \n    vuint8m1_t vec1, vec2;\n    size_t i = 0;\n    \n    while (i + 64 < len1 && i + 64 < len2) {\n        vec1 = vle8_v_u8m1(lst1.data() + i, 64);\n        vec2 = vle8_v_u8m1(lst2.data() + i, 64);\n        \n        sum1 += vredsum_vs_i32m1_i32m1(0, vlenb_v_i8m1(vec1), vec1, 0);\n        sum2 += vredsum_vs_i32m1_i32m1(0, vlenb_v_i8m1(vec2), vec2, 0);\n        \n        i += 64;\n    }\n    \n    while (i < len1) {\n        sum1 += lst1[i].size();\n        i++;\n    }\n    \n    while (i < len2) {\n        sum2 += lst2[i].size();\n        i++;\n    }\n    \n    return sum1 <= sum2 ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    size_t n1 = lst1.size(), n2 = lst2.size();\n    \n    if (n1 == 0 && n2 == 0) return {};\n    \n    vuint8m1_t vsum1 = vle8_v_u8m1(lst1.data(), n1 * sizeof(std::string), vlmax_v_e8m1(n1 * sizeof(std::string)));\n    vuint8m1_t vsum2 = vle8_v_u8m1(lst2.data(), n2 * sizeof(std::string), vlmax_v_e8m1(n2 * sizeof(std::string)));\n    \n    sum1 = vredsum_vs_u64_i64m1(sum1, vsum1, vsetvl_e64m1(1));\n    sum2 = vredsum_vs_u64_i64m1(sum2, vsum2, vsetvl_e64m1(1));\n    \n    if (sum1 <= sum2) return lst1;\n    else return lst2;\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    size_t len1 = lst1.size(), len2 = lst2.size();\n\n    vuint8m1_t vsum1 = vle8_v_u8m1(lst1.data(), len1);\n    vuint8m1_t vsum2 = vle8_v_u8m1(lst2.data(), len2);\n\n    sum1 = vredsum_vs_u64_i64m1(sum1, vsum1, vsetvl_e8m1(len1), vse64_v_i64m1(0));\n    sum2 = vredsum_vs_u64_i64m1(sum2, vsum2, vsetvl_e8m1(len2), vse64_v_i64m1(0));\n\n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t len1 = lst1.size();\n    size_t len2 = lst2.size();\n\n    if (len1 == 0 || len2 == 0) {\n        return {};\n    }\n\n    vuint32m8_t vlen1 = vle32_v_u32m8(lst1.data(), len1);\n    vuint32m8_t vlen2 = vle32_v_u32m8(lst2.data(), len2);\n\n    vbool8_t mask = vmseq_vv_b8(vlen1, vlen2);\n\n    if (vmfirst_m_b8(mask)) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    vuint64m1_t vx = vle64_v_u64m1(&x, 1);\n    vuint64m1_t vn = vle64_v_u64m1(&n, 1);\n    vuint64m1_t result = vmul_vv_u64m1(vn, vn);\n    bool is_power = vmseq_vv_b1(result, vx);\n    return is_power;\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    vuint64m1_t vx = vle64_v_u64m1(&x, 1);\n    vuint64m1_t vn = vle64_v_u64m1(&n, 1);\n    vuint64m1_t vmask = vmseq_vv_u64m1(vx, vn);\n    bool result = vmfirst_m_b1(vmask);\n    return result;\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    vuint64m1_t vx = vle64_v_u64m1(&x, 1);\n    vuint64m1_t vn = vle64_v_u64m1(&n, 1);\n    vuint64m1_t result = vmul_vv_u64m1(vn, vn);\n    bool is_power = vfirst_m_b1(result == vx);\n    return is_power;\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    vuint64m1_t vx = vle64_v_u64m1(&x, 1);\n    vuint64m1_t vn = vle64_v_u64m1(&n, 1);\n    vuint64m1_t vmask = vmseq_vv_u64m1(vx, vn);\n    bool result = vmfirst_m_b1(vmask);\n    return result;\n}\n```"}
{"task_id": "SimdBench_99_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    vuint64m1_t vx = vle64_v_u64m1(&x, 1);\n    vuint64m1_t vn = vle64_v_u64m1(&n, 1);\n    vuint64m1_t result = vmul_vv_u64m1(vn, vn);\n    bool is_power = vmseq_vv_b1(result, vx);\n    return is_power;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    size_t len = num.length();\n    vuint8m1_t vec_num = vle8_v_u8m1(num.data(), len);\n    vbool8_t mask = vmseq_vx_b8(vec_num, '2') | vmseq_vx_b8(vec_num, '3') |\n                    vmseq_vx_b8(vec_num, '5') | vmseq_vx_b8(vec_num, '7') |\n                    vmseq_vx_b8(vec_num, 'B') | vmseq_vx_b8(vec_num, 'D');\n    count += vpopcnt_m_i32(mask, vlmax_e8m1(len));\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    size_t len = num.length();\n    vuint8m1_t vec_num = vle8_v_u8m1(num.data(), len);\n    vbool8_t mask = vmseq_vx_u8m1_b8(vec_num, '2') |\n                    vmseq_vx_u8m1_b8(vec_num, '3') |\n                    vmseq_vx_u8m1_b8(vec_num, '5') |\n                    vmseq_vx_u8m1_b8(vec_num, '7') |\n                    vmseq_vx_u8m1_b8(vec_num, 'B') |\n                    vmseq_vx_u8m1_b8(vec_num, 'D');\n    count += vpopcnt_m_i32_mask(VM_VL16, mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    size_t len = num.length();\n    vuint8m1_t vec_num = vle8_v_u8m1(num.data(), len);\n    vbool8_t mask = vmseq_vx_b8(vec_num, 2) | vmseq_vx_b8(vec_num, 3) | vmseq_vx_b8(vec_num, 5) | vmseq_vx_b8(vec_num, 7) | vmseq_vx_b8(vec_num, 'B') | vmseq_vx_b8(vec_num, 'D');\n    count += vpopcnt_m_i32_mask(len, mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    size_t len = num.length();\n    vuint8m1_t vec_num = vle8_v_u8m1(num.data(), len);\n    vbool8_t mask = vmseq_vx_b8(vec_num, 2) | vmseq_vx_b8(vec_num, 3) | vmseq_vx_b8(vec_num, 5) | vmseq_vx_b8(vec_num, 7) | vmseq_vx_b8(vec_num, 'B') | vmseq_vx_b8(vec_num, 'D');\n    count += vpopcnt_m_i32_mask(len, mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    size_t len = num.length();\n    vuint8m1_t vec_num = vle8_v_u8m1(num.data(), len);\n    vbool8_t mask = vmseq_vx_u8m1_b8(vec_num, '2') |\n                    vmseq_vx_u8m1_b8(vec_num, '3') |\n                    vmseq_vx_u8m1_b8(vec_num, '5') |\n                    vmseq_vx_u8m1_b8(vec_num, '7') |\n                    vmseq_vx_u8m1_b8(vec_num, 'B') |\n                    vmseq_vx_u8m1_b8(vec_num, 'D');\n    vuint32m1_t vec_count = vpopcnt_m_v_i32m1(mask, 0);\n    count += vfirst_m_i32m1(vec_count);\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    vuint8m1_t vec_s;\n    vbool8_t mask;\n    vint32m1_t count;\n\n    for (size_t i = 0; i <= len - 3; i += 4) {\n        vec_s = vle8_v_u8m1(s.data() + i, 4);\n        mask = vmseq_vv_b8(vec_s, vle8_v_u8m1(s.data() + i + 1, 4));\n        mask = vmseq_vv_b8(mask, vmseq_vv_b8(vec_s, vle8_v_u8m1(s.data() + i + 2, 4)));\n        count = vredand_vs_i32m1_i8m1(0, mask, 4);\n\n        if (vmfirst_m_b8(count)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    vuint8m1_t v_s = vle8_v_u8m1(s.data(), n);\n    vuint8m1_t v_s_shifted_1 = vsra_vx_u8m1(v_s, 1);\n    vuint8m1_t v_s_shifted_2 = vsra_vx_u8m1(v_s, 2);\n\n    vboolm1_t v_mask = vmseq_vv_b1m1(v_s, v_s_shifted_1) | vmseq_vv_b1m1(v_s, v_s_shifted_2) | vmseq_vv_b1m1(v_s_shifted_1, v_s_shifted_2);\n\n    return !vmfirst_m_b1m1(v_mask);\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    vuint8m1_t v_s = vle8_v_u8m1(s.data(), len);\n    vuint8m1_t v_s2 = vle8_v_u8m1(s.data() + 1, len - 1);\n    vuint8m1_t v_s3 = vle8_v_u8m1(s.data() + 2, len - 2);\n\n    vboolm1_t v_eq = veq_vv_b_u8m1(v_s, v_s2);\n    v_eq = veq_vv_b_u8m1(v_eq, veq_vv_b_u8m1(v_s2, v_s3));\n\n    return !vredor_vs_b_i1m1(v_eq, v_eq, 0);\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cstring>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    size_t vec_len = vsetvl_e8m1(len);\n    vuint8m1_t vs1 = vle8_v_u8m1(s.data(), vec_len);\n\n    for (size_t i = 2; i < len; ++i) {\n        size_t vec_len = vsetvl_e8m1(len - i);\n        vuint8m1_t vs2 = vle8_v_u8m1(s.data() + i, vec_len);\n        vuint8m1_t vm = veq_vv_u8m1(vs1, vs2);\n        if (vm != vxor_vsx_b8(vm, vm)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s) {\n    size_t n = s.size();\n    if (n < 3) return false;\n\n    vuint8m1_t v_s = vle8_v_u8m1(s.data(), n);\n    vuint8m1_t v_s2 = vsrl_vx_u8m1(v_s, 1);\n    vuint8m1_t v_s3 = vsrl_vx_u8m1(v_s, 2);\n\n    vboolm1_t v_mask = vmseq_vv_b1m1(v_s, v_s2) | vmseq_vv_b1m1(v_s, v_s3) | vmseq_vv_b1m1(v_s2, v_s3);\n\n    return !vmfirst_m_b1m1(v_mask);\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    vuint64m8_t vec_len = vle64_v_u64m8(&len, 1);\n    vuint64m8_t vec_2 = vmul_vx_u64m8(vec_len, 2);\n    vuint64m8_t vec_3 = vmul_vx_u64m8(vec_len, 3);\n\n    bool is_prime = true;\n    for (size_t i = 5; i * i <= len; i += 6) {\n        vuint64m8_t vec_i = vmul_vx_u64m8(vec_len, i);\n        vuint64m8_t vec_i_plus_2 = vmul_vx_u64m8(vec_len, i + 2);\n        vbool64m8_t cond1 = vle64_v_b_u64m8(&i, 1);\n        vbool64m8_t cond2 = vle64_v_b_u64m8(&i + 2, 1);\n        vbool64m8_t cond3 = vand_vv_b_u64m8(cond1, vne_vv_u64m8(vec_len, vec_2));\n        vbool64m8_t cond4 = vand_vv_b_u64m8(cond2, vne_vv_u64m8(vec_len, vec_3));\n        vbool64m8_t cond5 = vor_vv_b_u64m8(cond3, cond4);\n        vbool64m8_t cond6 = vand_vv_b_u64m8(cond5, vne_vv_u64m8(vec_len, vec_i));\n        vbool64m8_t cond7 = vand_vv_b_u64m8(cond6, vne_vv_u64m8(vec_len, vec_i_plus_2));\n        if (vredor_vs_b_u64m8(cond7)) {\n            is_prime = false;\n            break;\n        }\n    }\n\n    return is_prime;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    vuint64m8_t vec_len = vle64_v_u64m8(&len, 1);\n    vuint64m8_t vec_2 = vmul_vx_u64m8(vec_len, 2);\n    vuint64m8_t vec_3 = vmul_vx_u64m8(vec_len, 3);\n\n    bool is_prime = true;\n    for (size_t i = 5; i * i <= len; i += 6) {\n        vuint64m8_t vec_i = vmul_vx_u64m8(vec_len, i);\n        vuint64m8_t vec_i_plus_2 = vmul_vx_u64m8(vec_len, i + 2);\n        vbool64m8_t mask_i = vle64_v_b64(&i, 1);\n        vbool64m8_t mask_i_plus_2 = vle64_v_b64(&i + 2, 1);\n        vuint64m8_t remainder_i = vrem_vx_u64m8(vec_len, vec_i);\n        vuint64m8_t remainder_i_plus_2 = vrem_vx_u64m8(vec_len, vec_i_plus_2);\n        vbool64m8_t is_divisible_i = vceq_vx_u64m8(remainder_i, 0);\n        vbool64m8_t is_divisible_i_plus_2 = vceq_vx_u64m8(remainder_i_plus_2, 0);\n        vbool64m8_t is_not_prime = vor_vvb_b64(is_divisible_i, is_divisible_i_plus_2);\n        is_prime &= !vredor_vs_b64(mask_i, is_not_prime, false);\n        is_prime &= !vredor_vs_b64(mask_i_plus_2, is_not_prime, false);\n    }\n\n    return is_prime;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    vuint64m8_t v = __rvm_vle64_v_u64m8(len - 5, reinterpret_cast<const uint64_t*>(str.c_str() + 5), len / 8);\n    vuint64m8_t mask = __rvm_vmseq_vx_u64m8(v, 2);\n    vuint64m8_t remainder = __rvm_vrem_vvm_u64m8(v, mask);\n\n    bool all_non_zero = __rvm_vredand_vs_u64m8_u64(remainder, 1, 0);\n    return !all_non_zero;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    vuint64m8_t v_mask = vmseq_vx_u64m8(len, 5);\n    vuint64m8_t v_step = vlse8_v_u64m8((const uint64_t[]){6}, 1, v_mask);\n    vuint64m8_t v_i = vle8_v_u64m8((const uint64_t[]){7}, 1);\n\n    while (true) {\n        vuint64m8_t v_div = vremu_vv_u64m8(v_i, v_step);\n        vuint64m8_t v_eq = vmseq_vv_u64m8(v_div, 0);\n        if (vlmax_v_u64m8(v_eq) != 0) break;\n        v_i = vadd_vv_u64m8(v_i, v_step);\n    }\n\n    return vle8_v_u64m8((const uint64_t[]){1}, 1)[0] == 1;\n}\n```"}
{"task_id": "SimdBench_102_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n\n    if (len % 2 == 0 || len % 3 == 0) return false;\n\n    vuint64m8_t v_mask = vmseq_vx_u64m8(len, 2);\n    vuint64m8_t v_divisors = vlseg8_v_u64m8(v_mask, len / 2);\n\n    vbool8_t v_is_prime = vmseq_vx_b8(v_divisors, 1);\n    vuint64m8_t v_remainders = vmul_vx_u64m8(v_divisors, len);\n\n    vbool8_t v_not_zero = vnnez_vx_b8(v_remainders, 0);\n    v_is_prime &= v_not_zero;\n\n    vuint64m8_t v_count = vredand_vs_u64m8(v_is_prime, v_uint64m8(0), LaneType::e64);\n\n    return v_count.v[0] == 0;\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    vuint8m1_t vN = vle8_v_u8m1((const uint8_t*)&N, 8);\n    vint32m1_t vSum = vredsum_vs_i32m1_i8m1(0, vN, vSetvl_e8m1(8), vSum);\n\n    int sum = vfirst_m_i32m1(vSum);\n    std::bitset<64> bits(sum);\n    return bits.to_string().substr(64 - bits.count());\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    vuint8m1_t v_N = vle8_v_u8m1((const uint8_t*)&N, 8);\n    vint32m1_t v_sum = vredsum_vs_i32m1_i8m1(0, v_N, v_narrow_i8m1_i32m1(v_N), vlmax_vv_i32m1_i32m1(8));\n    int sum = vfirst_m_i32m1_mask(v_sum);\n    std::bitset<64> bits(sum);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    vuint8m1_t vN = vle8_v_u8m1((const uint8_t*)&N, 8);\n    vint32m1_t vSum = vredsum_vs_i32m1_i8m1(0, vN, vSetvl_e8m1(8), vSum);\n\n    int sum = vfirst_m(vne0_mask_i32m1(vSum));\n    std::bitset<32> bits(sum);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    vuint8m1_t vN = vle8_v_u8m1((const uint8_t*)&N, 8);\n    vint32m1_t vSum = vredsum_vs_i32m1_i8m1(0, vN, vSetvl_e8m1(8), vSum);\n    int sum = vfirst_m(vne0_mask_i32m1(vSum));\n    std::bitset<64> bits(sum);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_103_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    vuint8m1_t vN = vle8_v_u8m1((const uint8_t*)&N, 8);\n    vuint8m1_t vSum = vredsum_vs_u8m1_u8m1(vSum, vN, vSetvl_e8m1(8), 0);\n\n    int sum = vfirst_m_b8(vne_vx_i8m1_b8(vSum, 0));\n    if (sum == -1) sum = 0;\n\n    std::bitset<64> bits(sum);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    size_t n = lst.size();\n    vint32m1_t v_lst = vle32_v_i32m1(lst.data(), n);\n    vbool8_t mask = vmseq_mask_b8(vlmax_e8(1), vadd_vv_u8(vmseq_mask_b8(vlmax_e8(1), vlsew_e8(n)), vle8_v_u8(&lst[0], n)));\n    vint32m1_t v_even_at_odd_indices = vand_vvm_i32m1(v_lst, vreinterpret_v_f32m1(mask));\n    sum += vredsum_vs_i32m1_i32m1_i32m1(v_even_at_odd_indices, 0, vnelem_v_i32m1(v_lst));\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    size_t n = lst.size();\n    vint32m1_t v_sum = vmv_v_x_i32m1(0);\n    \n    for (size_t i = 1; i < n; i += 2) {\n        if (lst[i] % 2 == 0) {\n            vint32m1_t v_val = vle32_v_i32m1(lst.data() + i, 1);\n            v_sum = vwredsum_vs_i32m1_i32m1(v_sum, v_val, v_sum, 0);\n        }\n    }\n    \n    sum = vfirst_m_i32m1(v_sum);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    vint32m1_t vsum = vmv_v_x_i32m1(0, vlmax);\n\n    for (size_t i = 1; i < n; i += 2) {\n        if (lst[i] % 2 == 0) {\n            vint32m1_t veven = vle32_v_i32m1(lst.data() + i, vlmax);\n            vsum = vwredsum_vs_i32m1_i32m1(vsum, veven, vsum, vlmax);\n        }\n    }\n\n    sum = vfirst_m_i32m1(vsum, vlmax);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    size_t n = lst.size();\n    vint32m1_t vec_lst = vle32_v_i32m1(lst.data(), n);\n    vbool8_t mask = vmseq_vi8_m(n, 1); // Create a mask for odd indices\n    vint32m1_t even_elements = vand_vx_i32m1(vec_lst, mask); // Select elements at odd indices\n    vint32m1_t result = vredsum_vs_i32m1_i32m1(0, even_elements, vone_v_i32m1()); // Sum the selected elements\n    sum = vfirst_m(result); // Extract the sum from the vector\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    vint32m1_t vlst = vle32_v_i32m1(lst.data(), vlmax);\n    vbool8_t vmask = vmseq_mask_b8(vlmax, vlmask, 1); // Odd indices mask\n    vint32m1_t vevens = vand_vx_i32m1(vlmax, vlst, 1); // Extract even numbers\n    vint32m1_t vresult = vadd_vv_i32m1(vlmax, vevens, vsext_vx_i32m1(vlmax, vmask, 0)); // Add even numbers at odd indices\n    sum = vredsum_vs_i32m1_i32m1_i32m1(0, vresult, vlmax, &sum);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    size_t vec_len = vsetvl_e8m1(len);\n    vuint8m1_t vec_in = vle8_v_u8m1(s.data(), vec_len);\n\n    vint32m1_t shift = vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vvmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(vmv_v_x_i32m1(2), vmul_vx_i32m1(v"}
{"task_id": "SimdBench_105_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    size_t vec_len = vsetvl_e8m1(len);\n    vuint8m1_t vstr = vle8_v_u8m1(s.data(), vec_len);\n\n    vint8m1_t vrot = vmul_vx_i8m1(vi32_m1(2), vi32_m1(2), vec_len);\n    vint8m1_t voffset = vadd_vx_i8m1(vi32_m1('a'), vrot, vec_len);\n\n    vuint8m1_t vres = vsub_vv_u8m1(vadd_vv_u8m1(vstr, voffset, vec_len), vi32_m1('z' - 'a' + 1), vec_len);\n\n    std::string res(len, '\\0');\n    vse8_v_u8m1(res.data(), vres, vec_len);\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    // Allocate memory for the result string\n    char *result = new char[len + 1];\n    std::memset(result, 0, len + 1);\n\n    // Convert input string to vector type\n    vchar8m1_t vec_in = vle8_v_f32m1(reinterpret_cast<const uint8_t*>(s.c_str()), len);\n\n    // Perform encryption\n    vchar8m1_t vec_out = vadd_vx_f32m1(vec_in, 2 * 2); // Shift by 4 positions\n\n    // Handle wrap-around for characters 'z' and 'Z'\n    vbool8_t mask = vmul_vx_u8_m1(veq_vx_u8_m1(vand_vx_u8_m1(vec_out, 0xFF), 0x7A), 0x01);\n    vec_out = vsub_vx_u8_m1(vec_out, 26, mask);\n    mask = vmul_vx_u8_m1(veq_vx_u8_m1(vand_vx_u8_m1(vec_out, 0xFF), 0x5A), 0x01);\n    vec_out = vsub_vx_u8_m1(vec_out, 26, mask);\n\n    // Store the result back to the output array\n    vse8_v_f32m1(reinterpret_cast<uint8_t*>(result), vec_out, len);\n\n    // Convert vector type back to string\n    std::string res(result);\n    delete[] result;\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    // Allocate memory for the result string\n    char *result = new char[len + 1];\n    std::memset(result, 0, len + 1);\n\n    // Convert input string to vector type\n    vchar8m1_t vec_in = vle8_v_f32m1(reinterpret_cast<const uint8_t*>(s.c_str()), len);\n\n    // Perform encryption\n    vint32m1_t vec_out = vfadd_vx_i32m1(vreinterpret_v_f32m1_i32m1(vec_in), 2 * 2);\n    vec_out = vmin_vx_i32m1(vec_out, 26);\n    vec_out = vsub_vx_i32m1(vec_out, 26);\n    vec_out = vadd_vx_i32m1(vec_out, 'a');\n\n    // Store the result back to the output array\n    vse8_v_f32m1(reinterpret_cast<uint8_t*>(result), vec_out, len);\n\n    // Convert the result vector back to string\n    std::string res(result);\n    delete[] result;\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_105_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len == 0) return \"\";\n\n    // Create a vector of characters from the input string\n    vchar8m1_t vec_s = vle8_v_f32m1(reinterpret_cast<const char*>(s.data()), len);\n\n    // Define the rotation value\n    int rotate = 4; // 2 * 2\n\n    // Perform the encryption using RVV intrinsics\n    vchar8m1_t vec_encrypted = vlsh_x_v_f32m1(vec_s, rotate, len);\n\n    // Convert the vector back to a string\n    std::string result(len, '\\0');\n    vse8_v_f32m1(reinterpret_cast<char*>(result.data()), vec_encrypted, len);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.size();\n    size_t vec_len = vsetvl_e8m1(len);\n    vchar8m1_t msg_vec = vle8_v_f32m1(reinterpret_cast<const char*>(message.c_str()), vec_len);\n    \n    vbool8_t is_upper = vmseq_vx_i8m1(msg_vec, 'A', vec_len);\n    vbool8_t is_lower = vmseq_vx_i8m1(msg_vec, 'a', vec_len);\n    vbool8_t is_vowel = vmor_vmv_b8(is_upper, is_lower, vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'E', vec_len), vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'e', vec_len), vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'I', vec_len), vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'i', vec_len), vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'O', vec_len), vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'o', vec_len), vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'U', vec_len), vec_len);\n    is_vowel = vmor_vmv_b8(is_vowel, vmseq_vx_i8m1(msg_vec, 'u', vec_len), vec_len);\n\n    vchar8m1_t swapped_case = vsub_vvm_i8m1(msg_vec, vmul_vx_i8m1(vand_vmv_b8(is_upper, is_lower, vec_len), 32), vec_len);\n    vchar8m1_t shifted_vowels = vadd_vvm_i8m1(msg_vec, vmul_vx_i8m1(vand_vmv_b8(is_vowel, is_upper, vec_len), -64), vec_len);\n    shifted_vowels = vadd_vvm_i8m1(shifted_vowels, vmul_vx_i8m1(vand_vmv_b8(is_vowel, is_lower, vec_len), 64), vec_len);\n\n    vchar8m1_t result = vselect_vvm_i8m1(is_vowel, shifted_vowels, swapped_case, vec_len);\n    vchar8m1_t final_result = vselect_vvm_i8m1(vmseq_vx_i8m1(result, 'Z', vec_len), vsub_vvm_i8m1(result, 52), result, vec_len);\n    final_result = vselect_vvm_i8m1(vmseq_vx_i8m1(final_result, 'z', vec_len), vsub_vvm_i8m1(final_result, 52), final_result, vec_len);\n\n    std::string encoded_message;\n    encoded_message.append(reinterpret_cast<char*>(vrecover_v_f32m1(final_result)), vec_len);\n    return encoded_message;\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.length();\n    size_t vec_len = vsetvl_e8m1(len);\n    vuint8m1_t msg_vec = vlseg8_v_u8m1(message.data(), vec_len);\n\n    // Swap case\n    vmul_vx_u8m1(msg_vec, msg_vec, 0x20);\n\n    // Replace vowels with the letter that appears 2 places ahead\n    vbool8_t is_vowel = vmseq_vx_b8(msg_vec, 'a') | vmseq_vx_b8(msg_vec, 'e') |\n                        vmseq_vx_b8(msg_vec, 'i') | vmseq_vx_b8(msg_vec, 'o') |\n                        vmseq_vx_b8(msg_vec, 'u') | vmseq_vx_b8(msg_vec, 'A') |\n                        vmseq_vx_b8(msg_vec, 'E') | vmseq_vx_b8(msg_vec, 'I') |\n                        vmseq_vx_b8(msg_vec, 'O') | vmseq_vx_b8(msg_vec, 'U');\n    vint8m1_t offset = vle8_v_i8m1((const int8_t*)\"aeiouAEIOU\", vec_len);\n    vint8m1_t new_chars = vadd_vx_i8m1(offset, 2);\n    vsub_vv_i8m1(new_chars, new_chars, 26);\n    vse8_v_i8m1(msg_vec + vfirst_m_b8(is_vowel), new_chars, vec_len);\n\n    return std::string((char*)msg_vec, len);\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.length();\n    size_t vec_len = vsetvl_e8m1(len);\n    vchar8m1_t msg_vec = vle8_v_f32m1(reinterpret_cast<const char*>(message.c_str()), vec_len);\n\n    // Swap case\n    vbool8_t is_upper = vmseq_vx_i8m1(msg_vec, 'A', vec_len);\n    vbool8_t is_lower = vmseq_vx_i8m1(msg_vec, 'a', vec_len);\n    msg_vec = vsub_vvm_i8m1(msg_vec, vselect_vvm_i8m1(is_upper, vsub_vvx_i8m1(msg_vec, 'A', vec_len), vadd_vvx_i8m1(msg_vec, 'a' - 'A', vec_len)), vec_len);\n\n    // Replace vowels\n    vchar8m1_t vowels = vmv_v_x_i8m1('aeiouAEIOU');\n    vchar8m1_t replacements = vmv_v_x_i8m1('cgikmoCGIKMO');\n    vbool8_t is_vowel = vmisin_vmv_i8m1(vowels, msg_vec, vec_len);\n    msg_vec = vselect_vvm_i8m1(is_vowel, replacements, msg_vec);\n\n    std::string result;\n    result.reserve(len);\n    for (size_t i = 0; i < len; i += vec_len) {\n        vse8_v_f32m1(reinterpret_cast<char*>(&result[i]), msg_vec, vec_len);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.size();\n    size_t vec_len = vsetvl_e8m1(len);\n    vchar8m1_t msg_vec = vlseg8_v_f32m1(reinterpret_cast<const f32_t*>(message.c_str()), len, vec_len);\n\n    // Swap case\n    vmul_vx_f32m1(msg_vec, msg_vec, -1.0f);\n\n    // Replace vowels with corresponding characters\n    vint32m1_t mask = vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'a', vec_len);\n    mask = vor_vv_i32m1(mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'e', vec_len));\n    mask = vor_vv_i32m1(mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'i', vec_len));\n    mask = vor_vv_i32m1(mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'o', vec_len));\n    mask = vor_vv_i32m1(mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'u', vec_len));\n\n    vint32m1_t offset = vsub_vx_i32m1(vadd_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 2), 97, vec_len);\n    vint32m1_t result = vadd_vx_i32m1(offset, 97, vec_len);\n\n    vint32m1_t final_mask = vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'A', vec_len);\n    final_mask = vor_vv_i32m1(final_mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'E', vec_len));\n    final_mask = vor_vv_i32m1(final_mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'I', vec_len));\n    final_mask = vor_vv_i32m1(final_mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'O', vec_len));\n    final_mask = vor_vv_i32m1(final_mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'U', vec_len));\n\n    vint32m1_t final_offset = vsub_vx_i32m1(vadd_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 2), 65, vec_len);\n    vint32m1_t final_result = vadd_vx_i32m1(final_offset, 65, vec_len);\n\n    vint32m1_t combined_mask = vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'a', vec_len);\n    combined_mask = vor_vv_i32m1(combined_mask, vmseq_vx_i32m1(vreinterpret_v_f32m1(msg_vec), 'A', vec_len));\n\n    vint32m1_t combined_result = vselect_vvm_i32m1(combined_mask, result, final_result, vec_len);\n\n    vfloat32m1_t encoded_msg_vec = vfmerge_vvm_f32m1(vreinterpret_v_f32m1(msg_vec), vreinterpret_v_f32m1(combined_result), vec_len);\n\n    std::string encoded_msg;\n    encoded_msg.resize(len);\n    vsse8_v_f32m1(reinterpret_cast<f32_t*>(encoded_msg.data()), len, encoded_msg_vec, vec_len);\n\n    return encoded_msg;\n}\n```"}
{"task_id": "SimdBench_106_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.size();\n    size_t vec_len = vsetvl_e8m1(len);\n    vchar8m1_t msg_vec = vle8_v_f32m1(reinterpret_cast<const char*>(message.data()), vec_len);\n\n    // Swap case\n    vbool8_t is_upper = vmseq_vx_i8m1(msg_vec, 'A', vec_len);\n    vbool8_t is_lower = vmseq_vx_i8m1(msg_vec, 'a', vec_len);\n    msg_vec = vsub_vvm_i8m1(msg_vec, vselect_vvm_i8m1(is_upper, vsub_vvx_i8m1(msg_vec, 'A', vec_len), vadd_vvx_i8m1(msg_vec, 'a' - 'A', vec_len)), vec_len);\n\n    // Replace vowels\n    vchar8m1_t vowels = vlui_vx_i8m1('aeiouAEIOU', vec_len);\n    vchar8m1_t replacements = vlui_vx_i8m1('cgikmoCGIKMO', vec_len);\n    vbool8_t is_vowel = vmisin_vmv_b8m1(vowels, msg_vec, vec_len);\n    msg_vec = vselect_vvm_i8m1(is_vowel, vadd_vvm_i8m1(replacements, vsub_vvm_i8m1(msg_vec, vowels, vec_len), vec_len), msg_vec);\n\n    std::string result;\n    for (size_t i = 0; i < len; ++i) {\n        result += static_cast<char>(msg_vec[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    size_t num_keys = dict.size();\n    std::vector<std::string> keys(num_keys);\n    std::transform(dict.begin(), dict.end(), keys.begin(),\n                   [](const std::pair<std::string, std::string>& p) { return p.first; });\n\n    vuint8m1_t vkeys = vle8_v_u8m1(keys.data(), num_keys * sizeof(char), vlmax_v_e8m1());\n    vboolm1_t vlower = vmseq_vx_b1(vkeys, 'a', vlmax_v_e8m1());\n    vboolm1_t vupper = vmseq_vx_b1(vkeys, 'A', vlmax_v_e8m1());\n\n    vboolm1_t vall_lower = vand_vv_b1(vlower, vupper, vlmax_v_e8m1());\n    bool result = !vfirst_m_b1(vall_lower, vlmax_v_e8m1());\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    size_t num_keys = dict.size();\n    std::vector<std::string> keys(num_keys);\n    std::transform(dict.begin(), dict.end(), keys.begin(),\n                   [](const std::pair<std::string, std::string>& p) { return p.first; });\n\n    vuint8m1_t vkeys = vle8_v_u8m1(keys.data(), num_keys * sizeof(char), vlmax_v_e8m1());\n    vboolm1_t vlower = vmseq_vx_b1(vkeys, 'a', vlmax_v_e8m1());\n    vboolm1_t vupper = vmseq_vx_b1(vkeys, 'A', vlmax_v_e8m1());\n\n    vboolm1_t vall_lower = vand_vv_b1(vlower, vupper, vlmax_v_e8m1());\n    bool result = !vfirst_m_b1(vall_lower, vlmax_v_e8m1());\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    size_t num_keys = dict.size();\n    std::vector<std::string> keys(num_keys);\n    std::transform(dict.begin(), dict.end(), keys.begin(),\n                   [](const std::pair<std::string, std::string>& p) { return p.first; });\n\n    vuint8m1_t vkeys = vle8_v_u8m1(keys.data(), num_keys * sizeof(char), vlmax_v_e8m1());\n    vboolm1_t vlower = vmseq_vx_b1(vkeys, 'a', vlmax_v_e8m1());\n    vboolm1_t vupper = vmseq_vx_b1(vkeys, 'A', vlmax_v_e8m1());\n\n    vboolm1_t vall_lower = vand_vv_b1(vlower, vupper, vlmax_v_e8m1());\n    bool result = !vfirst_m_b1(vall_lower, vlmax_v_e8m1());\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    size_t num_keys = dict.size();\n    std::vector<std::string> keys(num_keys);\n    std::transform(dict.begin(), dict.end(), keys.begin(),\n                   [](const std::pair<std::string, std::string>& p) { return p.first; });\n\n    vuint8m1_t vkeys = vle8_v_u8m1(keys.data(), num_keys * sizeof(char), vlmax_v_e8m1());\n    vboolm1_t vlower = vmseq_vx_b1(vkeys, 'a', vlmax_v_e8m1());\n    vboolm1_t vupper = vmseq_vx_b1(vkeys, 'A', vlmax_v_e8m1());\n\n    vboolm1_t vall_lower = vand_vv_b1(vlower, vupper, vlmax_v_e8m1());\n    bool result = !vfirst_m_b1(vall_lower, vlmax_v_e8m1());\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <map>\n#include <vector>\n#include <algorithm>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    size_t num_keys = dict.size();\n    std::vector<const char*> keys(num_keys);\n    std::vector<size_t> key_lengths(num_keys);\n\n    int i = 0;\n    for (const auto& pair : dict) {\n        keys[i] = pair.first.c_str();\n        key_lengths[i++] = pair.first.length();\n    }\n\n    vuint8m1_t vec_key_lengths = vlset_v_u8m1(key_lengths.data(), num_keys, 0);\n    vboolm1_t vec_is_lower = vmseq_vx_u8m1(vec_key_lengths, 'a');\n    vboolm1_t vec_is_upper = vmseq_vx_u8m1(vec_key_lengths, 'A');\n\n    for (size_t j = 0; j < num_keys; ++j) {\n        const char* key = keys[j];\n        size_t length = key_lengths[j];\n\n        vuint8m1_t vec_key = vlset_v_u8m1(reinterpret_cast<const uint8_t*>(key), length, 0);\n        vboolm1_t vec_key_is_lower = vmseq_vv_u8m1(vec_key, vmseq_vx_u8m1(vec_key, 'a'));\n        vboolm1_t vec_key_is_upper = vmseq_vv_u8m1(vec_key, vmseq_vx_u8m1(vec_key, 'A'));\n\n        if (!vand_vv_b1m1(vec_is_lower, vec_key_is_lower)) {\n            vec_is_lower = vfalse_v_b1m1();\n        }\n        if (!vand_vv_b1m1(vec_is_upper, vec_key_is_upper)) {\n            vec_is_upper = vfalse_v_b1m1();\n        }\n    }\n\n    return vredor_vs_b1m1_i1(vec_is_lower) || vredor_vs_b1m1_i1(vec_is_upper);\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    vbool8 mask = vmseq_vx_i8m1(vsetvl_e8m1(n), 0);\n    vint8m1_t vec = vle8_v_i8m1((const int8_t*)primes.data(), n, vsetvl_e8m1(n));\n    vuint8m1_t indices = vlse8_v_u8m1((const uint8_t*)primes.data(), n, vsetvl_e8m1(n));\n\n    for (int i = 2; i * i < n; i++) {\n        vint8m1_t multiples = vmul_vx_i8m1(vsetvl_e8m1(n), i, vsetvl_e8m1(n));\n        vbool8_t cond = vmul_vv_b8(multiples, multiples, vsetvl_e8m1(n));\n        vec = vsub_vv_i8m1(vec, vmul_vv_i8m1(cond, multiples, vsetvl_e8m1(n)), vsetvl_e8m1(n));\n    }\n\n    for (int i = 0; i < n; i++) {\n        if (vec[i] != 0) {\n            primes.push_back(i + 2);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    vuint32m1_t mask = vmseq_vx_u32m1(vle32_v_u32m1(primes.data(), n / sizeof(uint32_t)), 1);\n    vuint32m1_t sieve = vmseq_vx_u32m1(vle32_v_u32m1(primes.data(), n / sizeof(uint32_t)), 1);\n\n    for (int i = 2; i * i < n; i++) {\n        if (vmfirst_m_b1(mask)) {\n            int p = vfirst_m_i32(mask);\n            vuint32m1_t multiples = vmul_vx_u32m1(sieve, p);\n            sieve = vmin_vv_u32m1(sieve, vmseq_vx_u32m1(multiples, 1));\n        }\n    }\n\n    for (int i = 2; i < n; i++) {\n        if (vmfirst_m_b1(sieve)) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    int num_primes = 0;\n    for (int i = 2; i < n; ++i) {\n        bool is_prime = true;\n        for (int j = 2; j * j <= i; ++j) {\n            if (i % j == 0) {\n                is_prime = false;\n                break;\n            }\n        }\n        if (is_prime) {\n            ++num_primes;\n        }\n    }\n\n    std::vector<int> primes(num_primes);\n    int index = 0;\n    for (int i = 2; i < n; ++i) {\n        bool is_prime = true;\n        for (int j = 2; j * j <= i; ++j) {\n            if (i % j == 0) {\n                is_prime = false;\n                break;\n            }\n        }\n        if (is_prime) {\n            primes[index++] = i;\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    vuint32m1_t mask = vmseq_vx_u32m1(vle32_v_u32m1((const uint32_t*)primes.data(), n), 1);\n    vuint32m1_t sieve = vmseq_vx_u32m1(vle32_v_u32m1((const uint32_t*)primes.data(), n), 1);\n\n    for (int i = 2; i * i < n; i++) {\n        if (vmfirst_m_b1(mask)) {\n            int p = vsel_vx_i32m1(-1, i, vmfirst_m_b1(mask));\n            vuint32m1_t multiples = vmul_vx_u32m1(sieve, p);\n            sieve = vmin_vv_u32m1(sieve, vmseq_vx_u32m1(multiples, 1));\n        }\n    }\n\n    for (int i = 2; i < n; i++) {\n        if (vmfirst_m_b1(sieve)) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    int max_val = n - 1;\n    size_t num_elements = max_val / 32 + 1;\n    vbool32 mask = vmseq_vx_i32m1(num_elements * 32, max_val);\n\n    vuint32m1_t sieve = vle32_v_u32m1((const uint32_t*)mask, num_elements);\n    vuint32m1_t indices = vlse32_v_u32m1((const uint32_t*)mask, num_elements, 4);\n\n    for (int i = 2; i * i <= max_val; ++i) {\n        vuint32m1_t multiples = vmul_vx_u32m1(indices, i);\n        sieve = vsub_vv_u32m1(sieve, multiples);\n    }\n\n    vuint32m1_t result = vand_vv_u32m1(sieve, mask);\n    vuint32m1_t valid_indices = vadd_vx_u32m1(indices, vmul_vx_u32m1(vle32_v_u32m1((const uint32_t*)mask, num_elements), 3));\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        if (result[i]) {\n            primes.push_back(valid_indices[i]);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t vlmax = vsetvl_e8m1(len);\n    vuint8m1_t vs1 = vle8_v_u8m1(s.data(), vlmax);\n    vbool8_t vmask = vmseq_vx_u8m1(vs1, 'A', vlmax) &\n                    vmsltu_vx_u8m1(vs1, 'Z' + 1, vlmax) &\n                    vmseq_vx_u8m1(vs1, 'a', vlmax) &\n                    vmsltu_vx_u8m1(vs1, 'z' + 1, vlmax) &\n                    vmseq_vx_u8m1(vs1, 'E', vlmax) &\n                    vmseq_vx_u8m1(vs1, 'I', vlmax) &\n                    vmseq_vx_u8m1(vs1, 'O', vlmax) &\n                    vmseq_vx_u8m1(vs1, 'U', vlmax);\n    vuint32m1_t vcount = vfredosum_vs_u32m1_u8m1(0, vmask, vlmax);\n    count += vfirst_m(vcount, vlmax);\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t vlen = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vlen);\n    vbool8_t vm = __riscv_vmseq_vx_u8m1(vs, 'A', vlen) &\n                 __riscv_vmseq_vx_u8m1(vs, 'E', vlen) &\n                 __riscv_vmseq_vx_u8m1(vs, 'I', vlen) &\n                 __riscv_vmseq_vx_u8m1(vs, 'O', vlen) &\n                 __riscv_vmseq_vx_u8m1(vs, 'U', vlen);\n    vuint32m1_t vc = __riscv_vmv_s_x_u32m1(__builtin_popcountl(__riscv_vredsum_vs_i32m1_i8m1(0, vm, vlen)));\n    count += __riscv_vfirst_m_b1(vc);\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t vlmax = vsetvl_e8m1(len);\n    vuint8m1_t vs1 = vle8_v_u8m1(s.data(), vlmax);\n\n    for (size_t i = 0; i < len; i += vlmax) {\n        vbool8_t mask = vmseq_vx_b8(vs1, 'A', vlmax) & vmsltu_vx_b8(vs1, 'Z' + 1, vlmax);\n        vuint32m1_t vcount = vfredosum_vs_i32m1_i8m1(0, vmfirst_m(mask, vlmax), vlmax);\n        count += vget_v_i32m1(vcount, 0);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t vlen = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vlen);\n    vbool8_t vm = __riscv_vmseq_vx_b8(vs, 'A', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'E', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'I', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'O', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'U', vlen);\n    vuint32m1_t vc = __riscv_vredsum_vs_u32m1_u8m1(0, vm, vlen);\n    count += __riscv_vmv_x_s_u32m1_i32(vc);\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    size_t len = s.length();\n    size_t vlen = __riscv_vsetvl_e8m1(len);\n    vuint8m1_t vs = __riscv_vle8_v_u8m1(s.data(), vlen);\n    vbool8_t vm = __riscv_vmseq_vx_b8(vs, 'A', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'E', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'I', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'O', vlen) &\n                 __riscv_vmseq_vx_b8(vs, 'U', vlen);\n    vuint32m1_t vc = __riscv_vredsum_vs_i32m1_i8m1(0, vm, vlen);\n    count += __riscv_vmv_x_s_i32m1_i32(vc);\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    int levels = n;\n    std::vector<int> result(levels);\n\n    vint32m8_t v_n = vle32_v_i32m8(&n, 1);\n    vbool8_t mask = vmseq_vx_b8(v_n, 0);\n    vint32m8_t v_odd_even = vadd_vx_i32m8(v_n, 1, mask);\n    vint32m8_t v_result = vsew_v_i32m8(result.data(), v_odd_even, levels);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    int levels = n;\n    std::vector<int> result(levels);\n\n    vint32m8_t v_n = vle32_v_i32m8(&n, 1);\n    vbool8_t mask = vmseq_vx_b8(v_n, 0);\n    vint32m8_t v_odd_even = vadd_vx_i32m8(v_n, 1, mask);\n    vint32m8_t v_result = vsew_v_i32m8(result.data(), v_odd_even, levels);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    int levels = n;\n    std::vector<int> result(levels);\n\n    vint32m8_t v_n = vle32_v_i32m8(&n, 1);\n    vbool8_t v_mask = vmseq_vx_b8(v_n, 0);\n    vint32m8_t v_odd_even = vsub_vv_i32m8(v_n, v_and_vv_b8(v_mask, v_n));\n    vint32m8_t v_next = vadd_vx_i32m8(v_odd_even, 1);\n    vint32m8_t v_result = vsew_v_i32m8(result.data(), v_next, levels);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    int levels = n;\n    std::vector<int> result(levels);\n\n    vint32m8_t v_n = vle32_v_i32m8(&n, 1);\n    vbool8_t mask = vmseq_vx_b8(v_n, 0);\n    vint32m8_t v_odd_even = vadd_vx_i32m8(v_n, 1, mask);\n    vint32m8_t v_result = vsew_v_i32m8(result.data(), v_odd_even, levels);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    int levels = n;\n    std::vector<int> result(levels);\n\n    vint32m1_t v_n = vle32_v_i32m1(&n, 1);\n    vbool16_t mask = vmseq_vx_b16(v_n, 0);\n    vint32m1_t v_odd_even = vadd_vx_i32m1(v_n, 1, mask);\n    vint32m1_t v_result = vsew_v_i32m1(result.data(), v_odd_even, levels);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <bitset>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    size_t len = m - n + 1;\n    vint64m1_t vec_n = vle64_v_i64m1(n, len);\n    vint64m1_t vec_m = vle64_v_i64m1(m, len);\n\n    vint64m1_t sum = vredsum_vs_i64m1_i64m1(vsub_vv_i64m1(vec_m, vec_n), vec_n, vsetvl_e64m1(len));\n\n    int64_t avg = std::round(static_cast<double>(vsra_vx_i64m1(sum, vsrl_vx_u64m1(vlmax_v_i64m1(), len), 1)) / len);\n\n    std::bitset<64> bits(avg);\n    return bits.to_string().substr(64 - std::ceil(std::log2(avg + 1)));\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    size_t len = m - n + 1;\n    vint64m1_t vec_n = vle64_v_i64m1(n, &len);\n    vint64m1_t vec_m = vle64_v_i64m1(m, &len);\n\n    vint64m1_t sum = vredsum_vs_i64m1_i64m1(vsub_vv_i64m1(vec_m, vec_n), vec_n, vsetvl_e64m1(len));\n    vint64m1_t avg = vdiv_vv_i64m1(sum, vmul_vv_i64m1(vle64_v_i64m1(1, nullptr), vle64_v_i64m1(len, nullptr)), vsetvl_e64m1(len));\n\n    int64_t result = std::round(vfirst_m_i64(avg, vsetvl_e64m1(1)));\n\n    return std::bitset<64>(result).to_string();\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    size_t len = m - n + 1;\n    vint64m1_t vec_n = vle64_v_i64m1(n, &n, len);\n    vint64m1_t vec_m = vle64_v_i64m1(m, &m, len);\n\n    vint64m1_t sum = vredsum_vs_i64m1_i64m1(vsub_vv_i64m1(vec_m, vec_n), vec_n, vsetvl_e64m1(len));\n    vint64m1_t avg = vdiv_vx_i64m1(sum, len, vsetvl_e64m1(len));\n\n    int64_t result = std::round(vfirst_m_i64(avg, vsetvl_e64m1(1)));\n    return std::bitset<64>(result).to_string().substr(64 - std::ceil(std::log2(result + 1)));\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <bitset>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    size_t len = m - n + 1;\n    vint64m8_t vec_n = vle64_v_i64m8(n, len);\n    vint64m8_t vec_m = vle64_v_i64m8(m, len);\n\n    vint64m8_t sum = vredsum_vs_i64m8_i64m8(vec_n, vec_m, vsetvl_e64m8(len), 0);\n    vfloat64m8_t avg_f = vfdiv_vf_f64m8(vfcvt_s_w_f64m8(sum), vfmul_vf_f64m8(vfredmax_vs_i64m8_i64m8(vec_m, vec_n, vsetvl_e64m8(len)), vsetvl_e64m8(len)), vsetvl_e64m8(len));\n    vint64m8_t avg_i = vfwcvt_x_f64m8(avg_f, vsetvl_e64m8(len));\n\n    int64_t result = std::round(avg_i[0]);\n    return std::bitset<64>(result).to_string().substr(64 - std::ceil(std::log2(result + 1)));\n}\n```"}
{"task_id": "SimdBench_111_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <bitset>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    size_t len = m - n + 1;\n    vint64m1_t vec_n = vle64_v_i64m1(n, len);\n    vint64m1_t vec_m = vle64_v_i64m1(m, len);\n\n    vint64m1_t sum = vredsum_vs_i64m1_i64m1(vsub_vv_i64m1(vec_m, vec_n), vec_n, vsetvl_e64m1(len));\n\n    int64_t avg = std::round(static_cast<double>(vsra_vx_i64m1(sum, vsrl_vx_u64m1(vlen_v_i64m1(), vsetvl_e64m1(len)), 1)) / len);\n\n    std::bitset<64> bits(avg);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    int vec_size = n / vint32m8_t::size() + (n % vint32m8_t::size() != 0);\n    vint32m8_t result = vle32_v_i32m8((const int32_t*)nullptr, vec_size);\n    vbool8_t mask = vmseq_vi_b8(vindex_v_i8m8(vec_size), vsetvl_e8m8(1));\n\n    for (int i = 1; i <= n; ++i) {\n        vint32m8_t idx = vle32_v_i32m8((const int32_t*)&i, 1);\n        vint32m8_t val;\n        if (i % 2 == 0) {\n            val = vfmul_vv_f32m8(vfexp2_v_f32m8(vfcvt_s_s_f32m8(idx)), vfexp2_v_f32m8(vfcvt_s_s_f32m8(vsub_vv_i32m8(idx, vle32_v_i32m8((const int32_t*)&1, 1)))));\n        } else {\n            val = vfredsum_vs_i32m8_i32m8(vle32_v_i32m8((const int32_t*)&i, 1), vle32_v_i32m8((const int32_t*)&1, 1), vle32_v_i32m8((const int32_t*)&0, 1));\n        }\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v_i32m8((const int32_t*)&10000, 1));\n        val = vmul_vx_i32m8(val, vle32_v"}
{"task_id": "SimdBench_112_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    int vec_size = n / vint32m1_t::vlmax();\n    std::vector<int> result(n);\n\n    vint32m1_t indices = vlset_v_i32m1(1, vec_size);\n    vbool8_t mask_even = vmseq_vx_i8m1(indices, 0, vec_size);\n\n    for (int i = 0; i < vec_size; ++i) {\n        vint32m1_t current_index = vle32_v_i32m1(&indices[i], vec_size - i);\n        vint32m1_t factorial = vfmul_vv_f32m1(vfexp2_v_f32m1(current_index), vflog2_v_f32m1(current_index));\n        vint32m1_t sum = vfredsum_vs_i32m1_i32m1(viota_v_i32m1(1, vec_size - i), 0, vec_size - i);\n        vint32m1_t result_vec = vselect_b_i32m1(mask_even, factorial, sum);\n        vse32_v_i32m1(&result[i * vint32m1_t::vlmax()], result_vec, vec_size - i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    int v_n = n;\n    int vl = vsetvl_e32m1(v_n);\n    vint32m1_t v_i = vmul_vx_i32m1(vundefined_v32m1(), 1, vl);\n    vint32m1_t v_fact = vfmacc_vv_f32m1(vundefined_v32m1(), v_i, v_i, vl);\n    vint32m1_t v_sum = vfredsum_vs_i32m1_i32m1(vundefined_v32m1(), v_i, vl);\n    vint32m1_t v_mask = vmseq_vx_i32m1(v_i, 0, vl);\n    vfact = vselect_vvm_i32m1(v_mask, v_fact, v_sum, vl);\n    std::vector<int> result(n);\n    vst_v_i32m1(result.data(), v_fact, vl);\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    int vl = vsetvl_e32m8(n);\n    vint32m8_t vec_n = vmul_vx_i32m8(vle32_v_i32m8((const int32_t*)nullptr, vl), 1, vl);\n    vbool8_t mask = vmseq_vx_b8(vec_n, 0, vl);\n\n    vint32m8_t vec_fact = vfadd_vv_i32m8(vfsub_vv_i32m8(vec_n, vle32_v_i32m8((const int32_t*)nullptr, vl), vl), vone_v_i32m8(), vl);\n    vint32m8_t vec_sum = vfadd_vv_i32m8(vfsub_vv_i32m8(vec_n, vle32_v_i32m8((const int32_t*)nullptr, vl), vl), vone_v_i32m8(), vl);\n\n    vint32m8_t result = vselect_vvm_i32m8(mask, vec_fact, vec_sum, vl);\n\n    std::vector<int> res(result.vstart, result.vstop);\n    return res;\n}\n```"}
{"task_id": "SimdBench_112_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    int vec_size = n / vint32m1_t::vlmax();\n    std::vector<int> result(n);\n\n    vint32m1_t indices = vle32_v_i32m1(&result[0], vec_size);\n    vbool8_t mask = vmseq_vx_b8(indices, 0);\n\n    for (int i = 0; i < vec_size; ++i) {\n        vint32m1_t idx = vsub_vx_i32m1(vle32_v_i32m1(&result[i * vint32m1_t::vlmax()], vint32m1_t::vlmax()), 1);\n        vint32m1_t fact = vfadd_vv_f32m1(vfmul_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f32m1(vfadd_vv_f3"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    vint32m1_t v_n = vle32_v_i32m1(&n, 1);\n    vbool1_t mask = vmseq_vx_i32m1_b1(v_n, 1);\n\n    for (int i = 1; i <= n; i++) {\n        vint32m1_t v_i = vle32_v_i32m1(&i, 1);\n        vbool1_t is_palindrome = vmseq_vv_i32m1_b1(v_i, vrev32_v_i32m1(v_i));\n        vint32m1_t v_even = vmul_vx_i32m1(v_i, 2);\n        vbool1_t is_even = vmseq_vv_i32m1_b1(v_i, v_even);\n\n        even_count += vpopcnt_m_b1(mask & is_palindrome & is_even);\n        odd_count += vpopcnt_m_b1(mask & is_palindrome & ~is_even);\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    vint32m1_t v_n = vle32_v_i32m1(&n, 1);\n    vint32m1_t v_range = vmul_vx_i32m1(v_n, v_n, 1);\n\n    for (int i = 1; i <= n; i++) {\n        vint32m1_t v_num = vle32_v_i32m1(&i, 1);\n        vint32m1_t v_rev = vrev32_v_i32m1(v_num);\n        vbool8_t v_is_palindrome = veq_vv_b8(v_num, v_rev);\n        vint32m1_t v_even_odd = vmul_vx_i32m1(v_is_palindrome, v_num, 2);\n        vint32m1_t v_even = vand_vx_i32m1(v_even_odd, v_num, 1);\n        vint32m1_t v_odd = vsub_vx_i32m1(v_even_odd, v_even, 1);\n\n        even_count += vredsum_vs_i32_i32m1(even_count, v_even, vsew_e32, vlmax_e32m1());\n        odd_count += vredsum_vs_i32_i32m1(odd_count, v_odd, vsew_e32, vlmax_e32m1());\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    vint32m1_t v_n = vle32_v_i32m1(&n, 1);\n    vbool1_t mask = vmseq_vx_i32m1_b1(v_n, 1);\n\n    for (int i = 1; i <= n; i++) {\n        vint32m1_t v_i = vle32_v_i32m1(&i, 1);\n        vbool1_t is_palindrome = vmseq_vv_i32m1_b1(v_i, vrev32_v_i32m1(v_i));\n        vint32m1_t v_even = vand_vx_i32m1(v_i, 1);\n        vint32m1_t v_odd = vsub_vx_i32m1(v_i, v_and_vx_i32m1(v_i, 1));\n\n        even_count += vredsum_vs_i32_i32m1_i32(even_count, v_even, is_palindrome, vl);\n        odd_count += vredsum_vs_i32_i32m1_i32(odd_count, v_odd, is_palindrome, vl);\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    vint32m1_t v_n = vle32_v_i32m1(&n, 1);\n    vint32m1_t v_range = vmul_vx_i32m1(v_n, v_n, 1);\n\n    for (int i = 1; i <= n; i++) {\n        vint32m1_t v_num = vle32_v_i32m1(&i, 1);\n        vint32m1_t v_rev = vrev32_v_i32m1(v_num);\n        vboolm1_t v_is_palindrome = veq_vv_b1m1(v_num, v_rev);\n        vint32m1_t v_even_odd = vmul_vx_i32m1(v_is_palindrome, v_num, 2);\n        vint32m1_t v_sum = vredsum_vs_i32m1_i32m1_m(even_count, v_even_odd, v_is_palindrome, &even_count);\n        vint32m1_t v_diff = vsub_vv_i32m1(v_sum, v_even_odd);\n        odd_count += v_diff[0];\n    }\n\n    std::vector<int> result = {even_count, odd_count};\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0;\n    int odd_count = 0;\n\n    vint32m1_t v_n = vle32_v_i32m1(&n, 1);\n    vint32m1_t v_one = vmv_v_x_i32m1(1);\n\n    for (int i = 1; i <= n; i++) {\n        vint32m1_t v_i = vle32_v_i32m1(&i, 1);\n        vbool8_t is_palindrome = vand_vv_b8(v_eq_vv_i32m1(v_i, vrev32_v_i32m1(v_i)), vne_vx_i32m1(v_i, 0));\n        vint32m1_t v_is_even = veq_vx_i32m1(v_and_vv_i32m1(v_i, v_one), v_zero_v_i32m1());\n        vint32m1_t v_is_odd = vnand_vv_i32m1(v_is_even, v_one);\n\n        even_count += vredsum_vs_i32m1_i32m1_m(even_count, v_is_even, is_palindrome, vlmax_vv_ei32m1(is_palindrome, is_palindrome));\n        odd_count += vredsum_vs_i32m1_i32m1_m(odd_count, v_is_odd, is_palindrome, vlmax_vv_ei32m1(is_palindrome, is_palindrome));\n    }\n\n    std::vector<int> result = {even_count, odd_count};\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int size = nums.size();\n    if (size == 0) return 0;\n\n    vint32m8_t vnums = vle32_v_i32m8(nums.data(), size);\n    vbool8_t mask = vmseq_vx_i32m8(vnums, 0);\n    vuint32m8_t abs_nums = vabs_v_i32m8(vnums);\n\n    vint32m8_t sum_digits = vredsum_vs_i32m8_i32m8(0, abs_nums, mask, vse32_v_i32m8(nullptr, 0));\n    vbool8_t positive_sum = vgts_vx_i32m8(sum_digits, 0);\n\n    return vpopcnt_m_b8(positive_sum);\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    size_t num_elements = nums.size();\n    size_t vlen = __riscv_vsetvl_e32m1(num_elements);\n    vint32m1_t vnums = __riscv_vle32_v_i32m1(nums.data(), vlen);\n    vbool8_t vcond = __riscv_vgtu_vx_i32m1_b8(vnums, 0);\n    size_t count = __riscv_vredsum_vs_u8_i32m1(0, vcond, vlen);\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    size_t num_elements = nums.size();\n    size_t vlen = __riscv_vsetvl_e32m1(num_elements);\n    vint32m1_t vnums = __riscv_vle32_v_i32m1(nums.data(), vlen);\n    vbool8_t vcond = __riscv_vmseq_vx_i8m1(vnums, 0, vlen);\n    vuint32m1_t vcount = __riscv_vfredosum_vs_u32m1_u32m1(0, vcond, vlen);\n    return __riscv_vmv_x_s_u32m1_i32(vcount);\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int size = nums.size();\n    if (size == 0) return 0;\n\n    vint32m8_t vnums = vle32_v_i32m8(nums.data(), size);\n    vbool8_t mask = vmseq_vx_i32m8(vnums, 0);\n    vint32m8_t vnegatives = vsub_vx_i32m8(vnums, vand_vvm_i32m8(vnums, mask));\n    vint32m8_t vabs = vabs_v_i32m8(vnegatives);\n    vint32m8_t vsums = vredsum_vs_i32m8_i32m1(vabs, 0, vtrue_v_i1m1());\n    vint32m1_t vcount = vmseq_vx_i32m1(vsums, 0);\n\n    return vfirst_m_b1(vnot_v_b1(vcount));\n}\n```"}
{"task_id": "SimdBench_114_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int size = nums.size();\n    if (size == 0) return 0;\n\n    vint32m8_t vnums = vle32_v_i32m8(nums.data(), size);\n    vbool8_t mask = vmseq_vx_i32m8(vnums, 0);\n    vuint32m8_t abs_nums = vabs_v_i32m8(vnums);\n\n    vint32m8_t sum_digits = vredsum_vs_i32m8_i32m8(0, abs_nums, mask, vnums);\n    vbool8_t positive_sum = vgts_vx_i32m8(sum_digits, 0);\n\n    return vpopcnt_m_b8(positive_sum);\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    vint32m1_t v_arr = vle32_v_i32m1(arr.data(), n);\n    vint32m1_t v_sorted = vle32_v_i32m1(arr.data(), n);\n\n    v_sort_bubble(v_sorted, v_arr, n);\n\n    for (int i = 0; i < n; ++i) {\n        vint32m1_t v_shifted = v_rvv_vl_i32m1(v_arr, n - i);\n        vint32m1_t v_concat = v_concat_i32m1(v_shifted, v_arr, i);\n        if (veq_mask_vv_i8m1(v_concat, v_sorted, n)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    vint32m8_t vec = vle32_v_i32m8(arr.data(), n);\n    vint32m8_t shifted_vec = vshuff32_mask_m8(vec, vec, vmseq_vx_i8m8(vle8_v_i8m8(arr.data() + n - 1, 1), 0));\n\n    for (int i = 0; i < n; ++i) {\n        if (vfirst_m_b8(vmseq_vv_i32m8(shifted_vec, vec))) {\n            return true;\n        }\n        vec = vshuff32_mask_m8(vec, vec, vmseq_vx_i8m8(vle8_v_i8m8(arr.data() + n - 1, 1), 0));\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    vint32m1_t vec = vle32_v_i32m1(arr.data(), n);\n    vint32m1_t shifted_vec = vshuff32_mask_m1_i32m1(vec, vec, vsetvl_e32m1(n), 1);\n\n    bool is_sorted = true;\n    for (int i = 0; i < n - 1; ++i) {\n        if (vec[i] > vec[i + 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n\n    if (is_sorted) {\n        return true;\n    }\n\n    for (int i = 0; i < n - 1; ++i) {\n        if (shifted_vec[i] > shifted_vec[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    vint32m1_t vec = vle32_v_i32m1(arr.data(), n);\n    vint32m1_t shifted_vec = vssrl_vx_i32m1(vec, vec, 1);\n\n    bool is_sorted = true;\n    for (int i = 0; i < n - 1; ++i) {\n        if (vec[i] > vec[i + 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n\n    bool is_shifted_sorted = true;\n    for (int i = 0; i < n - 1; ++i) {\n        if (shifted_vec[i] > shifted_vec[i + 1]) {\n            is_shifted_sorted = false;\n            break;\n        }\n    }\n\n    return is_sorted || is_shifted_sorted;\n}\n```"}
{"task_id": "SimdBench_115_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool move_one_ball_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    vint32m1_t vec = vle32_v_i32m1(arr.data(), n);\n    vint32m1_t shifted_vec = vshuff32_mask_m1_i32m1(vec, vec, vsetvl_e32m1(n), 0, 1);\n\n    bool is_sorted = true;\n    for (int i = 0; i < n - 1; ++i) {\n        if (vec[i] > vec[i + 1]) {\n            is_sorted = false;\n            break;\n        }\n    }\n\n    if (is_sorted) {\n        return true;\n    }\n\n    for (int i = 0; i < n - 1; ++i) {\n        if (shifted_vec[i] > shifted_vec[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <iostream>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t n = lst1.size();\n    vint32m8_t v_lst1 = vle32_v_i32m8(lst1.data(), n);\n    vint32m8_t v_lst2 = vle32_v_i32m8(lst2.data(), n);\n\n    vbool8_t mask = vmseq_vx_i32m8(v_lst1, 0, n);\n    vint32m8_t even_elements = vand_vv_i32m8(v_lst1, mask, n);\n\n    vint32m8_t odd_elements = vsub_vv_i32m8(v_lst1, even_elements, n);\n    vint32m8_t available_even_elements = vand_vv_i32m8(v_lst2, mask, n);\n\n    vbool8_t can_exchange = vge_vv_i32m8(available_even_elements, odd_elements, n);\n\n    if (vredor_vs_u_b_i32m8(can_exchange)) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t n = lst1.size();\n    vint32m8_t v_lst1 = vle32_v_i32m8(lst1.data(), n);\n    vint32m8_t v_lst2 = vle32_v_i32m8(lst2.data(), n);\n\n    vbool8_t mask = vmseq_vx_i32m8_b8(v_lst1, 0, 1); // Check if all elements in lst1 are even\n\n    if (vmfirst_m_b8(mask) == -1) {\n        return \"YES\";\n    }\n\n    vbool8_t mask2 = vmseq_vx_i32m8_b8(v_lst2, 0, 1); // Check if all elements in lst2 are even\n\n    if (vmfirst_m_b8(mask2) == -1) {\n        return \"NO\";\n    }\n\n    vint32m8_t v_even_lst2 = vselect_vvm_i32m8(v_lst2, mask, vzero_v_i32m8()); // Select even elements from lst2\n    vint32m8_t v_odd_lst1 = vselect_vvm_i32m8(v_lst1, ~mask, vzero_v_i32m8()); // Select odd elements from lst1\n\n    vint32m8_t v_merged = vadd_vv_i32m8(v_even_lst2, v_odd_lst1); // Merge even elements from lst2 with odd elements from lst1\n\n    vbool8_t mask3 = vmseq_vx_i32m8_b8(v_merged, 0, 1); // Check if all elements in merged vector are even\n\n    if (vmfirst_m_b8(mask3) == -1) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <iostream>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t n = lst1.size();\n    vint32m8_t v_lst1 = vle32_v_i32m8(lst1.data(), n);\n    vint32m8_t v_lst2 = vle32_v_i32m8(lst2.data(), n);\n\n    vbool8_t v_is_odd_lst1 = vmseq_vx_i32m8_b8(v_lst1, 0, 1);\n    vbool8_t v_is_even_lst2 = vmsltu_vx_u32m8_b8(v_lst2, 0, 2);\n\n    vbool8_t v_can_exchange = vand_vv_b8(v_is_odd_lst1, v_is_even_lst2);\n\n    if (vfredor_vs_b8_i32m8_m(v_can_exchange, n)) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <iostream>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t n = lst1.size();\n    vint32m8_t v_lst1 = vle32_v_i32m8(lst1.data(), n);\n    vint32m8_t v_lst2 = vle32_v_i32m8(lst2.data(), n);\n\n    vbool8_t v_is_odd_lst1 = vmseq_vx_i32m8_b8(v_lst1, 0, 1);\n    vbool8_t v_is_even_lst2 = vmsltu_vx_u32m8_b8(v_lst2, 0, 2);\n\n    vint32m8_t v_even_elements_lst2 = vcompress_v_b8_m8(v_lst2, v_is_even_lst2);\n    vint32m8_t v_odd_elements_lst1 = vcompress_v_b8_m8(v_lst1, v_is_odd_lst1);\n\n    vint32m8_t v_merged = vmerge_vvm_i32m8(v_odd_elements_lst1, v_even_elements_lst2, n);\n\n    vbool8_t v_all_even = vmseq_vx_i32m8_b8(v_merged, 0, 1);\n\n    if (vfredor_vs_b8_i1(v_all_even)) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    size_t n = lst1.size();\n    vint32m8_t v_lst1 = vle32_v_i32m8(lst1.data(), n);\n    vint32m8_t v_lst2 = vle32_v_i32m8(lst2.data(), n);\n\n    vbool8_t v_is_odd_lst1 = vmseq_vx_i32m8_b8(v_lst1, 0, 1);\n    vbool8_t v_is_even_lst2 = vmseq_vx_i32m8_b8(v_lst2, 0, 2);\n\n    vint32m8_t v_even_elements_lst2 = vcompress_vm_i32m8(v_is_even_lst2, v_lst2);\n    vint32m8_t v_odd_elements_lst1 = vcompress_vm_i32m8(v_is_odd_lst1, v_lst1);\n\n    vint32m8_t v_merged = vmerge_vvm_i32m8(v_odd_elements_lst1, v_even_elements_lst2, n);\n\n    if (vfirst_m_b8(vne_vv_i32m8_b8(v_merged, v_lst1))) {\n        return \"NO\";\n    } else {\n        return \"YES\";\n    }\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    size_t n = lst.size();\n    std::vector<std::string> result(n);\n    \n    vuint8m1_t vstr;\n    vuint8m1_t vodd;\n    vint32m1_t vcount;\n    int32_t count;\n    \n    for (size_t i = 0; i < n; i += vl) {\n        size_t vl = vsetvl_e8m1(n - i);\n        vstr = vle8_v_u8m1(lst[i].c_str(), vl);\n        \n        vodd = vmseq_vx_u8m1(vstr, '1', vl);\n        vodd = vor_vv_u8m1(vodd, vmseq_vx_u8m1(vstr, '3', vl), vl);\n        vodd = vor_vv_u8m1(vodd, vmseq_vx_u8m1(vstr, '5', vl), vl);\n        vodd = vor_vv_u8m1(vodd, vmseq_vx_u8m1(vstr, '7', vl), vl);\n        vodd = vor_vv_u8m1(vodd, vmseq_vx_u8m1(vstr, '9', vl), vl);\n        \n        vcount = vredsum_vs_i32m1_i8m1(0, vodd, vl);\n        count = vfirst_m_i32m1(vbooltrue_v_i32m1(), vcount, vl);\n        \n        result[i] = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    size_t n = lst.size();\n    std::vector<std::string> result(n);\n\n    vuint8m1_t mask = vmseq_vx_u8m1(0b01010101, vle8_v_u8m1(lst.data(), n * sizeof(std::string)), n);\n    vuint32m1_t count = vfredosum_vs_u8m1_i32m1(vpopcnt_v_u8m1(mask), mask, vsetvl_e32m1(n));\n\n    for (size_t i = 0; i < n; ++i) {\n        char buffer[256];\n        snprintf(buffer, sizeof(buffer), \"the number of odd elements %dn the str%dn of the %dnput.\", count[i], i + 1, i + 1);\n        result[i] = buffer;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    size_t n = lst.size();\n    std::vector<std::string> result(n);\n\n    vuint8m1_t v_str;\n    vint32m1_t v_odd_count;\n    vbool8m1_t v_mask;\n\n    for (size_t i = 0; i < n; i += vl) {\n        size_t vl = vsetvl_e8m1(i + vl > n ? n - i : vl);\n        v_str = vle8_v_u8m1(lst[i].data(), vl);\n\n        v_odd_count = vmul_vx_i32m1(vmv_v_x_i32m1(0), vle8_v_u8m1(lst[i].data(), vl), vl);\n        v_mask = veq_vx_b8m1(v_str, vle8_v_u8m1(\"13579\", vl), vl);\n        v_odd_count = vadd_vv_i32m1(v_odd_count, vpopcnt_v_b8m1(v_mask, vl), vl);\n\n        char buffer[256];\n        snprintf(buffer, sizeof(buffer), \"the number of odd elements %dn the str%dn of the %dnput.\", v_odd_count[0], v_odd_count[0], v_odd_count[0]);\n        result[i] = std::string(buffer);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    size_t n = lst.size();\n    vuint8m1_t vec_in;\n    vint32m1_t vec_out;\n    int32_t count;\n\n    for (size_t i = 0; i < n; i += vl) {\n        size_t vl = vsetvl_e8m1(n - i);\n        vec_in = vle8_v_u8m1(lst[i].c_str() + i, vl);\n\n        count = vredsum_vs_i32m1_i8m1(0, vec_in, vl, nullptr);\n\n        std::string temp = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(temp);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    size_t n = lst.size();\n    std::vector<std::string> result(n);\n\n    vuint8m1_t mask = vmseq_vx_u8m1(0b10101010, 0b10101010); // Mask to extract odd bits\n\n    for (size_t i = 0; i < n; i += vlmax_v_i32m1()) {\n        size_t vl = vlmax_v_i32m1();\n        if (i + vl > n) {\n            vl = n - i;\n        }\n\n        vcharmxm1_t vec = vle8_v_charxm1(lst[i].c_str() + i, vl);\n        vuint8m1_t odd_bits = vand_vvm_u8m1(vec, mask);\n        vuint32m1_t count = vredsum_vs_u32m1_u8m1(vmv_v_x_u32m1(0), odd_bits, vl);\n\n        for (size_t j = 0; j < vl; ++j) {\n            char buffer[256];\n            snprintf(buffer, sizeof(buffer), \"the number of odd elements %dn the str%dn of the %dnput.\", count[j], count[j], count[j]);\n            result[i + j] = buffer;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    int64_t min_sum = std::numeric_limits<int64_t>::max();\n    vint64m8_t v_nums = vle64_v_i64m8(nums.data(), n);\n    vint64m8_t v_min_sum = vsetvl_e64m8(n);\n    vse64_v_i64m8(v_min_sum, &min_sum, n);\n\n    for (size_t i = 0; i < n; i += 8) {\n        vint64m8_t v_current_sum = vsetvl_e64m8(8);\n        vint64m8_t v_prev_sum = vsetvl_e64m8(8);\n        vint64m8_t v_temp = vsetvl_e64m8(8);\n\n        for (size_t j = 0; j < 8; ++j) {\n            v_temp = vfadd_vv_i64m8(v_prev_sum, v_nums, vsetvl_e64m8(1));\n            v_prev_sum = vfmin_vv_i64m8(v_temp, v_nums, vsetvl_e64m8(1));\n            v_current_sum = vfadd_vv_i64m8(v_current_sum, v_prev_sum, vsetvl_e64m8(1));\n        }\n\n        vint64m8_t v_min_current = vfmin_vv_i64m8(v_min_sum, v_current_sum, vsetvl_e64m8(8));\n        vse64_v_i64m8(v_min_sum, v_min_current, n);\n    }\n\n    return vfirst_m_i64m8(v_min_sum);\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    int64_t min_sum = std::numeric_limits<int64_t>::max();\n    vint64m8_t v_nums = vle64_v_i64m8(nums.data(), n);\n    vint64m8_t v_min_sum = vle64_v_i64m8(&min_sum, 1);\n\n    for (size_t i = 0; i < n; i += 8) {\n        vint64m8_t v_current_sum = vmul_vx_i64m8(v_nums, 1);\n        vint64m8_t v_min_current_sum = vmin_vv_i64m8(v_current_sum, v_min_sum);\n        vse64_v_i64m8(&min_sum, v_min_current_sum, 1);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    int64_t min_sum = std::numeric_limits<int64_t>::max();\n    vint64m8_t v_nums = vle64_v_i64m8(nums.data(), n);\n    vint64m8_t v_min_sum = vsetvl_e64m8(n);\n    vse64_v_i64m8(v_min_sum, &min_sum, n);\n\n    for (size_t i = 0; i < n; i += 8) {\n        vint64m8_t v_current_sum = vle64_v_i64m8(nums.data() + i, 8);\n        vint64m8_t v_temp = vfadd_vv_i64m8(v_current_sum, v_min_sum);\n        vint64m8_t v_mask = vcmpge_vv_i64m8(v_temp, v_current_sum);\n        vcurrent_sum = vselect_vvm_i64m8(v_mask, v_temp, v_current_sum);\n        vse64_v_i64m8(v_min_sum, &min_sum, n);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <limits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    int64_t min_sum = std::numeric_limits<int64_t>::max();\n    vint64m8_t v_nums = vle64_v_i64m8(nums.data(), n);\n    vint64m8_t v_min_sum = vle64_v_i64m8(&min_sum, 1);\n\n    for (size_t i = 0; i < n; i += 8) {\n        vint64m8_t v_current_sum = vmul_vx_i64m8(v_nums, 1);\n        vint64m8_t v_temp = vfadd_vv_i64m8(v_current_sum, v_min_sum);\n        v_min_sum = vmin_vv_i64m8(v_temp, v_min_sum);\n    }\n\n    return vfirst_m_i64m8(v_min_sum);\n}\n```"}
{"task_id": "SimdBench_118_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <algorithm>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    int64_t min_sum = INT64_MAX;\n    vint64m8_t v_nums = vle64_v_i64m8(nums.data(), n);\n    vint64m8_t v_min_sum = vsetvl_e64m8(n);\n    vse64_v_i64m8(v_min_sum, &min_sum, n);\n\n    for (size_t i = 0; i < n; i += 8) {\n        vint64m8_t v_current_sum = vle64_v_i64m8(nums.data() + i, 8);\n        vint64m8_t v_temp = vfadd_vv_i64m8(v_current_sum, v_min_sum);\n        vint64m8_t v_mask = vgts_vv_m_b64(v_temp, v_current_sum);\n        vint64m8_t v_new_min_sum = vselect_vvm_i64m8(v_mask, v_temp, v_current_sum);\n        vse64_v_i64m8(v_min_sum, v_new_min_sum, 8);\n    }\n\n    int64_t result;\n    vle64_v_i64m8(&result, v_min_sum, 1);\n    return result;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    vint8m1_t v_capacity = vle8_v_i8m1(&capacity, 1);\n    vint8m1_t v_zero = vmv_v_x_i8m1(0);\n\n    for (int i = 0; i < rows; i += 4) {\n        vint8m1_t v_sum = vzero_v_i8m1();\n        for (int j = 0; j < cols; j++) {\n            vint8m1_t v_row = vle8_v_i8m1(&grid[i][j], cols - j);\n            v_sum = vwredsum_vs_i8m1_i32m1(v_sum, v_row, v_zero);\n        }\n        vint32m1_t v_total_water = vfwredsum_vs_i32m1_i8m1(vzero_v_i32m1(), v_sum, v_zero);\n        total_water += vfirst_m_i32m1(v_total_water);\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    size_t vlmax = vsetvl_e8m1(cols);\n    vuint8m1_t v_capacity = vle8_v_u8m1(&capacity, vlmax);\n\n    for (int i = 0; i < rows; i++) {\n        vuint8m1_t v_row = vle8_v_u8m1(grid[i].data(), vlmax);\n        vuint8m1_t v_count = vmul_vx_u8m1(v_row, v_capacity, vlmax);\n        total_water += vredsum_vs_u8m1_i32m1(0, v_count, nullptr, vlmax);\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    size_t rows = grid.size();\n    size_t cols = grid[0].size();\n    size_t total_water = 0;\n\n    vuint8m1_t v_capacity = vle8_v_u8m1(&capacity, 1);\n    vuint8m1_t v_zero = vmv_v_x_u8m1(0);\n\n    for (size_t i = 0; i < rows; i += 4) {\n        vuint8m1_t v_sum = vzero_v_u8m1();\n        for (size_t j = 0; j < cols; j += 16) {\n            vuint8m1_t v_row = vle8_v_u8m1(grid[i + (j / 16)].data() + (j % 16), 16);\n            v_sum = vwredsum_vs_u8m1_u8m1(v_sum, v_row, v_zero);\n        }\n        total_water += vfirst_m_b8(vwmul_vx_u8m1(v_sum, v_capacity));\n    }\n\n    return total_water;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    vint8m1_t v_capacity = vle8_v_i8m1(&capacity, 1);\n    vint8m1_t v_zero = vmv_v_x_i8m1(0);\n\n    for (int i = 0; i < rows; i += 4) {\n        vint8m1_t v_sum = vzero_v_i8m1();\n        for (int j = 0; j < cols; j++) {\n            vint8m1_t v_row = vle8_v_i8m1(&grid[i][j], cols - j);\n            v_sum = vwredsum_vs_i8m1_i32m1(v_sum, v_row, v_zero);\n        }\n        vint32m1_t v_total_water = vfwredsum_vs_i8m1_i32m1(vzero_v_i32m1(), v_sum, v_zero);\n        total_water += vfirst_m_i32m1(v_total_water);\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    size_t vlmax = vsetvl_e8m1(cols);\n    vuint8m1_t v_capacity = vmul_vx_u8m1(vle8_v_u8m1(grid.data(), vlmax), capacity);\n\n    for (int i = 0; i < rows; i++) {\n        size_t vl = vsetvl_e8m1(cols);\n        vuint8m1_t v_water = vle8_v_u8m1(&grid[i][0], vl);\n        vuint8m1_t v_fill_count = vdivu_vv_u8m1(vwater, v_capacity, vl);\n        total_water += vredsum_vs_u8_i32(total_water, v_fill_count, vl);\n    }\n\n    return total_water;\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    int64_t sum = 0;\n    size_t vlmax = vsetvl_e16m1(n);\n    vint16m1_t vlst = vle16_v_i16m1(lst.data(), vlmax);\n    vbool16_t vmask = vmseq_vx_b16(vlst, 1, vlmax);\n    vint16m1_t vodd = vand_vvm_i16(vlst, vmask, vlmax);\n    vuint16m1_t veven = vmseq_vx_u16(vlst, 2, vlmax);\n    vint16m1_t vsum = vredsum_vs_i16_i64(0, vodd, veven, vlmax);\n    sum += vfirst_m_i64(vsum, vlmax);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    int64_t sum = 0;\n    size_t vlmax = vsetvl_e16m1(n);\n    vint16m1_t v_lst = vle16_v_i16m1(lst.data(), vlmax);\n    vbool16_t v_mask = vmseq_vi16_m_b16(v_lst, 1); // Check if the element is odd\n    vuint16m1_t v_indices = vle16_v_u16m1((const uint16_t*)lst.data(), vlmax);\n    vbool16_t v_even_indices = vmseq_vi16_m_b16(v_indices, 0); // Check if the index is even\n    vbool16_t v_and_mask = vbland_vv_b16(v_mask, v_even_indices);\n    vint16m1_t v_filtered = vcompress_vx_i16m1(v_lst, v_and_mask, 0);\n    sum = vredsum_vs_i16m1_i64m1(sum, v_filtered, v_setvl_e64m1(1));\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e16m1(n);\n    vint16m1_t vlst = vle16_v_i16m1(lst.data(), vlmax);\n    vbool16_t vmask = vmseq_vi16(vlst, 1, vlmax);\n    vint16m1_t vodd = vand_vv_i16(vlst, vmask, vlmax);\n    vint16m1_t veven = vmul_vx_i16(vodd, 2, vlmax);\n    vint64m1_t vsum = vwredsum_vs_i16_i64(veven, 0, vlmax);\n    sum = vfirst_m_i64(vsum, vlmax);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    int64_t sum = 0;\n    size_t vlmax = vsetvl_e16m1(n);\n    vint16m1_t vlst = vle16_v_i16m1(lst.data(), vlmax);\n    vbool16_t vmask = vmseq_vx_i16m1_b16(vlst, 1); // Check if the element is odd\n    vbool16_t vposmask = vmseq_vx_i16m1_b16(vlmax / 2, 0); // Check if the position is even\n    vbool16_t vandmask = vbland_vv_b16(vmask, vposmask); // Combine both conditions\n    vint16m1_t vfiltered = vcompress_vm_i16m1(vlst, vandmask, vlmax); // Filter elements\n    sum = vredsum_vs_i16m1_i64m1(sum, vfiltered, vlmax); // Sum filtered elements\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    size_t n = lst.size();\n    int64_t sum = 0;\n    size_t vlmax = vsetvl_e16m1(n);\n    vint16m1_t v_lst = vle16_v_i16m1(lst.data(), vlmax);\n    vbool16_t mask = vmseq_vi16_m_b16(v_lst, 1); // Check if the element is odd\n    vuint16m1_t v_indices = vmul_vx_u16_m_b16(vlmax, vsew_v_fv_u16(1), mask); // Create indices mask for even positions\n    vbool16_t even_mask = vmseq_vi16_m_b16(v_indices, 0); // Check if the index is even\n    vint16m1_t v_filtered = vand_vv_i16_m_b16(v_lst, v_lst, even_mask); // Filter elements at even positions\n    vint64m1_t v_sum = vwredsum_vs_i16m1_i64m1(0, v_filtered, vlmax); // Sum the filtered elements\n    sum = vfirst_m_i64m1(v_sum); // Extract the result\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_elements_simd(const std::vector<int>& arr, int k) {\n    int result = 0;\n    size_t num_chunks = (k + 7) / 8; // Calculate the number of chunks needed\n\n    for (size_t i = 0; i < num_chunks; ++i) {\n        vint32m1_t vec = vle32_v_i32m1(&arr[i * 8], 8); // Load 8 elements into a vector\n        vbool8_t mask = vmseq_vx_i8m1_b8(vabs_v_i8m1_m(vec, vec), 99); // Create a mask for elements with at most two digits\n        vint32m1_t filtered_vec = vand_vvm_i32m1(vec, vec, mask); // Apply the mask to filter elements\n        int chunk_sum = vredsum_vs_i32m1_i32m1_i32m1(0, filtered_vec, filtered_vec, vleng_v_i32m1()); // Sum the filtered elements\n        result += chunk_sum; // Add the chunk sum to the total result\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_elements_simd(const std::vector<int>& arr, int k) {\n    int result = 0;\n    size_t n = std::min(k, arr.size());\n    vint32m1_t vec_a = vle32_v_i32m1(arr.data(), n);\n    vbool1_t mask = vmseq_vx_i32m1_b1(vec_a, 99);\n    vint32m1_t filtered_vec = vand_vvm_i32m1(vec_a, mask);\n    result = vredsum_vs_i32m1_i32m1_i32m1(0, filtered_vec, vse32_v_i32m1(result, 1));\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_elements_simd(const std::vector<int>& arr, int k) {\n    int result = 0;\n    size_t num_chunks = (k + 7) / 8; // Calculate the number of chunks needed\n\n    for (size_t i = 0; i < num_chunks; ++i) {\n        vint32m1_t vec = vle32_v_i32m1(&arr[i * 8], 8); // Load 8 elements into a vector register\n        vbool8_t mask = vmseq_vx_i32m1_b8(vec, 100); // Create a mask where elements have at most two digits\n        vint32m1_t filtered_vec = vand_vvm_i32m1(vec, mask); // Apply the mask to filter elements\n        int chunk_sum = vredsum_vs_i32m1_i32m1_i32m1(0, filtered_vec, vse32_v_i32m1(result), 8); // Sum the filtered elements\n        result += chunk_sum;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_elements_simd(const std::vector<int>& arr, int k) {\n    int result = 0;\n    size_t num_elements = std::min(k, arr.size());\n    vint32m1_t vec_arr = vle32_v_i32m1(arr.data(), num_elements);\n    vbool1_t mask = vmseq_vx_i32m1_b1(vec_arr, 99);\n    vint32m1_t filtered_vec = vand_vvm_i32m1(vec_arr, mask);\n    result = vredsum_vs_i32m1_i32m1_i32m1(0, filtered_vec, vsetvl_e32m1(num_elements), vse32_v_i32m1(result));\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint add_elements_simd(const std::vector<int>& arr, int k) {\n    int result = 0;\n    size_t num_elements = std::min(k, arr.size());\n    vint32m1_t vec = vle32_v_i32m1(arr.data(), num_elements);\n    vbool1_t mask = vmseq_vx_i32m1_b1(vec, 99);\n    vint32m1_t filtered_vec = vand_vvm_i32m1(vec, mask);\n    result = vredsum_vs_i32m1_i32m1_i32m1(0, filtered_vec, vsetvl_e32m1(num_elements), vse32_v_i32m1(result));\n    return result;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    vint32m8_t vec1, vec2;\n    vbool8_t mask, dup_mask;\n    int i = 0;\n\n    while (i + 16 <= n) {\n        vec1 = vle32_v_i32m8(lst.data() + i);\n        vec2 = vle32_v_i32m8(lst.data() + i + 16);\n\n        mask = vgts_vv_i32m8_b8(vec1, vec2);\n        dup_mask = vmseq_vv_i32m8_b8(vec1, vec2);\n\n        if (!vmfirst_m_b8(mask)) return false;\n        if (vmfirst_m_b8(dup_mask)) return false;\n\n        i += 16;\n    }\n\n    while (i + 8 <= n) {\n        vec1 = vle32_v_i32m8(lst.data() + i);\n        vec2 = vle32_v_i32m8(lst.data() + i + 8);\n\n        mask = vgts_vv_i32m8_b8(vec1, vec2);\n        dup_mask = vmseq_vv_i32m8_b8(vec1, vec2);\n\n        if (!vmfirst_m_b8(mask)) return false;\n        if (vmfirst_m_b8(dup_mask)) return false;\n\n        i += 8;\n    }\n\n    while (i + 4 <= n) {\n        vec1 = vle32_v_i32m8(lst.data() + i);\n        vec2 = vle32_v_i32m8(lst.data() + i + 4);\n\n        mask = vgts_vv_i32m8_b8(vec1, vec2);\n        dup_mask = vmseq_vv_i32m8_b8(vec1, vec2);\n\n        if (!vmfirst_m_b8(mask)) return false;\n        if (vmfirst_m_b8(dup_mask)) return false;\n\n        i += 4;\n    }\n\n    while (i + 2 <= n) {\n        vec1 = vle32_v_i32m8(lst.data() + i);\n        vec2 = vle32_v_i32m8(lst.data() + i + 2);\n\n        mask = vgts_vv_i32m8_b8(vec1, vec2);\n        dup_mask = vmseq_vv_i32m8_b8(vec1, vec2);\n\n        if (!vmfirst_m_b8(mask)) return false;\n        if (vmfirst_m_b8(dup_mask)) return false;\n\n        i += 2;\n    }\n\n    if (i + 1 == n) {\n        if (lst[i] < lst[i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    vint32m8_t vec1 = vle32_v_i32m8(lst.data(), n);\n    vint32m8_t vec2 = vle32_v_i32m8(lst.data() + 1, n - 1);\n\n    vbool8_t is_sorted = vgts_vv_i32m8_b8(vec1, vec2);\n    vbool8_t is_unique = vmseq_vv_i32m8_b8(vec1, vec2);\n\n    bool result = vredand_vs_u_b8(is_sorted, is_unique, vtrue_v_b8());\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    vint32m8_t vec1 = vle32_v_i32m8(lst.data(), n);\n    vint32m8_t vec2 = vle32_v_i32m8(lst.data() + 1, n - 1);\n\n    vbool8_t is_sorted = vgts_vv_i32m8_b8(vec1, vec2);\n    vbool8_t has_duplicates = vmseq_vv_i32m8_b8(vec1, vec2);\n\n    bool result = vredand_vs_u_b8(is_sorted, has_duplicates, true);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    vint32m8_t vec_a = vle32_v_i32m8(lst.data(), n);\n    vint32m8_t vec_b = vle32_v_i32m8(lst.data() + 1, n - 1);\n\n    vbool8_t mask = vgts_vv_i32m8_mask(vec_a, vec_b, n - 1);\n    bool result = vmfirst_m_b8(mask) == 0xFFFFFFFFFFFFFFFF;\n\n    vint32m8_t vec_c = vle32_v_i32m8(lst.data(), n - 1);\n    vint32m8_t vec_d = vle32_v_i32m8(lst.data() + 2, n - 2);\n\n    vbool8_t dup_mask = veq_vv_i32m8_mask(vec_c, vec_d, n - 2);\n    bool duplicates = vmfirst_m_b8(dup_mask) != 0xFFFFFFFFFFFFFFFF;\n\n    return result && !duplicates;\n}\n```"}
{"task_id": "SimdBench_122_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    vint32m8_t vec_a = vle32_v_i32m8(lst.data(), n);\n    vint32m8_t vec_b = vle32_v_i32m8(lst.data() + 1, n - 1);\n\n    vbool8_t is_less = vlt_vv_i32m8(vec_a, vec_b);\n    vbool8_t is_not_equal = vnand_vv_i8(is_less, veq_vv_i32m8(vec_a, vec_b));\n\n    bool result = vredor_vs_u_i1(is_not_equal, vfalse_v_i8());\n    return !result;\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    size_t n = arr.size();\n    vint16m8_t v_arr = vle16_v_i16m8(arr.data(), n);\n    vbool16_t v_mask = vmseq_vx_i16m8_b16(v_arr, 0);\n\n    int sign_product = 1;\n    for (size_t i = 0; i < n; ++i) {\n        if (arr[i] < 0) {\n            sign_product *= -1;\n        } else if (arr[i] > 0) {\n            sign_product *= 1;\n        }\n    }\n\n    vint16m8_t v_abs_arr = vabs_v_i16m8(v_arr);\n    vint32m1_t v_sum_abs = vwredsum_vs_i16m8_i32m1(v_abs_arr, 0, v_mask);\n\n    return static_cast<int>(sign_product * v_sum_abs.v[0]);\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    size_t n = arr.size();\n    vint16m8_t v_arr = vle16_v_i16m8(arr.data(), n);\n    vbool16_t v_mask = vmseq_vx_i16m8(v_arr, 0);\n    int zero_count = vpopcnt_b_u8(vmfirst_m_b16(v_mask));\n\n    if (zero_count > 0) {\n        return 0;\n    }\n\n    vint16m8_t v_abs_arr = vabs_v_i16m8(v_arr);\n    vint32m1_t v_sum_abs = vwredsum_vs_i16m8_i32m1(v_abs_arr, 0);\n\n    vint16m8_t v_signs = vsext_vf16_i16m8(vsign_v_i16m8(v_arr), 16);\n    vint32m1_t v_prod_signs = vwredprod_vs_i16m8_i32m1(v_signs, 1);\n\n    return vle32_v_f32m1(&v_prod_signs)[0] * vle32_v_f32m1(&v_sum_abs)[0];\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    size_t n = arr.size();\n    vint32m1_t vec = vle32_v_i32m1(arr.data(), n);\n    vbool1_t mask = vmseq_vx_i32m1_b1(vec, 0);\n\n    int sum_magnitudes = vredsum_vs_i32m1_i32m1_i32m1(vabs_v_i32m1(vec), vec, 0, vsetvl_e32m1(n));\n    int count_zeros = vpopcnt_mb_mask_b1(mask);\n    int sign_product = vredand_vs_i32m1_i32m1_i32m1(vsign_v_i32m1(vec), vec, 0, vsetvl_e32m1(n));\n\n    if (count_zeros > 0) {\n        return 0;\n    } else {\n        return sum_magnitudes * sign_product;\n    }\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    size_t n = arr.size();\n    vint32m1_t v_arr = vle32_v_i32m1(arr.data(), n);\n    vbool1_t mask = vmseq_vx_i32m1_b1(v_arr, 0);\n\n    int sum_magnitudes = vfredosum_vs_i32m1_i32m1(v_srai_vx_i32m1(v_abs_v_i32m1(v_arr), 31), v_arr, v_setvl_e32m1(n));\n    int sign_product = vredand_vs_i32m1_i32m1(v_sub_vv_i32m1(v_and_vv_i32m1(v_arr, mask), v_neg_v_i32m1(v_and_vv_i32m1(v_arr, mask))), v_setvl_e32m1(n));\n\n    return sum_magnitudes * sign_product;\n}\n```"}
{"task_id": "SimdBench_123_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    size_t n = arr.size();\n    vint16m1_t vec_arr = vle16_v_i16m1(arr.data(), n);\n    vbool16_t mask = vmseq_vx_i16m1_b16(vec_arr, 0);\n\n    int sign_product = 1;\n    for (size_t i = 0; i < n; ++i) {\n        if (arr[i] < 0) {\n            sign_product *= -1;\n        } else if (arr[i] == 0) {\n            sign_product = 0;\n            break;\n        }\n    }\n\n    vint16m1_t abs_vec_arr = vabs_v_i16m1(vec_arr);\n    vint32m1_t sum_abs = vredsum_vs_i16m1_i32m1(0, abs_vec_arr, mask, vsetvl_e32m1(n));\n\n    return static_cast<int>(sign_product * sum_abs);\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k);\n\n    // Initialize the first element of the result array\n    vint32m1_t v_result = vle32_v_i32m1(&grid[0][0], N * N);\n    v_result = vsub_vx_i32m1(v_result, vle32_v_i32m1(&grid[0][0], N * N), 1);\n\n    // Iterate over the remaining elements of the result array\n    for (int i = 1; i < k; ++i) {\n        vint32m1_t v_min = vle32_v_i32m1(&grid[0][0], N * N);\n        v_min = vadd_vx_i32m1(v_min, vle32_v_i32m1(&grid[0][0], N * N), i);\n\n        v_result = vmin_vv_i32m1(v_result, v_min);\n    }\n\n    // Store the result back into the result array\n    vse32_v_i32m1(result.data(), v_result, k);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k);\n\n    // Initialize the first element of the result array\n    vint32m1_t v_result = vle32_v_i32m1(result.data(), k, vlmax_vv_i32m1(viota_v_i32m1(0, 1, k)));\n\n    // Initialize the grid data as a vector register\n    vint32m1_t v_grid[N][N];\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            v_grid[i][j] = vle32_v_i32m1(&grid[i][j], 1, vlmax_vv_i32m1(viota_v_i32m1(0, 1, 1)));\n        }\n    }\n\n    // Perform the path finding using RVV intrinsics\n    for (int step = 1; step < k; ++step) {\n        vint32m1_t v_min_path[k];\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                vint32m1_t v_neighbors[4];\n                if (i > 0) v_neighbors[0] = v_grid[i-1][j];\n                else v_neighbors[0] = vle32_v_i32m1(&grid[N-1][j], 1, vlmax_vv_i32m1(viota_v_i32m1(0, 1, 1)));\n                if (i < N-1) v_neighbors[1] = v_grid[i+1][j];\n                else v_neighbors[1] = vle32_v_i32m1(&grid[0][j], 1, vlmax_vv_i32m1(viota_v_i32m1(0, 1, 1)));\n                if (j > 0) v_neighbors[2] = v_grid[i][j-1];\n                else v_neighbors[2] = vle32_v_i32m1(&grid[i][N-1], 1, vlmax_vv_i32m1(viota_v_i32m1(0, 1, 1)));\n                if (j < N-1) v_neighbors[3] = v_grid[i][j+1];\n                else v_neighbors[3] = vle32_v_i32m1(&grid[i][0], 1, vlmax_vv_i32m1(viota_v_i32m1(0, 1, 1)));\n\n                vint32m1_t v_current_value = v_grid[i][j];\n                vint32m1_t v_min_neighbor = vmin_vv_i32m1(v_neighbors[0], v_neighbors[1]);\n                v_min_neighbor = vmin_vv_i32m1(v_min_neighbor, v_neighbors[2]);\n                v_min_neighbor = vmin_vv_i32m1(v_min_neighbor, v_neighbors[3]);\n\n                vint32m1_t v_new_value = vadd_vv_i32m1(v_current_value, v_min_neighbor);\n                v_result = vmerge_vvm_i32m1(v_result, v_new_value, vle32_v_i32m1(&result[step-1], 1, vlmax_vv_i32m1(viota_v_i32m1(0, 1, 1))));\n            }\n        }\n    }\n\n    // Store the final result\n    for (int i = 0; i < k; ++i) {\n        result[i] = vfirst_m_i32m1(v_result);\n        v_result = vslli_vx_i32m1(v_result, 32);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k);\n    \n    // Initialize the first element of the result array\n    vint32m1_t v_result = vle32_v_i32m1(result.data(), k, vlmax_vv_i32m1(viota_v_i32m1(0, 1, k, vlmax_vv_i32m1())));\n    \n    // Perform the path finding using RVV intrinsics\n    for (int i = 1; i < k; ++i) {\n        vint32m1_t v_grid = vle32_v_i32m1(grid[i].data(), N, vlmax_vv_i32m1());\n        v_result = vmin_vv_i32m1(v_result, v_grid, vlmax_vv_i32m1());\n    }\n    \n    // Store the result back to the result array\n    vse32_v_i32m1(result.data(), v_result, k, vlmax_vv_i32m1());\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your RVV implementation here\n}\n```"}
{"task_id": "SimdBench_124_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k);\n    \n    // Initialize the first element of the result array\n    vint32m1_t v_result = vle32_v_i32m1(&grid[0][0], N * N);\n    v_result = vsub_vx_i32m1(v_result, vle32_v_i32m1(&grid[0][0], N * N), 1);\n    \n    // Iterate over the remaining elements of the result array\n    for (int i = 1; i < k; ++i) {\n        vint32m1_t v_min = vle32_v_i32m1(&grid[0][0], N * N);\n        v_min = vadd_vx_i32m1(v_min, vle32_v_i32m1(&grid[0][0], N * N), i);\n        \n        v_result = vmin_vv_i32m1(v_result, v_min);\n    }\n    \n    // Store the result back into the result array\n    vse32_v_i32m1(result.data(), v_result, k);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t digits_simd(uint64_t n) {\n    vuint8m1_t v_n = vle8_v_u8m1((const uint8_t*)&n, 8);\n    vbool8_t mask_odd = vmseq_vx_u8m1_b8(v_n, 1);\n    vuint8m1_t v_odd = vcompress_vm_u8m1_u8m1(mask_odd, v_n, v_n);\n    vuint64m1_t v_product = vredsum_vs_u8u64m1_u64(v_odd, 1, v_setvl_e64m1(1));\n    return v_product;\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t digits_simd(uint64_t n) {\n    vuint8m1_t v_n = vle8_v_u8m1((const uint8_t*)&n, 8);\n    vbool8_t is_odd = vmseq_vx_u8m1_b8(v_n, 1, 8);\n    vuint8m1_t odd_digits = vand_vvm_u8m1(v_n, is_odd, 8);\n    vuint64m1_t prod = vredsum_vs_u8u64m1(0, odd_digits, is_odd, 8);\n    return vfirst_m_u64(prod, 1);\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t digits_simd(uint64_t n) {\n    vuint8m1_t v_n = vle8_v_u8m1((const uint8_t*)&n, 8);\n    vbool8_t is_odd = vmseq_vx_u8m1_b8(v_n, 1, 8);\n    vuint8m1_t odd_digits = vand_vvm_u8m1(v_n, is_odd, 8);\n    vuint64m1_t prod = vredsum_vs_u8u64m1(0, odd_digits, is_odd, 8);\n    return vfirst_m_u64(prod, 1);\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t digits_simd(uint64_t n) {\n    vuint8m1_t v_n = vle8_v_u8m1((const uint8_t*)&n, 8);\n    vbool8_t mask = vmseq_vx_b8(v_n, 1, 8);\n    vuint8m1_t v_odd = vand_vvm_u8(v_n, mask, 8);\n    vuint64m1_t v_prod = vredsum_vs_u8u64m1(0, v_odd, v_odd, 8);\n    return vle64_v_u64m1(&v_prod, 1)[0];\n}\n```"}
{"task_id": "SimdBench_125_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t digits_simd(uint64_t n) {\n    vuint8m1_t v_n = vle8_v_u8m1((const uint8_t*)&n, 8);\n    vbool8_t is_odd = vmseq_vx_u8m1_b8(v_n, 1, 8);\n    vuint8m1_t odd_digits = vand_vvm_u8m1(v_n, is_odd, 8);\n    vuint64m1_t prod = vredsum_vs_u8u64m1(0, odd_digits, is_odd, 8);\n    return vfirst_m_u64(prod, 1);\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    uint64_t sum = 0;\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vfloat32m1_t vec = vle32_v_f32m1(lst.data() + i, vl);\n        vfloat32m1_t ceil_vec = vfcvt_r_ceil_f32m1_f32m1(vec, vl);\n        vfloat32m1_t square_vec = vmul_vv_f32m1(ceil_vec, ceil_vec, vl);\n        vint32m1_t int_vec = vfncvt_x_fmv_i32m1_f32m1(square_vec, vl);\n        vint64m1_t sum_vec = vwredsum_vs_i64m1_i32m1_i32m1(sum, int_vec, sum_vec, vl);\n        sum += vfirst_m_i64m1(sum_vec);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    uint64_t sum = 0;\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vfloat32m1_t vec = vle32_v_f32m1(lst.data() + i, vl);\n        vfloat32m1_t ceil_vec = vfcvt_r_ceil_f32m1_f32m1(vec, vl);\n        vfloat32m1_t square_vec = vmul_vv_f32m1(ceil_vec, ceil_vec, vl);\n        vint32m1_t sum_vec = vwredsum_vs_i32m1_i32m1(viota_v_i32m1(vl), vreinterpret_v_f32m1_i32m1(square_vec), vl);\n        sum += viwredsum_vs_u64m1_u32m1(sum, vreinterpret_v_u32m1_i32m1(sum_vec), vl);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    uint64_t sum = 0;\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vfloat32m1_t vec = vle32_v_f32m1(lst.data() + i, vl);\n        vfloat32m1_t ceil_vec = vfceil_v_f32m1(vec, vl);\n        vfloat32m1_t sq_vec = vmul_vv_f32m1(ceil_vec, ceil_vec, vl);\n        vint32m1_t sq_int_vec = vfcvt_xu_v_i32m1(sq_vec, vl);\n        vint64m1_t sum_vec = vwredsum_vs_i32m1_i64m1(sum, sq_int_vec, sum, vl);\n        sum = vfirst_m_i64m1(sum_vec);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    vfloat32m1_t v_lst = vle32_v_f32m1(lst.data(), vlmax);\n    vfloat32m1_t v_ceiled = vfcvt_r_ceil_f32m1_f32m1(v_lst, vlmax);\n    vfloat32m1_t v_squared = vmul_vv_f32m1(v_ceiled, v_ceiled, vlmax);\n    vint32m1_t v_sum = vwredsum_vs_i32m1_i32m1(0, v_squared, vlmax);\n    uint64_t result = vfirst_m_i32m1(v_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    size_t vlmax = vsetvl_e32m1(n);\n    vfloat32m1_t v_lst = vle32_v_f32m1(lst.data(), vlmax);\n    vfloat32m1_t v_ceiled = vfcvt_r_ceil_f32m1_f32m1(v_lst, vlmax);\n    vfloat32m1_t v_squared = vmul_vv_f32m1(v_ceiled, v_ceiled, vlmax);\n    vint32m1_t v_sum = vwredsum_vs_i32m1_i32m1(0, v_squared, vlmax);\n    uint64_t result = vfirst_m_b32(v_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    vint32m8_t vec = vle32_v_i32m8(arr.data(), n);\n    vbool8_t mask = vmul_vv_m_b8(vle32_v_i32m8(arr.data() + 1, n - 1), vec, n - 1);\n\n    int idx = vfirst_m_b8(mask);\n    if (idx == -1) return -1;\n    else return idx + 1;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    vint32m8_t vec = vle32_v_i32m8(arr.data(), n);\n    vbool8_t mask = vmul_vv_m_b8(vle32_v_i32m8(arr.data() + 1, n), vec, n);\n\n    int idx = -1;\n    for (int i = 0; i < n - 1; ++i) {\n        if (!mask[i]) {\n            idx = i;\n            break;\n        }\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    vint32m8_t vec = vle32_v_i32m8(arr.data(), n);\n    vbool8_t mask = vmul_vv_m_b8(vle32_v_i32m8(arr.data() + 1, n - 1), vec, n - 1);\n    vint32m8_t max_val = vmax_vv_i32m8(vec, vmul_vv_m_b8(vec, vec, n - 1), n - 1);\n    vint32m8_t idx = vfirst_m_b8(mask);\n\n    return idx == -1 ? -1 : idx;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    vint32m8_t vec = vle32_v_i32m8(arr.data(), n);\n    vbool8_t mask = vmul_vv_m_b8(vle32_v_i32m8(arr.data() + 1, n), vec, n);\n\n    int idx = vfirst_m_b8(mask);\n    return idx == -1 ? -1 : idx;\n}\n```"}
{"task_id": "SimdBench_127_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    vint32m8_t vec = vle32_v_i32m8(arr.data(), n);\n    vbool8_t mask = vmul_gt_vv_i32m8(vec, vle32_v_i32m8(arr.data() + 1, n));\n    vuint32m8_t indices = vlset_v_u32m8(0, n);\n\n    vuint32m8_t max_index = vredmax_vs_u32m8_u32m8(vmv_v_x_u32m8(-1), indices, mask);\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    size_t n = lst.size();\n    vint32m8_t vec = vle32_v_i32m8(lst.data(), n);\n\n    vbool8_t mask_neg = vmsltu_vx_i32m8_b8(vec, 0);\n    vbool8_t mask_pos = vmgtu_vx_i32m8_b8(vec, 0);\n\n    int max_neg = vfredmax_vs_i32m8_i32m1(mask_neg, vec, 0);\n    int min_pos = vfredmin_vs_i32m8_i32m1(mask_pos, vec, 0);\n\n    if (!mask_neg[0]) {\n        max_neg = 0;\n    }\n    if (!mask_pos[0]) {\n        min_pos = 0;\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    size_t n = lst.size();\n    vint32m8_t vec = vle32_v_i32m8(lst.data(), n);\n\n    vbool8_t mask_neg = vmsltu_vx_i32m8_b8(vec, 0);\n    vbool8_t mask_pos = vmgtu_vx_i32m8_b8(vec, 0);\n\n    int max_neg = vfredmax_vs_i32m8_i32m1(vec, mask_neg, 0);\n    int min_pos = vfredmin_vs_i32m8_i32m1(vec, mask_pos, 0);\n\n    if (max_neg == INT_MIN) {\n        max_neg = 0;\n    }\n    if (min_pos == INT_MAX) {\n        min_pos = 0;\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    size_t n = lst.size();\n    vint32m8_t vec = vle32_v_i32m8(lst.data(), n);\n\n    vbool8_t mask_neg = vmsltu_vx_i32m8_b8(vec, 0);\n    vbool8_t mask_pos = vmgtu_vx_i32m8_b8(vec, 0);\n\n    int max_neg = vfredmax_vs_i32m8_i32m1(mask_neg, vec, 0);\n    int min_pos = vfredmin_vs_i32m8_i32m1(mask_pos, vec, 0);\n\n    if (!mask_neg[0]) {\n        max_neg = 0;\n    }\n    if (!mask_pos[0]) {\n        min_pos = 0;\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    vint32m8_t vec = vle32_v_i32m8(lst.data(), lst.size());\n    vbool8_t mask_neg = vmsltu_vx_i32m8_b8(vec, 0);\n    vbool8_t mask_pos = vmgtu_vx_i32m8_b8(vec, 0);\n\n    int max_neg = vfredmax_vs_i32m8_i32m1(mask_neg, vec, 0);\n    int min_pos = vfredmin_vs_i32m8_i32m1(mask_pos, vec, 0);\n\n    if (!mask_neg[0]) {\n        max_neg = 0;\n    }\n    if (!mask_pos[0]) {\n        min_pos = 0;\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    size_t n = lst.size();\n    vint32m8_t vec = vle32_v_i32m8(lst.data(), n);\n\n    vbool8_t mask_neg = vmsltu_vx_i32m8_b8(vec, 0);\n    vbool8_t mask_pos = vmgtu_vx_i32m8_b8(vec, 0);\n\n    int max_neg = vfredmax_vs_i32m8_i32m1(vec, mask_neg, 0);\n    int min_pos = vfredmin_vs_i32m8_i32m1(vec, mask_pos, 0);\n\n    if (!mask_neg) {\n        max_neg = 0;\n    }\n    if (!mask_pos) {\n        min_pos = 0;\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    for (int i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vle64_v_u64m1(&i, 1);\n        vuint64m1_t vec_fact = vmul_vx_u64m1(vec_i, result);\n        result = vredsum_vs_u64m1_u64(result, vec_fact, vsetvl_e64m1(1));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    for (int i = 1; i <= n; i++) {\n        vuint64m1_t vec_fact = vmul_vx_u64(vle64_v_u64(&result, 1), i);\n        result = vredsum_vs_u64_u64(result, vec_fact, vsetvl_e64m1(1));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    for (int i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vle64_v_u64m1(&i, 1);\n        vuint64m1_t vec_fact = vmul_vx_u64m1(vec_i, i);\n        result *= vredsum_vs_u64m1_u64m1(result, vec_fact, vsetvl_e64m1(1));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    for (int i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vle64_v_u64m1(i, &i, 1);\n        vuint64m1_t vec_fact = vmul_vx_u64m1(vec_i, vec_i, 1);\n        result *= vredsum_vs_u64m1_u64m1(result, vec_fact, result, 1);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    uint64_t result = 1;\n    for (int i = 1; i <= n; i++) {\n        vuint64m1_t vec_i = vle64_v_u64m1(&i, 1);\n        vuint64m1_t vec_fact = vmul_vx_u64m1(vec_i, result);\n        result = vredsum_vs_u64m1_u64(result, vec_fact, vsetvl_e64m1(1));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    int sum = 0;\n    size_t vlmax = vsetvl_e32m1(vlmax_e32());\n    int32_t* vec_lst = const_cast<int32_t*>(lst.data());\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t v_vec = vle32_v_i32m1(vec_lst + i, vl);\n        vint32m1_t v_mask_3 = vmul_vx_i32m1(vle32_v_i32m1(&i, vl), 3, vl);\n        vbool8_t v_mask_3_eq = veq_vx_b8(v_mask_3, 0, vl);\n        vint32m1_t v_mask_4 = vmul_vx_i32m1(vle32_v_i32m1(&i, vl), 4, vl);\n        vbool8_t v_mask_4_neq = vnand_vv_b8(v_mask_4, v_mask_3, vl);\n\n        vint32m1_t v_result = vadd_vv_i32m1(v_vec, v_vec, vl);\n        vresult.v[0] = vresult.v[0] * vresult.v[0];\n        vresult.v[1] = vresult.v[1] * vresult.v[1];\n        vresult.v[2] = vresult.v[2] * vresult.v[2];\n        vresult.v[3] = vresult.v[3] * vresult.v[3];\n\n        vint32m1_t v_result_4 = vadd_vv_i32m1(v_vec, v_vec, vl);\n        vresult_4.v[0] = vresult_4.v[0] * vresult_4.v[0] * vresult_4.v[0];\n        vresult_4.v[1] = vresult_4.v[1] * vresult_4.v[1] * vresult_4.v[1];\n        vresult_4.v[2] = vresult_4.v[2] * vresult_4.v[2] * vresult_4.v[2];\n        vresult_4.v[3] = vresult_4.v[3] * vresult_4.v[3] * vresult_4.v[3];\n\n        vint32m1_t v_final_result = vfadd_vv_i32m1(v_result, v_result_4, vl);\n        vfinal_result.v[0] = vfinal_result.v[0] * vfinal_result.v[0];\n        vfinal_result.v[1] = vfinal_result.v[1] * vfinal_result.v[1];\n        vfinal_result.v[2] = vfinal_result.v[2] * vfinal_result.v[2];\n        vfinal_result.v[3] = vfinal_result.v[3] * vfinal_result.v[3];\n\n        vint32m1_t v_sum = vredsum_vs_i32m1_i32m1(0, v_final_result, vl);\n        sum += vget_v_i32m1(v_sum, 0);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    int sum = 0;\n    size_t vlmax = vsetvl_e32m1(vlmax_e32);\n    vint32m1_t vsum = vmv_v_x_i32m1(0, vlmax);\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        size_t vl = vsetvl_e32m1(n - i);\n        vint32m1_t vin = vle32_v_i32m1(lst.data() + i, vl);\n        \n        vint32m1_t vsq = vfmul_vv_i32m1(vin, vin, vl);\n        vint32m1_t vcub = vfmul_vv_i32m1(vin, vfadd_vv_i32m1(vin, vin, vl), vl);\n        \n        vint32m1_t mask_sq = vmseq_vx_u8m1(viota_v_u8m1(0, 1, vl), 2, vl);\n        vint32m1_t mask_cub = vmseq_vx_u8m1(viota_v_u8m1(0, 1, vl), 3, vl);\n        \n        vsq = vselect(mask_sq, vsq, vin, vl);\n        vcub = vselect(mask_cub, vcub, vin, vl);\n        \n        vsum = vwredsum_vs_i32m1_i32m1(vsum, vsq, vin, vl);\n        vsum = vwredsum_vs_i32m1_i32m1(vsum, vcub, vin, vl);\n    }\n\n    sum = vfirst_m_i32m1(vsum);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int n = lst.size();\n    int sum = 0;\n    size_t vlmax = vsetvl_e32m1(n);\n    vint32m1_t vlst = vle32_v_i32m1(lst.data(), vlmax);\n    vint32m1_t vsum = vmul_vv_i32m1(vlst, vlst, vlmax);\n\n    for (size_t i = vlmax; i < n; ++i) {\n        if (i % 3 == 0) {\n            vsum[i / vlmax] += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            vsum[i / vlmax] += lst[i] * lst[i] * lst[i];\n        }\n    }\n\n    for (size_t i = 0; i < vlmax; ++i) {\n        sum += vsum[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    int sum = 0;\n    size_t vlmax = vsetvl_e32m1(n);\n    vint32m1_t vlst = vle32_v_i32m1(lst.data(), vlmax);\n    vbool8_t vmask3 = vmseq_vi8(vlmax, 3);\n    vbool8_t vmask4 = vmseq_vi8(vlmax, 4);\n    vbool8_t vmask_not3 = vmnot_b8(vmask3);\n    vbool8_t vmask_not4 = vmnot_b8(vmask4);\n\n    for (size_t i = 0; i < n; i += vlmax) {\n        vint32m1_t vsquared = vwmul_vv_i32m1(vlst, vlst, vlmax);\n        vint32m1_t vcubed = vwmul_vv_i32m1(vlst, vwmul_vv_i32m1(vlst, vlst, vlmax), vlmax);\n        vint32m1_t vresult = vfadd_vv_i32m1(vzero_v_i32m1(vlmax),\n                                           vfadd_vv_i32m1(vselect_b8(vmask3, vsquared, vzero_v_i32m1(vlmax)),\n                                                          vselect_b8(vand_b8(vmask4, vmask_not3), vcubed, vzero_v_i32m1(vlmax))),\n                                           vlmax);\n        sum += vredsum_vs_i32_i32m1(sum, vresult, vone_i32m1());\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int n = lst.size();\n    int sum = 0;\n    size_t vlmax = vsetvl_e32m8(n);\n    vint32m8_t vlst = vle32_v_i32m8(lst.data(), vlmax);\n    vint32m8_t vsum = vmul_vv_i32m8(vlst, vlst, vlmax);\n\n    for (size_t i = vlmax; i < n; ++i) {\n        if (i % 3 == 0) {\n            vsum[i / vlmax] += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            vsum[i / vlmax] += lst[i] * lst[i] * lst[i];\n        }\n    }\n\n    for (size_t i = 0; i < vlmax; ++i) {\n        sum += vsum[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    vint32m8_t v_nums = vle32_v_i32m8(nums.data(), n);\n    vbool8_t v_mask = vmseq_vx_i32m8(v_nums, 10);\n\n    vint32m8_t v_first_digit = vsub_vx_i32m8(v_nums, vrem_vx_i32m8(v_nums, 10));\n    vint32m8_t v_last_digit = vrem_vx_i32m8(v_nums, 10);\n    vlast_v_i32m8(&v_last_digit, v_nums);\n\n    vbool8_t v_odd_first_digit = vmseq_vx_i32m8(vand_vx_i32m8(v_first_digit, 1), 1);\n    vbool8_t v_odd_last_digit = vmseq_vx_i32m8(vand_vx_i32m8(v_last_digit, 1), 1);\n\n    vbool8_t v_final_mask = vbland_vv_b8(v_mask, v_and_vv_b8(v_odd_first_digit, v_odd_last_digit));\n\n    int count = vpopcnt_m_b8(n, v_final_mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    vint32m8_t v_nums = vle32_v_i32m8(nums.data(), n);\n    vbool8_t mask = vmseq_vx_i32m8(v_nums, 10);\n\n    vint32m8_t first_digit = vdivu_vx_i32m8(v_nums, 1000000000);\n    vint32m8_t last_digit = vremu_vx_i32m8(v_nums, 10);\n    vbool8_t first_odd = vmseq_vx_i32m8(first_digit, 1 | 3 | 5 | 7 | 9);\n    vbool8_t last_odd = vmseq_vx_i32m8(last_digit, 1 | 3 | 5 | 7 | 9);\n\n    vbool8_t combined_mask = vbland_vv_b8(mask, vbland_vv_b8(first_odd, last_odd));\n\n    int count = vpopcnt_m_b8(combined_mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    vint32m8_t v_nums = vle32_v_i32m8(nums.data(), n);\n    vbool8_t mask = vmseq_vx_i32m8(v_nums, 10);\n\n    vint32m8_t first_digits = vsub_vx_i32m8(v_nums, vrem_vx_i32m8(v_nums, 10));\n    vint32m8_t last_digits = vrem_vx_i32m8(v_nums, 10);\n\n    vbool8_t first_odd = vmor_vs_b_i8(vmseq_vx_i8(first_digits, 1), vmseq_vx_i8(first_digits, 3));\n    vbool8_t first_odd_and_last_odd = vmand_vs_b_b(first_odd, vmseq_vx_i8(last_digits, 1));\n\n    vbool8_t final_mask = vmand_vs_b_b(mask, first_odd_and_last_odd);\n\n    int count = vpopcnt_m_b(final_mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    vint32m8_t v_nums = vle32_v_i32m8(nums.data(), n);\n    vbool8_t mask = vmseq_vx_i32m8(v_nums, 10);\n\n    vint32m8_t first_digits = vand_vx_i32m8(v_nums, 0xF);\n    vint32m8_t last_digits = vsra_vx_i32m8(v_nums, 26, 26);\n    vbool8_t first_odd = vor_vv_b8(vmseq_vx_u8(first_digits, 1), vmseq_vx_u8(first_digits, 3));\n    vbool8_t last_odd = vor_vv_b8(vmseq_vx_u8(last_digits, 1), vmseq_vx_u8(last_digits, 3));\n\n    vbool8_t combined_mask = vand_vv_b8(mask, vand_vv_b8(first_odd, last_odd));\n    int count = vpopcnt_m_b(combined_mask, n);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    vint32m8_t v_nums = vle32_v_i32m8(nums.data(), n);\n    vbool8_t mask = vmseq_vx_i32m8(v_nums, 10);\n\n    vint32m8_t first_digit = vdivu_vx_i32m8(v_nums, 1000000000);\n    vint32m8_t last_digit = vremu_vx_i32m8(v_nums, 10);\n    vbool8_t first_odd = vmseq_vx_i32m8(first_digit, 1 | 3 | 5 | 7 | 9);\n    vbool8_t last_odd = vmseq_vx_i32m8(last_digit, 1 | 3 | 5 | 7 | 9);\n\n    vbool8_t combined_mask = vbland_vv_b8(mask, vbland_vv_b8(first_odd, last_odd));\n\n    int count = vpopcnt_m_b8(combined_mask);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    vint64m8_t va = vmul_vx_i64m8(vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)), 2);\n    va = vsub_vx_i64m8(va, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n    va = vadd_vx_i64m8(va, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n\n    vint64m8_t vb = vmul_vx_i64m8(vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)), 2);\n    vb = vsub_vx_i64m8(vb, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n    vb = vadd_vx_i64m8(vb, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n\n    vint64m8_t vc = vmul_vx_i64m8(vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)), 2);\n    vc = vsub_vx_i64m8(vc, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n    vc = vadd_vx_i64m8(vc, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n\n    vbool8_t mask = vmseq_vv_b8(va, vb);\n    mask = vmseq_vv_b8(mask, vc);\n\n    return vpopcnt_m_b8(mask, vlmax_v_e64m8(n));\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    vint64m8_t va = vle64_v_i64m8((const int64_t *)NULL, n);\n    va = vmul_vx_i64m8(va, va, 2);\n    va = vsub_vx_i64m8(va, va, 1);\n    va = vadd_vx_i64m8(va, va, 1);\n\n    vbool8_t vb = vmseq_vx_i64m8(va, 0, 2);\n    vint64m8_t vc = vand_vv_i64m8(va, va, vb);\n\n    vint64m8_t vd = vredsum_vs_i64m8_i64m8(0, vc, vl);\n\n    return vd[0];\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    vint64m8_t va = vlset_v_i64m8(0, n);\n    vint64m8_t vb = vmul_vx_i64m8(va, 2);\n    vint64m8_t vc = vsub_vx_i64m8(vb, 1);\n    vint64m8_t vd = vadd_vv_i64m8(va, vc);\n\n    vint64m8_t ve = vrem_vx_i64m8(vd, 3);\n    vbool8_t vf = vceq_vx_i64m8(ve, 0);\n\n    vint64m8_t vg = vpopcnt_m_b8(vf);\n    uint64_t result = vfirst_m_b8(vg);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    vint64m8_t va = vmul_vx_i64m8(vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)), 2);\n    va = vsub_vx_i64m8(va, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n    va = vadd_vx_i64m8(va, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n\n    vint64m8_t vb = vmul_vx_i64m8(vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)), 2);\n    vb = vsub_vx_i64m8(vb, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n    vb = vadd_vx_i64m8(vb, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n\n    vint64m8_t vc = vmul_vx_i64m8(vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)), 2);\n    vc = vsub_vx_i64m8(vc, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n    vc = vadd_vx_i64m8(vc, vle64_v_i64m8(0, &n, vlmax_v_e64m8(n)));\n\n    vbool8_t mask = vmul_vv_m_b8(va, vb, vc);\n    vint64m8_t sum = vfredosum_vs_i64m8_i64m8(0, va, mask);\n\n    return vget_v_i64m8(sum, 0);\n}\n```"}
{"task_id": "SimdBench_132_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    vint64m8_t va = vle64_v_i64m8((const int64_t *)NULL, n);\n    va = vmul_vx_i64m8(va, va, 2);\n    va = vsub_vx_i64m8(va, va, 1);\n    va = vadd_vx_i64m8(va, va, 1);\n\n    vbool8_t vb = vmseq_vx_i64m8(va, 0, 2);\n    vint64m8_t vc = vreinterpret_v_f64m8(vb);\n    vint64m8_t vd = vwmacc_vvm_i64m8(0, vc, vc, vc);\n\n    return vredsum_vs_i64m8_i64m1(vd, 0, NULL);\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    vbool8_t mask = vmseq_vx_i8m1(n, 2);\n    vuint8_t v_n = vmul_vx_u8m1(n, 1);\n    vuint8_t v_2 = vmul_vx_u8m1(2, 1);\n    vuint8_t v_3 = vmul_vx_u8m1(3, 1);\n    vuint8_t v_4 = vmul_vx_u8m1(4, 1);\n    vuint8_t v_5 = vmul_vx_u8m1(5, 1);\n    vuint8_t v_6 = vmul_vx_u8m1(6, 1);\n    vuint8_t v_7 = vmul_vx_u8m1(7, 1);\n    vuint8_t v_8 = vmul_vx_u8m1(8, 1);\n    vuint8_t v_9 = vmul_vx_u8m1(9, 1);\n    vuint8_t v_10 = vmul_vx_u8m1(10, 1);\n\n    vuint8_t v_remainders[10];\n    v_remainders[0] = vmul_vx_u8m1(v_n, 1);\n    v_remainders[1] = vmul_vx_u8m1(v_n, 2);\n    v_remainders[2] = vmul_vx_u8m1(v_n, 3);\n    v_remainders[3] = vmul_vx_u8m1(v_n, 4);\n    v_remainders[4] = vmul_vx_u8m1(v_n, 5);\n    v_remainders[5] = vmul_vx_u8m1(v_n, 6);\n    v_remainders[6] = vmul_vx_u8m1(v_n, 7);\n    v_remainders[7] = vmul_vx_u8m1(v_n, 8);\n    v_remainders[8] = vmul_vx_u8m1(v_n, 9);\n    v_remainders[9] = vmul_vx_u8m1(v_n, 10);\n\n    vbool8_t v_mask = vmseq_vv_u8m1(v_remainders[0], v_0);\n    for (int i = 1; i < 10; i++) {\n        v_mask = vor_vv_b8(v_mask, vmseq_vv_u8m1(v_remainders[i], v_0));\n    }\n\n    vbool8_t v_is_prime = vmseq_vx_i8m1(n, 2);\n    for (int i = 3; i <= 10; i += 2) {\n        v_is_prime = vand_vv_b8(v_is_prime, vmseq_vv_u8m1(v_remainders[i], v_0));\n    }\n\n    return v_is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    vbool8_t mask = vmseq_vx_i8m1(n, 2);\n    vuint8_t vn = vle8_v_i8m1(&n, 1);\n    vuint8_t vx = vle8_v_i8m1(&x, 1);\n    vuint8_t vy = vle8_v_i8m1(&y, 1);\n    vuint8_t result = vselect_vvm_i8(mask, vx, vy);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    vbool8_t mask = vmseq_vx_i8m1(n, 2);\n    vuint8_t vn = vle8_v_i8m1(&n, 1);\n    vuint8_t vx = vle8_v_i8m1(&x, 1);\n    vuint8_t vy = vle8_v_i8m1(&y, 1);\n    vuint8_t result = vselect_vvm_i8(mask, vy, vx);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    vbool8_t mask = vmseq_vx_i8m1(n, 2);\n    vuint8_t result = vsetvl_e8m1(1);\n    result = vle8_v_i8m1(result, (const int8_t*)&n, 0);\n    result = vmul_vx_u8m1(result, result, 2);\n    result = vsub_vv_u8m1(result, result, vle8_v_i8m1(vsetvl_e8m1(1), (const int8_t*)&n, 0));\n    result = vmul_vx_u8m1(result, result, n);\n    result = vadd_vx_u8m1(result, result, 1);\n    result = vand_vv_u8m1(result, result, vmseq_vx_u8m1(result, 0));\n    result = vnot_v_b8(mask, result);\n    result = vselect_vvm_u8(result, vle8_v_i8m1(vsetvl_e8m1(1), (const int8_t*)&y, 0), vle8_v_i8m1(vsetvl_e8m1(1), (const int8_t*)&x, 0));\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_133_RVV", "completion": "```c\n#include <riscv_vector.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    vbool8_t mask = vmseq_vx_i8m1(n, 2);\n    vuint8_t v_n = vle8_v_u8(&n, 1);\n    vuint8_t v_x = vle8_v_u8(&x, 1);\n    vuint8_t v_y = vle8_v_u8(&y, 1);\n    vuint8_t result = vselect_vvm_u8(mask, v_x, v_y);\n    return vfirst_m(result);\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    vfloat32m1_t v_lst = vle32_v_f32m1(lst.data(), n);\n    vbool1_t v_mask = vmseq_vf_b1(v_lst, 0.0f); // Mask out non-positive numbers\n    v_lst = vand_vv_f32m1(v_lst, v_mask);\n\n    vint32m1_t v_int_lst = vfcti32_v_f32m1(v_lst, 0); // Convert to integer\n    vbool1_t v_is_odd = vmremu_vx_i32m1_b1(v_int_lst, 2); // Check for odd numbers\n\n    vint32m1_t v_squares = vsmul_vv_i32m1(v_int_lst, v_int_lst, v_is_odd); // Square odd numbers\n    vint64m1_t v_sum = vsadav_vs_i64m1_i32m1(0, v_squares, v_squares); // Sum squares\n\n    uint64_t result = vfirst_m_i64m1(v_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    vfloat32m1_t v_lst = vle32_v_f32m1(lst.data(), n);\n    vbool1_t v_mask = vmseq_v_f32m1_b1(v_lst, vfmul_vf_f32m1(v_lst, v_lst));\n    vint32m1_t v_sum = vfredosum_vs_i32m1_i32m1(vint32m1_t(0), v_lst, v_mask);\n\n    return vsum;\n}\n```"}
{"task_id": "SimdBench_134_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    vfloat32m1_t v_lst = vle32_v_f32m1(lst.data(), n);\n    vbool1_t v_mask = vmseq_v_f32m1_b1(v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v"}
{"task_id": "SimdBench_134_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    vfloat32m1_t v_lst = vle32_v_f32m1(lst.data(), n);\n    vbool1_t v_mask = vmseq_v_f32m1_b1(v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v_lst, v_lst, vfmul_vf_f32m1(v"}
{"task_id": "SimdBench_134_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    size_t n = lst.size();\n    if (n == 0) return 0;\n\n    vfloat32m1_t v_lst = vle32_v_f32m1(lst.data(), n);\n    vbool1_t v_mask = vmseq_v_f32m1_b1(v_lst, vfmul_vf_f32m1(v_lst, v_lst));\n    vint32m1_t v_sum = vfredosum_vs_i32m1_i32m1(v_sum, v_lst, v_mask);\n\n    return vsum;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    vint32m8_t v_game = vle32_v_i32m8(game.data(), n);\n    vint32m8_t v_guess = vle32_v_i32m8(guess.data(), n);\n    vint32m8_t v_diff = vsub_vv_i32m8(v_game, v_guess);\n    vint32m8_t v_abs_diff = vabs_v_i32m8(v_diff);\n\n    vse32_v_i32m8(result.data(), v_abs_diff, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    vint32m8_t v_game = vle32_v_i32m8(game.data(), n);\n    vint32m8_t v_guess = vle32_v_i32m8(guess.data(), n);\n    vint32m8_t v_diff = vsub_vv_i32m8(v_game, v_guess);\n    vint32m8_t v_abs_diff = vmax_vv_i32m8(vabs_v_i32m8(v_diff), vabs_v_i32m8(vdiff));\n    vse32_v_i32m8(result.data(), v_abs_diff, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    vint32m8_t v_game = vle32_v_i32m8(game.data(), n);\n    vint32m8_t v_guess = vle32_v_i32m8(guess.data(), n);\n    vint32m8_t v_diff = vsub_vv_i32m8(v_game, v_guess);\n    vint32m8_t v_abs_diff = vabs_v_i32m8(v_diff);\n\n    vse32_v_i32m8(result.data(), v_abs_diff, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    vint32m8_t v_game = vle32_v_i32m8(game.data(), n);\n    vint32m8_t v_guess = vle32_v_i32m8(guess.data(), n);\n    vint32m8_t v_diff = vsub_vv_i32m8(v_game, v_guess);\n    vint32m8_t v_abs_diff = vabs_v_i32m8(v_diff);\n    vse32_v_i32m8(result.data(), v_abs_diff, n);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_RVV", "completion": "```cpp\n#include <riscv_vector.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    vint32m8_t v_game = vle32_v_i32m8(game.data(), n);\n    vint32m8_t v_guess = vle32_v_i32m8(guess.data(), n);\n    vint32m8_t v_diff = vsub_vv_i32m8(v_game, v_guess);\n    vint32m8_t v_abs_diff = vmax_vv_i32m8(vabs_v_i32m8(v_diff), vabs_v_i32m8(vdiff));\n\n    vse32_v_i32m8(result.data(), v_abs_diff, n);\n\n    return result;\n}\n```"}

{"task_id": "SimdBench_0_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m128i *src_ptr = (__m128i *)src;\n    __m128i *dst_ptr = (__m128i *)dst;\n    __m128i *mask_ptr = (__m128i *)mask;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m128i src_vec = _mm_load_si128(src_ptr + i);\n        __m128i mask_vec = _mm_load_si128(mask_ptr + i);\n\n        // Convert mask to integer mask\n        __m128i mask_int = _mm_and_si128(mask_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Perform conditional move\n        __m128i result = _mm_blendv_epi64(_mm_setzero_si128(), src_vec, mask_int);\n\n        _mm_store_si128(dst_ptr + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m128i *src_ptr = (__m128i *)src;\n    __m128i *dst_ptr = (__m128i *)dst;\n    __m128i *mask_ptr = (__m128i *)mask;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m128i src_vec = _mm_load_si128(src_ptr + i);\n        __m128i mask_vec = _mm_load_si128(mask_ptr + i);\n\n        // Convert mask to integer mask\n        __m128i mask_int = _mm_and_si128(mask_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Perform conditional move\n        __m128i result = _mm_blendv_epi64(_mm_setzero_si128(), src_vec, mask_int);\n\n        _mm_store_si128(dst_ptr + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m128i *src_ptr = (__m128i *)src;\n    __m128i *dst_ptr = (__m128i *)dst;\n    __m128i *mask_ptr = (__m128i *)mask;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m128i src_vec = _mm_load_si128(src_ptr + i);\n        __m128i mask_vec = _mm_load_si128(mask_ptr + i);\n\n        // Convert mask to integer mask\n        __m128i mask_int = _mm_and_si128(mask_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Perform conditional move\n        __m128i result = _mm_blendv_epi64(_mm_setzero_si128(), src_vec, mask_int);\n\n        _mm_store_si128(dst_ptr + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m128i *src_ptr = (__m128i *)src;\n    __m128i *dst_ptr = (__m128i *)dst;\n    __m128i *mask_ptr = (__m128i *)mask;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m128i src_vec = _mm_load_si128(src_ptr + i);\n        __m128i mask_vec = _mm_load_si128(mask_ptr + i);\n\n        // Convert mask to integer mask\n        __m128i mask_int = _mm_and_si128(mask_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Perform conditional move\n        __m128i result = _mm_blendv_epi64(_mm_setzero_si128(), src_vec, mask_int);\n\n        _mm_store_si128(dst_ptr + i, result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```cpp\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m128i *src_ptr = (__m128i *)src;\n    __m128i *dst_ptr = (__m128i *)dst;\n    __m128i *mask_ptr = (__m128i *)mask;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m128i src_vec = _mm_load_si128(src_ptr + i);\n        __m128i mask_vec = _mm_load_si128(mask_ptr + i);\n\n        // Convert mask to integer mask\n        __m128i mask_int = _mm_and_si128(mask_vec, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Perform conditional move\n        __m128i result = _mm_blendv_epi64(_mm_setzero_si128(), src_vec, mask_int);\n\n        _mm_store_si128(dst_ptr + i, result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_7 = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i *)&src[i]);\n\n        // Calculate tmp1 as (element * scale) right-shifted by 3 bits\n        __m128i v_tmp1 = _mm_srai_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n\n        // Calculate tmp2 as (element * scale) left-shifted by 3 bits\n        __m128i v_tmp2 = _mm_slli_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n\n        // Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7\n        __m128i v_result = _mm_div_epi32(_mm_add_epi32(_mm_mullo_epi32(v_src, v_scale), _mm_mullo_epi32(v_tmp1, v_tmp2)), v_7);\n\n        _mm_storeu_si128((__m128i *)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = ((src[i] * scale + (src[i] * scale >> 3) * (src[i] * scale << 3)) / 7);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_7 = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i *)&src[i]);\n\n        __m128i v_tmp1 = _mm_srai_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n        __m128i v_tmp2 = _mm_slli_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n        __m128i v_result = _mm_add_epi32(_mm_mullo_epi32(v_src, v_scale), _mm_mullo_epi32(v_tmp1, v_tmp2));\n        v_result = _mm_div_epi32(v_result, v_7);\n\n        _mm_storeu_si128((__m128i *)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = ((src[i] * scale + ((src[i] * scale) >> 3) * ((src[i] * scale) << 3)) / 7);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_7 = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i v_tmp1 = _mm_srai_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n        __m128i v_tmp2 = _mm_slli_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n        __m128i v_result = _mm_add_epi32(v_src, _mm_mullo_epi32(v_tmp1, v_tmp2));\n        v_result = _mm_div_epi32(v_result, v_7);\n        _mm_storeu_si128((__m128i *)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        int tmp1 = (src[i] * scale) >> 3;\n        int tmp2 = (src[i] * scale) << 3;\n        dst[i] = (src[i] * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_7 = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i *)&src[i]);\n\n        // Calculate tmp1 as (element * scale) right-shifted by 3 bits\n        __m128i v_tmp1 = _mm_srai_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n\n        // Calculate tmp2 as (element * scale) left-shifted by 3 bits\n        __m128i v_tmp2 = _mm_slli_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n\n        // Compute the final result as: (element * scale + tmp1 * tmp2) divided by 7\n        __m128i v_result = _mm_div_epi32(_mm_add_epi32(_mm_mullo_epi32(v_src, v_scale), _mm_mullo_epi32(v_tmp1, v_tmp2)), v_7);\n\n        // Store the result in dst\n        _mm_storeu_si128((__m128i *)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        int result = (src[i] * scale + ((src[i] * scale) >> 3) * ((src[i] * scale) << 3)) / 7;\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_7 = _mm_set1_epi32(7);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i v_tmp1 = _mm_srai_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n        __m128i v_tmp2 = _mm_slli_epi32(_mm_mullo_epi32(v_src, v_scale), 3);\n        __m128i v_result = _mm_add_epi32(v_src, _mm_mullo_epi32(v_tmp1, v_tmp2));\n        v_result = _mm_div_epi32(v_result, v_7);\n        _mm_storeu_si128((__m128i *)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        int tmp1 = (src[i] * scale) >> 3;\n        int tmp2 = (src[i] * scale) << 3;\n        dst[i] = (src[i] * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```cpp\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m128d v;\n    for (size_t r = 0; r < rows; ++r) {\n        v = _mm_loadu_pd(src + r * cols + stride);\n        _mm_storeu_pd(dst + r, v);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```cpp\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m128d v;\n    for (size_t r = 0; r < rows; ++r) {\n        v = _mm_loadu_pd(&src[r * cols + stride]);\n        _mm_storeu_pd(&dst[r], v);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```cpp\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m128d v;\n    for (size_t r = 0; r < rows; ++r) {\n        v = _mm_loadu_pd(src + r * cols + stride);\n        _mm_storeu_pd(dst + r, v);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```cpp\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m128d v;\n    for (size_t r = 0; r < rows; ++r) {\n        v = _mm_loadu_pd(&src[r * cols + stride]);\n        _mm_storeu_pd(&dst[r], v);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```cpp\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m128d v;\n    for (size_t r = 0; r < rows; ++r) {\n        v = _mm_loadu_pd(src + r * cols + stride);\n        _mm_storeu_pd(dst + r, v);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m128 v0, v1, v2, v3;\n    __m128i idx0, idx1, idx2, idx3;\n\n    for (size_t i = 0; i < length; i += 4) {\n        idx0 = _mm_loadu_si128((__m128i*)&indices[i]);\n        idx1 = _mm_loadu_si128((__m128i*)&indices[i + 4]);\n        idx2 = _mm_loadu_si128((__m128i*)&indices[i + 8]);\n        idx3 = _mm_loadu_si128((__m128i*)&indices[i + 12]);\n\n        v0 = _mm_loadu_ps(&src[idx0.m128i_i32[0]]);\n        v1 = _mm_loadu_ps(&src[idx1.m128i_i32[0]]);\n        v2 = _mm_loadu_ps(&src[idx2.m128i_i32[0]]);\n        v3 = _mm_loadu_ps(&src[idx3.m128i_i32[0]]);\n\n        _mm_storeu_ps(&dst[i], v0);\n        _mm_storeu_ps(&dst[i + 4], v1);\n        _mm_storeu_ps(&dst[i + 8], v2);\n        _mm_storeu_ps(&dst[i + 12], v3);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m128 v0, v1, v2, v3;\n    __m128i idx0, idx1, idx2, idx3;\n\n    for (size_t i = 0; i < length; i += 4) {\n        idx0 = _mm_loadu_si128((__m128i *)&indices[i]);\n        idx1 = _mm_loadu_si128((__m128i *)&indices[i + 4]);\n        idx2 = _mm_loadu_si128((__m128i *)&indices[i + 8]);\n        idx3 = _mm_loadu_si128((__m128i *)&indices[i + 12]);\n\n        v0 = _mm_loadu_ps(&src[idx0.m128i_i32[0]]);\n        v1 = _mm_loadu_ps(&src[idx1.m128i_i32[0]]);\n        v2 = _mm_loadu_ps(&src[idx2.m128i_i32[0]]);\n        v3 = _mm_loadu_ps(&src[idx3.m128i_i32[0]]);\n\n        _mm_storeu_ps(&dst[i], v0);\n        _mm_storeu_ps(&dst[i + 4], v1);\n        _mm_storeu_ps(&dst[i + 8], v2);\n        _mm_storeu_ps(&dst[i + 12], v3);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m128 v0, v1, v2, v3;\n    __m128i idx0, idx1, idx2, idx3;\n\n    for (size_t i = 0; i < length; i += 4) {\n        idx0 = _mm_loadu_si128((__m128i *)&indices[i]);\n        idx1 = _mm_loadu_si128((__m128i *)&indices[i + 4]);\n        idx2 = _mm_loadu_si128((__m128i *)&indices[i + 8]);\n        idx3 = _mm_loadu_si128((__m128i *)&indices[i + 12]);\n\n        v0 = _mm_loadu_ps(&src[idx0.m128i_i32[0]]);\n        v1 = _mm_loadu_ps(&src[idx1.m128i_i32[0]]);\n        v2 = _mm_loadu_ps(&src[idx2.m128i_i32[0]]);\n        v3 = _mm_loadu_ps(&src[idx3.m128i_i32[0]]);\n\n        _mm_storeu_ps(&dst[i], v0);\n        _mm_storeu_ps(&dst[i + 4], v1);\n        _mm_storeu_ps(&dst[i + 8], v2);\n        _mm_storeu_ps(&dst[i + 12], v3);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m128 v0, v1, v2, v3;\n    __m128i idx0, idx1, idx2, idx3;\n\n    for (size_t i = 0; i < length; i += 4) {\n        idx0 = _mm_loadu_si128((__m128i *)&indices[i]);\n        idx1 = _mm_loadu_si128((__m128i *)&indices[i + 4]);\n        idx2 = _mm_loadu_si128((__m128i *)&indices[i + 8]);\n        idx3 = _mm_loadu_si128((__m128i *)&indices[i + 12]);\n\n        v0 = _mm_loadu_ps(&src[idx0.m128i_i32[0]]);\n        v1 = _mm_loadu_ps(&src[idx1.m128i_i32[0]]);\n        v2 = _mm_loadu_ps(&src[idx2.m128i_i32[0]]);\n        v3 = _mm_loadu_ps(&src[idx3.m128i_i32[0]]);\n\n        _mm_storeu_ps(&dst[i], v0);\n        _mm_storeu_ps(&dst[i + 4], v1);\n        _mm_storeu_ps(&dst[i + 8], v2);\n        _mm_storeu_ps(&dst[i + 12], v3);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m128 v0, v1, v2, v3;\n    __m128i idx0, idx1, idx2, idx3;\n\n    for (size_t i = 0; i < length; i += 4) {\n        idx0 = _mm_loadu_si128((__m128i*)&indices[i]);\n        idx1 = _mm_loadu_si128((__m128i*)&indices[i + 4]);\n        idx2 = _mm_loadu_si128((__m128i*)&indices[i + 8]);\n        idx3 = _mm_loadu_si128((__m128i*)&indices[i + 12]);\n\n        v0 = _mm_loadu_ps(&src[idx0.m128i_i32[0]]);\n        v1 = _mm_loadu_ps(&src[idx1.m128i_i32[0]]);\n        v2 = _mm_loadu_ps(&src[idx2.m128i_i32[0]]);\n        v3 = _mm_loadu_ps(&src[idx3.m128i_i32[0]]);\n\n        _mm_storeu_ps(&dst[i], v0);\n        _mm_storeu_ps(&dst[i + 4], v1);\n        _mm_storeu_ps(&dst[i + 8], v2);\n        _mm_storeu_ps(&dst[i + 12], v3);\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || (end - start + 1) % 8 != 0) return;\n\n    __m128i temp;\n    int16_t *ptr = arr + start;\n    int16_t *limit = arr + end;\n\n    while (ptr < limit) {\n        temp = _mm_load_si128((__m128i *)ptr);\n        _mm_store_si128((__m128i *)(limit - 2), temp);\n        ptr += 8;\n        limit -= 8;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (end - start + 1 <= 0) return;\n\n    __m128i temp;\n    int16_t *ptr_start = arr + start;\n    int16_t *ptr_end = arr + end;\n\n    while (ptr_start < ptr_end) {\n        // Load two elements from the start and end pointers\n        __m128i start_vec = _mm_loadu_si128((__m128i*)ptr_start);\n        __m128i end_vec = _mm_loadu_si128((__m128i*)ptr_end);\n\n        // Swap the elements\n        temp = start_vec;\n        start_vec = end_vec;\n        end_vec = temp;\n\n        // Store the swapped elements back to memory\n        _mm_storeu_si128((__m128i*)ptr_start, start_vec);\n        _mm_storeu_si128((__m128i*)ptr_end, end_vec);\n\n        // Move the pointers towards the center\n        ptr_start += 8;\n        ptr_end -= 8;\n    }\n\n    // Handle the case where the number of elements is odd\n    if (ptr_start == ptr_end) {\n        __m128i mid_vec = _mm_loadu_si128((__m128i*)ptr_start);\n        _mm_storeu_si128((__m128i*)ptr_start, mid_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || (end - start + 1) % 8 != 0) return;\n\n    __m128i temp;\n    int16_t *ptr = arr + start;\n    int16_t *limit = arr + end;\n\n    while (ptr < limit) {\n        temp = _mm_load_si128((__m128i *)ptr);\n        _mm_store_si128((__m128i *)(limit - 2), temp);\n        ptr += 8;\n        limit -= 8;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || (end - start + 1) % 8 != 0) return;\n\n    __m128i temp;\n    int16_t *ptr = arr + start;\n    int16_t *end_ptr = arr + end;\n\n    while (ptr < end_ptr) {\n        temp = _mm_loadu_si128((__m128i *)ptr);\n        _mm_storeu_si128((__m128i *)end_ptr, temp);\n\n        ptr += 8;\n        end_ptr -= 8;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    __m128i temp;\n    int16_t *ptr_start = arr + start;\n    int16_t *ptr_end = arr + end;\n\n    while (ptr_start < ptr_end) {\n        if ((ptr_end - ptr_start) >= 8) {\n            temp = _mm_loadu_si128((__m128i *)ptr_start);\n            _mm_storeu_si128((__m128i *)ptr_end, temp);\n\n            temp = _mm_loadu_si128((__m128i *)(ptr_start + 8));\n            _mm_storeu_si128((__m128i *)(ptr_end - 8), temp);\n\n            ptr_start += 16;\n            ptr_end -= 16;\n        } else {\n            // Handle remaining elements less than 8\n            for (; ptr_start < ptr_end; ++ptr_start, --ptr_end) {\n                std::swap(*ptr_start, *ptr_end);\n            }\n            break;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```cpp\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    __m128i *tensor_ptr = (__m128i *)tensor;\n    __m128i *slice_ptr = (__m128i *)slice;\n\n    size_t offset = slice_idx * dim1 * dim2;\n    size_t num_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < num_elements; i += 16) {\n        __m128i data = _mm_loadu_si128(tensor_ptr + i);\n        _mm_storeu_si128(slice_ptr + i, data);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t slice_size = dim1 * dim2;\n    const size_t start_index = slice_idx * slice_size;\n\n    for (size_t i = 0; i < slice_size; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i *)(tensor + start_index + i));\n        _mm_storeu_si128((__m128i *)(slice + i), data);\n    }\n\n    // Handle remaining elements that do not fit into a 128-bit chunk\n    for (size_t i = slice_size - slice_size % 16; i < slice_size; ++i) {\n        slice[i] = tensor[start_index + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```cpp\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    __m128i *tensor_ptr = (__m128i *)tensor;\n    __m128i *slice_ptr = (__m128i *)slice;\n\n    size_t offset = slice_idx * dim1 * dim2;\n    size_t num_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < num_elements; i += 16) {\n        __m128i data = _mm_loadu_si128(tensor_ptr + i);\n        _mm_storeu_si128(slice_ptr + i, data);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```cpp\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    __m128i *tensor_ptr = (__m128i *)tensor;\n    __m128i *slice_ptr = (__m128i *)slice;\n\n    size_t offset = slice_idx * dim1 * dim2;\n    size_t num_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < num_elements; i += 16) {\n        __m128i data = _mm_loadu_si128(tensor_ptr + i);\n        _mm_storeu_si128(slice_ptr + i, data);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```cpp\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    __m128i *tensor_ptr = (__m128i *)tensor;\n    __m128i *slice_ptr = (__m128i *)slice;\n\n    size_t elements_per_slice = dim1 * dim2;\n    size_t slice_offset = slice_idx * elements_per_slice;\n\n    for (size_t i = 0; i < elements_per_slice / 4; ++i) {\n        __m128i data = _mm_loadu_si128(tensor_ptr + i);\n        _mm_storeu_si128(slice_ptr + i, data);\n    }\n\n    // Handle remaining elements if the total number of elements is not a multiple of 4\n    size_t remainder = elements_per_slice % 4;\n    for (size_t i = 0; i < remainder; ++i) {\n        slice[slice_offset + i] = tensor[slice_offset + i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m128i zero = _mm_setzero_si128();\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    __m128i src_val = _mm_loadu_si128((__m128i*)&src[(row_block + i) * cols + col_block + j]);\n                    __m128i dst_val = _mm_loadu_si128((__m128i*)&dst[(col_block + j) * rows + row_block + i]);\n\n                    dst_val = _mm_add_epi16(dst_val, src_val);\n\n                    _mm_storeu_si128((__m128i*)&dst[(col_block + j) * rows + row_block + i], dst_val);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m128i zero = _mm_setzero_si128();\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    if (row_block + i < rows && col_block + j < cols) {\n                        __m128i val = _mm_loadu_si128((__m128i *)&src[(row_block + i) * cols + col_block + j]);\n                        _mm_storeu_si128((__m128i *)&dst[(col_block + j) * rows + row_block + i], val);\n                    } else {\n                        _mm_storeu_si128((__m128i *)&dst[(col_block + j) * rows + row_block + i], zero);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```cpp\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    __m128i v0, v1, v2, v3;\n    size_t i, j, k;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; ++k) {\n                v0 = _mm_loadu_si128((__m128i *)&src[(i + k) * cols]);\n                v1 = _mm_loadu_si128((__m128i *)&src[(i + k) * cols + 4]);\n                v2 = _mm_loadu_si128((__m128i *)&src[(i + k) * cols + 8]);\n                v3 = _mm_loadu_si128((__m128i *)&src[(i + k) * cols + 12]);\n\n                _mm_storeu_si128((__m128i *)&dst[j * rows + i + k], v0);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + i + k + 4], v1);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + i + k + 8], v2);\n                _mm_storeu_si128((__m128i *)&dst[j * rows + i + k + 12], v3);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```cpp\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    __m128i v0, v1, v2, v3;\n    for (size_t row = 0; row < rows; row += block_size) {\n        for (size_t col = 0; col < cols; col += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    v0 = _mm_loadu_si128((__m128i*)&src[(row + i) * cols + col]);\n                    v1 = _mm_loadu_si128((__m128i*)&src[(row + i) * cols + col + 4]);\n                    v2 = _mm_loadu_si128((__m128i*)&src[(row + i) * cols + col + 8]);\n                    v3 = _mm_loadu_si128((__m128i*)&src[(row + i) * cols + col + 12]);\n\n                    _mm_storeu_si128((__m128i*)&dst[(col + j) * rows + row], v0);\n                    _mm_storeu_si128((__m128i*)&dst[(col + j) * rows + row + 4], v1);\n                    _mm_storeu_si128((__m128i*)&dst[(col + j) * rows + row + 8], v2);\n                    _mm_storeu_si128((__m128i*)&dst[(col + j) * rows + row + 12], v3);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```cpp\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const __m128i zero = _mm_setzero_si128();\n\n    for (size_t row_block = 0; row_block < rows; row_block += block_size) {\n        for (size_t col_block = 0; col_block < cols; col_block += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    if (row_block + i < rows && col_block + j < cols) {\n                        __m128i val = _mm_loadu_si128((__m128i *)(src + (row_block + i) * cols + col_block + j));\n                        _mm_storeu_si128((__m128i *)(dst + (col_block + j) * rows + row_block + i), val);\n                    } else {\n                        _mm_storeu_si128((__m128i *)(dst + (col_block + j) * rows + row_block + i), zero);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n\n    while (i + 4 <= dim * dim * dim) {\n        __m128 vec = _mm_loadu_ps(&array[i]);\n        sum = _mm_add_ps(sum, vec);\n        i += 4;\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n\n    while (i + 4 <= dim * dim * dim) {\n        __m128 v = _mm_loadu_ps(&array[i]);\n        sum = _mm_add_ps(sum, v);\n        i += 4;\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n\n    while (i + 4 <= dim * dim * dim) {\n        __m128 vec = _mm_loadu_ps(&array[i]);\n        sum = _mm_add_ps(sum, vec);\n        i += 4;\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n\n    while (i + 4 <= dim * dim * dim) {\n        __m128 v = _mm_loadu_ps(&array[i]);\n        sum = _mm_add_ps(sum, v);\n        i += 4;\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n\n    return temp[0] + temp[1] + temp[2] + temp[3];\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i = 0;\n\n    while (i + 4 <= dim * dim * dim) {\n        __m128 vec = _mm_loadu_ps(&array[i]);\n        sum = _mm_add_ps(sum, vec);\n        i += 4;\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, sum);\n\n    float result = 0.0f;\n    for (int j = 0; j < 4; ++j) {\n        if (i + j < dim * dim * dim) {\n            result += array[i + j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d v_threshold = _mm_set1_pd(threshold);\n    __m128d v_scale = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128d v_src = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d v_mask = _mm_cmpgt_pd(v_src, v_threshold);\n            __m128d v_scaled = _mm_mul_pd(v_src, v_scale);\n            _mm_storeu_pd(&dst[i * cols + j], _mm_blendv_pd(v_src, v_scaled, v_mask));\n        }\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src[i * cols + j] > threshold) {\n                dst[i * cols + j] = src[i * cols + j] * scale;\n            } else {\n                dst[i * cols + j] = src[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d v_threshold = _mm_set1_pd(threshold);\n    __m128d v_scale = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j <= cols - 2; j += 2) {\n            __m128d v_src = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d v_mask = _mm_cmpgt_pd(v_src, v_threshold);\n            __m128d v_scaled = _mm_mul_pd(v_src, v_scale);\n            __m128d v_result = _mm_blendv_pd(v_src, v_scaled, v_mask);\n            _mm_storeu_pd(&dst[i * cols + j], v_result);\n        }\n        if (cols % 2 != 0) {\n            dst[i * cols + cols - 1] = src[i * cols + cols - 1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d v_threshold = _mm_set1_pd(threshold);\n    __m128d v_scale = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128d v_src = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d v_mask = _mm_cmpgt_pd(v_src, v_threshold);\n            __m128d v_scaled = _mm_mul_pd(v_src, v_scale);\n            _mm_storeu_pd(&dst[i * cols + j], _mm_blendv_pd(v_src, v_scaled, v_mask));\n        }\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src[i * cols + j] > threshold) {\n                dst[i * cols + j] = src[i * cols + j] * scale;\n            } else {\n                dst[i * cols + j] = src[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d v_threshold = _mm_set1_pd(threshold);\n    __m128d v_scale = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128d v_src = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d v_mask = _mm_cmpgt_pd(v_src, v_threshold);\n            __m128d v_scaled = _mm_mul_pd(v_src, v_scale);\n            _mm_storeu_pd(&dst[i * cols + j], _mm_blendv_pd(v_src, v_scaled, v_mask));\n        }\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src[i * cols + j] > threshold) {\n                dst[i * cols + j] = src[i * cols + j] * scale;\n            } else {\n                dst[i * cols + j] = src[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d v_threshold = _mm_set1_pd(threshold);\n    __m128d v_scale = _mm_set1_pd(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m128d v_src = _mm_loadu_pd(&src[i * cols + j]);\n            __m128d v_mask = _mm_cmpgt_pd(v_src, v_threshold);\n            __m128d v_scaled = _mm_mul_pd(v_src, v_scale);\n            _mm_storeu_pd(&dst[i * cols + j], _mm_blendv_pd(v_src, v_scaled, v_mask));\n        }\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (src[i * cols + j] > threshold) {\n                dst[i * cols + j] = src[i * cols + j] * scale;\n            } else {\n                dst[i * cols + j] = src[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m128d v0, v1, v2, v3;\n    for (size_t i = 0; i < rows; i += 2) {\n        size_t idx0 = indices[i];\n        size_t idx1 = indices[i + 1];\n\n        // Load row data from src into registers\n        v0 = _mm_loadu_pd(&src[idx0 * cols]);\n        v1 = _mm_loadu_pd(&src[idx1 * cols]);\n\n        // Store row data to dst\n        _mm_storeu_pd(&dst[i * cols], v0);\n        _mm_storeu_pd(&dst[(i + 1) * cols], v1);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m128d v0, v1, v2, v3;\n    for (size_t i = 0; i < rows; i += 2) {\n        size_t idx0 = indices[i];\n        size_t idx1 = indices[i + 1];\n\n        // Load row data from src into registers\n        v0 = _mm_loadu_pd(&src[idx0 * cols]);\n        v1 = _mm_loadu_pd(&src[idx1 * cols]);\n\n        // Store row data into dst\n        _mm_storeu_pd(&dst[i * cols], v0);\n        _mm_storeu_pd(&dst[(i + 1) * cols], v1);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m128d v0, v1, v2, v3;\n    for (size_t i = 0; i < rows; i += 2) {\n        size_t idx0 = indices[i];\n        size_t idx1 = indices[i + 1];\n\n        // Load row data from src into registers\n        v0 = _mm_loadu_pd(&src[idx0 * cols]);\n        v1 = _mm_loadu_pd(&src[idx1 * cols]);\n\n        // Store row data to dst\n        _mm_storeu_pd(&dst[i * cols], v0);\n        _mm_storeu_pd(&dst[(i + 1) * cols], v1);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m128d v0, v1, v2, v3;\n    for (size_t i = 0; i < rows; i += 2) {\n        size_t idx0 = indices[i];\n        size_t idx1 = indices[i + 1];\n\n        // Load row data from src into vectors\n        v0 = _mm_loadu_pd(&src[idx0 * cols]);\n        v1 = _mm_loadu_pd(&src[idx1 * cols]);\n\n        // Store row data into dst\n        _mm_storeu_pd(&dst[i * cols], v0);\n        _mm_storeu_pd(&dst[(i + 1) * cols], v1);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m128d v0, v1, v2, v3;\n    for (size_t i = 0; i < rows; i += 2) {\n        size_t idx0 = indices[i];\n        size_t idx1 = indices[i + 1];\n\n        // Load row data from src into registers\n        v0 = _mm_loadu_pd(&src[idx0 * cols]);\n        v1 = _mm_loadu_pd(&src[idx1 * cols]);\n\n        // Store row data to dst\n        _mm_storeu_pd(&dst[i * cols], v0);\n        _mm_storeu_pd(&dst[(i + 1) * cols], v1);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 2; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i *)&src1[i]);\n        __m128i src2_vec = _mm_loadu_si128((__m128i *)&src2[i]);\n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        __m128i result_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        result_vec = _mm_srai_epi64(result_vec, 1);\n        _mm_storeu_si128((__m128i *)&dst[i], result_vec);\n    }\n    if (length % 2 != 0) {\n        dst[length - 1] = (src1[length - 1] + src2[length - 1] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 2; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i *)&src1[i]);\n        __m128i src2_vec = _mm_loadu_si128((__m128i *)&src2[i]);\n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        __m128i result_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        result_vec = _mm_srai_epi64(result_vec, 1);\n        _mm_storeu_si128((__m128i *)&dst[i], result_vec);\n    }\n    if (length % 2 != 0) {\n        dst[length - 1] = (src1[length - 1] + src2[length - 1] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```cpp\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 2; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i *)&src1[i]);\n        __m128i src2_vec = _mm_loadu_si128((__m128i *)&src2[i]);\n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        __m128i result_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        result_vec = _mm_srai_epi64(result_vec, 1);\n        _mm_storeu_si128((__m128i *)&dst[i], result_vec);\n    }\n    if (length % 2 != 0) {\n        dst[length - 1] = (src1[length - 1] + src2[length - 1] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 2; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i *)&src1[i]);\n        __m128i src2_vec = _mm_loadu_si128((__m128i *)&src2[i]);\n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        __m128i result_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        result_vec = _mm_srai_epi64(result_vec, 1);\n        _mm_storeu_si128((__m128i *)&dst[i], result_vec);\n    }\n    if (length % 2 != 0) {\n        dst[length - 1] = (src1[length - 1] + src2[length - 1] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```cpp\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    for (size_t i = 0; i <= length - 2; i += 2) {\n        __m128i src1_vec = _mm_loadu_si128((__m128i *)&src1[i]);\n        __m128i src2_vec = _mm_loadu_si128((__m128i *)&src2[i]);\n        __m128i sum_vec = _mm_add_epi64(src1_vec, src2_vec);\n        __m128i result_vec = _mm_add_epi64(sum_vec, scalar_vec);\n        result_vec = _mm_srai_epi64(result_vec, 1);\n        _mm_storeu_si128((__m128i *)&dst[i], result_vec);\n    }\n    if (length % 2 != 0) {\n        dst[length - 1] = (src1[length - 1] + src2[length - 1] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process elements in groups of 4\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vals = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i mask = _mm_and_si128(src_vals, _mm_set1_epi32(0x55555555));\n        min_val = _mm_min_epi32(min_val, mask);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < _mm_cvtsi128_si32(min_val)) {\n            min_val = _mm_set1_epi32(src[i]);\n        }\n    }\n\n    return _mm_cvtsi128_si32(min_val);\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process elements in chunks of 4\n    while (i + 7 < length) {\n        __m128i src_vals = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i mask = _mm_and_si128(src_vals, _mm_set1_epi32(0x55555555));\n        min_val = _mm_min_epi32(min_val, mask);\n        i += 8;\n    }\n\n    // Process remaining elements\n    while (i < length) {\n        if ((src[i] & 1) == 0 && src[i] < _mm_extract_epi32(min_val, 0)) {\n            min_val = _mm_set1_epi32(src[i]);\n        }\n        i++;\n    }\n\n    return _mm_extract_epi32(min_val, 0);\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process elements in groups of 4\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vals = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i mask = _mm_and_si128(src_vals, _mm_set1_epi32(0x55555555));\n        min_val = _mm_min_epi32(min_val, mask);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < _mm_cvtsi128_si32(min_val)) {\n            min_val = _mm_set1_epi32(src[i]);\n        }\n    }\n\n    return _mm_cvtsi128_si32(min_val);\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i even_vals = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i mask = _mm_and_si128(_mm_cmpeqd_epi32(even_vals, _mm_slli_epi32(even_vals, 1)), _mm_set1_epi32(0x55555555));\n        even_vals = _mm_and_si128(even_vals, mask);\n        min_val = _mm_min_epi32(min_val, even_vals);\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i *)result, min_val);\n\n    int min_value = INT_MAX;\n    for (int j = 0; j < 4; ++j) {\n        if (result[j] % 2 == 0 && result[j] < min_value) {\n            min_value = result[j];\n        }\n    }\n\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (src[i] % 2 == 0 && src[i] < min_value) {\n            min_value = src[i];\n        }\n    }\n\n    return min_value;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```cpp\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    size_t i = 0;\n\n    // Process elements in chunks of 4\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vals = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i mask = _mm_and_si128(src_vals, _mm_set1_epi32(0x55555555)); // Mask to select even-indexed elements\n        min_val = _mm_min_epi32(min_val, mask);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (i % 2 == 0 && src[i] < _mm_extract_epi32(min_val, 0)) {\n            min_val = _mm_set1_epi32(src[i]);\n        }\n    }\n\n    return _mm_extract_epi32(min_val, 0);\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m128i shift_mask = _mm_set1_epi32(shift);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i *)&dst[i]);\n\n        // Arithmetic right shift\n        __m128i arith_shifted = _mm_sra_epi32(src_vec, shift_mask);\n\n        // Logical right shift\n        __m128i logic_shifted = _mm_srli_epi32(_mm_castsi128_si256(_mm_cvtepi32_epi64(src_vec)), shift);\n\n        // Add results\n        __m128i result = _mm_add_epi32(arith_shifted, logic_shifted);\n\n        _mm_storeu_si128((__m128i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + (static_cast<uint32_t>(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m128i mask = _mm_set1_epi32(0xFFFFFFFF);\n    const __m128i shift_mask = _mm_set1_epi32(shift);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i arith_shifted = _mm_sra_epi32(src_vec, shift_mask);\n        __m128i logic_shifted = _mm_srli_epi32(_mm_and_si128(src_vec, mask), shift);\n        __m128i result = _mm_add_epi32(arith_shifted, logic_shifted);\n        _mm_storeu_si128((__m128i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m128i shift_mask = _mm_set1_epi32(shift);\n    const __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i dst_vec_arith = _mm_sra_epi32(src_vec, shift_mask);\n        __m128i src_vec_uint = _mm_castsi128_si256(_mm_cvtepi32_epi64(src_vec));\n        __m128i dst_vec_logic = _mm_srli_epi32(_mm_castsi256_si128(src_vec_uint), shift);\n        __m128i dst_vec = _mm_add_epi32(dst_vec_arith, dst_vec_logic);\n        _mm_storeu_si128((__m128i *)&dst[i], dst_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m128i shift_mask = _mm_set1_epi32(shift);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i *)&dst[i]);\n\n        // Arithmetic right shift\n        __m128i arith_shifted = _mm_sra_epi32(src_vec, shift_mask);\n\n        // Logical right shift\n        __m128i logical_shifted = _mm_srli_epi32(_mm_castsi128_si256(_mm_cvtepi32_epi64(src_vec)), shift);\n\n        // Add results\n        __m128i result = _mm_add_epi32(arith_shifted, logical_shifted);\n\n        _mm_storeu_si128((__m128i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + (static_cast<uint32_t>(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const __m128i shift_mask = _mm_set1_epi32(shift);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i *)&dst[i]);\n\n        // Arithmetic right shift\n        __m128i arith_shifted = _mm_sra_epi32(src_vec, shift_mask);\n\n        // Logical right shift\n        __m128i logical_shifted = _mm_srli_epi32(_mm_castsi128_si256(_mm_cvtepi32_epi64(src_vec)), shift);\n\n        // Add results\n        __m128i result = _mm_add_epi32(arith_shifted, logical_shifted);\n\n        _mm_storeu_si128((__m128i *)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = (src[i] >> shift) + (static_cast<uint32_t>(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m128i row_vec = _mm_loadu_si128((__m128i *)&row_start[j]);\n            sum_vec = _mm_add_epi32(sum_vec, row_vec);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            total_sum += row_start[j];\n        }\n    }\n\n    // Sum up the values in the vector\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i *)temp, sum_vec);\n    total_sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m128i row_vec = _mm_loadu_si128((__m128i *)&row_start[j]);\n            sum_vec = _mm_add_epi32(sum_vec, row_vec);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            total_sum += row_start[j];\n        }\n    }\n\n    // Sum up the values in the vector\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i *)temp, sum_vec);\n    total_sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m128i row_vec = _mm_loadu_si128((__m128i *)&row_start[j]);\n            sum_vec = _mm_add_epi32(sum_vec, row_vec);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            total_sum += row_start[j];\n        }\n    }\n\n    // Sum up the values in the vector\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i *)temp, sum_vec);\n    total_sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m128i row_vec = _mm_loadu_si128((__m128i *)&row_start[j]);\n            sum_vec = _mm_add_epi32(sum_vec, row_vec);\n        }\n\n        // Handle remaining elements that don't fit into a full vector\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            total_sum += row_start[j];\n        }\n    }\n\n    // Sum up the values in the vector\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i *)temp, sum_vec);\n    total_sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int64_t total_sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int *row_start = &matrix[i * cols];\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m128i row_vec = _mm_loadu_si128((__m128i *)&row_start[j]);\n            sum_vec = _mm_add_epi32(sum_vec, row_vec);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            total_sum += row_start[j];\n        }\n    }\n\n    // Sum up the values in the vector\n    int32_t temp[4];\n    _mm_storeu_si128((__m128i *)temp, sum_vec);\n    total_sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_val = _mm_set1_epi8(src[0]);\n    size_t max_idx = 0;\n\n    for (size_t i = 1; i + 15 < length; i += 16) {\n        __m128i current_vals = _mm_loadu_si128((__m128i *)&src[i]);\n        __m128i greater_mask = _mm_cmpgt_epi8(current_vals, max_val);\n\n        max_val = _mm_max_epi8(max_val, current_vals);\n        max_idx += _mm_movemask_epi8(greater_mask) << (i / 16);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    __m128i max_value = _mm_set1_epi8(src[0]);\n    size_t i = 0;\n\n    while (i + 15 < length) {\n        __m128i current_values = _mm_loadu_si128((__m128i *)(src + i));\n        __m128i greater_mask = _mm_cmpgt_epi8(current_values, max_value);\n        max_value = _mm_max_epu8(max_value, current_values);\n\n        // Find the index of the maximum value in the current block\n        uint32_t mask = _mm_movemask_epi8(greater_mask);\n        if (mask != 0) {\n            size_t block_max_index = i + __builtin_ctz(mask);\n            if (src[block_max_index] > src[max_index]) {\n                max_index = block_max_index;\n            }\n        }\n\n        i += 16;\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```cpp\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_val = _mm_set1_epi8(src[0]);\n    size_t max_idx = 0;\n\n    for (size_t i = 1; i + 15 < length; i += 16) {\n        __m128i current_vals = _mm_loadu_si128((__m128i *)(src + i));\n        __m128i greater_mask = _mm_cmpgt_epi8(current_vals, max_val);\n        max_val = _mm_max_epu8(max_val, current_vals);\n\n        __m128i idx_vec = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        idx_vec = _mm_add_epi32(idx_vec, _mm_and_si128(greater_mask, _mm_set1_epi32(4)));\n        idx_vec = _mm_add_epi32(idx_vec, _mm_and_si128(_mm_srli_epi16(greater_mask, 2), _mm_set1_epi32(8)));\n        idx_vec = _mm_add_epi32(idx_vec, _mm_and_si128(_mm_srli_epi16(greater_mask, 4), _mm_set1_epi32(16)));\n        idx_vec = _mm_add_epi32(idx_vec, _mm_and_si128(_mm_srli_epi16(greater_mask, 6), _mm_set1_epi32(32)));\n\n        __m128i max_idx_vec = _mm_max_epu32(_mm_set1_epi32(max_idx), idx_vec);\n        max_idx = _mm_cvtsi128_si32(max_idx_vec);\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    size_t max_index = 0;\n    __m128i max_value = _mm_set1_epi8(src[0]);\n    size_t i = 0;\n\n    while (i + 15 < length) {\n        __m128i current_values = _mm_loadu_si128((__m128i *)(src + i));\n        __m128i greater_mask = _mm_cmpgt_epi8(current_values, max_value);\n        max_value = _mm_max_epu8(max_value, current_values);\n\n        // Find the index of the maximum value in the current chunk\n        __m128i indices = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        indices = _mm_add_epi32(indices, _mm_and_si128(greater_mask, _mm_set1_epi32(4)));\n        indices = _mm_add_epi32(indices, _mm_and_si128(_mm_srli_epi16(greater_mask, 2), _mm_set1_epi32(8)));\n        indices = _mm_add_epi32(indices, _mm_and_si128(_mm_srli_epi16(greater_mask, 4), _mm_set1_epi32(16)));\n        indices = _mm_add_epi32(indices, _mm_and_si128(_mm_srli_epi16(greater_mask, 6), _mm_set1_epi32(32)));\n\n        // Update the global max_index\n        __m128i temp_max_index = _mm_movemask_epi8(greater_mask);\n        if ((temp_max_index & 0xFF) != 0) {\n            max_index = _mm_extract_epi32(indices, 0);\n        }\n        if ((temp_max_index >> 8) != 0) {\n            max_index = _mm_extract_epi32(indices, 1);\n        }\n        if ((temp_max_index >> 16) != 0) {\n            max_index = _mm_extract_epi32(indices, 2);\n        }\n        if ((temp_max_index >> 24) != 0) {\n            max_index = _mm_extract_epi32(indices, 3);\n        }\n\n        i += 16;\n    }\n\n    // Handle remaining elements\n    while (i < length) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n        i++;\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```cpp\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m128i max_val = _mm_set1_epi8(src[0]);\n    size_t max_idx = 0;\n\n    for (size_t i = 1; i + 15 < length; i += 16) {\n        __m128i current_vals = _mm_loadu_si128((__m128i *)(src + i));\n        __m128i greater_mask = _mm_cmpgt_epi8(current_vals, max_val);\n\n        max_val = _mm_max_epu8(max_val, current_vals);\n        max_idx += _mm_movemask_epi8(greater_mask) << (i / 16);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i v_mask = _mm_set1_epi32(mask);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_index = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        __m128i v_and_result = _mm_and_epi32(v_index, v_mask);\n\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_cond = _mm_cmpneq_epi32(v_and_result, v_zero);\n        __m128i v_dst = _mm_blendv_epi8(v_src2, v_src1, v_cond);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i v_mask = _mm_set1_epi16(static_cast<short>(mask));\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        __m128i v_index = _mm_setr_epi16(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m128i v_and_result = _mm_and_si128(v_index, v_mask);\n\n        __m128i v_src1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&src1[i]));\n        __m128i v_src2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&src2[i]));\n\n        __m128i v_conditional_select = _mm_cmpeq_epi16(v_and_result, _mm_setzero_si128());\n        __m128i v_dst = _mm_blendv_epi8(v_src2, v_src1, v_conditional_select);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&dst[i]), v_dst);\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i v_mask = _mm_set1_epi32(mask);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_index = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        __m128i v_index_and_mask = _mm_and_si128(v_index, v_mask);\n\n        __m128i v_condition = _mm_cmpgt_epi32(v_index_and_mask, v_zero);\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_result = _mm_blendv_epi8(v_src2, v_src1, v_condition);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i v_mask = _mm_set1_epi16(static_cast<short>(mask));\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_index = _mm_setr_epi16(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m128i v_and_result = _mm_and_si128(v_index, v_mask);\n\n        __m128i v_src1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&src1[i]));\n        __m128i v_src2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&src2[i]));\n\n        __m128i v_conditional_select = _mm_cmpneq_epi16(v_and_result, v_zero);\n        __m128i v_dst = _mm_blendv_epi8(v_src2, v_src1, v_conditional_select);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&dst[i]), v_dst);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m128i v_mask = _mm_set1_epi32(mask);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_index = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        __m128i v_and_result = _mm_and_si128(v_index, v_mask);\n\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_cond = _mm_cmpneq_epi32(v_and_result, v_zero);\n        __m128i v_dst = _mm_blendv_epi8(v_src2, v_src1, v_cond);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_dst);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t);\n    __m128i v_one = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n    __m128i v_two = _mm_set1_epi32(0x33333333); // Binary: 00110011001100110011001100110011\n    __m128i v_four = _mm_set1_epi32(0x0F0F0F0F); // Binary: 00001111000011110000111100001111\n    __m128i v_eight = _mm_set1_epi32(0x00FF00FF); // Binary: 00000000111111110000000011111111\n    __m128i v_sixteen = _mm_set1_epi32(0x0000FFFF); // Binary: 00000000000000001111111111111111\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_popcount = v_src;\n\n        v_popcount = _mm_and_si128(v_popcount, v_one);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 1));\n        v_popcount = _mm_and_si128(v_popcount, v_two);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 2));\n        v_popcount = _mm_and_si128(v_popcount, v_four);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 4));\n        v_popcount = _mm_and_si128(v_popcount, v_eight);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 8));\n        v_popcount = _mm_and_si128(v_popcount, v_sixteen);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 16));\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_popcount);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```cpp\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t);\n    __m128i v_ones = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n    __m128i v_twos = _mm_set1_epi32(0x33333333); // Binary: 00110011001100110011001100110011\n    __m128i v_fours = _mm_set1_epi32(0x0F0F0F0F); // Binary: 00001111000011110000111100001111\n    __m128i v_eights = _mm_set1_epi32(0x00FF00FF); // Binary: 00000000111111110000000011111111\n    __m128i v_sixteens = _mm_set1_epi32(0x0000FFFF); // Binary: 00000000000000001111111111111111\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_popcount = v_src;\n\n        v_popcount = _mm_and_si128(v_popcount, v_ones);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 1));\n        v_popcount = _mm_and_si128(v_popcount, v_twos);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 2));\n        v_popcount = _mm_and_si128(v_popcount, v_fours);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 4));\n        v_popcount = _mm_and_si128(v_popcount, v_eights);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 8));\n        v_popcount = _mm_and_si128(v_popcount, v_sixteens);\n        v_popcount = _mm_add_epi32(v_popcount, _mm_srli_epi32(v_popcount, 16));\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_popcount);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t);\n    size_t i;\n\n    // Process elements in chunks of 4 (SSE2)\n    for (i = 0; i <= length - simd_width; i += simd_width) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_popcount = _mm_popcnt_epi32(v_src);\n        _mm_storeu_si128((__m128i*)&dst[i], v_popcount);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t);\n    size_t i;\n\n    // Process elements in chunks of 4 using SSE2\n    for (i = 0; i <= length - simd_width; i += simd_width) {\n        __m128i vsrc = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i vones = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n        __m128i vh = _mm_and_si128(vsrc, vones);\n        __m128i vl = _mm_andnot_si128(vones, vsrc);\n\n        vones = _mm_set1_epi32(0x33333333); // Binary: 00110011001100110011001100110011\n        vh = _mm_add_epi32(vh, vl);\n        vl = _mm_and_si128(vh, vones);\n        vh = _mm_srli_epi32(vh, 2);\n        vh = _mm_add_epi32(vh, vl);\n\n        vones = _mm_set1_epi32(0x0F0F0F0F); // Binary: 00001111000011110000111100001111\n        vl = _mm_and_si128(vh, vones);\n        vh = _mm_srli_epi32(vh, 4);\n        vh = _mm_add_epi32(vh, vl);\n\n        vones = _mm_set1_epi32(0x00FF00FF); // Binary: 00000000111111110000000011111111\n        vl = _mm_and_si128(vh, vones);\n        vh = _mm_srli_epi32(vh, 8);\n        vh = _mm_add_epi32(vh, vl);\n\n        vones = _mm_set1_epi32(0x0000FFFF); // Binary: 00000000000000001111111111111111\n        vl = _mm_and_si128(vh, vones);\n        vh = _mm_srli_epi32(vh, 16);\n        vh = _mm_add_epi32(vh, vl);\n\n        _mm_storeu_si128((__m128i*)&dst[i], vh);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t);\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_and_si128(vec, ones);\n        __m128i cnt = _mm_popcnt_epi32(mask);\n        _mm_storeu_si128((__m128i*)&dst[i], cnt);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m128i mask = _mm_set1_epi16(0xFFFF);\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m128i v_max = _mm_set1_epi16(0xFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_sum = _mm_adds_epu16(v_src1, v_src2);\n        __m128i v_clamped = _mm_min_epu16(v_sum, v_max);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_clamped);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m128i v_max = _mm_set1_epi16(0xFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_sum = _mm_adds_epu16(v_src1, v_src2);\n        __m128i v_clamped = _mm_min_epu16(v_sum, v_max);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_clamped);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m128i v_max = _mm_set1_epi16(0xFFFF);\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i v_sum = _mm_adds_epu16(v_src1, v_src2);\n        _mm_storeu_si128((__m128i*)&dst[i], v_sum);\n    }\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m128i v_max = _mm_set1_epi16(0xFFFF);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i v_sum = _mm_adds_epu16(v_src1, v_src2);\n        __m128i v_clamped = _mm_min_epu16(v_sum, v_max);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_clamped);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d v_zero = _mm_setzero_pd();\n    __m128d v_min = _mm_set1_pd(-100.0);\n    __m128d v_max = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d v_c0 = v_zero;\n            __m128d v_c1 = v_zero;\n\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d v_a0 = _mm_loadu_pd(&A[i * n + k]);\n                __m128d v_b0 = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d v_mask0 = _mm_and_pd(_mm_cmpge_pd(v_a0, v_min), _mm_cmple_pd(v_a0, v_max));\n                __m128d v_mask1 = _mm_and_pd(_mm_cmpge_pd(v_b0, v_min), _mm_cmple_pd(v_b0, v_max));\n\n                v_c0 = _mm_add_pd(v_c0, _mm_mul_pd(v_a0, v_b0));\n                v_c1 = _mm_add_pd(v_c1, _mm_mul_pd(v_a0, v_b0));\n            }\n\n            v_c0 = _mm_hadd_pd(v_c0, v_c0);\n            v_c1 = _mm_hadd_pd(v_c1, v_c1);\n\n            double c0[2];\n            double c1[2];\n\n            _mm_storeu_pd(c0, v_c0);\n            _mm_storeu_pd(c1, v_c1);\n\n            C[i * p + j] = c0[0];\n            if (j + 1 < p) {\n                C[i * p + j + 1] = c1[0];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d hundred = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d mask_a = _mm_and_pd(_mm_cmpgt_pd(a, zero), _mm_cmplt_pd(a, hundred));\n                __m128d mask_b = _mm_and_pd(_mm_cmpgt_pd(b, zero), _mm_cmplt_pd(b, hundred));\n\n                __m128d masked_a = _mm_and_pd(a, mask_a);\n                __m128d masked_b = _mm_and_pd(b, mask_b);\n\n                sum = _mm_add_pd(sum, _mm_mul_pd(masked_a, masked_b));\n            }\n\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0];\n            if (j + 1 < p) {\n                C[i * p + j + 1] = temp[1];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d v_zero = _mm_setzero_pd();\n    __m128d v_neg_100 = _mm_set1_pd(-100.0);\n    __m128d v_100 = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = v_zero;\n            for (size_t k = 0; k < n; ++k) {\n                __m128d va = _mm_loadu_pd(&A[i * n + k]);\n                __m128d vb = _mm_loadu_pd(&B[k * p + j]);\n                __m128d va_clipped = _mm_max_pd(_mm_min_pd(va, v_100), v_neg_100);\n                __m128d vb_clipped = _mm_max_pd(_mm_min_pd(vb, v_100), v_neg_100);\n                sum = _mm_add_pd(sum, _mm_mul_pd(va_clipped, vb_clipped));\n            }\n            double s[2];\n            _mm_storeu_pd(s, sum);\n            C[i * p + j] = s[0];\n            if (j + 1 < p) {\n                C[i * p + j + 1] = s[1];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d hundred = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                __m128d mask_a = _mm_and_pd(_mm_cmpgt_pd(a, zero), _mm_cmplt_pd(a, hundred));\n                __m128d mask_b = _mm_and_pd(_mm_cmpgt_pd(b, zero), _mm_cmplt_pd(b, hundred));\n\n                __m128d masked_a = _mm_and_pd(a, mask_a);\n                __m128d masked_b = _mm_and_pd(b, mask_b);\n\n                sum = _mm_add_pd(sum, _mm_mul_pd(masked_a, masked_b));\n            }\n\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] = temp[0];\n            if (j + 1 < p) {\n                C[i * p + j + 1] = temp[1];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if elements are within the range [-100, 100]\n                __m128d mask_a = _mm_and_pd(_mm_cmpge_pd(a, lower_bound), _mm_cmple_pd(a, upper_bound));\n                __m128d mask_b = _mm_and_pd(_mm_cmpge_pd(b, lower_bound), _mm_cmple_pd(b, upper_bound));\n\n                // Multiply and accumulate only if both elements are within the range\n                __m128d product = _mm_mul_pd(a, b);\n                sum = _mm_add_pd(sum, _mm_and_pd(product, _mm_and_pd(mask_a, mask_b)));\n            }\n\n            // Store the result\n            _mm_storeu_pd(&C[i * p + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m128i vecA = _mm_load_si128((__m128i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m128i vecB = _mm_load_si128((__m128i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m128i vecC = _mm_add_epi64(vecA, vecB);\n            _mm_store_si128((__m128i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m128i vecA = _mm_load_si128(reinterpret_cast<const __m128i*>(&A[i * dim2 * dim3 + j * dim3]));\n            __m128i vecB = _mm_load_si128(reinterpret_cast<const __m128i*>(&B[i * dim2 * dim3 + j * dim3]));\n            __m128i vecC = _mm_add_epi64(vecA, vecB);\n            _mm_store_si128(reinterpret_cast<__m128i*>(&C[i * dim2 * dim3 + j * dim3]), vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m128i vecA = _mm_load_si128((__m128i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m128i vecB = _mm_load_si128((__m128i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m128i vecC = _mm_add_epi64(vecA, vecB);\n            _mm_store_si128((__m128i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m128i vecA = _mm_load_si128(reinterpret_cast<const __m128i*>(&A[i * dim2 * dim3 + j * dim3]));\n            __m128i vecB = _mm_load_si128(reinterpret_cast<const __m128i*>(&B[i * dim2 * dim3 + j * dim3]));\n            __m128i vecC = _mm_add_epi64(vecA, vecB);\n            _mm_store_si128(reinterpret_cast<__m128i*>(&C[i * dim2 * dim3 + j * dim3]), vecC);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            __m128i vecA = _mm_load_si128((__m128i*)&A[i * dim2 * dim3 + j * dim3]);\n            __m128i vecB = _mm_load_si128((__m128i*)&B[i * dim2 * dim3 + j * dim3]);\n            __m128i vecC = _mm_add_epi64(vecA, vecB);\n            _mm_store_si128((__m128i*)&C[i * dim2 * dim3 + j * dim3], vecC);\n        }\n        for (size_t k = dim2 % 4; k < dim2; ++k) {\n            C[i * dim2 * dim3 + k * dim3] = A[i * dim2 * dim3 + k * dim3] + B[i * dim2 * dim3 + k * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n        __m128 v_sum = _mm_add_ps(v_src1, v_src2);\n        __m128i v_rounded = _mm_cvtps_epi32(_mm_round_ps(v_sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n        _mm_storeu_si128((__m128i*)&dst[i], v_rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(round(src1[i] + src2[i])));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n        __m128 v_sum = _mm_add_ps(v_src1, v_src2);\n        __m128i v_rounded = _mm_cvtps_epi32(_mm_add_ps(v_sum, _mm_set1_ps(0.5f)));\n        _mm_storeu_si128((__m128i*)&dst[i], v_rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(src1[i] + src2[i] + 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n        __m128 v_sum = _mm_add_ps(v_src1, v_src2);\n        __m128i v_rounded = _mm_cvtps_epi32(_mm_round_ps(v_sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));\n        _mm_storeu_si128((__m128i*)&dst[i], v_rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(round(src1[i] + src2[i])));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n        __m128 v_sum = _mm_add_ps(v_src1, v_src2);\n        __m128i v_rounded = _mm_cvtps_epi32(_mm_round_ps(v_sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC));\n        _mm_storeu_si128((__m128i*)&dst[i], v_rounded);\n    }\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(round(src1[i] + src2[i])));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(&src1[i]);\n        __m128 vec2 = _mm_loadu_ps(&src2[i]);\n        __m128 sum = _mm_add_ps(vec1, vec2);\n        __m128i rounded = _mm_cvtps_epi32(sum);\n        __m128 rounded_float = _mm_cvtepi32_ps(rounded);\n        _mm_storeu_ps(&dst[i], rounded_float);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(src1[i] + src2[i] + 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 v_threshold = _mm_set1_ps(50.0f);\n    __m128 v_one = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 v_diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 v_abs_diff = _mm_and_ps(v_diff, _mm_set1_ps(0x7FFFFFFF));\n        __m128 v_condition = _mm_cmple_ps(v_abs_diff, v_threshold);\n\n        __m128 v_product = _mm_mul_ps(v_src1, v_src2);\n        __m128 v_mask = _mm_and_ps(v_condition, _mm_set1_ps(0xFFFFFFFF));\n        v_product = _mm_or_ps(v_product, _mm_andnot_ps(v_condition, v_one));\n\n        v_product = _mm_add_ps(v_product, _mm_set1_ps(0.5f));\n        v_product = _mm_cvtepi32_ps(_mm_cvtps_epi32(v_product));\n\n        _mm_storeu_ps(&dst[i], v_product);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 v_threshold = _mm_set1_ps(50.0f);\n    __m128 v_neg_one = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 v_diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 v_abs_diff = _mm_and_ps(v_diff, _mm_set1_ps(0x7FFFFFFF));\n        __m128 v_condition = _mm_cmple_ps(v_abs_diff, v_threshold);\n\n        __m128 v_product = _mm_mul_ps(v_src1, v_src2);\n        __m128 v_mask = _mm_and_ps(v_condition, _mm_set1_ps(0xFFFFFFFF));\n        v_product = _mm_or_ps(v_product, _mm_andnot_ps(v_condition, v_neg_one));\n\n        v_product = _mm_add_ps(v_product, _mm_set1_ps(0.5f));\n        v_product = _mm_cvtepi32_ps(_mm_cvttps_epi32(v_product));\n\n        _mm_storeu_ps(&dst[i], v_product);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 v_threshold = _mm_set1_ps(50.0f);\n    __m128 v_one = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 v_diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 v_abs_diff = _mm_and_ps(v_diff, _mm_set1_ps(0x7FFFFFFF));\n        __m128 v_condition = _mm_cmple_ps(v_abs_diff, v_threshold);\n\n        __m128 v_product = _mm_mul_ps(v_src1, v_src2);\n        __m128 v_mask = _mm_and_ps(v_condition, _mm_set1_ps(0xFFFFFFFF));\n        v_product = _mm_or_ps(v_product, _mm_andnot_ps(v_condition, v_one));\n\n        v_product = _mm_add_ps(v_product, _mm_set1_ps(0.5f));\n        v_product = _mm_cvtepi32_ps(_mm_cvttps_epi32(v_product));\n\n        _mm_storeu_ps(&dst[i], v_product);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 v_threshold = _mm_set1_ps(50.0f);\n    __m128 v_one = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 v_diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 v_abs_diff = _mm_and_ps(v_diff, _mm_set1_ps(0x7FFFFFFF));\n        __m128 v_condition = _mm_cmple_ps(v_abs_diff, v_threshold);\n\n        __m128 v_product = _mm_mul_ps(v_src1, v_src2);\n        __m128 v_mask = _mm_and_ps(v_condition, _mm_set1_ps(0xFFFFFFFF));\n        __m128 v_result = _mm_or_ps(_mm_andnot_ps(v_condition, v_one), v_mask);\n\n        _mm_storeu_ps(&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m128 v_threshold = _mm_set1_ps(50.0f);\n    __m128 v_one = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128 v_src1 = _mm_loadu_ps(&src1[i]);\n        __m128 v_src2 = _mm_loadu_ps(&src2[i]);\n\n        __m128 v_diff = _mm_sub_ps(v_src1, v_src2);\n        __m128 v_abs_diff = _mm_and_ps(v_diff, _mm_set1_ps(0x7FFFFFFF));\n        __m128 v_condition = _mm_cmple_ps(v_abs_diff, v_threshold);\n\n        __m128 v_product = _mm_mul_ps(v_src1, v_src2);\n        __m128 v_mask = _mm_and_ps(v_condition, _mm_set1_ps(1.0f));\n        __m128 v_result = _mm_or_ps(_mm_andnot_ps(v_condition, v_one), _mm_and_ps(v_mask, v_product));\n\n        v_result = _mm_add_ps(v_result, _mm_set1_ps(0.5f));\n        v_result = _mm_cvtepi32_ps(_mm_cvtps_epi32(v_result));\n\n        _mm_storeu_ps(&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (std::abs(src1[i] - src2[i]) <= 50 && i % 2 == 0) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```cpp\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result_vec = _mm_mul_pd(src_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```cpp\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result_vec = _mm_mul_pd(src_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d res_vec = _mm_mul_pd(src_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], res_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t step = 4;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n - step + 1; j += step) {\n            __m128d src_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d res_vec = _mm_mul_pd(src_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], res_vec);\n        }\n\n        // Handle remaining elements\n        for (size_t j = n - n % step; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```cpp\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(&mat[i * n + j]);\n            __m128d result_vec = _mm_mul_pd(src_vec, scalar_vec);\n            _mm_storeu_pd(&dst[i * n + j], result_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```cpp\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum_squares = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_elements = _mm_mul_ps(row_elements, row_elements);\n            sum_squares = _mm_add_ps(sum_squares, squared_elements);\n        }\n\n        float sum_squares_array[4];\n        _mm_storeu_ps(sum_squares_array, sum_squares);\n        float l2_norm = sqrt(sum_squares_array[0] + sum_squares_array[1] + sum_squares_array[2] + sum_squares_array[3]);\n\n        if (l2_norm != 0.0f) {\n            for (size_t j = 0; j < n; j += 4) {\n                __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n                __m128 normalized_elements = _mm_div_ps(row_elements, _mm_set1_ps(l2_norm));\n                _mm_storeu_ps(&mat[i * n + j], normalized_elements);\n            }\n        } else {\n            // Handle zero norm case if needed\n            for (size_t j = 0; j < n; j += 4) {\n                _mm_storeu_ps(&mat[i * n + j], _mm_setzero_ps());\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```cpp\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    __m128 one = _mm_set1_ps(1.0f);\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum_squares = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_elements = _mm_mul_ps(row_elements, row_elements);\n            sum_squares = _mm_add_ps(sum_squares, squared_elements);\n        }\n        float sum_squares_array[4];\n        _mm_storeu_ps(sum_squares_array, sum_squares);\n        float l2_norm = sqrt(sum_squares_array[0] + sum_squares_array[1] + sum_squares_array[2] + sum_squares_array[3]);\n        if (l2_norm != 0.0f) {\n            for (size_t j = 0; j < n; j += 4) {\n                __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n                __m128 normalized_elements = _mm_div_ps(row_elements, _mm_set1_ps(l2_norm));\n                _mm_storeu_ps(&mat[i * n + j], normalized_elements);\n            }\n        } else {\n            // Handle zero norm case if needed\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        float sum_squares = 0.0f;\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_elements = _mm_mul_ps(row_elements, row_elements);\n            sum_squares += _mm_cvtss_f32(_mm_sqrt_ss(_mm_hadd_ps(squared_elements, _mm_setzero_ps())));\n        }\n\n        for (size_t j = n - n % 4; j < n; ++j) {\n            sum_squares += mat[i * n + j] * mat[i * n + j];\n        }\n\n        sum_squares = sqrt(sum_squares);\n\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized_elements = _mm_div_ps(row_elements, _mm_set1_ps(sum_squares));\n            _mm_storeu_ps(&mat[i * n + j], normalized_elements);\n        }\n\n        for (size_t j = n - n % 4; j < n; ++j) {\n            mat[i * n + j] /= sum_squares;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```cpp\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m128 one = _mm_set1_ps(1.0f);\n    const __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum_squares = zero;\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_elements = _mm_mul_ps(row_elements, row_elements);\n            sum_squares = _mm_add_ps(sum_squares, squared_elements);\n        }\n\n        // Horizontal add to get the sum of squares\n        float temp[4];\n        _mm_storeu_ps(temp, sum_squares);\n        float sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Calculate the L^2 norm\n        float norm = sqrtf(sum);\n\n        // Normalize the row\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 normalized_elements = _mm_div_ps(row_elements, _mm_set1_ps(norm));\n            _mm_storeu_ps(&mat[i * n + j], normalized_elements);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```cpp\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum_squares = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 squared_elements = _mm_mul_ps(row_elements, row_elements);\n            sum_squares = _mm_add_ps(sum_squares, squared_elements);\n        }\n\n        float sum_squares_array[4];\n        _mm_storeu_ps(sum_squares_array, sum_squares);\n        float l2_norm = sqrt(sum_squares_array[0] + sum_squares_array[1] + sum_squares_array[2] + sum_squares_array[3]);\n\n        if (l2_norm != 0.0f) {\n            for (size_t j = 0; j < n; j += 4) {\n                __m128 row_elements = _mm_loadu_ps(&mat[i * n + j]);\n                __m128 normalized_elements = _mm_div_ps(row_elements, _mm_set1_ps(l2_norm));\n                _mm_storeu_ps(&mat[i * n + j], normalized_elements);\n            }\n        } else {\n            // Handle zero norm case if needed\n            for (size_t j = 0; j < n; j += 4) {\n                _mm_storeu_ps(&mat[i * n + j], _mm_setzero_ps());\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = zero;\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d v1 = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d v2 = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(v1, v2));\n            }\n            __m128d lo = _mm_cvtpd_epi32(sum);\n            __m128d hi = _mm_cvtpd_epi32(_mm_movehl_ps(_mm_castpd_ps(zero), _mm_castpd_ps(sum)));\n            __m128d rounded_lo = _mm_add_pd(_mm_cvtepi32_pd(lo), _mm_and_pd(sum, one));\n            __m128d rounded_hi = _mm_add_pd(_mm_cvtepi32_pd(hi), _mm_and_pd(_mm_movehl_ps(_mm_castpd_ps(zero), sum), one));\n            _mm_storeu_pd(&dst[i * p + j], rounded_lo);\n            if (j + 1 < p) {\n                _mm_storeu_pd(&dst[i * p + j + 1], rounded_hi);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = zero;\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d b = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            __m128d rounded_sum = _mm_cvtpd_epi32(_mm_add_pd(sum, _mm_andnot_si128(_mm_slli_epi32(_mm_castpd_si128(sum), 63), one)));\n            _mm_storeu_pd(&dst[i * p + j], _mm_cvtepi32_pd(rounded_sum));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```cpp\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = zero;\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d v1 = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d v2 = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(v1, v2));\n            }\n            __m128d rounded_sum = _mm_cvtpd_epi32(_mm_add_pd(sum, _mm_andnot_si128(_mm_slli_epi32(_mm_castpd_si128(sum), 63), one)));\n            _mm_storeu_pd(&dst[i * p + j], _mm_cvtepi32_pd(rounded_sum));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```cpp\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum = zero;\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d v1 = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d v2 = _mm_loadu_pd(&mat2[k * p + j]);\n                sum = _mm_add_pd(sum, _mm_mul_pd(v1, v2));\n            }\n            __m128d rounded_sum = _mm_cvtpd_epi32(_mm_add_pd(sum, one));\n            _mm_storeu_si128((__m128i*)&dst[i * p + j], rounded_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m128d sum0 = _mm_setzero_pd();\n            __m128d sum1 = _mm_setzero_pd();\n            __m128d sum2 = _mm_setzero_pd();\n            __m128d sum3 = _mm_setzero_pd();\n\n            for (size_t k = 0; k < n; ++k) {\n                __m128d val1 = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d val2 = _mm_loadu_pd(&mat2[k * p + j]);\n\n                sum0 = _mm_add_pd(sum0, _mm_mul_pd(val1, val2));\n                if (j + 2 < p) {\n                    sum1 = _mm_add_pd(sum1, _mm_mul_pd(_mm_shuffle_pd(val1, val1, 1), _mm_shuffle_pd(val2, val2, 1)));\n                }\n                if (j + 4 < p) {\n                    sum2 = _mm_add_pd(sum2, _mm_mul_pd(_mm_shuffle_pd(val1, val1, 2), _mm_shuffle_pd(val2, val2, 2)));\n                }\n                if (j + 6 < p) {\n                    sum3 = _mm_add_pd(sum3, _mm_mul_pd(_mm_shuffle_pd(val1, val1, 3), _mm_shuffle_pd(val2, val2, 3)));\n                }\n            }\n\n            __m128d rounded_sum0 = _mm_cvtpd_epi32(sum0);\n            __m128d rounded_sum1 = _mm_cvtpd_epi32(sum1);\n            __m128d rounded_sum2 = _mm_cvtpd_epi32(sum2);\n            __m128d rounded_sum3 = _mm_cvtpd_epi32(sum3);\n\n            _mm_storeu_si128((__m128i*)&dst[i * p + j], rounded_sum0);\n            _mm_storeu_si128((__m128i*)&dst[i * p + j + 2], rounded_sum1);\n            _mm_storeu_si128((__m128i*)&dst[i * p + j + 4], rounded_sum2);\n            _mm_storeu_si128((__m128i*)&dst[i * p + j + 6], rounded_sum3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int vec_size = 4;\n    for (size_t i = 0; i < rows; i += vec_size) {\n        for (size_t j = 0; j < cols; j++) {\n            __m128 row_vec = _mm_loadu_ps(src + i * cols + j);\n            __m128 rounded_row_vec = _mm_add_ps(_mm_mul_ps(row_vec, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            rounded_row_vec = _mm_cvtepi32_ps(_mm_cvtps_epi32(rounded_row_vec));\n            rounded_row_vec = _mm_div_ps(rounded_row_vec, _mm_set1_ps(4.0f));\n\n            _mm_storeu_ps(dst + j * rows + i, rounded_row_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```cpp\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int vec_size = 4;\n    for (size_t i = 0; i < rows; i += vec_size) {\n        for (size_t j = 0; j < cols; j++) {\n            __m128 row_vec = _mm_loadu_ps(src + i * cols + j);\n            __m128 rounded_vec = _mm_add_ps(_mm_mul_ps(row_vec, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            rounded_vec = _mm_cvtepi32_ps(_mm_cvtps_epi32(rounded_vec));\n            _mm_storeu_ps(dst + j * rows + i, rounded_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```cpp\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int vec_size = 4;\n    for (size_t i = 0; i < rows; i += vec_size) {\n        for (size_t j = 0; j < cols; j++) {\n            __m128 row_vec = _mm_loadu_ps(src + i * cols + j);\n            __m128 rounded_vec = _mm_add_ps(_mm_mul_ps(row_vec, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            rounded_vec = _mm_cvtepi32_ps(_mm_cvtps_epi32(rounded_vec));\n            _mm_storeu_ps(dst + j * rows + i, rounded_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int vec_size = 4;\n    for (size_t i = 0; i < rows; i += vec_size) {\n        for (size_t j = 0; j < cols; j++) {\n            __m128 row_vec = _mm_loadu_ps(src + i * cols + j);\n            __m128 rounded_row_vec = _mm_add_ps(_mm_mul_ps(row_vec, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            rounded_row_vec = _mm_cvtepi32_ps(_mm_cvtps_epi32(rounded_row_vec));\n            rounded_row_vec = _mm_div_ps(rounded_row_vec, _mm_set1_ps(4.0f));\n\n            _mm_storeu_ps(dst + j * rows + i, rounded_row_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const int vec_size = 4;\n    for (size_t i = 0; i < rows; i += vec_size) {\n        for (size_t j = 0; j < cols; j++) {\n            __m128 row_vec = _mm_loadu_ps(src + i * cols + j);\n            __m128 rounded_row_vec = _mm_add_ps(_mm_mul_ps(row_vec, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            rounded_row_vec = _mm_cvtepi32_ps(_mm_cvtps_epi32(rounded_row_vec));\n            rounded_row_vec = _mm_div_ps(rounded_row_vec, _mm_set1_ps(4.0f));\n\n            _mm_storeu_ps(dst + j * rows + i, rounded_row_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i, j, k;\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j < dim2; j += 4) {\n            __m128 v1 = _mm_loadu_ps(&tensor1[i * dim2 * dim3 + j * dim3]);\n            __m128 v2 = _mm_loadu_ps(&tensor2[i * dim2 * dim3 + j * dim3]);\n            __m128 diff = _mm_sub_ps(v1, v2);\n            __m128 rounded_diff = _mm_setzero_ps();\n            __asm__ volatile (\n                \"roundss %1, %0, %2\\n\"\n                : \"=x\" (rounded_diff)\n                : \"x\" (diff), \"N\" (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)\n            );\n            _mm_storeu_ps(&dst[i * dim2 * dim3 + j * dim3], rounded_diff);\n        }\n        for (; j < dim2; ++j) {\n            dst[i * dim2 * dim3 + j * dim3] = tensor1[i * dim2 * dim3 + j * dim3] - tensor2[i * dim2 * dim3 + j * dim3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 v2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 sub = _mm_sub_ps(v1, v2);\n        __m128i rounded = _mm_cvtps_epi32(sub);\n        __m128 result = _mm_cvtepi32_ps(rounded);\n        _mm_storeu_ps(&dst[i], result);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 v2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 sub = _mm_sub_ps(v1, v2);\n        __m128i rounded = _mm_cvtps_epi32(sub);\n        __m128 result = _mm_cvtepi32_ps(rounded);\n        _mm_storeu_ps(&dst[i], result);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i, j, k;\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j < dim2; j += 4) {\n            __m128 v1 = _mm_loadu_ps(&tensor1[i * dim2 * dim3 + j * dim3]);\n            __m128 v2 = _mm_loadu_ps(&tensor2[i * dim2 * dim3 + j * dim3]);\n            __m128 diff = _mm_sub_ps(v1, v2);\n            __m128 rounded_diff = _mm_setzero_ps();\n            __asm__ volatile (\n                \"movaps %1, %%xmm2\\n\"\n                \"movaps %2, %%xmm3\\n\"\n                \"movaps %%xmm2, %%xmm0\\n\"\n                \"movaps %%xmm3, %%xmm1\\n\"\n                \"psubusw %%xmm1, %%xmm0\\n\"\n                \"paddusw %%xmm3, %%xmm0\\n\"\n                \"movaps %%xmm0, %%xmm1\\n\"\n                \"psubusw %%xmm2, %%xmm1\\n\"\n                \"paddusw %%xmm2, %%xmm1\\n\"\n                \"movaps %%xmm1, %%xmm2\\n\"\n                \"psubusw %%xmm3, %%xmm2\\n\"\n                \"paddusw %%xmm3, %%xmm2\\n\"\n                \"movaps %%xmm2, %%xmm3\\n\"\n                \"psubusw %%xmm0, %%xmm3\\n\"\n                \"paddusw %%xmm0, %%xmm3\\n\"\n                \"movaps %%xmm3, %%xmm0\\n\"\n                \"psubusw %%xmm1, %%xmm0\\n\"\n                \"paddusw %%xmm1, %%xmm0\\n\"\n                \"movaps %%xmm0, %0\\n\"\n                : \"=m\" (rounded_diff)\n                : \"m\" (diff), \"m\" (rounded_diff)\n                : \"%xmm0\", \"%xmm1\", \"%xmm2\", \"%xmm3\"\n            );\n            _mm_storeu_ps(&dst[i * dim2 * dim3 + j * dim3], rounded_diff);\n        }\n        for (; j < dim2; ++j) {\n            dst[i * dim2 * dim3 + j * dim3] = static_cast<float>(static_cast<int>(tensor1[i * dim2 * dim3 + j * dim3] - tensor2[i * dim2 * dim3 + j * dim3]));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i = 0;\n    for (; i <= dim1 * dim2 * dim3 - 4; i += 4) {\n        __m128 v1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 v2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 sub = _mm_sub_ps(v1, v2);\n        __m128i rounded = _mm_cvtps_epi32(sub);\n        __m128 rsub = _mm_cvtepi32_ps(rounded);\n        _mm_storeu_ps(&dst[i], rsub);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m128d v1 = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d v2 = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d v3 = _mm_mul_pd(v1, v2);\n            _mm_storeu_pd(&dst[i * n + j], v3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m128d v1 = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d v2 = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d v3 = _mm_mul_pd(v1, v2);\n            _mm_storeu_pd(&dst[i * n + j], v3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m128d v1 = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d v2 = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d v3 = _mm_mul_pd(v1, v2);\n            _mm_storeu_pd(&dst[i * n + j], v3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m128d v1 = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d v2 = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d v3 = _mm_mul_pd(v1, v2);\n            _mm_storeu_pd(&dst[i * n + j], v3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m128d v1 = _mm_loadu_pd(&mat1[i * n + j]);\n            __m128d v2 = _mm_loadu_pd(&mat2[i * n + j]);\n            __m128d v3 = _mm_mul_pd(v1, v2);\n            _mm_storeu_pd(&dst[i * n + j], v3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m128 v0, v1, v2, v3;\n    __m128 sum;\n\n    for (size_t i = 0; i < m; ++i) {\n        sum = _mm_setzero_ps();\n\n        for (size_t j = 0; j < n; j += 4) {\n            v0 = _mm_loadu_ps(&mat[i * n + j]);\n            v1 = _mm_loadu_ps(&vec[j]);\n\n            sum = _mm_add_ps(sum, _mm_mul_ps(v0, v1));\n        }\n\n        _mm_storeu_ps(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m128 v0, v1, v2, v3;\n    __m128 sum;\n\n    for (size_t i = 0; i < m; ++i) {\n        sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            v0 = _mm_loadu_ps(&mat[i * n + j]);\n            v1 = _mm_loadu_ps(&vec[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(v0, v1));\n        }\n        _mm_storeu_ps(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 col = _mm_loadu_ps(&vec[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(row, col));\n        }\n        _mm_storeu_ps(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 row = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 col = _mm_loadu_ps(&vec[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(row, col));\n        }\n        _mm_storeu_ps(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m128 v0, v1, v2, v3;\n    __m128 sum;\n\n    for (size_t i = 0; i < m; ++i) {\n        sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            v0 = _mm_loadu_ps(&mat[i * n + j]);\n            v1 = _mm_loadu_ps(&vec[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(v0, v1));\n        }\n        _mm_storeu_ps(&dst[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t total_elements = m * n;\n    size_t i;\n\n    // Process elements in blocks of 4\n    for (i = 0; i <= total_elements - 4; i += 4) {\n        __m128d row_sum = _mm_loadu_pd(&mat[i]);\n        sum = _mm_add_pd(sum, row_sum);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        sum = _mm_add_sd(sum, _mm_set_sd(mat[i]));\n    }\n\n    // Sum up all partial sums\n    double temp[2];\n    _mm_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1];\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i, j;\n    size_t aligned_n = n & ~0x3; // Align n to the nearest multiple of 4\n\n    // Process aligned part of the matrix\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < aligned_n; j += 4) {\n            __m128d row = _mm_loadu_pd(&mat[i * n + j]);\n            sum = _mm_add_pd(sum, row);\n        }\n    }\n\n    // Process remaining elements\n    for (; j < n; ++j) {\n        sum[0] += mat[i * n + j];\n    }\n\n    // Horizontal add to get the total sum\n    sum = _mm_hadd_pd(sum, sum);\n    sum = _mm_hadd_pd(sum, sum);\n\n    // Calculate the average\n    double total_sum = sum[0];\n    double average = total_sum / (m * n);\n\n    return average;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i, j;\n    size_t num_elements = m * n;\n\n    // Process elements in blocks of 4\n    for (i = 0; i < num_elements / 4; ++i) {\n        __m128d row_sum = _mm_setzero_pd();\n        for (j = 0; j < n; ++j) {\n            row_sum = _mm_add_sd(row_sum, _mm_loadu_pd(&mat[i * n + j]));\n        }\n        sum = _mm_add_pd(sum, row_sum);\n    }\n\n    // Handle remaining elements\n    for (; i < num_elements; ++i) {\n        sum = _mm_add_sd(sum, _mm_loadu_pd(&mat[i * n]));\n    }\n\n    // Sum up all partial sums\n    double temp[2];\n    _mm_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1];\n\n    // Calculate average\n    return total_sum / num_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```cpp\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i, j;\n    size_t num_elements = m * n;\n\n    // Process elements in blocks of 4\n    for (i = 0; i + 3 < num_elements; i += 4) {\n        __m128d row1 = _mm_loadu_pd(&mat[i]);\n        __m128d row2 = _mm_loadu_pd(&mat[i + 2]);\n        sum = _mm_add_pd(sum, row1);\n        sum = _mm_add_pd(sum, row2);\n    }\n\n    // Process remaining elements\n    for (; i < num_elements; ++i) {\n        sum = _mm_add_sd(sum, _mm_set_sd(mat[i]));\n    }\n\n    // Horizontal add to get the total sum\n    sum = _mm_hadd_pd(sum, sum);\n\n    // Extract the result from the sum vector\n    double total_sum;\n    _mm_store_sd(&total_sum, sum);\n\n    return total_sum / num_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i, j;\n    size_t num_elements = m * n;\n\n    // Process elements in blocks of 4\n    for (i = 0; i + 3 < num_elements; i += 4) {\n        __m128d row1 = _mm_loadu_pd(&mat[i]);\n        __m128d row2 = _mm_loadu_pd(&mat[i + 2]);\n        sum = _mm_add_pd(sum, row1);\n        sum = _mm_add_pd(sum, row2);\n    }\n\n    // Handle remaining elements\n    for (; i < num_elements; ++i) {\n        sum = _mm_add_sd(sum, _mm_set_sd(mat[i]));\n    }\n\n    // Sum up all partial sums\n    double temp[2];\n    _mm_storeu_pd(temp, sum);\n    double total_sum = temp[0] + temp[1];\n\n    return total_sum / num_elements;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128i prev = _mm_loadu_si128((__m128i*)(matrix + i * cols));\n        __m128i curr = _mm_loadu_si128((__m128i*)(matrix + i * cols + 4));\n\n        for (size_t j = 8; j < cols; j += 8) {\n            __m128i next = _mm_loadu_si128((__m128i*)(matrix + i * cols + j));\n\n            if (directions[i]) {\n                __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n                if (_mm_movemask_epi8(cmp) != 0) return false;\n\n                cmp = _mm_cmpgt_epi32(curr, next);\n                if (_mm_movemask_epi8(cmp) != 0) return false;\n            } else {\n                __m128i cmp = _mm_cmplt_epi32(prev, curr);\n                if (_mm_movemask_epi8(cmp) != 0) return false;\n\n                cmp = _mm_cmplt_epi32(curr, next);\n                if (_mm_movemask_epi8(cmp) != 0) return false;\n            }\n\n            prev = curr;\n            curr = next;\n        }\n\n        // Handle remaining elements that don't fit into a full 128-bit chunk\n        for (; j < cols; ++j) {\n            int p = *(matrix + i * cols + j - 4);\n            int c = *(matrix + i * cols + j - 3);\n            int n = *(matrix + i * cols + j - 2);\n\n            if (directions[i]) {\n                if (c > p || c > n) return false;\n            } else {\n                if (c < p || c < n) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128i prev = _mm_loadu_si128((__m128i*)(matrix + i * cols));\n        __m128i curr = _mm_loadu_si128((__m128i*)(matrix + i * cols + 4));\n\n        for (size_t j = 8; j < cols; j += 4) {\n            __m128i next = _mm_loadu_si128((__m128i*)(matrix + i * cols + j));\n\n            if (directions[i]) {\n                __m128i cmp1 = _mm_cmpgt_epi32(prev, curr);\n                __m128i cmp2 = _mm_cmpgt_epi32(curr, next);\n\n                if (_mm_movemask_epi8(cmp1) != 0 || _mm_movemask_epi8(cmp2) != 0) {\n                    return false;\n                }\n            } else {\n                __m128i cmp1 = _mm_cmplt_epi32(prev, curr);\n                __m128i cmp2 = _mm_cmplt_epi32(curr, next);\n\n                if (_mm_movemask_epi8(cmp1) != 0 || _mm_movemask_epi8(cmp2) != 0) {\n                    return false;\n                }\n            }\n\n            prev = curr;\n            curr = next;\n        }\n\n        // Handle remaining elements that don't fit into a full 128-bit chunk\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            int prev_val = *(matrix + i * cols + j - 1);\n            int curr_val = *(matrix + i * cols + j);\n\n            if (directions[i] && prev_val > curr_val) {\n                return false;\n            } else if (!directions[i] && prev_val < curr_val) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m12i zero = _mm_setzero_si128();\n    __m12i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m12i prev = _mm_loadu_si128((__m12i*)(matrix + i * cols));\n        __m12i mask = _mm_set1_epi32(directions[i] ? -1 : 1);\n\n        for (size_t j = 1; j < cols; j += 4) {\n            __m12i curr = _mm_loadu_si128((__m12i*)(matrix + i * cols + j));\n\n            __m12i diff = _mm_sub_epi32(curr, prev);\n            __m12i sign = _mm_srai_epi32(diff, 31);\n            __m12i cmp = _mm_and_si128(sign, mask);\n\n            if (!_mm_test_all_zeros(cmp, cmp)) {\n                return false;\n            }\n\n            prev = curr;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m12i zero = _mm_setzero_si128();\n    __m12i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m12i prev = _mm_loadu_si128((__m12i*)(matrix + i * cols));\n        __m12i mask = _mm_set1_epi32(directions[i] ? -1 : 1);\n\n        for (size_t j = 1; j < cols; j += 4) {\n            __m12i curr = _mm_loadu_si128((__m12i*)(matrix + i * cols + j));\n\n            __m12i diff = _mm_sub_epi32(curr, prev);\n            __m12i sign = _mm_srai_epi32(diff, 31);\n            __m12i cmp = _mm_and_si128(sign, mask);\n\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                return false;\n            }\n\n            prev = curr;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m12i zero = _mm_setzero_si128();\n    __m12i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m12i prev = _mm_loadu_si128((__m12i*)(matrix + i * cols));\n        __m12i mask = _mm_setzero_si128();\n\n        for (size_t j = 1; j < cols; j += 4) {\n            __m12i curr = _mm_loadu_si128((__m12i*)(matrix + i * cols + j));\n\n            __m12i cmp = _mm_cmplt_epi32(prev, curr);\n            __m12i dir = _mm_cvtepi8_epi32(_mm_loadu_si128((__m12i*)(directions + i * cols + j / 4)));\n            __m12i cond = _mm_and_si128(cmp, dir);\n            mask = _mm_or_si128(mask, cond);\n\n            prev = curr;\n        }\n\n        if (_mm_movemask_epi8(mask) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```cpp\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i v_base = _mm_set1_epi16(base);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_max = _mm_set1_epi16(INT16_MAX);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_quotient = _mm_srli_epi16(v_src, 4); // Divide by 16 (approximation)\n        __m128i v_mul = _mm_mullo_epi16(v_quotient, v_base);\n        __m128i v_diff = _mm_sub_epi16(v_src, v_mul);\n        __m128i v_abs_diff = _mm_andnot_si128(_mm_cmpgt_epi16(v_diff, v_zero), v_diff);\n        __m128i v_next = _mm_add_epi16(v_mul, v_base);\n        __m128i v_min_diff = _mm_min_epi16(v_abs_diff, _mm_sub_epi16(v_next, v_src));\n        __m128i v_result = _mm_or_si128(_mm_cmplt_epi16(v_abs_diff, _mm_sub_epi16(v_next, v_src)), v_mul);\n        __m128i v_clamp = _mm_min_epi16(v_result, v_max);\n        _mm_storeu_si128((__m128i*)&dst[i], v_clamp);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i v_base = _mm_set1_epi16(base);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_quotient = _mm_srli_epi16(v_src, 4); // Divide by 16 (base)\n        __m128i v_mul = _mm_mullo_epi16(v_quotient, v_base);\n        __m128i v_diff = _mm_sub_epi16(v_src, v_mul);\n        __m128i v_mask = _mm_cmplt_epi16(v_diff, v_zero);\n        __m128i v_result = _mm_add_epi16(v_mul, v_base);\n\n        v_result = _mm_andnot_si128(v_mask, v_result);\n        v_result = _mm_or_si128(v_result, v_mul);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m128i base_vec = _mm_set1_epi16(base);\n    const __m128i zero_vec = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i div_result = _mm_div_epi16(src_vec, base_vec);\n        __m128i mul_result = _mm_mullo_epi16(div_result, base_vec);\n        __m128i sub_result = _mm_sub_epi16(src_vec, mul_result);\n        __m128i cmp_result = _mm_cmplt_epi16(sub_result, zero_vec);\n        __m128i add_one = _mm_add_epi16(mul_result, base_vec);\n        __m128i final_result = _mm_blendv_epi16(mul_result, add_one, cmp_result);\n\n        _mm_storeu_si128((__m128i*)&dst[i], final_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] / base) * base + ((src[i] % base != 0) ? base : 0);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```cpp\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i v_base = _mm_set1_epi16(base);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_max = _mm_set1_epi16(INT16_MAX);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_quotient = _mm_srli_epi16(v_src, 4); // Divide by 16 (approximation)\n        __m128i v_mul = _mm_mullo_epi16(v_quotient, v_base);\n        __m128i v_diff = _mm_sub_epi16(v_src, v_mul);\n        __m128i v_abs_diff = _mm_andnot_si128(_mm_cmpgt_epi16(v_diff, v_zero), v_diff);\n        __m128i v_next = _mm_add_epi16(v_mul, v_base);\n        __m128i v_min_diff = _mm_min_epi16(v_abs_diff, _mm_andnot_si128(_mm_cmpgt_epi16(v_next, v_src), v_next));\n        __m128i v_result = _mm_or_si128(_mm_cmplt_epi16(v_abs_diff, v_min_diff), v_next);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```cpp\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m128i v_base = _mm_set1_epi16(base);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i v_quotient = _mm_srli_epi16(v_src, 4); // Divide by 16 (approximation)\n        __m128i v_mul = _mm_mullo_epi16(v_quotient, v_base);\n        __m128i v_diff = _mm_sub_epi16(v_src, v_mul);\n        __m128i v_mask = _mm_cmplt_epi16(v_diff, v_base);\n        __m128i v_add = _mm_and_si128(v_mask, v_base);\n        __m128i v_result = _mm_add_epi16(v_mul, v_add);\n\n        _mm_storeu_si128((__m128i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 8; i < length; ++i) {\n        dst[i] = (src[i] / base) * base + (src[i] % base >= base / 2 ? base : 0);\n    }\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d v0 = _mm_loadu_pd(vector);\n    __m128d v1 = _mm_loadu_pd(vector + 2);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d m0 = _mm_loadu_pd(matrix + i * cols);\n        __m128d m1 = _mm_loadu_pd(matrix + i * cols + 2);\n\n        __m128d cmp0 = _mm_cmpeq_pd(m0, v0);\n        __m128d cmp1 = _mm_cmpeq_pd(m1, v1);\n\n        int mask0 = _mm_movemask_epi8(cmp0);\n        int mask1 = _mm_movemask_epi8(cmp1);\n\n        if ((mask0 & 0xF) == 0xF || (mask1 & 0xF) == 0xF) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d v0 = _mm_loadu_pd(vector);\n    __m128d v1 = _mm_loadu_pd(vector + 2);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d m0 = _mm_loadu_pd(matrix + i * cols);\n        __m128d m1 = _mm_loadu_pd(matrix + i * cols + 2);\n\n        __m128d cmp0 = _mm_cmpeq_pd(m0, v0);\n        __m128d cmp1 = _mm_cmpeq_pd(m1, v1);\n\n        int mask0 = _mm_movemask_epi8(_mm_castpd_si128(cmp0));\n        int mask1 = _mm_movemask_epi8(_mm_castpd_si128(cmp1));\n\n        if ((mask0 & 0x3) == 0x3 || (mask1 & 0x3) == 0x3) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d v0 = _mm_loadu_pd(vector);\n    __m128d v1 = _mm_loadu_pd(vector + 2);\n    __m128d v2 = _mm_loadu_pd(vector + 4);\n    __m128d v3 = _mm_loadu_pd(vector + 6);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d m0 = _mm_loadu_pd(matrix + i * cols);\n        __m128d m1 = _mm_loadu_pd(matrix + i * cols + 2);\n        __m128d m2 = _mm_loadu_pd(matrix + i * cols + 4);\n        __m128d m3 = _mm_loadu_pd(matrix + i * cols + 6);\n\n        __m128d cmp0 = _mm_cmpeq_pd(m0, v0);\n        __m128d cmp1 = _mm_cmpeq_pd(m1, v1);\n        __m128d cmp2 = _mm_cmpeq_pd(m2, v2);\n        __m128d cmp3 = _mm_cmpeq_pd(m3, v3);\n\n        __m128d mask = _mm_and_si128(_mm_and_si128(_mm_castps_si128(cmp0), _mm_castps_si128(cmp1)),\n                                     _mm_and_si128(_mm_castps_si128(cmp2), _mm_castps_si128(cmp3)));\n\n        if (_mm_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d v0 = _mm_loadu_pd(vector);\n    __m128d v1 = _mm_loadu_pd(vector + 2);\n    __m128d v2 = _mm_loadu_pd(vector + 4);\n    __m128d v3 = _mm_loadu_pd(vector + 6);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d m0 = _mm_loadu_pd(matrix + i * cols);\n        __m128d m1 = _mm_loadu_pd(matrix + i * cols + 2);\n        __m128d m2 = _mm_loadu_pd(matrix + i * cols + 4);\n        __m128d m3 = _mm_loadu_pd(matrix + i * cols + 6);\n\n        __m128d cmp0 = _mm_cmpeq_pd(m0, v0);\n        __m128d cmp1 = _mm_cmpeq_pd(m1, v1);\n        __m128d cmp2 = _mm_cmpeq_pd(m2, v2);\n        __m128d cmp3 = _mm_cmpeq_pd(m3, v3);\n\n        __m128d mask = _mm_and_si128(_mm_castps_si128(cmp0), _mm_castps_si128(cmp1));\n        mask = _mm_and_si128(mask, _mm_castps_si128(cmp2));\n        mask = _mm_and_si128(mask, _mm_castps_si128(cmp3));\n\n        if (_mm_movemask_epi8(mask) == 0xFFFF) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```cpp\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d v0 = _mm_loadu_pd(vector);\n    __m128d v1 = _mm_loadu_pd(vector + 2);\n    __m128d v2 = _mm_loadu_pd(vector + 4);\n    __m128d v3 = _mm_loadu_pd(vector + 6);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d m0 = _mm_loadu_pd(matrix + i * cols);\n        __m128d m1 = _mm_loadu_pd(matrix + i * cols + 2);\n        __m128d m2 = _mm_loadu_pd(matrix + i * cols + 4);\n        __m128d m3 = _mm_loadu_pd(matrix + i * cols + 6);\n\n        __m128d cmp0 = _mm_cmpeq_pd(m0, v0);\n        __m128d cmp1 = _mm_cmpeq_pd(m1, v1);\n        __m128d cmp2 = _mm_cmpeq_pd(m2, v2);\n        __m128d cmp3 = _mm_cmpeq_pd(m3, v3);\n\n        __m128d mask = _mm_and_si128(_mm_castpd_si128(cmp0), _mm_castpd_si128(cmp1));\n        mask = _mm_and_si128(mask, _mm_castpd_si128(cmp2));\n        mask = _mm_and_si128(mask, _mm_castpd_si128(cmp3));\n\n        if (_mm_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 16; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq_mask = _mm_cmpeq_epi8(vecA, vecB);\n        int eq_count = _mm_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 16; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq_mask = _mm_cmpeq_epi8(vecA, vecB);\n        int eq_count = _mm_popcnt_epi8(eq_mask);\n        count += eq_count;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 16; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq_mask = _mm_cmpeq_epi8(vecA, vecB);\n        int eq_count = _mm_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 16; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq_mask = _mm_cmpeq_epi8(vecA, vecB);\n        int eq_count = _mm_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```cpp\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    for (i = 0; i <= total_elements - 16; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i eq_mask = _mm_cmpeq_epi8(vecA, vecB);\n        int eq_count = _mm_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```cpp\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i va = _mm_set1_epi64x(a);\n    size_t i;\n\n    for (i = 0; i <= length - 2; i += 2) {\n        __m128i vx = _mm_load_si128((__m128i *)&x[i]);\n        __m128i vy = _mm_mul_epi64(vx, va);\n        vy = _mm_and_si128(vy, _mm_set1_epi64x(0x7FFFFFFFFFFFFFFF));\n        vy = _mm_sub_epi64(_mm_setzero_si128(), vy);\n        vy = _mm_andnot_si128(_mm_cmpeq_epi64(vy, _mm_setzero_si128()), vy);\n        _mm_store_si128((__m128i *)&y[i], vy);\n    }\n\n    if (length % 2 != 0) {\n        y[length - 1] = std::abs(x[length - 1] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```cpp\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i vec_x = _mm_load_si128((__m128i *)&x[i]);\n        __m128i vec_a = _mm_set1_epi64x(a);\n        __m128i vec_result = _mm_mul_epi64(vec_x, vec_a);\n\n        __m128i abs_vec_result = _mm_andnot_si128(_mm_cmpgt_epi64(vec_result, _mm_setzero_si128()), vec_result);\n        _mm_store_si128((__m128i *)&y[i], abs_vec_result);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```cpp\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 2; i += 2) {\n        __m128i vec_x = _mm_load_si128((__m128i *)&x[i]);\n        __m128i vec_a = _mm_set1_epi64x(a);\n        __m128i vec_result = _mm_mul_epi64(vec_x, vec_a);\n\n        __m128i abs_result = _mm_andnot_si128(_mm_cmpgt_epi64(vec_result, _mm_setzero_si128()), vec_result);\n        abs_result = _mm_or_si128(abs_result, _mm_cmplt_epi64(vec_result, _mm_setzero_si128()));\n\n        _mm_store_si128((__m128i *)&y[i], abs_result);\n    }\n\n    // Handle remaining elements if length is odd\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```cpp\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m128i va = _mm_set1_epi64x(a);\n    for (size_t i = 0; i <= length - 2; i += 2) {\n        __m128i vx = _mm_load_si128((__m128i *)&x[i]);\n        __m128i vy = _mm_mul_epi64(vx, va);\n        vy = _mm_andnot_si128(_mm_cmpgt_epi64(vy, _mm_setzero_si128()), vy);\n        _mm_store_si128((__m128i *)&y[i], vy);\n    }\n    if (length % 2 != 0) {\n        y[length - 1] = std::abs(x[length - 1] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i vec_x = _mm_load_si128((__m128i *)&x[i]);\n        __m128i vec_a = _mm_set1_epi64x(a);\n        __m128i vec_result = _mm_mul_epi64(vec_x, vec_a);\n\n        __m128i abs_vec_result = _mm_andnot_si128(_mm_cmpgt_epi64(vec_result, _mm_setzero_si128()), vec_result);\n        _mm_store_si128((__m128i *)&y[i], abs_vec_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        y[i] = std::abs(x[i] * a);\n    }\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, current_vals);\n        max_val = _mm_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[8];\n    int16_t local_max[8];\n\n    _mm_storeu_si128((__m128i*)local_min, min_val);\n    _mm_storeu_si128((__m128i*)local_max, max_val);\n\n    int16_t global_min = local_min[0];\n    int16_t global_max = local_max[0];\n\n    for (int j = 1; j < 8; ++j) {\n        if (local_min[j] < global_min) {\n            global_min = local_min[j];\n        }\n        if (local_max[j] > global_max) {\n            global_max = local_max[j];\n        }\n    }\n\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if (vec[i] < global_min) {\n            global_min = vec[i];\n        }\n        if (vec[i] > global_max) {\n            global_max = vec[i];\n        }\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, current_vals);\n        max_val = _mm_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[8];\n    int16_t local_max[8];\n\n    _mm_storeu_si128((__m128i*)local_min, min_val);\n    _mm_storeu_si128((__m128i*)local_max, max_val);\n\n    int16_t global_min = local_min[0];\n    int16_t global_max = local_max[0];\n\n    for (int j = 1; j < 8; ++j) {\n        if (local_min[j] < global_min) {\n            global_min = local_min[j];\n        }\n        if (local_max[j] > global_max) {\n            global_max = local_max[j];\n        }\n    }\n\n    for (size_t i = length - length % 8; i < length; ++i) {\n        if (vec[i] < global_min) {\n            global_min = vec[i];\n        }\n        if (vec[i] > global_max) {\n            global_max = vec[i];\n        }\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```cpp\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i <= length - 8; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, current_vals);\n        max_val = _mm_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[8], local_max[8];\n    _mm_storeu_si128((__m128i*)local_min, min_val);\n    _mm_storeu_si128((__m128i*)local_max, max_val);\n\n    int16_t global_min = local_min[0];\n    int16_t global_max = local_max[0];\n\n    for (int j = 1; j < 8; ++j) {\n        if (local_min[j] < global_min) {\n            global_min = local_min[j];\n        }\n        if (local_max[j] > global_max) {\n            global_max = local_max[j];\n        }\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]);\n\n        min_val = _mm_min_epi16(min_val, current_vals);\n        max_val = _mm_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[8];\n    int16_t local_max[8];\n\n    _mm_storeu_si128((__m128i*)local_min, min_val);\n    _mm_storeu_si128((__m128i*)local_max, max_val);\n\n    int16_t global_min = INT16_MAX;\n    int16_t global_max = INT16_MIN;\n\n    for (int j = 0; j < 8; ++j) {\n        if (local_min[j] < global_min) global_min = local_min[j];\n        if (local_max[j] > global_max) global_max = local_max[j];\n    }\n\n    return {global_min, global_max};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&vec[i]);\n        min_val = _mm_min_epi16(min_val, current_vals);\n        max_val = _mm_max_epi16(max_val, current_vals);\n    }\n\n    int16_t local_min[8];\n    int16_t local_max[8];\n\n    _mm_storeu_si128((__m128i*)local_min, min_val);\n    _mm_storeu_si128((__m128i*)local_max, max_val);\n\n    int16_t global_min = local_min[0];\n    int16_t global_max = local_max[0];\n\n    for (int j = 1; j < 8; ++j) {\n        if (local_min[j] < global_min) {\n            global_min = local_min[j];\n        }\n        if (local_max[j] > global_max) {\n            global_max = local_max[j];\n        }\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m128i* row_ptr = reinterpret_cast<const __m128i*>(matrix + i * cols);\n        bool row_increasing = true;\n\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m128i current_row = _mm_loadu_si128(row_ptr++);\n            __m128i next_row = _mm_loadu_si128(row_ptr++);\n\n            __m128i diff = _mm_sub_epi32(next_row, current_row);\n            __m128i mask = _mm_cmpgt_epi32(diff, zero);\n\n            if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                row_increasing = false;\n                break;\n            }\n        }\n\n        if (row_increasing && cols % 4 != 0) {\n            const int* remaining_elements = reinterpret_cast<const int*>(row_ptr);\n            for (size_t j = 0; j < cols % 4; ++j) {\n                if (remaining_elements[j] <= remaining_elements[j + 1]) {\n                    return false;\n                }\n            }\n        }\n\n        if (!row_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m128i* row_ptr = reinterpret_cast<const __m128i*>(matrix + i * cols);\n\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m128i current_row = _mm_loadu_si128(row_ptr + j);\n            __m128i next_row = _mm_loadu_si128(row_ptr + j + 1);\n\n            __m128i cmp_result = _mm_cmpgt_epi32(current_row, next_row);\n            if (_mm_movemask_epi8(cmp_result) != 0xFFFF) {\n                return false;\n            }\n        }\n\n        // Handle remaining elements that don't fit into a full 128-bit chunk\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (j > 0 && matrix[i * cols + j] <= matrix[i * cols + j - 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m128i* row_ptr = reinterpret_cast<const __m128i*>(matrix + i * cols);\n\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m128i current_row = _mm_loadu_si128(row_ptr++);\n            __m128i next_row = _mm_loadu_si128(row_ptr++);\n\n            __m128i cmp_result = _mm_cmpgt_epi32(current_row, next_row);\n            if (_mm_movemask_epi8(cmp_result) != 0xFFFF) {\n                return false;\n            }\n        }\n\n        // Handle remaining elements that do not fit into a full 128-bit register\n        for (size_t j = (cols / 4) * 4; j < cols; ++j) {\n            if (matrix[i * cols + j] <= matrix[i * cols + j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m128i* row_ptr = reinterpret_cast<const __m128i*>(matrix + i * cols);\n        bool row_increasing = true;\n\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m128i current_row = _mm_loadu_si128(row_ptr++);\n            __m128i next_row = _mm_loadu_si128(row_ptr++);\n\n            __m128i diff = _mm_sub_epi32(next_row, current_row);\n            __m128i mask = _mm_cmpgt_epi32(diff, zero);\n\n            if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                row_increasing = false;\n                break;\n            }\n        }\n\n        if (row_increasing && cols % 4 != 0) {\n            const int* remaining_elements = reinterpret_cast<const int*>(row_ptr);\n            for (size_t j = 0; j < cols % 4; ++j) {\n                if (!(remaining_elements[j] > remaining_elements[j + 1])) {\n                    row_increasing = false;\n                    break;\n                }\n            }\n        }\n\n        if (!row_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const __m128i* row_ptr = reinterpret_cast<const __m128i*>(matrix + i * cols);\n        bool row_increasing = true;\n\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m128i current_row = _mm_loadu_si128(row_ptr++);\n            __m128i next_row = _mm_loadu_si128(row_ptr++);\n\n            __m128i diff = _mm_sub_epi32(next_row, current_row);\n            __m128i mask = _mm_cmpgt_epi32(diff, zero);\n\n            if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                row_increasing = false;\n                break;\n            }\n        }\n\n        if (row_increasing && cols % 4 != 0) {\n            for (size_t j = cols - (cols % 4); j < cols; ++j) {\n                if (matrix[i * cols + j] <= matrix[i * cols + j + 1]) {\n                    return false;\n                }\n            }\n        }\n\n        if (!row_increasing) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m128i first_block = _mm_loadu_si128((__m128i*)(matrix));\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            __m128i current_block = _mm_loadu_si128((__m128i*)(matrix + i * cols + j));\n            if (!_mm_cmpeq_epi32(current_block, first_block)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m12i first_block = _mm_loadu_si128((__m12i*)matrix);\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            const int* block_start = &matrix[i * cols + j];\n            __m12i current_block = _mm_loadu_si128((__m12i*)block_start);\n\n            __m12i cmp_result = _mm_cmpeq_epi32(first_block, current_block);\n            int mask = _mm_movemask_epi8(cmp_result);\n\n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m12i first_block = _mm_loadu_si128((__m12i*)matrix);\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            const int* block_ptr = &matrix[i * cols + j];\n            __m12i current_block = _mm_loadu_si128((__m12i*)block_ptr);\n\n            __m12i cmp_result = _mm_cmpeq_epi32(first_block, current_block);\n            int mask = _mm_movemask_epi8(cmp_result);\n\n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m12i first_block = _mm_loadu_si128((__m12i*)matrix);\n\n    size_t num_blocks_row = cols / block_size;\n    size_t num_blocks_col = rows / block_size;\n\n    for (size_t i = 1; i <= num_blocks_col; ++i) {\n        const int* row_start = matrix + i * block_size * cols;\n        for (size_t j = 0; j < num_blocks_row; ++j) {\n            __m12i current_block = _mm_loadu_si128((__m12i*)(row_start + j * block_size));\n            if (!_mm_cmpeq_epi32(current_block, first_block)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    __m12i first_block = _mm_loadu_si128((__m12i*)matrix);\n\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            const int* block_ptr = &matrix[i * cols + j];\n            __m12i current_block = _mm_loadu_si128((__m12i*)block_ptr);\n\n            __m12i cmp_result = _mm_cmpeq_epi32(first_block, current_block);\n            int mask = _mm_movemask_epi8(cmp_result);\n\n            if (mask != 0xFFFF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 4 != 0) {\n        return false;\n    }\n\n    __m128d v_tolerance = _mm_set1_pd(tolerance);\n\n    for (size_t i = 0; i <= length - block_size; i += block_size) {\n        __m128d v_first_block = _mm_loadu_pd(&vec[i]);\n        for (size_t j = i + block_size; j < i + 2 * block_size; j += 8) {\n            __m128d v_current_block = _mm_loadu_pd(&vec[j]);\n            __m128d v_diff = _mm_sub_pd(v_current_block, v_first_block);\n            __m128d v_abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(v_diff, _mm_setzero_pd()), v_diff);\n            __m128d v_abs_diff_max = _mm_max_pd(_mm_max_pd(v_abs_diff, _mm_shuffle_pd(v_abs_diff, v_abs_diff, 1)), _mm_shuffle_pd(v_abs_diff, v_abs_diff, 2));\n            __m128d v_abs_diff_max_max = _mm_max_pd(v_abs_diff_max, _mm_shuffle_pd(v_abs_diff_max, v_abs_diff_max, 3));\n            if (_mm_movemask_epi8(_mm_castpd_si128(v_abs_diff_max_max)) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 4 != 0) {\n        return false;\n    }\n\n    __m128d zero = _mm_setzero_pd();\n    __m128d tol = _mm_set1_pd(tolerance);\n\n    for (size_t i = 0; i <= length - block_size; i += block_size) {\n        __m128d v0 = _mm_loadu_pd(&vec[i]);\n        __m128d v1 = _mm_loadu_pd(&vec[i + block_size]);\n\n        __m128d diff = _mm_sub_pd(v0, v1);\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, zero), diff);\n        __m128d abs_diff_gt_tol = _mm_cmpgt_pd(abs_diff, tol);\n\n        if (_mm_movemask_epi8(_mm_castpd_si128(abs_diff_gt_tol)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 4 != 0) {\n        return false;\n    }\n\n    __m128d zero = _mm_setzero_pd();\n    __m128d tol = _mm_set1_pd(tolerance);\n\n    for (size_t i = 0; i <= length - block_size; i += block_size) {\n        __m128d v0 = _mm_loadu_pd(&vec[i]);\n        __m128d v1 = _mm_loadu_pd(&vec[i + block_size]);\n\n        __m128d diff = _mm_sub_pd(v0, v1);\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, zero), diff);\n        __m128d abs_diff_gt_tol = _mm_cmpgt_pd(abs_diff, tol);\n\n        if (_mm_movemask_epi8(_mm_castpd_si128(abs_diff_gt_tol)) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 4 != 0) {\n        return false;\n    }\n\n    __m128d v_tolerance = _mm_set1_pd(tolerance);\n    __m128d v_first_block[4];\n    for (size_t i = 0; i < 4; ++i) {\n        v_first_block[i] = _mm_loadu_pd(&vec[i * block_size]);\n    }\n\n    for (size_t i = block_size; i < length - block_size + 1; i += block_size) {\n        __m128d v_current_block[4];\n        for (size_t j = 0; j < 4; ++j) {\n            v_current_block[j] = _mm_loadu_pd(&vec[(i + j) * block_size]);\n        }\n\n        for (size_t k = 0; k < 4; ++k) {\n            __m128d v_diff = _mm_sub_pd(v_current_block[k], v_first_block[k]);\n            __m128d v_abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), v_diff);\n            __m128d v_mask = _mm_cmpgt_pd(v_abs_diff, v_tolerance);\n            int mask_int = _mm_movemask_epi8(_mm_castpd_si128(v_mask));\n            if (mask_int != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size % 4 != 0) {\n        return false;\n    }\n\n    __m128d tolerance_vec = _mm_set1_pd(tolerance);\n\n    for (size_t i = 0; i <= length - block_size; i += block_size) {\n        __m128d first_block = _mm_loadu_pd(&vec[i]);\n        for (size_t j = i + block_size; j < i + 2 * block_size; j += 8) {\n            __m128d current_block = _mm_loadu_pd(&vec[j]);\n            __m128d diff = _mm_sub_pd(current_block, first_block);\n            __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, _mm_setzero_pd()), diff);\n            __m128d abs_diff_gt_tolerance = _mm_cmpgt_pd(abs_diff, tolerance_vec);\n            if (_mm_movemask_epi8(_mm_castpd_si128(abs_diff_gt_tolerance)) != 0) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = zero;\n        for (size_t j = 0; j + 3 < cols; j += 4) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec));\n        }\n        for (; j < cols; ++j) {\n            sum = _mm_add_sd(sum, _mm_mul_sd(_mm_load_sd(&matrix[i * cols + j]), _mm_load_sd(&matrix[i * cols + j])));\n        }\n\n        __m128d norm = _mm_sqrt_pd(sum);\n        __m128d threshold = _mm_loadu_pd(&thresholds[i]);\n\n        __m128d cmp = _mm_cmpgt_pd(norm, threshold);\n        __m128d res = _mm_andnot_pd(cmp, one);\n\n        _mm_storeu_si128((__m128i*)&result[i], _mm_castpd_si128(res));\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = zero;\n        for (size_t j = 0; j + 3 < cols; j += 4) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec));\n        }\n        for (; j < cols; ++j) {\n            sum = _mm_add_sd(sum, _mm_mul_sd(_mm_load_sd(&matrix[i * cols + j]), _mm_load_sd(&matrix[i * cols + j])));\n        }\n\n        __m128d norm = _mm_sqrt_pd(sum);\n        __m128d threshold = _mm_loadu_pd(&thresholds[i]);\n\n        __m128d cmp = _mm_cmpgt_pd(norm, threshold);\n        __m128d res = _mm_andnot_pd(cmp, one);\n\n        _mm_storeu_si128((__m128i*)&result[i], _mm_castpd_si128(res));\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec));\n        }\n\n        // Handle remaining elements if cols is odd\n        if (cols % 2 != 0) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + cols - 2]);\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec));\n        }\n\n        // Horizontal add to get the final sum\n        __m128d temp = _mm_hadd_pd(sum, sum);\n        double norm_squared = _mm_cvtsd_f64(_mm_hadd_pd(temp, temp));\n\n        // Compare with threshold\n        double threshold = thresholds[i];\n        result[i] = norm_squared > threshold * threshold;\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```cpp\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec));\n        }\n        __m128d norm = _mm_sqrt_pd(sum);\n        __m128d threshold = _mm_loadu_pd(&thresholds[i]);\n        __m128d cmp = _mm_cmpgt_pd(norm, threshold);\n        int mask = _mm_movemask_epi8(_mm_castpd_si128(cmp));\n        result[i] = (mask & 0xF) != 0 || (mask >> 4) != 0;\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(&matrix[i * cols + j]);\n            sum = _mm_add_pd(sum, _mm_mul_pd(vec, vec));\n        }\n        double temp[2];\n        _mm_storeu_pd(temp, sum);\n        double norm = std::sqrt(temp[0] + temp[1]);\n\n        if (norm > thresholds[i]) {\n            result[i] = true;\n        } else {\n            result[i] = false;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&B[i]));\n        __m128i vc = _mm_or_si128(va, vb);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&C[i]), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i vecA = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128i vecB = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&B[i]));\n        __m128i result = _mm_or_si128(vecA, vecB);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&C[i]), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i vecA = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128i vecB = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&B[i]));\n        __m128i result = _mm_or_si128(vecA, vecB);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&C[i]), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i vecA = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128i vecB = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&B[i]));\n        __m128i result = _mm_or_si128(vecA, vecB);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&C[i]), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i vecA = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128i vecB = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&B[i]));\n        __m128i result = _mm_or_si128(vecA, vecB);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&C[i]), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i *)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i *)(B + i));\n\n        __m128i a_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(a), _mm_cvtepi8_epi16(a));\n        __m128i b_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(b), _mm_cvtepi8_epi16(b));\n        __m128i ab_product = _mm_mullo_epi16(a, b);\n\n        sum_squares = _mm_add_epi32(sum_squares, a_squared);\n        sum_squares = _mm_add_epi32(sum_squares, b_squared);\n        sum_products = _mm_add_epi32(sum_products, ab_product);\n    }\n\n    int32_t s_s[4], s_p[4];\n    _mm_storeu_si128((__m128i *)s_s, sum_squares);\n    _mm_storeu_si128((__m128i *)s_p, sum_products);\n\n    int64_t result = 0;\n    for (int j = 0; j < 4; ++j) {\n        result += static_cast<int64_t>(s_s[j]) + static_cast<int64_t>(s_p[j]);\n    }\n\n    for (size_t i = length - 16; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        result += static_cast<int64_t>(a * a) + static_cast<int64_t>(b * b) - static_cast<int64_t>(a * b);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i *)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i *)(B + i));\n\n        __m128i a_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(a), _mm_cvtepi8_epi16(a));\n        __m128i b_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(b), _mm_cvtepi8_epi16(b));\n        __m128i ab_product = _mm_mullo_epi16(a, b);\n\n        sum_squares = _mm_add_epi32(sum_squares, _mm_add_epi32(a_squared, b_squared));\n        sum_products = _mm_add_epi32(sum_products, ab_product);\n    }\n\n    int32_t s_s[4], s_p[4];\n    _mm_storeu_si128((__m128i *)s_s, sum_squares);\n    _mm_storeu_si128((__m128i *)s_p, sum_products);\n\n    int64_t result = 0;\n    for (int j = 0; j < 4; ++j) {\n        result += static_cast<int64_t>(s_s[j]) + static_cast<int64_t>(s_p[j]);\n    }\n\n    for (size_t i = length - 16; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        result += static_cast<int64_t>(a * a) + static_cast<int64_t>(b * b) - static_cast<int64_t>(a * b);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i *)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i *)(B + i));\n\n        __m128i a_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(a), _mm_cvtepi8_epi16(a));\n        __m128i b_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(b), _mm_cvtepi8_epi16(b));\n        __m128i ab_product = _mm_mullo_epi16(a, b);\n\n        sum_squares = _mm_add_epi32(sum_squares, _mm_add_epi32(a_squared, b_squared));\n        sum_products = _mm_add_epi32(sum_products, ab_product);\n    }\n\n    int32_t s_s[4], s_p[4];\n    _mm_storeu_si128((__m128i *)s_s, sum_squares);\n    _mm_storeu_si128((__m128i *)s_p, sum_products);\n\n    int64_t result = 0;\n    for (int j = 0; j < 4; ++j) {\n        result += static_cast<int64_t>(s_s[j]) + static_cast<int64_t>(s_p[j]);\n    }\n\n    for (size_t i = length - 16; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        result += static_cast<int64_t>(a * a) + static_cast<int64_t>(b * b) - static_cast<int64_t>(a * b);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i vec_A = _mm_loadu_si128((__m128i *)(A + i));\n        __m128i vec_B = _mm_loadu_si128((__m128i *)(B + i));\n\n        __m128i sq_A = _mm_madd_epi16(_mm_mullo_epi16(vec_A, vec_A), _mm_set1_epi16(1));\n        __m128i sq_B = _mm_madd_epi16(_mm_mullo_epi16(vec_B, vec_B), _mm_set1_epi16(1));\n        __m128i prod_AB = _mm_madd_epi16(_mm_mullo_epi16(vec_A, vec_B), _mm_set1_epi16(1));\n\n        sum_squares = _mm_add_epi32(sum_squares, sq_A);\n        sum_squares = _mm_add_epi32(sum_squares, sq_B);\n        sum_products = _mm_add_epi32(sum_products, prod_AB);\n    }\n\n    int32_t result_squares[4];\n    int32_t result_products[4];\n\n    _mm_storeu_si128((__m128i *)result_squares, sum_squares);\n    _mm_storeu_si128((__m128i *)result_products, sum_products);\n\n    int64_t total_sum_squares = result_squares[0] + result_squares[1] + result_squares[2] + result_squares[3];\n    int64_t total_sum_products = result_products[0] + result_products[1] + result_products[2] + result_products[3];\n\n    for (size_t i = length - 16; i < length; ++i) {\n        total_sum_squares += static_cast<int64_t>(A[i]) * A[i] + static_cast<int64_t>(B[i]) * B[i];\n        total_sum_products += static_cast<int64_t>(A[i]) * B[i];\n    }\n\n    return total_sum_squares - 2 * total_sum_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```cpp\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i *)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i *)&B[i]);\n\n        __m128i a_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(a), _mm_cvtepi8_epi16(a));\n        __m128i b_squared = _mm_mullo_epi16(_mm_cvtepi8_epi16(b), _mm_cvtepi8_epi16(b));\n        __m128i ab_product = _mm_mullo_epi16(a, b);\n\n        sum_squares = _mm_add_epi32(sum_squares, a_squared);\n        sum_squares = _mm_add_epi32(sum_squares, b_squared);\n        sum_products = _mm_add_epi32(sum_products, ab_product);\n    }\n\n    int32_t s_squares[4];\n    int32_t s_products[4];\n\n    _mm_storeu_si128((__m128i *)s_squares, sum_squares);\n    _mm_storeu_si128((__m128i *)s_products, sum_products);\n\n    int64_t result = 0;\n    for (int j = 0; j < 4; ++j) {\n        result += static_cast<int64_t>(s_squares[j]) + static_cast<int64_t>(s_products[j]);\n    }\n\n    for (size_t i = length - 16; i < length; ++i) {\n        int32_t a = static_cast<int32_t>(A[i]);\n        int32_t b = static_cast<int32_t>(B[i]);\n        result += static_cast<int64_t>(a * a) + static_cast<int64_t>(b * b) - static_cast<int64_t>(a * b);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i va = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vb = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vn = _mm_andnot_si128(vb, va);\n        _mm_storeu_si128((__m128i*)&C[i], vn);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128i vec_B = _mm_loadu_si128(reinterpret_cast<const __m128i*>(B + i));\n        __m128i vec_C = _mm_andnot_si128(vec_B, vec_A);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(C + i), vec_C);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(B + i));\n        __m128i vc = _mm_andnot_si128(vb, va);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(C + i), vc);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&B[i]));\n        __m128i vc = _mm_andnot_si128(vb, va);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&C[i]), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i va = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vb = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vc = _mm_andnot_si128(vb, va);\n        _mm_storeu_si128((__m128i*)&C[i], vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i a_positive = _mm_cmpgt_epi8(a_vec, zero);\n        __m128i b_positive = _mm_cmpgt_epi8(b_vec, zero);\n        __m128i a_negative = _mm_cmplt_epi8(a_vec, zero);\n        __m128i b_negative = _mm_cmplt_epi8(b_vec, zero);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(a_positive, b_positive), _mm_and_si128(a_negative, b_negative));\n        result = _mm_or_si128(result, _mm_andnot_si128(_mm_or_si128(a_positive, b_positive), _mm_or_si128(a_negative, b_negative)));\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i a_pos = _mm_cmpgt_epi8(a_vec, zero);\n        __m128i b_pos = _mm_cmpgt_epi8(b_vec, zero);\n        __m128i a_neg = _mm_cmplt_epi8(a_vec, zero);\n        __m128i b_neg = _mm_cmplt_epi8(b_vec, zero);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(a_pos, b_pos), _mm_and_si128(a_neg, b_neg));\n        result = _mm_or_si128(result, _mm_xor_si128(result, _mm_andnot_si128(result, one)));\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i sign_a = _mm_srai_epi8(a, 7);\n        __m128i sign_b = _mm_srai_epi8(b, 7);\n\n        __m128i pos_mask = _mm_and_si128(sign_a, sign_b);\n        __m128i neg_mask = _mm_andnot_si128(pos_mask, _mm_or_si128(sign_a, sign_b));\n\n        __m128i result = _mm_or_si128(_mm_and_si128(pos_mask, one), _mm_and_si128(neg_mask, minus_one));\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i a_positive = _mm_cmpgt_epi8(a, zero);\n        __m128i b_positive = _mm_cmpgt_epi8(b, zero);\n        __m128i a_negative = _mm_cmplt_epi8(a, zero);\n        __m128i b_negative = _mm_cmplt_epi8(b, zero);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(a_positive, b_positive), _mm_and_si128(a_negative, b_negative));\n        result = _mm_or_si128(result, _mm_andnot_si128(_mm_or_si128(a_positive, b_positive), _mm_or_si128(a_negative, b_negative)));\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i sign_a = _mm_srai_epi8(a, 7);\n        __m128i sign_b = _mm_srai_epi8(b, 7);\n\n        __m128i pos_mask = _mm_and_si128(sign_a, sign_b);\n        __m128i neg_mask = _mm_andnot_si128(pos_mask, _mm_or_si128(sign_a, sign_b));\n\n        __m128i result = _mm_or_si128(_mm_and_si128(pos_mask, one), _mm_and_si128(neg_mask, minus_one));\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i mid_point = _mm_set1_epi16(length / 2);\n    __m128i mask = _mm_cmpgt_epi16(_mm_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7), mid_point);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        __m128i select_mask = _mm_and_si128(mask, _mm_setr_epi16(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7));\n        __m128i final_result = _mm_or_si128(select_mask, _mm_andnot_si128(select_mask, _mm_or_si128(xor_result, and_result)));\n\n        _mm_storeu_si128((__m128i*)&C[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i midpoint = _mm_set1_epi16(length / 2);\n    __m128i mask = _mm_cmpgt_epi16(_mm_setr_epi16(0, 1, 2, 3), midpoint);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        __m128i select_mask = _mm_and_si128(mask, _mm_cmplt_epi16(_mm_setr_epi16(i, i + 1, i + 2, i + 3), midpoint));\n        __m128i select_mask2 = _mm_andnot_si128(select_mask, _mm_cmplt_epi16(_mm_setr_epi16(i + 4, i + 5, i + 6, i + 7), midpoint));\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(select_mask, xor_result),\n            _mm_and_si128(select_mask2, and_result)\n        );\n\n        if (length - i <= 4) {\n            uint16_t temp[8];\n            _mm_storeu_si128((__m128i*)temp, result);\n            for (int j = 0; j < 8 && i + j < length; ++j) {\n                C[i + j] = temp[j];\n            }\n        } else {\n            _mm_storeu_si128((__m128i*)&C[i], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i midpoint_mask = _mm_set1_epi16(length / 2);\n    __m128i mask = _mm_cmpgt_epi16(_mm_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7), midpoint_mask);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        __m128i final_result = _mm_blendv_epi8(xor_result, and_result, mask);\n\n        _mm_storeu_si128((__m128i*)&C[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i midpoint_mask = _mm_set1_epi16(length / 2);\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i mask = _mm_cmpgt_epi16(midpoint_mask, _mm_set1_epi16(i));\n\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        __m128i result = _mm_or_si128(_mm_andnot_si128(mask, xor_result), _mm_and_si128(mask, and_result));\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i mid_mask = _mm_setzero_si128();\n    if (length >= 8) {\n        mid_mask = _mm_cmpeq_epi16(_mm_set1_epi16(length / 2), _mm_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7));\n    }\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i mask = _mm_cmpgt_epi16(_mm_setr_epi16(i + 0, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7), mid_mask);\n\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        __m128i result = _mm_or_si128(mask, xor_result);\n        result = _mm_andnot_si128(mask, and_result);\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m128 va, vb, vc, vd;\n    __m128 vcond1, vcond2, vout;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            va = _mm_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm_castsi128_ps(_mm_set1_epi32(*(const int*)(cond1 + i * dim2 * dim3 + j * dim3)));\n            vcond2 = _mm_castsi128_ps(_mm_set1_epi32(*(const int*)(cond2 + i * dim2 * dim3 + j * dim3)));\n\n            vout = _mm_or_ps(\n                _mm_and_ps(vcond1, _mm_and_ps(vcond2, va)),\n                _mm_and_ps(vcond1, _mm_andnot_ps(vcond2, vb))\n            );\n            vout = _mm_or_ps(\n                vout,\n                _mm_and_ps(_mm_andnot_ps(vcond1, vcond2), vc)\n            );\n            vout = _mm_or_ps(\n                vout,\n                _mm_andnot_ps(vcond1, _mm_andnot_ps(vcond2, vd))\n            );\n\n            _mm_storeu_ps(out + i * dim2 * dim3 + j * dim3, vout);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m128 va, vb, vc, vd;\n    __m128 vcond1, vcond2, vout;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            for (size_t k = 0; k < dim3; k += 4) {\n                va = _mm_loadu_ps(&A[i * dim2 * dim3 + j * dim3 + k]);\n                vb = _mm_loadu_ps(&B[i * dim2 * dim3 + j * dim3 + k]);\n                vc = _mm_loadu_ps(&C[i * dim2 * dim3 + j * dim3 + k]);\n                vd = _mm_loadu_ps(&D[i * dim2 * dim3 + j * dim3 + k]);\n\n                vcond1 = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond1[i * dim2 * dim3 + j * dim3 + k]));\n                vcond2 = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond2[i * dim2 * dim3 + j * dim3 + k]));\n\n                vout = _mm_or_ps(\n                    _mm_and_ps(vcond1, _mm_and_ps(vcond2, va)),\n                    _mm_and_ps(vcond1, _mm_andnot_ps(vcond2, vb))\n                );\n                vout = _mm_or_ps(\n                    vout,\n                    _mm_and_ps(_mm_andnot_ps(vcond1, vcond2), vc)\n                );\n                vout = _mm_or_ps(\n                    vout,\n                    _mm_andnot_ps(vcond1, _mm_andnot_ps(vcond2, vd))\n                );\n\n                _mm_storeu_ps(&out[i * dim2 * dim3 + j * dim3 + k], vout);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m128 va, vb, vc, vd;\n    __m128 vcond1, vcond2, vout;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            va = _mm_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)(cond1 + i * dim2 * dim3 + j * dim3)));\n            vcond2 = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)(cond2 + i * dim2 * dim3 + j * dim3)));\n\n            vout = _mm_or_ps(\n                _mm_and_ps(vcond1, _mm_and_ps(vcond2, va)),\n                _mm_and_ps(vcond1, _mm_andnot_ps(vcond2, vb))\n            );\n            vout = _mm_or_ps(\n                vout,\n                _mm_and_ps(_mm_andnot_ps(vcond1, vcond2), vc)\n            );\n            vout = _mm_or_ps(\n                vout,\n                _mm_andnot_ps(vcond1, _mm_andnot_ps(vcond2, vd))\n            );\n\n            _mm_storeu_ps(out + i * dim2 * dim3 + j * dim3, vout);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m128 va, vb, vc, vd;\n    __m128 vcond1, vcond2, vout;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            for (size_t k = 0; k < dim3; k += 4) {\n                va = _mm_loadu_ps(&A[i * dim2 * dim3 + j * dim3 + k]);\n                vb = _mm_loadu_ps(&B[i * dim2 * dim3 + j * dim3 + k]);\n                vc = _mm_loadu_ps(&C[i * dim2 * dim3 + j * dim3 + k]);\n                vd = _mm_loadu_ps(&D[i * dim2 * dim3 + j * dim3 + k]);\n\n                vcond1 = _mm_castsi128_ps(_mm_set_epi32(cond1[i * dim2 * dim3 + j * dim3 + k + 3], cond1[i * dim2 * dim3 + j * dim3 + k + 2], cond1[i * dim2 * dim3 + j * dim3 + k + 1], cond1[i * dim2 * dim3 + j * dim3 + k]));\n                vcond2 = _mm_castsi128_ps(_mm_set_epi32(cond2[i * dim2 * dim3 + j * dim3 + k + 3], cond2[i * dim2 * dim3 + j * dim3 + k + 2], cond2[i * dim2 * dim3 + j * dim3 + k + 1], cond2[i * dim2 * dim3 + j * dim3 + k]));\n\n                vout = _mm_or_ps(\n                    _mm_and_ps(vcond1, _mm_and_ps(va, vcond2)),\n                    _mm_andnot_ps(vcond1, _mm_and_ps(vb, vcond2))\n                );\n                vout = _mm_or_ps(\n                    vout,\n                    _mm_andnot_ps(vcond2, _mm_and_ps(vc, _mm_andnot_ps(vcond1, vcond2)))\n                );\n                vout = _mm_or_ps(\n                    vout,\n                    _mm_andnot_ps(vcond1, _mm_andnot_ps(vcond2, vd))\n                );\n\n                _mm_storeu_ps(&out[i * dim2 * dim3 + j * dim3 + k], vout);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m128 va, vb, vc, vd;\n    __m128 vcond1, vcond2, vout;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += 4) {\n            va = _mm_loadu_ps(A + i * dim2 * dim3 + j * dim3);\n            vb = _mm_loadu_ps(B + i * dim2 * dim3 + j * dim3);\n            vc = _mm_loadu_ps(C + i * dim2 * dim3 + j * dim3);\n            vd = _mm_loadu_ps(D + i * dim2 * dim3 + j * dim3);\n\n            vcond1 = _mm_castsi128_ps(_mm_set1_epi32(*(const int*)(cond1 + i * dim2 * dim3 + j * dim3)));\n            vcond2 = _mm_castsi128_ps(_mm_set1_epi32(*(const int*)(cond2 + i * dim2 * dim3 + j * dim3)));\n\n            vout = _mm_or_ps(\n                _mm_and_ps(vcond1, _mm_and_ps(vcond2, va)),\n                _mm_and_ps(vcond1, _mm_andnot_ps(vcond2, vb))\n            );\n            vout = _mm_or_ps(\n                vout,\n                _mm_and_ps(_mm_andnot_ps(vcond1, vcond2), vc)\n            );\n            vout = _mm_or_ps(\n                vout,\n                _mm_andnot_ps(vcond1, _mm_andnot_ps(vcond2, vd))\n            );\n\n            _mm_storeu_ps(out + i * dim2 * dim3 + j * dim3, vout);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 16; // Number of elements processed per iteration\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(0x55555555); // Mask to extract odd bits\n    __m128i shift_mask = _mm_set1_epi32(0xAAAAAAAA); // Mask to shift even bits\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t k = 0; k < dim3; k += simd_width) {\n                __m128i vec = _mm_loadu_si128((__m128i*)&A[i * dim2 * dim3 + j * dim3 + k]);\n                __m128i odd_bits = _mm_and_si128(vec, mask);\n                __m128i even_bits = _mm_and_si128(_mm_srli_epi32(vec, 1), shift_mask);\n                __m128i sum = _mm_add_epi32(odd_bits, even_bits);\n                sum = _mm_add_epi32(sum, _mm_srli_epi32(sum, 4));\n                sum = _mm_add_epi32(sum, _mm_srli_epi32(sum, 8));\n                sum = _mm_add_epi32(sum, _mm_srli_epi32(sum, 16));\n                int result[4];\n                _mm_storeu_si128((__m128i*)result, sum);\n                for (int l = 0; l < 4; ++l) {\n                    out[(i * dim2 * dim3 + j * dim3 + k + l)] = static_cast<uint8_t>(result[l] & 0xFF);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 16; // Number of elements processed per iteration\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m128i sum = zero;\n            for (size_t k = 0; k < dim3; ++k) {\n                __m128i data = _mm_loadu_si128((__m128i*)&A[i * dim2 * dim3 + j * dim3 + k]);\n                __m128i mask = _mm_cmpeq_epi32(data, zero);\n                __m128i count = _mm_andnot_si128(mask, data);\n                count = _mm_add_epi32(count, _mm_srli_epi32(count, 1));\n                count = _mm_add_epi32(count, _mm_srli_epi32(count, 2));\n                count = _mm_add_epi32(count, _mm_srli_epi32(count, 4));\n                count = _mm_add_epi32(count, _mm_srli_epi32(count, 8));\n                count = _mm_add_epi32(count, _mm_srli_epi32(count, 16));\n                sum = _mm_add_epi32(sum, count);\n            }\n            _mm_storeu_si128((__m128i*)&out[i * dim2 / simd_width + j / simd_width], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m128i sum = zero;\n            for (size_t k = 0; k < dim3; ++k) {\n                __m128i data = _mm_loadu_si128((__m128i*)&A[i * dim2 * dim3 + j * dim3 + k]);\n                __m128i mask = _mm_and_si128(data, ones);\n                __m128i count = _mm_popcnt_epi32(mask);\n                sum = _mm_add_epi32(sum, count);\n            }\n            __m128i temp = _mm_hadd_epi32(sum, sum);\n            temp = _mm_hadd_epi32(temp, temp);\n            int result = _mm_cvtsi128_si32(temp);\n            out[i * dim2 / simd_width + j / simd_width] = static_cast<uint8_t>(result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 4;\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m128i vec = _mm_loadu_si128((__m128i*)&A[i * dim2 * dim3 + j]);\n            __m128i count = _mm_setzero_si128();\n            for (int k = 0; k < 32; k += 8) {\n                __m128i mask = _mm_cmpeq_epi8(_mm_and_si128(vec, _mm_set1_epi32(0xFF)), _mm_set1_epi32(0xFF));\n                count = _mm_add_epi8(count, _mm_popcnt_epi8(mask));\n                vec = _mm_srli_epi32(vec, 8);\n            }\n            _mm_storeu_si128((__m128i*)&out[i * dim2 + j], count);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += simd_width) {\n            __m128i sum = zero;\n            for (size_t k = 0; k < dim3; ++k) {\n                __m128i data = _mm_loadu_si128((__m128i*)&A[i * dim2 * dim3 + j * dim3 + k]);\n                __m128i count = _mm_popcnt_epi32(data);\n                sum = _mm_add_epi32(sum, count);\n            }\n            __m128i temp = _mm_hadd_epi32(sum, sum);\n            temp = _mm_hadd_epi32(temp, temp);\n            int result = _mm_cvtsi128_si32(temp);\n            out[i * dim2 / simd_width + j / simd_width] = static_cast<uint8_t>(result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const __m12i mask = _mm_set1_epi32(3);\n    const __m12i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m12i vec_A = _mm_loadu_si128((__m12i*)&A[i]);\n        __m12i mod_result = _mm_and_si128(vec_A, mask);\n        __m12i cmp_result = _mm_cmpeq_epi32(mod_result, zero);\n        __m12i abs_result = _mm_sub_epi32(zero, vec_A);\n        abs_result = _mm_max_epi32(abs_result, vec_A);\n\n        _mm_storeu_si128((__m12i*)&B[i], _mm_blendv_epi8(vec_A, abs_result, cmp_result));\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const __m12i three = _mm_set1_epi32(3);\n    const __m12i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m12i vec_A = _mm_loadu_si128((__m12i*)&A[i * cols + j]);\n            __m12i div_by_3 = _mm_and_si128(_mm_srai_epi32(vec_A, 31), _mm_cmpeq_epi32(_mm_mod_epi32(vec_A, three), zero));\n            __m12i abs_vec_A = _mm_abs_epi32(vec_A);\n            __m12i result = _mm_or_si128(_mm_andnot_si128(div_by_3, vec_A), _mm_and_si128(div_by_3, abs_vec_A));\n            _mm_storeu_si128((__m12i*)&B[i * cols + j], result);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = std::abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const __m12i three = _mm_set1_epi32(3);\n    const __m12i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j <= cols - 4; j += 4) {\n            __m12i vec_A = _mm_loadu_si128((__m12i*)&A[i * cols + j]);\n            __m12i mod_result = _mm_and_si128(_mm_srai_epi32(vec_A, 31), three);\n            __m12i cmp_result = _mm_cmpeq_epi32(mod_result, zero);\n            __m12i abs_vec_A = _mm_abs_epi32(vec_A);\n            __m12i result = _mm_blendv_epi8(abs_vec_A, vec_A, cmp_result);\n            _mm_storeu_si128((__m12i*)&B[i * cols + j], result);\n        }\n\n        // Handle remaining elements\n        for (size_t j = cols - cols % 4; j < cols; ++j) {\n            if (A[i * cols + j] % 3 == 0) {\n                B[i * cols + j] = std::abs(A[i * cols + j]);\n            } else {\n                B[i * cols + j] = A[i * cols + j];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```cpp\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const __m12i zero = _mm_setzero_si128();\n    const __m12i three = _mm_set1_epi32(3);\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m12i vec_A = _mm_loadu_si128((__m12i*)&A[i]);\n        __m12i mod_result = _mm_and_si128(_mm_srai_epi32(vec_A, 31), _mm_sub_epi32(vec_A, _mm_and_si128(vec_A, three)));\n        __m12i abs_result = _mm_or_si128(mod_result, _mm_andnot_si128(mod_result, vec_A));\n        _mm_storeu_si128((__m12i*)&B[i], abs_result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const __m12i three = _mm_set1_epi32(3);\n    const __m12i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m12i a = _mm_loadu_si128((__m12i*)&A[i]);\n        __m12i remainder = _mm_and_si128(_mm_srai_epi32(a, 31), three);\n        __m12i condition = _mm_cmpeq_epi32(remainder, zero);\n\n        __m12i abs_a = _mm_abs_epi32(a);\n        __m12i result = _mm_blendv_epi8(abs_a, a, condition);\n\n        _mm_storeu_si128((__m12i*)&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m128i rotate_mask = _mm_set1_epi32((1 << rotate_bits) - 1);\n    const __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i cond_mask = _mm_and_si128(src_vec, _mm_set1_epi32(0xF));\n        __m128i all_ones = _mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(0xF));\n\n        __m128i rotated = _mm_rotl_epi32(src_vec, rotate_bits);\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(-1));\n\n        __m128i result = _mm_or_si128(_mm_and_si128(all_ones, rotated), _mm_andnot_si128(all_ones, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m128i rotate_mask = _mm_set1_epi32((1 << rotate_bits) - 1);\n    const __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i cond_mask = _mm_and_si128(src_vec, _mm_set1_epi32(0xF));\n        __m128i all_ones = _mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(0xF));\n\n        __m128i rotated = _mm_rotl_epi32(src_vec, rotate_bits);\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(-1));\n\n        __m128i result = _mm_or_si128(_mm_and_si128(all_ones, rotated), _mm_andnot_si128(all_ones, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m128i rotate_mask = _mm_set1_epi32((1 << rotate_bits) - 1);\n    const __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i cond_mask = _mm_and_si128(src_vec, _mm_set1_epi32(0xF));\n        __m128i all_ones = _mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(0xF));\n\n        __m128i rotated = _mm_srl_epi32(_mm_and_si128(src_vec, rotate_mask), rotate_bits);\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(-1));\n\n        __m128i result = _mm_or_si128(rotated, _mm_andnot_si128(all_ones, negated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m128i rotate_mask = _mm_set1_epi32(0xF); // Mask to check if low 4 bits are all 1\n    const __m128i rotate_shift = _mm_cvtsi32_si128(rotate_bits); // Convert rotate_bits to an SSE register\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i cond_mask = _mm_and_si128(src_vec, rotate_mask); // Check if low 4 bits are all 1\n        __m128i rotated = _mm_rotl_epi32(src_vec, rotate_shift); // Rotate elements by rotate_bits\n        __m128i negated = _mm_xor_si128(src_vec, _mm_set1_epi32(-1)); // Bitwise negation\n\n        __m128i result = _mm_or_si128(_mm_cmpeq_epi32(cond_mask, rotate_mask), negated); // Conditional selection\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m128i rotate_mask = _mm_set1_epi32((1 << rotate_bits) - 1);\n    const __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i cond_mask = _mm_and_si128(src_vec, _mm_set1_epi32(0xF));\n        __m128i eq_mask = _mm_cmpeq_epi32(cond_mask, _mm_set1_epi32(0xF));\n\n        __m128i rotated = _mm_rotl_epi32(src_vec, rotate_bits);\n        __m128i negated = _mm_xor_si128(src_vec, ones);\n\n        __m128i result = _mm_or_si128(_mm_andnot_si128(eq_mask, negated), _mm_and_si128(eq_mask, rotated));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 v_epsilon = _mm_set1_ps(epsilon);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128 v_neg_one = _mm_set1_ps(-1.0f);\n\n    for (size_t i = 0; i <= length - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n\n        __m128 diff = _mm_sub_ps(va, vb);\n        __m128 abs_diff = _mm_andnot_ps(_mm_cmplt_ps(diff, v_zero), diff);\n        __m128 mask_gt = _mm_cmpgt_ps(abs_diff, v_epsilon);\n        __m128 mask_lt = _mm_cmplt_ps(abs_diff, v_neg_epsilon);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(mask_gt), _mm_set1_epi8(1)),\n            _mm_and_si128(_mm_castps_si128(mask_lt), _mm_set1_epi8(-1))\n        );\n\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - length % 4; i < length; ++i) {\n        if (fabs(A[i] - B[i]) > fabs(epsilon)) {\n            out[i] = 1;\n        } else if (fabs(A[i] - B[i]) < -fabs(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 v_epsilon = _mm_set1_ps(epsilon);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128 v_neg_one = _mm_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n\n        __m128 diff = _mm_sub_ps(va, vb);\n        __m128 abs_diff = _mm_andnot_ps(_mm_cmpge_ps(diff, v_zero), diff);\n        abs_diff = _mm_or_ps(abs_diff, _mm_and_ps(_mm_cmpge_ps(v_zero, diff), diff));\n\n        __m128i cmp_greater = _mm_cmpgt_ps(abs_diff, v_epsilon);\n        __m128i cmp_less = _mm_cmplt_ps(abs_diff, v_epsilon);\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(cmp_greater, _mm_set1_epi8(1)),\n            _mm_and_si128(cmp_less, _mm_set1_epi8(-1))\n        );\n\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (fabs(A[i] - B[i]) > fabs(epsilon)) {\n            out[i] = 1;\n        } else if (fabs(A[i] - B[i]) < fabs(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 v_epsilon = _mm_set1_ps(epsilon);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128i v_one = _mm_set1_epi8(1);\n    __m128i v_neg_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n\n        __m128 diff = _mm_sub_ps(va, vb);\n        __m128 abs_diff = _mm_andnot_ps(_mm_cmplt_ps(diff, v_zero), diff);\n        __m128 abs_diff_gt_epsilon = _mm_cmpgt_ps(abs_diff, v_epsilon);\n        __m128 abs_diff_lt_neg_epsilon = _mm_cmplt_ps(abs_diff, _mm_mul_ps(v_epsilon, v_neg_one));\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(_mm_castps_si128(abs_diff_gt_epsilon), v_one),\n            _mm_and_si128(_mm_castps_si128(abs_diff_lt_neg_epsilon), v_neg_one)\n        );\n\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 v_epsilon = _mm_set1_ps(epsilon);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128 v_neg_one = _mm_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n\n        __m128 diff = _mm_sub_ps(va, vb);\n        __m128 abs_diff = _mm_andnot_ps(_mm_cmpge_ps(diff, v_zero), diff);\n        abs_diff = _mm_or_ps(abs_diff, _mm_and_ps(_mm_cmpge_ps(v_zero, diff), diff));\n\n        __m128i cmp_greater = _mm_cmpgt_ps(abs_diff, v_epsilon);\n        __m128i cmp_less = _mm_cmplt_ps(abs_diff, v_epsilon);\n        __m128i result = _mm_or_si128(_mm_and_si128(cmp_greater, _mm_set1_epi8(1)), _mm_and_si128(cmp_less, _mm_set1_epi8(-1)));\n\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 v_epsilon = _mm_set1_ps(epsilon);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128 v_neg_one = _mm_set1_ps(-1.0f);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n\n        __m128 diff = _mm_sub_ps(va, vb);\n        __m128 abs_diff = _mm_andnot_ps(_mm_cmpge_ps(diff, v_zero), diff);\n        abs_diff = _mm_or_ps(abs_diff, _mm_and_ps(_mm_cmpge_ps(v_zero, diff), diff));\n\n        __m128i cmp_greater = _mm_cmpgt_ps(abs_diff, v_epsilon);\n        __m128i cmp_less = _mm_cmplt_ps(abs_diff, v_epsilon);\n        __m128i result = _mm_or_si128(_mm_and_si128(cmp_greater, _mm_set1_epi8(1)), _mm_and_si128(cmp_less, _mm_set1_epi8(-1)));\n\n        _mm_storeu_si128((__m128i*)&out[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 vINT32_MAX = _mm_set1_ps(INT32_MAX);\n    const __m128 vINT32_MIN = _mm_set1_ps(INT32_MIN);\n\n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128i vi = _mm_cvtps_epi32(va);\n        __m128 vvi = _mm_cvtepi32_ps(vi);\n        __m128 mask = _mm_cmpgt_ps(va, vINT32_MAX);\n        vi = _mm_or_si128(_mm_and_si128(mask, _mm_castps_si128(vINT32_MAX)), vi);\n        mask = _mm_cmplt_ps(va, vINT32_MIN);\n        vi = _mm_or_si128(_mm_and_si128(mask, _mm_castps_si128(vINT32_MIN)), vi);\n        _mm_storeu_si128((__m128i*)&B[i], vi);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::max(std::min(A[i], static_cast<float>(INT32_MAX)), static_cast<float>(INT32_MIN)));\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 vINT32_MAX = _mm_set1_ps(INT32_MAX);\n    const __m128 vINT32_MIN = _mm_set1_ps(INT32_MIN);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128i vi = _mm_cvtps_epi32(va);\n        __m128 vb = _mm_cvtepi32_ps(vi);\n        __m128 vc = _mm_max_ps(vb, vINT32_MIN);\n        __m128 vd = _mm_min_ps(vc, vINT32_MAX);\n        vi = _mm_cvttps_epi32(vd);\n        _mm_storeu_si128((__m128i*)&B[i], vi);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::max(std::min(A[i], static_cast<float>(INT32_MAX)), static_cast<float>(INT32_MIN)));\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 vINT32_MAX = _mm_set1_ps(INT32_MAX);\n    const __m128 vINT32_MIN = _mm_set1_ps(INT32_MIN);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 vA = _mm_loadu_ps(&A[i]);\n        __m128 vClamped = _mm_min_ps(_mm_max_ps(vA, vINT32_MIN), vINT32_MAX);\n        __m128i vResult = _mm_cvtps_epi32(vClamped);\n        _mm_storeu_si128((__m128i*)&B[i], vResult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::clamp(A[i], INT32_MIN, INT32_MAX));\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 vINT32_MAX = _mm_set1_ps(INT32_MAX);\n    const __m128 vINT32_MIN = _mm_set1_ps(INT32_MIN);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 vA = _mm_loadu_ps(&A[i]);\n        __m128 vClamped = _mm_min_ps(_mm_max_ps(vA, vINT32_MIN), vINT32_MAX);\n        __m128i vResult = _mm_cvtps_epi32(vClamped);\n        _mm_storeu_si128((__m128i*)&B[i], vResult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::clamp(A[i], INT32_MIN, INT32_MAX));\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 vINT32_MAX = _mm_set1_ps(INT32_MAX);\n    const __m128 vINT32_MIN = _mm_set1_ps(INT32_MIN);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 vA = _mm_loadu_ps(&A[i]);\n        __m128 vClamped = _mm_min_ps(_mm_max_ps(vA, vINT32_MIN), vINT32_MAX);\n        __m128i vResult = _mm_cvtps_epi32(vClamped);\n        _mm_storeu_si128((__m128i*)&B[i], vResult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<int32_t>(std::max(std::min(A[i], static_cast<float>(INT32_MAX)), static_cast<float>(INT32_MIN)));\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m128 scale_vec = _mm_set1_ps(scale / 255.0f); // Convert scale to float and set it as a vector\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i])); // Load 4 integers from A\n        __m128i sign_mask = _mm_set1_epi32(0x80000000); // Mask for extracting signs\n        __m128i abs_a_vec = _mm_andnot_si128(sign_mask, a_vec); // Extract absolute values\n        __m128 abs_a_float = _mm_cvtepi32_ps(abs_a_vec); // Convert absolute values to floats\n        __m128 scaled_abs_a_float = _mm_mul_ps(abs_a_float, scale_vec); // Scale the floats\n        __m128 result = _mm_or_ps(scaled_abs_a_float, _mm_and_ps(_mm_cmpgt_epi32(a_vec, _mm_setzero_si128()), sign_mask)); // Restore signs\n        _mm_storeu_ps(&B[i], result); // Store the results back to B\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * (scale / 255.0f);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m128 scale_vec = _mm_set1_ps(scale / 255.0f); // Convert scale to float and normalize\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128i a_vec_scaled = _mm_mullo_epi32(a_vec, _mm_set1_epi32(256)); // Scale up by 256\n        __m128 b_vec = _mm_cvtepi32_ps(a_vec_scaled); // Convert to float\n        b_vec = _mm_mul_ps(b_vec, scale_vec); // Apply scale factor\n        _mm_storeu_ps(&B[i], b_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * (scale / 255.0f);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m128i scale_vec = _mm_set1_epi32(scale);\n    const __m128 scale_factor = _mm_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 scaled_a = _mm_mul_ps(_mm_cvtepi32_ps(a_vec), scale_factor);\n        _mm_storeu_ps(&B[i], scaled_a);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m128i scale_vec = _mm_set1_epi32(scale);\n    const __m128 scale_factor = _mm_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128 scaled_a = _mm_mul_ps(_mm_cvtepi32_ps(a_vec), scale_factor);\n        _mm_storeu_ps(&B[i], scaled_a);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```cpp\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m128i scale_vec = _mm_set1_epi32(scale);\n    const __m128 scale_factor = _mm_cvtepi32_ps(scale_vec);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128 scaled_a = _mm_mul_ps(_mm_cvtepi32_ps(a_vec), scale_factor);\n        _mm_storeu_ps(&B[i], scaled_a);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n        __m128 vresult = _mm_div_ps(va, vb);\n        __m128i viresult = _mm_cvtps_epi32(vresult);\n        _mm_storeu_si128((__m128i*)&C[i], viresult);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n        __m128 vresult = _mm_div_ps(va, vb);\n        __m128i viresult = _mm_cvtps_epi32(vresult);\n        _mm_storeu_si128((__m128i*)&C[i], viresult);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n        __m128 vresult = _mm_div_ps(va, vb);\n        vresult = _mm_add_ps(vresult, _mm_set1_ps(0.5f));\n        vresult = _mm_cvttps_epi32(vresult);\n        _mm_storeu_si128((__m128i*)&C[i], vresult);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n        __m128 vresult = _mm_div_ps(va, vb);\n        vresult = _mm_add_ps(vresult, _mm_set1_ps(0.5f));\n        vresult = _mm_cvttps_epi32(vresult);\n        _mm_storeu_si128((__m128i*)&C[i], vresult);\n    }\n    for (size_t i = size - size % 4; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vb = _mm_loadu_ps(&B[i]);\n        __m128 vresult = _mm_div_ps(va, vb);\n        __m128i viresult = _mm_cvtps_epi32(vresult);\n        _mm_storeu_si128((__m128i*)&C[i], viresult);\n    }\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128 maxFloat = _mm_set1_ps(FLT_MAX);\n    const __m128 minFloat = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128d dValues = _mm_loadu_pd(&A[i]);\n        __m128d dMax = _mm_max_sd(dValues, _mm_set1_pd(FLT_MAX));\n        __m128d dMin = _mm_min_sd(dMax, _mm_set1_pd(-FLT_MAX));\n\n        __m128 sValues = _mm_cvtpd_ps(dMin);\n        _mm_storeu_ps(&B[i], sValues);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(value);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128 maxFloat = _mm_set1_ps(FLT_MAX);\n    const __m128 minFloat = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128d dValues = _mm_loadu_pd(&A[i]);\n        __m128d dMax = _mm_max_sd(dValues, _mm_setzero_pd());\n        __m128d dMin = _mm_min_sd(dValues, _mm_setzero_pd());\n\n        __m128 sValues = _mm_cvtpd_ps(_mm_or_pd(dMax, dMin));\n        __m128 saturatedValues = _mm_min_ps(sValues, maxFloat);\n        saturatedValues = _mm_max_ps(saturatedValues, minFloat);\n\n        _mm_storeu_ps(&B[i], saturatedValues);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(value);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128 maxFloat = _mm_set1_ps(FLT_MAX);\n    const __m128 minFloat = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128d dValues = _mm_loadu_pd(&A[i]);\n        __m128d dMax = _mm_max_sd(dValues, _mm_setzero_pd());\n        __m128d dMin = _mm_min_sd(dValues, _mm_setzero_pd());\n\n        __m128d dAbsMax = _mm_max_sd(dMax, _mm_set1_pd(FLT_MAX));\n        __m128d dAbsMin = _mm_min_sd(dMin, _mm_set1_pd(-FLT_MAX));\n\n        __m128 sValues = _mm_cvtpd_ps(dAbsMax);\n        _mm_storeu_ps(&B[i], sValues);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(value);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128 maxFloat = _mm_set1_ps(FLT_MAX);\n    const __m128 minFloat = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128d dValues = _mm_loadu_pd(&A[i]);\n        __m128d dMax = _mm_max_sd(dValues, _mm_setzero_pd());\n        __m128d dMin = _mm_min_sd(dValues, _mm_setzero_pd());\n\n        __m128 sValues = _mm_cvtpd_ps(dValues);\n        __m128 sMax = _mm_cmpgt_ps(sValues, maxFloat);\n        __m128 sMin = _mm_cmplt_ps(sValues, minFloat);\n\n        sValues = _mm_or_ps(_mm_and_ps(sValues, sMax), _mm_andnot_ps(sMax, maxFloat));\n        sValues = _mm_or_ps(_mm_and_ps(sValues, sMin), _mm_andnot_ps(sMin, minFloat));\n\n        _mm_storeu_ps(&B[i], sValues);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (A[i] > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (A[i] < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128 maxFloat = _mm_set1_ps(FLT_MAX);\n    const __m128 minFloat = _mm_set1_ps(-FLT_MAX);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128d dValues = _mm_loadu_pd(&A[i]);\n        __m128d dMax = _mm_max_sd(dValues, _mm_set1_pd(FLT_MAX));\n        __m128d dMin = _mm_min_sd(dMax, _mm_set1_pd(-FLT_MAX));\n\n        __m128 sValues = _mm_cvtpd_ps(dMin);\n        _mm_storeu_ps(&B[i], sValues);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        double value = A[i];\n        if (value > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (value < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(value);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // Number of 16-bit integers that can fit in an SSE register\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vec_B = _mm_cvtepi16_epi32(vec_A);\n        _mm_storeu_si128((__m128i*)&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // Each __m128i can hold 8 int16_t values\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128i vec_B = _mm_cvtepi16_epi32(vec_A);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(B + i), vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // Number of int16 elements per SSE register\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vec_B = _mm_cvtepi16_epi32(vec_A);\n        _mm_storeu_si128((__m128i*)&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // Number of int16 elements per SSE register\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128i vec_B = _mm_cvtepi16_epi32(vec_A);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(B + i), vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```cpp\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8; // Number of elements processed per iteration\n    size_t i;\n\n    // Process elements in chunks of 8\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128i vec_B = _mm_cvtepi16_epi32(vec_A);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(B + i), vec_B);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min_val = _mm_set1_ps(min_val);\n    __m128 v_max_val = _mm_set1_ps(max_val);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vcontrol = _mm_loadu_si128((__m128i*)&control[i]);\n\n        __m128 vcond = _mm_castsi128_ps(_mm_srai_epi32(vcontrol, 31));\n        __m128 vnorm = _mm_sub_ps(va, v_min_val);\n        vnorm = _mm_div_ps(vnorm, _mm_sub_ps(v_max_val, v_min_val));\n\n        vnorm = _mm_max_ps(vnorm, v_zero);\n        vnorm = _mm_min_ps(vnorm, v_one);\n\n        __m128 vresult = _mm_and_ps(vcond, vnorm);\n        vresult = _mm_or_ps(vresult, _mm_andnot_ps(vcond, va));\n\n        _mm_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min_val = _mm_set1_ps(min_val);\n    __m128 v_max_val = _mm_set1_ps(max_val);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vcontrol = _mm_loadu_si128((__m128i*)&control[i]);\n\n        __m128 vcond = _mm_castsi128_ps(_mm_srai_epi32(_mm_srli_epi32(vcontrol, 31), 31));\n        __m128 vresult = _mm_mul_ps(va, v_cond);\n        vresult = _mm_sub_ps(vresult, v_min_val);\n        vresult = _mm_div_ps(vresult, _mm_sub_ps(v_max_val, v_min_val));\n\n        vresult = _mm_max_ps(vresult, v_zero);\n        vresult = _mm_min_ps(vresult, v_one);\n\n        _mm_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min_val = _mm_set1_ps(min_val);\n    __m128 v_max_val = _mm_set1_ps(max_val);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vcontrol = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&control[i]));\n        __m128 vb = _mm_and_ps(va, _mm_castsi128_ps(_mm_cmpgt_epi32(vcontrol, _mm_setzero_si128())));\n\n        vb = _mm_sub_ps(vb, v_min_val);\n        vb = _mm_div_ps(vb, _mm_sub_ps(v_max_val, v_min_val));\n        vb = _mm_max_ps(vb, v_zero);\n        vb = _mm_min_ps(vb, v_one);\n\n        _mm_storeu_ps(&B[i], vb);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min_val = _mm_set1_ps(min_val);\n    __m128 v_max_val = _mm_set1_ps(max_val);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vcontrol = _mm_loadu_si128((__m128i*)&control[i]);\n\n        __m128 vcond = _mm_cmpgt_epi32(vcontrol, _mm_setzero_si128());\n        __m128 vnorm = _mm_sub_ps(va, v_min_val);\n        vnorm = _mm_div_ps(vnorm, _mm_sub_ps(v_max_val, v_min_val));\n        vnorm = _mm_max_ps(vnorm, v_zero);\n        vnorm = _mm_min_ps(vnorm, v_one);\n\n        __m128 vresult = _mm_blendv_ps(vnorm, va, vcond);\n\n        _mm_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 v_min_val = _mm_set1_ps(min_val);\n    __m128 v_max_val = _mm_set1_ps(max_val);\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128 vcontrol = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&control[i]));\n        __m128 vb = _mm_and_ps(va, _mm_castsi128_ps(_mm_cmpgt_epi32(vcontrol, _mm_setzero_si128())));\n\n        vb = _mm_sub_ps(vb, v_min_val);\n        vb = _mm_div_ps(vb, _mm_sub_ps(v_max_val, v_min_val));\n        vb = _mm_max_ps(vb, v_zero);\n        vb = _mm_min_ps(vb, v_one);\n\n        _mm_storeu_ps(&B[i], vb);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (control[i] > 0) {\n            B[i] = std::max(std::min((A[i] - min_val) / (max_val - min_val), 1.0f), 0.0f);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```cpp\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    size_t i;\n\n    __m128i vec_A;\n    __m128 vec_B;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        // Load 4 uint32_t values from A into a vector register\n        vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Extract the high 16 bits of each uint32_t value and convert them to float\n        vec_B = _mm_cvtepu32_ps(_mm_srli_epi32(vec_A, 16));\n\n        // Take the square root of the float values\n        vec_B = _mm_sqrt_ps(vec_B);\n\n        // Store the results in B\n        _mm_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements that do not fit into a full SIMD width\n    for (; i < size; ++i) {\n        B[i] = sqrtf(static_cast<float>((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```cpp\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    size_t i;\n\n    __m128i vec_A;\n    __m128 vec_B;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        // Load 4 uint32_t values from A into a vector register\n        vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Extract the high 16 bits of each uint32_t value and convert them to float\n        vec_B = _mm_cvtepu32_ps(_mm_and_si128(vec_A, _mm_set1_epi32(0xFFFF0000)));\n\n        // Take the square root of the float values\n        vec_B = _mm_sqrt_ps(vec_B);\n\n        // Store the results in B\n        _mm_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements that do not fit into a full SIMD width\n    for (; i < size; ++i) {\n        B[i] = sqrtf((A[i] >> 16) & 0xFFFF);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```cpp\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    size_t i;\n\n    __m128i vec_A;\n    __m128 vec_B;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        // Load 4 uint32_t values from A into a vector register\n        vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Extract the high 16 bits of each uint32_t value and convert them to float\n        vec_B = _mm_cvtepu32_ps(_mm_srli_epi32(vec_A, 16));\n\n        // Take the square root of the float values\n        vec_B = _mm_sqrt_ps(vec_B);\n\n        // Store the results in B\n        _mm_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements that do not fit into a full SIMD width\n    for (; i < size; ++i) {\n        B[i] = sqrtf(static_cast<float>((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    size_t i;\n\n    // Process elements in chunks of 4\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 vec_B = _mm_cvtepi32_ps(vec_A); // Convert int32 to float32\n        vec_B = _mm_sqrt_ps(vec_B); // Take square root\n        _mm_storeu_ps(&B[i], vec_B); // Store results\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = sqrt(static_cast<float>(A[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    size_t i;\n\n    // Process elements in chunks of 4\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 vec_B;\n\n        // Extract high 16 bits and convert to float\n        __m128i high_bits = _mm_and_si128(vec_A, _mm_set1_epi32(0xFFFF0000));\n        __m128 vec_high_bits = _mm_cvtepi32_ps(high_bits);\n\n        // Take square root\n        vec_B = _mm_sqrt_ps(vec_high_bits);\n\n        // Store the result\n        _mm_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = sqrtf(static_cast<float>((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&A[i]));\n        __m128 vec_B = _mm_castsi128_ps(vec_A);\n        _mm_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128 vec_B = _mm_castsi128_ps(vec_A);\n        _mm_storeu_ps(B + i, vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i;\n\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 vec_B = _mm_castsi128_ps(vec_A);\n        _mm_storeu_ps(&B[i], vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i;\n\n    // Process elements in chunks of 4\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128 vec_B = _mm_castsi128_ps(vec_A);\n        _mm_storeu_ps(B + i, vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i;\n\n    // Process elements in chunks of 4\n    for (i = 0; i <= size - simd_width; i += simd_width) {\n        __m128i vec_A = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128 vec_B = _mm_castsi128_ps(vec_A);\n        _mm_storeu_ps(B + i, vec_B);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i zero = _mm_setzero_si128();\n    __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i prev_a_vec = _mm_loadu_si128((__m128i*)&A[i - 4]);\n\n        __m128i mask = _mm_cmpeq_epi32(_mm_and_si128(_mm_castps_si128(_mm_loadu_ps((const float*)&cond[i])), one), one);\n\n        __m128i diff_vec = _mm_sub_epi32(a_vec, prev_a_vec);\n        __m128 diff_float_vec = _mm_cvtepi32_ps(diff_vec);\n\n        __m128 result_vec = _mm_and_ps(diff_float_vec, _mm_castsi128_ps(mask));\n        if (i == 0) {\n            result_vec = _mm_or_ps(result_vec, _mm_setzero_ps());\n        }\n\n        _mm_storeu_ps(&diff[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i zero = _mm_setzero_si128();\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 zero_float = _mm_setzero_ps();\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i prev_a_vec = _mm_loadu_si128((__m128i*)&A[i - 4]);\n\n        __m128i mask = _mm_and_si128(_mm_castps_si128(_mm_loadu_ps((const float*)&cond[i])), _mm_castsi128_ps(zero));\n        __m128i diff_vec = _mm_sub_epi32(a_vec, prev_a_vec);\n\n        __m128 diff_float_vec = _mm_cvtepi32_ps(diff_vec);\n        diff_float_vec = _mm_mul_ps(diff_float_vec, one);\n        diff_float_vec = _mm_andnot_ps(mask, diff_float_vec);\n        diff_float_vec = _mm_or_ps(diff_float_vec, _mm_and_ps(mask, zero_float));\n\n        _mm_storeu_ps(&diff[i], diff_float_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i zero_i = _mm_setzero_si128();\n    __m128 zero_f = _mm_setzero_ps();\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i a_prev_vec = _mm_loadu_si128((__m128i*)&A[i - 4]);\n\n        __m128i cond_vec = _mm_loadu_si128((__m128i*)&cond[i]);\n        __m128i cond_prev_vec = _mm_loadu_si128((__m128i*)&cond[i - 4]);\n\n        __m128i diff_vec = _mm_sub_epi32(a_vec, a_prev_vec);\n\n        __m128 diff_float_vec = _mm_cvtepi32_ps(diff_vec);\n\n        __m128 mask = _mm_and_ps(_mm_castsi128_ps(cond_vec), _mm_castsi128_ps(zero_i));\n        diff_float_vec = _mm_or_ps(diff_float_vec, mask);\n\n        _mm_storeu_ps(&diff[i], diff_float_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size - size % 4; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i zero_i = _mm_setzero_si128();\n    __m128 zero_f = _mm_setzero_ps();\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i prev_a_vec = i == 0 ? zero_i : _mm_loadu_si128((__m128i*)&A[i - 4]);\n\n        __m128i diff_vec = _mm_sub_epi32(a_vec, prev_a_vec);\n\n        __m128 cond_vec = _mm_castsi128_ps(_mm_and_si128(_mm_castps_si128(_mm_cmpeq_epi32(diff_vec, zero_i)), _mm_castps_si128(_mm_cvtepi32_ps(cond[i]))));\n\n        __m128 result_vec = _mm_or_ps(_mm_mul_ps(_mm_cvtepi32_ps(diff_vec), cond_vec), _mm_andnot_ps(cond_vec, zero_f));\n\n        _mm_storeu_ps(&diff[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i zero = _mm_setzero_si128();\n    __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i prev_a_vec = _mm_loadu_si128((__m128i*)&A[i - 1]);\n        __m128i cond_vec = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        __m128i diff_vec = _mm_sub_epi32(a_vec, prev_a_vec);\n        __m128 diff_float_vec = _mm_cvtepi32_ps(diff_vec);\n\n        __m128 mask = _mm_castsi128_ps(_mm_and_si128(cond_vec, _mm_cmpeq_epi32(cond_vec, cond_vec)));\n        diff_float_vec = _mm_and_ps(diff_float_vec, mask);\n        diff_float_vec = _mm_or_ps(diff_float_vec, _mm_andnot_ps(mask, one));\n\n        _mm_storeu_ps(&diff[i], diff_float_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t); // 4 elements per SIMD register\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i src_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i result_vec = _mm_or_si128(src_vec, ones);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t); // 4 elements per SSE register\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i result_vec = _mm_or_si128(src_vec, _mm_slli_epi32(ones, 32));\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t); // 4 elements per SSE register\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i src_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i result = _mm_or_si128(src_vec, ones);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 4; // Number of elements processed per iteration\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF); // Vector of 4 ones\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]); // Load 4 uint32_t values from src\n        __m128i result_vec = _mm_or_si128(src_vec, ones); // OR with vector of ones\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec); // Store the result back to dst\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```cpp\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 16 / sizeof(uint32_t); // 4 elements per 128-bit register\n    __m128i ones = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < length; i += simd_width) {\n        __m128i src_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src + i));\n        __m128i result = _mm_or_si128(src_vec, ones);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v0 = _mm_loadu_pd(&vec[_mm_extract_epi32(idx, 0)]);\n        __m128d v1 = _mm_loadu_pd(&vec[_mm_extract_epi32(idx, 2)]);\n        sum = _mm_add_pd(sum, v0);\n        sum = _mm_add_pd(sum, v1);\n    }\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i idx = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&index[i]));\n        __m128d v1 = _mm_loadu_pd(&vec[idx.m128i_i32[0]]);\n        __m128d v2 = _mm_loadu_pd(&vec[idx.m128i_i32[1]]);\n        sum = _mm_add_pd(sum, v1);\n        sum = _mm_add_pd(sum, v2);\n    }\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i idx = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&index[i]));\n        __m128d v0 = _mm_loadu_pd(&vec[_mm_extract_epi32(idx, 0)]);\n        __m128d v1 = _mm_loadu_pd(&vec[_mm_extract_epi32(idx, 1)]);\n        sum = _mm_add_pd(sum, v0);\n        sum = _mm_add_pd(sum, v1);\n    }\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```cpp\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i idx = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&index[i]));\n        __m128d v0 = _mm_loadu_pd(&vec[idx.m128i_i32[0]]);\n        __m128d v1 = _mm_loadu_pd(&vec[idx.m128i_i32[1]]);\n        sum = _mm_add_pd(sum, v0);\n        sum = _mm_add_pd(sum, v1);\n    }\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```cpp\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    for (size_t i = 0; i < length; i += 2) {\n        __m128i idx = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&index[i]));\n        __m128d v0 = _mm_loadu_pd(&vec[_mm_extract_epi32(idx, 0)]);\n        __m128d v1 = _mm_loadu_pd(&vec[_mm_extract_epi32(idx, 2)]);\n        sum = _mm_add_pd(sum, v0);\n        sum = _mm_add_pd(sum, v1);\n    }\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ki += 2) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    __m128d input_val = _mm_loadu_pd(&input[(i + ki) * input_size + j + kj]);\n                    __m128d kernel_val = _mm_loadu_pd(&kernel[ki * kernel_size + kj]);\n\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d relu_result = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[i * (input_size - kernel_size + 1) + j], relu_result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    __m128d input_val = _mm_loadu_pd(&input[(i + ki) * input_size + j + kj]);\n                    __m128d kernel_val = _mm_loadu_pd(&kernel[ki * kernel_size + kj]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d relu_result = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[i * (input_size - kernel_size + 1) + j], relu_result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ki += 2) {\n                for (size_t kj = 0; kj < kernel_size; kj += 2) {\n                    __m128d input_val = _mm_loadu_pd(&input[(i + ki) * input_size + j + kj]);\n                    __m128d kernel_val = _mm_loadu_pd(&kernel[ki * kernel_size + kj]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d relu_result = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[i * (input_size - kernel_size + 1) + j], relu_result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            __m128d sum = zero;\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    __m128d val = _mm_loadu_pd(&input[(i + ki) * input_size + j + kj]);\n                    __m128d ker = _mm_loadu_pd(&kernel[ki * kernel_size + kj]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(val, ker));\n                }\n            }\n            __m128d relu = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[i * (input_size - kernel_size + 1) + j], relu);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    __m128d zero = _mm_setzero_pd();\n    __m128d one = _mm_set1_pd(1.0);\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    __m128d input_val = _mm_loadu_pd(&input[(i + ki) * input_size + j + kj]);\n                    __m128d kernel_val = _mm_loadu_pd(&kernel[ki * kernel_size + kj]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n\n            __m128d relu_result = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[i * (input_size - kernel_size + 1) + j], relu_result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int vec_size = 4;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < numbers.size() - vec_size + 1; i += vec_size) {\n        __m128d vec1 = _mm_loadu_pd(&numbers[i]);\n        __m128d vec2 = _mm_loadu_pd(&numbers[i + vec_size]);\n\n        __m128d diff = _mm_sub_pd(vec1, vec2);\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff);\n        __m128d cmp = _mm_cmple_sd(abs_diff, threshold_vec);\n\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return true;\n        }\n    }\n\n    // Check remaining elements\n    for (size_t i = numbers.size() - vec_size + 1; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            if (std::abs(numbers[i] - numbers[j]) <= threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m128d vec1 = _mm_loadu_pd(&numbers[i]);\n        __m128d vec2 = _mm_loadu_pd(&numbers[i + 4]);\n\n        __m128d diff1 = _mm_sub_pd(vec1, vec2);\n        __m128d abs_diff1 = _mm_andnot_pd(_mm_set1_pd(-0.0), diff1);\n        __m128d cmp1 = _mm_cmple_ps(abs_diff1, threshold_vec);\n        if (_mm_movemask_epi8(cmp1)) return true;\n\n        __m128d diff2 = _mm_sub_pd(vec2, vec1);\n        __m128d abs_diff2 = _mm_andnot_pd(_mm_set1_pd(-0.0), diff2);\n        __m128d cmp2 = _mm_cmple_ps(abs_diff2, threshold_vec);\n        if (_mm_movemask_epi8(cmp2)) return true;\n    }\n\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    int n = numbers.size();\n    if (n < 2) return false;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m128d vec1 = _mm_loadu_pd(&numbers[i]);\n        __m128d vec2 = _mm_loadu_pd(&numbers[i + 2]);\n\n        __m128d diff1 = _mm_sub_pd(vec1, vec2);\n        __m128d abs_diff1 = _mm_andnot_pd(_mm_set1_pd(-0.0), diff1);\n        __m128d cmp1 = _mm_cmple_pd(abs_diff1, threshold_vec);\n        if (_mm_movemask_epi8(cmp1) != 0) return true;\n\n        __m128d diff2 = _mm_sub_pd(vec2, vec1);\n        __m128d abs_diff2 = _mm_andnot_pd(_mm_set1_pd(-0.0), diff2);\n        __m128d cmp2 = _mm_cmple_pd(abs_diff2, threshold_vec);\n        if (_mm_movemask_epi8(cmp2) != 0) return true;\n    }\n\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) <= threshold) return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int vec_size = 4;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < numbers.size() - vec_size + 1; i += vec_size) {\n        __m128d vec1 = _mm_loadu_pd(&numbers[i]);\n        __m128d vec2 = _mm_loadu_pd(&numbers[i + vec_size]);\n\n        __m128d diff = _mm_sub_pd(vec1, vec2);\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff);\n        __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n\n        if (_mm_movemask_epi8(_mm_castpd_si128(cmp)) != 0) {\n            return true;\n        }\n    }\n\n    // Check remaining elements\n    for (size_t i = numbers.size() - numbers.size() % vec_size; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    const int vec_size = 4;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < numbers.size() - vec_size + 1; i += vec_size) {\n        __m128d vec1 = _mm_loadu_pd(&numbers[i]);\n        __m128d vec2 = _mm_loadu_pd(&numbers[i + vec_size]);\n\n        __m128d diff = _mm_sub_pd(vec1, vec2);\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff);\n        __m128d cmp = _mm_cmplt_pd(abs_diff, threshold_vec);\n\n        if (_mm_movemask_epi8(_mm_castpd_si128(cmp)) != 0) {\n            return true;\n        }\n    }\n\n    // Check remaining elements\n    for (size_t i = numbers.size() - numbers.size() % vec_size; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i threshold = _mm_set1_epi32(0);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i ops = _mm_loadu_si128((__m128i*)&data[i]);\n        balance = _mm_add_epi32(balance, ops);\n        __m128i below_zero = _mm_cmplt_epi32(balance, threshold);\n        if (_mm_movemask_epi8(below_zero)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 4; i < size; ++i) {\n        if (balance.m256i_i32[0] + data[i] < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i balance = _mm_setzero_si128();\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i ops = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        balance = _mm_add_epi32(balance, ops);\n\n        __m128i below_zero_mask = _mm_cmplt_epi32(balance, zero);\n        if (_mm_movemask_epi8(below_zero_mask)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 4; i < size; ++i) {\n        if (data[i] < 0 && _mm_extract_epi32(balance, 0) + data[i] < 0) {\n            return true;\n        }\n        balance = _mm_add_epi32(balance, _mm_set1_epi32(data[i]));\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i threshold = _mm_set1_epi32(0);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i ops = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        balance = _mm_add_epi32(balance, ops);\n        __m128i below_zero = _mm_cmplt_epi32(balance, threshold);\n        if (_mm_movemask_epi8(below_zero)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 4; i < size; ++i) {\n        if (data[i] < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int* data = operations.data();\n    int size = operations.size();\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i threshold = _mm_set1_epi32(0);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i ops = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        balance = _mm_add_epi32(balance, ops);\n        __m128i below_zero = _mm_cmplt_epi32(balance, threshold);\n        if (_mm_movemask_epi8(below_zero)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 4; i < size; ++i) {\n        if (data[i] < 0 && _mm_extract_epi32(balance, 0) + data[i] < 0) {\n            return true;\n        }\n        balance = _mm_add_epi32(balance, _mm_set1_epi32(data[i]));\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    int size = operations.size();\n    __m128i balance = _mm_setzero_si128();\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i ops = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        balance = _mm_add_epi32(balance, ops);\n        __m128i mask = _mm_cmplt_epi32(balance, _mm_setzero_si128());\n        if (_mm_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = size - size % 4; i < size; ++i) {\n        if (operations[i] < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, vec);\n    }\n\n    double total_sum;\n    _mm_storeu_pd(&total_sum, sum);\n    double mean = total_sum / n;\n\n    __m128d mad_sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(vec, _mm_set1_pd(mean));\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, _mm_setzero_pd()), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double total_mad_sum;\n    _mm_storeu_pd(&total_mad_sum, mad_sum);\n    double mad = total_mad_sum / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d val = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, val);\n    }\n\n    double total_sum;\n    _mm_storeu_pd(&total_sum, sum);\n    double mean = total_sum / n;\n\n    __m128d mad_sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d val = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(val, _mm_set1_pd(mean));\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, _mm_setzero_pd()), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double total_mad_sum;\n    _mm_storeu_pd(&total_mad_sum, mad_sum);\n    double mad = total_mad_sum / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d val = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, val);\n    }\n\n    double total_sum;\n    _mm_storeu_pd(&total_sum, sum);\n\n    double mean = total_sum / n;\n\n    __m128d mad_sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d val = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(val, _mm_set1_pd(mean));\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, _mm_setzero_pd()), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double total_mad_sum;\n    _mm_storeu_pd(&total_mad_sum, mad_sum);\n\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, vec);\n    }\n\n    double total_sum;\n    _mm_storeu_pd(&total_sum, sum);\n\n    double mean = total_sum / n;\n\n    __m128d mad_sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(vec, _mm_set1_pd(mean));\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, _mm_setzero_pd()), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double total_mad_sum;\n    _mm_storeu_pd(&total_mad_sum, mad_sum);\n\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    const int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    __m128d sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, vec);\n    }\n\n    double total_sum;\n    _mm_storeu_pd(&total_sum, sum);\n    double mean = total_sum / n;\n\n    __m128d mad_sum = _mm_setzero_pd();\n    for (int i = 0; i < n; i += 2) {\n        __m128d vec = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(vec, _mm_set1_pd(mean));\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmpgt_pd(diff, _mm_setzero_pd()), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double total_mad_sum;\n    _mm_storeu_pd(&total_mad_sum, mad_sum);\n    double mad = total_mad_sum / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    __m128i v_delimeter = _mm_set1_epi32(delimeter);\n    \n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m128i v_numbers = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        if(i + 3 < numbers.size()){\n            _mm_storeu_si128((__m128i*)&result.back(), v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 4, v_delimeter);\n            _mm_storeu_si128((__m128i*)&result.back() + 8, v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 12, v_delimeter);\n            _mm_storeu_si128((__m128i*)&result.back() + 16, v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 20, v_delimeter);\n            _mm_storeu_si128((__m128i*)&result.back() + 24, v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 28, v_delimeter);\n        } else {\n            _mm_storeu_si128((__m128i*)&result.back(), v_numbers);\n            _mm_maskstore_epi32(&result.back() + 4, _mm_cmplt_epi32(_mm_setr_epi32(3, 2, 1, 0), _mm_setzero_si128()), v_delimeter);\n        }\n        \n        result.resize(result.size() + 32);\n    }\n    \n    result.resize(result.size() - 4); // Remove extra space\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    __m128i delim = _mm_set1_epi32(delimeter);\n    \n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i mask = _mm_cmplt_epi32(_mm_setr_epi32(i+1, i+2, i+3, i+4), _mm_set1_epi32(numbers.size()));\n        \n        __m128i masked_vec = _mm_and_si128(vec, mask);\n        __m128i masked_delim = _mm_and_si128(delim, mask);\n        \n        _mm_storeu_si128((__m128i*)&result, masked_vec);\n        _mm_storeu_si128((__m128i*)&result[4], masked_delim);\n        \n        result.resize(result.size() + 5);\n    }\n    \n    // Remove extra delimiter at the end\n    if(!result.empty()){\n        result.pop_back();\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    __m128i v_delimeter = _mm_set1_epi32(delimeter);\n    \n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m128i v_numbers = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        if(i + 3 < numbers.size()){\n            _mm_storeu_si128((__m128i*)&result.back(), v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 4, v_delimeter);\n            _mm_storeu_si128((__m128i*)&result.back() + 8, v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 12, v_delimeter);\n            _mm_storeu_si128((__m128i*)&result.back() + 16, v_numbers);\n        } else {\n            _mm_storeu_si128((__m128i*)&result.back(), v_numbers);\n            _mm_maskstore_epi32(&result.back() + 4, _mm_cmplt_epi32(_mm_setr_epi32(3, 2, 1, 0), _mm_setzero_si128()), v_delimeter);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    __m128i v_delimeter = _mm_set1_epi32(delimeter);\n    \n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m128i v_numbers = _mm_loadu_si128((__m128i*)&numbers[i]);\n        \n        if(i + 3 < numbers.size()){\n            _mm_storeu_si128((__m128i*)&result.back(), v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 4, v_delimeter);\n            _mm_storeu_si128((__m128i*)&result.back() + 8, v_numbers);\n            _mm_storeu_si128((__m128i*)&result.back() + 12, v_delimeter);\n            _mm_storeu_si128((__m128i*)&result.back() + 16, v_numbers);\n        } else {\n            _mm_storeu_si128((__m128i*)&result.back(), v_numbers);\n            for(size_t j = i + 1; j < numbers.size(); ++j){\n                result.push_back(delimeter);\n                result.push_back(numbers[j]);\n            }\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    __m128i delim = _mm_set1_epi32(delimeter);\n    \n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i mask = _mm_cmplt_epi32(_mm_setr_epi32(i+1, i+2, i+3, i+4), _mm_set1_epi32(numbers.size()));\n        \n        __m128i masked_vec = _mm_and_si128(vec, mask);\n        __m128i masked_delim = _mm_and_si128(delim, mask);\n        \n        result.insert(result.end(), (int*)_mm_addr_si128(masked_vec, 0), (int*)_mm_addr_si128(masked_vec, 15));\n        result.insert(result.end(), (int*)_mm_addr_si128(masked_delim, 0), (int*)_mm_addr_si128(masked_delim, 15));\n    }\n    \n    if(numbers.size() % 4 != 0){\n        size_t remaining = numbers.size() % 4;\n        __m128i vec = _mm_loadu_si128((__m128i*)&numbers[numbers.size() - remaining]);\n        __m128i mask = _mm_cmplt_epi32(_mm_setr_epi32(0, 1, 2, 3), _mm_set1_epi32(remaining));\n        \n        __m128i masked_vec = _mm_and_si128(vec, mask);\n        __m128i masked_delim = _mm_and_si128(delim, mask);\n        \n        result.insert(result.end(), (int*)_mm_addr_si128(masked_vec, 0), (int*)_mm_addr_si128(masked_vec, 15));\n        result.insert(result.end(), (int*)_mm_addr_si128(masked_delim, 0), (int*)_mm_addr_si128(masked_delim, 15));\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (!numbers.empty()) {\n        __m128i vsum = _mm_setzero_si128();\n        __m128i vproduct = _mm_set1_epi32(1);\n\n        size_t i = 0;\n        for (; i + 7 < numbers.size(); i += 8) {\n            __m128i vnums = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n            vsum = _mm_add_epi32(vsum, vnums);\n            vproduct = _mm_mullo_epi32(vproduct, vnums);\n        }\n\n        int32_t s[4];\n        int32_t p[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(s), vsum);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(p), vproduct);\n\n        sum = static_cast<int64_t>(s[0]) + s[1] + s[2] + s[3];\n        product = static_cast<int64_t>(p[0]) * p[1] * p[2] * p[3];\n\n        for (; i < numbers.size(); ++i) {\n            sum += numbers[i];\n            product *= numbers[i];\n        }\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (!numbers.empty()) {\n        __m12i vsum = _mm_setzero_si128();\n        __m12i vproduct = _mm_set1_epi32(1);\n\n        for (size_t i = 0; i < numbers.size(); i += 8) {\n            __m12i vnumbers = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&numbers[i]));\n            vsum = _mm_add_epi32(vsum, vnumbers);\n            vproduct = _mm_mullo_epi32(vproduct, vnumbers);\n        }\n\n        int32_t s[4];\n        int32_t p[4];\n\n        _mm_storeu_si128(reinterpret_cast<__m12i*>(s), vsum);\n        _mm_storeu_si128(reinterpret_cast<__m12i*>(p), vproduct);\n\n        sum = static_cast<int64_t>(s[0]) + s[1] + s[2] + s[3];\n        product = static_cast<int64_t>(p[0]) * p[1] * p[2] * p[3];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int n = numbers.size();\n    if (n == 0) {\n        return {0, 1};\n    }\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n\n    for (int i = 0; i < n - 7; i += 8) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n        sum_vec = _mm_add_epi16(sum_vec, vec);\n        prod_vec = _mm_mullo_epi16(prod_vec, vec);\n    }\n\n    int16_t sum[8];\n    int16_t prod[8];\n\n    _mm_storeu_si128((__m128i*)sum, sum_vec);\n    _mm_storeu_si128((__m128i*)prod, prod_vec);\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sum[i];\n        total_prod *= prod[i];\n    }\n\n    for (int i = n - 7; i < n; ++i) {\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (!numbers.empty()) {\n        __m128i sum_vec = _mm_setzero_si128();\n        __m128i product_vec = _mm_set1_epi32(1);\n\n        for (size_t i = 0; i + 7 < numbers.size(); i += 8) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n            sum_vec = _mm_add_epi32(sum_vec, vec);\n            product_vec = _mm_mullo_epi32(product_vec, vec);\n        }\n\n        int32_t temp[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum_vec);\n        sum = static_cast<int64_t>(temp[0]) + temp[1] + temp[2] + temp[3];\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), product_vec);\n        product = static_cast<int64_t>(temp[0]) * temp[1] * temp[2] * temp[3];\n\n        for (size_t i = numbers.size() - numbers.size() % 8; i < numbers.size(); ++i) {\n            sum += numbers[i];\n            product *= numbers[i];\n        }\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (!numbers.empty()) {\n        __m128i sum_vec = _mm_setzero_si128();\n        __m128i product_vec = _mm_set1_epi32(1);\n\n        for (size_t i = 0; i < numbers.size(); i += 8) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&numbers[i]));\n\n            // Sum\n            sum_vec = _mm_add_epi32(sum_vec, vec);\n\n            // Product\n            product_vec = _mm_mullo_epi32(product_vec, vec);\n        }\n\n        // Horizontal add for sum\n        int32_t temp_sum[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp_sum), sum_vec);\n        sum = static_cast<int64_t>(temp_sum[0]) + temp_sum[1] + temp_sum[2] + temp_sum[3];\n\n        // Horizontal multiply for product\n        int32_t temp_product[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp_product), product_vec);\n        product = static_cast<int64_t>(static_cast<uint64_t>(temp_product[0]) * temp_product[1] *\n                                       temp_product[2] * temp_product[3]);\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    if (numbers.empty()) return {};\n\n    int size = numbers.size();\n    std::vector<int> result(size);\n    __m128i max_vec = _mm_set1_epi32(numbers[0]);\n\n    for (int i = 1; i < size; ++i) {\n        __m128i current_vec = _mm_set1_epi32(numbers[i]);\n        max_vec = _mm_max_epi32(max_vec, current_vec);\n        _mm_storeu_si128((__m128i*)&result[i], max_vec);\n    }\n\n    // Handle remaining elements that do not fit into a 128-bit register\n    for (int i = size - size % 4; i < size; ++i) {\n        result[i] = std::max(result[i-1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    if (numbers.empty()) return {};\n\n    int n = numbers.size();\n    std::vector<int> result(n);\n    __m128i max_vec = _mm_set1_epi32(numbers[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m128i current_vec = _mm_set1_epi32(numbers[i]);\n        max_vec = _mm_max_epi32(max_vec, current_vec);\n        _mm_storeu_si128((__m128i*)&result[i], max_vec);\n    }\n\n    // Handle remaining elements that do not fit into a 128-bit register\n    for (int i = n - n % 4; i < n; ++i) {\n        result[i] = std::max(result[i-1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m128i max_vec = _mm_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (int i = 1; i < n; ++i) {\n        __m128i current_vec = _mm_set1_epi32(numbers[i]);\n        max_vec = _mm_max_epi32(max_vec, current_vec);\n        result[i] = _mm_extract_epi32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m128i max_vec = _mm_set1_epi32(numbers[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m128i current_vec = _mm_set1_epi32(numbers[i]);\n        max_vec = _mm_max_epi32(max_vec, current_vec);\n        result[i] = _mm_extract_epi32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    __m128i max_vec = _mm_set1_epi32(numbers[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m128i current_vec = _mm_set1_epi32(numbers[i]);\n        max_vec = _mm_max_epi32(max_vec, current_vec);\n        result[i] = _mm_extract_epi32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m128i* pa = (__m128i*)a.data();\n    __m128i* pb = (__m128i*)b.data();\n    __m128i* pr = (__m128i*)result.data();\n\n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(pa++);\n        __m128i vb = _mm_loadu_si128(pb++);\n        __m128i vr = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    int len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m128i* pa = (__m128i*)a.c_str();\n    __m128i* pb = (__m128i*)b.c_str();\n    __m128i* pr = (__m128i*)result.c_str();\n\n    int i;\n    for (i = 0; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(pa++);\n        __m128i vb = _mm_loadu_si128(pb++);\n        __m128i vr = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    int len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m128i* pa = (__m128i*)a.c_str();\n    __m128i* pb = (__m128i*)b.c_str();\n    __m128i* pr = (__m128i*)result.c_str();\n\n    int i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(pa++);\n        __m128i vb = _mm_loadu_si128(pb++);\n        __m128i vr = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    int len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m128i* pa = (__m128i*)a.c_str();\n    __m128i* pb = (__m128i*)b.c_str();\n    __m128i* pr = (__m128i*)result.c_str();\n\n    int i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(pa++);\n        __m128i vb = _mm_loadu_si128(pb++);\n        __m128i vr = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    int len = std::max(a.size(), b.size());\n    std::string result(len, '0');\n\n    __m128i* pa = (__m128i*)a.c_str();\n    __m128i* pb = (__m128i*)b.c_str();\n    __m128i* pr = (__m128i*)result.c_str();\n\n    int i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(pa++);\n        __m128i vb = _mm_loadu_si128(pb++);\n        __m128i vr = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(pr++, vr);\n    }\n\n    for (; i < len; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* longest = nullptr;\n    size_t max_length = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_length) {\n            longest = str.c_str();\n            max_length = str.length();\n        }\n    }\n\n    return std::string(longest, max_length);\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* max_str = nullptr;\n    size_t max_len = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_len) {\n            max_str = str.c_str();\n            max_len = str.length();\n        }\n    }\n\n    return max_str ? std::string(max_str, max_len) : \"\";\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* max_str = nullptr;\n    size_t max_len = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_len) {\n            max_str = str.c_str();\n            max_len = str.length();\n        }\n    }\n\n    return std::string(max_str, max_len);\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* longest = nullptr;\n    size_t max_length = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_length) {\n            longest = str.c_str();\n            max_length = str.length();\n        }\n    }\n\n    return std::string(longest, max_length);\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const char* longest = nullptr;\n    size_t max_length = 0;\n\n    for (const auto& str : strings) {\n        if (str.length() > max_length) {\n            longest = str.c_str();\n            max_length = str.length();\n        }\n    }\n\n    return std::string(longest, max_length);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi64(va, vb)) == 0) {\n        __m128i temp = _mm_min_epu64(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i va = _mm_set_epi64x(a, 0);\n    __m128i vb = _mm_set_epi64x(b, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(va, vb)) != 0) {\n        __m128i temp = _mm_and_si128(va, vb);\n        va = _mm_sub_epi64(va, temp);\n        vb = _mm_sub_epi64(vb, temp);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m128 min_diff = _mm_set1_ps(std::numeric_limits<float>::max());\n    __m128 closest_pair = _mm_setzero_ps();\n\n    for (size_t i = 0; i + 7 < numbers.size(); i += 8) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n        __m128 vec2 = _mm_loadu_ps(&numbers[i + 4]);\n\n        __m128 diff1 = _mm_sub_ps(vec1, vec2);\n        __m128 abs_diff1 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff1);\n\n        __m128 diff2 = _mm_sub_ps(vec2, vec1);\n        __m128 abs_diff2 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff2);\n\n        __m128 min_diff1 = _mm_min_ps(abs_diff1, abs_diff2);\n        __m128 mask = _mm_cmplt_ps(min_diff1, min_diff);\n        min_diff = _mm_blendv_ps(min_diff, min_diff1, mask);\n\n        __m128 selected_pair = _mm_select_ps(diff1, diff2, mask);\n        closest_pair = _mm_add_ps(closest_pair, selected_pair);\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, closest_pair);\n\n    float min_diff_value = std::numeric_limits<float>::max();\n    for (int j = 0; j < 4; ++j) {\n        if (result[j] > 0 && result[j] < min_diff_value) {\n            min_diff_value = result[j];\n        }\n    }\n\n    float closest_elements[2];\n    for (size_t i = 0; i + 7 < numbers.size(); i += 8) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 diff = _mm_sub_ps(vec, closest_pair);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff);\n\n        __m128 mask = _mm_cmpeq_ps(abs_diff, _mm_set1_ps(min_diff_value));\n        __m128 selected_elements = _mm_select_ps(vec, closest_pair, mask);\n\n        _mm_storeu_ps(closest_elements, selected_elements);\n        break;\n    }\n\n    std::sort(closest_elements, closest_elements + 2);\n    return {closest_elements[0], closest_elements[1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m128 min_diff = _mm_set1_ps(std::numeric_limits<float>::max());\n    __m128 closest_pair = _mm_setzero_ps();\n\n    for (size_t i = 0; i + 7 < numbers.size(); i += 8) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n        __m128 vec2 = _mm_loadu_ps(&numbers[i + 4]);\n\n        __m128 diff1 = _mm_sub_ps(vec1, vec2);\n        __m128 abs_diff1 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff1);\n\n        __m128 diff2 = _mm_sub_ps(vec2, vec1);\n        __m128 abs_diff2 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff2);\n\n        __m128 min_diff1 = _mm_min_ps(abs_diff1, abs_diff2);\n        __m128 min_diff2 = _mm_min_ps(min_diff1, min_diff);\n\n        __m128 mask = _mm_cmpeq_ps(min_diff2, min_diff1);\n        min_diff = _mm_blendv_ps(min_diff, min_diff2, mask);\n\n        __m128 pair1 = _mm_add_ps(vec1, vec2);\n        __m128 pair2 = _mm_sub_ps(pair1, _mm_set1_ps(2.0f * _mm_cvtss_f32(min_diff)));\n\n        __m128 closest_pair1 = _mm_min_ps(pair1, pair2);\n        __m128 closest_pair2 = _mm_max_ps(pair1, pair2);\n\n        __m128 mask2 = _mm_cmpgt_ps(min_diff2, min_diff);\n        closest_pair = _mm_blendv_ps(closest_pair, closest_pair2, mask2);\n    }\n\n    // Handle remaining elements\n    for (size_t i = numbers.size() - numbers.size() % 8; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < _mm_cvtss_f32(min_diff)) {\n                min_diff = _mm_set1_ps(diff);\n                closest_pair = _mm_set_ps(numbers[j], numbers[i], 0.0f, 0.0f);\n            }\n        }\n    }\n\n    float closest1 = _mm_cvtss_f32(_mm_min_ps(closest_pair, _mm_shuffle_ps(closest_pair, closest_pair, _MM_SHUFFLE(0, 0, 2, 2))));\n    float closest2 = _mm_cvtss_f32(_mm_max_ps(closest_pair, _mm_shuffle_ps(closest_pair, closest_pair, _MM_SHUFFLE(0, 0, 2, 2))));\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements.\");\n    }\n\n    __m128 min_diff = _mm_set1_ps(std::numeric_limits<float>::max());\n    __m128 closest_pair = _mm_setzero_ps();\n\n    for (size_t i = 0; i <= numbers.size() - 8; i += 8) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n        __m128 vec2 = _mm_loadu_ps(&numbers[i + 4]);\n\n        __m128 diff1 = _mm_sub_ps(vec1, vec2);\n        __m128 abs_diff1 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff1);\n\n        __m128 diff2 = _mm_sub_ps(vec2, vec1);\n        __m128 abs_diff2 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff2);\n\n        __m128 min_diff_new = _mm_min_ps(min_diff, abs_diff1);\n        min_diff = _mm_min_ps(min_diff_new, abs_diff2);\n\n        __m128 mask1 = _mm_cmpeq_ps(min_diff_new, abs_diff1);\n        __m128 mask2 = _mm_cmpeq_ps(min_diff_new, abs_diff2);\n\n        closest_pair = _mm_or_ps(_mm_and_ps(mask1, vec1), _mm_and_ps(mask2, vec2));\n    }\n\n    float closest[4];\n    _mm_storeu_ps(closest, closest_pair);\n\n    float min_diff_value = std::numeric_limits<float>::max();\n    for (int j = 0; j < 4; ++j) {\n        for (size_t k = i; k < numbers.size(); ++k) {\n            float diff = std::abs(numbers[k] - closest[j]);\n            if (diff < min_diff_value) {\n                min_diff_value = diff;\n            }\n        }\n    }\n\n    std::sort(closest, closest + 4);\n    return {closest[0], closest[1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements.\");\n    }\n\n    __m128 min_diff = _mm_set1_ps(std::numeric_limits<float>::max());\n    __m128 closest_pair = _mm_setzero_ps();\n\n    for (size_t i = 0; i <= numbers.size() - 8; i += 8) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n        __m128 vec2 = _mm_loadu_ps(&numbers[i + 4]);\n\n        __m128 diff1 = _mm_sub_ps(vec1, vec2);\n        __m128 abs_diff1 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff1);\n\n        __m128 diff2 = _mm_sub_ps(vec2, vec1);\n        __m128 abs_diff2 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff2);\n\n        __m128 min_diff1 = _mm_min_ps(abs_diff1, abs_diff2);\n        __m128 mask = _mm_cmplt_ps(min_diff1, min_diff);\n        min_diff = _mm_blendv_ps(min_diff, min_diff1, mask);\n\n        __m128 selected_pair1 = _mm_add_ps(vec1, vec2);\n        __m128 selected_pair2 = _mm_sub_ps(vec1, vec2);\n        __m128 selected_pair = _mm_select_ps(selected_pair1, selected_pair2, mask);\n\n        closest_pair = _mm_min_ps(closest_pair, selected_pair);\n    }\n\n    // Handle remaining elements\n    for (size_t i = numbers.size() - numbers.size() % 8; i < numbers.size(); ++i) {\n        for (size_t j = i + 1; j < numbers.size(); ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < _mm_cvtss_f32(min_diff)) {\n                min_diff = _mm_set1_ps(diff);\n                closest_pair = _mm_set_ps(std::min(numbers[i], numbers[j]), std::max(numbers[i], numbers[j]));\n            }\n        }\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, closest_pair);\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n#include <algorithm>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must contain at least two elements\");\n    }\n\n    __m128 min_diff = _mm_set1_ps(std::numeric_limits<float>::max());\n    __m128 closest_pair = _mm_setzero_ps();\n\n    for (size_t i = 0; i + 7 < numbers.size(); i += 8) {\n        __m128 vec1 = _mm_loadu_ps(&numbers[i]);\n        __m128 vec2 = _mm_loadu_ps(&numbers[i + 4]);\n\n        __m128 diff1 = _mm_sub_ps(vec1, vec2);\n        __m128 abs_diff1 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff1);\n\n        __m128 diff2 = _mm_sub_ps(vec2, vec1);\n        __m128 abs_diff2 = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff2);\n\n        __m128 min_diff1 = _mm_min_ps(abs_diff1, abs_diff2);\n        __m128 mask1 = _mm_cmpeq_ps(min_diff1, abs_diff1);\n        __m128 min_diff2 = _mm_min_ps(min_diff, min_diff1);\n        __m128 mask2 = _mm_cmpneq_ps(min_diff, min_diff1);\n\n        min_diff = _mm_or_ps(min_diff2, _mm_and_ps(mask1, diff1));\n        min_diff = _mm_or_ps(min_diff, _mm_and_ps(mask2, diff2));\n\n        closest_pair = _mm_or_ps(closest_pair, _mm_and_ps(mask1, vec1));\n        closest_pair = _mm_or_ps(closest_pair, _mm_and_ps(mask2, vec2));\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, closest_pair);\n\n    float min_diff_value = std::numeric_limits<float>::max();\n    for (int i = 0; i < 4; ++i) {\n        for (int j = i + 1; j < 4; ++j) {\n            float diff = std::abs(result[i] - result[j]);\n            if (diff < min_diff_value) {\n                min_diff_value = diff;\n            }\n        }\n    }\n\n    for (size_t i = 0; i + 7 < numbers.size(); i += 8) {\n        for (size_t j = i; j < i + 8; ++j) {\n            for (size_t k = j + 1; k < i + 8; ++k) {\n                float diff = std::abs(numbers[j] - numbers[k]);\n                if (diff == min_diff_value) {\n                    return {std::min(numbers[j], numbers[k]), std::max(numbers[j], numbers[k])};\n                }\n            }\n        }\n    }\n\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    if (n >= 8) {\n        __m128 min_val = _mm_set1_ps(FLT_MAX);\n        __m128 max_val = _mm_set1_ps(-FLT_MAX);\n\n        // Find min and max values in the first 8 elements\n        for (int i = 0; i < n; i += 8) {\n            __m128 vec = _mm_loadu_ps(&numbers[i]);\n            min_val = _mm_min_ps(min_val, vec);\n            max_val = _mm_max_ps(max_val, vec);\n        }\n\n        float min_val_f, max_val_f;\n        _mm_store_ss(&min_val_f, min_val);\n        _mm_store_ss(&max_val_f, max_val);\n\n        // Rescale the remaining elements\n        for (int i = 0; i < n; i++) {\n            result[i] = (numbers[i] - min_val_f) / (max_val_f - min_val_f);\n        }\n    } else {\n        // Fallback to scalar operation for small vectors\n        float min_val = *std::min_element(numbers.begin(), numbers.end());\n        float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n        for (int i = 0; i < n; i++) {\n            result[i] = (numbers[i] - min_val) / (max_val - min_val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    if (n >= 8) {\n        __m128 min_val = _mm_set1_ps(FLT_MAX);\n        __m128 max_val = _mm_set1_ps(-FLT_MAX);\n\n        // Find min and max values in parallel\n        for (int i = 0; i <= n - 8; i += 8) {\n            __m128 vec = _mm_loadu_ps(&numbers[i]);\n            min_val = _mm_min_ps(min_val, vec);\n            max_val = _mm_max_ps(max_val, vec);\n        }\n\n        float min_val_f, max_val_f;\n        _mm_store_ss(&min_val_f, min_val);\n        _mm_store_ss(&max_val_f, max_val);\n\n        // Rescale values in parallel\n        for (int i = 0; i <= n - 8; i += 8) {\n            __m128 vec = _mm_loadu_ps(&numbers[i]);\n            __m128 diff = _mm_sub_ps(vec, _mm_set1_ps(min_val_f));\n            __m128 scale = _mm_div_ps(diff, _mm_set1_ps(max_val_f - min_val_f));\n            _mm_storeu_ps(&result[i], scale);\n        }\n    } else {\n        // Fallback to scalar operation for small vectors\n        float min_val = *std::min_element(numbers.begin(), numbers.end());\n        float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n        for (int i = 0; i < n; ++i) {\n            result[i] = (numbers[i] - min_val) / (max_val - min_val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    __m128 min_val = _mm_set1_ps(FLT_MAX);\n    __m128 max_val = _mm_set1_ps(-FLT_MAX);\n\n    // Find min and max values in parallel\n    for(int i = 0; i <= n - 4; i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_val = _mm_min_ps(min_val, vec);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n\n    float min_val_f, max_val_f;\n    _mm_store_ss(&min_val_f, min_val);\n    _mm_store_ss(&max_val_f, max_val);\n\n    if(n % 4 != 0){\n        for(int i = n - n % 4; i < n; ++i){\n            if(numbers[i] < min_val_f) min_val_f = numbers[i];\n            if(numbers[i] > max_val_f) max_val_f = numbers[i];\n        }\n    }\n\n    float range = max_val_f - min_val_f;\n\n    // Rescale values in parallel\n    for(int i = 0; i <= n - 4; i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 scaled_vec = _mm_sub_ps(vec, _mm_set1_ps(min_val_f));\n        scaled_vec = _mm_div_ps(scaled_vec, _mm_set1_ps(range));\n        _mm_storeu_ps(&result[i], scaled_vec);\n    }\n\n    if(n % 4 != 0){\n        for(int i = n - n % 4; i < n; ++i){\n            result[i] = (numbers[i] - min_val_f) / range;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    if (n >= 8) {\n        __m128 min_val = _mm_set1_ps(FLT_MAX);\n        __m128 max_val = _mm_set1_ps(-FLT_MAX);\n\n        // Find min and max values in the first 8 elements\n        for (int i = 0; i < n; i += 8) {\n            __m128 vec = _mm_loadu_ps(&numbers[i]);\n            min_val = _mm_min_ps(min_val, vec);\n            max_val = _mm_max_ps(max_val, vec);\n        }\n\n        float min_val_f, max_val_f;\n        _mm_store_ss(&min_val_f, min_val);\n        _mm_store_ss(&max_val_f, max_val);\n\n        // Rescale the remaining elements\n        for (int i = 0; i < n; i++) {\n            result[i] = (numbers[i] - min_val_f) / (max_val_f - min_val_f);\n        }\n    } else {\n        // Fallback to scalar operation for small vectors\n        float min_val = *std::min_element(numbers.begin(), numbers.end());\n        float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n        for (int i = 0; i < n; i++) {\n            result[i] = (numbers[i] - min_val) / (max_val - min_val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    if (n >= 8){\n        __m128 min_val = _mm_set1_ps(FLT_MAX);\n        __m128 max_val = _mm_set1_ps(-FLT_MAX);\n\n        // Find min and max values in the first 8 elements\n        for (int i = 0; i < n; i += 8){\n            __m128 vec = _mm_loadu_ps(&numbers[i]);\n            min_val = _mm_min_ps(min_val, vec);\n            max_val = _mm_max_ps(max_val, vec);\n        }\n\n        float min_f, max_f;\n        _mm_store_ss(&min_f, min_val);\n        _mm_store_ss(&max_f, max_val);\n\n        // Rescale the remaining elements\n        for (int i = 0; i < n; i++){\n            result[i] = (numbers[i] - min_f) / (max_f - min_f);\n        }\n    } else {\n        // Fallback to scalar operation for small vectors\n        float min_val = *std::min_element(numbers.begin(), numbers.end());\n        float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n        for (int i = 0; i < n; i++){\n            result[i] = (numbers[i] - min_val) / (max_val - min_val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i v_n = _mm_set_epi64x(n, 0);\n    __m128i v_one = _mm_set_epi64x(1, 0);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (int i = 3; i <= n / 2; i += 2) {\n        __m128i v_i = _mm_set_epi64x(i, 0);\n        __m128i v_remainder = _mm_sub_epi64(v_n, _mm_mul_epu64(v_i, _mm_srli_epi64(_mm_ucomieq_ss(_mm_cvtepi32_ps(_mm_cvtsi64_si32(v_n)), _mm_cvtepi32_ps(_mm_cvtsi64_si32(v_i))), 0)));\n        if (_mm_movemask_epi8(_mm_cmpeq_epi8(v_remainder, v_zero)) != 0) {\n            return i;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i v_n = _mm_set_epi64x(n, 0);\n    __m128i v_one = _mm_set_epi64x(1, 0);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (int i = 32; i > 0; --i) {\n        __m128i v_i = _mm_set_epi32(i, i, i, i);\n        __m128i v_div = _mm_div_epi32(v_n, v_i);\n        __m128i v_mul = _mm_mul_epi32(v_div, v_i);\n        __m128i v_cmp = _mm_cmpeq_epi32(v_mul, v_n);\n        if (_mm_movemask_epi8(v_cmp)) {\n            return i;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i v_n = _mm_set_epi64x(n, 0);\n    __m128i v_one = _mm_set_epi64x(1, 0);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (int i = 32; i >= 1; --i) {\n        __m128i v_i = _mm_set_epi32(i, i, i, i);\n        __m128i v_remainder = _mm_rem_epi64(v_n, v_i);\n        __m128i v_mask = _mm_cmpeq_epi64(v_remainder, v_zero);\n        if (_mm_movemask_epi8(v_mask)) {\n            return i;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i v_n = _mm_set_epi64x(n, 0);\n    __m128i v_one = _mm_set_epi64x(1, 0);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (int i = 3; i <= n / 2; i += 2) {\n        __m128i v_i = _mm_set_epi64x(i, 0);\n        __m128i v_remainder = _mm_sub_epi64(v_n, _mm_mul_epu64(v_n, _mm_cvtepi32_epi64(_mm_srli_epi64(v_i, 32))));\n        if (_mm_testz_si128(v_remainder, v_remainder)) {\n            return i;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m128i v_n = _mm_set_epi64x(n, 0);\n    __m128i v_one = _mm_set_epi64x(1, 0);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (int i = 3; i <= n / 2; i += 2) {\n        __m128i v_i = _mm_set_epi64x(i, 0);\n        __m128i v_remainder = _mm_sub_epi64(v_n, _mm_mul_epu64(v_i, _mm_srli_epi64(_mm_ucomieq_ss(_mm_cvtepi32_ps(_mm_cvtsi64_si128(v_n)), _mm_cvtepi32_ps(_mm_cvtsi64_si128(v_i))), 0)));\n        if (_mm_movemask_epi8(_mm_cmpeq_epi8(v_remainder, v_zero)) != 0) {\n            return i;\n        }\n    }\n\n    return 1;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m12i v_n = _mm_set1_epi32(n);\n    __m12i v_2 = _mm_set1_epi32(2);\n    __m12i v_0 = _mm_setzero_si128();\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi32(v_n, v_0)) != 0) {\n        __m12i v_mod = _mm_and_si128(v_n, v_2);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(v_mod, v_0)) == 0) {\n            factors.push_back(2);\n            v_n = _mm_sub_epi32(v_n, v_2);\n        } else {\n            v_n = _mm_add_epi32(v_n, v_2);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m12i v_n = _mm_set1_epi32(n);\n    __m12i v_2 = _mm_set1_epi32(2);\n    __m12i v_0 = _mm_setzero_si128();\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi32(v_n, v_0))) {\n        __m12i v_remainder = _mm_and_si128(v_n, v_2);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(v_remainder, v_0))) {\n            factors.push_back(2);\n            v_n = _mm_srli_epi32(v_n, 1);\n        } else {\n            break;\n        }\n    }\n\n    int remaining = _mm_cvtsi128_si32(v_n);\n    while (remaining % 2 == 0) {\n        factors.push_back(2);\n        remaining /= 2;\n    }\n\n    for (int i = 3; i * i <= remaining; i += 2) {\n        while (remaining % i == 0) {\n            factors.push_back(i);\n            remaining /= i;\n        }\n    }\n\n    if (remaining > 2) {\n        factors.push_back(remaining);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m12i v_n = _mm_set1_epi32(n);\n    __m12i v_2 = _mm_set1_epi32(2);\n    __m12i v_1 = _mm_setzero_si128();\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(v_n, v_1)) == 0) {\n        __m12i v_mod = _mm_and_si128(v_n, v_2);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(v_mod, v_1)) != 0) {\n            factors.push_back(2);\n            v_n = _mm_sub_epi32(v_n, v_2);\n        } else {\n            v_2 = _mm_add_epi32(v_2, v_2);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m12i v_n = _mm_set1_epi32(n);\n    __m12i v_two = _mm_set1_epi32(2);\n    __m12i v_one = _mm_set1_epi32(1);\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(v_n, v_one)) == 0) {\n        if (_mm_movemask_epi8(_mm_and_si128(v_n, v_two)) != 0) {\n            factors.push_back(2);\n            v_n = _mm_sub_epi32(v_n, v_two);\n        } else {\n            __m12i v_three = _mm_set1_epi32(3);\n            __m12i v_five = _mm_set1_epi32(5);\n            __m12i v_seven = _mm_set1_epi32(7);\n            __m12i v_eleven = _mm_set1_epi32(11);\n            __m12i v_thirteen = _mm_set1_epi32(13);\n            __m12i v_fifteen = _mm_set1_epi32(15);\n            __m12i v_seventeen = _mm_set1_epi32(17);\n            __m12i v_nineteen = _mm_set1_epi32(19);\n            __m12i v_twentyone = _mm_set1_epi32(21);\n            __m12i v_twentythree = _mm_set1_epi32(23);\n\n            __m12i v_factors = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(v_three, v_five), v_seven), v_eleven), v_thirteen), v_fifteen), v_seventeen), v_nineteen), v_twentyone), v_twentythree);\n            __m12i v_mask = _mm_cmple_epi32(v_factors, v_n);\n            int mask = _mm_movemask_epi8(v_mask);\n            int factor = 3 + (mask & 1 ? 0 : 2) + (mask & 2 ? 0 : 4) + (mask & 4 ? 0 : 6) + (mask & 8 ? 0 : 8) + (mask & 16 ? 0 : 10) + (mask & 32 ? 0 : 12) + (mask & 64 ? 0 : 14);\n            factors.push_back(factor);\n            v_n = _mm_sub_epi32(v_n, _mm_set1_epi32(factor));\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    __m12i v_n = _mm_set1_epi32(n);\n    __m12i v_2 = _mm_set1_epi32(2);\n    __m12i v_0 = _mm_setzero_si128();\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(v_n, v_0)) == 0) {\n        if (_mm_movemask_epi8(_mm_and_si128(v_n, v_2)) != 0) {\n            factors.push_back(2);\n            v_n = _mm_sub_epi32(v_n, v_2);\n        } else {\n            v_2 = _mm_add_epi32(v_2, v_2);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    int n = l.size();\n    int i = 0;\n\n    // Process elements in chunks of 4\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        __m128 pos_vec = _mm_and_ps(vec, mask);\n        float temp[4];\n        _mm_storeu_ps(temp, pos_vec);\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    int n = l.size();\n    int i = 0;\n\n    // Process elements in chunks of 4\n    for (; i <= n - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(vec, zero);\n        __m128 pos_vec = _mm_and_ps(vec, mask);\n\n        float temp[4];\n        _mm_storeu_ps(temp, pos_vec);\n\n        if (temp[0] > 0) result.push_back(temp[0]);\n        if (temp[1] > 0) result.push_back(temp[1]);\n        if (temp[2] > 0) result.push_back(temp[2]);\n        if (temp[3] > 0) result.push_back(temp[3]);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    int n = l.size();\n    int i = 0;\n\n    // Process elements in chunks of 4\n    for (; i + 4 <= n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(vec, zero);\n        __m128 pos_vec = _mm_and_ps(vec, mask);\n\n        float temp[4];\n        _mm_storeu_ps(temp, pos_vec);\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    const int n = l.size();\n    const int step = 4;\n    for (int i = 0; i <= n - step; i += step) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        __m128 pos_vec = _mm_and_ps(vec, mask);\n        float temp[step];\n        _mm_storeu_ps(temp, pos_vec);\n        for (int j = 0; j < step; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n    // Handle remaining elements\n    for (int i = n - n % step; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    int n = l.size();\n    int i = 0;\n\n    // Process elements in chunks of 4\n    for (; i + 3 < n; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 zero = _mm_setzero_ps();\n        __m128 mask = _mm_cmpgt_ps(vec, zero);\n        __m128 pos_vec = _mm_and_ps(vec, mask);\n\n        float temp[4];\n        _mm_storeu_ps(temp, pos_vec);\n        for (int j = 0; j < 4; ++j) {\n            if (temp[j] > 0) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m12i v = _mm_set_epi64x(n - 1, n - 2);\n    __m12i divisors = _mm_set_epi64x(6, 5);\n\n    while (_mm_movemask_epi8(_mm_cmplt_epi64(v, divisors)) != 0) {\n        __m12i mod1 = _mm_and_si128(v, _mm_set1_epi64x(1));\n        __m12i mod2 = _mm_and_si128(v, _mm_set1_epi64x(5));\n\n        __m12i cond1 = _mm_cmpeq_epi64(mod1, _mm_setzero_si128());\n        __m12i cond2 = _mm_cmpeq_epi64(mod2, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(cond1 | cond2) != 0) return false;\n\n        v = _mm_sub_epi64(v, divisors);\n        divisors = _mm_add_epi64(divisors, _mm_set_epi64x(12, 10));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m12i v = _mm_set_epi64x(n - 1, n - 2);\n    __m12i divisors = _mm_set_epi64x(6, 5);\n\n    for (int i = 0; i < 10; ++i) {\n        __m12i mod = _mm_mod_si128(v, divisors);\n        __m12i zero = _mm_setzero_si128();\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(mod, zero)) != 0) {\n            return false;\n        }\n        divisors = _mm_add_epi64(divisors, _mm_set_epi64x(6, 5));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m12i v_n = _mm_set1_epi64x(n);\n    __m12i v_i = _mm_set_epi64x(5, 7);\n\n    for (int i = 0; i < 10; ++i) {\n        __m12i v_div = _mm_set1_epi64x(v_i);\n        __m12i v_mod = _mm_and_si128(_mm_sad_epu8(_mm_mullo_epi32(v_n, v_div), _mm_mulhi_epi32(v_n, v_div)), _mm_set1_epi64x(0xFFFFFFFF));\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(v_mod, _mm_setzero_si128())) != 0) return false;\n        v_i = _mm_add_epi64(v_i, _mm_set_epi64x(12, 12));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m12i v_n = _mm_set1_epi64x(n);\n    __m12i v_i = _mm_set_epi64x(5, 7);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(v_i, v_n)) != 0) {\n        __m12i v_mod_2 = _mm_and_si128(v_i, _mm_set1_epi64x(1));\n        __m12i v_mod_3 = _mm_and_si128(v_i, _mm_set1_epi64x(3));\n\n        __m12i v_not_divisible_by_2 = _mm_cmpeq_epi64(v_mod_2, _mm_setzero_si128());\n        __m12i v_not_divisible_by_3 = _mm_cmpeq_epi64(v_mod_3, _mm_setzero_si128());\n\n        __m12i v_and = _mm_and_si128(v_not_divisible_by_2, v_not_divisible_by_3);\n        __m12i v_or = _mm_or_si128(v_and, _mm_cmpeq_epi64(v_i, v_n));\n\n        if (_mm_movemask_epi8(v_or) != 0) {\n            return false;\n        }\n\n        v_i = _mm_add_epi64(v_i, _mm_set_epi64x(6, 6));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m12i v_n = _mm_set1_epi64x(n);\n    __m12i v_i = _mm_set_epi64x(5, 7);\n\n    while (_mm_movemask_epi8(_mm_cmplt_epi64(v_i, v_n)) != 0) {\n        __m12i v_mod_2 = _mm_and_si128(v_i, _mm_set1_epi64x(1));\n        __m12i v_mod_3 = _mm_and_si128(v_i, _mm_set1_epi64x(3));\n\n        __m12i v_not_divisible_by_2 = _mm_cmpeq_epi64(v_mod_2, _mm_setzero_si128());\n        __m12i v_not_divisible_by_3 = _mm_cmpeq_epi64(v_mod_3, _mm_setzero_si128());\n\n        __m12i v_or = _mm_or_si128(v_not_divisible_by_2, v_not_divisible_by_3);\n        __m12i v_all_true = _mm_cmpeq_epi64(v_or, _mm_set1_epi64x(-1));\n\n        if (_mm_movemask_epi8(v_all_true) == 0xFF) {\n            int64_t i = _mm_cvtsi128_si64(v_i);\n            if (n % i == 0 || n % (i + 2) == 0) return false;\n        }\n\n        v_i = _mm_add_epi64(v_i, _mm_set_epi64x(6, 6));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    const int len = s.length();\n    if (len == 0) return s;\n\n    std::string result(len, '\\0');\n    __m128i mask = _mm_set1_epi8(0x20); // Mask for case conversion\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i str1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i str2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i + 16]));\n\n        __m128i is_alpha1 = _mm_and_si128(str1, _mm_set1_epi8(0x5F));\n        __m128i is_alpha2 = _mm_and_si128(str2, _mm_set1_epi8(0x5F));\n\n        __m128i lower_mask1 = _mm_cmpeq_epi8(is_alpha1, str1);\n        __m128i lower_mask2 = _mm_cmpeq_epi8(is_alpha2, str2);\n\n        __m128i upper_mask1 = _mm_andnot_si128(lower_mask1, _mm_set1_epi8(0xFF));\n        __m128i upper_mask2 = _mm_andnot_si128(lower_mask2, _mm_set1_epi8(0xFF));\n\n        __m128i result1 = _mm_or_si128(_mm_and_si128(str1, upper_mask1), _mm_and_si128(_mm_xor_si128(str1, mask), lower_mask1));\n        __m128i result2 = _mm_or_si128(_mm_and_si128(str2, upper_mask2), _mm_and_si128(_mm_xor_si128(str2, mask), lower_mask2));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), result1);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i + 16]), result2);\n    }\n\n    for (int i = len - len % 16; i < len; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            result[i] = c ^ 0x20;\n        } else {\n            result[i] = c;\n        }\n    }\n\n    if (len % 16 != 0) {\n        std::reverse(result.begin() + len - len % 16, result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    int len = s.length();\n    if (len == 0) return s;\n\n    __m128i mask = _mm_set1_epi8(0x20);\n    __m128i zero = _mm_setzero_si128();\n\n    std::string result(len, '\\0');\n    const char* src = s.c_str();\n    char* dst = &result[0];\n\n    while (len >= 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n        __m128i is_alpha = _mm_and_si128(_mm_or_si128(_mm_cmpgt_epi8(data, _mm_set1_epi8('9')), _mm_cmplt_epi8(data, _mm_set1_epi8('A'))), _mm_or_si128(_mm_cmpgt_epi8(data, _mm_set1_epi8('Z')), _mm_cmplt_epi8(data, _mm_set1_epi8('a'))));\n        __m128i swapped = _mm_xor_si128(data, mask);\n        __m128i final_data = _mm_blendv_epi8(data, swapped, is_alpha);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), final_data);\n\n        src += 16;\n        dst += 16;\n        len -= 16;\n    }\n\n    while (len > 0) {\n        *dst = (*src >= 'a' && *src <= 'z') ? toupper(*src) : tolower(*src);\n        src++;\n        dst++;\n        len--;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(const std::string & s) {\n    const int len = s.length();\n    if (len == 0) return s;\n\n    std::string result(len, '\\0');\n    __m128i mask = _mm_set1_epi8(0x20); // Mask to toggle case\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i str1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i str2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i + 16]));\n\n        __m128i is_alpha1 = _mm_and_si128(str1, _mm_set1_epi8(0x5F));\n        __m128i is_alpha2 = _mm_and_si128(str2, _mm_set1_epi8(0x5F));\n\n        __m128i toggled1 = _mm_xor_si128(str1, mask);\n        __m128i toggled2 = _mm_xor_si128(str2, mask);\n\n        __m128i result1 = _mm_or_si128(_mm_andnot_si128(is_alpha1, str1), toggled1);\n        __m128i result2 = _mm_or_si128(_mm_andnot_si128(is_alpha2, str2), toggled2);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), result1);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i + 16]), result2);\n    }\n\n    for (int i = len - len % 16; i < len; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            c ^= 0x20;\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(const std::string & s) {\n    const int n = s.size();\n    if (n == 0) return s;\n\n    __m128i mask = _mm_set1_epi8(0x20);\n    __m128i zero = _mm_setzero_si128();\n\n    std::string result(n, '\\0');\n    int i = 0;\n    while (i + 15 < n) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i is_alpha = _mm_and_si128(_mm_or_si128(_mm_cmpgt_epi8(str, _mm_set1_epi8('a')), _mm_cmpgt_epi8(str, _mm_set1_epi8('A'))), _mm_and_si128(_mm_cmplt_epi8(str, _mm_set1_epi8('z')), _mm_cmplt_epi8(str, _mm_set1_epi8('Z'))));\n        __m128i reversed_case = _mm_xor_si128(str, mask);\n        __m128i final_str = _mm_blendv_epi8(reversed_case, str, is_alpha);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), final_str);\n        i += 16;\n    }\n\n    // Handle remaining characters\n    for (; i < n; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            c ^= 0x20;\n        }\n        result[i] = c;\n    }\n\n    // Reverse the string if no letters were found\n    if (_mm_movemask_epi8(is_alpha) == 0) {\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    int len = s.length();\n    if (len == 0) return s;\n\n    __m128i mask = _mm_set1_epi8(0x20);\n    __m128i zero = _mm_setzero_si128();\n\n    std::string result(len, '\\0');\n    const char* src = s.c_str();\n    char* dst = &result[0];\n\n    while (len >= 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(src));\n        __m128i is_alpha = _mm_and_si128(_mm_or_si128(_mm_cmpgt_epi8(data, _mm_set1_epi8('9')), _mm_cmplt_epi8(data, _mm_set1_epi8('A'))), _mm_or_si128(_mm_cmpgt_epi8(data, _mm_set1_epi8('Z')), _mm_cmplt_epi8(data, _mm_set1_epi8('a'))));\n        __m128i swapped = _mm_xor_si128(data, mask);\n        __m128i final_data = _mm_blendv_epi8(swapped, data, is_alpha);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(dst), final_data);\n\n        src += 16;\n        dst += 16;\n        len -= 16;\n    }\n\n    while (len > 0) {\n        *dst = (*src >= 'a' && *src <= 'z') ? toupper(*src) : tolower(*src);\n        src++;\n        dst++;\n        len--;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i mask = _mm_set1_epi32(0x00000003);\n    __m12i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 4) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        __m12i indices = _mm_and_si128(vec, mask);\n        __m12i sorted_indices = _mm_sort_epi32(indices, _MM_CMPINT_LE);\n\n        __m12i sorted_values = _mm_shuffle_epi32(vec, _MM_SHUFFLE(3, 2, 1, 0));\n        sorted_values = _mm_min_epi32(sorted_values, _mm_shuffle_epi32(vec, _MM_SHUFFLE(2, 1, 0, 3)));\n        sorted_values = _mm_min_epi32(sorted_values, _mm_shuffle_epi32(vec, _MM_SHUFFLE(1, 0, 3, 2)));\n\n        _mm_storeu_si128((__m12i*)&result[i], sorted_values);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m128i v0, v1, v2, v3;\n    int i;\n\n    for (i = 0; i <= n - 12; i += 12) {\n        v0 = _mm_loadu_si128((__m128i*)&l[i]);\n        v1 = _mm_loadu_si128((__m128i*)&l[i + 4]);\n        v2 = _mm_loadu_si128((__m128i*)&l[i + 8]);\n\n        // Sort every third element\n        v3 = _mm_shuffle_epi32(v0, _MM_SHUFFLE(2, 0, 2, 0));\n        v3 = _mm_min_epi32(v3, _mm_shuffle_epi32(v1, _MM_SHUFFLE(2, 0, 2, 0)));\n        v3 = _mm_min_epi32(v3, _mm_shuffle_epi32(v2, _MM_SHUFFLE(2, 0, 2, 0)));\n\n        _mm_storeu_si128((__m128i*)&result[i], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            // Sort the value\n            int temp = l[i];\n            if (i + 1 < n && l[i + 1] < temp) {\n                temp = l[i + 1];\n                l[i + 1] = l[i];\n                l[i] = temp;\n            }\n            if (i + 2 < n && l[i + 2] < temp) {\n                temp = l[i + 2];\n                l[i + 2] = l[i];\n                l[i] = temp;\n            }\n        }\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m128i v0, v1, v2, v3;\n    int i;\n\n    for (i = 0; i <= n - 12; i += 12) {\n        v0 = _mm_loadu_si128((__m128i*)&l[i]);\n        v1 = _mm_loadu_si128((__m128i*)&l[i + 4]);\n        v2 = _mm_loadu_si128((__m128i*)&l[i + 8]);\n\n        v3 = _mm_setzero_si128();\n\n        // Sort elements at indices divisible by three\n        v3 = _mm_min_epi32(v3, v0);\n        v3 = _mm_min_epi32(v3, v1);\n        v3 = _mm_min_epi32(v3, v2);\n\n        _mm_storeu_si128((__m128i*)&result[i], v3);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            result[i] = l[i];\n        } else {\n            result[i] = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i mask = _mm_set_epi32(0, 0, 0, 0);\n    __m12i indices = _mm_set_epi32(2, 1, 0, -1);\n\n    for (int i = 0; i < n; i += 4) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        __m12i sorted_vec = vec;\n\n        __m12i idx_mask = _mm_and_si128(indices, _mm_cmpgt_epi32(_mm_setzero_si128(), vec));\n        __m12i idx = _mm_shuffle_epi32(idx_mask, _MM_SHUFFLE(3, 2, 1, 0));\n\n        __m12i temp = _mm_permutevar8x32_epi32(vec, idx);\n        sorted_vec = _mm_sort_epi32(temp, _MM_CMPINT_GT);\n\n        _mm_storeu_si128((__m12i*)&result[i], sorted_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i mask = _mm_set1_epi32(0x00000003);\n    __m12i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 4) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        __m12i indices = _mm_and_si128(vec, mask);\n        __m12i sorted_indices = _mm_sort_epi32(indices, _MM_CMPINT_LE);\n\n        for (int j = 0; j < 4; ++j) {\n            if (_mm_movemask_epi8(_mm_cmpeq_epi32(sorted_indices, _mm_set1_epi32(i + j))) & 0x0F) {\n                result[i + j] = l[i + j];\n            } else {\n                result[i + j] = l[i + j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    if (l.empty()) {\n        throw std::invalid_argument(\"Vector cannot be empty\");\n    }\n\n    __m128 max_vec = _mm_set1_ps(l[0]);\n    size_t i = 4;\n\n    while (i + 4 <= l.size()) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n        i += 4;\n    }\n\n    float max_values[4];\n    _mm_storeu_ps(max_values, max_vec);\n\n    float max_val = max_values[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_values[j] > max_val) {\n            max_val = max_values[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\nfloat max_element_simd(const std::vector<float>& l) {\n    if (l.empty()) return 0.0f;\n\n    __m128 max_val = _mm_set1_ps(l[0]);\n    size_t i = 4;\n    for (; i <= l.size() - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, max_val);\n\n    float result = temp[0];\n    for (int j = 1; j < 4; ++j) {\n        if (temp[j] > result) {\n            result = temp[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\nfloat max_element_simd(const std::vector<float>& l) {\n    if (l.empty()) {\n        throw std::invalid_argument(\"Vector cannot be empty\");\n    }\n\n    __m128 max_val = _mm_set1_ps(l[0]);\n    size_t i = 4;\n\n    while (i + 4 <= l.size()) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, vec);\n        i += 4;\n    }\n\n    float temp[4];\n    _mm_storeu_ps(temp, max_val);\n\n    float result = temp[0];\n    for (size_t j = 1; j < 4; ++j) {\n        if (temp[j] > result) {\n            result = temp[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    if (l.empty()) {\n        throw std::invalid_argument(\"Vector cannot be empty\");\n    }\n\n    __m128 max_vec = _mm_set1_ps(l[0]);\n    size_t i = 4;\n\n    while (i + 4 <= l.size()) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n        i += 4;\n    }\n\n    float max_values[4];\n    _mm_storeu_ps(max_values, max_vec);\n\n    float max_val = max_values[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_values[j] > max_val) {\n            max_val = max_values[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    if (l.size() == 0) {\n        return 0; // Handle empty vector case\n    }\n\n    __m128 max_vec = _mm_set1_ps(l[0]);\n    int i;\n\n    for (i = 4; i + 4 <= l.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_vec = _mm_max_ps(max_vec, vec);\n    }\n\n    float max_values[4];\n    _mm_storeu_ps(max_values, max_vec);\n\n    float max_val = max_values[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_values[j] > max_val) {\n            max_val = max_values[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```cpp\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m12i v7 = _mm_set1_epi32(7);\n    __m12i vmul11 = _mm_set1_epi32(11);\n    __m12i vmul13 = _mm_set1_epi32(13);\n    __m12i vzero = _mm_setzero_si128();\n    __m12i vcount = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 4) {\n        __m12i vi = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n        __m12i vmul11_i = _mm_mullo_epi32(vi, vmul11);\n        __m12i vmul13_i = _mm_mullo_epi32(vi, vmul13);\n        __m12i vmod11 = _mm_andnot_si128(_mm_cmpeq_epi32(vmul11_i, vzero), vi);\n        __m12i vmod13 = _mm_andnot_si128(_mm_cmpeq_epi32(vmul13_i, vzero), vi);\n        __m12i vsum = _mm_add_epi32(vmod11, vmod13);\n        __m12i vcontains7 = _mm_or_si128(_mm_cmpeq_epi32(vsum, v7), _mm_cmpestrm_si128(vsum, 4, v7, 1, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_EACH));\n        vcount = _mm_add_epi32(vcount, vcontains7);\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, vcount);\n    count = result[0] + result[1] + result[2] + result[3];\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```cpp\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m12i zero = _mm_setzero_si128();\n    __m12i seven = _mm_set1_epi8('7');\n    __m12i one = _mm_set1_epi8('1');\n    __m12i three = _mm_set1_epi8('3');\n\n    for (int i = 0; i < n; i += 16) {\n        __m12i numbers = _mm_loadu_si128((__m12i*)&i);\n        __m12i mod_11 = _mm_and_si128(numbers, _mm_set1_epi8(10));\n        __m12i mod_13 = _mm_and_si128(numbers, _mm_set1_epi8(12));\n        __m12i mask_11 = _mm_cmpeq_epi8(mod_11, zero);\n        __m12i mask_13 = _mm_cmpeq_epi8(mod_13, zero);\n\n        __m12i result = _mm_or_si128(mask_11, mask_13);\n        __m12i digits = _mm_setzero_si128();\n\n        for (int j = 0; j < 4; j++) {\n            __m12i temp = _mm_loadu_si128((__m12i*)&numbers[j]);\n            __m12i temp_mask_11 = _mm_cmpeq_epi8(temp, one);\n            __m12i temp_mask_13 = _mm_cmpeq_epi8(temp, three);\n            __m12i temp_result = _mm_or_si128(temp_mask_11, temp_mask_13);\n            digits = _mm_add_epi8(digits, temp_result);\n        }\n\n        __m12i count_digits = _mm_popcnt_epi8(digits);\n        count += _mm_movemask_epi8(count_digits);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```cpp\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m12i zero = _mm_setzero_si128();\n    __m12i seven = _mm_set1_epi8('7');\n    __m12i one = _mm_set1_epi8('1');\n    __m12i three = _mm_set1_epi8('3');\n\n    for (int i = 0; i < n; i += 16) {\n        __m12i numbers = _mm_loadu_si128((__m12i*)&i);\n        __m12i mod_11 = _mm_and_si128(_mm_add_epi8(numbers, _mm_set1_epi8(1)), _mm_set1_epi8(10));\n        __m12i mod_13 = _mm_and_si128(_mm_add_epi8(numbers, _mm_set1_epi8(2)), _mm_set1_epi8(13));\n\n        __m12i mask_11 = _mm_cmpeq_epi8(mod_11, zero);\n        __m12i mask_13 = _mm_cmpeq_epi8(mod_13, zero);\n\n        __m12i combined_mask = _mm_or_si128(mask_11, mask_13);\n\n        __m12i digits = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            __m12i temp = _mm_loadu_si128((__m12i*)&numbers[j]);\n            __m12i ones = _mm_and_si128(temp, _mm_set1_epi8(1));\n            __m12i threes = _mm_and_si128(temp, _mm_set1_epi8(3));\n            __m12i sevens = _mm_and_si128(temp, _mm_set1_epi8(7));\n\n            __m12i count_ones = _mm_popcnt_epi8(ones);\n            __m12i count_threes = _mm_popcnt_epi8(threes);\n            __m12i count_sevens = _mm_popcnt_epi8(sevens);\n\n            digits = _mm_add_epi8(digits, count_ones);\n            digits = _mm_add_epi8(digits, count_threes);\n            digits = _mm_add_epi8(digits, count_sevens);\n        }\n\n        __m12i result = _mm_reduce_add_epi8(digits);\n        count += static_cast<int>(result[0]) + static_cast<int>(result[1]) + static_cast<int>(result[2]) + static_cast<int>(result[3]);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```cpp\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m12i zero = _mm_setzero_si128();\n    __m12i seven = _mm_set1_epi8('7');\n    __m12i one = _mm_set1_epi8('1');\n    __m12i three = _mm_set1_epi8('3');\n\n    for (int i = 0; i < n; i += 16) {\n        __m12i numbers = _mm_loadu_si128((__m12i*)&i);\n        __m12i mod_11 = _mm_and_si128(_mm_add_epi8(numbers, _mm_sub_epi8(zero, _mm_mullo_epi8(numbers, _mm_set1_epi8(10)))), _mm_set1_epi8(10));\n        __m12i mod_13 = _mm_and_si128(_mm_add_epi8(numbers, _mm_sub_epi8(zero, _mm_mullo_epi8(numbers, _mm_set1_epi8(13)))), _mm_set1_epi8(13));\n\n        __m12i mask_11 = _mm_cmpeq_epi8(mod_11, one);\n        __m12i mask_13 = _mm_cmpeq_epi8(mod_13, three);\n\n        __m12i combined_mask = _mm_or_si128(mask_11, mask_13);\n\n        __m12i digits = _mm_setzero_si128();\n        for (int j = 0; j < 4; ++j) {\n            __m12i temp = _mm_loadu_si128((__m12i*)&numbers[j]);\n            __m12i temp_mask = _mm_and_si128(combined_mask, _mm_cmpgt_epi8(temp, zero));\n            digits = _mm_add_epi8(digits, _mm_and_si128(seven, temp_mask));\n        }\n\n        count += _mm_popcnt_u32(_mm_movemask_epi8(digits));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```cpp\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m12i zero = _mm_setzero_si128();\n    __m12i seven = _mm_set1_epi8('7');\n    __m12i one = _mm_set1_epi8('1');\n    __m12i three = _mm_set1_epi8('3');\n\n    for (int i = 0; i < n; i += 16) {\n        __m12i numbers = _mm_loadu_si128((__m12i*)&i);\n        __m12i mask1 = _mm_and_si128(numbers, _mm_set1_epi8(0xFF));\n        __m12i mask2 = _mm_and_si128(_mm_add_epi8(numbers, _mm_set1_epi8(1)), _mm_set1_epi8(0xFF));\n        __m12i mask3 = _mm_and_si128(_mm_add_epi8(numbers, _mm_set1_epi8(2)), _mm_set1_epi8(0xFF));\n\n        __m12i result1 = _mm_cmpeq_epi8(mask1, seven);\n        __m12i result2 = _mm_cmpeq_epi8(mask2, seven);\n        __m12i result3 = _mm_cmpeq_epi8(mask3, seven);\n\n        __m12i result4 = _mm_or_si128(result1, result2);\n        __m12i result5 = _mm_or_si128(result4, result3);\n\n        __m12i result6 = _mm_and_si128(result5, _mm_cmpeqd(numbers, _mm_set1_epi32(i + 11)));\n        __m12i result7 = _mm_and_si128(result5, _mm_cmpeqd(numbers, _mm_set1_epi32(i + 13)));\n\n        __m12i result8 = _mm_or_si128(result6, result7);\n\n        int temp[16];\n        _mm_storeu_si128((__m12i*)temp, result8);\n\n        for (int j = 0; j < 16; j++) {\n            if (temp[j] != 0) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n % 2 != 0) {\n        throw std::invalid_argument(\"Input vector size must be even\");\n    }\n\n    __m128* vec = (__m128*)l.data();\n    int num_elements = n / 4;\n\n    // Extract even elements into an array\n    float even_elements[4];\n    for (int i = 0; i < num_elements; ++i) {\n        even_elements[i] = _mm_cvtss_f32(vec[i]);\n    }\n\n    // Sort the even elements\n    for (int i = 0; i < num_elements - 1; ++i) {\n        for (int j = 0; j < num_elements - i - 1; ++j) {\n            if (even_elements[j] > even_elements[j + 1]) {\n                std::swap(even_elements[j], even_elements[j + 1]);\n            }\n        }\n    }\n\n    // Place sorted even elements back into the vector\n    for (int i = 0; i < num_elements; ++i) {\n        vec[i] = _mm_set_ss(even_elements[i]);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n % 2 != 0) {\n        throw std::invalid_argument(\"Vector size must be even\");\n    }\n\n    __m128* vec = (__m128*)l.data();\n    int num_elements = n / 4;\n\n    // Load even elements into an array\n    float even_elements[4];\n    for (int i = 0; i < num_elements; ++i) {\n        even_elements[i] = _mm_cvtss_f32(vec[i]);\n    }\n\n    // Sort the even elements\n    for (int i = 0; i < num_elements - 1; ++i) {\n        for (int j = 0; j < num_elements - i - 1; ++j) {\n            if (even_elements[j] > even_elements[j + 1]) {\n                std::swap(even_elements[j], even_elements[j + 1]);\n            }\n        }\n    }\n\n    // Store the sorted even elements back into the vector\n    for (int i = 0; i < num_elements; ++i) {\n        vec[i] = _mm_set_ss(even_elements[i]);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n % 2 != 0) {\n        throw std::invalid_argument(\"Vector size must be even\");\n    }\n\n    __m128* vec = (__m128*)l.data();\n    int num_elements = n / 4;\n\n    // Load even elements into an array\n    float even_elements[4];\n    for (int i = 0; i < num_elements; ++i) {\n        even_elements[i] = _mm_cvtss_f32(vec[i]);\n    }\n\n    // Sort the even elements\n    for (int i = 0; i < num_elements - 1; ++i) {\n        for (int j = 0; j < num_elements - i - 1; ++j) {\n            if (even_elements[j] > even_elements[j + 1]) {\n                std::swap(even_elements[j], even_elements[j + 1]);\n            }\n        }\n    }\n\n    // Store the sorted even elements back into the vector\n    for (int i = 0; i < num_elements; ++i) {\n        vec[i] = _mm_set_ss(even_elements[i]);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n % 2 != 0) {\n        throw std::invalid_argument(\"Input vector size must be even\");\n    }\n\n    __m128* vec = (__m128*)l.data();\n    int num_elements = n / 4;\n\n    // Extract even elements into an array\n    float even[4];\n    for (int i = 0; i < num_elements; ++i) {\n        even[i] = _mm_cvtss_f32(vec[i]);\n    }\n\n    // Sort the even elements\n    for (int i = 0; i < num_elements - 1; ++i) {\n        for (int j = 0; j < num_elements - i - 1; ++j) {\n            if (even[j] > even[j + 1]) {\n                std::swap(even[j], even[j + 1]);\n            }\n        }\n    }\n\n    // Place sorted even elements back into the original vector\n    for (int i = 0; i < num_elements; ++i) {\n        vec[i] = _mm_set_ss(even[i]);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n % 2 != 0) {\n        throw std::invalid_argument(\"Vector size must be even\");\n    }\n\n    __m128* vec = (__m128*)l.data();\n    int num_elements = n / 4;\n\n    // Load even elements into an array\n    float even_elements[4];\n    for (int i = 0; i < num_elements; ++i) {\n        even_elements[i] = _mm_cvtss_f32(vec[i]);\n    }\n\n    // Sort the even elements\n    for (int i = 0; i < num_elements - 1; ++i) {\n        for (int j = 0; j < num_elements - i - 1; ++j) {\n            if (even_elements[j] > even_elements[j + 1]) {\n                std::swap(even_elements[j], even_elements[j + 1]);\n            }\n        }\n    }\n\n    // Store the sorted even elements back into the vector\n    for (int i = 0; i < num_elements; ++i) {\n        vec[i] = _mm_set_ss(even_elements[i]);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i v0, v1, v2, v3, v4, v5;\n    __m128i sum;\n\n    for (int i = 0; i <= n - 3; ++i) {\n        v0 = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j <= n - 2; ++j) {\n            v1 = _mm_set1_epi32(l[j]);\n            for (int k = j + 1; k <= n - 1; ++k) {\n                v2 = _mm_set1_epi32(l[k]);\n                sum = _mm_add_epi32(v0, v1);\n                sum = _mm_add_epi32(sum, v2);\n                if (_mm_movemask_epi8(_mm_cmpeq_epi32(sum, _mm_setzero_si128())) != 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i v0, v1, v2, v3, v4, v5;\n    __m128i sum, mask;\n\n    for (int i = 0; i <= n - 3; ++i) {\n        v0 = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j <= n - 2; ++j) {\n            v1 = _mm_set1_epi32(l[j]);\n            for (int k = j + 1; k <= n - 1; ++k) {\n                v2 = _mm_set1_epi32(l[k]);\n                sum = _mm_add_epi32(_mm_add_epi32(v0, v1), v2);\n                mask = _mm_cmpeq_epi32(sum, _mm_setzero_si128());\n                if (_mm_movemask_epi8(mask)) return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 3) return false;\n\n    __m128i* data = (__m128i*)l.data();\n    int i = 0;\n    for (; i + 3 <= n; i += 3) {\n        __m128i v1 = _mm_loadu_si128(&data[i]);\n        __m128i v2 = _mm_loadu_si128(&data[i + 1]);\n        __m128i v3 = _mm_loadu_si128(&data[i + 2]);\n\n        __m128i sum = _mm_add_epi32(v1, v2);\n        sum = _mm_add_epi32(sum, v3);\n\n        __m128i zero = _mm_setzero_si128();\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(sum, zero)) != 0) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            for (int k = j + 1; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m128i sum = _mm_add_epi32(_mm_set1_epi32(l[i]), _mm_set1_epi32(l[j]));\n            for (int k = j + 1; k < n; ++k) {\n                __m128i temp = _mm_add_epi32(sum, _mm_set1_epi32(l[k]));\n                if (_mm_cmpeq_epi32(temp, zero)) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    __m128i v0, v1, v2, v3, v4, v5;\n    __m128i sum, mask;\n\n    for (int i = 0; i <= n - 3; ++i) {\n        v0 = _mm_set1_epi32(l[i]);\n        for (int j = i + 1; j <= n - 2; ++j) {\n            v1 = _mm_set1_epi32(l[j]);\n            for (int k = j + 1; k <= n - 1; ++k) {\n                v2 = _mm_set1_epi32(l[k]);\n                sum = _mm_add_epi32(v0, _mm_add_epi32(v1, v2));\n                mask = _mm_cmpeq_epi32(sum, _mm_setzero_si128());\n                if (_mm_movemask_epi8(mask)) return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i one = _mm_set1_epi32(1);\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        __m12i inc = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m12i*)&result[i], inc);\n    }\n\n    for (int i = n - n % 4; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i one = _mm_set1_epi32(1);\n    int i = 0;\n\n    // Process elements in chunks of 4\n    while (i + 4 <= n) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m12i*)&result[i], vec);\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i one = _mm_set1_epi32(1);\n    int i = 0;\n\n    // Process elements in chunks of 4\n    while (i + 4 <= n) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m12i*)&result[i], vec);\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i one = _mm_set1_epi32(1);\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        __m12i inc = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m12i*)&result[i], inc);\n    }\n\n    for (int i = n - n % 4; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    __m12i one = _mm_set1_epi32(1);\n    int i = 0;\n\n    // Process elements in chunks of 4\n    while (i + 4 <= n) {\n        __m12i vec = _mm_loadu_si128((__m12i*)&l[i]);\n        vec = _mm_add_epi32(vec, one);\n        _mm_storeu_si128((__m12i*)&result[i], vec);\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m128i* vec = (__m128i*)l.data();\n    int i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m128i v1 = _mm_loadu_si128(&vec[i]);\n        __m128i v2 = _mm_loadu_si128(&vec[i + 1]);\n        __m128i v3 = _mm_loadu_si128(&vec[i + 2]);\n        __m128i v4 = _mm_loadu_si128(&vec[i + 3]);\n\n        __m128i sum1 = _mm_add_epi32(v1, v2);\n        __m128i sum2 = _mm_add_epi32(v3, v4);\n\n        __m128i mask1 = _mm_cmpeq_epi32(sum1, _mm_setzero_si128());\n        __m128i mask2 = _mm_cmpeq_epi32(sum2, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(mask1) || _mm_movemask_epi8(mask2)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m128i* vec = (__m128i*)l.data();\n    int i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m128i v1 = _mm_loadu_si128(&vec[i]);\n        __m128i v2 = _mm_loadu_si128(&vec[i + 1]);\n        __m128i v3 = _mm_loadu_si128(&vec[i + 2]);\n        __m128i v4 = _mm_loadu_si128(&vec[i + 3]);\n\n        __m128i sum1 = _mm_add_epi32(v1, v2);\n        __m128i sum2 = _mm_add_epi32(v3, v4);\n\n        __m128i mask1 = _mm_cmpeq_epi32(sum1, _mm_setzero_si128());\n        __m128i mask2 = _mm_cmpeq_epi32(sum2, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(mask1) || _mm_movemask_epi8(mask2)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m128i* vec = (__m128i*)l.data();\n    int i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m128i v1 = _mm_loadu_si128(&vec[i]);\n        __m128i v2 = _mm_loadu_si128(&vec[i + 1]);\n        __m128i v3 = _mm_loadu_si128(&vec[i + 2]);\n        __m128i v4 = _mm_loadu_si128(&vec[i + 3]);\n\n        __m128i sum1 = _mm_add_epi32(v1, v2);\n        __m128i sum2 = _mm_add_epi32(v3, v4);\n\n        __m128i mask1 = _mm_cmpeq_epi32(sum1, _mm_setzero_si128());\n        __m128i mask2 = _mm_cmpeq_epi32(sum2, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(mask1) || _mm_movemask_epi8(mask2)) {\n            return true;\n        }\n    }\n\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    const int n = l.size();\n    if (n < 2) return false;\n\n    __m128i* data = (__m128i*)l.data();\n    int i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m128i v1 = _mm_loadu_si128(&data[i]);\n        __m128i v2 = _mm_loadu_si128(&data[i + 1]);\n        __m128i v3 = _mm_loadu_si128(&data[i + 2]);\n        __m128i v4 = _mm_loadu_si128(&data[i + 3]);\n\n        __m128i sum1 = _mm_add_epi32(v1, v2);\n        __m128i sum2 = _mm_add_epi32(v3, v4);\n\n        __m128i mask1 = _mm_cmpeq_epi32(sum1, _mm_setzero_si128());\n        __m128i mask2 = _mm_cmpeq_epi32(sum2, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(mask1) || _mm_movemask_epi8(mask2)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    __m128i* vec = (__m128i*)l.data();\n    int i = 0;\n    for (; i + 3 < n; i += 4) {\n        __m128i v1 = _mm_loadu_si128(&vec[i]);\n        __m128i v2 = _mm_loadu_si128(&vec[i + 1]);\n        __m128i v3 = _mm_loadu_si128(&vec[i + 2]);\n        __m128i v4 = _mm_loadu_si128(&vec[i + 3]);\n\n        __m128i sum1 = _mm_add_epi32(v1, v2);\n        __m128i sum2 = _mm_add_epi32(v3, v4);\n\n        __m128i mask1 = _mm_cmpeq_epi32(sum1, _mm_setzero_si128());\n        __m128i mask2 = _mm_cmpeq_epi32(sum2, _mm_setzero_si128());\n\n        if (_mm_movemask_epi8(mask1) || _mm_movemask_epi8(mask2)) {\n            return true;\n        }\n    }\n\n    for (; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            if (l[i] + l[j] == 0) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <iostream>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result = \"\";\n    if (x == 0) return \"0\";\n    while (x != 0) {\n        result = std::to_string(x % base) + result;\n        x /= base;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <iostream>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base <= 1 || base >= 10) {\n        return \"Invalid base\";\n    }\n\n    std::string result = \"\";\n    while (x != 0) {\n        int remainder = x % base;\n        result = std::to_string(remainder) + result;\n        x /= base;\n    }\n\n    return result.empty() ? \"0\" : result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    // Implementation using SSE/SSE2 intrinsics\n    __m128i vec_x = _mm_set_epi64x(x, 0);\n    __m128i vec_base = _mm_set1_epi64x(base);\n\n    // Perform division and remainder operations in parallel\n    __m128i quotient, remainder;\n    _mm_divmod_epi64(vec_x, vec_base, &quotient, &remainder);\n\n    // Convert the result to string representation\n    std::string result;\n    while (_mm_movemask_epi8(_mm_cmpeq_epi8(quotient, _mm_setzero_si128())) == 0) {\n        result += '0' + _mm_extract_epi8(remainder, 0);\n        _mm_divmod_epi64(quotient, vec_base, &quotient, &remainder);\n    }\n    result += '0' + _mm_extract_epi8(remainder, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    std::string result = \"\";\n    __m128i vec_x = _mm_set_epi64x(x, 0);\n    while (_mm_movemask_epi8(vec_x) != 0) {\n        __m128i div_result = _mm_div_epi64(vec_x, _mm_set1_epi64x(base));\n        __m128i mod_result = _mm_sub_epi64(vec_x, _mm_mul_epi64(div_result, _mm_set1_epi64x(base)));\n        char digit = static_cast<char>(_mm_extract_epi64(mod_result, 0) + '0');\n        result += digit;\n        vec_x = div_result;\n    }\n    return std::string(result.rbegin(), result.rend());\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    // Implementation using SSE/SSE2 intrinsics\n    __m128i vec_x = _mm_set_epi64x(x, 0);\n    __m128i vec_base = _mm_set1_epi64x(base);\n\n    // Perform division and remainder operations in parallel\n    __m128i quotient, remainder;\n    _mm_divmod_epi64(vec_x, vec_base, &quotient, &remainder);\n\n    // Convert the result to a string\n    std::string result;\n    int64_t q = _mm_cvtsi128_si64(quotient);\n    int64_t r = _mm_cvtsi128_si64(remainder);\n\n    while (q != 0 || r != 0) {\n        if (r < 10) {\n            result += '0' + r;\n        } else {\n            result += 'A' + (r - 10);\n        }\n        int64_t temp_q = q / base;\n        int64_t temp_r = q % base;\n        q = temp_q;\n        r = temp_r;\n    }\n\n    return std::string(result.rbegin(), result.rend());\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m128i mask = _mm_setzero_si128();\n\n    for (int i = 0; i < len / 16; ++i) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + i * 16));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + len - 1 - i * 16));\n\n        __m128i reversed_right = _mm_reverse_epi8(right);\n        __m128i cmp = _mm_cmpeq_epi8(left, reversed_right);\n\n        mask = _mm_or_si128(mask, cmp);\n    }\n\n    int remaining = len % 16;\n    for (int i = 0; i < remaining; ++i) {\n        if (text[i] != text[len - 1 - i]) return false;\n    }\n\n    return _mm_movemask_epi8(mask) == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m128i mask = _mm_set1_epi8(0xFF);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < len / 16; ++i) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + i * 16));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + (len - 1 - i) * 16));\n\n        left = _mm_and_si128(left, mask);\n        right = _mm_and_si128(right, mask);\n\n        left = _mm_reverse_epi8(left);\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(3, 2, 1, 0));\n\n        if (!_mm_cmpeq_epi8(left, right).mmask16) {\n            return false;\n        }\n    }\n\n    // Check remaining characters\n    for (int i = len - len % 16; i < len; ++i) {\n        if (text[i] != text[len - 1 - i]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m128i mask = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < len / 16; ++i) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + i * 16));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + len - 1 - i * 16));\n\n        // Reverse the right vector\n        __m128i reversed_right = _mm_reverse_epi8(right);\n\n        // Compare left and reversed right\n        __m128i cmp = _mm_cmpeq_epi8(left, reversed_right);\n        mask = _mm_or_si128(mask, cmp);\n    }\n\n    // Check remaining characters\n    for (int i = len - len % 16; i < len; ++i) {\n        if (text[i] != text[len - 1 - i]) {\n            return false;\n        }\n    }\n\n    // Check if all comparisons were equal\n    return _mm_movemask_epi8(mask) == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m128i mask = _mm_setzero_si128();\n\n    for (int i = 0; i < len / 16; ++i) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + i * 16));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + (len - 1 - i) * 16));\n\n        __m128i reversed_right = _mm_reverse_epi8(right);\n        __m128i cmp = _mm_cmpeq_epi8(left, reversed_right);\n\n        mask = _mm_or_si128(mask, cmp);\n    }\n\n    int remaining = len % 16;\n    for (int i = 0; i < remaining; ++i) {\n        if (text[i] != text[len - 1 - i]) {\n            return false;\n        }\n    }\n\n    return _mm_movemask_epi8(mask) == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(const std::string& text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    __m128i mask = _mm_set1_epi8(0xFF);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int i = 0; i < len / 16; ++i) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + i * 16));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(text.c_str() + len - 16 - i * 16));\n\n        left = _mm_and_si128(left, mask);\n        right = _mm_and_si128(right, mask);\n\n        left = _mm_shuffle_epi32(left, _MM_SHUFFLE(3, 2, 1, 0));\n        right = _mm_shuffle_epi32(right, _MM_SHUFFLE(3, 2, 1, 0));\n\n        left = _mm_shuffle_epi8(left, _mm_setr_epi8(0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15));\n        right = _mm_shuffle_epi8(right, _mm_setr_epi8(0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15));\n\n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) return false;\n    }\n\n    // Check remaining characters\n    for (int i = len - len % 16; i < len; ++i) {\n        if (text[i] != text[len - 1 - i]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(const std::string& s) {\n    const size_t len = s.length();\n    std::string out(len, '\\0');\n    __m128i* in_ptr = (__m128i*)s.c_str();\n    __m128i* out_ptr = (__m128i*)out.data();\n    const __m128i shift = _mm_set1_epi8(5);\n    const __m128i base = _mm_set1_epi8('a');\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i data = _mm_loadu_si128(in_ptr + i / 16);\n        __m128i shifted = _mm_sub_epi8(data, shift);\n        __m128i wrapped = _mm_add_epi8(shifted, base);\n        __m128i masked = _mm_and_si128(wrapped, _mm_set1_epi8(0x7F));\n        _mm_storeu_si128(out_ptr + i / 16, masked);\n    }\n\n    // Handle remaining bytes\n    for (size_t i = len - len % 16; i < len; ++i) {\n        out[i] = ((s[i] - 'a' - 5 + 26) % 26) + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(const std::string& s) {\n    const int n = s.size();\n    std::string out(n, '\\0');\n    __m128i* in_ptr = (__m128i*)s.c_str();\n    __m128i* out_ptr = (__m128i*)out.data();\n    int i;\n\n    // Process 16 characters at a time\n    for (i = 0; i <= n - 16; i += 16) {\n        __m128i vec_in = _mm_loadu_si128(in_ptr + i / 16);\n        __m128i vec_out = _mm_sub_epi8(vec_in, _mm_set1_epi8(5));\n        vec_out = _mm_add_epi8(vec_out, _mm_set1_epi8('a'));\n        vec_out = _mm_and_si128(vec_out, _mm_set1_epi8(0x7F)); // Ensure it's a printable ASCII character\n        _mm_storeu_si128(out_ptr + i / 16, vec_out);\n    }\n\n    // Handle remaining characters\n    for (; i < n; ++i) {\n        out[i] = ((s[i] - 'a' - 5 + 26) % 26) + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s) {\n    const int chunk_size = 16;\n    std::string out(s.size(), '\\0');\n    __m128i shift = _mm_set1_epi8(5);\n    __m128i base = _mm_set1_epi8('a');\n\n    for (size_t i = 0; i <= s.size() - chunk_size; i += chunk_size) {\n        __m128i input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i shifted = _mm_sub_epi8(input, shift);\n        __m128i wrapped = _mm_add_epi8(shifted, base);\n        __m128i result = _mm_and_si128(wrapped, _mm_cmplt_epi8(wrapped, base));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), result);\n    }\n\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % chunk_size; i < s.size(); ++i) {\n        out[i] = ((s[i] - 'a' - 5 + 26) % 26) + 'a';\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s) {\n    const int chunk_size = 16;\n    std::string out(s.size(), '\\0');\n    __m128i shift_amount = _mm_set1_epi8(5);\n    __m128i base_char = _mm_set1_epi8('a');\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        size_t end = std::min(i + chunk_size, s.size());\n        __m128i input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i shifted = _mm_sub_epi8(input, shift_amount);\n        __m128i wrapped = _mm_add_epi8(shifted, base_char);\n        __m128i result = _mm_and_si128(wrapped, _mm_cmplt_epi8(wrapped, base_char));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), result);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s) {\n    const int chunk_size = 16;\n    std::string out(s.size(), '\\0');\n    __m128i shift_val = _mm_set1_epi8(5);\n    __m128i base_val = _mm_set1_epi8('a');\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        size_t end = std::min(i + chunk_size, s.size());\n        __m128i input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i shifted = _mm_sub_epi8(input, shift_val);\n        __m128i wrapped = _mm_add_epi8(shifted, base_val);\n        __m128i result = _mm_and_si128(wrapped, _mm_cmplt_epi8(wrapped, base_val));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), result);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    const __m128i threshold = _mm_set1_epi32(t);\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) return false;\n    }\n    for (size_t i = l.size() - l.size() % 4; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) return false;\n    }\n    for (size_t i = l.size() - l.size() % 4; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) return false;\n    }\n    for (size_t i = l.size() - l.size() % 4; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) return false;\n    }\n    for (size_t i = l.size() - l.size() % 4; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    __m128i threshold = _mm_set1_epi32(t);\n    for (size_t i = 0; i <= l.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) return false;\n    }\n    for (size_t i = l.size() - l.size() % 4; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 16 != 0) {\n        return false;\n    }\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i count_open = zero;\n\n    for (int i = 0; i < length; i += 16) {\n        __m128i current = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_mask = _mm_cmpeq_epi8(current, _mm_set1_epi8('<'));\n        __m128i close_mask = _mm_cmpeq_epi8(current, _mm_set1_epi8('>'));\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_open = _mm_sub_epi32(count_open, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, count_open);\n    return result[0] == 0 && result[1] == 0 && result[2] == 0 && result[3] == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 16 != 0) {\n        return false;\n    }\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < length; i += 16) {\n        __m128i current = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_mask = _mm_cmpeq_epi8(current, _mm_set1_epi8('<'));\n        __m128i close_mask = _mm_cmpeq_epi8(current, _mm_set1_epi8('>'));\n        count = _mm_add_epi32(count, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count = _mm_sub_epi32(count, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, count);\n    return result[0] == 0 && result[1] == 0 && result[2] == 0 && result[3] == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.c_str() + i));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i mask_open = _mm_cmpeq_epi8(data, _mm_set1_epi8('<'));\n        __m128i mask_close = _mm_cmpeq_epi8(data, _mm_set1_epi8('>'));\n        count = _mm_add_epi32(count, _mm_and_si128(mask_open, _mm_set1_epi32(1)));\n        count = _mm_sub_epi32(count, _mm_and_si128(mask_close, _mm_set1_epi32(1)));\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), count);\n\n    return result[0] == 0 && result[1] == 0 && result[2] == 0 && result[3] == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int length = brackets.length();\n    if (length % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < length; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8(')'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i current = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_mask = _mm_cmpeq_epi8(current, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(current, _mm_set1_epi8(')'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int length = brackets.length();\n    if (length % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < length; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8(')'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int length = brackets.length();\n    if (length % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < length; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8(')'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    const int n = brackets.size();\n    if (n % 16 != 0) return false;\n\n    __m128i count_open = _mm_setzero_si128();\n    __m128i count_close = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i open_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8('('));\n        __m128i close_mask = _mm_cmpeq_epi8(data, _mm_set1_epi8(')'));\n\n        count_open = _mm_add_epi32(count_open, _mm_and_si128(open_mask, _mm_set1_epi32(1)));\n        count_close = _mm_add_epi32(count_close, _mm_and_si128(close_mask, _mm_set1_epi32(1)));\n    }\n\n    int total_open = _mm_extract_epi32(count_open, 0);\n    int total_close = _mm_extract_epi32(count_close, 0);\n\n    return total_open == total_close;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    if (n <= 1) return result;\n\n    __m128 v0, v1, v2;\n    int i = 0;\n\n    // Process elements in groups of 4\n    while (i + 3 < n) {\n        v0 = _mm_loadu_ps(&xs[i]);\n        v1 = _mm_setzero_ps();\n        v2 = _mm_setzero_ps();\n\n        v1 = _mm_add_ps(v1, _mm_mul_ps(_mm_set1_ps(1), _mm_slli_si128(v0, 1)));\n        v1 = _mm_add_ps(v1, _mm_mul_ps(_mm_set1_ps(2), _mm_slli_si128(v0, 2)));\n        v1 = _mm_add_ps(v1, _mm_mul_ps(_mm_set1_ps(3), _mm_slli_si128(v0, 3)));\n\n        _mm_storeu_ps(&result[i], v1);\n        i += 4;\n    }\n\n    // Process remaining elements\n    while (i < n) {\n        result[i] = xs[i] * i;\n        i++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    if (n <= 1) return result;\n\n    __m128 v0, v1, v2;\n    int i = 0;\n\n    // Process elements in groups of 4\n    while (i + 3 < n) {\n        v0 = _mm_loadu_ps(&xs[i]);\n        v1 = _mm_mul_ps(v0, _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f));\n        _mm_storeu_ps(&result[i], v1);\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i - 1] = xs[i] * static_cast<float>(i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    if (n <= 1) return result;\n\n    __m128 v0, v1, v2;\n    int i = 0;\n\n    // Process elements in groups of 4\n    while (i + 3 < n) {\n        v0 = _mm_loadu_ps(&xs[i]);\n        v1 = _mm_mul_ps(v0, _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f));\n        v2 = _mm_add_ps(_mm_shuffle_ps(v1, v1, _MM_SHUFFLE(0, 0, 0, 0)), _mm_shuffle_ps(v1, v1, _MM_SHUFFLE(1, 1, 1, 1)));\n        _mm_storeu_ps(&result[i], v2);\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i - 1] = xs[i] * static_cast<float>(n - i - 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    if (n <= 1) return result;\n\n    __m128 vzero = _mm_setzero_ps();\n\n    for (int i = 0; i < n - 1; i += 4) {\n        __m128 va = _mm_loadu_ps(&xs[i]);\n        __m128 vb = _mm_setr_ps(1.0f, 2.0f, 3.0f, 4.0f);\n        __m128 vc = _mm_mul_ps(va, vb);\n        __m128 vd = _mm_add_ps(vc, vzero);\n        _mm_storeu_ps(&result[i], vd);\n    }\n\n    // Handle remaining elements\n    for (int i = (n - 1) & ~3; i < n - 1; ++i) {\n        result[i] = xs[i + 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n\n    if (n <= 1) return result;\n\n    __m128 v0, v1, v2;\n    int i = 0;\n\n    // Process elements in groups of 4\n    while (i + 4 <= n - 1) {\n        v0 = _mm_loadu_ps(&xs[i]);\n        v1 = _mm_setzero_ps();\n\n        v0 = _mm_mul_ps(v0, _mm_set_ps(4.0f, 3.0f, 2.0f, 1.0f));\n        v1 = _mm_add_ps(v1, _mm_shuffle_ps(v0, v0, _MM_SHUFFLE(0, 0, 0, 0)));\n        v1 = _mm_add_ps(v1, _mm_shuffle_ps(v0, v0, _MM_SHUFFLE(1, 1, 1, 1)));\n        v1 = _mm_add_ps(v1, _mm_shuffle_ps(v0, v0, _MM_SHUFFLE(2, 2, 2, 2)));\n        v1 = _mm_add_ps(v1, _mm_shuffle_ps(v0, v0, _MM_SHUFFLE(3, 3, 3, 3)));\n\n        _mm_storeu_ps(&result[i], v1);\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i] * (n - 1 - i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi8('a');\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('A'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('e'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('E'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('i'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('I'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('o'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('O'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('u'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('U'));\n\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask_and = _mm_and_si128(str_vec, vowels_mask);\n        __m128i mask_eq = _mm_cmpeq_epi8(mask_and, str_vec);\n        int eq_bits = _mm_movemask_epi8(mask_eq);\n        count += __builtin_popcount(eq_bits);\n\n        if (i + 3 == s.size()) {\n            char last_char = s.back();\n            if (last_char == 'y' || last_char == 'Y') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi8('a');\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('A'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('e'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('E'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('i'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('I'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('o'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('O'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('u'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('U'));\n\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask_and = _mm_and_si128(str_vec, vowels_mask);\n        __m128i mask_eq = _mm_cmpeq_epi8(mask_and, str_vec);\n        int mask_val = _mm_movemask_epi8(mask_eq);\n        count += __builtin_popcount(mask_val);\n\n        if (i + 3 == s.size()) {\n            char last_char = s.back();\n            if (last_char == 'y' || last_char == 'Y') {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi8('a');\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('e'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('i'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('o'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('u'));\n\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str_vec, vowels_mask);\n        __m128i eq_mask = _mm_cmpeq_epi8(mask, str_vec);\n        int eq_count = _mm_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n\n        if (i + 3 < s.size()) {\n            char last_char = s[i + 3];\n            if (last_char == 'y' || last_char == 'Y') {\n                count++;\n            }\n        }\n    }\n\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % 4; i < s.size(); ++i) {\n        if (s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u' ||\n            (i == s.size() - 1 && (s[i] == 'y' || s[i] == 'Y'))) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi8('a');\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('e'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('i'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('o'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('u'));\n\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str_vec, vowels_mask);\n        __m128i eq_mask = _mm_cmpeq_epi8(mask, str_vec);\n        int eq_count = _mm_movemask_epi8(eq_mask);\n        count += __builtin_popcount(eq_count);\n\n        if (i + 3 == s.size()) {\n            char last_char = s.back();\n            if (last_char == 'a' || last_char == 'e' || last_char == 'i' || last_char == 'o' || last_char == 'u' || (last_char == 'y' && i + 3 == s.size() - 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi8('a');\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('e'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('i'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('o'));\n    vowels_mask = _mm_or_si128(vowels_mask, _mm_set1_epi8('u'));\n\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_str_vec = _mm_cmplt_epi8(str_vec, _mm_setzero_si128());\n        str_vec = _mm_andnot_si128(lower_str_vec, str_vec);\n        str_vec = _mm_or_si128(str_vec, _mm_cmpeq_epi8(str_vec, vowels_mask));\n        str_vec = _mm_and_si128(str_vec, _mm_cmpeq_epi8(_mm_srli_epi16(str_vec, 8), _mm_setzero_si128()));\n        count += _mm_popcnt_u32(_mm_movemask_epi8(str_vec));\n    }\n\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % 4; i < s.size(); ++i) {\n        if (std::tolower(s[i]) == 'a' || std::tolower(s[i]) == 'e' || std::tolower(s[i]) == 'i' ||\n            std::tolower(s[i]) == 'o' || std::tolower(s[i]) == 'u') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\nint digitSum_simd(const std::string& s) {\n    __m128i sum = _mm_setzero_si128();\n    int len = s.length();\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0xDF));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_set1_epi8(0), mask);\n        __m128i valid = _mm_and_si128(cmp, ascii);\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(valid, _mm_setzero_si128()));\n    }\n\n    int remaining = len % 16;\n    if (remaining > 0) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[len - remaining]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0xDF));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_set1_epi8(0), mask);\n        __m128i valid = _mm_and_si128(cmp, ascii);\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(valid, _mm_setzero_si128()));\n    }\n\n    int result = 0;\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&result), sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\nint digitSum_simd(const std::string& s) {\n    __m128i sum = _mm_setzero_si128();\n    int len = s.length();\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0xDF));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_set1_epi8('A'), _mm_sub_epi8(mask, _mm_set1_epi8('a')));\n        __m128i result = _mm_and_si128(cmp, ascii);\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(result, _mm_setzero_si128()));\n    }\n\n    int remaining = len % 16;\n    if (remaining > 0) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[len - remaining]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0xDF));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_set1_epi8('A'), _mm_sub_epi8(mask, _mm_set1_epi8('a')));\n        __m128i result = _mm_and_si128(cmp, ascii);\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(result, _mm_setzero_si128()));\n    }\n\n    int total_sum = 0;\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&total_sum), sum);\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(const std::string& s) {\n    int sum = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i ascii_a = _mm_set1_epi8('A');\n    __m128i ascii_z = _mm_set1_epi8('Z');\n\n    for (size_t i = 0; i <= s.size() - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(_mm_cmpgt_epi8(chunk, ascii_a), _mm_cmplt_epi8(chunk, ascii_z));\n        __m128i ascii_values = _mm_add_epi8(chunk, _mm_set1_epi8(-'A'));\n        __m128i sum_chunk = _mm_mullo_epi8(ascii_values, _mm_set1_epi8(1));\n        sum += _mm_extract_epi32(sum_chunk, 0);\n        sum += _mm_extract_epi32(sum_chunk, 1);\n        sum += _mm_extract_epi32(sum_chunk, 2);\n        sum += _mm_extract_epi32(sum_chunk, 3);\n    }\n\n    // Handle remaining bytes\n    for (size_t i = s.size() - s.size() % 16; i < s.size(); ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i] - 'A';\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\nint digitSum_simd(const std::string& s) {\n    __m128i sum = _mm_setzero_si128();\n    int len = s.length();\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0xDF));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_mullo_epi8(cmp, str), _mm_set1_epi8(-48));\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(ascii, _mm_setzero_si128()));\n    }\n\n    int remaining = len % 16;\n    if (remaining > 0) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[len - remaining]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0xDF));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_mullo_epi8(cmp, str), _mm_set1_epi8(-48));\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(ascii, _mm_setzero_si128()));\n    }\n\n    int result = 0;\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&result), sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\nint digitSum_simd(const std::string& s) {\n    __m128i sum = _mm_setzero_si128();\n    int len = s.length();\n\n    for (int i = 0; i <= len - 16; i += 16) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0x5F));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_mullo_epi8(cmp, str), _mm_set1_epi8(-64));\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(ascii, _mm_setzero_si128()));\n    }\n\n    int remaining = len % 16;\n    if (remaining > 0) {\n        __m128i str = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[len - remaining]));\n        __m128i mask = _mm_and_si128(str, _mm_set1_epi8(0x5F));\n        __m128i cmp = _mm_cmpeq_epi8(mask, str);\n        __m128i ascii = _mm_add_epi8(_mm_mullo_epi8(cmp, str), _mm_set1_epi8(-64));\n        sum = _mm_add_epi32(sum, _mm_sad_epu8(ascii, _mm_setzero_si128()));\n    }\n\n    int result = 0;\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&result), sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int size = arr.size();\n\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    __m128i min_idx = _mm_set1_epi32(-1);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i val = _mm_loadu_si128((__m128i*)&data[i]);\n        __m128i idx = _mm_set1_epi32(i);\n\n        __m128i mask = _mm_and_si128(val, _mm_cmpeq_epi32(_mm_and_si128(val, _mm_set1_epi32(1)), _mm_setzero_si128()));\n        __m128i new_min_val = _mm_min_epi32(val, min_val);\n        __m128i new_min_idx = _mm_blendv_epi8(min_idx, idx, _mm_cmpgt_epi32(new_min_val, min_val));\n\n        min_val = _mm_or_si128(_mm_andnot_si128(mask, min_val), _mm_and_si128(mask, new_min_val));\n        min_idx = _mm_or_si128(_mm_andnot_si128(mask, min_idx), _mm_and_si128(mask, new_min_idx));\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, min_val);\n\n    std::vector<int> ret;\n    for (int i = 0; i < 4; ++i) {\n        if (result[i] != INT_MAX && (result[i] % 2 == 0 || ret.empty())) {\n            ret.push_back(result[i]);\n            ret.push_back(i);\n            break;\n        }\n    }\n\n    return ret;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m128i v_min_even_value = _mm_set1_epi32(INT_MAX);\n    __m128i v_min_index = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i v_values = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        __m128i v_indices = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n\n        __m128i v_mask = _mm_and_si128(v_values, _mm_cmpeq_epi32(_mm_and_si128(v_values, _mm_set1_epi32(1)), _mm_setzero_si128()));\n        __m128i v_even_values = _mm_and_si128(v_values, v_mask);\n        __m128i v_even_indices = _mm_and_si128(v_indices, v_mask);\n\n        __m128i v_min_even_value_new = _mm_min_epi32(v_min_even_value, v_even_values);\n        __m128i v_min_index_new = _mm_or_si128(_mm_and_si128(_mm_cmpgt_epi32(v_min_even_value, v_even_values), v_min_index), v_even_indices);\n\n        v_min_even_value = v_min_even_value_new;\n        v_min_index = v_min_index_new;\n    }\n\n    for (size_t i = arr.size() - arr.size() % 4; i < arr.size(); ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even_value, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m128i v_min_even_value = _mm_set1_epi32(INT_MAX);\n    __m128i v_min_index = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i v_values = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        __m128i v_mask = _mm_and_si128(v_values, _mm_set1_epi32(1));\n        __m128i v_is_even = _mm_cmpeq_epi32(v_mask, _mm_setzero_si128());\n\n        __m128i v_indices = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n\n        __m128i v_selected_values = _mm_blendv_epi32(_mm_set1_epi32(INT_MAX), v_values, v_is_even);\n        __m128i v_selected_indices = _mm_blendv_epi32(_mm_set1_epi32(-1), v_indices, v_is_even);\n\n        v_min_even_value = _mm_min_epi32(v_min_even_value, v_selected_values);\n        v_min_index = _mm_min_epi32(v_min_index, v_selected_indices);\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), v_min_even_value);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result + 2), v_min_index);\n\n    for (int i = 0; i < 4; ++i) {\n        if (result[i] != INT_MAX && result[i] < min_even_value) {\n            min_even_value = result[i];\n            min_index = result[i + 2];\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m128i v_min_even_value = _mm_set1_epi32(INT_MAX);\n    __m128i v_min_index = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i v_values = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i v_indices = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n\n        __m128i v_mask = _mm_and_si128(v_values, _mm_cmpeq_epi32(_mm_and_si128(v_values, _mm_set1_epi32(1)), _mm_setzero_si128()));\n        __m128i v_even_values = _mm_and_si128(v_values, v_mask);\n        __m128i v_even_indices = _mm_and_si128(v_indices, v_mask);\n\n        __m128i v_min_even_values_new = _mm_min_epi32(v_min_even_values, v_even_values);\n        __m128i v_min_indices_new = _mm_or_si128(_mm_andnot_si128(v_min_even_values, v_even_indices), _mm_and_si128(v_min_even_values, v_min_indices));\n\n        v_min_even_values = v_min_even_values_new;\n        v_min_indices = v_min_indices_new;\n    }\n\n    for (size_t i = arr.size() - arr.size() % 4; i < arr.size(); ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_even_value, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return {};\n    }\n\n    const int* data = arr.data();\n    int min_even_value = INT_MAX;\n    int min_index = -1;\n\n    __m128i v_min_even_value = _mm_set1_epi32(INT_MAX);\n    __m128i v_min_index = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i v_values = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        __m128i v_indices = _mm_setr_epi32(i, i + 1, i + 2, i + 3);\n\n        __m128i v_mask = _mm_and_si128(v_values, _mm_cmpeq_epi32(_mm_and_si128(v_values, _mm_set1_epi32(1)), _mm_setzero_si128()));\n        __m128i v_even_values = _mm_and_si128(v_values, v_mask);\n        __m128i v_even_indices = _mm_and_si128(v_indices, v_mask);\n\n        __m128i v_min_even_values_new = _mm_min_epi32(v_min_even_value, v_even_values);\n        __m128i v_min_indices_new = _mm_or_si128(_mm_andnot_si128(v_min_even_values, v_min_indices), _mm_and_si128(v_min_even_values, v_even_indices));\n\n        v_min_even_value = v_min_even_values_new;\n        v_min_index = v_min_indices_new;\n    }\n\n    for (size_t i = arr.size() - arr.size() % 4; i < arr.size(); ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even_value) {\n            min_even_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) {\n        return {};\n    }\n\n    return {min_even_value, min_index};\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    const int n = lst.size();\n    if (n < 16) return -1;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    std::vector<int> count(16, 0);\n    for (int i = 0; i < n; i += 16) {\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        for (int j = 0; j < 16; ++j) {\n            int val = _mm_extract_epi32(v, j);\n            if (val > 0 && val <= 16) {\n                count[val - 1]++;\n            }\n        }\n    }\n\n    for (int i = 15; i >= 0; --i) {\n        if (count[i] >= i + 1) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    const int n = lst.size();\n    if (n < 16) {\n        // Fallback to scalar implementation for small vectors\n        std::unordered_map<int, int> freq;\n        for (int num : lst) {\n            ++freq[num];\n        }\n        for (const auto& pair : freq) {\n            if (pair.first <= pair.second) {\n                return pair.first;\n            }\n        }\n        return -1;\n    }\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i four = _mm_set1_epi32(4);\n\n    std::unordered_map<int, __m128i> freq;\n    for (size_t i = 0; i < n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        for (int j = 0; j < 4; ++j) {\n            int num = _mm_extract_epi32(vec, j);\n            if (num > 0 && num <= 15) {\n                __m128i count = freq.count(num) ? freq[num] : zero;\n                freq[num] = _mm_add_epi32(count, one);\n            }\n        }\n    }\n\n    for (const auto& pair : freq) {\n        if (pair.first <= _mm_extract_epi32(pair.second, 0)) {\n            return pair.first;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint search_simd(std::vector<int> lst) {\n    const int n = lst.size();\n    if (n < 10) {\n        // Fallback to scalar implementation for small vectors\n        std::unordered_map<int, int> freq;\n        for (int num : lst) {\n            ++freq[num];\n        }\n        for (int i = n; i > 0; --i) {\n            if (freq[i] >= i) {\n                return i;\n            }\n        }\n        return -1;\n    }\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i four = _mm_set1_epi32(4);\n\n    std::unordered_map<int, int> freq;\n    for (size_t i = 0; i + 3 < n; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask = _mm_cmpgt_epi32(v, zero);\n        __m128i count = _mm_and_si128(mask, one);\n        _mm_add_epi32(freq[v[0]], count);\n        _mm_add_epi32(freq[v[1]], count);\n        _mm_add_epi32(freq[v[2]], count);\n        _mm_add_epi32(freq[v[3]], count);\n    }\n\n    for (size_t i = n - n % 4; i < n; ++i) {\n        if (lst[i] > 0) {\n            ++freq[lst[i]];\n        }\n    }\n\n    for (int i = n; i > 0; --i) {\n        if (freq[i] >= i) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    const int n = lst.size();\n    if (n < 1024) {\n        // Fallback to scalar implementation for small vectors\n        std::map<int, int> freq;\n        for (int num : lst) {\n            ++freq[num];\n        }\n        for (auto it = freq.rbegin(); it != freq.rend(); ++it) {\n            if (it->first <= it->second) {\n                return it->first;\n            }\n        }\n        return -1;\n    }\n\n    __m128i* data = (__m128i*)lst.data();\n    int max_val = -1;\n\n    for (int i = 0; i + 15 < n; i += 16) {\n        __m128i v0 = _mm_loadu_si128(&data[i]);\n        __m128i v1 = _mm_loadu_si128(&data[i + 1]);\n        __m128i v2 = _mm_loadu_si128(&data[i + 2]);\n        __m128i v3 = _mm_loadu_si128(&data[i + 3]);\n        __m128i v4 = _mm_loadu_si128(&data[i + 4]);\n        __m128i v5 = _mm_loadu_si128(&data[i + 5]);\n        __m128i v6 = _mm_loadu_si128(&data[i + 6]);\n        __m128i v7 = _mm_loadu_si128(&data[i + 7]);\n        __m128i v8 = _mm_loadu_si128(&data[i + 8]);\n        __m128i v9 = _mm_loadu_si128(&data[i + 9]);\n        __m128i va = _mm_loadu_si128(&data[i + 10]);\n        __m128i vb = _mm_loadu_si128(&data[i + 11]);\n        __m128i vc = _mm_loadu_si128(&data[i + 12]);\n        __m128i vd = _mm_loadu_si128(&data[i + 13]);\n        __m128i ve = _mm_loadu_si128(&data[i + 14]);\n\n        __m128i mask = _mm_set1_epi32(0xFFFFFFFF);\n        __m128i count[16];\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_and_si128(_mm_cmpeq_epi32(v0, _mm_set1_epi32(i + j)), mask);\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v1, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v2, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v3, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v4, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v5, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v6, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v7, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v8, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(v9, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(va, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(vb, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(vc, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(vd, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            count[j] = _mm_add_epi32(count[j], _mm_and_si128(_mm_cmpeq_epi32(ve, _mm_set1_epi32(i + j)), mask));\n        }\n\n        for (int j = 0; j < 16; ++j) {\n            int val = i + j;\n            int cnt = _mm_extract_epi32(count[j], 0);\n            if (val > 0 && cnt >= val && val > max_val) {\n                max_val = val;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 16; i < n; ++i) {\n        if (lst[i] > 0 && lst[i] <= n / 16 && lst[i] > max_val) {\n            max_val = lst[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(const std::vector<int>& lst) {\n    const int n = lst.size();\n    if (n < 1024) {\n        // Fallback to scalar implementation for small vectors\n        std::map<int, int> freq;\n        for (int num : lst) {\n            ++freq[num];\n        }\n        for (auto it = freq.rbegin(); it != freq.rend(); ++it) {\n            if (it->first <= it->second) {\n                return it->first;\n            }\n        }\n        return -1;\n    }\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i four = _mm_set1_epi32(4);\n\n    std::vector<int> count(1024, 0);\n    for (size_t i = 0; i < n; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i mask = _mm_and_si128(v, _mm_cmpgt_epi32(v, zero));\n        __m128i idx = _mm_and_si128(v, mask);\n        __m128i cnt = _mm_add_epi32(_mm_loadu_si128(reinterpret_cast<const __m128i*>(&count[idx])), one);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&count[idx]), cnt);\n    }\n\n    for (int i = 0; i < 1024; ++i) {\n        if (count[i] >= i && i > 0) {\n            return i;\n        }\n    }\n    return -1;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    int size = q.size();\n    if (size == 0) return false;\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i max_weight_vec = _mm_set1_epi32(w);\n    __m128i balance_vec = _mm_setzero_si128();\n\n    int i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n        balance_vec = _mm_sub_epi32(balance_vec, vec);\n    }\n\n    int remaining_elements = size - i;\n    for (; i < size; ++i) {\n        sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(q[i]));\n        balance_vec = _mm_sub_epi32(balance_vec, _mm_set1_epi32(q[i]));\n    }\n\n    int sum[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(sum), sum_vec);\n\n    int balance[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(balance), balance_vec);\n\n    int total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n    int total_balance = balance[0] + balance[1] + balance[2] + balance[3];\n\n    bool is_balanced = (total_balance == 0);\n    bool is_within_weight = (total_sum <= w);\n\n    return is_balanced && is_within_weight;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i max_weight = _mm_set1_epi32(w);\n    __m128i balance = _mm_setzero_si128();\n\n    int size = q.size();\n    int half_size = (size + 1) / 2;\n    int remaining = size % 2;\n\n    for (int i = 0; i < half_size; ++i) {\n        __m128i val = _mm_set1_epi32(q[i]);\n        sum = _mm_add_epi32(sum, val);\n        balance = _mm_sub_epi32(balance, val);\n\n        if (i + remaining < half_size) {\n            val = _mm_set1_epi32(q[size - 1 - i]);\n            sum = _mm_add_epi32(sum, val);\n            balance = _mm_add_epi32(balance, val);\n        }\n    }\n\n    int total_sum = _mm_cvtsi128_si32(_mm_hadd_epi32(sum, sum));\n    int total_balance = _mm_cvtsi128_si32(_mm_hadd_epi32(balance, balance));\n\n    return total_sum <= w && total_balance == 0;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int size = q.size();\n    if (size == 0) return false;\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i max_weight_vec = _mm_set1_epi32(w);\n    __m128i zero_vec = _mm_setzero_si128();\n\n    bool is_balanced = true;\n    int i = 0;\n\n    while (i + 4 <= size) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n\n        __m128i reversed_vec = _mm_shuffle_epi32(vec, _MM_SHUFFLE(3, 2, 1, 0));\n        if (!_mm_cmpeq_epi32(vec, reversed_vec).mmask16) {\n            is_balanced = false;\n        }\n\n        i += 4;\n    }\n\n    int remaining_elements = size - i;\n    for (; i < size; ++i) {\n        sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(q[i]));\n        if (q[i] != q[size - 1 - i]) {\n            is_balanced = false;\n        }\n    }\n\n    int sum = _mm_extract_epi32(sum_vec, 0) +\n              _mm_extract_epi32(sum_vec, 1) +\n              _mm_extract_epi32(sum_vec, 2) +\n              _mm_extract_epi32(sum_vec, 3);\n\n    return is_balanced && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int size = q.size();\n    if (size == 0) return false;\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i max_weight_vec = _mm_set1_epi32(w);\n    __m128i balance_vec = _mm_setzero_si128();\n\n    int i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n        balance_vec = _mm_sub_epi32(balance_vec, vec);\n    }\n\n    int remaining = size - i;\n    for (; i < size; ++i) {\n        sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(q[i]));\n        balance_vec = _mm_sub_epi32(balance_vec, _mm_set1_epi32(q[i]));\n    }\n\n    int sum[4];\n    _mm_storeu_si128((__m128i*)sum, sum_vec);\n\n    int balance[4];\n    _mm_storeu_si128((__m128i*)balance, balance_vec);\n\n    bool is_balanced = true;\n    for (int j = 0; j < remaining / 2; ++j) {\n        if (balance[j] != balance[remaining - 1 - j]) {\n            is_balanced = false;\n            break;\n        }\n    }\n\n    int total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    return is_balanced && total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int size = q.size();\n    if (size == 0) return false;\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i max_weight_vec = _mm_set1_epi32(w);\n    __m128i zero_vec = _mm_setzero_si128();\n\n    bool is_balanced = true;\n    int i = 0;\n\n    while (i + 4 <= size) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n\n        __m128i reversed_vec = _mm_shuffle_epi32(vec, _MM_SHUFFLE(3, 2, 1, 0));\n        if (!_mm_cmpeq_epi32(vec, reversed_vec).mmask16) {\n            is_balanced = false;\n        }\n\n        i += 4;\n    }\n\n    int remaining_elements = size - i;\n    for (; i < size; ++i) {\n        sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(q[i]));\n        if (q[i] != q[size - 1 - i]) {\n            is_balanced = false;\n        }\n    }\n\n    int sum = _mm_extract_epi32(sum_vec, 0) +\n              _mm_extract_epi32(sum_vec, 1) +\n              _mm_extract_epi32(sum_vec, 2) +\n              _mm_extract_epi32(sum_vec, 3);\n\n    return is_balanced && sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    __m12i v1, v2;\n    for (int i = 0; i < n / 2; ++i) {\n        v1 = _mm_loadu_si128((__m12i*)&arr[i * 4]);\n        v2 = _mm_loadu_si128((__m12i*)&arr[(n - i - 1) * 4]);\n        __m12i cmp = _mm_cmpeq_epi32(v1, v2);\n        int mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            changes += __builtin_popcount(~mask) / 4;\n        }\n    }\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    __m12i v1, v2;\n    int i = 0, j = n - 1;\n\n    while (i < j) {\n        v1 = _mm_loadu_si128((__m12i*)&arr[i]);\n        v2 = _mm_loadu_si128((__m12i*)&arr[j]);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(v1, v2)) != 0xFFFF) {\n            changes++;\n        }\n\n        i += 4;\n        j -= 4;\n    }\n\n    // Handle remaining elements\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    int changes = 0;\n    __m12i v1, v2;\n    for (int i = 0; i < n / 8; ++i) {\n        v1 = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&arr[i * 8]));\n        v2 = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&arr[n - 1 - i * 8]));\n        v1 = _mm_min_epi32(v1, v2);\n        v2 = _mm_max_epi32(v1, v2);\n        changes += _mm_movemask_epi8(_mm_cmpeq_epi32(v1, v2));\n    }\n    for (int i = n / 8 * 8; i < n; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            ++changes;\n        }\n    }\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n\n    __m12i v1, v2;\n    int i = 0, j = n - 1;\n\n    while (i < j) {\n        v1 = _mm_loadu_si128((__m12i*)&arr[i]);\n        v2 = _mm_loadu_si128((__m12i*)&arr[j]);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(v1, v2)) != 0xFFFF) {\n            changes++;\n        }\n\n        i += 4;\n        j -= 4;\n    }\n\n    // Handle remaining elements if n is odd\n    while (i < j) {\n        if (arr[i] != arr[j]) {\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return 0;\n\n    int changes = 0;\n    __m12i v1, v2;\n    int i = 0, j = n - 1;\n\n    while (i < j) {\n        v1 = _mm_loadu_si128((__m12i*)&arr[i]);\n        v2 = _mm_loadu_si128((__m12i*)&arr[j]);\n\n        __m12i cmp = _mm_cmpeq_epi32(v1, v2);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) {\n            changes += __builtin_popcount(~mask) / 4;\n        }\n\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        int len1 = lst1[i].length();\n        int len2 = lst2[i].length();\n        if (len1 > 127 || len2 > 127) {\n            // Handle strings longer than 127 characters\n            continue;\n        }\n        __m128i vlen1 = _mm_set1_epi8(len1);\n        __m128i vlen2 = _mm_set1_epi8(len2);\n        sum1 = _mm_add_epi8(sum1, vlen1);\n        sum2 = _mm_add_epi8(sum2, vlen2);\n    }\n\n    int ssum1 = _mm_extract_epi32(sum1, 0) + _mm_extract_epi32(sum1, 1);\n    int ssum2 = _mm_extract_epi32(sum2, 0) + _mm_extract_epi32(sum2, 1);\n\n    if (ssum1 <= ssum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        int len1 = lst1[i].length();\n        int len2 = lst2[i].length();\n        if (len1 > 127 || len2 > 127) {\n            // Handle strings longer than 127 characters\n            continue;\n        }\n        __m128i vlen1 = _mm_set1_epi8(len1);\n        __m128i vlen2 = _mm_set1_epi8(len2);\n        sum1 = _mm_add_epi8(sum1, vlen1);\n        sum2 = _mm_add_epi8(sum2, vlen2);\n    }\n\n    int ssum1 = _mm_extract_epi8(sum1, 0) + _mm_extract_epi8(sum1, 1) +\n                _mm_extract_epi8(sum1, 2) + _mm_extract_epi8(sum1, 3) +\n                _mm_extract_epi8(sum1, 4) + _mm_extract_epi8(sum1, 5) +\n                _mm_extract_epi8(sum1, 6) + _mm_extract_epi8(sum1, 7);\n\n    int ssum2 = _mm_extract_epi8(sum2, 0) + _mm_extract_epi8(sum2, 1) +\n                _mm_extract_epi8(sum2, 2) + _mm_extract_epi8(sum2, 3) +\n                _mm_extract_epi8(sum2, 4) + _mm_extract_epi8(sum2, 5) +\n                _mm_extract_epi8(sum2, 6) + _mm_extract_epi8(sum2, 7);\n\n    if (ssum1 <= ssum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        __m128i str1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(lst1[i].c_str()));\n        __m128i str2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(lst2[i].c_str()));\n\n        int len1 = lst1[i].length();\n        int len2 = lst2[i].length();\n\n        if (len1 >= 16) {\n            sum1 = _mm_add_epi8(sum1, _mm_set1_epi8(len1));\n        }\n        if (len2 >= 16) {\n            sum2 = _mm_add_epi8(sum2, _mm_set1_epi8(len2));\n        }\n\n        for (int j = 0; j < len1 % 16; ++j) {\n            sum1 = _mm_add_epi8(sum1, _mm_set1_epi8(lst1[i][j]));\n        }\n        for (int j = 0; j < len2 % 16; ++j) {\n            sum2 = _mm_add_epi8(sum2, _mm_set1_epi8(lst2[i][j]));\n        }\n    }\n\n    int total1 = _mm_extract_epi32(sum1, 0);\n    int total2 = _mm_extract_epi32(sum2, 0);\n\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        __m128i str1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(lst1[i].c_str()));\n        __m128i str2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(lst2[i].c_str()));\n\n        __m128i len1 = _mm_and_si128(str1, _mm_cmpeq_epi8(str1, _mm_set1_epi8(0)));\n        __m128i len2 = _mm_and_si128(str2, _mm_cmpeq_epi8(str2, _mm_set1_epi8(0)));\n\n        sum1 = _mm_add_epi8(sum1, len1);\n        sum2 = _mm_add_epi8(sum2, len2);\n    }\n\n    int total_chars1 = _mm_extract_epi32(sum1, 0) + _mm_extract_epi32(sum1, 1);\n    int total_chars2 = _mm_extract_epi32(sum2, 0) + _mm_extract_epi32(sum2, 1);\n\n    if (total_chars1 <= total_chars2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        __m128i str1 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(lst1[i].c_str()));\n        __m128i str2 = _mm_loadu_si128(reinterpret_cast<const __m128i*>(lst2[i].c_str()));\n\n        __m128i len1 = _mm_and_si128(str1, _mm_cmpeq_epi8(str1, _mm_set1_epi8('\\0')));\n        __m128i len2 = _mm_and_si128(str2, _mm_cmpeq_epi8(str2, _mm_set1_epi8('\\0')));\n\n        sum1 = _mm_add_epi32(sum1, len1);\n        sum2 = _mm_add_epi32(sum2, len2);\n    }\n\n    int total_chars1 = _mm_extract_epi32(sum1, 0) + _mm_extract_epi32(sum1, 1);\n    int total_chars2 = _mm_extract_epi32(sum2, 0) + _mm_extract_epi32(sum2, 1);\n\n    if (total_chars1 < total_chars2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m128i v_x = _mm_set_epi64x(x, 0);\n    __m128i v_n = _mm_set_epi64x(n, 0);\n    __m128i v_one = _mm_set_epi64x(1, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(v_x, v_n)) == 0) {\n        v_x = _mm_mul_epi64(v_x, v_n);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(v_x, v_one)) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m128i v_x = _mm_set_epi64x(x, 0);\n    __m128i v_n = _mm_set_epi64x(n, 0);\n\n    for (int i = 0; i < 8; ++i) {\n        __m128i v_result = _mm_mul_epu64(v_x, v_n);\n        v_x = v_result;\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(v_x, v_result)) == 0xFF) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m128i v_x = _mm_set_epi64x(x, 0);\n    __m128i v_n = _mm_set_epi64x(n, 0);\n\n    for (int i = 0; i < 8; ++i) {\n        __m128i v_result = _mm_mul_epu64(v_x, v_n);\n        v_x = v_result;\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(v_x, v_result)) == 0xFF) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m128i v_x = _mm_set_epi64x(x, 0);\n    __m128i v_n = _mm_set_epi64x(n, 0);\n\n    for (int i = 0; i < 8; ++i) {\n        __m128i v_result = _mm_mul_epu64(v_x, v_n);\n        v_x = v_result;\n        if (_mm_movemask_epi8(_mm_cmpeq_epi64(v_x, v_result)) == 0xFF) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    __m128i v_x = _mm_set_epi64x(x, 0);\n    __m128i v_n = _mm_set_epi64x(n, 0);\n    __m128i v_one = _mm_set_epi64x(1, 0);\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi64(v_x, v_one)) != 0) {\n        v_x = _mm_mul_epi64(v_x, v_n);\n    }\n\n    return _mm_cmpeq_epi64(v_x, v_one)[0];\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m128i mask = _mm_set1_epi8(0x0F);\n    __m128i primes_mask = _mm_set1_epi8(0x03); // 0b0011\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        data = _mm_and_si128(data, mask);\n\n        __m128i eq2 = _mm_cmpeq_epi8(data, _mm_set1_epi8('2'));\n        __m128i eq3 = _mm_cmpeq_epi8(data, _mm_set1_epi8('3'));\n        __m128i eq5 = _mm_cmpeq_epi8(data, _mm_set1_epi8('5'));\n        __m128i eq7 = _mm_cmpeq_epi8(data, _mm_set1_epi8('7'));\n        __m128i eqB = _mm_cmpeq_epi8(data, _mm_set1_epi8('B'));\n        __m128i eqD = _mm_cmpeq_epi8(data, _mm_set1_epi8('D'));\n\n        __m128i sum = _mm_add_epi8(eq2, eq3);\n        sum = _mm_add_epi8(sum, eq5);\n        sum = _mm_add_epi8(sum, eq7);\n        sum = _mm_add_epi8(sum, eqB);\n        sum = _mm_add_epi8(sum, eqD);\n\n        __m128i result = _mm_and_si128(sum, primes_mask);\n        int temp[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), result);\n        for (int j = 0; j < 16; ++j) {\n            if (temp[j] != 0) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m128i mask = _mm_set1_epi8(0x0F); // Mask to extract lower 4 bits\n    __m128i primes_mask = _mm_set1_epi8(0x0A); // Mask for prime digits (2, 3, 5, 7)\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        data = _mm_and_si128(data, mask); // Extract lower 4 bits\n\n        __m128i eq_mask = _mm_cmpeq_epi8(data, primes_mask);\n        int result = _mm_movemask_epi8(eq_mask);\n\n        while (result) {\n            if (result & 1) {\n                count++;\n            }\n            result >>= 1;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m128i mask = _mm_set1_epi8(0x0F); // Mask to extract lower 4 bits\n    __m128i primes_mask = _mm_set1_epi8(0x0A); // Mask for prime digits (2, 3, 5, 7)\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        data = _mm_and_si128(data, mask); // Extract lower 4 bits\n\n        __m128i eq_mask = _mm_cmpeq_epi8(data, primes_mask);\n        int result = _mm_movemask_epi8(eq_mask);\n\n        while (result) {\n            if (result & 1)\n                count++;\n            result >>= 1;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m128i mask = _mm_set1_epi8(0xF);\n    __m128i primes_mask = _mm_set1_epi8((1 << '2') | (1 << '3') | (1 << '5') | (1 << '7') | (1 << 'B') | (1 << 'D'));\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        data = _mm_and_si128(data, mask);\n        __m128i result = _mm_and_si128(data, primes_mask);\n        int temp[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), result);\n        for (int j = 0; j < 16 && i + j < num.size(); ++j) {\n            if (temp[j] != 0) {\n                ++count;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    __m128i mask = _mm_set1_epi8(0xF);\n    __m128i primes = _mm_set1_epi8(0x2357BD);\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        data = _mm_and_si128(data, mask);\n        __m128i result = _mm_cmpeq_epi8(_mm_shuffle_epi8(primes, data), data);\n        int temp = _mm_movemask_epi8(result);\n        count += __builtin_popcount(temp);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i v0 = _mm_setzero_si128();\n    __m128i v1 = _mm_setzero_si128();\n    __m128i v2 = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= s.length() - 3; ++i) {\n        __m128i v3 = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i v4 = _mm_and_si128(v3, _mm_set1_epi8(0xFF));\n        __m128i v5 = _mm_cmpeq_epi8(v4, v0);\n        __m128i v6 = _mm_cmpeq_epi8(v4, v1);\n        __m128i v7 = _mm_cmpeq_epi8(v4, v2);\n\n        if (_mm_movemask_epi8(_mm_or_si128(_mm_or_si128(v5, v6), v7)) != 0) {\n            return false;\n        }\n\n        v0 = v1;\n        v1 = v2;\n        v2 = v4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i v0 = _mm_setzero_si128();\n    __m128i v1 = _mm_setzero_si128();\n    __m128i v2 = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= s.length() - 3; ++i) {\n        __m128i c0 = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i c1 = _mm_loadu_si128((__m128i*)&s[i + 1]);\n        __m128i c2 = _mm_loadu_si128((__m128i*)&s[i + 2]);\n\n        __m128i eq0 = _mm_cmpeq_epi8(c0, c1);\n        __m128i eq1 = _mm_cmpeq_epi8(c1, c2);\n        __m128i eq2 = _mm_cmpeq_epi8(c0, c2);\n\n        __m128i mask = _mm_or_si128(eq0, eq1);\n        mask = _mm_or_si128(mask, eq2);\n\n        if (_mm_movemask_epi8(mask) != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i v0 = _mm_setzero_si128();\n    __m128i v1 = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 2 < s.length(); ++i) {\n        __m128i v2 = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i v3 = _mm_and_si128(v2, _mm_set1_epi8(0xFF));\n        __m128i v4 = _mm_cmpeq_epi8(v3, v0);\n        __m128i v5 = _mm_cmpeq_epi8(v3, v1);\n        __m128i v6 = _mm_or_si128(v4, v5);\n        if (_mm_movemask_epi8(v6)) return false;\n        v0 = v1;\n        v1 = v3;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i v0 = _mm_setzero_si128();\n    __m128i v1 = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 2 < s.length(); ++i) {\n        __m128i v2 = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i v3 = _mm_and_si128(v2, _mm_set1_epi8(0xFF));\n        __m128i v4 = _mm_cmpeq_epi8(v3, v0);\n        __m128i v5 = _mm_cmpeq_epi8(v3, v1);\n        __m128i v6 = _mm_or_si128(v4, v5);\n        if (_mm_movemask_epi8(v6)) return false;\n        v0 = v1;\n        v1 = v3;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s) {\n    if (s.length() < 3) return false;\n\n    __m128i v0 = _mm_setzero_si128();\n    __m128i v1 = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 2 < s.length(); ++i) {\n        __m128i v2 = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i v3 = _mm_and_si128(v2, _mm_set1_epi8(0xFF));\n        __m128i v4 = _mm_cmpeq_epi8(v3, v0);\n        __m128i v5 = _mm_cmpeq_epi8(v3, v1);\n\n        if (_mm_movemask_epi8(_mm_or_si128(v4, v5)) != 0) {\n            return false;\n        }\n\n        v0 = v1;\n        v1 = v3;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    __m128i v1 = _mm_set1_epi32(0);\n    __m128i v2 = _mm_set1_epi32(length);\n\n    for (int i = 2; i <= length / 2; ++i) {\n        __m128i v3 = _mm_set1_epi32(i);\n        __m128i v4 = _mm_div_epi32(v2, v3);\n        __m128i v5 = _mm_mul_epi32(v3, v4);\n        __m128i v6 = _mm_cmpeq_epi32(v2, v5);\n        v1 = _mm_or_si128(v1, v6);\n    }\n\n    return _mm_movemask_epi8(v1) == 0;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n#include <bitset>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length <= 1) return false;\n    if (length == 2 || length == 3) return true;\n\n    __m128i v1 = _mm_set1_epi32(2);\n    __m128i v2 = _mm_set1_epi32(length - 1);\n\n    while (_mm_movemask_epi8(_mm_cmplt_epi32(v1, v2)) != 0) {\n        __m128i remainder = _mm_rem_epi32(v1, _mm_set1_epi32(length));\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(remainder, _mm_setzero_si128())) != 0) {\n            return false;\n        }\n        v1 = _mm_add_epi32(v1, _mm_set1_epi32(1));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nbool prime_length_simd(const std::string & str) {\n    int length = str.length();\n    if (length <= 1) return false;\n    if (length <= 3) return true;\n\n    if (length % 2 == 0 || length % 3 == 0) return false;\n\n    __m128i v1 = _mm_set1_epi32(5);\n    __m128i v2 = _mm_set1_epi32(7);\n\n    while (v1.m128i_i32[0] * v1.m128i_i32[0] <= length) {\n        if (length % v1.m128i_i32[0] == 0 || length % v1.m128i_i32[1] == 0) return false;\n        v1 = _mm_add_epi32(v1, _mm_set1_epi32(6));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    __m128i v0 = _mm_setzero_si128();\n    __m128i v1 = _mm_set1_epi32(2);\n    __m128i v2 = _mm_set1_epi32(length);\n\n    while (_mm_movemask_epi8(_mm_cmplt_epi32(v1, v2)) != 0) {\n        __m128i remainder = _mm_rem_epi32(v2, v1);\n        v1 = _mm_add_epi32(v1, _mm_set1_epi32(1));\n        v0 = _mm_or_si128(v0, _mm_and_si128(remainder, _mm_cmpeq_epi32(remainder, _mm_setzero_si128())));\n    }\n\n    return _mm_movemask_epi8(_mm_cmpeq_epi32(v0, _mm_setzero_si128())) == 0;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\nbool prime_length_simd(const std::string & str){\n    __m128i primes = _mm_set_epi32(0, 2, 3, 5);\n    int len = str.length();\n    __m128i len_vec = _mm_set1_epi32(len);\n\n    __m128i mask = _mm_cmpeq_epi32(primes, len_vec);\n    int result = _mm_movemask_epi8(mask);\n\n    return (result & 0xF) != 0;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    while (N > 0) {\n        __m128i num = _mm_set1_epi64x(N);\n        __m128i mask = _mm_set1_epi64x(0xF);\n        __m128i digits = _mm_and_si128(num, mask);\n        sum = _mm_add_epi64(sum, digits);\n        N >>= 4;\n    }\n    uint64_t result = 0;\n    _mm_storeu_si128((__m128i*)&result, sum);\n    std::stringstream ss;\n    ss << std::bitset<64>(result).to_string();\n    std::string binary = ss.str();\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    \n    while (N > 0) {\n        __m128i num = _mm_set1_epi32(static_cast<int>(N & 0xFFFFFFFF));\n        __m128i mask = _mm_cmpeq_epi32(num, _mm_setzero_si128());\n        __m128i count = _mm_andnot_si128(mask, _mm_set1_epi32(1));\n        sum = _mm_add_epi32(sum, count);\n        \n        N >>= 32;\n    }\n    \n    int result = 0;\n    _mm_storeu_si128((__m128i*)&result, sum);\n    \n    std::ostringstream oss;\n    oss << std::bitset<32>(result);\n    std::string binary = oss.str();\n    \n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    \n    while (N > 0) {\n        __m128i num = _mm_set1_epi64x(N);\n        __m128i mask = _mm_set1_epi64x(0xF);\n        __m128i digits = _mm_and_si128(num, mask);\n        \n        __m128i shifted = _mm_slli_epi64(digits, 3);\n        sum = _mm_add_epi64(sum, shifted);\n        \n        N >>= 4;\n    }\n    \n    uint64_t result = 0;\n    _mm_storeu_si128((__m128i*)&result, sum);\n    \n    std::stringstream ss;\n    ss << std::bitset<64>(result);\n    std::string binary = ss.str();\n    \n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    \n    while (N > 0) {\n        __m128i num = _mm_set1_epi64x(N);\n        __m128i mask = _mm_cmpeq_epi8(_mm_and_si128(num, _mm_set1_epi64x(0xF)), _mm_setzero_si128());\n        __m128i count = _mm_popcnt_epi32(_mm_cvtepi8_epi32(mask));\n        sum = _mm_add_epi64(sum, count);\n        \n        N >>= 4;\n    }\n    \n    uint64_t result = _mm_extract_epi64(sum, 0);\n    std::stringstream ss;\n    ss << std::bitset<64>(result).to_string();\n    std::string binary = ss.str();\n    binary.erase(0, binary.find_first_not_of('0'));\n    if (binary.empty()) binary = \"0\";\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <sstream>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    __m128i sum = _mm_setzero_si128();\n    \n    while (N > 0) {\n        __m128i num = _mm_set1_epi64x(N);\n        __m128i mask = _mm_set1_epi64x(0xF);\n        __m128i digits = _mm_and_si128(num, mask);\n        \n        __m128i shifted = _mm_slli_epi64(digits, 3);\n        sum = _mm_add_epi64(sum, shifted);\n        \n        N >>= 4;\n    }\n    \n    uint64_t result = 0;\n    _mm_storeu_si128((__m128i*)&result, sum);\n    \n    std::ostringstream oss;\n    oss << std::bitset<64>(result);\n    std::string binary = oss.str();\n    \n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n    \n    return binary;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    int n = lst.size();\n\n    for(int i = 1; i < n; i += 2){\n        if(i + 3 < n){\n            __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n            __m128i mask = _mm_and_si128(vec, _mm_set1_epi32(0x55555555));\n            vsum = _mm_add_epi32(vsum, mask);\n        } else {\n            for(; i < n; i += 2){\n                if(lst[i] % 2 == 0){\n                    sum += lst[i];\n                }\n            }\n        }\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    for(int i = 0; i < 4; i++){\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = zero;\n\n    int n = lst.size();\n    for (int i = 1; i < n; i += 2) {\n        if (lst[i] % 2 == 0) {\n            __m128i value = _mm_set1_epi32(lst[i]);\n            result = _mm_add_epi32(result, value);\n        }\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, result);\n\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = zero;\n\n    for (size_t i = 1; i < lst.size(); i += 2) {\n        if (lst[i] % 2 == 0) {\n            __m128i value = _mm_set1_epi32(lst[i]);\n            result = _mm_add_epi32(result, value);\n        }\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, result);\n\n    for (int j = 0; j < 4; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = zero;\n\n    for (size_t i = 1; i < lst.size(); i += 2) {\n        if (lst[i] % 2 == 0) {\n            __m128i value = _mm_set1_epi32(lst[i]);\n            result = _mm_add_epi32(result, value);\n        }\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, result);\n\n    for (int j = 0; j < 4; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    int n = lst.size();\n\n    for(int i = 1; i < n; i += 2){\n        if(i + 3 < n && lst[i] % 2 == 0){\n            __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n            __m128i mask = _mm_cmpeq_epi32(_mm_and_si128(vec, _mm_set1_epi32(1)), _mm_setzero_si128());\n            vec = _mm_and_si128(vec, mask);\n            vsum = _mm_add_epi32(vsum, vec);\n        } else {\n            if(lst[i] % 2 == 0){\n                sum += lst[i];\n            }\n        }\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    for(int i = 0; i < 4; ++i){\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int block_size = 16;\n    std::string result(s);\n    __m128i key = _mm_set1_epi8(32); // Shift by 2 * 2 = 4\n\n    for (size_t i = 0; i <= s.size() - block_size; i += block_size) {\n        __m128i input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&result[i]));\n        __m128i shifted = _mm_add_epi8(input, key);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), shifted);\n    }\n\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % block_size; i < s.size(); ++i) {\n        result[i] = static_cast<char>(static_cast<unsigned char>(result[i]) + 4);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 16;\n    std::string result(s.size() + chunk_size - 1, '\\0');\n    __m128i *input_ptr = (__m128i *)s.c_str();\n    __m128i *output_ptr = (__m128i *)result.c_str();\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m128i input = _mm_loadu_si128(input_ptr++);\n        __m128i shifted = _mm_add_epi8(input, _mm_set1_epi8(4));\n        _mm_storeu_si128(output_ptr++, shifted);\n    }\n\n    return result.substr(0, s.size());\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 16;\n    std::string result(s);\n    __m128i *input_ptr = (__m128i *)s.c_str();\n    __m128i *output_ptr = (__m128i *)result.data();\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m128i input = _mm_loadu_si128(input_ptr + i / chunk_size);\n        __m128i shifted = _mm_add_epi8(input, _mm_set1_epi8(4));\n        output_ptr[i / chunk_size] = shifted;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 16;\n    std::string result(s.size(), '\\0');\n    \n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m128i input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i output = _mm_add_epi8(input, _mm_set1_epi8(4));\n        \n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), output);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    const int chunk_size = 16;\n    std::string result(s);\n    __m128i *input_ptr = (__m128i *)s.c_str();\n    __m128i *output_ptr = (__m128i *)result.data();\n\n    for (size_t i = 0; i < s.size(); i += chunk_size) {\n        __m128i input = _mm_loadu_si128(input_ptr + i / chunk_size);\n        __m128i shifted = _mm_add_epi8(input, _mm_set1_epi8(4));\n        output_ptr[i / chunk_size] = shifted;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char * msg = message.c_str();\n    size_t len = message.length();\n    std::string result(len, '\\0');\n\n    __m128i vowels = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m128i shift = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i *>(msg + i));\n\n        // Swap case\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_set1_epi8(32));\n\n        // Replace vowels\n        __m128i mask = _mm_cmpeq_epi8(swapped_case, vowels);\n        __m128i shifted_vowels = _mm_adds_epu8(vowels, shift);\n        __m128i replaced_vowels = _mm_or_si128(_mm_andnot_si128(mask, swapped_case), _mm_and_si128(mask, shifted_vowels));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i *>(&result[i]), replaced_vowels);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 16; i < len; ++i) {\n        char c = msg[i];\n        if (c >= 'a' && c <= 'z') {\n            c = c ^ 32;\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n                c = (c - 'a' + 2) % 26 + 'a';\n            }\n        } else if (c >= 'A' && c <= 'Z') {\n            c = c ^ 32;\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c = (c - 'A' + 2) % 26 + 'A';\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char *msg = message.c_str();\n    size_t len = message.length();\n    std::string result(len, '\\0');\n\n    __m128i vowels = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m128i shift = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i *>(msg + i));\n\n        // Swap case\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_set1_epi8(32));\n\n        // Replace vowels with the letter 2 places ahead\n        __m128i mask = _mm_cmpeq_epi8(swapped_case, vowels);\n        __m128i shifted_vowels = _mm_adds_epu8(vowels, shift);\n        __m128i replaced_vowels = _mm_or_si128(_mm_and_si128(mask, shifted_vowels), _mm_andnot_si128(mask, swapped_case));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i *>(&result[i]), replaced_vowels);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 16; i < len; ++i) {\n        char c = msg[i];\n        if (c >= 'a' && c <= 'z') {\n            c = c ^ 32;\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n                c = (c - 'a' + 2) % 26 + 'a';\n            }\n        } else if (c >= 'A' && c <= 'Z') {\n            c = c ^ 32;\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c = (c - 'A' + 2) % 26 + 'A';\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char *input = message.c_str();\n    size_t length = message.length();\n    std::string result(length, '\\0');\n\n    __m128i vowels = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m128i shift = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i <= length - 16; i += 16) {\n        __m128i input_vec = _mm_loadu_si128((__m128i *)(input + i));\n        __m128i swapped_case = _mm_xor_si128(input_vec, _mm_set1_epi8(32));\n\n        __m128i mask_vowels = _mm_cmpeq_epi8(swapped_case, vowels);\n        __m128i shifted_vowels = _mm_adds_epu8(swapped_case, shift);\n        __m128i final_result = _mm_or_si128(_mm_andnot_si128(mask_vowels, swapped_case), _mm_and_si128(mask_vowels, shifted_vowels));\n\n        _mm_storeu_si128((__m128i *)(result.data() + i), final_result);\n    }\n\n    // Handle remaining characters\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if (isalpha(message[i])) {\n            char c = message[i];\n            if (islower(c)) {\n                c = toupper(c);\n            } else {\n                c = tolower(c);\n            }\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c = ((c - 'a' + 2) % 26) + 'a';\n            }\n            result[i] = c;\n        } else {\n            result[i] = message[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char *msg = message.c_str();\n    size_t len = message.length();\n    std::string result(len, '\\0');\n\n    __m128i vowels = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m128i shift = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(msg + i));\n\n        // Swap case\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_set1_epi8(32));\n\n        // Replace vowels\n        __m128i mask = _mm_cmpeq_epi8(swapped_case, vowels);\n        __m128i shifted_vowels = _mm_adds_epu8(vowels, shift);\n        __m128i replaced_vowels = _mm_or_si128(_mm_and_si128(mask, shifted_vowels), _mm_andnot_si128(mask, swapped_case));\n\n        _mm_storeu_si128((__m128i *)(result.data() + i), replaced_vowels);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 16; i < len; ++i) {\n        char c = msg[i];\n        if (c >= 'a' && c <= 'z') {\n            c = c ^ 32;\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n                c = (c - 'a' + 2) % 26 + 'a';\n            }\n        } else if (c >= 'A' && c <= 'Z') {\n            c = c ^ 32;\n            if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c = (c - 'A' + 2) % 26 + 'A';\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    const char *msg = message.c_str();\n    size_t len = message.length();\n    std::string result(len, '\\0');\n\n    __m128i vowels = _mm_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1);\n    __m128i shift = _mm_set1_epi8(2);\n\n    for (size_t i = 0; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i *)(msg + i));\n\n        // Swap case\n        __m128i swapped_case = _mm_xor_si128(chunk, _mm_set1_epi8(32));\n\n        // Replace vowels\n        __m128i mask = _mm_cmpeq_epi8(swapped_case, vowels);\n        __m128i shifted_vowels = _mm_adds_epu8(vowels, shift);\n        __m128i replaced_vowels = _mm_or_si128(_mm_andnot_si128(mask, swapped_case), _mm_and_si128(mask, shifted_vowels));\n\n        // Store result\n        _mm_storeu_si128((__m128i *)(result.data() + i), replaced_vowels);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 16; i < len; ++i) {\n        char c = msg[i];\n        if (c >= 'a' && c <= 'z') {\n            c -= 32;\n        } else if (c >= 'A' && c <= 'Z') {\n            c += 32;\n        }\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            if (c >= 'a' && c <= 'y') {\n                c += 2;\n            } else if (c >= 'A' && c <= 'Y') {\n                c += 2;\n            } else if (c == 'z') {\n                c = 'b';\n            } else if (c == 'Z') {\n                c = 'B';\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_mask = _mm_set1_epi8(0x5F);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int length = key.size();\n        if (length % 16 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < length; i += 16) {\n            __m128i key_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.c_str() + i));\n\n            __m128i lower_check = _mm_and_si128(key_vec, lower_mask);\n            __m128i upper_check = _mm_and_si128(key_vec, upper_mask);\n\n            __m128i lower_eq = _mm_cmpeq_epi8(lower_check, key_vec);\n            __m128i upper_eq = _mm_cmpeq_epi8(upper_check, key_vec);\n\n            int lower_all = _mm_movemask_epi8(lower_eq);\n            int upper_all = _mm_movemask_epi8(upper_eq);\n\n            if ((lower_all & 0xFFFF) == 0xFFFF || (upper_all & 0xFFFF) == 0xFFFF) {\n                continue;\n            } else {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_mask = _mm_set1_epi8(0x5F);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 16 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 16) {\n            __m128i key_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.c_str() + i));\n\n            __m128i lower_check = _mm_and_si128(key_vec, lower_mask);\n            __m128i upper_check = _mm_and_si128(key_vec, upper_mask);\n\n            __m128i lower_eq = _mm_cmpeq_epi8(lower_check, key_vec);\n            __m128i upper_eq = _mm_cmpeq_epi8(upper_check, key_vec);\n\n            if (!_mm_movemask_epi8(lower_eq)) {\n                return false;\n            }\n            if (!_mm_movemask_epi8(upper_eq)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_mask = _mm_set1_epi8(0x5F);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 16 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 16) {\n            __m128i key_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.c_str() + i));\n\n            __m128i lower_check = _mm_and_si128(key_vec, lower_mask);\n            __m128i upper_check = _mm_and_si128(key_vec, upper_mask);\n\n            __m128i lower_diff = _mm_cmpeq_epi8(lower_check, key_vec);\n            __m128i upper_diff = _mm_cmpeq_epi8(upper_check, key_vec);\n\n            if (!_mm_test_all_ones(lower_diff)) {\n                if (!_mm_test_all_ones(upper_diff)) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m128i lower_mask = _mm_set1_epi8(0xDF); // Mask to convert uppercase to lowercase\n    __m128i upper_mask = _mm_set1_epi8(0x20); // Mask to convert lowercase to uppercase\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int length = key.length();\n        if (length % 16 != 0) {\n            return false; // Length must be a multiple of 16 for SSE processing\n        }\n\n        for (int i = 0; i < length; i += 16) {\n            __m128i key_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.c_str() + i));\n\n            __m128i lower_key_vec = _mm_and_si128(key_vec, lower_mask);\n            __m128i upper_key_vec = _mm_or_si128(lower_key_vec, upper_mask);\n\n            if (!_mm_cmpeq_epi8(key_vec, lower_key_vec).movemask() ||\n                !_mm_cmpeq_epi8(key_vec, upper_key_vec).movemask()) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_mask = _mm_set1_epi8(0x20);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        int len = key.size();\n        if (len % 16 != 0) {\n            return false;\n        }\n\n        for (int i = 0; i < len; i += 16) {\n            __m128i key_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.c_str() + i));\n\n            __m128i lower_check = _mm_and_si128(key_vec, lower_mask);\n            __m128i upper_check = _mm_and_si128(key_vec, upper_mask);\n\n            if (!_mm_test_all_zeros(lower_check, lower_check) && !_mm_test_all_zeros(upper_check, upper_check)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m12i mask = _mm_setzero_si128();\n    __m12i ones = _mm_set1_epi32(1);\n    __m12i two = _mm_set1_epi32(2);\n    __m12i three = _mm_set1_epi32(3);\n    __m12i four = _mm_set1_epi32(4);\n    __m12i five = _mm_set1_epi32(5);\n    __m12i six = _mm_set1_epi32(6);\n    __m12i seven = _mm_set1_epi32(7);\n    __m12i eight = _mm_set1_epi32(8);\n    __m12i nine = _mm_set1_epi32(9);\n    __m12i ten = _mm_set1_epi32(10);\n\n    for (int i = 2; i < n; ++i) {\n        __m12i num = _mm_set1_epi32(i);\n        __m12i mod2 = _mm_and_si128(num, two);\n        __m12i mod3 = _mm_and_si128(num, three);\n        __m12i mod5 = _mm_and_si128(num, five);\n        __m12i mod7 = _mm_and_si128(num, seven);\n\n        __m12i eq2 = _mm_cmpeq_epi32(mod2, zero);\n        __m12i eq3 = _mm_cmpeq_epi32(mod3, zero);\n        __m12i eq5 = _mm_cmpeq_epi32(mod5, zero);\n        __m12i eq7 = _mm_cmpeq_epi32(mod7, zero);\n\n        __m12i or_eq = _mm_or_si128(eq2, eq3);\n        or_eq = _mm_or_si128(or_eq, eq5);\n        or_eq = _mm_or_si128(or_eq, eq7);\n\n        __m12i not_or_eq = _mm_andnot_si128(or_eq, ones);\n\n        mask = _mm_or_si128(mask, not_or_eq);\n    }\n\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (_mm_movemask_epi8(_mm_srai_epi32(mask, i)) & 1) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m12i mask = _mm_setzero_si128();\n    __m12i ones = _mm_set1_epi32(1);\n    __m12i two = _mm_set1_epi32(2);\n    __m12i three = _mm_set1_epi32(3);\n    __m12i four = _mm_set1_epi32(4);\n    __m12i five = _mm_set1_epi32(5);\n    __m12i six = _mm_set1_epi32(6);\n    __m12i seven = _mm_set1_epi32(7);\n    __m12i eight = _mm_set1_epi32(8);\n    __m12i nine = _mm_set1_epi32(9);\n    __m12i ten = _mm_set1_epi32(10);\n    __m12i eleven = _mm_set1_epi32(11);\n    __m12i twelve = _mm_set1_epi32(12);\n    __m12i thirteen = _mm_set1_epi32(13);\n    __m12i fourteen = _mm_set1_epi32(14);\n    __m12i fifteen = _mm_set1_epi32(15);\n    __m12i sixteen = _mm_set1_epi32(16);\n\n    for (int i = 2; i < n; ++i) {\n        __m12i num = _mm_set1_epi32(i);\n        __m12i mod2 = _mm_and_si128(num, two);\n        __m12i mod3 = _mm_and_si128(num, three);\n        __m12i mod5 = _mm_and_si128(num, five);\n        __m12i mod7 = _mm_and_si128(num, seven);\n\n        __m12i eq2 = _mm_cmpeq_epi32(mod2, zero);\n        __m12i eq3 = _mm_cmpeq_epi32(mod3, zero);\n        __m12i eq5 = _mm_cmpeq_epi32(mod5, zero);\n        __m12i eq7 = _mm_cmpeq_epi32(mod7, zero);\n\n        __m12i or_eq = _mm_or_si128(eq2, eq3);\n        or_eq = _mm_or_si128(or_eq, eq5);\n        or_eq = _mm_or_si128(or_eq, eq7);\n\n        __m12i not_or_eq = _mm_andnot_si128(or_eq, ones);\n        mask = _mm_or_si128(mask, not_or_eq);\n    }\n\n    int count = 0;\n    for (int i = 0; i < n; ++i) {\n        if (_mm_extract_epi32(mask, i / 4) & (1 << (i % 4))) {\n            primes.push_back(i + 2);\n            ++count;\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m12i mask = _mm_setzero_si128();\n    __m12i ones = _mm_set1_epi32(1);\n    __m12i two = _mm_set1_epi32(2);\n    __m12i three = _mm_set1_epi32(3);\n    __m12i four = _mm_set1_epi32(4);\n    __m12i five = _mm_set1_epi32(5);\n    __m12i six = _mm_set1_epi32(6);\n    __m12i seven = _mm_set1_epi32(7);\n    __m12i eight = _mm_set1_epi32(8);\n    __m12i nine = _mm_set1_epi32(9);\n    __m12i ten = _mm_set1_epi32(10);\n    __m12i eleven = _mm_set1_epi32(11);\n    __m12i twelve = _mm_set1_epi32(12);\n    __m12i thirteen = _mm_set1_epi32(13);\n    __m12i fourteen = _mm_set1_epi32(14);\n    __m12i fifteen = _mm_set1_epi32(15);\n    __m12i sixteen = _mm_set1_epi32(16);\n\n    for (int i = 2; i < n; ++i) {\n        __m12i num = _mm_set1_epi32(i);\n        __m12i mod2 = _mm_and_si128(num, two);\n        __m12i mod3 = _mm_and_si128(num, three);\n        __m12i mod5 = _mm_and_si128(num, five);\n        __m12i mod7 = _mm_and_si128(num, seven);\n\n        __m12i eq2 = _mm_cmpeq_epi32(mod2, zero);\n        __m12i eq3 = _mm_cmpeq_epi32(mod3, zero);\n        __m12i eq5 = _mm_cmpeq_epi32(mod5, zero);\n        __m12i eq7 = _mm_cmpeq_epi32(mod7, zero);\n\n        __m12i or_eq = _mm_or_si128(eq2, eq3);\n        or_eq = _mm_or_si128(or_eq, eq5);\n        or_eq = _mm_or_si128(or_eq, eq7);\n\n        __m12i not_or_eq = _mm_andnot_si128(or_eq, ones);\n        __m12i result = _mm_and_si128(not_or_eq, num);\n\n        int result_int = _mm_cvtsi128_si32(result);\n        if (result_int != 0) {\n            primes.push_back(result_int);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m12i mask = _mm_setzero_si128();\n    __m12i ones = _mm_set1_epi32(1);\n    __m12i two = _mm_set1_epi32(2);\n    __m12i three = _mm_set1_epi32(3);\n    __m12i four = _mm_set1_epi32(4);\n    __m12i five = _mm_set1_epi32(5);\n    __m12i six = _mm_set1_epi32(6);\n    __m12i seven = _mm_set1_epi32(7);\n    __m12i eight = _mm_set1_epi32(8);\n    __m12i nine = _mm_set1_epi32(9);\n    __m12i ten = _mm_set1_epi32(10);\n    __m12i eleven = _mm_set1_epi32(11);\n    __m12i twelve = _mm_set1_epi32(12);\n    __m12i thirteen = _mm_set1_epi32(13);\n    __m12i fourteen = _mm_set1_epi32(14);\n    __m12i fifteen = _mm_set1_epi32(15);\n    __m12i sixteen = _mm_set1_epi32(16);\n\n    for (int i = 2; i < n; ++i) {\n        __m12i num = _mm_set1_epi32(i);\n        __m12i mod2 = _mm_and_si128(num, two);\n        __m12i mod3 = _mm_and_si128(num, three);\n        __m12i mod5 = _mm_and_si128(num, five);\n        __m12i mod7 = _mm_and_si128(num, seven);\n\n        __m12i eq2 = _mm_cmpeq_epi32(mod2, zero);\n        __m12i eq3 = _mm_cmpeq_epi32(mod3, zero);\n        __m12i eq5 = _mm_cmpeq_epi32(mod5, zero);\n        __m12i eq7 = _mm_cmpeq_epi32(mod7, zero);\n\n        __m12i or_eq = _mm_or_si128(eq2, eq3);\n        or_eq = _mm_or_si128(or_eq, eq5);\n        or_eq = _mm_or_si128(or_eq, eq7);\n\n        if (_mm_movemask_epi8(or_eq) == 0) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nstd::vector<int> count_up_to_simd(int n) {\n    if (n <= 2) return {};\n\n    std::vector<int> primes;\n    primes.reserve(n);\n\n    __m12i mask = _mm_setzero_si128();\n    __m12i ones = _mm_set1_epi32(1);\n    __m12i two = _mm_set1_epi32(2);\n    __m12i three = _mm_set1_epi32(3);\n    __m12i four = _mm_set1_epi32(4);\n    __m12i five = _mm_set1_epi32(5);\n    __m12i six = _mm_set1_epi32(6);\n    __m12i seven = _mm_set1_epi32(7);\n    __m12i eight = _mm_set1_epi32(8);\n    __m12i nine = _mm_set1_epi32(9);\n    __m12i ten = _mm_set1_epi32(10);\n    __m12i eleven = _mm_set1_epi32(11);\n    __m12i twelve = _mm_set1_epi32(12);\n    __m12i thirteen = _mm_set1_epi32(13);\n    __m12i fourteen = _mm_set1_epi32(14);\n    __m12i fifteen = _mm_set1_epi32(15);\n    __m12i sixteen = _mm_set1_epi32(16);\n\n    for (int i = 2; i < n; ++i) {\n        __m12i num = _mm_set1_epi32(i);\n        __m12i mod2 = _mm_and_si128(num, two);\n        __m12i mod3 = _mm_and_si128(num, three);\n        __m12i mod5 = _mm_and_si128(num, five);\n        __m12i mod7 = _mm_and_si128(num, seven);\n\n        __m12i is_prime = _mm_cmpeq_epi32(mod2, mask);\n        is_prime = _mm_and_si128(is_prime, _mm_cmpeq_epi32(mod3, mask));\n        is_prime = _mm_and_si128(is_prime, _mm_cmpeq_epi32(mod5, mask));\n        is_prime = _mm_and_si128(is_prime, _mm_cmpeq_epi32(mod7, mask));\n\n        int result[4];\n        _mm_storeu_si128((__m128i*)result, is_prime);\n\n        if (result[0] && result[1] && result[2] && result[3]) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set_epi8('A', 'E', 'I', 'O', 'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i upper_mask = _mm_and_si128(str_vec, _mm_set1_epi8(0xDF));\n        __m128i result = _mm_cmpeq_epi8(upper_mask, vowels_mask);\n        int mask = _mm_movemask_epi8(result);\n        count += __builtin_popcount(mask & 0x5555); // Count bits in odd positions\n    }\n    for (size_t i = s.size() - s.size() % 4; i < s.size(); ++i) {\n        if (i % 2 == 0 && s[i] >= 'A' && s[i] <= 'Z' && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            ++count;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n    __m128i upper_mask = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i even_indices = _mm_and_si128(str_vec, _mm_set1_epi32(0x000000FF));\n        __m128i vowels = _mm_and_si128(even_indices, vowels_mask);\n        __m128i upper = _mm_and_si128(vowels, upper_mask);\n        __m128i result = _mm_cmpeq_epi8(upper, vowels);\n        int mask = _mm_movemask_epi8(result);\n        count += __builtin_popcount(mask);\n    }\n\n    for (size_t i = s.size() - s.size() % 4; i < s.size(); ++i) {\n        if ((i % 2 == 0) && (s[i] >= 'A' && s[i] <= 'E')) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi8('A' | 'E' | 'I' | 'O' | 'U');\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str_vec, vowels_mask);\n        __m128i upper_mask = _mm_cmpeq_epi8(mask, str_vec);\n        int temp_count = _mm_movemask_epi8(upper_mask);\n        count += __builtin_popcount(temp_count);\n    }\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % 4; i < s.size(); ++i) {\n        if (isupper(s[i]) && (i % 2 == 0)) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi8('A' | 'E' | 'I' | 'O' | 'U');\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str_vec, vowels_mask);\n        __m128i upper_mask = _mm_cmpeq_epi8(mask, str_vec);\n        int result = _mm_movemask_epi8(upper_mask);\n        count += __builtin_popcount(result);\n    }\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % 4; i < s.size(); ++i) {\n        if (isupper(s[i]) && (i % 2 == 0)) {\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    __m128i vowels_mask = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n    __m128i upper_mask = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i <= s.size() - 4; i += 4) {\n        __m128i str_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i mask = _mm_and_si128(str_vec, vowels_mask);\n        __m128i upper = _mm_and_si128(mask, upper_mask);\n        __m128i result = _mm_cmpeq_epi8(upper, str_vec);\n        int temp_count = _mm_movemask_epi8(result);\n        count += __builtin_popcount(temp_count);\n    }\n\n    // Handle remaining characters\n    for (size_t i = s.size() - s.size() % 4; i < s.size(); ++i) {\n        if ((s[i] >= 'A' && s[i] <= 'E') || (s[i] >= 'I' && s[i] <= 'O') || (s[i] >= 'U' && s[i] <= 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i current = _mm_set1_epi32(n);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i mask = _mm_and_si128(_mm_cmpeq_epi32(current, _mm_setzero_si128()), ones);\n        __m128i next_odd = _mm_add_epi32(current, two);\n        __m128i next_even = _mm_add_epi32(current, ones);\n        __m128i next = _mm_or_si128(next_odd, _mm_andnot_si128(mask, next_even));\n        _mm_storeu_si128((__m128i*)&result[i], next);\n        current = _mm_add_epi32(current, _mm_set1_epi32(4));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i current = _mm_set1_epi32(n);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i mask = _mm_and_si128(_mm_cmpeq_epi32(current, _mm_setzero_si128()), ones);\n        __m128i next_odd = _mm_add_epi32(current, two);\n        __m128i next_even = _mm_add_epi32(current, ones);\n        __m128i next = _mm_or_si128(next_odd, _mm_andnot_si128(mask, next_even));\n        _mm_storeu_si128((__m128i*)&result[i], next);\n        current = _mm_add_epi32(next, two);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i current = _mm_set1_epi32(n);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i mask = _mm_and_si128(_mm_cmpeq_epi32(current, _mm_setzero_si128()), ones);\n        __m128i next_odd = _mm_add_epi32(current, two);\n        __m128i next_even = _mm_add_epi32(current, ones);\n        __m128i next = _mm_or_si128(next_odd, _mm_andnot_si128(mask, next_even));\n        _mm_storeu_si128((__m128i*)&result[i], next);\n        current = _mm_add_epi32(next, two);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i current = _mm_set1_epi32(n);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i mask = _mm_and_si128(_mm_cmpeq_epi32(current, _mm_setzero_si128()), ones);\n        __m128i next_odd = _mm_add_epi32(current, two);\n        __m128i next_even = _mm_add_epi32(current, ones);\n        __m128i next = _mm_or_si128(next_odd, _mm_andnot_si128(mask, next_even));\n        _mm_storeu_si128((__m128i*)&result[i], next);\n        current = _mm_add_epi32(next, two);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i four = _mm_set1_epi32(4);\n\n    int i = 0;\n    while (i + 4 <= n) {\n        __m128i indices = _mm_set_epi32(i + 3, i + 2, i + 1, i);\n        __m128i base = _mm_add_epi32(indices, ones);\n        __m128i mask = _mm_and_si128(_mm_cmpeq_epi32(base, _mm_and_si128(base, ones)), ones);\n        __m128i next = _mm_add_epi32(base, two);\n        __m128i result_vec = _mm_or_si128(next, mask);\n        _mm_storeu_si128((__m128i*)&result[i], result_vec);\n        i += 4;\n    }\n\n    while (i < n) {\n        result[i] = i + 1 + (n % 2 == 0 ? 0 : 1);\n        i++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i count = _mm_setzero_si128();\n\n    for (int64_t i = n; i <= m; i += 8) {\n        __m128i values = _mm_set_epi64x(i + 7, i + 6);\n        values = _mm_add_epi64(values, _mm_set_epi64x(i + 5, i + 4));\n        values = _mm_add_epi64(values, _mm_set_epi64x(i + 3, i + 2));\n        values = _mm_add_epi64(values, _mm_set_epi64x(i + 1, i));\n\n        sum = _mm_add_epi64(sum, values);\n        count = _mm_add_epi64(count, _mm_set1_epi64x(8));\n    }\n\n    int64_t remaining = m - (m & ~7);\n    for (; remaining <= m; ++remaining) {\n        sum = _mm_add_epi64(sum, _mm_set_epi64x(remaining, 0));\n        count = _mm_add_epi64(count, _mm_set1_epi64x(1));\n    }\n\n    __m128i avg = _mm_div_epi64(sum, count);\n\n    int64_t result = _mm_cvtsi128_si64(avg);\n    result = std::round(result / 8.0);\n\n    std::stringstream ss;\n    ss << std::bitset<64>(result).to_string();\n    std::string binary = ss.str();\n    binary.erase(0, binary.find_first_not_of('0'));\n\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    int count = 0;\n\n    // Process in chunks of 8 numbers at a time\n    while (n + 7 <= m) {\n        __m128i nums = _mm_set_epi64x(n + 7, n + 6, n + 5, n + 4);\n        nums = _mm_add_epi64(nums, _mm_set_epi64x(n + 3, n + 2, n + 1, n));\n        sum = _mm_add_epi64(sum, nums);\n        n += 8;\n        count += 8;\n    }\n\n    // Process remaining numbers\n    while (n <= m) {\n        sum = _mm_add_epi64(sum, _mm_set_epi64x(0, 0, 0, n));\n        n++;\n        count++;\n    }\n\n    // Calculate average\n    __m128i avg = _mm_srli_epi64(_mm_add_epi64(sum, _mm_set1_epi64x(count / 2)), 3);\n\n    // Convert to integer and round down\n    int result = _mm_cvtsi128_si32(avg);\n\n    // Convert to binary\n    std::stringstream ss;\n    ss << std::bitset<32>(result);\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i count = _mm_setzero_si128();\n\n    for (int64_t i = n; i <= m; i += 8) {\n        __m128i values = _mm_set_epi64x(i + 7, i + 6);\n        values = _mm_add_epi64(values, _mm_set_epi64x(i + 5, i + 4));\n        values = _mm_add_epi64(values, _mm_set_epi64x(i + 3, i + 2));\n        values = _mm_add_epi64(values, _mm_set_epi64x(i + 1, i));\n\n        sum = _mm_add_epi64(sum, values);\n        count = _mm_add_epi64(count, _mm_set1_epi64x(8));\n    }\n\n    int64_t remaining = m - (m & ~7);\n    for (; remaining <= m; ++remaining) {\n        sum = _mm_add_epi64(sum, _mm_set_epi64x(remaining, 0));\n        count = _mm_add_epi64(count, _mm_set1_epi64x(1));\n    }\n\n    __m128i avg = _mm_div_epi64(_mm_add_epi64(sum, _mm_srli_si128(count, 8)), count);\n\n    int64_t avg_value = _mm_cvtsi128_si64(avg);\n    std::stringstream ss;\n    ss << std::bitset<64>(avg_value).to_string();\n    std::string result = ss.str();\n    size_t first_one = result.find('1');\n    if (first_one != std::string::npos) {\n        result = result.substr(first_one);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    int count = 0;\n\n    // Process in chunks of 8 numbers at a time\n    for (; n + 7 <= m; n += 8) {\n        __m128i nums = _mm_set_epi64x(n + 7, n + 6);\n        nums = _mm_add_epi64(nums, _mm_set_epi64x(n + 5, n + 4));\n        nums = _mm_add_epi64(nums, _mm_set_epi64x(n + 3, n + 2));\n        nums = _mm_add_epi64(nums, _mm_set_epi64x(n + 1, n));\n        sum = _mm_add_epi64(sum, nums);\n        count += 8;\n    }\n\n    // Process remaining numbers\n    for (; n <= m; ++n) {\n        sum = _mm_add_epi64(sum, _mm_set_epi64x(0, n));\n        ++count;\n    }\n\n    // Calculate average\n    __m128i avg = _mm_srli_epi64(_mm_add_epi64(sum, _mm_set1_epi64x(count / 2)), 1);\n\n    // Convert to integer and round\n    int result = static_cast<int>(_mm_cvtsi128_si32(avg));\n\n    // Convert to binary\n    std::stringstream ss;\n    ss << std::bitset<32>(result);\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <bitset>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    int count = m - n + 1;\n\n    for (int i = 0; i < count; i += 8) {\n        __m128i values = _mm_set_epi64x(n + i + 7, n + i);\n        sum = _mm_add_epi64(sum, values);\n    }\n\n    int64_t total_sum = _mm_cvtsi128_si64(_mm_extract_epi64(sum, 0)) +\n                        _mm_cvtsi128_si64(_mm_extract_epi64(sum, 1));\n\n    int avg = std::round(static_cast<double>(total_sum) / count);\n\n    std::stringstream ss;\n    ss << std::bitset<32>(avg).to_string();\n    std::string result = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = result.find('1');\n    if (first_one != std::string::npos) {\n        result = result.substr(first_one);\n    } else {\n        result = \"0\";\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i fact_vec = _mm_setzero_si128();\n    int sum = 0;\n    int fact = 1;\n\n    for (int i = 1; i <= n; ++i) {\n        if (i % 2 == 0) {\n            // Calculate factorial\n            fact *= i;\n            fact %= 10000;\n            _mm_storeu_si128(&fact_vec, _mm_set1_epi32(fact));\n            _mm_add_epi32(sum_vec, fact_vec);\n        } else {\n            // Calculate sum\n            sum += i;\n            _mm_storeu_si128(&sum_vec, _mm_set1_epi32(sum));\n        }\n        _mm_storeu_si128((__m128i*)&result[i-1], sum_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i v_index = _mm_set_epi32(3, 2, 1, 0);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_ten_thousand = _mm_set1_epi32(10000);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i v_fact = _mm_setzero_si128();\n        __m128i v_sum = _mm_setzero_si128();\n\n        for (int j = 0; j < 4; ++j) {\n            int idx = i + j;\n            if (idx >= n) break;\n\n            __m128i v_idx = _mm_set1_epi32(idx + 1);\n            __m128i v_is_even = _mm_cmpeq_epi32(_mm_and_si128(v_idx, _mm_set1_epi32(1)), v_zero);\n\n            __m128i v_fact_next = _mm_mullo_epi32(v_fact, v_idx);\n            v_fact = _mm_add_epi32(v_fact_next, v_one);\n\n            __m128i v_sum_next = _mm_add_epi32(v_sum, v_idx);\n            v_sum = _mm_add_epi32(v_sum_next, v_one);\n\n            v_fact = _mm_blendv_epi8(v_fact, v_sum, v_is_even);\n        }\n\n        _mm_storeu_si128((__m128i*)&result[i], v_fact);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i v_index = _mm_set_epi32(3, 2, 1, 0);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_ten_thousand = _mm_set1_epi32(10000);\n\n    for (int i = 0; i < n; i += 4) {\n        __m128i v_fact = _mm_setzero_si128();\n        __m128i v_sum = _mm_setzero_si128();\n\n        for (int j = 0; j < 4; ++j) {\n            int idx = i + j;\n            if (idx >= n) break;\n\n            __m128i v_idx = _mm_set1_epi32(idx + 1);\n            __m128i v_is_even = _mm_cmpeq_epi32(_mm_and_si128(v_idx, _mm_set1_epi32(1)), v_zero);\n\n            __m128i v_temp = _mm_mullo_epi32(v_fact, v_idx);\n            v_fact = _mm_add_epi32(v_fact, _mm_and_si128(v_temp, v_is_even));\n            v_sum = _mm_add_epi32(v_sum, _mm_andnot_si128(v_is_even, v_idx));\n\n            v_fact = _mm_add_epi32(v_fact, v_one);\n            v_fact = _mm_sub_epi32(v_fact, _mm_cmpgt_epi32(v_fact, v_ten_thousand));\n            v_fact = _mm_and_si128(v_fact, v_ten_thousand);\n        }\n\n        _mm_storeu_si128((__m128i*)&result[i], v_fact);\n        _mm_storeu_si128((__m128i*)&result[i + 2], v_sum);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i v_index = _mm_set_epi32(3, 2, 1, 0);\n    __m128i v_fact = _mm_set_epi32(1, 1, 1, 1);\n    __m128i v_sum = _mm_set_epi32(0, 0, 0, 0);\n\n    for (int i = 4; i < n; i += 4) {\n        __m128i v_i = _mm_set_epi32(i, i - 1, i - 2, i - 3);\n        __m128i v_mask = _mm_cmpeq_epi32(v_i, _mm_setzero_si128());\n        v_fact = _mm_mul_epu32(v_fact, v_i);\n        v_sum = _mm_add_epi32(v_sum, v_i);\n        v_sum = _mm_andnot_si128(v_mask, v_sum);\n        v_fact = _mm_and_si128(v_mask, v_fact);\n        v_index = _mm_add_epi32(v_index, _mm_set_epi32(4, 4, 4, 4));\n        _mm_storeu_si128((__m128i*)&result[i - 3], v_fact);\n        _mm_storeu_si128((__m128i*)&result[i - 2], v_fact);\n        _mm_storeu_si128((__m128i*)&result[i - 1], v_fact);\n        _mm_storeu_si128((__m128i*)&result[i - 0], v_fact);\n    }\n\n    for (int i = n % 4; i > 0; --i) {\n        int idx = n - i;\n        if (idx % 2 == 0) {\n            result[idx] = 1;\n            for (int j = 2; j <= idx; ++j) {\n                result[idx] = (result[idx] * j) % 10000;\n            }\n        } else {\n            result[idx] = 0;\n            for (int j = 1; j <= idx; ++j) {\n                result[idx] += j;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m128i vec = _mm_setzero_si128();\n    int i = 1;\n\n    while (i <= n) {\n        __m128i sum = _mm_setzero_si128();\n        __m128i fact = _mm_setzero_si128();\n\n        for (int j = 0; j < 4 && i + j <= n; ++j) {\n            if ((i + j) % 2 == 0) {\n                // Calculate factorial\n                __m128i num = _mm_set1_epi32(i + j);\n                for (int k = 1; k < i + j; ++k) {\n                    num = _mm_mullo_epi32(num, _mm_set1_epi32(k));\n                }\n                fact = _mm_add_epi32(fact, num);\n            } else {\n                // Calculate sum\n                __m128i num = _mm_set1_epi32(i + j);\n                __m128i one = _mm_set1_epi32(1);\n                __m128i two = _mm_set1_epi32(2);\n                __m128i mul = _mm_mullo_epi32(num, _mm_add_epi32(one, num));\n                __m128i div = _mm_srai_epi32(mul, 1);\n                sum = _mm_add_epi32(sum, div);\n            }\n        }\n\n        _mm_storeu_si128((__m128i*)&result[i - 1], sum);\n        _mm_storeu_si128((__m128i*)&result[i + 3], fact);\n\n        i += 4;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i count_even = _mm_setzero_si128();\n    __m128i count_odd = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        bool is_palindrome = true;\n\n        // Check if the number is a palindrome\n        while (num > 0) {\n            int digit = num % 10;\n            if (digit != num / 10) {\n                is_palindrome = false;\n                break;\n            }\n            num /= 10;\n        }\n\n        if (is_palindrome) {\n            if ((i & 1) == 0) {\n                count_even = _mm_add_epi32(count_even, _mm_set1_epi32(1));\n            } else {\n                count_odd = _mm_add_epi32(count_odd, _mm_set1_epi32(1));\n            }\n        }\n    }\n\n    int even_count = _mm_extract_epi32(count_even, 0);\n    int odd_count = _mm_extract_epi32(count_odd, 0);\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i count = _mm_setzero_si128();\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            int bit = (i % 2 == 0) ? 0 : 1;\n            count = _mm_add_epi32(count, _mm_slli_epi32(_mm_set1_epi32(1), bit));\n        }\n    }\n    int even_count = _mm_extract_epi32(count, 0);\n    int odd_count = _mm_extract_epi32(count, 1);\n    return {even_count, odd_count};\n}\n\nbool is_palindrome(int num) {\n    char str[12];\n    sprintf(str, \"%d\", num);\n    int len = strlen(str);\n    for (int i = 0; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i count = _mm_setzero_si128();\n    int even = 0, odd = 0;\n\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            if ((i & 1) == 0) {\n                even += 1;\n            } else {\n                odd += 1;\n            }\n        }\n    }\n\n    std::vector<int> result;\n    result.push_back(even);\n    result.push_back(odd);\n\n    return result;\n}\n\nbool is_palindrome(int num) {\n    char str[11];\n    sprintf(str, \"%d\", num);\n    int len = strlen(str);\n    for (int i = 0; i < len / 2; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i count_even = _mm_setzero_si128();\n    __m128i count_odd = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        bool is_palindrome = true;\n        while (num > 0) {\n            if (num % 10 != num / 10) {\n                is_palindrome = false;\n                break;\n            }\n            num /= 10;\n        }\n\n        if (is_palindrome) {\n            if (i % 2 == 0) {\n                count_even = _mm_add_epi32(count_even, _mm_set1_epi32(1));\n            } else {\n                count_odd = _mm_add_epi32(count_odd, _mm_set1_epi32(1));\n            }\n        }\n    }\n\n    int even_count = _mm_extract_epi32(count_even, 0);\n    int odd_count = _mm_extract_epi32(count_odd, 0);\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    __m128i count_even = _mm_setzero_si128();\n    __m128i count_odd = _mm_setzero_si128();\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        int reversed = 0;\n\n        // Reverse the number\n        while (num > 0) {\n            reversed = reversed * 10 + num % 10;\n            num /= 10;\n        }\n\n        // Check if the original number is equal to its reversed version\n        if (i == reversed) {\n            if (i % 2 == 0) {\n                count_even = _mm_add_epi32(count_even, _mm_set1_epi32(1));\n            } else {\n                count_odd = _mm_add_epi32(count_odd, _mm_set1_epi32(1));\n            }\n        }\n    }\n\n    int result_even[4];\n    int result_odd[4];\n\n    _mm_storeu_si128((__m128i*)result_even, count_even);\n    _mm_storeu_si128((__m128i*)result_odd, count_odd);\n\n    std::vector<int> result(result_even[0] + result_even[1], result_odd[0] + result_odd[1]);\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i neg_mask = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i abs_vec = _mm_andnot_si128(neg_mask, vec);\n        __m128i sign_vec = _mm_and_si128(vec, neg_mask);\n        __m128i sum_vec = _mm_add_epi8(abs_vec, sign_vec);\n        __m128i gt_zero = _mm_cmpgt_epi8(sum_vec, zero);\n        int mask = _mm_movemask_epi8(gt_zero);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] != 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i neg_mask = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i sign_bits = _mm_and_si128(vec, neg_mask);\n        __m128i abs_vec = _mm_or_si128(_mm_add_epi8(sign_bits, ones), vec);\n        __m128i sum = _mm_sad_epu8(abs_vec, zero);\n        __m128i gt_zero = _mm_cmpgt_epi16(sum, zero);\n        int mask = _mm_movemask_epi8(gt_zero);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] != 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i neg_mask = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i sign_bits = _mm_srai_epi32(vec, 31);\n        __m128i abs_vec = _mm_andnot_si128(sign_bits, vec);\n        __m128i sum_digits = _mm_add_epi8(abs_vec, _mm_mullo_epi8(_mm_srli_epi32(abs_vec, 3), ones));\n        __m128i mask = _mm_cmpgt_epi8(sum_digits, zero);\n        int result = _mm_movemask_epi8(mask);\n        count += __builtin_popcount(result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] != 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i abs_vec = _mm_andnot_si128(_mm_srai_epi32(vec, 31), vec);\n        __m128i sum = _mm_add_epi32(abs_vec, _mm_srli_epi32(abs_vec, 1));\n        sum = _mm_add_epi32(sum, _mm_srli_epi32(sum, 2));\n        sum = _mm_add_epi32(sum, _mm_srli_epi32(sum, 4));\n        sum = _mm_add_epi32(sum, _mm_srli_epi32(sum, 8));\n\n        __m128i mask = _mm_cmpgt_epi32(sum, zero);\n        int result = _mm_movemask_epi8(mask);\n        count += __builtin_popcount(result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] != 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i neg_one = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i abs_vec = _mm_andnot_si128(_mm_srai_epi32(vec, 31), vec);\n        __m128i sign_vec = _mm_srai_epi32(vec, 31);\n        __m128i sum_vec = _mm_add_epi32(abs_vec, sign_vec);\n        __m128i gt_zero = _mm_cmpgt_epi32(sum_vec, zero);\n        int mask = _mm_movemask_epi8(gt_zero);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] != 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m128i vec1, vec2;\n    bool is_sorted = true;\n\n    for (int i = 0; i < N - 1; i += 4) {\n        vec1 = _mm_loadu_si128((__m128i*)&arr[i]);\n        vec2 = _mm_loadu_si128((__m128i*)&arr[i + 4]);\n\n        __m128i cmp1 = _mm_cmpgt_epi32(vec1, vec2);\n        __m128i cmp2 = _mm_cmplt_epi32(vec1, vec2);\n\n        if (_mm_movemask_epi8(cmp1) != 0 || _mm_movemask_epi8(cmp2) != 0) {\n            is_sorted = false;\n            break;\n        }\n    }\n\n    if (is_sorted) return true;\n\n    for (int i = 0; i < N - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            std::rotate(arr.begin(), arr.begin() + i + 1, arr.end());\n            break;\n        }\n    }\n\n    for (int i = 0; i < N - 1; i++) {\n        if (arr[i] > arr[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m128i v0, v1, v2;\n    bool found = false;\n\n    for (int i = 0; i < N - 3; i += 4) {\n        v0 = _mm_loadu_si128((__m128i*)&arr[i]);\n        v1 = _mm_loadu_si128((__m128i*)&arr[i + 1]);\n        v2 = _mm_loadu_si128((__m128i*)&arr[i + 2]);\n\n        __m128i cmp0 = _mm_cmpgt_epi32(v0, v1);\n        __m128i cmp1 = _mm_cmpgt_epi32(v1, v2);\n\n        if (_mm_movemask_epi8(cmp0) || _mm_movemask_epi8(cmp1)) {\n            found = true;\n            break;\n        }\n    }\n\n    if (!found) {\n        for (int i = N - 3; i < N - 1; ++i) {\n            if (arr[i] > arr[i + 1]) {\n                found = true;\n                break;\n            }\n        }\n    }\n\n    return !found;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m128i vec1, vec2;\n    bool found = false;\n\n    for (int i = 0; i < N - 1; ++i) {\n        vec1 = _mm_loadu_si128((__m128i*)&arr[i]);\n        vec2 = _mm_loadu_si128((__m128i*)&arr[i + 1]);\n\n        __m128i cmp = _mm_cmplt_epi32(vec1, vec2);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask == 0xFFFF) {\n            found = true;\n            break;\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m128i v0, v1, v2;\n    bool found = false;\n\n    for (int i = 0; i < N - 1; ++i) {\n        v0 = _mm_set_epi32(arr[(i + N - 1) % N], arr[i], arr[(i + 1) % N], arr[(i + 2) % N]);\n        v1 = _mm_set_epi32(arr[(i + 2) % N], arr[(i + 3) % N], arr[(i + 4) % N], arr[(i + 5) % N]);\n\n        v2 = _mm_cmpgt_epi32(v0, v1);\n        if (_mm_movemask_epi8(v2) == 0) {\n            found = true;\n            break;\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N <= 1) return true;\n\n    __m128i v0, v1, v2;\n    bool found = false;\n\n    for (int i = 0; i < N - 1; ++i) {\n        v0 = _mm_set_epi32(arr[(i + N - 1) % N], arr[i], arr[(i + 1) % N], arr[(i + 2) % N]);\n        v1 = _mm_set_epi32(arr[(i + 2) % N], arr[(i + 3) % N], arr[(i + 4) % N], arr[(i + 5) % N]);\n\n        v2 = _mm_cmpgt_epi32(v0, v1);\n        if (_mm_movemask_epi8(v2) != 0) {\n            found = true;\n            break;\n        }\n    }\n\n    return !found;\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string exchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m128i vzero = _mm_setzero_si128();\n    __m128i veven_mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i vlst1 = _mm_loadu_si128((__m128i*)&lst1[i]);\n        __m128i vlst2 = _mm_loadu_si128((__m128i*)&lst2[i]);\n\n        __m128i vlst1_even = _mm_and_si128(vlst1, veven_mask);\n        __m128i vlst2_odd = _mm_andnot_si128(veven_mask, vlst2);\n\n        if (!_mm_cmpeq_epi32(vlst1_even, vlst1).mmask8) {\n            if (_mm_movemask_epi8(_mm_cmpneq_epi32(vlst2_odd, vzero)) != 0) {\n                return \"YES\";\n            }\n        }\n    }\n\n    return \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m128i vzero = _mm_setzero_si128();\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i vlst1 = _mm_loadu_si128((__m128i*)&lst1[i]);\n        __m128i vlst2 = _mm_loadu_si128((__m128i*)&lst2[i]);\n\n        __m128i veven1 = _mm_and_si128(vlst1, vzero);\n        __m128i veven2 = _mm_and_si128(vlst2, vzero);\n\n        if (!_mm_testz_si128(veven1, veven1)) {\n            if (_mm_movemask_epi8(_mm_cmpeq_epi32(veven2, vzero)) != 0xFFFF) {\n                return \"NO\";\n            }\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m128i vzero = _mm_setzero_si128();\n    __m128i veven_mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i vlst1 = _mm_loadu_si128((__m128i*)&lst1[i]);\n        __m128i vlst2 = _mm_loadu_si128((__m128i*)&lst2[i]);\n\n        __m128i vlst1_even = _mm_and_si128(vlst1, veven_mask);\n        __m128i vlst2_odd = _mm_andnot_si128(veven_mask, vlst2);\n\n        if (!_mm_cmpeq_epi32(vlst1_even, vlst1).mmask16) {\n            return \"NO\";\n        }\n\n        if (!_mm_cmpeq_epi32(vlst2_odd, vlst2).mmask16) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m128i vzero = _mm_setzero_si128();\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i vlst1 = _mm_loadu_si128((__m128i*)&lst1[i]);\n        __m128i vlst2 = _mm_loadu_si128((__m128i*)&lst2[i]);\n\n        __m128i veven1 = _mm_and_si128(vlst1, _mm_set1_epi32(0x00000001));\n        __m128i veven2 = _mm_and_si128(vlst2, _mm_set1_epi32(0x00000001));\n\n        if (!_mm_testz_si128(veven1, veven1)) {\n            if (_mm_movemask_epi8(_mm_cmpeq_epi8(veven2, vzero)) != 0xF) {\n                return \"NO\";\n            }\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string exchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int size = lst1.size();\n    __m128i vzero = _mm_setzero_si128();\n    __m128i veven_mask = _mm_set1_epi32(0x55555555);\n\n    for (int i = 0; i < size; i += 4) {\n        __m128i vlst1 = _mm_loadu_si128((__m128i*)&lst1[i]);\n        __m128i vlst2 = _mm_loadu_si128((__m128i*)&lst2[i]);\n\n        __m128i vlst1_even = _mm_and_si128(vlst1, veven_mask);\n        __m128i vlst2_odd = _mm_andnot_si128(veven_mask, vlst2);\n\n        if (!_mm_cmpeq_epi32(vlst1_even, vlst1).mmask16) {\n            return \"NO\";\n        }\n\n        if (!_mm_cmpeq_epi32(vlst2_odd, vlst2).mmask16) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m128i count = _mm_setzero_si128();\n        int len = str.size();\n        for (int i = 0; i <= len - 16; i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + i));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int remaining = len % 16;\n        if (remaining > 0) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + len - remaining));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int total_odd = _mm_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m128i count = _mm_setzero_si128();\n        int len = str.size();\n        for (int i = 0; i <= len - 16; i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + i));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int remaining = len % 16;\n        if (remaining > 0) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + len - remaining));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int total_odd = _mm_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m128i count = _mm_setzero_si128();\n        int len = str.size();\n        for (int i = 0; i <= len - 16; i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + i));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int remaining = len % 16;\n        if (remaining > 0) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + len - remaining));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int total_odd = _mm_cvtsi128_si32(count);\n        std::string temp = \"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\";\n        result.push_back(temp);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m128i count = _mm_setzero_si128();\n        int len = str.size();\n        for (int i = 0; i <= len - 16; i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + i));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int remaining = len % 16;\n        if (remaining > 0) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + len - remaining));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int total_odd = _mm_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        __m128i count = _mm_setzero_si128();\n        int len = str.size();\n        for (int i = 0; i <= len - 16; i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + i));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int remaining = len % 16;\n        if (remaining > 0) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str.c_str() + len - remaining));\n            __m128i mask = _mm_and_si128(data, _mm_set1_epi8(0xF));\n            __m128i odd_mask = _mm_cmpeq_epi8(_mm_and_si128(mask, _mm_set1_epi8(1)), _mm_setzero_si128());\n            count = _mm_add_epi32(count, _mm_popcnt_epi8(odd_mask));\n        }\n        int total_odd = _mm_extract_epi32(count, 0);\n        result.push_back(\"the number of odd elements \" + std::to_string(total_odd) + \"n the str\" + std::to_string(total_odd) + \"ng \" + std::to_string(total_odd) + \" of the \" + std::to_string(total_odd) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t min_sum = INT64_MAX;\n    __m128i min_sum_vec = _mm_set1_epi64x(INT64_MAX);\n    __m128i current_sum_vec = _mm_setzero_si128();\n    __m128i zero_vec = _mm_setzero_si128();\n\n    for (size_t i = 0; i < nums.size(); ++i) {\n        __m128i num_vec = _mm_set1_epi64x(nums[i]);\n        current_sum_vec = _mm_add_epi64(current_sum_vec, num_vec);\n\n        __m128i temp_min_sum_vec = _mm_min_epi64(min_sum_vec, current_sum_vec);\n        min_sum_vec = _mm_or_si128(temp_min_sum_vec, _mm_cmpeq_epi64(zero_vec, current_sum_vec));\n\n        if (_mm_movemask_epi8(_mm_cmpgt_epi64(zero_vec, current_sum_vec)) & 0xF) {\n            current_sum_vec = zero_vec;\n        }\n    }\n\n    int64_t result[2];\n    _mm_storeu_si128((__m128i*)result, min_sum_vec);\n    return std::min(result[0], result[1]);\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m128i min_sum = _mm_set1_epi64x(nums[0]);\n    __m128i current_sum = _mm_set1_epi64x(nums[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m128i num = _mm_set1_epi64x(nums[i]);\n        __m128i temp = _mm_add_epi64(current_sum, num);\n        current_sum = _mm_min_epi64(temp, num);\n\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n\n    int64_t result;\n    _mm_store_si128((__m128i*)&result, min_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m128i min_sum = _mm_set1_epi64x(nums[0]);\n    __m128i current_sum = _mm_set1_epi64x(nums[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m128i num = _mm_set1_epi64x(nums[i]);\n        __m128i temp = _mm_add_epi64(current_sum, num);\n        current_sum = _mm_min_epi64(temp, num);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n\n    int64_t result;\n    _mm_store_si128((__m128i*)&result, min_sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m128i min_sum = _mm_set1_epi64x(nums[0]);\n    __m128i current_sum = _mm_set1_epi64x(nums[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m128i num = _mm_set1_epi64x(nums[i]);\n        __m128i temp = _mm_add_epi64(current_sum, num);\n        current_sum = _mm_min_epi64(temp, num);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n\n    int64_t result;\n    _mm_store_si128((__m128i*)&result, min_sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int n = nums.size();\n    if (n == 0) return 0;\n\n    __m128i min_sum = _mm_set1_epi64x(nums[0]);\n    __m128i current_sum = _mm_set1_epi64x(nums[0]);\n\n    for (int i = 1; i < n; ++i) {\n        __m128i num = _mm_set1_epi64x(nums[i]);\n        __m128i temp = _mm_add_epi64(current_sum, num);\n        current_sum = _mm_min_epi64(temp, num);\n\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n\n    int64_t result;\n    _mm_store_si128((__m128i*)&result, min_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    const int vec_size = 16 / sizeof(int);\n    int total_buckets = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(1);\n\n    for (int i = 0; i < rows; i += vec_size) {\n        int remaining_rows = std::min(rows - i, vec_size);\n        __m128i sum = zero;\n        for (int j = 0; j < cols; ++j) {\n            __m128i col_data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&grid[i][j]));\n            __m128i mask = _mm_cmpeq_epi32(col_data, ones);\n            __m128i count = _mm_and_si128(mask, _mm_set1_epi32(remaining_rows));\n            sum = _mm_add_epi32(sum, count);\n        }\n        int local_sum = _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1) +\n                        _mm_extract_epi32(sum, 2) + _mm_extract_epi32(sum, 3);\n        total_buckets += (local_sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    const int vec_size = 16 / sizeof(int);\n    int total_buckets = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(1);\n\n    for (int i = 0; i < rows; i += vec_size) {\n        int remaining_rows = std::min(rows - i, vec_size);\n        __m128i sum = zero;\n        for (int j = 0; j < cols; ++j) {\n            __m128i col_data = zero;\n            for (int k = 0; k < remaining_rows; ++k) {\n                col_data = _mm_or_si128(col_data, _mm_loadu_si128((__m128i*)&grid[i + k][j]));\n            }\n            sum = _mm_add_epi32(sum, col_data);\n        }\n        int local_sum = 0;\n        _mm_storeu_si128((__m128i*)&local_sum, sum);\n        total_buckets += (local_sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    const int blocks = (cols + 7) / 8;\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i capacity_vec = _mm_set1_epi32(capacity);\n\n    int total_buckets = 0;\n\n    for (int i = 0; i < rows; ++i) {\n        __m128i sum = zero;\n        for (int j = 0; j < blocks; ++j) {\n            __m128i block = _mm_loadu_si128((__m128i*)&grid[i][j * 8]);\n            sum = _mm_add_epi32(sum, _mm_and_si128(block, ones));\n        }\n        int local_sum = 0;\n        _mm_storeu_si128((__m128i*)&local_sum, sum);\n        total_buckets += (local_sum + capacity - 1) / capacity;\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    const int vec_size = 4; // SSE2 vector size\n    int total_buckets = 0;\n\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i one_vec = _mm_set1_epi32(1);\n    __m128i capacity_vec = _mm_set1_epi32(capacity);\n\n    for (int i = 0; i < rows; ++i) {\n        int remaining_cols = cols;\n        while (remaining_cols >= vec_size) {\n            __m128i column_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&grid[i][cols - remaining_cols]));\n            __m128i sum_vec = _mm_add_epi32(column_vec, zero_vec);\n            __m128i count_vec = _mm_popcnt_epi32(sum_vec);\n            __m128i buckets_vec = _mm_div_epi32(count_vec, capacity_vec);\n            __m128i remainder_vec = _mm_and_si128(_mm_sub_epi32(count_vec, _mm_mul_epi32(buckets_vec, capacity_vec)), one_vec);\n            __m128i total_buckets_vec = _mm_add_epi32(buckets_vec, remainder_vec);\n            total_buckets += _mm_cvtsi128_si32(total_buckets_vec);\n            remaining_cols -= vec_size;\n        }\n\n        if (remaining_cols > 0) {\n            int partial_sum = 0;\n            for (int j = cols - remaining_cols; j < cols; ++j) {\n                partial_sum += grid[i][j];\n            }\n            int partial_buckets = (partial_sum + capacity - 1) / capacity;\n            total_buckets += partial_buckets;\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    const int vec_size = 4; // SSE2 vector size\n    int total_buckets = 0;\n\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i ones_vec = _mm_set1_epi32(1);\n    __m128i capacity_vec = _mm_set1_epi32(capacity);\n\n    for (int i = 0; i < rows; ++i) {\n        int remaining_cols = cols;\n        while (remaining_cols >= vec_size) {\n            __m128i col_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&grid[i][cols - remaining_cols]));\n            __m128i sum_vec = _mm_add_epi32(col_vec, zero_vec);\n            __m128i count_vec = _mm_popcnt_epi32(sum_vec);\n            __m128i div_vec = _mm_div_epi32(count_vec, capacity_vec);\n            __m128i mod_vec = _mm_and_si128(_mm_sub_epi32(count_vec, _mm_mul_epi32(div_vec, capacity_vec)), ones_vec);\n            __m128i total_vec = _mm_add_epi32(div_vec, mod_vec);\n            total_buckets += _mm_extract_epi32(total_vec, 0) + _mm_extract_epi32(total_vec, 1) +\n                             _mm_extract_epi32(total_vec, 2) + _mm_extract_epi32(total_vec, 3);\n            remaining_cols -= vec_size;\n        }\n\n        if (remaining_cols > 0) {\n            int partial_sum = 0;\n            for (int j = cols - remaining_cols; j < cols; ++j) {\n                partial_sum += grid[i][j];\n            }\n            total_buckets += (partial_sum + capacity - 1) / capacity;\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi16(0x0001));\n        __m128i even_mask = _mm_and_si128(_mm_slli_epi16(mask, 1), _mm_set1_epi16(0x0001));\n        __m128i result = _mm_add_epi16(vec, _mm_sub_epi16(_mm_xor_si128(even_mask, vec), even_mask));\n        vsum = _mm_add_epi64(vsum, _mm_cvtepi32_epi64(_mm_madd_epi16(result, _mm_set1_epi16(1))));\n    }\n\n    int16_t temp[8];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vsum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi16(1));\n        __m128i even_mask = _mm_cmpeq_epi16(_mm_and_si128(vec, _mm_set1_epi16(2)), _mm_setzero_si128());\n        __m128i result = _mm_and_si128(vec, _mm_andnot_si128(mask, even_mask));\n        vsum = _mm_add_epi32(vsum, _mm_sad_epu8(result, _mm_setzero_si128()));\n    }\n\n    int32_t temp[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vsum);\n    sum = static_cast<int64_t>(temp[0]) + temp[1] + temp[2] + temp[3];\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi16(0x0001));\n        __m128i even_mask = _mm_and_si128(_mm_slli_epi16(mask, 1), _mm_set1_epi16(0x0001));\n        __m128i result = _mm_add_epi16(vec, _mm_sub_epi16(_mm_xor_si128(vec, even_mask), even_mask));\n        vsum = _mm_add_epi64(vsum, _mm_cvtepi32_epi64(_mm_srai_epi16(result, 1)));\n    }\n\n    int16_t temp[8];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vsum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    for (size_t i = n - n % 8; i < n; ++i) {\n        if (i % 2 == 0 && lst[i] % 2 != 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m128i vdata = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i vodd = _mm_and_si128(vdata, _mm_set1_epi16(1));\n        __m128i veven = _mm_and_si128(_mm_srli_epi16(vdata, 1), _mm_set1_epi16(1));\n        __m128i vresult = _mm_add_epi16(vodd, veven);\n        vsum = _mm_add_epi16(vsum, vresult);\n    }\n\n    int16_t temp[8];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vsum);\n\n    for (size_t i = 0; i < 8; ++i) {\n        if (i * 2 + 1 < n && (lst[i * 2 + 1] & 1)) {\n            sum += lst[i * 2 + 1];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t n = lst.size();\n\n    for (size_t i = 0; i <= n - 8; i += 8) {\n        __m128i vdata = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i vodd = _mm_and_si128(vdata, _mm_set1_epi16(1));\n        __m128i veven = _mm_and_si128(_mm_srli_epi16(vdata, 1), _mm_set1_epi16(1));\n        __m128i vresult = _mm_add_epi16(vodd, veven);\n        vsum = _mm_add_epi16(vsum, vresult);\n    }\n\n    int16_t temp[8];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), vsum);\n\n    for (size_t i = 0; i < 8; ++i) {\n        if (i < n % 8) {\n            sum += temp[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi32(0xFFFF));\n        __m128i cmp = _mm_cmpgt_epi32(mask, _mm_set1_epi32(99));\n        mask = _mm_andnot_si128(cmp, mask);\n        sum_vec = _mm_add_epi32(sum_vec, mask);\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, sum_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        if (i < k % 4) {\n            sum += result[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi32(0xFFFF));\n        __m128i cmp = _mm_cmpgt_epi32(mask, _mm_set1_epi32(99));\n        __m128i masked = _mm_andnot_si128(cmp, vec);\n        sum_vec = _mm_add_epi32(sum_vec, masked);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n\n    for (int i = 0; i < 4; i++) {\n        if (temp[i] > 99) continue;\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi32(0xFFFF));\n        __m128i cmp = _mm_cmpgt_epi16(mask, _mm_set1_epi16(99));\n        mask = _mm_andnot_si128(cmp, mask);\n        sum_vec = _mm_add_epi32(sum_vec, mask);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi32(0xFFFF));\n        __m128i cmp = _mm_cmpgt_epi16(mask, _mm_set1_epi16(99));\n        mask = _mm_andnot_si128(cmp, mask);\n        sum_vec = _mm_add_epi32(sum_vec, mask);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    __m128i sum_vec = _mm_setzero_si128();\n    int sum = 0;\n\n    for (int i = 0; i < k; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_and_si128(vec, _mm_set1_epi32(0xFFFF));\n        __m128i cmp = _mm_cmpgt_epi16(mask, _mm_set1_epi16(99));\n        mask = _mm_andnot_si128(cmp, mask);\n        sum_vec = _mm_add_epi32(sum_vec, mask);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m12i prev = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 1; i += 4) {\n        __m12i current = _mm_loadu_si128((__m12i*)&lst[i]);\n        __m12i cmp = _mm_cmple_epi32(prev, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) {\n            return false;\n        }\n\n        prev = current;\n    }\n\n    // Check remaining elements\n    for (int i = n - 4; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m12i prev = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 4; i += 4) {\n        __m12i current = _mm_loadu_si128((__m12i*)&lst[i]);\n        __m12i cmp = _mm_cmple_epi32(prev, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) {\n            return false;\n        }\n\n        prev = current;\n    }\n\n    // Check remaining elements\n    for (int i = n - 4; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m12i prev = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 1; i += 4) {\n        __m12i current = _mm_loadu_si128((__m12i*)&lst[i]);\n        __m12i cmp = _mm_cmple_epi32(prev, current);\n\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n\n        prev = current;\n    }\n\n    // Check remaining elements\n    for (int i = n - 4; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m12i prev = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 1; i += 4) {\n        __m12i current = _mm_loadu_si128((__m12i*)&lst[i]);\n        __m12i cmp = _mm_cmple_epi32(prev, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) {\n            return false;\n        }\n\n        prev = current;\n    }\n\n    // Check remaining elements\n    for (int i = n - 4; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    if (lst.size() <= 1) {\n        return true;\n    }\n\n    int n = lst.size();\n    __m12i prev = _mm_setzero_si128();\n\n    for (int i = 0; i < n - 1; i += 4) {\n        __m12i current = _mm_loadu_si128((__m12i*)&lst[i]);\n        __m12i cmp = _mm_cmple_epi32(prev, current);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFFFF) {\n            return false;\n        }\n\n        prev = current;\n    }\n\n    // Check remaining elements\n    for (int i = (n / 4) * 4; i < n - 1; ++i) {\n        if (lst[i] > lst[i + 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int sum = 0;\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n    __m128i zero = _mm_setzero_si128();\n    __m128i result = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_vec = _mm_andnot_si128(sign_mask, vec);\n        __m128i sign_vec = _mm_srai_epi32(vec, 31);\n\n        __m128i sum_vec = _mm_add_epi32(abs_vec, sum_vec);\n        __m128i result_vec = _mm_mullo_epi32(sign_vec, result_vec);\n    }\n\n    int remaining = arr.size() % 4;\n    for (size_t i = arr.size() - remaining; i < arr.size(); ++i) {\n        int num = arr[i];\n        sum += abs(num);\n        result *= (num > 0) ? 1 : ((num < 0) ? -1 : 0);\n    }\n\n    int final_result = _mm_cvtsi128_si32(result);\n    return final_result * sum;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i sign_product = _mm_setzero_si128();\n\n    int n = arr.size();\n    int i = 0;\n\n    // Process elements in chunks of 4\n    while (i + 4 <= n) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        sum = _mm_add_epi32(sum, abs_vec);\n\n        __m128i sign_mask = _mm_srai_epi32(vec, 31);\n        sign_product = _mm_mullo_epi32(sign_product, sign_mask);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    while (i < n) {\n        int val = arr[i];\n        sum = _mm_add_epi32(sum, _mm_set1_epi32(abs(val)));\n        sign_product = _mm_mullo_epi32(sign_product, _mm_set1_epi32((val >= 0) ? 1 : -1));\n        i++;\n    }\n\n    // Sum up the results from the SIMD registers\n    int result_sum = _mm_cvtsi128_si32(_mm_hadd_epi32(sum, sum));\n    result_sum += _mm_extract_epi32(sum, 2);\n    result_sum += _mm_extract_epi32(sum, 3);\n\n    int result_sign = _mm_cvtsi128_si32(sign_product);\n\n    return result_sum * result_sign;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int sum_magnitudes = 0;\n    __m128i sign_product = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        sum_magnitudes += _mm_extract_epi32(_mm_sad_epu8(abs_vec, _mm_setzero_si128()), 0);\n        __m128i sign_mask = _mm_and_si128(vec, _mm_set1_epi32(0x80000000));\n        sign_product = _mm_xor_si128(sign_product, sign_mask);\n    }\n\n    // Handle remaining elements\n    for (size_t i = arr.size() - arr.size() % 4; i < arr.size(); ++i) {\n        int value = arr[i];\n        sum_magnitudes += abs(value);\n        if (value < 0) {\n            sign_product = _mm_or_si128(sign_product, _mm_set1_epi32(0x80000000));\n        }\n    }\n\n    int total_sum = sum_magnitudes;\n    int total_sign = _mm_movemask_epi8(sign_product);\n\n    if (total_sign != 0) {\n        total_sum *= (total_sign >> 31) * 2 - 1;\n    } else {\n        total_sum *= 1;\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int sum = 0;\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_vec = _mm_andnot_si128(sign_mask, vec);\n        __m128i sign_vec = _mm_and_si128(vec, sign_mask);\n        __m128i count_negatives = _mm_cmpeq_epi32(sign_vec, zero);\n        __m128i count_positives = _mm_cmpgt_epi32(abs_vec, zero);\n        __m128i count_zeros = _mm_cmpeq_epi32(abs_vec, zero);\n\n        __m128i neg_count = _mm_movemask_epi8(count_negatives);\n        __m128i pos_count = _mm_movemask_epi8(count_positives);\n        __m128i zero_count = _mm_movemask_epi8(count_zeros);\n\n        int neg_sum = __builtin_popcount(neg_count);\n        int pos_sum = __builtin_popcount(pos_count);\n        int zero_sum = __builtin_popcount(zero_count);\n\n        sum += neg_sum * pos_sum * (zero_sum > 0 ? 0 : 1);\n    }\n\n    // Handle remaining elements\n    for (size_t i = arr.size() - arr.size() % 4; i < arr.size(); ++i) {\n        int val = arr[i];\n        if (val != 0) {\n            sum += (val < 0 ? -1 : 1);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    int sum = 0;\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i + 3 < arr.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_vec = _mm_andnot_si128(sign_mask, vec);\n        __m128i sign_vec = _mm_srai_epi32(vec, 31);\n        __m128i sum_vec = _mm_add_epi32(abs_vec, sum_vec);\n        __m128i sign_product = _mm_mullo_epi32(sign_vec, sign_product);\n        sum = _mm_cvtsi128_si32(sum_vec);\n        sign_product = _mm_cvtsi128_si32(sign_product);\n    }\n\n    // Handle remaining elements\n    for (; i < arr.size(); ++i) {\n        int num = arr[i];\n        sum += abs(num);\n        sign_product *= (num > 0) ? 1 : ((num < 0) ? -1 : 0);\n    }\n\n    return sign_product * sum;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SSE/SSE2 implementation here\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SSE/SSE2 code here\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SSE/SSE2 code here\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SSE/SSE2 implementation goes here\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SSE/SSE2 code here\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi8(0x0F);\n    __m128i result = _mm_setzero_si128();\n\n    while (n > 0) {\n        __m128i num = _mm_set1_epi64x(n);\n        __m128i odd_mask = _mm_and_si128(_mm_srl_epi64(num, _mm_set1_epi8(1)), mask);\n        __m128i odd_digits = _mm_and_si128(odd_mask, num);\n        __m128i non_zero = _mm_cmpneq_epi8(odd_digits, zero);\n        result = _mm_or_si128(result, _mm_and_si128(non_zero, odd_digits));\n        n /= 10;\n    }\n\n    uint64_t sum[2];\n    _mm_storeu_si128((__m128i*)sum, result);\n    return sum[0] + sum[1];\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi8(0x0F);\n    __m128i result = _mm_setzero_si128();\n\n    while (n > 0) {\n        __m128i num = _mm_set1_epi64x(n & 0xFFFFFFFFFFFFFFFF);\n        __m128i odd_mask = _mm_and_si128(num, mask);\n        __m128i odd_digits = _mm_and_si128(odd_mask, _mm_cmpgt_epi8(odd_mask, zero));\n        result = _mm_add_epi8(result, odd_digits);\n\n        n >>= 64;\n    }\n\n    uint8_t sum[16];\n    _mm_storeu_si128((__m128i*)sum, result);\n\n    uint64_t prod = 1;\n    bool has_odd = false;\n    for (int i = 0; i < 16; ++i) {\n        if (sum[i] != 0) {\n            prod *= sum[i];\n            has_odd = true;\n        }\n    }\n\n    return has_odd ? prod : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi8(0x0F);\n    __m128i result = _mm_setzero_si128();\n\n    while (n > 0) {\n        __m128i digits = _mm_cvtsi64_si128(n & 0xFFFFFFFF);\n        digits = _mm_and_si128(digits, mask);\n        digits = _mm_cmpgt_epi8(digits, zero);\n        digits = _mm_and_si128(digits, _mm_set1_epi8(0x01));\n        digits = _mm_mullo_epi8(digits, _mm_set1_epi8(1));\n        result = _mm_add_epi8(result, digits);\n        n >>= 32;\n    }\n\n    uint8_t sum[16];\n    _mm_storeu_si128((__m128i*)sum, result);\n\n    uint64_t prod = 1;\n    bool has_odd = false;\n    for (int i = 0; i < 16; ++i) {\n        if (sum[i] != 0) {\n            prod *= sum[i];\n            has_odd = true;\n        }\n    }\n\n    return has_odd ? prod : 0;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi8(0x55); // Mask to extract odd digits (bitwise AND with 0x55)\n    __m128i result = _mm_setzero_si128();\n\n    while (n > 0) {\n        __m128i digit = _mm_cvtsi64_si128(n % 10);\n        __m128i odd_digit = _mm_and_si128(digit, mask);\n        __m128i is_odd = _mm_cmpeq_epi8(odd_digit, digit);\n\n        result = _mm_or_si128(result, _mm_and_si128(odd_digit, is_odd));\n\n        n /= 10;\n    }\n\n    uint64_t final_result = 0;\n    _mm_storeu_si128((__m128i*)&final_result, result);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi8(0x0F);\n    __m128i result = _mm_setzero_si128();\n\n    while (n > 0) {\n        __m128i num = _mm_set1_epi64x(n);\n        __m128i odd_mask = _mm_and_si128(num, mask);\n        __m128i is_odd = _mm_cmpgt_epi8(odd_mask, zero);\n        __m128i odd_digits = _mm_and_si128(odd_mask, is_odd);\n        result = _mm_mullo_epi8(result, odd_digits);\n\n        n >>= 8;\n    }\n\n    uint64_t prod = 0;\n    _mm_storeu_si128((__m128i*)&prod, result);\n    return prod;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m128 vec, ceil_vec, square_vec;\n    float temp[4];\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        // Load 4 elements from the list into a vector\n        vec = _mm_loadu_ps(&lst[i]);\n\n        // Ceiling each element\n        ceil_vec = _mm_add_ps(vec, _mm_set1_ps(0.5f));\n        ceil_vec = _mm_max_ps(_mm_floor_ps(ceil_vec), vec);\n\n        // Square each element\n        square_vec = _mm_mul_ps(ceil_vec, ceil_vec);\n\n        // Store the results back into an array\n        _mm_storeu_ps(temp, square_vec);\n\n        // Sum the squared values\n        sum += static_cast<uint64_t>(temp[0]) + static_cast<uint64_t>(temp[1]) +\n               static_cast<uint64_t>(temp[2]) + static_cast<uint64_t>(temp[3]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    __m128 vec, ceil_vec, square_vec;\n    float temp[4];\n    \n    for(size_t i = 0; i < lst.size(); i += 4){\n        vec = _mm_loadu_ps(&lst[i]);\n        \n        // Ceiling operation\n        ceil_vec = _mm_add_ps(vec, _mm_set1_ps(0.5f));\n        ceil_vec = _mm_max_ps(_mm_floor_ps(ceil_vec), vec);\n        \n        // Square operation\n        square_vec = _mm_mul_ps(ceil_vec, ceil_vec);\n        \n        // Sum up the squares\n        _mm_storeu_ps(temp, square_vec);\n        sum += static_cast<uint64_t>(temp[0]) + static_cast<uint64_t>(temp[1]) +\n               static_cast<uint64_t>(temp[2]) + static_cast<uint64_t>(temp[3]);\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m128 vec, ceil_vec, square_vec;\n    float temp[4];\n\n    size_t n = lst.size();\n    size_t i = 0;\n\n    // Process elements in chunks of 4\n    for (; i + 3 < n; i += 4) {\n        vec = _mm_loadu_ps(&lst[i]);\n        // Ceiling each element\n        ceil_vec = _mm_add_ps(vec, _mm_set1_ps(0.5f));\n        ceil_vec = _mm_max_ps(_mm_floor_ps(ceil_vec), vec);\n        // Square each element\n        square_vec = _mm_mul_ps(ceil_vec, ceil_vec);\n        // Sum the squares\n        _mm_storeu_ps(temp, square_vec);\n        sum += static_cast<uint64_t>(temp[0]) + static_cast<uint64_t>(temp[1]) +\n               static_cast<uint64_t>(temp[2]) + static_cast<uint64_t>(temp[3]);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        float val = lst[i];\n        val = std::ceil(val);\n        sum += static_cast<uint64_t>(val * val);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m128 vec, ceil_vec, square_vec;\n    float temp[4];\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        // Load 4 elements from the list into a vector\n        vec = _mm_loadu_ps(&lst[i]);\n\n        // Ceiling each element\n        ceil_vec = _mm_add_ps(vec, _mm_set1_ps(0.5f));\n        ceil_vec = _mm_max_ps(ceil_vec, vec);\n\n        // Square each element\n        square_vec = _mm_mul_ps(ceil_vec, ceil_vec);\n\n        // Store the results back into an array\n        _mm_storeu_ps(temp, square_vec);\n\n        // Sum the squared values\n        for (int j = 0; j < 4; ++j) {\n            if (i + j < lst.size()) {\n                sum += static_cast<uint64_t>(temp[j]);\n            }\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    __m128 vec, ceil_vec, square_vec;\n    float temp[4];\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        // Load 4 elements from the list into a vector\n        vec = _mm_loadu_ps(&lst[i]);\n\n        // Ceiling each element\n        ceil_vec = _mm_ceill_ps(vec);\n\n        // Square each element\n        square_vec = _mm_mul_ps(ceil_vec, ceil_vec);\n\n        // Store the squared values back into an array\n        _mm_storeu_ps(temp, square_vec);\n\n        // Sum the squared values\n        sum += static_cast<uint64_t>(temp[0]) + static_cast<uint64_t>(temp[1]) +\n               static_cast<uint64_t>(temp[2]) + static_cast<uint64_t>(temp[3]);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() <= 1) {\n        return -1;\n    }\n\n    int n = arr.size();\n    __m128i prev = _mm_set1_epi32(arr[0]);\n    int idx = -1;\n\n    for (int i = 1; i + 3 < n; i += 4) {\n        __m128i curr = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i mask = _mm_cmpgt_epi32(prev, curr);\n        int result = _mm_movemask_epi8(mask);\n\n        if (result != 0) {\n            idx = i + __builtin_ctz(result) / 4;\n            break;\n        }\n\n        prev = curr;\n    }\n\n    // Check remaining elements\n    for (; i < n; ++i) {\n        if (arr[i] > arr[i - 1]) {\n            idx = i;\n            break;\n        }\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int n = arr.size();\n    __m128i* vec = (__m128i*)arr.data();\n\n    for (int i = 0; i <= n - 8; i += 4) {\n        __m128i prev = _mm_loadu_si128(&vec[i]);\n        __m128i curr = _mm_loadu_si128(&vec[i + 1]);\n\n        __m128i cmp = _mm_cmple_epi32(prev, curr);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFF) {\n            return i + 4 + __builtin_ctz(mask) / 4;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - 8; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int n = arr.size();\n    __m128i* vec = (__m128i*)arr.data();\n\n    for (int i = 0; i <= n - 8; i += 4) {\n        __m128i prev = _mm_loadu_si128(&vec[i]);\n        __m128i curr = _mm_loadu_si128(&vec[i + 1]);\n\n        __m128i cmp = _mm_cmple_epi32(prev, curr);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFF) {\n            return i + 4 + __builtin_ctz(mask) / 4;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - 8; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() <= 1) {\n        return -1;\n    }\n\n    int n = arr.size();\n    __m128i* vec = (__m128i*)arr.data();\n\n    for (int i = 0; i + 3 < n; i += 4) {\n        __m128i prev = _mm_loadu_si128(&vec[i - 1]);\n        __m128i curr = _mm_loadu_si128(&vec[i]);\n\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            return i - 1 + __builtin_ctz(mask) / 4;\n        }\n    }\n\n    // Check remaining elements\n    for (int i = n - 4; i < n; ++i) {\n        if (arr[i] > arr[i - 1]) {\n            return i - 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int>& arr) {\n    if (arr.size() < 2) {\n        return -1;\n    }\n\n    int n = arr.size();\n    __m12i* vec = (__m12i*)arr.data();\n\n    for (int i = 0; i <= n - 8; i += 4) {\n        __m12i prev = _mm_loadu_si128(&vec[i]);\n        __m12i curr = _mm_loadu_si128(&vec[i + 1]);\n\n        __m12i cmp = _mm_cmple_epi32(prev, curr);\n        int mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0xFF) {\n            int idx = i + __builtin_ctz(mask) / 4;\n            return idx + 4;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = n - 8; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            return i + 1;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    int largest_negative = INT_MIN;\n    int smallest_positive = INT_MAX;\n\n    __m128i v_neg_inf = _mm_set1_epi32(INT_MIN);\n    __m128i v_pos_inf = _mm_set1_epi32(INT_MAX);\n\n    for (size_t i = 0; i + 3 < lst.size(); i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        __m128i v_neg = _mm_cmpgt_epi32(v_neg_inf, v);\n        __m128i v_pos = _mm_cmplt_epi32(v, v_pos_inf);\n\n        __m128i v_neg_mask = _mm_and_si128(v_neg, v);\n        __m128i v_pos_mask = _mm_and_si128(v_pos, v);\n\n        largest_negative = std::max(largest_negative, _mm_extract_epi32(v_neg_mask, 0));\n        largest_negative = std::max(largest_negative, _mm_extract_epi32(v_neg_mask, 1));\n        largest_negative = std::max(largest_negative, _mm_extract_epi32(v_neg_mask, 2));\n        largest_negative = std::max(largest_negative, _mm_extract_epi32(v_neg_mask, 3));\n\n        smallest_positive = std::min(smallest_positive, _mm_extract_epi32(v_pos_mask, 0));\n        smallest_positive = std::min(smallest_positive, _mm_extract_epi32(v_pos_mask, 1));\n        smallest_positive = std::min(smallest_positive, _mm_extract_epi32(v_pos_mask, 2));\n        smallest_positive = std::min(smallest_positive, _mm_extract_epi32(v_pos_mask, 3));\n    }\n\n    for (size_t i = lst.size() - lst.size() % 4; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_positive) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i min_pos = _mm_set1_epi32(INT_MAX);\n    __m128i max_neg = _mm_set1_epi32(INT_MIN);\n\n    int n = lst.size();\n    int i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n        __m128i neg_mask = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n\n        min_pos = _mm_min_epu32(min_pos, _mm_and_si128(pos_mask, vec));\n        max_neg = _mm_max_epu32(max_neg, _mm_and_si128(neg_mask, vec));\n    }\n\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val > 0) {\n            min_pos = _mm_min_epi32(min_pos, _mm_set1_epi32(val));\n        } else if (val < 0) {\n            max_neg = _mm_max_epu32(max_neg, _mm_set1_epi32(val));\n        }\n    }\n\n    int min_pos_val = INT_MAX;\n    int max_neg_val = INT_MIN;\n\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&min_pos_val), min_pos);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&max_neg_val), max_neg);\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i min_pos = _mm_set1_epi32(INT_MAX);\n    __m128i max_neg = _mm_set1_epi32(INT_MIN);\n\n    int n = lst.size();\n    int i = 0;\n\n    while (i + 4 <= n) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n        __m128i neg_mask = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n\n        min_pos = _mm_min_epi32(min_pos, _mm_and_si128(pos_mask, vec));\n        max_neg = _mm_max_epi32(max_neg, _mm_and_si128(neg_mask, vec));\n\n        i += 4;\n    }\n\n    int remaining = n - i;\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val > 0) {\n            min_pos = _mm_min_epi32(min_pos, _mm_set1_epi32(val));\n        } else if (val < 0) {\n            max_neg = _mm_max_epi32(max_neg, _mm_set1_epi32(val));\n        }\n    }\n\n    int min_val = _mm_extract_epi32(min_pos, 0);\n    int max_val = _mm_extract_epi32(max_neg, 0);\n\n    return {max_val, min_val};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i min_pos = _mm_set1_epi32(INT_MAX);\n    __m128i max_neg = _mm_set1_epi32(INT_MIN);\n\n    int n = lst.size();\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n        __m128i neg_mask = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n\n        min_pos = _mm_min_epu32(min_pos, _mm_and_si128(pos_mask, vec));\n        max_neg = _mm_max_epu32(max_neg, _mm_and_si128(neg_mask, vec));\n    }\n\n    int remaining = n - i;\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val > 0) {\n            min_pos = _mm_min_epi32(min_pos, _mm_set1_epi32(val));\n        } else if (val < 0) {\n            max_neg = _mm_max_epu32(max_neg, _mm_set1_epi32(val));\n        }\n    }\n\n    int min_pos_val = _mm_extract_epi32(min_pos, 0);\n    int max_neg_val = _mm_extract_epi32(max_neg, 0);\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    __m128i min_pos = _mm_set1_epi32(INT_MAX);\n    __m128i max_neg = _mm_set1_epi32(INT_MIN);\n\n    int n = lst.size();\n    int i = 0;\n\n    // Process elements in chunks of 4\n    while (i + 4 <= n) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n        __m128i neg_mask = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n\n        min_pos = _mm_min_epi32(min_pos, _mm_and_si128(pos_mask, vec));\n        max_neg = _mm_max_epi32(max_neg, _mm_and_si128(neg_mask, vec));\n\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val > 0) {\n            min_pos = _mm_min_epi32(min_pos, _mm_set1_epi32(val));\n        } else if (val < 0) {\n            max_neg = _mm_max_epi32(max_neg, _mm_set1_epi32(val));\n        }\n    }\n\n    int result[4];\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result), min_pos);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(result + 2), max_neg);\n\n    return {result[2], result[3]};\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i sum = _mm_setzero_si128();\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n        __m128i factor = _mm_set1_epi64x(fact);\n        sum = _mm_add_epi64(sum, factor);\n    }\n    uint64_t result[2];\n    _mm_storeu_si128((__m128i*)result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i sum = _mm_setzero_si128();\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n        __m128i fact_vec = _mm_set1_epi64x(fact);\n        sum = _mm_add_epi64(sum, fact_vec);\n    }\n    uint64_t result[2];\n    _mm_storeu_si128((__m128i*)result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m128i sum = _mm_setzero_si128();\n    for (uint64_t i = 1; i <= n; ++i) {\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n        __m128i val = _mm_set1_epi64x(fact);\n        sum = _mm_add_epi64(sum, val);\n    }\n    uint64_t result[2];\n    _mm_storeu_si128((__m128i*)result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) return 1;\n\n    uint64_t result = 1;\n    __m128i sum = _mm_setzero_si128();\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n        result *= fact;\n        sum = _mm_add_epi64(sum, _mm_set1_epi64x(result));\n    }\n\n    uint64_t final_result = 0;\n    _mm_storeu_si128((__m128i*)&final_result, sum);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) return 1;\n\n    __m128i sum = _mm_setzero_si128();\n    uint64_t result = 1;\n\n    for (uint64_t i = 2; i <= n; ++i) {\n        __m128i fact = _mm_set1_epi64x(i);\n        for (int j = 0; j < 8; ++j) {\n            result *= i;\n            fact = _mm_mul_epi64(fact, _mm_set1_epi64x(result));\n        }\n        sum = _mm_add_epi64(sum, fact);\n    }\n\n    uint64_t final_result = 0;\n    _mm_storeu_si128((__m128i*)&final_result, sum);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i four = _mm_set1_epi32(4);\n\n    for (size_t i = 0; i <= lst.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i mask3 = _mm_and_si128(_mm_cmpeq_epi32(_mm_mod_epi32(vec, four), three), _mm_cmpneq_epi32(_mm_mod_epi32(vec, four), zero));\n        __m128i mask4 = _mm_and_si128(_mm_cmpeq_epi32(_mm_mod_epi32(vec, four), four), _mm_cmpneq_epi32(_mm_mod_epi32(vec, four), zero));\n\n        __m128i squares = _mm_mullo_epi32(vec, vec);\n        __m128i cubes = _mm_mullo_epi32(squares, vec);\n\n        __m128i result = _mm_add_epi32(zero, squares);\n        result = _mm_add_epi32(result, _mm_and_si128(mask3, cubes));\n        result = _mm_add_epi32(result, _mm_andnot_si128(mask4, squares));\n\n        sum += _mm_cvtsi128_si32(result);\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(result, 4));\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(result, 8));\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(result, 12));\n    }\n\n    for (size_t i = lst.size() - lst.size() % 4; i < lst.size(); ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            sum += value * value;\n        } else if (i % 4 == 0 && i % 3 != 0) {\n            sum += value * value * value;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    int n = lst.size();\n\n    for(int i = 0; i <= n - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask1 = _mm_and_si128(_mm_cmpeqd_epi32(vec, _mm_setzero_si128()), _mm_cmpeq_epi32(_mm_and_si128(vec, _mm_set1_epi32(3)), _mm_setzero_si128()));\n        __m128i mask2 = _mm_andnot_si128(mask1, _mm_and_si128(_mm_cmpeqd_epi32(vec, _mm_setzero_si128()), _mm_cmpeq_epi32(_mm_and_si128(vec, _mm_set1_epi32(4)), _mm_setzero_si128())));\n        __m128i squares = _mm_mullo_epi32(vec, vec);\n        __m128i cubes = _mm_mullo_epi32(squares, vec);\n        __m128i result = _mm_add_epi32(squares, cubes);\n        result = _mm_and_si128(result, mask1);\n        result = _mm_or_si128(result, _mm_and_si128(cubes, mask2));\n        vsum = _mm_add_epi32(vsum, result);\n    }\n\n    int remaining = n % 4;\n    for(int i = n - remaining; i < n; ++i){\n        int val = lst[i];\n        if(i % 3 == 0){\n            sum += val * val;\n        } else if(i % 4 == 0){\n            sum += val * val * val;\n        }\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n    __m128i four = _mm_set1_epi32(4);\n\n    for (size_t i = 0; i <= lst.size() - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i mask3 = _mm_and_si128(_mm_cmpeq_epi32(_mm_mod_epi32(vec, four), three), _mm_cmpneq_epi32(_mm_mod_epi32(vec, four), zero));\n        __m128i mask4 = _mm_and_si128(_mm_cmpeqd_epi32(_mm_mod_epi32(vec, four), four), _mm_cmpneq_epi32(_mm_mod_epi32(vec, four), zero));\n\n        __m128i squares = _mm_mullo_epi32(vec, vec);\n        __m128i cubes = _mm_mullo_epi32(squares, vec);\n\n        __m128i result = _mm_add_epi32(zero, _mm_blendv_epi8(cubes, squares, mask3));\n        result = _mm_add_epi32(result, _mm_blendv_epi8(result, zero, mask4));\n\n        sum += _mm_cvtsi128_si32(result);\n    }\n\n    for (size_t i = lst.size() - lst.size() % 4; i < lst.size(); ++i) {\n        int value = lst[i];\n        if (i % 3 == 0) {\n            sum += value * value;\n        } else if (i % 4 == 0 && i % 3 != 0) {\n            sum += value * value * value;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    int n = lst.size();\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask3 = _mm_and_si128(_mm_set1_epi32(3), _mm_setr_epi32(i, i + 1, i + 2, i + 3));\n        __m128i mask4 = _mm_and_si128(_mm_set1_epi32(4), _mm_setr_epi32(i, i + 1, i + 2, i + 3));\n\n        __m128i squared = _mm_mullo_epi32(vec, vec);\n        __m128i cubed = _mm_mullo_epi32(squared, vec);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mask3, squared),\n            _mm_andnot_si128(mask3, _mm_and_si128(mask4, cubed))\n        );\n\n        vsum = _mm_add_epi32(vsum, result);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    for (int i = n - n % 4; i < n; ++i) {\n        int val = lst[i];\n        if (i % 3 == 0) {\n            sum += val * val;\n        } else if (i % 4 == 0) {\n            sum += val * val * val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    const int* ptr = lst.data();\n    int size = lst.size();\n\n    for (int i = 0; i <= size - 4; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&ptr[i]);\n        __m128i mask3 = _mm_cmpestrm(_mm_set1_epi32(3), 4, vec, 4, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_EACH);\n        __m128i mask4 = _mm_cmpestrm(_mm_set1_epi32(4), 4, vec, 4, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_EACH);\n        __m128i mask_not3 = _mm_andnot_si128(mask3, _mm_set1_epi32(-1));\n        __m128i mask_not4 = _mm_andnot_si128(mask4, _mm_set1_epi32(-1));\n\n        __m128i squared = _mm_mullo_epi32(vec, vec);\n        __m128i cubed = _mm_mullo_epi32(squared, vec);\n\n        squared = _mm_and_si128(squared, mask3);\n        cubed = _mm_and_si128(cubed, mask4);\n\n        cubed = _mm_and_si128(cubed, mask_not3);\n\n        __m128i result = _mm_add_epi32(squared, cubed);\n        vsum = _mm_add_epi32(vsum, result);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    for (int i = size - size % 4; i < size; ++i) {\n        int value = ptr[i];\n        if (i % 3 == 0) {\n            sum += value * value;\n        } else if (i % 4 == 0) {\n            sum += value * value * value;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i mask_odd_first_last = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n\n        // Check if elements are greater than 10\n        __m128i gt_threshold = _mm_cmpgt_epi32(vec, threshold);\n\n        // Extract first and last digits\n        __m128i abs_vec = _mm_andnot_si128(_mm_srai_epi32(vec, 31), vec);\n        __m128i first_digit = _mm_and_si128(abs_vec, _mm_set1_epi32(0xF));\n        __m128i last_digit = _mm_and_si128(abs_vec, _mm_set1_epi32(0xF0));\n\n        // Shift last digit right by 4 bits to align with first digit\n        __m128i shifted_last_digit = _mm_srli_epi32(last_digit, 4);\n\n        // Combine first and last digits\n        __m128i combined_digits = _mm_or_si128(first_digit, shifted_last_digit);\n\n        // Apply mask to check if both digits are odd\n        __m128i odd_masked = _mm_and_si128(combined_digits, mask_odd_first_last);\n\n        // Count elements that satisfy all conditions\n        __m128i result = _mm_and_si128(gt_threshold, _mm_cmpeq_epi32(odd_masked, combined_digits));\n        int result_int[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result_int), result);\n        count += result_int[0] + result_int[1] + result_int[2] + result_int[3];\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] > 10) {\n            int num = nums[i];\n            int first_digit = abs(num) % 10;\n            int last_digit = abs(num) / 1000000000;\n            if (first_digit % 2 != 0 && last_digit % 2 != 0) {\n                ++count;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i mask_odd_first_last = _mm_set1_epi32(0x55555555); // Binary: 01010101...\n\n    for (size_t i = 0; i + 3 < nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n\n        // Check if each element is greater than 10\n        __m128i gt_threshold = _mm_cmpgt_epi32(vec, threshold);\n\n        // Extract first and last digits\n        __m128i abs_vec = _mm_andnot_si128(_mm_srai_epi32(vec, 31), vec);\n        __m128i first_digit = _mm_and_si128(abs_vec, _mm_set1_epi32(0xF));\n        __m128i last_digit = _mm_and_si128(abs_vec, _mm_set1_epi32(0xF0));\n\n        // Check if first and last digits are odd\n        __m128i odd_first_last = _mm_and_si128(first_digit, mask_odd_first_last);\n        __m128i odd_last = _mm_and_si128(last_digit, mask_odd_first_last);\n\n        // Combine conditions\n        __m128i combined = _mm_and_si128(gt_threshold, _mm_and_si128(odd_first_last, odd_last));\n\n        // Count true values\n        int result[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result), combined);\n        for (int j = 0; j < 4; ++j) {\n            count += (result[j] != 0);\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] > 10 && ((abs(nums[i]) / 10) % 10) % 2 == 1 && (abs(nums[i]) % 10) % 2 == 1) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i mask_odd = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n\n        // Check if elements are greater than 10\n        __m128i gt_threshold = _mm_cmpgt_epi32(vec, threshold);\n\n        // Extract first and last digits\n        __m128i abs_vec = _mm_andnot_si128(_mm_srai_epi32(vec, 31), vec);\n        __m128i last_digit = _mm_and_si128(abs_vec, _mm_set1_epi32(9));\n        __m128i first_digit = _mm_srli_epi32(abs_vec, 28);\n\n        // Check if first and last digits are odd\n        __m128i odd_digits = _mm_and_si128(first_digit, mask_odd);\n        __m128i odd_last_digit = _mm_and_si128(last_digit, mask_odd);\n\n        // Combine conditions\n        __m128i combined = _mm_and_si128(gt_threshold, _mm_and_si128(odd_digits, odd_last_digit));\n\n        // Count true values\n        int result[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result), combined);\n        for (int j = 0; j < 4; ++j) {\n            count += (result[j] != 0);\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] > 10 && (nums[i] % 10) % 2 != 0 && ((nums[i] / 10) % 10) % 2 != 0) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i mask = _mm_set1_epi32(0x55555555); // Mask to check if first and last bits are set (odd)\n    __m128i ten = _mm_set1_epi32(10);\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i cmp = _mm_cmpgt_epi32(vec, ten);\n        __m128i and_mask = _mm_and_si128(cmp, mask);\n        __m128i result = _mm_movemask_epi8(and_mask);\n        count += __builtin_popcount(result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] > 10 && ((nums[i] / 10) % 2 != 0 && nums[i] % 2 != 0)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i mask_odd = _mm_set1_epi32(0x55555555); // Binary: 01010101010101010101010101010101\n\n    for (size_t i = 0; i + 4 <= nums.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i greater_than_threshold = _mm_cmpgt_epi32(vec, threshold);\n\n        __m128i first_digit = _mm_and_si128(vec, _mm_set1_epi32(0xF));\n        __m128i last_digit = _mm_srli_epi32(vec, 28);\n        __m128i odd_digits = _mm_and_si128(_mm_or_si128(first_digit, last_digit), mask_odd);\n\n        __m128i result = _mm_and_si128(greater_than_threshold, odd_digits);\n        int result_int[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result_int), result);\n        count += (result_int[0] != 0) + (result_int[1] != 0) + (result_int[2] != 0) + (result_int[3] != 0);\n    }\n\n    // Handle remaining elements\n    for (size_t i = nums.size() - nums.size() % 4; i < nums.size(); ++i) {\n        if (nums[i] > 10 && ((nums[i] / 10) % 10) % 2 != 0 && (nums[i] % 10) % 2 != 0) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    const __m128i one = _mm_set1_epi32(1);\n    const __m128i three = _mm_set1_epi32(3);\n\n    uint64_t count = 0;\n    for (uint64_t i = 0; i < n; i += 4) {\n        __m128i ai = _mm_mullo_epi32(_mm_add_epi32(_mm_set_epi32(i+3, i+2, i+1, i), one), _mm_sub_epi32(_mm_set_epi32(i+3, i+2, i+1, i), one));\n        ai = _mm_add_epi32(ai, one);\n\n        __m128i sum = _mm_setzero_si128();\n        for (uint64_t j = i + 1; j < n; j += 4) {\n            __m128i aj = _mm_mullo_epi32(_mm_add_epi32(_mm_set_epi32(j+3, j+2, j+1, j), one), _mm_sub_epi32(_mm_set_epi32(j+3, j+2, j+1, j), one));\n            aj = _mm_add_epi32(aj, one);\n\n            __m128i ak = _mm_mullo_epi32(_mm_add_epi32(_mm_set_epi32(k+3, k+2, k+1, k), one), _mm_sub_epi32(_mm_set_epi32(k+3, k+2, k+1, k), one));\n            ak = _mm_add_epi32(ak, one);\n\n            __m128i temp_sum = _mm_add_epi32(ai, aj);\n            temp_sum = _mm_add_epi32(temp_sum, ak);\n\n            __m128i mod_result = _mm_and_si128(_mm_srai_epi32(_mm_add_epi32(temp_sum, three), 31), three);\n            count += _mm_movemask_epi8(_mm_cmpeq_epi32(mod_result, _mm_setzero_si128()));\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    __m128i sum_vec = _mm_setzero_si128();\n    uint64_t count = 0;\n\n    for (uint64_t i = 0; i < n; ++i) {\n        __m128i a_i = _mm_set1_epi64x(i * i - i + 1);\n        for (uint64_t j = i + 1; j < n; ++j) {\n            __m128i a_j = _mm_set1_epi64x(j * j - j + 1);\n            for (uint64_t k = j + 1; k < n; ++k) {\n                __m128i a_k = _mm_set1_epi64x(k * k - k + 1);\n                __m128i sum = _mm_add_epi64(_mm_add_epi64(a_i, a_j), a_k);\n                __m128i mod_3 = _mm_and_si128(sum, _mm_set1_epi64x(2));\n                if (_mm_movemask_epi8(mod_3) == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    const uint64_t size = 16;\n    __m128i sum_mod_3 = _mm_setzero_si128();\n    __m128i count_mod_3[3] = {_mm_setzero_si128(), _mm_setzero_si128(), _mm_setzero_si128()};\n\n    for (uint64_t i = 0; i < n; i += size) {\n        __m128i indices = _mm_setr_epi64x(i, i+1, i+2, i+3);\n        __m128i values = _mm_mullo_epi64(_mm_add_epi64(indices, _mm_set1_epi64x(1)), indices);\n        values = _mm_sub_epi64(values, indices);\n        values = _mm_add_epi64(values, _mm_set1_epi64x(1));\n\n        __m128i mod_3 = _mm_and_epi64(values, _mm_set1_epi64x(2));\n        mod_3 = _mm_cmpgt_epi64(mod_3, _mm_setzero_si128());\n        mod_3 = _mm_and_si128(mod_3, _mm_set1_epi64x(1));\n\n        __m128i count = _mm_set1_epi64x(size);\n        count = _mm_and_si128(count, _mm_cmplt_epi64(indices, _mm_set1_epi64x(n)));\n\n        sum_mod_3 = _mm_add_epi64(sum_mod_3, _mm_madd_epi16(mod_3, count));\n\n        for (int j = 0; j < 3; ++j) {\n            count_mod_3[j] = _mm_add_epi64(count_mod_3[j], _mm_and_si128(mod_3, _mm_set1_epi64x(j)));\n        }\n    }\n\n    uint64_t result = 0;\n    for (int j = 0; j < 3; ++j) {\n        result += _mm_extract_epi64(count_mod_3[j], 0) * (_mm_extract_epi64(count_mod_3[(j+1)%3], 0) - _mm_extract_epi64(count_mod_3[(j+2)%3], 0));\n    }\n\n    return result + _mm_extract_epi64(sum_mod_3, 0);\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    __m128i sum_vec = _mm_setzero_si128();\n    uint64_t count = 0;\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        __m128i a_i = _mm_set1_epi64x(i * i - i + 1);\n        sum_vec = _mm_add_epi64(sum_vec, a_i);\n\n        if (_mm_movemask_epi8(_mm_and_si128(sum_vec, _mm_set1_epi64x(3))) == 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    __m128i sum_vec = _mm_setzero_si128();\n    uint64_t count = 0;\n\n    for (uint64_t i = 0; i < n; ++i) {\n        __m128i a_i = _mm_set1_epi64x(i * i - i + 1);\n        sum_vec = _mm_add_epi64(sum_vec, a_i);\n\n        if (_mm_movemask_epi8(_mm_and_si128(sum_vec, _mm_set1_epi64x(3))) == 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_x = _mm_set1_epi32(x);\n    __m128i v_y = _mm_set1_epi32(y);\n\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    __m128i v_is_prime = _mm_cmpeq_epi32(v_n, v_one);\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_two));\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_three));\n\n    for (int i = 4; i <= 10; ++i) {\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i v_div = _mm_div_epi32(v_n, v_i);\n        __m128i v_mul = _mm_mul_epi32(v_i, v_div);\n        __m128i v_eq = _mm_cmpeq_epi32(v_n, v_mul);\n        v_is_prime = _mm_andnot_si128(v_eq, v_is_prime);\n    }\n\n    __m128i v_result = _mm_blendv_epi8(v_x, v_y, v_is_prime);\n    int result;\n    _mm_storeu_si128((__m128i*)&result, v_result);\n    return result;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_x = _mm_set1_epi32(x);\n    __m128i v_y = _mm_set1_epi32(y);\n\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    __m128i v_is_prime = _mm_cmpeq_epi32(v_n, v_one);\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_two));\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_three));\n\n    for (int i = 4; i <= 10; ++i) {\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i v_div = _mm_div_epi32(v_n, v_i);\n        __m128i v_mod = _mm_sub_epi32(v_n, _mm_mul_epi32(v_div, v_i));\n        v_is_prime = _mm_andnot_si128(_mm_cmpeq_epi32(v_mod, v_zero), v_is_prime);\n    }\n\n    __m128i v_result = _mm_or_si128(_mm_and_si128(v_is_prime, v_x), _mm_andnot_si128(v_is_prime, v_y));\n    int result;\n    _mm_storeu_si128((__m128i*)&result, v_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m12i v_n = _mm_set1_epi32(n);\n    __m12i v_x = _mm_set1_epi32(x);\n    __m12i v_y = _mm_set1_epi32(y);\n\n    __m12i v_one = _mm_set1_epi32(1);\n    __m12i v_two = _mm_set1_epi32(2);\n    __m12i v_three = _mm_set1_epi32(3);\n\n    __m12i v_is_prime = _mm_cmpeq_epi32(v_n, v_one);\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_two));\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_three));\n\n    for (int i = 0; i < 4; ++i) {\n        int num = _mm_extract_epi32(v_n, i);\n        if (num > 3) {\n            for (int j = 2; j * j <= num; ++j) {\n                if (num % j == 0) {\n                    v_is_prime = _mm_or_si128(v_is_prime, _mm_set1_epi32(i));\n                    break;\n                }\n            }\n        }\n    }\n\n    __m12i v_mask = _mm_andnot_si128(v_is_prime, v_x);\n    v_mask = _mm_or_si128(v_mask, _mm_and_si128(v_is_prime, v_y));\n\n    int result = 0;\n    for (int i = 0; i < 4; ++i) {\n        result |= _mm_extract_epi32(v_mask, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_x = _mm_set1_epi32(x);\n    __m128i v_y = _mm_set1_epi32(y);\n\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_three = _mm_set1_epi32(3);\n\n    __m128i v_is_prime = _mm_cmpeq_epi32(v_n, v_one);\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_two));\n    v_is_prime = _mm_or_si128(v_is_prime, _mm_cmpeq_epi32(v_n, v_three));\n\n    for (int i = 4; i <= 10; ++i) {\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i v_div = _mm_div_epi32(v_n, v_i);\n        __m128i v_mod = _mm_and_si128(_mm_mul_epi32(v_div, v_i), v_n);\n        v_is_prime = _mm_andnot_si128(_mm_cmpeq_epi32(v_mod, v_zero), v_is_prime);\n    }\n\n    __m128i v_result = _mm_or_si128(_mm_and_si128(v_is_prime, v_x), _mm_andnot_si128(v_is_prime, v_y));\n    int result;\n    _mm_storeu_si128((__m128i*)&result, v_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```cpp\nint x_or_y_simd(int n, int x, int y) {\n    __m128i num = _mm_set1_epi32(n);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i three = _mm_set1_epi32(3);\n    __m128i four = _mm_set1_epi32(4);\n\n    __m128i is_prime = _mm_cmpeq_epi32(num, one);\n    is_prime = _mm_or_si128(is_prime, _mm_cmpeq_epi32(num, two));\n    is_prime = _mm_or_si128(is_prime, _mm_cmpeq_epi32(num, three));\n\n    for (int i = 0; i < 10; ++i) {\n        __m128i divisor = _mm_set1_epi32(i + 4);\n        __m128i remainder = _mm_rem_epi32(num, divisor);\n        __m128i zero = _mm_setzero_si128();\n        is_prime = _mm_andnot_si128(_mm_cmpeq_epi32(remainder, zero), is_prime);\n    }\n\n    __m128i result = _mm_or_si128(_mm_and_si128(is_prime, _mm_set1_epi32(x)), _mm_andnot_si128(is_prime, _mm_set1_epi32(y)));\n    return _mm_cvtsi128_si32(result);\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 two = _mm_set1_ps(2.0f);\n\n    size_t n = lst.size();\n    size_t i = 0;\n\n    // Process elements in chunks of 4\n    for (; i <= n - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 mask = _mm_and_ps(vec, _mm_cmpgt_ps(vec, zero));\n        __m128 int_mask = _mm_and_ps(mask, _mm_cmpeq_ps(_mm_floor_ps(vec), vec));\n        __m128 odd_mask = _mm_and_ps(int_mask, _mm_andnot_ps(_mm_and_ps(vec, one), one));\n\n        __m128 squared = _mm_mul_ps(vec, vec);\n        __m128 sum = _mm_add_ps(squared, zero);\n\n        sum = _mm_and_ps(sum, odd_mask);\n        _mm_storeu_si128((__m128i*)&result, _mm_add_epi64(_mm_cvtps_epi32(sum), _mm_cvtps_epi32(_mm_movehl_ps(zero, sum))));\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (lst[i] > 0 && fmod(lst[i], 1.0f) == 0 && static_cast<int>(lst[i]) % 2 != 0) {\n            result += static_cast<uint64_t>(pow(lst[i], 2));\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 4;\n    uint64_t result = 0;\n\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    __m128 two_vec = _mm_set1_ps(2.0f);\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 abs_vec = _mm_andnot_ps(_mm_cmpgt_ps(vec, zero_vec), vec);\n        __m128 floor_vec = _mm_floor_ps(abs_vec);\n        __m128 diff_vec = _mm_sub_ps(abs_vec, floor_vec);\n        __m128 is_int_vec = _mm_cmpeq_ps(diff_vec, zero_vec);\n        __m128 is_odd_vec = _mm_and_ps(is_int_vec, _mm_cmpeq_epi32(_mm_cvtps_epi32(floor_vec), _mm_slli_epi32(_mm_cvtps_epi32(floor_vec), 1)));\n        __m128 square_vec = _mm_mul_ps(floor_vec, floor_vec);\n        __m128 sum_vec = _mm_add_ps(square_vec, zero_vec);\n\n        __m128i mask = _mm_castps_si128(is_odd_vec);\n        uint32_t mask_val = _mm_movemask_epi8(mask);\n\n        if (mask_val != 0) {\n            float *sum_ptr = reinterpret_cast<float*>(&sum_vec);\n            for (int j = 0; j < vec_size; ++j) {\n                if ((mask_val >> (j * 4)) & 0xF) {\n                    result += static_cast<uint64_t>(sum_ptr[j]);\n                }\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 two = _mm_set1_ps(2.0f);\n\n    for (size_t i = 0; i <= lst.size() - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 abs_vec = _mm_andnot_ps(_mm_cmplt_ps(vec, zero), vec);\n        __m128 int_mask = _mm_and_ps(abs_vec, _mm_set1_ps(0.5f));\n        __m128 is_odd = _mm_cmpneq_ps(int_mask, zero);\n        __m128 square = _mm_mul_ps(abs_vec, abs_vec);\n        __m128 sum = _mm_add_ps(square, zero);\n        sum = _mm_and_ps(sum, is_odd);\n        result += _mm_cvtss_si64(_mm_hadd_ps(sum, sum));\n    }\n\n    // Handle remaining elements\n    for (size_t i = lst.size() - lst.size() % 4; i < lst.size(); ++i) {\n        if (lst[i] >= 0 && fmod(lst[i], 2.0f) != 0) {\n            result += static_cast<uint64_t>(pow(lst[i], 2));\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t result = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 two = _mm_set1_ps(2.0f);\n\n    for(size_t i = 0; i <= lst.size() - 4; i += 4){\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 mask = _mm_and_ps(vec, _mm_cmpgt_ps(vec, zero));\n        __m128 int_mask = _mm_and_ps(mask, _mm_cmpeq_ps(_mm_floor_ps(vec), vec));\n        __m128 odd_mask = _mm_and_ps(int_mask, _mm_andnot_ps(_mm_and_ps(vec, one), one));\n\n        __m128 squared = _mm_mul_ps(vec, vec);\n        __m128 sum = _mm_add_ps(squared, zero);\n\n        sum = _mm_and_ps(sum, odd_mask);\n        result += _mm_cvtss_si64(_mm_hadd_ps(sum, sum));\n    }\n\n    for(size_t i = lst.size() - lst.size() % 4; i < lst.size(); ++i){\n        if(lst[i] > 0 && floor(lst[i]) == lst[i] && fmod(lst[i], 2.0f) != 0){\n            result += static_cast<uint64_t>(pow(lst[i], 2));\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    __m128 sum = _mm_setzero_ps();\n    size_t n = lst.size();\n    size_t i = 0;\n\n    // Process elements in chunks of 4\n    while (i + 4 <= n) {\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 mask = _mm_and_ps(vec, _mm_set1_ps(0x7FFFFFFF)); // Remove sign bit\n        __m128 is_odd = _mm_and_ps(_mm_sub_ps(mask, _mm_and_ps(mask, _mm_set1_ps(2))), _mm_set1_ps(1));\n        __m128 squares = _mm_mul_ps(vec, vec);\n        __m128 result = _mm_and_ps(squares, is_odd);\n        sum = _mm_add_ps(sum, result);\n        i += 4;\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (lst[i] >= 0 && fmod(lst[i], 2.0f) != 0) {\n            sum = _mm_add_ss(sum, _mm_set_ss(lst[i] * lst[i]));\n        }\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, sum);\n    return static_cast<uint64_t>(result[0] + result[1] + result[2] + result[3]);\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m12i v_game = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&game[i]));\n        __m12i v_guess = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&guess[i]));\n\n        __m12i v_diff = _mm_abs_epi32(_mm_sub_epi32(v_game, v_guess));\n        _mm_storeu_si128(reinterpret_cast<__m12i*>(&result[i]), v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 4; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    std::vector<int> result(game.size());\n    __m128i* pGame = (__m128i*)game.data();\n    __m128i* pGuess = (__m128i*)guess.data();\n    __m128i* pResult = (__m128i*)result.data();\n\n    int i = 0;\n    for (; i <= game.size() - 4; i += 4) {\n        __m128i vGame = _mm_loadu_si128(pGame + i);\n        __m128i vGuess = _mm_loadu_si128(pGuess + i);\n        __m128i vDiff = _mm_abs_epi32(_mm_sub_epi32(vGame, vGuess));\n        _mm_storeu_si128(pResult + i, vDiff);\n    }\n\n    for (; i < game.size(); ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m12i v_game = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&game[i]));\n        __m12i v_guess = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&guess[i]));\n\n        __m12i v_diff = _mm_abs_epi32(_mm_sub_epi32(v_game, v_guess));\n        _mm_storeu_si128(reinterpret_cast<__m12i*>(&result[i]), v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 4; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    __m128i* pGame = (__m128i*)game.data();\n    __m128i* pGuess = (__m128i*)guess.data();\n    __m128i* pResult = (__m128i*)result.data();\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m128i vGame = _mm_loadu_si128(pGame + i);\n        __m128i vGuess = _mm_loadu_si128(pGuess + i);\n        __m128i vDiff = _mm_abs_epi32(_mm_sub_epi32(vGame, vGuess));\n        _mm_storeu_si128(pResult + i, vDiff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 4; i < n; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    for (int i = 0; i <= n - 4; i += 4) {\n        __m12i v_game = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&game[i]));\n        __m12i v_guess = _mm_loadu_si128(reinterpret_cast<const __m12i*>(&guess[i]));\n\n        __m12i v_diff = _mm_abs_epi32(_mm_sub_epi32(v_game, v_guess));\n        _mm_storeu_si128(reinterpret_cast<__m12i*>(&result[i]), v_diff);\n    }\n\n    // Handle remaining elements\n    for (int i = n - n % 4; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}

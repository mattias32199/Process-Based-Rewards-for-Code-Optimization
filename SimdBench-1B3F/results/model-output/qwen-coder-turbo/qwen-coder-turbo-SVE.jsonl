{"task_id": "SimdBench_0_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    svbool_t p = svwhilelt_b64(length);\n    for (; !svptestall(p); p = svwhilele_b64(svadd_z_p_zz(svptrue_b64(), p, svdup_n_u32(1)))) {\n        svint64_t vsrc = svld1_m_z(svptrue_b64(), src, p);\n        svint64_t vmask = svreinterpret_m_z(svreinterpret_m_b64(svand_m_b64(p, svreinterpret_m_b64(svld1_m_z(svptrue_b64(), mask, p)))));\n        svst1_m_z(svptrue_b64(), dst, vmask, vsrc);\n        src += svcntp_b64(vmask);\n        dst += svcntp_b64(vmask);\n    }\n}\n```"}
{"task_id": "SimdBench_0_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    svbool_t p = svwhilelt_b64(length);\n    for (; !svptest_any(p); p = svwhilelt_b64(svadd_z_u64(svptrue_b64(), p))) {\n        svint64_t vsrc = svld1_m_z(src, p);\n        svint64_t vmask = svreinterpret_m_b64(svld1_m_z(mask, p));\n        svint64_t vdst = svld1_m_z(dst, p);\n        vdst = svcmov_m(vdst, vmask, vsrc);\n        svst1_m_z(dst, p, vdst);\n        src += svcntp_b64(p);\n        dst += svcntp_b64(p);\n        mask += svcntp_b64(p);\n    }\n}\n```"}
{"task_id": "SimdBench_0_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    svbool_t p = svptrue_b64();\n    size_t i = 0;\n\n    while (i + SV_INT64VL <= length) {\n        svint64_t vsrc = svld1_m(svptrue_b64(), src + i);\n        svbfloat64_t vmask = svreinterpret_bf64_u8(svld1_m(p, (uint8_t *)(mask + i)));\n        svint64_t vdst = svcmov_m(vmask, vsrc, svzero_s64());\n        svst1_m(p, vdst, dst + i);\n        i += SV_INT64VL;\n    }\n\n    if (i < length) {\n        svint64_t vsrc = svld1_m(svptrue_b64(), src + i);\n        svbfloat64_t vmask = svreinterpret_bf64_u8(svld1_m(p, (uint8_t *)(mask + i)));\n        svint64_t vdst = svcmov_m(vmask, vsrc, svzero_s64());\n        svst1_m(p, vdst, dst + i);\n    }\n}\n```"}
{"task_id": "SimdBench_0_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    svbool_t p = svptrue_b64();\n    size_t i = 0;\n\n    while (i + SV_INT64_SIZE <= length) {\n        svint64_t vsrc = svld1_m(svptrue_b64(), src + i);\n        svint64_t vmask = svld1_m(svptrue_b64(), (const svbool_t *)(mask + i));\n        svint64_t vdst = svld1_m(svptrue_b64(), dst + i);\n\n        vdst = svcmov_m(p, vmask, vsrc, vdst);\n\n        svst1_m(svptrue_b64(), dst + i, vdst);\n        i += SV_INT64_SIZE;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    svbool_t pg = svptrue_b64();\n    size_t i = 0;\n\n    while (i + SV_INT64_SIZE <= length) {\n        svint64_t vsrc = svld1_m(svptrue_b64(), src + i);\n        svbfloat64_t vmask = svreinterpret_bf64_u8(svld1_m(pg, (uint8_t *)(mask + i)));\n        svint64_t vdst = svcmov_m(pg, vdst, vsrc, vmask);\n        svst1_m(pg, dst + i, vdst);\n        i += SV_INT64_SIZE;\n    }\n\n    if (i < length) {\n        svint64_t vsrc = svld1_m(svptrue_b64(), src + i);\n        svbfloat64_t vmask = svreinterpret_bf64_u8(svld1_m(pg, (uint8_t *)(mask + i)));\n        svint64_t vdst = svcmov_m(pg, vdst, vsrc, vmask);\n        svst1_m(pg, dst + i, vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    svint32_t v_scale = svdup_n_s32(scale);\n    svint32_t v_shift_3 = svdup_n_s32(3);\n    svint32_t v_divisor = svdup_n_s32(7);\n\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svint32_t v_src = svld1(svptrue_b32(), src + i);\n        svint32_t v_mul = svmul_n_s32(v_src, scale);\n        svint32_t v_tmp1 = svrshl_n_s32(v_mul, v_shift_3);\n        svint32_t v_tmp2 = svlsl_n_s32(v_mul, v_shift_3);\n        svint32_t v_result = svdiv_s32(svadd_s32(v_mul, svmul_s32(v_tmp1, v_tmp2)), v_divisor);\n        svst1(svptrue_b32(), dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    svint32_t v_scale = svdup_n_s32(scale);\n    svuint32_t v_length = svdup_n_u32(length);\n\n    for (size_t i = 0; i < length; i += SV_INT32_ELEMS) {\n        svint32_t v_src = svld1_s32(src + i);\n        svint32_t v_tmp1 = svrshl_s32(svsmulh_s32(v_src, v_scale), svdup_n_s32(3));\n        svint32_t v_tmp2 = svlsl_s32(svsmulh_s32(v_src, v_scale), svdup_n_s32(3));\n        svint32_t v_result = svdiv_s32(svadd_s32(svsmulh_s32(v_src, v_scale), svmul_s32(v_tmp1, v_tmp2)), svdup_n_s32(7));\n        svst1_s32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    svint32_t v_scale = svdup_n_s32(scale);\n    svuint32_t v_length = svdup_n_u32(length);\n\n    for (size_t i = 0; i < length; i += SV_INT32_ELEMS) {\n        svint32_t v_src = svld1(svptrue_b32(), src + i);\n        svint32_t v_tmp1 = svrshl(v_src, v_scale, 3);\n        svint32_t v_tmp2 = svlsl(v_src, v_scale, 3);\n        svint32_t v_result = svdiv(svadd(v_src, svmul(v_tmp1, v_tmp2)), 7);\n        svst1(svptrue_b32(), dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    svint32_t v_scale = svdup_n_s32(scale);\n    svuint32_t v_length = svdup_n_u32(length);\n\n    for (size_t i = 0; i < length; i += SV_INT32_SIZE) {\n        svint32_t v_src = svld1_s32(src + i);\n        svint32_t v_tmp1 = svrshl_s32(svsmulh_s32(v_src, v_scale), svdup_n_s32(3));\n        svint32_t v_tmp2 = svlsl_s32(svsmulh_s32(v_src, v_scale), svdup_n_s32(3));\n        svint32_t v_result = svdiv_s32(svadd_s32(svsmulh_s32(v_src, v_scale), svmul_s32(v_tmp1, v_tmp2)), svdup_n_s32(7));\n        svst1_s32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_1_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    svint32_t v_scale = svdup_n_s32(scale);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint32_t v_src = svld1_u32(src + i);\n\n        svint32_t v_tmp1 = svrshl_s32(svsmulh_s32(v_src, v_scale), 3);\n        svint32_t v_tmp2 = svslal_s32(svsmulh_s32(v_src, v_scale), 3);\n        svint32_t v_result = svdiv_s32(svadd_s32(svsmulh_s32(v_src, v_scale), svmul_s32(v_tmp1, v_tmp2)), 7);\n\n        svst1_u32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    svfloat64_t vstride = svdup_f64(stride);\n    for (size_t r = 0; r < rows; ++r) {\n        size_t index = r * cols + stride;\n        svfloat64_t vload = svld1_f64(src + index);\n        svst1_f64(dst + r, vload);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    svfloat64_t vstride = svdup_n_f64(stride);\n    for (size_t r = 0; r < rows; ++r) {\n        size_t idx = r * cols + stride;\n        svfloat64_t vval = svld1_f64(src + idx);\n        svst1_f64(dst + r, vval);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    svfloat64_t vstride = svdup_n_f64(stride);\n    for (size_t r = 0; r < rows; ++r) {\n        svfloat64_t vindex = svdup_n_f64(r * cols);\n        vindex = svadd_f64(vindex, vstride);\n        svfloat64_t vvalue = svld1_f64(src + svreinterpret_u32_u64(svuzp1_u32(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret_u32_u64(svreinterpret"}
{"task_id": "SimdBench_2_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    svfloat64_t vstride = svdup_n_f64(stride);\n    for (size_t r = 0; r < rows; ++r) {\n        size_t index = r * cols + stride;\n        svfloat64_t vval = svld1_f64(src + index);\n        svst1_f64(dst + r, vval);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    svfloat64_t vstride = svdup_n_f64(stride);\n    for (size_t r = 0; r < rows; ++r) {\n        svfloat64_t vindex = svadd_x_f64(svreinterpret_u64_z_f64(svdup_n_f64(r)), vstride);\n        svfloat64_t vvalue = svld1_f64(src + svuzp1_u32_z_u32(svptrue_b64(), svreinterpret_u32_z_f64(vindex)));\n        svst1_f64(dst + r, vvalue);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    svfloat32_t v_src;\n    svint32_t v_indices;\n    svfloat32_t v_dst;\n\n    for (size_t i = 0; i < length; i += svcntw()) {\n        v_src = svld1_f32(src + i);\n        v_indices = svld1_s32(indices + i);\n\n        // Gather operation\n        v_dst = svindex_f32(v_indices, v_src);\n\n        // Scatter operation\n        svst1_f32(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    svfloat32_t v_src;\n    svint32_t v_indices;\n    svfloat32_t v_dst;\n\n    for (size_t i = 0; i < length; i += SV_INT32_COUNT) {\n        v_indices = svld1_u32(indices + i);\n        v_src = svld1_f32(src + i);\n\n        // Gather operation\n        v_dst = svindex_f32(v_indices, src, SV_INT32_COUNT);\n\n        // Scatter operation\n        svst1_f32(dst + i, v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    svfloat32_t v_src;\n    svint32_t v_indices;\n    svfloat32_t v_dst;\n\n    for (size_t i = 0; i < length; i += svcntw()) {\n        v_indices = svld1_u32(indices + i);\n        v_src = svld1_f32(src + i);\n\n        svst1_gather_f32(dst + i, v_indices, v_src);\n        svst1_scatter_f32(dst + i, v_indices, v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    svfloat32_t v_src;\n    svint32_t v_indices;\n    svfloat32_t v_dst;\n\n    for (size_t i = 0; i < length; i += SV_INT32_COUNT) {\n        v_indices = svld1_u32(indices + i);\n        v_src = svld1_f32(src + i);\n\n        // Gather operation\n        v_dst = svindex_f32(v_indices, src, SV_INT32_COUNT);\n\n        // Scatter operation\n        svst1_f32(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    svfloat32_t v_src;\n    svint32_t v_indices;\n    svfloat32_t v_dst;\n\n    for (size_t i = 0; i < length; i += SV_INT32_ELEMS) {\n        v_indices = svld1_u32(indices + i);\n        v_src = svld1_f32(src + i);\n\n        // Gather operation\n        v_dst = svindex_f32(v_indices, src, SV_GATHER);\n\n        // Scatter operation\n        svst1_f32(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_4_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    svbool_t p = svptrue_b16();\n    int16_t *begin = arr + start;\n    int16_t *end_ptr = arr + end;\n\n    while (begin < end_ptr) {\n        svint16x8_t v1 = svld1_x8(p, begin);\n        svint16x8_t v2 = svld1_x8(p, end_ptr);\n\n        svst1_x8(p, begin, v2);\n        svst1_x8(p, end_ptr, v1);\n\n        begin += 8;\n        end_ptr -= 8;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    svint16_t vzero = svdup_n_s16(0);\n    svbool_t pg = svwhile_lt_b32(start, end);\n\n    while (pg) {\n        svuint64_t idx_start = svreinterpret_u64(pg, svld1_u64(&start));\n        svuint64_t idx_end = svreinterpret_u64(pg, svld1_u64(&end));\n\n        svint16_t vload_start = svld1_s16(arr + idx_start);\n        svint16_t vload_end = svld1_s16(arr + idx_end);\n\n        svint16_t vrev = svrev16(vload_start);\n        svst1_s16(arr + idx_start, vrev);\n\n        svst1_s16(arr + idx_end, vload_end);\n\n        start++;\n        end--;\n        pg = svwhile_lt_b32(start, end);\n    }\n}\n```"}
{"task_id": "SimdBench_4_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    svbool_t p = svptrue_b16();\n    int16_t *base = arr + start;\n    int16_t *left = base;\n    int16_t *right = base + (end - start);\n\n    while (left < right) {\n        svint16x8_t vleft = svld1q_m(p, left, svwhilelt_b16(svindex_u16(0, 8), svsub_u16(svreinterpret_u16(left), svreinterpret_u16(base))));\n        svint16x8_t vright = svld1q_m(p, right, svwhilelt_b16(svindex_u16(0, 8), svsub_u16(svreinterpret_u16(right), svreinterpret_u16(base))));\n        svst1q_m(p, left, svrev32q(vright));\n        svst1q_m(p, right, svrev32q(vleft));\n        left += 8;\n        right -= 8;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    svbool_t p = svptrue_b16();\n    int16_t *begin = arr + start;\n    int16_t *end_ptr = arr + end;\n    int16_t *mid = begin + ((end - start + 1) / 2);\n\n    while (begin < mid && end_ptr > mid) {\n        svint16x8_t v1 = svld1_x8(p, begin);\n        svint16x8_t v2 = svld1_x8(p, end_ptr);\n        svst1_x8(p, begin, v2);\n        svst1_x8(p, end_ptr, v1);\n        begin += 8;\n        end_ptr -= 8;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    svint16_t vzero = svdup_n_s16(0);\n    svbool_t pg = svwhile_lt_b32(start, end);\n\n    while (pg) {\n        svint16_t vload_start = svld1(pg, arr + start);\n        svint16_t vload_end = svld1(pg, arr + end);\n\n        svint16_t vswap = svext_x(pg, vload_start, vload_end, 8);\n\n        svst1(pg, arr + start, vswap);\n\n        start++;\n        end--;\n        pg = svwhile_lt_b32(start, end);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    svuint8_t *sv_tensor = (svuint8_t *)tensor;\n    svuint8_t *sv_slice = (svuint8_t *)slice;\n\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t vector_size = svcntw();\n\n    for (size_t i = 0; i < dim1; i++) {\n        for (size_t j = 0; j < dim2; j += vector_size) {\n            svuint8_t vec = svld1_u8(svaddressof(sv_tensor, slice_offset + i * dim2 * dim3 + j));\n            svst1_u8(svaddressof(sv_slice, i * dim2 + j), vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    svuint8_t *sv_tensor = (svuint8_t *)tensor;\n    svuint8_t *sv_slice = (svuint8_t *)slice;\n\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t vector_size = svcntw();\n\n    for (size_t i = 0; i < dim1 * dim2; i += vector_size) {\n        svuint8_t vec = svld1(vector_size, sv_tensor + slice_offset + i);\n        svst1(vector_size, sv_slice + i, vec);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    svuint8_t *sv_tensor = (svuint8_t *)tensor;\n    svuint8_t *sv_slice = (svuint8_t *)slice;\n\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t vector_size = svcntw();\n\n    for (size_t i = 0; i < dim1; i += vector_size) {\n        svuint8_t vec = svld1_u8(svaddressof(sv_tensor, slice_offset + i * dim2));\n        svst1_u8(svaddressof(sv_slice, i * dim2), vec);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    svuint8_t *sv_tensor = (svuint8_t *)tensor;\n    svuint8_t *sv_slice = (svuint8_t *)slice;\n\n    size_t offset = slice_idx * dim1 * dim2;\n    svuint8_t indices = sv_dup_u8_z(svptrue_b8(), offset);\n\n    for (size_t i = 0; i < dim1; i++) {\n        svuint8_t idx = sv_add_x_u8(indices, svdup_u8(i * dim2));\n        svuint8_t row = svld1_gather_offset_z(svbool_t(), sv_tensor, idx, dim2 * sizeof(uint8_t), SV_MEM_MASK_FULL);\n        svst1_u8(slice + i * dim2, row);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    svuint8_t *sv_tensor = (svuint8_t *)tensor;\n    svuint8_t *sv_slice = (svuint8_t *)slice;\n\n    size_t offset = slice_idx * dim1 * dim2;\n    svuint8_t idx = svdup_u8(offset);\n\n    for (size_t i = 0; i < dim1; i++) {\n        svuint8_t row_offset = svadd_x_u8(idx, svdup_u8(i * dim2));\n        for (size_t j = 0; j < dim2; j += SV_INT8_SIZE) {\n            svuint8_t row_data = svld1_u8(row_offset);\n            svst1_u8(sv_slice + j, row_data);\n            row_offset = svadd_x_u8(row_offset, svdup_u8(SV_INT8_SIZE));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    svint16_t v0, v1, v2, v3;\n    size_t i, j, k;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; k++) {\n                v0 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j], svptrue_b16());\n                v1 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j + block_size], svptrue_b16());\n                v2 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j + 2 * block_size], svptrue_b16());\n                v3 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j + 3 * block_size], svptrue_b16());\n\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k], v0, svptrue_b16());\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k + block_size], v1, svptrue_b16());\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k + 2 * block_size], v2, svptrue_b16());\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k + 3 * block_size], v3, svptrue_b16());\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    svint16_t v0, v1, v2, v3;\n    size_t i, j, k;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; k++) {\n                v0 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j]);\n                v1 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + block_size]);\n                v2 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + 2 * block_size]);\n                v3 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + 3 * block_size]);\n\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k], v0);\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k + block_size], v1);\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k + 2 * block_size], v2);\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k + 3 * block_size], v3);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    svint16_t v0, v1, v2, v3;\n    size_t i, j, k;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; k++) {\n                v0 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j]);\n                v1 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + block_size]);\n                v2 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + 2 * block_size]);\n                v3 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + 3 * block_size]);\n\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k], v0);\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k + block_size], v1);\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k + 2 * block_size], v2);\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k + 3 * block_size], v3);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    svint16_t v0, v1, v2, v3;\n    size_t i, j, k;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; k++) {\n                v0 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j]);\n                v1 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + block_size]);\n                v2 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + 2 * block_size]);\n                v3 = svld1h_z(svptrue_b16(), &src[(i + k) * cols + j + 3 * block_size]);\n\n                svst1h_z(svptrue_b16(), &dst[j * rows + i + k], v0);\n                svst1h_z(svptrue_b16(), &dst[(j + block_size) * rows + i + k], v1);\n                svst1h_z(svptrue_b16(), &dst[(j + 2 * block_size) * rows + i + k], v2);\n                svst1h_z(svptrue_b16(), &dst[(j + 3 * block_size) * rows + i + k], v3);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    svint16_t v0, v1, v2, v3;\n    size_t i, j, k;\n\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = 0; k < block_size; k++) {\n                v0 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j], svptrue_b16());\n                v1 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j + block_size], svptrue_b16());\n                v2 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j + 2 * block_size], svptrue_b16());\n                v3 = svld1h_m(svptrue_b16(), &src[(i + k) * cols + j + 3 * block_size], svptrue_b16());\n\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k], v0, svptrue_b16());\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k + block_size], v1, svptrue_b16());\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k + 2 * block_size], v2, svptrue_b16());\n                svst1h_m(svptrue_b16(), &dst[j * rows + i + k + 3 * block_size], v3, svptrue_b16());\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SVE", "completion": "```c\n#include <arm_sve.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    svfloat32_t sum = svdup_f32(0.0f);\n    size_t num_elements = dim * dim * dim;\n\n    for (size_t i = 0; i < num_elements; i += SV_INT32_ELEMS) {\n        svint32_t idx = svld1_u32((const int32_t *)&i);\n        svbool_t mask = svcmplt_z_i32(idx, svdup_u32(dim));\n        svint32_t valid_idx = svsel(mask, idx, svdup_u32(-1));\n        svfloat32_t val = svld1_f32(array + svreinterpret_u32(valid_idx));\n        sum = svadd_f32(sum, val);\n    }\n\n    return svget_f32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_7_SVE", "completion": "```c\n#include <arm_sve.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    svfloat32_t sum = svdup_f32(0.0f);\n    size_t num_elements = dim * dim * dim;\n\n    for (size_t i = 0; i < num_elements; i += svcntw()) {\n        svuint32_t idx = svindex_u32(svcntw(), i);\n        svbool_t mask = svcmplt_b32(idx, svdup_u32(dim));\n        svfloat32_t diag_elements = svld1_f32(array + i, svzip2(mask, idx));\n        sum = svadd_f32(sum, diag_elements);\n    }\n\n    return svget_f32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_7_SVE", "completion": "```c\n#include <arm_sve.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    svfloat32_t sum = svdup_f32(0.0f);\n    size_t num_elements = dim * dim * dim;\n\n    for (size_t i = 0; i < num_elements; i += SV_INT32_ELEMS) {\n        svint32_t idx = svld1_u32((const int32_t *)&i);\n        svbool_t mask = svcmplt_z_i32(idx, svdup_u32(dim));\n        svint32_t valid_idx = svsel(mask, idx, svdup_u32(-1));\n        svfloat32_t value = svld1_f32(array + valid_idx);\n        sum = svadd_f32(sum, value);\n    }\n\n    return svget_f32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_7_SVE", "completion": "```c\n#include <arm_sve.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    svfloat32_t sum = svdup_n_f32(0.0f);\n    size_t i = 0;\n\n    while (i + sve_cntb_u8(svptrue_b32()) <= dim * dim * dim) {\n        svuint32_t idx = svld1_u32((const uint32_t *)&i);\n        svfloat32_t val = svld1_f32(array + idx);\n        sum = svadd_f32(sum, val);\n        i += sve_cntb_u8(svptrue_b32());\n    }\n\n    return svget_lane_f32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_7_SVE", "completion": "```c\n#include <arm_sve.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    svfloat32_t sum = svdup_n_f32(0.0f);\n    size_t n = dim * dim * dim;\n\n    for (size_t i = 0; i < n; i += SV_INT32_ELEMS) {\n        svfloat32_t vec = svld1_u32((uint32_t *)&array[i]);\n        svfloat32_t mask = svcmple_z_f32(svindex_u32(SV_INT32_ELEMS), svdup_n_u32(dim - 1));\n        vec = svsel(mask, vec, svzero_f32());\n        sum = svadd_f32(sum, vec);\n    }\n\n    return svget_f32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_8_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    svfloat64_t v_scale = svdup_f64(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        for (; j <= cols - 2; j += 2) {\n            svfloat64_t v_src = svld1_f64(src + i * cols + j);\n            svbool_t mask = svcmpgt_f64(v_src, v_threshold);\n            svfloat64_t v_scaled = svcmul_f64(mask, v_src, v_scale);\n            svst1_f64(dst + i * cols + j, v_scaled);\n        }\n        if (j < cols) {\n            svfloat64_t v_src = svld1_lane_f64(src + i * cols + j, svzero_f64(), 0);\n            svbool_t mask = svcmpgt_f64(v_src, v_threshold);\n            svfloat64_t v_scaled = svcmul_f64(mask, v_src, v_scale);\n            svst1_lane_f64(dst + i * cols + j, v_scaled, 0);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    svfloat64_t v_scale = svdup_f64(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t v_src = svld1_f64(src + i * cols);\n        svbool_t mask = svcmpgt_f64(v_src, v_threshold);\n        svfloat64_t v_scaled = svcmul_f64(mask, v_src, v_scale);\n        svst1_f64(dst + i * cols, v_scaled);\n    }\n}\n```"}
{"task_id": "SimdBench_8_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    svfloat64_t v_scale = svdup_f64(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t v_src = svld1_f64(src + i * cols);\n        svbool_t mask = svcmpgt_f64(v_src, v_threshold);\n        svfloat64_t v_scaled = svcmul_f64(mask, v_src, v_scale);\n        svst1_f64(dst + i * cols, v_scaled);\n    }\n}\n```"}
{"task_id": "SimdBench_8_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    svfloat64_t v_scale = svdup_f64(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t v_src = svld1_f64(src + i * cols);\n        svbool_t mask = svcmpgt_f64(v_src, v_threshold);\n        svfloat64_t v_scaled = svmul_f64(v_src, v_scale);\n        svst1_f64(dst + i * cols, svsel(mask, v_scaled, v_src));\n    }\n}\n```"}
{"task_id": "SimdBench_8_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    svfloat64_t v_scale = svdup_f64(scale);\n\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t v_src = svld1_f64(src + i * cols);\n        svbool_t mask = svcmpgt_f64(v_src, v_threshold);\n        svfloat64_t v_scaled = svmul_f64(v_src, v_scale);\n        svst1_f64(dst + i * cols, svsel(mask, v_scaled, v_src));\n    }\n}\n```"}
{"task_id": "SimdBench_9_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    svfloat64_t v_src[2];\n    svuint32_t v_indices[2];\n    svbool_t mask;\n\n    for (size_t i = 0; i < rows; i += 2) {\n        v_indices[0] = svld1_u32(&indices[i]);\n        v_indices[1] = svld1_u32(&indices[i + 1]);\n\n        for (size_t j = 0; j < cols; j += 2) {\n            v_src[0] = svld1_f64(&src[v_indices[0][0] * cols + j]);\n            v_src[1] = svld1_f64(&src[v_indices[0][1] * cols + j]);\n            svst1_f64(&dst[i * cols + j], v_src[0]);\n            svst1_f64(&dst[(i + 1) * cols + j], v_src[1]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    svfloat64_t v_src[2];\n    svfloat64_t v_dst[2];\n    svuint32_t v_indices[2];\n\n    for (size_t i = 0; i < rows; i += 2) {\n        v_indices[0] = indices[i];\n        v_indices[1] = indices[i + 1];\n\n        v_src[0] = svld1_f64(src + v_indices[0] * cols);\n        v_src[1] = svld1_f64(src + v_indices[1] * cols);\n\n        svst1_f64(dst + i * cols, v_src[0]);\n        svst1_f64(dst + (i + 1) * cols, v_src[1]);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    svfloat64_t v0, v1, v2, v3;\n    svuint32_t vidx;\n\n    for (size_t i = 0; i < rows; i += 4) {\n        vidx = svld1_u32(indices + i);\n\n        v0 = svld1_f64(src + (vidx[0] * cols));\n        v1 = svld1_f64(src + (vidx[1] * cols));\n        v2 = svld1_f64(src + (vidx[2] * cols));\n        v3 = svld1_f64(src + (vidx[3] * cols));\n\n        svst1_f64(dst + (i * cols), v0);\n        svst1_f64(dst + ((i + 1) * cols), v1);\n        svst1_f64(dst + ((i + 2) * cols), v2);\n        svst1_f64(dst + ((i + 3) * cols), v3);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    svfloat64_t v_src[2];\n    svuint32_t v_indices[2];\n    svbool_t p = svptrue_b64();\n\n    for (size_t i = 0; i < rows; i += 2) {\n        v_indices[0] = svdup_u32(indices[i]);\n        v_indices[1] = svdup_u32(indices[i + 1]);\n\n        v_src[0] = svld1_f64(src + v_indices[0] * cols);\n        v_src[1] = svld1_f64(src + v_indices[1] * cols);\n\n        svst1_f64(dst + i * cols, v_src[0], p);\n        svst1_f64(dst + (i + 1) * cols, v_src[1], p);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    svfloat64_t v_src[2];\n    svfloat64_t v_dst[2];\n    svuint32_t v_indices[2];\n\n    for (size_t i = 0; i < rows; i += 2) {\n        v_indices[0] = svdup_u32(indices[i]);\n        v_indices[1] = svdup_u32(indices[i + 1]);\n\n        v_src[0] = svld1_f64(src + i * cols);\n        v_src[1] = svld1_f64(src + (i + 1) * cols);\n\n        svst1_f64(dst + v_indices[0] * cols, v_src[0]);\n        svst1_f64(dst + v_indices[1] * cols, v_src[1]);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    svint64_t v_scalar = svdup_n_s64(scalar);\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        svint64_t v_src1 = svld1_s64(src1 + i);\n        svint64_t v_src2 = svld1_s64(src2 + i);\n        svint64_t v_result = svadd_s64(v_src1, v_src2);\n        v_result = svadd_s64(v_result, v_scalar);\n        v_result = svrshrq_s64(v_result, 1);\n        svst1_s64(dst + i, v_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    svint64_t v_src1, v_src2, v_dst, v_scalar;\n    size_t i;\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v_src1 = svld1_m(svptrue_b64(), src1 + i, svptrue_b64());\n        v_src2 = svld1_m(svptrue_b64(), src2 + i, svptrue_b64());\n        v_scalar = svdup_n_s64(scalar);\n\n        v_dst = svaddl_x(v_src1, v_src2);\n        v_dst = svaddl_x(v_dst, v_scalar);\n        v_dst = svshrq_x(v_dst, 1);\n\n        svst1_m(svptrue_b64(), dst + i, v_dst, svptrue_b64());\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    svint64_t v_scalar = svdup_n_s64(scalar);\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        svint64_t v_src1 = svld1_m_s64(svbool_t(), src1 + i);\n        svint64_t v_src2 = svld1_m_s64(svbool_t(), src2 + i);\n        svint64_t v_result = svaddl_m_s64(svbool_t(), v_src1, v_src2);\n        v_result = svaddl_m_s64(svbool_t(), v_result, v_scalar);\n        v_result = svrshrq_m_s64(svbool_t(), v_result, 1);\n        svst1_m_s64(svbool_t(), dst + i, v_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    svint64_t v_scalar = svdup_n_s64(scalar);\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        svint64_t v_src1 = svld1_s64(src1 + i);\n        svint64_t v_src2 = svld1_s64(src2 + i);\n        svint64_t v_result = svaddl_s64(v_src1, v_src2);\n        v_result = svadd_s64(v_result, v_scalar);\n        v_result = svrshrq_n_s64(v_result, 1);\n        svst1_s64(dst + i, v_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    svint64_t v_scalar = svdup_n_s64(scalar);\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        svint64_t v_src1 = svld1_m_s64(svbool_t(), src1 + i, svptrue_b64());\n        svint64_t v_src2 = svld1_m_s64(svbool_t(), src2 + i, svptrue_b64());\n        svint64_t v_result = svaddl_m_s64(svbool_t(), v_src1, v_src2, svptrue_b64());\n        v_result = svadd_m_s64(v_result, v_scalar, svptrue_b64());\n        v_result = svdiv_m_s64(v_result, svconst_i64(2), svptrue_b64());\n        svst1_m_s64(svbool_t(), dst + i, v_result, svptrue_b64());\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_SVE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    svbool_t p = svptrue_b32();\n    svint32_t min_val = svdup_n_s32(INT_MAX);\n\n    for (size_t i = 0; i < length; i += 8) {\n        svint32_t vec = svld1_u32(src + i);\n        svbool_t is_even = svcmple_z_s32(svrextw_s32(vec), svdup_n_s32(0));\n        svint32_t even_vals = svsel(is_even, vec, svdup_n_s32(0));\n        min_val = svinvalb(min_val, svminv_m(p, min_val, even_vals));\n    }\n\n    return svfirst_m(p, min_val);\n}\n```"}
{"task_id": "SimdBench_11_SVE", "completion": "```c\n#include <arm_sve.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    svbool_t p = svptrue_b32();\n    svuint32_t min_val = svdup_u32(INT_MAX);\n    svuint32_t indices = svindex_u32(length);\n\n    for (size_t i = 0; i < length; i += SV_INT32_SIZE) {\n        svuint32_t vec = svld1_u32(src + i);\n        svuint32_t mask = svand_u32(svcmpgt_z_u32(indices, svdup_u32(i)), svcmpeq_z_u32(svmod_u32(indices, svdup_u32(2)), svdup_u32(0)));\n        min_val = svinval_u32(min_val, svmin_u32(min_val, svsel(mask, vec, min_val)));\n    }\n\n    return svfirst_m_b32(p, svcmpne_z_u32(min_val, svdup_u32(INT_MAX))) ? svuzp1_u32(min_val)[0] : -1;\n}\n```"}
{"task_id": "SimdBench_11_SVE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    svbool_t p = svptrue_b32();\n    svint32_t min_val = svdup_n_s32(INT_MAX);\n\n    for (size_t i = 0; i < length; i += 8) {\n        svint32_t vec = svld1_u32(src + i);\n        svbool_t is_even = svcmplt_z_s32(svand_x_s32(vec, svdup_n_s32(0x1)), svdup_n_s32(0));\n        svint32_t even_vals = svsel(is_even, vec, svdup_n_s32(0));\n        min_val = svinvalb(min_val, svminv_m(is_even, min_val, even_vals));\n    }\n\n    int result = svfirst_m(svptrue_b32(), min_val);\n    return result == INT_MAX ? -1 : result;\n}\n```"}
{"task_id": "SimdBench_11_SVE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    svbool_t p = svptrue_b32();\n    svint32_t min_val = svdup_n_s32(INT_MAX);\n\n    for (size_t i = 0; i < length; i += 8) {\n        svint32_t vec = svld1_u32(src + i);\n        svbool_t is_even = svcmplt_z_s32(svand_x_s32(vec, svdup_n_s32(0x1)), svdup_n_s32(0));\n        svint32_t even_vals = svsel(is_even, vec, svdup_n_s32(0));\n        min_val = svinvalb(min_val, svminv_m(is_even, min_val, even_vals));\n    }\n\n    int result = INT_MAX;\n    svbool_t last_mask = svptrue_b32();\n    for (size_t i = 0; i < 8; ++i) {\n        if (svgetlane_b32(last_mask, i)) {\n            result = svgetlane_s32(min_val, i);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_SVE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    svbool_t p = svptrue_b32();\n    svint32_t min_val = svdup_n_s32(INT_MAX);\n\n    for (size_t i = 0; i < length; i += 8) {\n        svint32_t vec = svld1_u32(src + i);\n        svbool_t mask = svcmplt_z_s32(vec, svdup_n_s32(0));\n        svint32_t even_vals = svsel(mask, svdup_n_s32(0), vec);\n        min_val = svinval_u32(min_val, even_vals);\n    }\n\n    int result = INT_MAX;\n    for (size_t i = 0; i < 8; ++i) {\n        if ((min_val >> (i * 32)) & 0xFFFFFFFF) {\n            result = (min_val >> (i * 32)) & 0xFFFFFFFF;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_12_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    svint32_t vsrc = svld1(svptrue_b32(), src);\n    svuint32_t vsrc_u = svreinterpret_u32(vsrc);\n    svint32_t varith_shift = svrshl_s32(vsrc, svdup_n_s32(shift));\n    svuint32_t vlog_shift = svrshl_u32(vsrc_u, svdup_n_u32(shift));\n    svint32_t vresult = svreinterpret_s32(svadd_x_u32(varith_shift, vlog_shift));\n    svst1(svptrue_b32(), dst, vresult);\n}\n```"}
{"task_id": "SimdBench_12_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    svint32_t vsrc = svld1_u32(src);\n    svuint32_t vsrcu = svreinterpret_u32(vsrc);\n    svint32_t varithmetic_shift = svrshl_s32(vsrc, svdup_u32(shift));\n    svint32_t vlogical_shift = svrshl_u32(vsrcu, svdup_u32(shift));\n    svint32_t vresult = svadd_s32(varithmetic_shift, vlogical_shift);\n    svst1_u32(dst, vresult);\n}\n```"}
{"task_id": "SimdBench_12_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    svint32_t vsrc = svld1_u32(src);\n    svuint32_t vsrc_uint = svreinterpret_u32(vsrc);\n    svint32_t varith_shift = svrshl_s32(vsrc, svdup_u32(shift));\n    svint32_t vlog_shift = svrshl_u32(vsrc_uint, svdup_u32(shift));\n    svint32_t vresult = svadd_s32(varith_shift, vlog_shift);\n    svst1_u32(dst, vresult);\n}\n```"}
{"task_id": "SimdBench_12_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    svint32_t v_src = svld1(svptrue_b32(), src);\n    svuint32_t v_src_u = svreinterpret_u32(v_src);\n    svint32_t v_arith_shift = svrshl_s32(v_src, svdup_n_u32(shift));\n    svuint32_t v_logic_shift = svrshl_u32(v_src_u, svdup_n_u32(shift));\n    svint32_t v_result = svreinterpret_s32(svadd_x_u32(v_logic_shift, svreinterpret_u32(v_arith_shift)));\n    svst1(svptrue_b32(), dst, v_result);\n\n    for (size_t i = 16; i < length; i += 16) {\n        v_src = svld1(svptrue_b32(), &src[i]);\n        v_src_u = svreinterpret_u32(v_src);\n        v_arith_shift = svrshl_s32(v_src, svdup_n_u32(shift));\n        v_logic_shift = svrshl_u32(v_src_u, svdup_n_u32(shift));\n        v_result = svreinterpret_s32(svadd_x_u32(v_logic_shift, svreinterpret_u32(v_arith_shift)));\n        svst1(svptrue_b32(), &dst[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    svint32_t v_src = svld1(svptrue_b32(), src);\n    svuint32_t v_src_u = svreinterpret_u32(v_src);\n    svint32_t v_arith_shift = svrshl_s32(v_src, svdup_n_u32(shift));\n    svuint32_t v_logic_shift = svrshl_u32(v_src_u, svdup_n_u32(shift));\n    svint32_t v_result = svreinterpret_s32(svadd_x_u32(v_logic_shift, svreinterpret_u32(v_arith_shift)));\n    svst1(svptrue_b32(), dst, v_result);\n\n    for (size_t i = 16; i < length; i += 16) {\n        v_src = svld1(svptrue_b32(), &src[i]);\n        v_src_u = svreinterpret_u32(v_src);\n        v_arith_shift = svrshl_s32(v_src, svdup_n_u32(shift));\n        v_logic_shift = svrshl_u32(v_src_u, svdup_n_u32(shift));\n        v_result = svreinterpret_s32(svadd_x_u32(v_logic_shift, svreinterpret_u32(v_arith_shift)));\n        svst1(svptrue_b32(), &dst[i], v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_13_SVE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    svuint32_t vec;\n    svint64_t sum = svdup_n_s64(0);\n\n    for (size_t i = 0; i < rows * cols; i += SV_INT32_ELEMS) {\n        vec = svld1_u32(matrix + i);\n        sum = svaddlv_s32(sum, vec);\n    }\n\n    return svgetlane_s64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_13_SVE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    svuint32_t vec;\n    svint64_t sum = svdup_n_s64(0);\n\n    for (size_t i = 0; i < rows * cols; i += SV_INT32_ELEMS) {\n        vec = svld1_u32(matrix + i);\n        sum = svaddlv_s32(sum, vec);\n    }\n\n    return svgetlane_s64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_13_SVE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    svuint32_t vec;\n    svint64_t sum = svdup_lane_s64(0, 0);\n    size_t i;\n\n    for (i = 0; i < rows * cols; i += SV_INT32_ELEMS) {\n        vec = svld1_u32(matrix + i);\n        sum = svaddlv_s32(sum, vec);\n    }\n\n    return svgetlane_s64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_13_SVE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    svuint32_t vec;\n    svint64_t sum = svdup_lane_s64(0, 0);\n    size_t i;\n\n    for (i = 0; i < rows * cols; i += SV_INT32_ELEMS) {\n        vec = svld1_u32(matrix + i);\n        sum = svaddlv_s32(sum, vec);\n    }\n\n    return svgetlane_s64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_13_SVE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    svuint32_t vec;\n    svint64_t sum = svdup_lane_s64(0, 0);\n\n    for (size_t i = 0; i < rows * cols; i += SV_INT32_ELEMS) {\n        vec = svld1_u32(matrix + i);\n        sum = svaddlv_s32(sum, vec);\n    }\n\n    return svgetlane_s64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_14_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    svint8_t max_val = svld1_u8(src);\n    svuint32_t indices = svdup_u32(0);\n    svuint32_t idx = svdup_u32(0);\n\n    for (size_t i = 16; i < length; i += 16) {\n        svint8_t val = svld1_u8(src + i);\n        svbool_t mask = svcmpgt(val, max_val);\n        max_val = svsel(mask, val, max_val);\n        indices = svaddl_u32(indices, svreinterpret_u32(svandb(mask, svreinterpret_b32(svptrue_b32()))));\n        idx = svadd(idx, svreinterpret_u32(svandb(mask, svreinterpret_b32(svptrue_b32()))));\n    }\n\n    size_t remaining = length % 16;\n    for (size_t i = 0; i < remaining; ++i) {\n        if (src[i] > src[idx]) {\n            idx = i;\n        }\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_14_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    svint8_t max_val = svld1_u8(src);\n    svuint32_t indices = svdup_u32(0);\n    size_t max_index = 0;\n\n    for (size_t i = 16; i < length; i += 16) {\n        svint8_t vals = svld1_u8(src + i);\n        svbool_t mask = svcmpgt(val, max_val);\n        max_val = svsel(mask, val, max_val);\n        indices = svadd_z_u32(indices, svreinterpret_u32(mask), i / 16);\n    }\n\n    for (size_t i = length - length % 16; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    svint8_t max_val = svld1_u8(src);\n    svuint32_t indices = svdup_u32(0);\n    svuint32_t idx = svdup_u32(0);\n\n    for (size_t i = 16; i < length; i += 16) {\n        svint8_t val = svld1_u8(src + i);\n        svbool_t mask = svcmpgt(val, max_val);\n        max_val = svsel(mask, val, max_val);\n        indices = svaddl_u32(indices, svreinterpret_u32(svsltu_u8(val, max_val)));\n        idx = svadd_u32(idx, svreinterpret_u32(svmul_n_u32(svclz_u32(svreinterpret_u32(svsltu_u8(val, max_val))), 4)));\n    }\n\n    size_t remaining = length % 16;\n    for (size_t i = 0; i < remaining; ++i) {\n        if (src[i] > src[idx]) {\n            idx = i;\n        }\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_14_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    svint8_t max_val = svld1_u8(src);\n    svuint32_t indices = svdup_u32(0);\n    svuint32_t idx = svdup_u32(0);\n\n    for (size_t i = 16; i < length; i += 16) {\n        svint8_t vals = svld1_u8(src + i);\n        svbool_t mask = svcmpgt(val, max_val);\n        max_val = svsel(mask, val, max_val);\n        indices = svadd(indices, svreinterpret_u32(svsls_z_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(svreinterpret_u32(svuzp1_u32(sv"}
{"task_id": "SimdBench_14_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    svint8_t max_val = svld1_u8(src);\n    svuint32_t indices = svdup_u32(0);\n    svuint32_t idx = svdup_u32(0);\n\n    for (size_t i = 16; i < length; i += 16) {\n        svint8_t val = svld1_u8(src + i);\n        svbool_t mask = svcmpgt(val, max_val);\n        max_val = svsel(mask, val, max_val);\n        indices = svadd(indices, svreinterpret_u32(svclz(mask)));\n        idx = svadd(idx, svreinterpret_u32(svmul_n_u32(svclz(mask), 16)));\n    }\n\n    size_t remaining = length % 16;\n    for (size_t i = 0; i < remaining; ++i) {\n        if (src[i] > src[idx]) {\n            idx = i;\n        }\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_15_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    svbool_t p = svptrue_b16();\n    svuint16_t v_mask = svdup_u16(mask);\n    svuint16_t v_index = svindex_u16(length);\n\n    for (size_t i = 0; i < length; i += SV_INT16_ELEMS) {\n        svuint16_t v_src1 = svld1_u16(src1 + i);\n        svuint16_t v_src2 = svld1_u16(src2 + i);\n        svuint16_t v_cond = svand_u16(v_index, v_mask);\n        svuint16_t v_res = svsel(p, v_src1, v_src2, v_cond);\n        svst1_u16(dst + i, v_res);\n    }\n}\n```"}
{"task_id": "SimdBench_15_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    svbool_t p = svptrue_b16();\n    svuint16_t v_mask = svdup_u16(mask);\n    svuint16_t v_index = svindex_u16(length);\n\n    for (size_t i = 0; i < length; i += SV_MAX_ELEMS_U16) {\n        svuint16_t v_src1 = svld1_u16(src1 + i);\n        svuint16_t v_src2 = svld1_u16(src2 + i);\n        svuint16_t v_cond = svand_u16(v_index, v_mask);\n        svuint16_t v_dst = svsel(p, v_src1, v_src2);\n        svst1_u16(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_15_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    svuint16_t v_mask = svdup_u16(mask);\n    svbool_t v_all_true = svptrue_b16();\n\n    for (size_t i = 0; i < length; i += SV_INT16_ELEMS) {\n        svuint16_t v_idx = svindex_u16(SV_INT16_ELEMS);\n        svuint16_t v_cond = svand_u16(v_idx, v_mask);\n\n        svuint16_t v_src1 = svld1_u16(src1 + i);\n        svuint16_t v_src2 = svld1_u16(src2 + i);\n\n        svuint16_t v_dst = svsel_z_u16(v_all_true, v_cond, v_src1, v_src2);\n        svst1_u16(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_15_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    svbool_t p = svptrue_b16();\n    svuint16_t v_mask = svdup_u16(mask);\n    svuint16_t v_index = svindex_u16(length);\n\n    for (size_t i = 0; i < length; i += SV_INT16_ELEMS) {\n        svuint16_t v_src1 = svld1_u16(src1 + i);\n        svuint16_t v_src2 = svld1_u16(src2 + i);\n        svuint16_t v_cond = svand_u16(v_index, v_mask);\n        svuint16_t v_result = svsel_b16(svnot_b16(v_cond), v_src1, v_src2);\n        svst1_u16(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_15_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    svuint16_t v_mask = svdup_u16(mask);\n    svbool_t v_all_true = svptrue_b16();\n\n    for (size_t i = 0; i < length; i += SV_INT16_ELEMS) {\n        svuint16_t v_idx = svindex_u16(SV_INT16_ELEMS);\n        svuint16_t v_cond = svand_u16(v_idx, v_mask);\n\n        svuint16_t v_src1 = svld1_u16(src1 + i);\n        svuint16_t v_src2 = svld1_u16(src2 + i);\n\n        svuint16_t v_dst = svsel_z_u16(v_all_true, v_cond, v_src1, v_src2);\n        svst1_u16(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    svuint32_t v_src;\n    svuint32_t v_popcount;\n    size_t i;\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v_src = svld1_u32(src + i);\n        v_popcount = svpopcntb_u32(v_src);\n        svst1_u32(dst + i, v_popcount);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    svuint32_t vsrc;\n    svuint32_t vpopcount;\n    size_t i;\n\n    for (i = 0; i <= length - 8; i += 8) {\n        vsrc = svld1_u32(src + i);\n        vpopcount = svcntp_b32(vsrc);\n        svst1_u32(dst + i, vpopcount);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    svuint32_t v_src;\n    svuint32_t v_popcount;\n    size_t i = 0;\n\n    while (i + SV_INT32_ELT_SIZE <= length) {\n        v_src = svld1_u32(src + i);\n        v_popcount = svcntb_u32(v_src);\n        svst1_u32(dst + i, v_popcount);\n        i += SV_INT32_ELT_SIZE;\n    }\n\n    if (i < length) {\n        svuint32_t v_remaining = svld1_u32(src + i);\n        svuint32_t v_popcount_remaining = svcntb_u32(v_remaining);\n        svst1_u32(dst + i, v_popcount_remaining);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    svuint32_t vsrc;\n    svuint32_t vpopcount;\n    size_t i;\n\n    for (i = 0; i + 8 <= length; i += 8) {\n        vsrc = svld1_u32(src + i);\n        vpopcount = svcntp_b32(vsrc);\n        svst1_u32(dst + i, vpopcount);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    svuint32_t v_src;\n    svuint32_t v_popcount;\n    size_t i;\n\n    for (i = 0; i <= length - 8; i += 8) {\n        v_src = svld1_u32(src + i);\n        v_popcount = svcntp_b32(v_src);\n        svst1_u32(dst + i, v_popcount);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    svuint16_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    for (; i <= length - 16; i += 16) {\n        v_src1 = svld1_u16(src1 + i);\n        v_src2 = svld1_u16(src2 + i);\n        v_dst = svaddh_u16(v_src1, v_src2);\n        svst1_u16(dst + i, v_dst);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    svuint16_t vsrc1 = svld1_u16(src1);\n    svuint16_t vsrc2 = svld1_u16(src2);\n    svuint16_t vdst = svaddlv_u8(svreinterpret_u8_u16(vsrc1), svreinterpret_u8_u16(vsrc2));\n    svst1_u16(dst, vdst);\n}\n```"}
{"task_id": "SimdBench_17_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    svuint16_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    while (i + 8 <= length) {\n        v_src1 = svld1_u16(src1 + i);\n        v_src2 = svld1_u16(src2 + i);\n        v_dst = svaddlv_u16(v_src1, v_src2);\n        svst1_u16(dst + i, v_dst);\n        i += 8;\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    svuint16_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    for (; i <= length - 16; i += 16) {\n        v_src1 = svld1_u16(src1 + i);\n        v_src2 = svld1_u16(src2 + i);\n        v_dst = svaddlv_u8(svreinterpret_u8_u16(v_src1), svreinterpret_u8_u16(v_src2));\n        v_dst = svminv_u16(svmul_n_u16(v_dst, 257), svdup_u16(UINT16_MAX));\n        svst1_u16(dst + i, v_dst);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    svuint16_t vsrc1, vsrc2, vdst;\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        vsrc1 = svld1_u16(src1 + i);\n        vsrc2 = svld1_u16(src2 + i);\n        vdst = svaddh_u16(vsrc1, vsrc2);\n        svst1_u16(dst + i, vdst);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_18_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svbool_t vmask = svcmple_f64(svld1_f64(A), svdup_f64(100.0));\n    vmask = svcmplt_f64(vmask, svld1_f64(A), svdup_f64(-100.0));\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t vc = vzero;\n            for (size_t k = 0; k < n; k += SV_INT32_ELEMS) {\n                svfloat64_t va = svld1_f64(&A[i * n + k]);\n                svfloat64_t vb = svld1_f64(&B[k * p + j]);\n                va = svsel(vmask, vzero, va);\n                vb = svsel(vmask, vzero, vb);\n                vc = svmla_x_f64(vc, va, vb);\n            }\n            svst1_f64(&C[i * p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svbool_t vmask = svcmple_f64(svld1_f64(A), svdup_f64(100.0));\n    vmask = svcmplt_f64(vmask, svld1_f64(A), svdup_f64(-100.0));\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t vc = vzero;\n            for (size_t k = 0; k < n; k += SV_INT32_ELEMS) {\n                svfloat64_t va = svld1_f64(&A[i * n + k]);\n                svfloat64_t vb = svld1_f64(&B[k * p + j]);\n                va = svsel(vmask, vzero, va);\n                vb = svsel(vmask, vzero, vb);\n                vc = svmla_x_f64(vc, va, vb);\n            }\n            svst1_f64(&C[i * p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svbool_t vmask = svcmple_f64(svld1_f64(A), svdup_f64(100.0));\n    vmask = svcmplt_f64(vmask, svld1_f64(A), svdup_f64(-100.0));\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t vc = vzero;\n            for (size_t k = 0; k < n; k += SV_INT64_ELEMS) {\n                svfloat64_t va = svld1_f64(&A[i * n + k]);\n                svfloat64_t vb = svld1_f64(&B[k * p + j]);\n                va = svsel(vmask, vzero, va);\n                vb = svsel(vmask, vzero, vb);\n                vc = svmla_x_f64(vc, va, vb);\n            }\n            svst1_f64(&C[i * p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svbool_t vmask = svcmple_f64(svld1_f64(A), svdup_f64(100.0));\n    vmask = svcmplt_f64(vmask, svld1_f64(A), svdup_f64(-100.0));\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t vc = vzero;\n            for (size_t k = 0; k < n; k += SV_INT64_ELEMS) {\n                svfloat64_t va = svld1_f64(&A[i * n + k]);\n                svfloat64_t vb = svld1_f64(&B[k * p + j]);\n                va = svsel(vmask, vzero, va);\n                vb = svsel(vmask, vzero, vb);\n                vc = svmla_x_f64(vc, va, vb);\n            }\n            svst1_f64(&C[i * p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svbool_t vmask = svcmple_f64(svld1_f64(A), svdup_f64(100.0));\n    vmask = svcmplt_f64(vmask, svld1_f64(A), svdup_f64(-100.0));\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t vc = vzero;\n            for (size_t k = 0; k < n; k += SV_INT64_ELEMS) {\n                svfloat64_t va = svld1_f64(&A[i * n + k]);\n                svfloat64_t vb = svld1_f64(&B[k * p + j]);\n                va = svsel(vmask, vzero, va);\n                vb = svsel(vmask, vzero, vb);\n                vc = svmla_x_f64(vc, va, vb);\n            }\n            svst1_f64(&C[i * p + j], vc);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    svlabel_t label;\n    svuint64_t mask = svptrue_b64();\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; j += SV_INT64_ELEMS) {\n            svint64_t va = svld1_m(svptrue_b64(), A + i * dim2 * dim3 + j * dim3, &label);\n            svint64_t vb = svld1_m(svptrue_b64(), B + i * dim2 * dim3 + j * dim3, &label);\n            svint64_t vc = svadd_s64(va, vb);\n            svst1_m(mask, C + i * dim2 * dim3 + j * dim3, vc, &label);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    svint64_t va, vb, vc;\n    size_t i, j, k;\n\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j < dim2; j += svcntq()) {\n            size_t elements_to_process = dim2 - j > svcntq() ? svcntq() : dim2 - j;\n            va = svld1_m(svptrue_b64(), &A[i * dim2 * dim3 + j * dim3], svptrue_b64());\n            vb = svld1_m(svptrue_b64(), &B[i * dim2 * dim3 + j * dim3], svptrue_b64());\n            vc = svadd_s64(va, vb);\n            svst1_m(svptrue_b64(), &C[i * dim2 * dim3 + j * dim3], vc, svptrue_b64());\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    svint64_t va, vb, vc;\n    size_t i = 0;\n\n    while (i < dim1 * dim2 * dim3) {\n        size_t elements_to_process = MIN(dim1 * dim2 * dim3 - i, SV_INT64_ELT_SIZE);\n\n        va = svld1_m(svptrue_b64(), &A[i], svpattern_b64(0x5555555555555555));\n        vb = svld1_m(svptrue_b64(), &B[i], svpattern_b64(0x5555555555555555));\n        vc = svadd_s64(va, vb);\n\n        svst1_m(svptrue_b64(), &C[i], vc, svpattern_b64(0x5555555555555555));\n\n        i += elements_to_process;\n    }\n}\n```"}
{"task_id": "SimdBench_19_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    svint64_t va, vb, vc;\n    size_t i = 0;\n\n    while (i + 8 <= dim1 * dim2 * dim3) {\n        va = svld1_m(svptrue_b64(), A + i);\n        vb = svld1_m(svptrue_b64(), B + i);\n        vc = svadd_s64(va, vb);\n        svst1_m(svptrue_b64(), C + i, vc);\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    svlabel_t label;\n    svuint64_t mask = svptrue_b64();\n    size_t i = 0;\n\n    for (; i + 8 <= dim1 * dim2 * dim3; i += 8) {\n        svint64_t va = svld1_m(svptrue_b64(), A + i, &label);\n        svint64_t vb = svld1_m(svptrue_b64(), B + i, &label);\n        svint64_t vc = svadd_s64(va, vb);\n        svst1_m(mask, vc, C + i, &label);\n    }\n\n    for (; i < dim1 * dim2 * dim3; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    while (i + 8 <= length) {\n        v_src1 = svld1_f32(src1 + i);\n        v_src2 = svld1_f32(src2 + i);\n        v_dst = svadd_f32(v_src1, v_src2);\n        v_dst = svrnd_n_f32(v_dst, 0);\n        svst1_f32(dst + i, v_dst);\n        i += 8;\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    while (i + 8 <= length) {\n        v_src1 = svld1_f32(src1 + i);\n        v_src2 = svld1_f32(src2 + i);\n        v_dst = svadd_f32(v_src1, v_src2);\n        v_dst = svrnd_f32(v_dst);\n        svst1_f32(dst + i, v_dst);\n        i += 8;\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        v_src1 = svld1_f32(src1 + i);\n        v_src2 = svld1_f32(src2 + i);\n        v_dst = svadd_f32(v_src1, v_src2);\n        v_dst = svrnd_f32(v_dst);\n        svst1_f32(dst + i, v_dst);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        dst[i] = roundf(dst[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        v_src1 = svld1_f32(src1 + i);\n        v_src2 = svld1_f32(src2 + i);\n        v_dst = svadd_f32(v_src1, v_src2);\n        v_dst = svrnd_f32(v_dst);\n        svst1_f32(dst + i, v_dst);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t v_src1, v_src2, v_dst;\n    size_t i = 0;\n\n    while (i + 8 <= length) {\n        v_src1 = svld1_f32(src1 + i);\n        v_src2 = svld1_f32(src2 + i);\n        v_dst = svadd_f32(v_src1, v_src2);\n        v_dst = svrnd_f32(v_dst);\n        svst1_f32(dst + i, v_dst);\n        i += 8;\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t vdiff, vabs_diff, vmask, vresult;\n    svbool_t p;\n\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svuint32_t indices = svindex_u32(svcntw());\n        svbool_t mask = svcmpgt_b32(svabs_f32(vld1q_f32(src1 + i)), svdup_n_f32(50.0f));\n        mask = svand_b32(mask, svmod_u32(indices, svdup_n_u32(2)));\n\n        vdiff = svsub_f32(vld1q_f32(src1 + i), vld1q_f32(src2 + i));\n        vabs_diff = svabs_f32(vdiff);\n\n        p = svcmple_f32(vabs_diff, svdup_n_f32(50.0f));\n        p = svand_b32(p, mask);\n\n        vresult = svdiv_f32(vld1q_f32(src1 + i), vld1q_f32(src2 + i));\n        vresult = svrndup_f32(vresult);\n\n        svst1q_f32(dst + i, svsel_b32(p, vresult, svdup_n_f32(-1.0f)));\n    }\n}\n```"}
{"task_id": "SimdBench_21_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t vdiff, vabs_diff, vmask, vresult;\n    svbool_t p;\n\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svuint32_t indices = svindex_u32(svcntw());\n        svbool_t mask = svcmpgt_b32(indices, svdup_u32(length - 1));\n\n        svfloat32_t vsrc1 = svld1_f32(src1 + i);\n        svfloat32_t vsrc2 = svld1_f32(src2 + i);\n\n        vdiff = svsub_f32(vsrc1, vsrc2);\n        vabs_diff = svabs_f32(vdiff);\n        vmask = svcmple_f32(vabs_diff, svdup_f32(50.0f));\n        vmask = svand_b32(mask, vmask);\n\n        vresult = svmul_f32(vsrc1, vsrc2);\n        vresult = svrndup_f32(vresult);\n        vresult = svsel_b32(vmask, vresult, svdup_f32(-1.0f));\n\n        svst1_f32(dst + i, vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t vdiff, vabs_diff, vmask, vresult;\n    svbool_t p;\n\n    for (size_t i = 0; i < length; i += SV_INT32_ELEMS) {\n        svint32_t indices = svindex(SV_INT32_ELEMS);\n\n        // Load data\n        svfloat32_t vs1 = svld1_f32(src1 + i);\n        svfloat32_t vs2 = svld1_f32(src2 + i);\n\n        // Calculate difference and absolute value\n        vdiff = svsub_f32(vs1, vs2);\n        vabs_diff = svabs_f32(vdiff);\n\n        // Create mask for condition 1: abs_diff <= 50\n        vmask = svle_f32(vabs_diff, svdup_n_f32(50.0f));\n\n        // Create mask for condition 2: index is multiple of 2\n        p = svcmpgt_b32(indices, svdup_n_s32(SV_INT32_ELEMS / 2));\n        p = svbic_b32(p, svreinterpret_b32(svnot_b32(p)));\n\n        // Combine masks\n        vmask = svand_b32(vmask, p);\n\n        // Perform element-wise multiplication where condition is met\n        vresult = svcmul_f32(vmask, vs1, vs2);\n\n        // Round up toward positive infinity\n        vresult = svrndup_f32(vresult);\n\n        // Set -1 for elements that do not meet the conditions\n        vresult = svsel_b32(vmask, vresult, svdup_n_f32(-1.0f));\n\n        // Store result\n        svst1_f32(dst + i, vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t vdiff, vabs_diff, vmask, vresult;\n    svbool_t p;\n\n    for (size_t i = 0; i < length; i += SV_INT32_ELEMS) {\n        svint32_t indices = svdup_lane_s32(i, SV_INT32_ELEMS);\n        svbool_t mask = svcmple_f32(svabs_f32(svsub_f32(svld1_f32(src1 + i), svld1_f32(src2 + i))), svdup_n_f32(50.0f));\n        mask = svand_b32(mask, svbic_u32(sveor_u32(mask, svclrsb_u32(svreinterpret_u32(indices))), svdup_n_u32(1)));\n\n        vdiff = svsub_f32(svld1_f32(src1 + i), svld1_f32(src2 + i));\n        vabs_diff = svabs_f32(vdiff);\n        vmask = svcmple_f32(vabs_diff, svdup_n_f32(50.0f));\n        vmask = svand_b32(vmask, svbic_u32(sveor_u32(vmask, svclrsb_u32(svreinterpret_u32(indices))), svdup_n_u32(1)));\n        vresult = svdiv_f32(svld1_f32(src1 + i), svld1_f32(src2 + i));\n        vresult = svadd_f32(vresult, svdup_n_f32(0.5f));\n        vresult = svfloor_f32(vresult);\n\n        p = svwhilelt_b32(indices, svdup_n_s32(SV_INT32_ELEMS));\n        svst1_f32(dst + i, svsel_b32(p, vresult, svdup_n_f32(-1.0f)));\n    }\n}\n```"}
{"task_id": "SimdBench_21_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    svfloat32_t vdiff, vabs_diff, vmask, vresult;\n    svbool_t p;\n\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svfloat32_t vs1 = svld1(svptrue_b32(), src1 + i);\n        svfloat32_t vs2 = svld1(svptrue_b32(), src2 + i);\n\n        vdiff = svsub_f32(vs1, vs2);\n        vabs_diff = svabss_f32(vdiff);\n        vmask = svcmple_f32(vabs_diff, svdup_n_f32(50.0f));\n        vmask = svand_b32(vmask, svcmpne_z_f32(svmod_u32(svptrue_b32(), svreinterpret_u32(i)), svdup_n_u32(2)));\n\n        vresult = svmul_f32(vs1, vs2);\n        vresult = svrndup_f32(vresult);\n\n        p = svwhilelt_b32(svptrue_b32(), i, length);\n        svst1(p, dst + i, svsel_b32(p, vresult, svdup_n_f32(-1.0f)));\n    }\n}\n```"}
{"task_id": "SimdBench_22_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    svfloat64_t v_scalar = svdup_f64(scalar);\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_INT64_ELEMS) {\n        svfloat64_t v_mat = svld1_f64(mat + i);\n        svfloat64_t v_dst = svfmul_f64(v_mat, v_scalar);\n        svst1_f64(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_22_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    svfloat64_t v_scalar = svdup_f64(scalar);\n    size_t i;\n\n    for (i = 0; i + 8 <= m * n; i += 8) {\n        svfloat64_t v_mat = svld1_f64(mat + i);\n        svfloat64_t v_dst = svfmul_f64(v_mat, v_scalar);\n        svst1_f64(dst + i, v_dst);\n    }\n\n    for (; i < m * n; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    svfloat64_t v_scalar = svdup_f64(scalar);\n    size_t i;\n\n    for (i = 0; i + 8 <= m * n; i += 8) {\n        svfloat64_t v_mat = svld1_f64(mat + i);\n        svfloat64_t v_dst = svmul_f64(v_mat, v_scalar);\n        svst1_f64(dst + i, v_dst);\n    }\n\n    for (; i < m * n; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    svfloat64_t v_scalar = svdup_f64(scalar);\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_QREG_SIZE / sizeof(double)) {\n        svfloat64_t v_mat = svld1q_f64(mat + i);\n        svfloat64_t v_dst = svmulq_f64(v_mat, v_scalar);\n        svst1q_f64(dst + i, v_dst);\n    }\n\n    // Handle remaining elements that do not fit into a full vector\n    for (; i < m * n; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    svfloat64_t v_scalar = svdup_f64(scalar);\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_INT64_ELEMS) {\n        svfloat64_t v_mat = svld1_f64(mat + i);\n        svfloat64_t v_dst = svmul_f64(v_mat, v_scalar);\n        svst1_f64(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_23_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svuint32_t vone = svdup_n_u32(1);\n\n    for (size_t i = 0; i < m; ++i) {\n        float32_t* row = &mat[i * n];\n        svfloat32_t sum_squares = vzero;\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(&row[j]);\n            svfloat32_t vec_squared = svmul_f32(vec, vec);\n            sum_squares = svadd_f32(sum_squares, vec_squared);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            sum_squares = svadd_f32(sum_squares, svdup_n_f32(row[j] * row[j]));\n        }\n\n        svfloat32_t l2_norm = svrsqrteq_f32(svadd_f32(sum_squares, sum_squares));\n        l2_norm = svdiv_f32(l2_norm, svsqrt_f32(svadd_f32(vzero, vzero)));\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(&row[j]);\n            svst1_f32(&row[j], svmul_f32(vec, svrecpe_f32(l2_norm)));\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svuint32_t vone = svdup_n_u32(1);\n\n    for (size_t i = 0; i < m; ++i) {\n        float32_t* row = &mat[i * n];\n        svfloat32_t sum_of_squares = vzero;\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(row + j);\n            svfloat32_t sq_vec = svmul_f32(vec, vec);\n            sum_of_squares = svadd_f32(sum_of_squares, sq_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            sum_of_squares[0] += row[j] * row[j];\n        }\n\n        svfloat32_t l2_norm = svrsqrteq_f32(svadd_f32(sum_of_squares, sum_of_squares));\n        l2_norm = svdiv_f32(l2_norm, svsqrt_f32(vsetvl_f32(n)));\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(row + j);\n            svst1_f32(row + j, svmul_f32(vec, svrecpe_f32(l2_norm)));\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= l2_norm[0];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svuint32_t vone = svdup_n_u32(1);\n\n    for (size_t i = 0; i < m; ++i) {\n        float32_t* row = &mat[i * n];\n        svfloat32_t sum_of_squares = vzero;\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(&row[j]);\n            svfloat32_t sq_vec = svmul_f32(vec, vec);\n            sum_of_squares = svadd_f32(sum_of_squares, sq_vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            sum_of_squares = svadd_f32(sum_of_squares, svdup_n_f32(row[j] * row[j]));\n        }\n\n        svfloat32_t l2_norm = svrsqrteq_f32(svadd_f32(sum_of_squares, vzero));\n        l2_norm = svmul_f32(l2_norm, l2_norm);\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(&row[j]);\n            svst1_f32(&row[j], svmul_f32(vec, svrecpe_f32(l2_norm)));\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svuint32_t vone = svdup_n_u32(1);\n\n    for (size_t i = 0; i < m; ++i) {\n        float32_t* row = &mat[i * n];\n        svfloat32_t sum_squares = vzero;\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vrow = svld1_f32(row + j);\n            svfloat32_t vsq = svmul_f32(vrow, vrow);\n            sum_squares = svadd_f32(sum_squares, vsq);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            sum_squares = svadd_f32(sum_squares, svdup_n_f32(row[j] * row[j]));\n        }\n\n        svfloat32_t l2_norm = svrsqrte_f32(svadd_f32(sum_squares, vzero));\n        l2_norm = svmul_f32(l2_norm, svrsqrte_f32(svadd_f32(l2_norm, vzero)));\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vrow = svld1_f32(row + j);\n            svst1_f32(row + j, svmul_f32(vrow, svrecpe_f32(l2_norm)));\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= l2_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svuint32_t vone = svdup_n_u32(1);\n\n    for (size_t i = 0; i < m; ++i) {\n        float32_t* row = &mat[i * n];\n        svfloat32_t sum = vzero;\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(&row[j]);\n            sum = svadd_f32(sum, svmul_f32(vec, vec));\n        }\n\n        for (; j < n; ++j) {\n            sum = svadd_f32(sum, svdup_n_f32(row[j] * row[j]));\n        }\n\n        svfloat32_t norm = svrsqrteq_f32(svmaxnmv_f32(sum, vzero));\n\n        for (size_t j = 0; j + 8 <= n; j += 8) {\n            svfloat32_t vec = svld1_f32(&row[j]);\n            svst1_f32(&row[j], svmul_f32(vec, svrecpeq_f32(norm)));\n        }\n\n        for (; j < n; ++j) {\n            row[j] *= 1.0f / norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svint32_t vone = svdup_i32(1);\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t sum = vzero;\n            for (size_t k = 0; k < n; k += SV_INT64_ELEMS) {\n                svfloat64_t va = svld1_f64(mat1 + i * n + k);\n                svfloat64_t vb = svld1_f64(mat2 + k * p + j);\n                sum = svmla_x_f64(sum, va, vb);\n            }\n            svint32_t rounded_sum = svrndi_f64(sum);\n            svst1_u32((uint32_t*)(dst + i * p + j), rounded_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svint32_t vone = svdup_i32(1);\n\n    for (size_t i = 0; i < m; i += SV_INT64_ELEMS) {\n        for (size_t j = 0; j < p; j += SV_INT64_ELEMS) {\n            svfloat64_t sum = svdup_f64(0.0);\n            for (size_t k = 0; k < n; k += SV_INT64_ELEMS) {\n                svfloat64_t vmat1 = svld1_f64(mat1 + i * n + k);\n                svfloat64_t vmat2 = svld1_f64(mat2 + k * p + j);\n                sum = svmla_x_f64(sum, vmat1, vmat2);\n            }\n            svint32_t vresult = svrndi_f64(sum);\n            svst1_s32(dst + i * p + j, vresult);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svint32_t vone = svdup_i32(1);\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t sum = vzero;\n            for (size_t k = 0; k < n; k += SV_INT64_ELEMS) {\n                svfloat64_t va = svld1_f64(mat1 + i * n + k);\n                svfloat64_t vb = svld1_f64(mat2 + k * p + j);\n                sum = svadd_f64(sum, svmul_f64(va, vb));\n            }\n            int32_t result = svrinta_f64(sum);\n            dst[i * p + j] = (double)result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svint32_t vone = svdup_i32(1);\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t sum = vzero;\n            for (size_t k = 0; k < n; k += SV_INT64_ELEMS) {\n                svfloat64_t va = svld1_f64(mat1 + i * n + k);\n                svfloat64_t vb = svld1_f64(mat2 + k * p + j);\n                sum = svmla_x_f64(sum, va, vb);\n            }\n            int32_t result = svround_narrow_f64(sum);\n            dst[i * p + j] = (double)result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svint32_t vone = svdup_i32(1);\n\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            svfloat64_t sum = vzero;\n            for (size_t k = 0; k < n; k += 8) {\n                svfloat64_t va = svld1_f64(mat1 + i * n + k);\n                svfloat64_t vb = svld1_f64(mat2 + k * p + j);\n                sum = svmla_x_f64(sum, va, vb);\n            }\n            int32_t result = svround_narrow_f64(sum);\n            dst[i * p + j] = (double)result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svfloat32_t vquarter = svdup_n_f32(0.25f);\n\n    for (size_t i = 0; i < rows; i += 16) {\n        for (size_t j = 0; j < cols; j += 16) {\n            svfloat32_t vsrc[16];\n            svfloat32_t vdst[16];\n\n            for (int k = 0; k < 16; k++) {\n                vsrc[k] = svld1q_f32(src + (i + k) * cols + j);\n            }\n\n            for (int k = 0; k < 16; k++) {\n                vdst[k] = svaddq_f32(vsrc[k], vquarter);\n                vdst[k] = svrndiq_f32(vdst[k]);\n                vdst[k] = svsubq_f32(vdst[k], vquarter);\n            }\n\n            for (int k = 0; k < 16; k++) {\n                svst1q_f32(dst + (j + k) * rows + i, vdst[k]);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svfloat32_t vquarter = svdup_n_f32(0.25f);\n\n    for (size_t i = 0; i < rows; i += 16) {\n        for (size_t j = 0; j < cols; j += 16) {\n            svuint32x4_t idx = svindex_u32(j, 16);\n            svuint32x4_t row_idx = svadd_x_u32(idx, svdup_n_u32(i * cols));\n\n            svfloat32x4_t v0 = svld1q_f32(src + svudotw(row_idx, svdup_n_u32(cols), 0));\n            svfloat32x4_t v1 = svld1q_f32(src + svudotw(row_idx, svdup_n_u32(cols), 1));\n            svfloat32x4_t v2 = svld1q_f32(src + svudotw(row_idx, svdup_n_u32(cols), 2));\n            svfloat32x4_t v3 = svld1q_f32(src + svudotw(row_idx, svdup_n_u32(cols), 3));\n\n            v0 = svadd_f32(v0, vquarter);\n            v1 = svadd_f32(v1, vquarter);\n            v2 = svadd_f32(v2, vquarter);\n            v3 = svadd_f32(v3, vquarter);\n\n            v0 = svrnd_f32(v0);\n            v1 = svrnd_f32(v1);\n            v2 = svrnd_f32(v2);\n            v3 = svrnd_f32(v3);\n\n            svst1q_f32(dst + svudotw(svadd_x_u32(idx, svdup_n_u32(i)), svdup_n_u32(cols), 0), v0);\n            svst1q_f32(dst + svudotw(svadd_x_u32(idx, svdup_n_u32(i)), svdup_n_u32(cols), 1), v1);\n            svst1q_f32(dst + svudotw(svadd_x_u32(idx, svdup_n_u32(i)), svdup_n_u32(cols), 2), v2);\n            svst1q_f32(dst + svudotw(svadd_x_u32(idx, svdup_n_u32(i)), svdup_n_u32(cols), 3), v3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svfloat32_t vquarter = svdup_n_f32(0.25f);\n    svuint32_t vmask = svdup_n_u32(0x3FFFFFFF);\n\n    size_t i, j;\n    for (i = 0; i < rows; i += 16) {\n        for (j = 0; j < cols; j += 16) {\n            svfloat32_t vsrc[4];\n            vsrc[0] = svld1q_f32(src + i * cols + j);\n            vsrc[1] = svld1q_f32(src + i * cols + j + 8);\n            vsrc[2] = svld1q_f32(src + (i + 8) * cols + j);\n            vsrc[3] = svld1q_f32(src + (i + 8) * cols + j + 8);\n\n            svfloat32_t vrounded[4];\n            vrounded[0] = svaddq_f32(svroundmq_f32(vsrc[0]), vquarter);\n            vrounded[1] = svaddq_f32(svroundmq_f32(vsrc[1]), vquarter);\n            vrounded[2] = svaddq_f32(svroundmq_f32(vsrc[2]), vquarter);\n            vrounded[3] = svaddq_f32(svroundmq_f32(vsrc[3]), vquarter);\n\n            svuint32_t vint[4];\n            vint[0] = svreinterpret_u32_f32(vrounded[0]);\n            vint[1] = svreinterpret_u32_f32(vrounded[1]);\n            vint[2] = svreinterpret_u32_f32(vrounded[2]);\n            vint[3] = svreinterpret_u32_f32(vrounded[3]);\n\n            vint[0] &= vmask;\n            vint[1] &= vmask;\n            vint[2] &= vmask;\n            vint[3] &= vmask;\n\n            svfloat32_t vresult[4];\n            vresult[0] = svreinterpret_f32_u32(vint[0]);\n            vresult[1] = svreinterpret_f32_u32(vint[1]);\n            vresult[2] = svreinterpret_f32_u32(vint[2]);\n            vresult[3] = svreinterpret_f32_u32(vint[3]);\n\n            svst1q_f32(dst + j * rows + i, vresult[0]);\n            svst1q_f32(dst + j * rows + i + 8, vresult[1]);\n            svst1q_f32(dst + (j + 8) * rows + i, vresult[2]);\n            svst1q_f32(dst + (j + 8) * rows + i + 8, vresult[3]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svfloat32_t vquarter = svdup_n_f32(0.25f);\n\n    for (size_t i = 0; i < rows; i += SV_INT32_ELEMS) {\n        for (size_t j = 0; j < cols; j += SV_INT32_ELEMS) {\n            svint32x4_t vi = svld1q_s32((const int32_t*)(src + i * cols + j));\n            svfloat32_t vf = svreinterpret_f32_u32(vi);\n            vf = svadd_f32(vf, vquarter);\n            vf = svfloor_f32(vf);\n            vi = svreinterpret_u32_f32(vf);\n            svst1q_s32((int32_t*)(dst + j * rows + i), vi);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svfloat32_t vquarter = svdup_n_f32(0.25f);\n    svuint32_t vmask = svdup_n_u32(0x3FFFFFFF);\n\n    size_t i, j;\n    for (i = 0; i < rows; i += 8) {\n        for (j = 0; j < cols; j += 8) {\n            svfloat32_t vsrc0 = svld1q_f32(src + i * cols + j);\n            svfloat32_t vsrc1 = svld1q_f32(src + (i + 1) * cols + j);\n            svfloat32_t vsrc2 = svld1q_f32(src + (i + 2) * cols + j);\n            svfloat32_t vsrc3 = svld1q_f32(src + (i + 3) * cols + j);\n            svfloat32_t vsrc4 = svld1q_f32(src + (i + 4) * cols + j);\n            svfloat32_t vsrc5 = svld1q_f32(src + (i + 5) * cols + j);\n            svfloat32_t vsrc6 = svld1q_f32(src + (i + 6) * cols + j);\n            svfloat32_t vsrc7 = svld1q_f32(src + (i + 7) * cols + j);\n\n            svfloat32_t vdst0 = svaddq_f32(vsrc0, vquarter);\n            svfloat32_t vdst1 = svaddq_f32(vsrc1, vquarter);\n            svfloat32_t vdst2 = svaddq_f32(vsrc2, vquarter);\n            svfloat32_t vdst3 = svaddq_f32(vsrc3, vquarter);\n            svfloat32_t vdst4 = svaddq_f32(vsrc4, vquarter);\n            svfloat32_t vdst5 = svaddq_f32(vsrc5, vquarter);\n            svfloat32_t vdst6 = svaddq_f32(vsrc6, vquarter);\n            svfloat32_t vdst7 = svaddq_f32(vsrc7, vquarter);\n\n            vdst0 = svandq_u32(vreinterpretq_u32_f32(vdst0), vmask);\n            vdst1 = svandq_u32(vreinterpretq_u32_f32(vdst1), vmask);\n            vdst2 = svandq_u32(vreinterpretq_u32_f32(vdst2), vmask);\n            vdst3 = svandq_u32(vreinterpretq_u32_f32(vdst3), vmask);\n            vdst4 = svandq_u32(vreinterpretq_u32_f32(vdst4), vmask);\n            vdst5 = svandq_u32(vreinterpretq_u32_f32(vdst5), vmask);\n            vdst6 = svandq_u32(vreinterpretq_u32_f32(vdst6), vmask);\n            vdst7 = svandq_u32(vreinterpretq_u32_f32(vdst7), vmask);\n\n            vdst0 = svreinterpretq_f32_u32(vdst0);\n            vdst1 = svreinterpretq_f32_u32(vdst1);\n            vdst2 = svreinterpretq_f32_u32(vdst2);\n            vdst3 = svreinterpretq_f32_u32(vdst3);\n            vdst4 = svreinterpretq_f32_u32(vdst4);\n            vdst5 = svreinterpretq_f32_u32(vdst5);\n            vdst6 = svreinterpretq_f32_u32(vdst6);\n            vdst7 = svreinterpretq_f32_u32(vdst7);\n\n            svst1q_f32(dst + j * rows + i, vdst0);\n            svst1q_f32(dst + (j + 1) * rows + i, vdst1);\n            svst1q_f32(dst + (j + 2) * rows + i, vdst2);\n            svst1q_f32(dst + (j + 3) * rows + i, vdst3);\n            svst1q_f32(dst + (j + 4) * rows + i, vdst4);\n            svst1q_f32(dst + (j + 5) * rows + i, vdst5);\n            svst1q_f32(dst + (j + 6) * rows + i, vdst6);\n            svst1q_f32(dst + (j + 7) * rows + i, vdst7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    size_t i;\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += SV_INT32_ELEMS) {\n        svfloat32_t vsub = svld1_f32(tensor1 + i);\n        svfloat32_t vround = svrnd_f32(vsub);\n        svst1_f32(dst + i, vround);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    size_t i;\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += SV_INT32_ELEMS) {\n        svfloat32_t vsub = svsub_f32(svld1_f32(tensor1 + i), svld1_f32(tensor2 + i));\n        svst1_f32(dst + i, svroundz_f32(vsub));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    size_t i;\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += SV_INT32_ELEMS) {\n        svfloat32_t va = svld1_f32(tensor1 + i);\n        svfloat32_t vb = svld1_f32(tensor2 + i);\n        svfloat32_t vc = svsub_f32(va, vb);\n        svst1_f32(dst + i, svcvt_x_f32_u32(svrndraq_f32(vc)));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    size_t i;\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += SV_INT32_ELEMS) {\n        svfloat32_t vsub = svsub_f32(svld1_f32(tensor1 + i), svld1_f32(tensor2 + i));\n        svst1_f32(dst + i, svroundz_f32(vsub));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    size_t i;\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += SV_INT32_ELEMS) {\n        svfloat32_t vsub = svld1_f32(tensor1 + i);\n        svfloat32_t vround = svrnd_f32(vsub);\n        svst1_f32(dst + i, vround);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    svfloat64_t vmat1, vmat2, vresult;\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_INT64_ELEMS) {\n        vmat1 = svld1_f64(mat1 + i);\n        vmat2 = svld1_f64(mat2 + i);\n        vresult = svmul_f64(vmat1, vmat2);\n        svst1_f64(dst + i, vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    svfloat64_t vmat1, vmat2, vdst;\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_INT64_ELEMS) {\n        vmat1 = svld1_f64(mat1 + i);\n        vmat2 = svld1_f64(mat2 + i);\n        vdst = svmul_f64(vmat1, vmat2);\n        svst1_f64(dst + i, vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    svfloat64_t vmat1, vmat2, vdst;\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_INT64_ELEMS) {\n        vmat1 = svld1_f64(mat1 + i);\n        vmat2 = svld1_f64(mat2 + i);\n        vdst = svfmul_f64(vmat1, vmat2);\n        svst1_f64(dst + i, vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    svfloat64_t vmat1, vmat2, vdst;\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_INT64_ELEMS) {\n        vmat1 = svld1_f64(mat1 + i);\n        vmat2 = svld1_f64(mat2 + i);\n        vdst = svfmul_f64(vmat1, vmat2);\n        svst1_f64(dst + i, vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    svfloat64_t vmat1, vmat2, vdst;\n    size_t i;\n\n    for (i = 0; i < m * n; i += SV_INT64_ELEMS) {\n        vmat1 = svld1_f64(mat1 + i);\n        vmat2 = svld1_f64(mat2 + i);\n        vdst = svfmul_x_f64(vmat1, vmat2);\n        svst1_f64(dst + i, vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    svfloat32_t v_vec = svld1_f32(vec);\n    for (size_t i = 0; i < m; i++) {\n        svfloat32_t v_row = svld1_f32(mat + i * n);\n        svfloat32_t v_result = svmla_x(vzero_f32(), v_row, v_vec);\n        svst1_f32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    svfloat32_t v_vec = svld1_f32(vec);\n    for (size_t i = 0; i < m; i++) {\n        svfloat32_t v_row = svld1_f32(mat + i * n);\n        svfloat32_t v_result = svmla_x(vzero_f32(), v_row, v_vec);\n        svst1_f32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    svfloat32_t v_vec = svld1q_f32(vec);\n    for (size_t i = 0; i < m; i++) {\n        svfloat32_t v_row = svld1q_f32(mat + i * n);\n        svfloat32_t v_result = svmlaq_z_f32(vzero_f32(), v_row, v_vec);\n        svst1q_f32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    svfloat32_t v_vec = svld1_f32(vec);\n    for (size_t i = 0; i < m; i++) {\n        svfloat32_t v_row = svld1_f32(mat + i * n);\n        svfloat32_t v_result = svmla_x(vzero_f32(), v_row, v_vec);\n        svst1_f32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    svfloat32_t v_vec = svld1_f32(vec);\n    for (size_t i = 0; i < m; i++) {\n        svfloat32_t v_row = svld1_f32(mat + i * n);\n        svfloat32_t v_result = svmla_x(v_zero_f32(), v_row, v_vec);\n        svst1_f32(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_29_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    svfloat64_t sum = svdup_f64(0.0);\n    size_t total_elements = m * n;\n\n    for (size_t i = 0; i < total_elements; i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(mat + i);\n        sum = svadd_f64(sum, vec);\n    }\n\n    double avg = svget_f64(sum, 0) / total_elements;\n    return avg;\n}\n```"}
{"task_id": "SimdBench_29_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    svfloat64_t sum = svdup_f64(0.0);\n    size_t total_elements = m * n;\n\n    for (size_t i = 0; i < total_elements; i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(mat + i);\n        sum = svadd_f64(sum, vec);\n    }\n\n    double avg = svget_f64(sum, 0) / total_elements;\n    return avg;\n}\n```"}
{"task_id": "SimdBench_29_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    svfloat64_t sum = svdup_f64(0.0);\n    size_t total_elements = m * n;\n    size_t elements_per_vector = svcntd();\n\n    for (size_t i = 0; i < total_elements; i += elements_per_vector) {\n        svfloat64_t vec = svld1_f64(mat + i);\n        sum = svadd_f64(sum, vec);\n    }\n\n    double final_sum = svget_f64(sum, 0);\n    for (size_t i = elements_per_vector; i < total_elements; ++i) {\n        final_sum += mat[i];\n    }\n\n    return final_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    svfloat64_t sum = svdup_f64(0.0);\n    size_t total_elements = m * n;\n    size_t i;\n\n    for (i = 0; i < total_elements; i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(mat + i);\n        sum = svadd_f64(sum, vec);\n    }\n\n    double final_sum = svget_f64(sum, 0);\n    for (; i < total_elements; ++i) {\n        final_sum += mat[i];\n    }\n\n    return final_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    svfloat64_t sum = svdup_f64(0.0);\n    size_t total_elements = m * n;\n    size_t elements_per_vector = svcntd();\n\n    for (size_t i = 0; i < total_elements; i += elements_per_vector) {\n        svfloat64_t vec = svld1_f64(mat + i);\n        sum = svadd_f64(sum, vec);\n    }\n\n    double final_sum = svget_f64(sum, 0);\n    for (size_t i = elements_per_vector; i < total_elements; ++i) {\n        final_sum += mat[i];\n    }\n\n    return final_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = &matrix[i * cols];\n        const bool direction = directions[i];\n        svint32_t v0 = svld1_u32(pg, row);\n        for (size_t j = 1; j < cols; ++j) {\n            svint32_t v1 = svld1_u32(pg, &row[j]);\n            if (direction) {\n                svbool_t mask = svclt_z_s32(pg, v0, v1);\n                if (!svptest_any(mask)) return false;\n            } else {\n                svbool_t mask = svcle_z_s32(pg, v0, v1);\n                if (!svptest_any(mask)) return false;\n            }\n            v0 = v1;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        svint32_t vrow = svld1_u32(pg, row);\n        svbool_t vdir = svld1_u8(pg, directions + i);\n\n        for (size_t j = 1; j < cols; ++j) {\n            const int* next_row = row + j * cols;\n            svint32_t vnrow = svld1_u32(pg, next_row);\n\n            svbool_t va = svcmpgt_z_s32(pg, vnrow, vrow);\n            svbool_t vd = svcmplt_z_s32(pg, vnrow, vrow);\n\n            vrow = vnrow;\n\n            if (svptest_any(pg, svand_b(pg, va, vdir))) {\n                return false;\n            }\n            if (svptest_any(pg, svandnot_b(pg, vd, vdir))) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = &matrix[i * cols];\n        svint32x4_t prev = svld1q_s32(row);\n        for (size_t j = 1; j + 3 < cols; j += 4) {\n            svint32x4_t curr = svld1q_s32(&row[j]);\n            svbool_t asc = svcmpgtq_s32(curr, prev);\n            svbool_t desc = svcmpltq_s32(curr, prev);\n            svbool_t valid_asc = svand_b32(asc, svreinterpret_u32_u8(svptrue_e8()));\n            svbool_t valid_desc = svand_b32(desc, svreinterpret_u32_u8(svptrue_e8()));\n            if (!svallany_b32(valid_asc) && !svallany_b32(valid_desc)) {\n                return false;\n            }\n            prev = curr;\n        }\n        // Handle remaining elements that are not multiples of 4\n        for (; j < cols; ++j) {\n            int curr = row[j];\n            if ((directions[i] && curr < prev) || (!directions[i] && curr > prev)) {\n                return false;\n            }\n            prev = curr;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    svint32_t zero = svdup_n_s32(0);\n    svbool_t all_true = svcntp_b_f64(rows);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = matrix + i * cols;\n        const bool direction = directions[i];\n\n        svint32_t prev = svld1_s32(row_start);\n        svbool_t is_sorted = svcntp_b_f64(cols - 1);\n\n        for (size_t j = 1; j < cols; ++j) {\n            svint32_t curr = svld1_lane_s32(curr, row_start, j);\n            svbool_t cmp = direction ? svlt_s32(prev, curr) : svgt_s32(prev, curr);\n            is_sorted = svandb_m(is_sorted, cmp, svcntp_b_f64(j));\n            prev = curr;\n        }\n\n        all_true = svandb_m(all_true, is_sorted, svcntp_b_f64(i));\n    }\n\n    return svptest_any(all_true);\n}\n```"}
{"task_id": "SimdBench_30_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = matrix + i * cols;\n        svint32_t prev = svld1_u32(pg, row_start);\n        for (size_t j = 1; j < cols; ++j) {\n            svint32_t curr = svld1_lane_u32(pg, row_start, j);\n            svbool_t cond = svcmpgt_u32(pg, prev, curr);\n            if (svptest_any(pg, cond) != 0 && directions[i]) {\n                return false;\n            }\n            svbool_t cond_desc = svcmplt_u32(pg, prev, curr);\n            if (svptest_any(pg, cond_desc) != 0 && !directions[i]) {\n                return false;\n            }\n            prev = curr;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    svuint16_t v_base = svdup_u16(base);\n    svint16_t v_zero = svdup_i16(0);\n\n    for (size_t i = 0; i < length; i += SV_INT16_SIZE) {\n        svint16_t v_src = svld1(svptrue_b16(), src + i);\n        svint16_t v_quotient = svdiv_s16(v_src, v_base);\n        svint16_t v_floor = svmul_s16(v_quotient, v_base);\n        svint16_t v_ceil = svmul_s16(svadd_s16(v_quotient, svdup_i16(1)), v_base);\n        svint16_t v_diff_floor = svsub_s16(v_src, v_floor);\n        svint16_t v_diff_ceil = svsub_s16(v_ceil, v_src);\n        svbool_t v_mask = svcmple_s16(v_diff_floor, v_diff_ceil);\n        svint16_t v_result = svsel(v_mask, v_floor, v_ceil);\n        svst1(svptrue_b16(), dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    svint16_t v_base = svdupw_b16(base);\n    svbool_t mask = svptrue_b16();\n\n    for (size_t i = 0; i < length; i += SV_INT16_SIZE) {\n        svint16_t v_src = svld1x4_u16(src + i);\n        svint16_t v_div = svdiv_s16(v_src, v_base);\n        svint16_t v_mul = svmul_s16(v_div, v_base);\n        svint16_t v_abs_diff = svabsdiff_s16(v_src, v_mul);\n        svint16_t v_next_mul = svmul_s16(svadd_s16(v_div, svdupw_b16(1)), v_base);\n        svint16_t v_min_diff = svmin_s16(v_abs_diff, svabsdiff_s16(v_src, v_next_mul));\n        svuint16_t v_mask = svcmple_s16(v_min_diff, v_abs_diff);\n        svint16_t v_result = svsel(mask, v_mul, v_next_mul);\n\n        svst1x4_u16(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    svint16_t v_base = svdup_n_s16(base);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint16_t v_src = svld1_u16(src + i);\n        svint16_t v_div = svdiv_s16(v_src, v_base);\n        svint16_t v_mul = svmul_s16(v_div, v_base);\n        svint16_t v_abs_diff = svabs_s16(svsub_s16(v_src, v_mul));\n        svint16_t v_next_mul = svadd_s16(v_mul, v_base);\n        svint16_t v_next_abs_diff = svabs_s16(svsub_s16(v_src, v_next_mul));\n        svint16_t v_mask = svcmple_s16(v_abs_diff, v_next_abs_diff);\n        svint16_t v_result = svsel_s16(v_mask, v_mul, v_next_mul);\n        svst1_u16(dst + i, v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    svuint16_t v_base = svdup_u16(base);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint16_t v_src = svld1(svptrue_b16(), src + i);\n        svint16_t v_div = svdiv_s16(v_src, v_base);\n        svint16_t v_mul = svmul_s16(v_div, v_base);\n        svint16_t v_abs_diff = svabs_s16(svsub_s16(v_src, v_mul));\n        svint16_t v_next_mul = svadd_s16(v_mul, v_base);\n        svint16_t v_next_abs_diff = svabs_s16(svsub_s16(v_src, v_next_mul));\n        svbool_t v_mask = svcmple_s16(v_abs_diff, v_next_abs_diff);\n        svst1(svptrue_b16(), dst + i, svsel(v_mask, v_mul, v_next_mul));\n    }\n}\n```"}
{"task_id": "SimdBench_31_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    svuint16_t v_base = svdup_u16(base);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint16_t v_src = svld1(svptrue_b16(), src + i);\n        svint16_t v_div = svdiv_s16(v_src, v_base);\n        svint16_t v_mul = svmul_s16(v_div, v_base);\n        svint16_t v_max = svmax_s16(v_mul, v_src);\n        svst1(svptrue_b16(), dst + i, v_max);\n    }\n}\n```"}
{"task_id": "SimdBench_32_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    svfloat64_t v = svld1_f64(vector, svsetvl_u32(cols));\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t m = svld1_f64(matrix + i * cols, svsetvl_u32(cols));\n        svbool_t match = svcmple_f64(m, v);\n        if (svptest_any_p(match)) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    svfloat64_t v = svld1_f64(vector, svsetvl_u32(cols));\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t m = svld1_f64(matrix + i * cols, svsetvl_u32(cols));\n        svbool_t match = svcmple_f64(m, v);\n        if (svptest_any_p(match)) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    svfloat64_t v = svld1_f64(vector, svsetvl_u32(cols));\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t m = svld1_f64(matrix + i * cols, svsetvl_u32(cols));\n        svbool_t match = svcmple_f64(m, v);\n        if (svptest_any_p(match)) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    svfloat64_t v = svld1_f64(vector, svsetvl_u32(cols));\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t m = svld1_f64(matrix + i * cols, svsetvl_u32(cols));\n        svbool_t match = svcmple_f64(m, v);\n        if (svptest_any_p(match)) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    svfloat64_t v = svld1_f64(vector, svsetvl_u32(cols));\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t m = svld1_f64(matrix + i * cols, svsetvl_u32(cols));\n        svbool_t match = svcmple_f64(m, v);\n        if (svptest_any_p(match)) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    svuint8_t va, vb;\n    svbool_t mask;\n    size_t count = 0;\n\n    for (size_t i = 0; i < dims[0] * dims[1] * dims[2]; i += SV_INT8_ELEMS) {\n        va = svld1_u8(A + i * dims[3]);\n        vb = svld1_u8(B + i * dims[3]);\n\n        mask = svcmpeq_u8(va, vb);\n        count += svcntp_b(mask, SV_INT8_ELEMS);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    svuint8_t zero = svdup_u8(0);\n    svbool_t mask;\n\n    for (size_t i = 0; i < dims[0]; ++i) {\n        for (size_t j = 0; j < dims[1]; ++j) {\n            for (size_t k = 0; k < dims[2]; ++k) {\n                for (size_t l = 0; l < dims[3]; l += 16) {\n                    svuint8_t va = svld1_u8(A + ((i * dims[1] + j) * dims[2] + k) * dims[3] + l);\n                    svuint8_t vb = svld1_u8(B + ((i * dims[1] + j) * dims[2] + k) * dims[3] + l);\n\n                    mask = sveor_u8(va, vb);\n                    total_count += svcntp_b(mask);\n                }\n            }\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_SVE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    svuint8_t vA, vB, vEqual;\n    size_t i;\n\n    for (i = 0; i < dims[0] * dims[1] * dims[2]; i += SV_INT8_ELEMS) {\n        vA = svld1_u8(&A[i]);\n        vB = svld1_u8(&B[i]);\n        vEqual = sveor_u8(vA, vB);\n        total_count += svcntp_b32(svptrue_b8(), svcntp_u8(vEqual));\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_SVE", "completion": "```c\n#include <arm_sve.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    svbool_t p = svptrue_b8();\n    size_t remaining_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t vector_size = svcntb();\n\n    while (remaining_elements >= vector_size) {\n        svuint8_t va = svld1_u8(A);\n        svuint8_t vb = svld1_u8(B);\n        svbool_t eq = sveor_z_b8(p, va, vb);\n        size_t count = svcntw_b8(eq);\n        total_count += count;\n        A += vector_size;\n        B += vector_size;\n        remaining_elements -= vector_size;\n    }\n\n    if (remaining_elements > 0) {\n        svuint8_t va = svld1_u8(A);\n        svuint8_t vb = svld1_u8(B);\n        svbool_t eq = sveor_z_b8(p, va, vb);\n        size_t count = svcntw_b8(eq);\n        total_count += count;\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_SVE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    svuint8_t vA, vB, vEq;\n    size_t i = 0;\n\n    while (i < dims[3] * dims[2] * dims[1] * dims[0]) {\n        size_t remaining = dims[3] * dims[2] * dims[1] * dims[0] - i;\n        size_t vec_size = min(remaining, SV_MAX_VL / sizeof(uint8_t));\n\n        vA = svld1_u8(&A[i]);\n        vB = svld1_u8(&B[i]);\n\n        vEq = sveor_u8(vA, vB);\n\n        count += svcntb_z_u8(vEq);\n\n        i += vec_size;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    svint64_t va = svdup_n_s64(a);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint64_t vx = svld1_m(vl, x + i, svptrue_b64());\n        svint64_t vy = svabs_s64(vx);\n        svint64_t vmul = svmul_n_s64(vy, a);\n        svst1_m(vl, y + i, vmul, svptrue_b64());\n    }\n}\n```"}
{"task_id": "SimdBench_34_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    svint64_t va = svdup_n_s64(a);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint64_t vx = svld1_m(vl, x + i, svptrue_b64());\n        svint64_t vy = svabs_s64(vx);\n        vy = sveor_s64(vy, svdup_n_s64(a));\n        svst1_m(vl, y + i, vy, svptrue_b64());\n    }\n}\n```"}
{"task_id": "SimdBench_34_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    svint64_t va = svdup_n_s64(a);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint64_t vx = svld1_m(vl, x + i, svptrue_b64());\n        svint64_t vy = svabs_s64(vx);\n        vy = sveor_s64(vy, va);\n        svst1_m(vl, y + i, vy, svptrue_b64());\n    }\n}\n```"}
{"task_id": "SimdBench_34_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    svint64_t va = svdup_n_s64(a);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint64_t vx = svld1_m(vl, x + i, svptrue_b64());\n        svint64_t vy = svabs_s64(vx);\n        svint64_t vz = svsub_s64(vy, va);\n        svst1_m(vl, y + i, vz, svptrue_b64());\n    }\n}\n```"}
{"task_id": "SimdBench_34_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    svint64_t va = svdup_n_s64(a);\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint64_t vx = svld1_m(vl, x + i, svptrue_b64());\n        svint64_t vy = svabs_s64(vx);\n        svint64_t vmul = svmul_n_s64(vy, a);\n        svst1_m(vl, y + i, vmul, svptrue_b64());\n    }\n}\n```"}
{"task_id": "SimdBench_35_SVE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    svint16_t v_min = svdup_n_s16(INT16_MAX);\n    svint16_t v_max = svdup_n_s16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        svint16_t v = svld1_s16(&vec[i]);\n        v_min = svminv_s16(v_min, v);\n        v_max = svmaxv_s16(v_max, v);\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < v_min[0]) v_min[0] = vec[i];\n        if (vec[i] > v_max[0]) v_max[0] = vec[i];\n    }\n\n    MinMaxPair result = {svget_lane_s16(v_min, 0), svget_lane_s16(v_max, 0)};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SVE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    svint16_t v_min = svld1_u16(vec);\n    svint16_t v_max = svld1_u16(vec);\n\n    for (size_t i = 1; i * 16 <= length; i++) {\n        svint16_t v = svld1_u16(&vec[i * 16]);\n        v_min = svminv_s16(v_min, v);\n        v_max = svmaxv_s16(v_max, v);\n    }\n\n    int16_t min_val = svget_lane_s16(v_min, 0);\n    int16_t max_val = svget_lane_s16(v_max, 0);\n\n    for (size_t i = 16 * (length / 16); i < length; i++) {\n        if (vec[i] < min_val) {\n            min_val = vec[i];\n        }\n        if (vec[i] > max_val) {\n            max_val = vec[i];\n        }\n    }\n\n    MinMaxPair result = {min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SVE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    svint16_t v_min = svdupw_s16(INT16_MAX);\n    svint16_t v_max = svdupw_s16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        svint16_t v = svld1_s16(&vec[i]);\n        v_min = svminv_s16(v_min, v);\n        v_max = svmaxv_s16(v_max, v);\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < v_min[0]) v_min[0] = vec[i];\n        if (vec[i] > v_max[0]) v_max[0] = vec[i];\n    }\n\n    MinMaxPair result = {svget_lane_s16(v_min, 0), svget_lane_s16(v_max, 0)};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SVE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    svint16_t v_min = svdupw_s16(INT16_MAX);\n    svint16_t v_max = svdupw_s16(INT16_MIN);\n\n    for (size_t i = 0; i < length; i += SV_INT16_ELEMS) {\n        svint16_t v_vec = svld1_s16(&vec[i]);\n        v_min = svminv_s16(v_min, v_vec);\n        v_max = svmaxv_s16(v_max, v_vec);\n    }\n\n    int16_t min_num = svgetlane_s16(v_min, 0);\n    int16_t max_num = svgetlane_s16(v_max, 0);\n\n    MinMaxPair result = {min_num, max_num};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SVE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    svint16_t v_min = svdup_n_s16(INT16_MAX);\n    svint16_t v_max = svdup_n_s16(INT16_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        svint16_t v = svld1_s16(&vec[i]);\n        v_min = svmin_s16(v_min, v);\n        v_max = svmax_s16(v_max, v);\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] < v_min[0]) {\n            v_min[0] = vec[i];\n        }\n        if (vec[i] > v_max[0]) {\n            v_max[0] = vec[i];\n        }\n    }\n\n    MinMaxPair result = {svget_lane_s16(v_min, 0), svget_lane_s16(v_max, 0)};\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    svint32_t zero = svdup_n_s32(0);\n    svbool_t all_true = svcntp_b32(rows);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = matrix + i * cols;\n        svint32_t prev = svld1_s32(row_start);\n        svbool_t is_increasing = sveor_z_b32(all_true, svcmpgt_z_s32(svptrue_b32(), prev, svld1_s32(row_start + 1)));\n\n        for (size_t j = 1; j < cols - 1; ++j) {\n            prev = svld1_s32(row_start + j);\n            is_increasing &= sveor_z_b32(all_true, svcmpgt_z_s32(svptrue_b32(), prev, svld1_s32(row_start + j + 1)));\n        }\n\n        if (!svptest_any(is_increasing)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    svint32_t zero = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n\n        svint32_t prev = svld1_s32(row);\n        svint32_t curr = svld1_s32(row + cols);\n\n        svbool_t cmp = svcmpgt_z_s32(mask, curr, prev);\n\n        if (!svptest_any(mask, cmp)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    svint32_t zero = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = matrix + i * cols;\n        svint32_t prev = svld1_s32(row_start);\n\n        for (size_t j = 1; j < cols; ++j) {\n            svint32_t curr = svld1_lane_s32(curr, row_start, j);\n            svbool_t cond = svgt_s32(prev, curr);\n            mask = svand(mask, cond);\n            prev = curr;\n        }\n    }\n\n    return svall(mask);\n}\n```"}
{"task_id": "SimdBench_36_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    svint32_t zero = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = &matrix[i * cols];\n        svint32_t prev = svld1_s32(row);\n\n        for (size_t j = 1; j < cols; ++j) {\n            svint32_t curr = svld1_lane_s32(curr, row, j);\n            svbool_t cond = svlt_z_s32(prev, curr);\n            mask = svand_b32(mask, cond);\n            prev = curr;\n        }\n    }\n\n    return svall(mask);\n}\n```"}
{"task_id": "SimdBench_36_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    svint32_t zero = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_start = matrix + i * cols;\n\n        for (size_t j = 1; j < cols; ++j) {\n            svint32_t current = svld1_s32(row_start + j - 1);\n            svint32_t next = svld1_s32(row_start + j);\n\n            svbool_t cmp = svcmpgt_z_s32(mask, next, current);\n            mask = svand_b32(mask, cmp);\n        }\n\n        if (!svptest_any_p(mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    svint32_t first_block = svld1_u32(matrix);\n\n    for (size_t row = block_size; row < rows; row += block_size) {\n        for (size_t col = 0; col < cols; col += block_size) {\n            const int* block_ptr = &matrix[row * cols + col];\n            svint32_t current_block = svld1_u32(block_ptr);\n            svbool_t mask = sveor_z_pst(svptrue_b32(), first_block, current_block);\n            if (!svptest_any(mask)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    svint32_t first_block = svld1_u32(matrix);\n\n    for (size_t row = block_size; row < rows; row += block_size) {\n        for (size_t col = 0; col < cols; col += block_size) {\n            const int* block_ptr = &matrix[row * cols + col];\n            svint32_t current_block = svld1_u32(block_ptr);\n            svbool_t mask = sveor_z(svptrue_b32(), first_block, current_block);\n            if (!svptest_any(mask)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    svint32_t first_block = svld1_u32(matrix);\n\n    for (size_t row = block_size; row < rows; row += block_size) {\n        for (size_t col = 0; col < cols; col += block_size) {\n            const int* block_ptr = &matrix[row * cols + col];\n            svint32_t current_block = svld1_u32(block_ptr);\n            svbool_t mask = sveor_z_pst(svptrue_b32(), first_block, current_block);\n            if (!svptest_any(mask)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    svint32_t first_block = svld1_u32(matrix);\n\n    for (size_t row = block_size; row < rows; row += block_size) {\n        for (size_t col = 0; col < cols; col += block_size) {\n            const int* block_ptr = &matrix[row * cols + col];\n            svint32_t current_block = svld1_u32(block_ptr);\n            svbool_t mask = sveor_z(svptrue_b32(), first_block, current_block);\n            if (!svptest_any(mask)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    svint32_t first_block = svld1_u32(matrix);\n\n    for (size_t row = block_size; row < rows; row += block_size) {\n        for (size_t col = 0; col < cols; col += block_size) {\n            const int* block_ptr = &matrix[row * cols + col];\n            svint32_t current_block = svld1_u32(block_ptr);\n            svbool_t mask = sveor_z_pst(svptrue_b32(), first_block, current_block);\n            if (!svptest_any(mask)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    svfloat64_t first_block = svld1_f64(vec);\n    svfloat64_t tolerance_vec = svdup_n_f64(tolerance);\n\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        svfloat64_t current_block = svld1_f64(vec + i * block_size);\n        svfloat64_t diff = svabs_f64(svsub_f64(current_block, first_block));\n        svbool_t mask = svcmple_f64(diff, tolerance_vec);\n        if (!svptest_any(mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    svfloat64_t v_tolerance = svdup_f64(tolerance);\n    svbool_t mask = svptrue_b64();\n\n    for (size_t i = 0; i <= length - block_size; i += block_size) {\n        svfloat64_t v_first_block = svld1_f64(&vec[i]);\n        svfloat64_t v_current_block = svld1_f64(&vec[i + block_size]);\n\n        svfloat64_t v_diff = svsub_f64(v_current_block, v_first_block);\n        svfloat64_t v_abs_diff = svabs_f64(v_diff);\n\n        mask = svcmple_f64(mask, v_abs_diff, v_tolerance);\n    }\n\n    return svptest_any(mask);\n}\n```"}
{"task_id": "SimdBench_38_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    svfloat64_t first_block = svld1_f64(vec);\n    svfloat64_t tolerance_vec = svdup_n_f64(tolerance);\n\n    size_t num_blocks = length / block_size;\n    const double* current_block = vec;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        svfloat64_t current_block_vec = svld1_f64(current_block);\n        svfloat64_t diff = svabs_f64(svsub_f64(current_block_vec, first_block));\n        svbool_t mask = svcmple_f64(diff, tolerance_vec);\n        if (!svptest_any(mask)) {\n            return false;\n        }\n        current_block += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    svfloat64_t first_block = svld1_f64(vec);\n    svfloat64_t tolerance_vec = svdup_n_f64(tolerance);\n\n    size_t num_blocks = length / block_size;\n    const double* ptr = vec + block_size;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        svfloat64_t current_block = svld1_f64(ptr);\n        svfloat64_t diff = svabs_f64(svsub_f64(current_block, first_block));\n        svbool_t mask = svcmple_f64(diff, tolerance_vec);\n        if (!svptest_any(mask)) {\n            return false;\n        }\n        ptr += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length < block_size || block_size == 0) {\n        return false;\n    }\n\n    svfloat64_t first_block = svld1_f64(vec);\n    svfloat64_t tolerance_vec = svdup_n_f64(tolerance);\n\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        svfloat64_t current_block = svld1_f64(vec + i * block_size);\n        svfloat64_t diff = svabs_f64(svsub_f64(current_block, first_block));\n        svbool_t mask = svcmple_f64(diff, tolerance_vec);\n        if (!svptest_any(mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    svfloat64_t v_threshold;\n    svbool_t vb;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_start = &matrix[i * cols];\n        svfloat64_t v_sum = svdup_f64(0.0);\n\n        for (size_t j = 0; j < cols; j += SV_INT64_ELEMS) {\n            svfloat64_t v_row = svld1_f64(&row_start[j]);\n            v_sum = svmla_f64(v_sum, v_row, v_row);\n        }\n\n        v_threshold = svld1_f64(&thresholds[i]);\n        vb = sveor_p_b64(svptrue_b64(), svcmpgt_f64(v_sum, v_threshold));\n        svst1_u8(result + i, svreinterpret_u8_m_b64(vb));\n    }\n}\n```"}
{"task_id": "SimdBench_39_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    svfloat64_t zero = svdup_f64(0.0);\n    svbool_t ptrue = svptrue_b64();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_start = matrix + i * cols;\n        svfloat64_t sum = svdup_f64(0.0);\n\n        for (size_t j = 0; j < cols; j += SV_INT64_ELEMS) {\n            svfloat64_t vec = svld1_f64(row_start + j);\n            sum = svmla_f64(sum, vec, vec);\n        }\n\n        svfloat64_t norm = svrsqrte_f64(svadd_f64(sum, zero));\n        svfloat64_t threshold = vld1q_lane_f64(thresholds + i, svdupq_n_f64(0.0), 0);\n\n        svbool_t cond = svcmpgt_f64(norm, threshold);\n        svst1_u8(result + i, svreinterpret_u8(cond));\n    }\n}\n```"}
{"task_id": "SimdBench_39_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    svfloat64_t zero = svdup_f64(0.0);\n    svbool_t ptrue = svptrue_b64();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_start = &matrix[i * cols];\n        svfloat64_t sum = svdup_f64(0.0);\n\n        for (size_t j = 0; j < cols; j += SV_INT64_ELEMS) {\n            svfloat64_t vec = svld1_f64(&row_start[j]);\n            sum = svmla_f64(sum, vec, vec);\n        }\n\n        svfloat64_t norm = svrsqrte_f64(svadd_f64(sum, zero));\n        svfloat64_t threshold = vld1q_lane_f64(&thresholds[i], svdupq_n_f64(thresholds[i]), 0);\n\n        svbool_t cond = svcmpgt_f64(norm, threshold);\n        svst1_u8(result + i, svreinterpret_u8(cond));\n    }\n}\n```"}
{"task_id": "SimdBench_39_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    svfloat64_t v_thresholds;\n    svbool_t v_all_true = svptrue_b64();\n    svuint32_t v_col_indices = svdup_u32(0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        svfloat64_t v_sum = svdup_f64(0.0);\n        for (size_t j = 0; j < cols; j += 8) {\n            svfloat64_t v_row = svld1_f64(matrix + i * cols + j);\n            v_sum = svmla_x(v_sum, v_row, v_row);\n        }\n        svfloat64_t v_norm = svrsqrteq_f64(v_sum);\n        v_norm = svmul_x(v_norm, v_norm);\n        v_thresholds = svld1_f64(thresholds + i);\n        svbool_t v_gt = svcmpgt_f64(v_norm, v_thresholds);\n        svst1_b64(result + i, v_gt);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    svfloat64_t zero = svdup_f64(0.0);\n    svbool_t ptrue = svptrue_b64();\n\n    for (size_t i = 0; i < rows; ++i) {\n        const double* row_start = matrix + i * cols;\n        svfloat64_t sum = svdup_f64(0.0);\n\n        for (size_t j = 0; j < cols; j += SV_INT64_ELEMS) {\n            svfloat64_t vec = svld1_f64(row_start + j);\n            sum = svmla_x_f64(sum, vec, vec);\n        }\n\n        svfloat64_t norm = svrsqrteq_f64(svadd_f64(sum, svdup_f64(1e-12)));\n        svfloat64_t threshold = vld1_f64(thresholds + i);\n        svbool_t comp = svcmpge_f64(norm, threshold);\n\n        svst1_u8(result + i, svreinterpret_u8(comp));\n    }\n}\n```"}
{"task_id": "SimdBench_40_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    svuint32_t va, vb, vc;\n    size_t i = 0;\n\n    while (i + 8 <= length) {\n        va = svld1_u32(&A[i]);\n        vb = svld1_u32(&B[i]);\n        vc = svor_u32(va, vb);\n        svst1_u32(&C[i], vc);\n        i += 8;\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    svbool_t pg = svwhilelt_b32(length);\n    for (; !svptestall(pg); pg = svwhilelt_b32(length)) {\n        svuint32_t va = svld1_u32(pg, A);\n        svuint32_t vb = svld1_u32(pg, B);\n        svst1_u32(pg, C, svor_u32(va, vb));\n        A += svcntp_b32(pg);\n        B += svcntp_b32(pg);\n        C += svcntp_b32(pg);\n    }\n}\n```"}
{"task_id": "SimdBench_40_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    svuint32_t va, vb, vc;\n    size_t i = 0;\n\n    for (; i <= length - 16; i += 16) {\n        va = svld1_u32(&A[i]);\n        vb = svld1_u32(&B[i]);\n        vc = svor_u32(va, vb);\n        svst1_u32(&C[i], vc);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    svbool_t pg = svwhilelt_b32(length);\n    for (; !svptestall(pg); pg = svwhilelt_b32(length)) {\n        svuint32_t va = svld1_u32(pg, A);\n        svuint32_t vb = svld1_u32(pg, B);\n        svst1_u32(pg, C, svor_u32(va, vb));\n        A += svcntp_b32(pg);\n        B += svcntp_b32(pg);\n        C += svcntp_b32(pg);\n    }\n}\n```"}
{"task_id": "SimdBench_40_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    svbool_t pg = svwhilelt_b32(length);\n    for (; !svptestall(pg); pg = svwhilelt_b32(length)) {\n        svuint32_t va = svld1_u32(pg, A);\n        svuint32_t vb = svld1_u32(pg, B);\n        svst1_u32(pg, C, svor_u32(va, vb));\n        A += svcntp_b32(pg);\n        B += svcntp_b32(pg);\n        C += svcntp_b32(pg);\n    }\n}\n```"}
{"task_id": "SimdBench_41_SVE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    svint8_t va, vb;\n    svint16_t vsum_sq_a = svdup_n_s16(0), vsum_sq_b = svdup_n_s16(0);\n    svint32_t vsum_prod = svdup_n_s32(0);\n\n    for (size_t i = 0; i < length; i += 16) {\n        va = svld1_s8(&A[i]);\n        vb = svld1_s8(&B[i]);\n\n        vsum_sq_a = svadd_x_s16(vsum_sq_a, svmul_x_s16(va, va));\n        vsum_sq_b = svadd_x_s16(vsum_sq_b, svmul_x_s16(vb, vb));\n        vsum_prod = svadd_x_s32(vsum_prod, svdot_s32_z_s16(svzero_s16(), va, vb));\n    }\n\n    int64_t sum_sq_a = svreinterpret_s64_u64(svgetlane_u64(svreinterpret_u64_s16(vsum_sq_a), 0));\n    int64_t sum_sq_b = svreinterpret_s64_u64(svgetlane_u64(svreinterpret_u64_s16(vsum_sq_b), 0));\n    int64_t sum_prod = svreinterpret_s64_u64(svgetlane_u64(svreinterpret_u64_s32(vsum_prod), 0));\n\n    return sum_sq_a - sum_sq_b + 2 * sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_SVE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    svint8_t va, vb;\n    svint16_t vsum_sq_a = svdup_n_s16(0), vsum_sq_b = svdup_n_s16(0);\n    svint32_t vsum_prod = svdup_n_s32(0);\n\n    for (size_t i = 0; i < length; i += SV_INT8_SIZE) {\n        va = svld1_u8(&A[i]);\n        vb = svld1_u8(&B[i]);\n\n        vsum_sq_a = svadd_x_s16(vsum_sq_a, svmul_x_s16(va, va));\n        vsum_sq_b = svadd_x_s16(vsum_sq_b, svmul_x_s16(vb, vb));\n        vsum_prod = svadd_x_s32(vsum_prod, svdot_s16_z_s8(va, vb, svdup_n_s8(1)));\n    }\n\n    int64_t sum_sq_a = svreinterpret_s64_m_i16(vsum_sq_a)[0];\n    int64_t sum_sq_b = svreinterpret_s64_m_i16(vsum_sq_b)[0];\n    int64_t sum_prod = svreinterpret_s64_m_i32(vsum_prod)[0];\n\n    return sum_sq_a - sum_sq_b + 2 * sum_prod;\n}\n```"}
{"task_id": "SimdBench_41_SVE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    svint8_t va, vb;\n    svint32_t vsumsq = svdup_n_s32(0);\n    svint32_t vsumprod = svdup_n_s32(0);\n\n    for (size_t i = 0; i < length; i += SV_INT8_SIZE) {\n        va = svld1_u8(&A[i]);\n        vb = svld1_u8(&B[i]);\n\n        svint32_t vdiff = svsub_x_s32(va, vb);\n        svint32_t vsq = svmul_x_s32(vdiff, vdiff);\n        vsumsq = svadd_x_s32(vsumsq, vsq);\n\n        svint32_t vdot = svdot_s32_z(svptrue_b8(), va, vb);\n        vsumprod = svadd_x_s32(vsumprod, vdot);\n    }\n\n    int32_t sumsq = svgetlane_s32(vsumsq, 0);\n    int32_t sumprod = svgetlane_s32(vsumprod, 0);\n\n    return (int64_t)sumsq - (int64_t)sumprod;\n}\n```"}
{"task_id": "SimdBench_41_SVE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    svint8_t va, vb;\n    svint32_t vsum_squares = svdup_n_s32(0);\n    svint32_t vsum_products = svdup_n_s32(0);\n\n    for (size_t i = 0; i < length; i += SV_INT8_SIZE) {\n        va = svld1_u8(&A[i]);\n        vb = svld1_u8(&B[i]);\n\n        svint32_t vdiff = svsub_x_s32(va, vb);\n        svint32_t vsqr_diff = svmul_x_s32(vdiff, vdiff);\n        vsum_squares = svadd_x_s32(vsum_squares, vsqr_diff);\n\n        svint32_t vprod = svmul_x_s32(va, vb);\n        vsum_products = svadd_x_s32(vsum_products, vprod);\n    }\n\n    int32_t sum_squares = svgetlane_s32(vsum_squares, 0);\n    int32_t sum_products = svgetlane_s32(vsum_products, 0);\n\n    return (int64_t)sum_squares - (int64_t)sum_products;\n}\n```"}
{"task_id": "SimdBench_41_SVE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    svint8_t va, vb;\n    svint32_t vsum_squares = svdup_n_s32(0);\n    svint32_t vsum_products = svdup_n_s32(0);\n\n    for (size_t i = 0; i < length; i += SV_INT8_SIZE) {\n        va = svld1_u8(&A[i]);\n        vb = svld1_u8(&B[i]);\n\n        svint32_t vsquares = svmulb_x_s32(va, va);\n        svint32_t vproducts = svmlab_x_s32(va, vb);\n\n        vsum_squares = svaddlv_s32(vsum_squares, vsquares);\n        vsum_products = svaddlv_s32(vsum_products, vproducts);\n    }\n\n    int32_t sum_squares = svgetlane_s32(vsum_squares, 0);\n    int32_t sum_products = svgetlane_s32(vsum_products, 0);\n\n    return ((int64_t)sum_squares - (int64_t)sum_products);\n}\n```"}
{"task_id": "SimdBench_42_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    svbool_t pg = svwhilelt_b64(length);\n    for (; !svptestall(pg); pg = svwhilelt_b64(svadd_z_u64(pg, svdup_imm_u64(1)))) {\n        svuint64_t va = svld1_u64(pg, A);\n        svuint64_t vb = svld1_u64(pg, B);\n        svuint64_t vc = svand_not_u64(va, vb);\n        svst1_u64(pg, C, vc);\n        A += svcntp_b64(pg, 1);\n        B += svcntp_b64(pg, 1);\n        C += svcntp_b64(pg, 1);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    svbool_t pg = svwhilelt_b64(length);\n    for (; !svptestall(pg); pg = svwhilelt_b64(svadd_z_u32(pg, svdup_imm_u32(1)))) {\n        svuint64_t va = svld1pg_b64(pg, A);\n        svuint64_t vb = svld1pg_b64(pg, B);\n        svuint64_t vc = svand_z_u64(pg, va, svnot_z_u64(pg, vb));\n        svst1pg_b64(pg, C, vc);\n        A += svcntp_b64(pg, 0);\n        B += svcntp_b64(pg, 0);\n        C += svcntp_b64(pg, 0);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    svbool_t pg = svptrue_b64();\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        svuint64_t va = svld1_u64(pg, &A[i]);\n        svuint64_t vb = svld1_u64(pg, &B[i]);\n        svuint64_t vc = svand_z(pg, va, svnot_z(pg, vb));\n        svst1_u64(pg, &C[i], vc);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    svuint64_t va, vb, vc;\n    size_t i = 0;\n\n    while (i + 8 <= length) {\n        va = svld1_u64(&A[i]);\n        vb = svld1_u64(&B[i]);\n        vc = svand_not_z_u64(va, vb);\n        svst1_u64(&C[i], vc);\n        i += 8;\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    svuint64_t va, vb, vc;\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        va = svld1_u64(&A[i]);\n        vb = svld1_u64(&B[i]);\n        vc = svand_u64(va, svnot_u64(vb));\n        svst1_u64(&C[i], vc);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    svbool_t pg = svptrue_b8();\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint8x16_t va = svld1_x16(pg, A + i);\n        svint8x16_t vb = svld1_x16(pg, B + i);\n\n        svint8x16_t vpositive_a = svcmpgt_z_s8(pg, va, svdup_n_s8(0));\n        svint8x16_t vnegative_a = svcmplt_z_s8(pg, va, svdup_n_s8(0));\n        svint8x16_t vpositive_b = svcmpgt_z_s8(pg, vb, svdup_n_s8(0));\n        svint8x16_t vnegative_b = svcmplt_z_s8(pg, vb, svdup_n_s8(0));\n\n        svint8x16_t vresult = svdup_n_s8(0);\n        vresult = svsel_z(pg, vresult, svdup_n_s8(1), svand_z(pg, vpositive_a, vpositive_b));\n        vresult = svsel_z(pg, vresult, svdup_n_s8(-1), svand_z(pg, vnegative_a, vnegative_b));\n\n        svst1_x16(pg, C + i, vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_43_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    svbool_t p = svptrue_b8();\n    size_t vl = svcntb();\n\n    for (size_t i = 0; i < length; i += vl) {\n        int8_t a[vl], b[vl], c[vl];\n        memcpy(a, A + i, sizeof(a));\n        memcpy(b, B + i, sizeof(b));\n\n        svint8_t va = svld1_u8(a);\n        svint8_t vb = svld1_u8(b);\n\n        svint8_t vpositive_a = svcmpgt_z_s8(va, svdup_i8(0), p);\n        svint8_t vnegative_a = svcmplt_z_s8(va, svdup_i8(0), p);\n        svint8_t vpositive_b = svcmpgt_z_s8(vb, svdup_i8(0), p);\n        svint8_t vnegative_b = svcmplt_z_s8(vb, svdup_i8(0), p);\n\n        svint8_t vresult = svsel_z(p, svdup_i8(1), svand_z(p, vpositive_a, vpositive_b), svdup_i8(0));\n        vresult = svsel_z(p, svneg_z(p, vresult), svand_z(p, vnegative_a, vnegative_b), vresult);\n\n        memcpy(C + i, &vresult, sizeof(c));\n    }\n}\n```"}
{"task_id": "SimdBench_43_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    svbool_t pg = svptrue_b8();\n    size_t vl = svcntw();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint8x16_t va = svld1_x16(pg, A + i);\n        svint8x16_t vb = svld1_x16(pg, B + i);\n\n        svint8x16_t pos_mask = svcmpgt_z_s8(pg, va, svdup_n_s8(0));\n        svint8x16_t neg_mask = svcmplt_z_s8(pg, va, svdup_n_s8(0));\n\n        svint8x16_t result = svdup_n_s8(0);\n        result = svsel(pos_mask & neg_mask, svdup_n_s8(1), result);\n        result = svsel(~pos_mask & ~neg_mask, svdup_n_s8(-1), result);\n\n        svst1_x16(pg, C + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_43_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    svbool_t p = svptrue_b8();\n    size_t vl = svcntb();\n\n    for (size_t i = 0; i < length; i += vl) {\n        svint8x16_t va = svld1_x16(p, A + i);\n        svint8x16_t vb = svld1_x16(p, B + i);\n        svint8x16_t vc = svdup_n_s8(0);\n\n        svint8x16_t mask_pos = svcmpgt_z_s8(p, va, svdup_n_s8(0));\n        svint8x16_t mask_neg = svcmplt_z_s8(p, va, svdup_n_s8(0));\n\n        vc = svsel(mask_pos, svdup_n_s8(1), vc);\n        vc = svsel(svand_z_s8(p, mask_pos, mask_neg), svdup_n_s8(-1), vc);\n\n        svst1_x16(p, C + i, vc);\n    }\n}\n```"}
{"task_id": "SimdBench_43_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    svbool_t pg = svptrue_b8();\n    size_t vl = svcntb();\n\n    for (size_t i = 0; i < length; i += vl) {\n        int8x16_t va = vld1q_s8(&A[i]);\n        int8x16_t vb = vld1q_s8(&B[i]);\n        int8x16_t vc = vdupq_n_s8(0);\n\n        svbool_t mask_pos = svcmpgt_b8(va, vdupq_n_s8(0));\n        svbool_t mask_neg = svcmplt_b8(va, vdupq_n_s8(0));\n\n        vc = svsel(mask_pos, vc, vdupq_n_s8(1));\n        vc = svsel(svand_b8(mask_pos, mask_neg), vc, vdupq_n_s8(-1));\n\n        vst1q_s8(&C[i], vc);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    svbool_t p = svptrue_b16();\n    size_t vl = svcntp_b16(p);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t remaining = length - i;\n        size_t actual_vl = remaining > vl ? vl : remaining;\n\n        svuint16_t va = svld1_u16(&A[i]);\n        svuint16_t vb = svld1_u16(&B[i]);\n\n        svuint16_t mid_mask = svdup_n_u16(i + actual_vl / 2 == length / 2);\n        svuint16_t xor_result = svor_u16(va, vb);\n        svuint16_t and_result = svand_u16(va, vb);\n\n        svst1_u16(&C[i], svsel(mid_mask, va, svand(xor_result, and_result)));\n    }\n}\n```"}
{"task_id": "SimdBench_44_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    svbool_t p = svptrue_b16();\n    size_t vl = svcntp_b16(p);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t remaining = length - i;\n        size_t effective_vl = remaining > vl ? vl : remaining;\n\n        svuint16_t va = svld1_u16(&A[i]);\n        svuint16_t vb = svld1_u16(&B[i]);\n\n        svuint16_t mid_mask = svdup_n_u16(i + effective_vl / 2 == length / 2);\n        svuint16_t xor_result = svorn_x(va, vb);\n        svuint16_t and_result = svand_x(va, vb);\n\n        svst1_u16(&C[i], svsel(mid_mask, va, svor(xor_result, and_result)));\n    }\n}\n```"}
{"task_id": "SimdBench_44_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    svbool_t p = svptrue_b16();\n    size_t vl = svcntp_b16(p);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t remaining = length - i;\n        size_t effective_vl = remaining > vl ? vl : remaining;\n\n        svuint16_t va = svld1_u16(&A[i]);\n        svuint16_t vb = svld1_u16(&B[i]);\n\n        svuint16_t mid_mask = svdup_n_u16(i + effective_vl / 2 == length / 2);\n        svuint16_t xor_result = sveor_u16(va, vb);\n        svuint16_t and_result = svtst_u16(va, vb);\n\n        svuint16_t result = svsel_z_u16(mid_mask, va, svadd_u16(xor_result, and_result));\n\n        svst1_u16(&C[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    svbool_t p = svptrue_b16();\n    size_t vl = svcntp_b16(p);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t remaining = length - i;\n        size_t process = remaining > vl ? vl : remaining;\n\n        svuint16_t va = svld1_u16(&A[i]);\n        svuint16_t vb = svld1_u16(&B[i]);\n\n        svuint16_t vmid = svdup_n_u16(i + process / 2);\n        svuint16_t vmid_mask = svcmple_z_u16(vmid, svdup_n_u16(length));\n\n        svuint16_t vxor = svbitwise_xor_u16(va, vb, vmid_mask);\n        svuint16_t vand = svbitwise_and_u16(va, vb, vmid_mask);\n        svuint16_t vkeep = svselect(vmid_mask, va, svzero_u16());\n\n        svst1_u16(&C[i], svadd_u16(svadd_u16(vxor, vand), vkeep));\n    }\n}\n```"}
{"task_id": "SimdBench_44_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    svbool_t p = svptrue_b16();\n    size_t vl = svcntp_b16(p);\n\n    for (size_t i = 0; i < length; i += vl) {\n        size_t remaining = length - i;\n        size_t effective_vl = remaining > vl ? vl : remaining;\n\n        svuint16_t va = svld1_u16(&A[i]);\n        svuint16_t vb = svld1_u16(&B[i]);\n\n        svuint16_t mid_mask = svdup_n_u16(i + effective_vl / 2 == length / 2);\n        svuint16_t xor_result = svorn_x(va, vb);\n        svuint16_t and_result = svand_x(va, vb);\n\n        svst1_u16(&C[i], svsel_x(mid_mask, va, svor_x(xor_result, and_result)));\n    }\n}\n```"}
{"task_id": "SimdBench_45_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    svbool_t p = svptrue_b32();\n    size_t i = 0;\n    for (; i + 16 <= dim1 * dim2 * dim3; i += 16) {\n        svfloat32_t va = svld1_f32(&A[i]);\n        svfloat32_t vb = svld1_f32(&B[i]);\n        svfloat32_t vc = svld1_f32(&C[i]);\n        svfloat32_t vd = svld1_f32(&D[i]);\n        svuint8x16_t vcond1 = svreinterpret_u8_x16(svld1_u8(&cond1[i]));\n        svuint8x16_t vcond2 = svreinterpret_u8_x16(svld1_u8(&cond2[i]));\n\n        svbool_t mask1 = svcmpne_u8(vcond1, svdup_u8(0));\n        svbool_t mask2 = svcmpne_u8(vcond2, svdup_u8(0));\n\n        svfloat32_t vout = svsel(mask1 & mask2, va, vb);\n        vout = svsel(mask1 & ~mask2, vout, vc);\n        vout = svsel(~mask1 & mask2, vout, vd);\n\n        svst1_f32(&out[i], vout);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    svbool_t p = svptrue_b32();\n    size_t n = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 16 <= n) {\n        svfloat32_t va = svld1q_f32(A + i);\n        svfloat32_t vb = svld1q_f32(B + i);\n        svfloat32_t vc = svld1q_f32(C + i);\n        svfloat32_t vd = svld1q_f32(D + i);\n\n        svuint32x4_t vcond1 = svreinterpret_u32_x(svld1q_u8((const uint8_t*)(cond1 + i)));\n        svuint32x4_t vcond2 = svreinterpret_u32_x(svld1q_u8((const uint8_t*)(cond2 + i)));\n\n        svbool_t mask1 = svcmpne_z_u32(vcond1, svdup_n_u32(0));\n        svbool_t mask2 = svcmpne_z_u32(vcond2, svdup_n_u32(0));\n\n        svbool_t mask_and = svand(mask1, mask2);\n        svbool_t mask_or = svor(mask1, mask2);\n\n        svfloat32_t vout = svsel(mask_and, va, svsel(mask_or, svsel(mask1, vb, vc), vd));\n\n        svst1q_f32(out + i, vout);\n\n        i += 16;\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i]) {\n            out[i] = B[i];\n        } else if (cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    svbool_t p = svptrue_b32();\n    size_t i = 0;\n    for (; i + 16 <= dim1 * dim2 * dim3; i += 16) {\n        svfloat32_t va = svld1q_f32(A + i);\n        svfloat32_t vb = svld1q_f32(B + i);\n        svfloat32_t vc = svld1q_f32(C + i);\n        svfloat32_t vd = svld1q_f32(D + i);\n        svuint32x4_t vcond1 = svreinterpret_u32_x(svld1q_u8((const uint8_t*)(cond1 + i)));\n        svuint32x4_t vcond2 = svreinterpret_u32_x(svld1q_u8((const uint8_t*)(cond2 + i)));\n\n        svfloat32_t vout = svsel(p, va, vb);\n        vout = svsel(p, vout, vc);\n        vout = svsel(p, vout, vd);\n\n        svst1q_f32(out + i, vout);\n    }\n    for (; i < dim1 * dim2 * dim3; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    svbool_t p = svptrue_b32();\n    size_t n = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i < n) {\n        size_t chunk_size = svget_lanes(p);\n        if (chunk_size > n - i) {\n            chunk_size = n - i;\n        }\n\n        svfloat32_t va = svld1_f32(A + i);\n        svfloat32_t vb = svld1_f32(B + i);\n        svfloat32_t vc = svld1_f32(C + i);\n        svfloat32_t vd = svld1_f32(D + i);\n\n        svuint8x16_t vcond1 = svreinterpret_u8(svld1_u8((const uint8_t*)(cond1 + i)));\n        svuint8x16_t vcond2 = svreinterpret_u8(svld1_u8((const uint8_t*)(cond2 + i)));\n\n        svbool_t vcond1_true = svcmpne_z_u8(vcond1, svdup_u8(0));\n        svbool_t vcond2_true = svcmpne_z_u8(vcond2, svdup_u8(0));\n\n        svbool_t vcond1_and_cond2_true = svand_z(p, vcond1_true, vcond2_true);\n        svbool_t vcond1_true_and_cond2_false = svandnot_z(p, vcond1_true, vcond2_true);\n        svbool_t vcond1_false_and_cond2_true = svandnot_z(p, vcond1_true, vcond2_true);\n        svbool_t vcond1_false_and_cond2_false = svandnot_z(p, vcond1_true, vcond2_true);\n\n        svfloat32_t vout = svsel_z(p, vcond1_and_cond2_true, va, svsel_z(p, vcond1_true_and_cond2_false, vb, svsel_z(p, vcond1_false_and_cond2_true, vc, vd)));\n\n        svst1_f32(out + i, vout);\n\n        i += chunk_size;\n    }\n}\n```"}
{"task_id": "SimdBench_45_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    svbool_t p = svptrue_b32();\n    size_t n = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i < n) {\n        size_t chunk_size = svget_lanes(p);\n        if (chunk_size > n - i) {\n            chunk_size = n - i;\n        }\n\n        svfloat32_t va = svld1_f32(&A[i]);\n        svfloat32_t vb = svld1_f32(&B[i]);\n        svfloat32_t vc = svld1_f32(&C[i]);\n        svfloat32_t vd = svld1_f32(&D[i]);\n\n        svuint8x16_t vcond1 = svreinterpret_u8_x16(svld1_u8((const uint8_t*)&cond1[i]));\n        svuint8x16_t vcond2 = svreinterpret_u8_x16(svld1_u8((const uint8_t*)&cond2[i]));\n\n        svuint8x16_t vcond1_and_vcond2 = svand_u8(vcond1, vcond2);\n        svuint8x16_t vcond1_or_vcond2 = svor_u8(vcond1, vcond2);\n\n        svuint8x16_t vcond1_not_vcond2 = svbic_u8(vcond1_or_vcond2, vcond1_and_vcond2);\n        svuint8x16_t vcond2_not_vcond1 = svbic_u8(vcond1_or_vcond2, vcond1_and_vcond2);\n\n        svfloat32_t vout = svsel_f32(vcond1_and_vcond2, va, svsel_f32(vcond1_not_vcond2, vb, svsel_f32(vcond2_not_vcond1, vc, vd)));\n\n        svst1_f32(&out[i], vout);\n\n        i += chunk_size;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    svuint32_t vA;\n    svuint8_t vOut;\n    svbool_t p;\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += svcntw()) {\n        vA = svld1_u32(A + i);\n        vOut = svclz_u32(vA);\n        vOut = svsub_u8(vOut, svdup_u8(32));\n        svst1_u8(out + i, vOut);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    svuint32_t vA;\n    svbool_t mask;\n    svuint8_t vOut;\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += SV_INT32_ELEMS) {\n        vA = svld1_u32(A + i);\n\n        // Count the number of bits set to 1 in each element\n        vOut = svcntb_u32(vA);\n\n        // Store the result in the output tensor\n        svst1_u8(out + i, vOut);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    svuint32_t vA;\n    svbool_t mask;\n    svuint8_t vOut;\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += svcntw()) {\n        vA = svld1_u32(&A[i]);\n        mask = svcmpgt_z_u32(svpopcntb_m_b32(mask, svclz_u32(vA)), svdup_imm_u32(0));\n        vOut = svreinterpret_u8_u32(svaddlv_u32(svreinterpret_u32_u8(mask)));\n        svst1_u8(&out[i], vOut);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    svuint32_t vA;\n    svuint8_t vOut;\n    svbool_t pg;\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += svcntw()) {\n        vA = svld1_u32(&A[i]);\n        pg = svptrue_b32();\n        vOut = svcntb_z(pg, svclz_u32(vA));\n        svst1_u8(&out[i], vOut);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    svuint32_t vA;\n    svuint8_t vOut;\n    svbool_t pg;\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += svcntw()) {\n        pg = svptrue_b32();\n        vA = svld1_u32(pg, &A[i]);\n        vOut = svclzcntb_z(pg, svreinterpret_u8_u32(vA));\n        svst1_u8(pg, &out[i], vOut);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    svbool_t p = svptrue_b32();\n    size_t vec_size = svcntw();\n    size_t num_vecs = (rows * cols + vec_size - 1) / vec_size;\n\n    for (size_t i = 0; i < num_vecs; ++i) {\n        size_t start_idx = i * vec_size;\n        size_t end_idx = start_idx + vec_size > rows * cols ? rows * cols : start_idx + vec_size;\n        size_t num_elements = end_idx - start_idx;\n\n        svint32x4_t va = svld1_u32(A + start_idx);\n        svuint32x4_t vb = svabs_s32(va);\n\n        svbool_t mask = svcmple_z_s32(svreinterpret_s32_u32(va), svdup_imm_s32(3));\n        svst1_u32(B + start_idx, svsel(mask, vb, va));\n\n        if (num_elements % 4 != 0) {\n            svint32x4_t vlast = svld1_u32(A + end_idx - 4);\n            svuint32x4_t vblast = svabs_s32(vlast);\n            svbool_t last_mask = svcmple_z_s32(svreinterpret_s32_u32(vlast), svdup_imm_s32(3));\n            svst1_u32(B + end_idx - 4, svsel(last_mask, vblast, vlast));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    svbool_t p = svptrue_b32();\n    size_t n = svcntw();\n\n    for (size_t i = 0; i < rows * cols; i += n) {\n        svint32x4_t va = svld1q_s32(A + i);\n        svuint32x4_t vm = svcmple_u32(va, svdup_n_u32(3));\n        svint32x4_t vb = svabsq_s32(va);\n\n        svst1q_s32(B + i, svsel(p, vb, va));\n    }\n}\n```"}
{"task_id": "SimdBench_47_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    svint32_t vzero = svdup_n_s32(0);\n    svbool_t vmask;\n\n    for (size_t i = 0; i < rows * cols; i += SV_INT32_SIZE) {\n        svint32_t va = svld1_u32(A + i);\n\n        vmask = svcmple_z_s32(va, vdup_n_s32(0));\n        vmask = sveor_b_m(svmul_n_s32(vmask, -1), vmask);\n\n        svst1_u32(B + i, svabs_z_s32(vand_b_m(va, vmask)));\n    }\n}\n```"}
{"task_id": "SimdBench_47_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    svbool_t p = svptrue_b32();\n    size_t n = svcntw();\n\n    for (size_t i = 0; i < rows * cols; i += n) {\n        svint32x4_t va = svld1q_s32(A + i);\n        svuint32x4_t vm = svand_u32(svmul_n_u32(svabs_u32(va), 3), svdup_u32(3));\n        svint32x4_t vb = svcmov_s32(p, va, svmul_n_s32(va, -1));\n        svst1q_s32(B + i, vb);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    svbool_t pg = svptrue_b32();\n    size_t vec_size = svcntw();\n\n    for (size_t i = 0; i < rows * cols; i += vec_size) {\n        svint32x4_t va = svld1_u32(&A[i]);\n        svuint32x4_t vb = svabs_s32(va);\n        svint32x4_t vc = svcmple_z_s32(va, svdup_imm_s32(0));\n        svst1_u32(&B[i], svsel_z(pg, vb, va, vc));\n    }\n}\n```"}
{"task_id": "SimdBench_48_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    svuint32_t v_src = svld1_u32(src);\n    svuint32_t v_dst = svdup_u32(0);\n    svbool_t v_mask = svcmpgt_b32(svand_u32(v_src, svdup_u32(0xF)), svdup_u32(0));\n\n    for (size_t i = 0; i < length; i += SV_INT32_SIZE) {\n        v_src = svld1_u32(src + i);\n        v_mask = svcmpgt_b32(svand_u32(v_src, svdup_u32(0xF)), svdup_u32(0));\n        v_dst = svsel_b32(v_mask, svrorv_u32(v_src, svdup_u32(rotate_bits)), svnot_u32(v_src));\n        svst1_u32(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    svuint32_t v_src = svld1_u32(src);\n    svuint32_t v_dst = svdup_u32(0);\n    svbool_t v_mask = svcmpgt_b32(svand_u32(v_src, svdup_u32(0xF)), svdup_u32(0));\n\n    for (size_t i = 0; i < length; i += SV_INT32_SIZE) {\n        v_src = svld1_u32(src + i);\n        v_mask = svcmpgt_b32(svand_u32(v_src, svdup_u32(0xF)), svdup_u32(0));\n        v_dst = svsel_b32(v_mask, svrorv_u32(v_src, svdup_u32(rotate_bits)), svnot_u32(v_src));\n        svst1_u32(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    svuint32_t v_src = svld1_u32(src);\n    svuint32_t v_dst = svdup_u32(0);\n    svbool_t v_mask = sveor_z_u32(svptrue_b32(), svand_z_u32(svptrue_b32(), svlsl_n_u32(v_src, 28), svdup_u32(0xF)));\n\n    v_dst = svsel_z_u32(v_mask, svror_n_u32(v_src, rotate_bits), svnot_u32(v_src));\n\n    svst1_u32(dst, v_dst);\n\n    for (size_t i = 16; i < length; i += 16) {\n        v_src = svld1_u32(&src[i]);\n        v_dst = svdup_u32(0);\n        v_mask = sveor_z_u32(svptrue_b32(), svand_z_u32(svptrue_b32(), svlsl_n_u32(v_src, 28), svdup_u32(0xF)));\n\n        v_dst = svsel_z_u32(v_mask, svror_n_u32(v_src, rotate_bits), svnot_u32(v_src));\n\n        svst1_u32(&dst[i], v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    svuint32_t v_src = svld1_u32(src);\n    svuint32_t v_dst = svdup_u32(0);\n    svbool_t v_mask = svcmple_u32(v_src, svdup_u32(0xF));\n\n    for (size_t i = 0; i < length; i += SV_INT32_SIZE) {\n        v_src = svld1_u32(src + i);\n        v_mask = svcmple_u32(v_src, svdup_u32(0xF));\n        v_dst = svsel_b_u32(v_mask, svrorv_u32(v_src, svdup_u32(rotate_bits)), svnot_u32(v_src));\n        svst1_u32(dst + i, v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    svuint32_t v_src = svld1_u32(src);\n    svuint32_t v_dst = svdup_u32(0);\n    svbool_t v_mask = svcmple_u32(v_src, svdup_u32(0xF));\n\n    for (size_t i = 0; i < length; i += SV_INT32_SIZE) {\n        v_dst = svifelse(v_mask, svrorv_u32(v_src, svdup_u32(rotate_bits)), svnot_u32(v_src));\n        svst1_u32(dst + i, v_dst);\n\n        if (i + SV_INT32_SIZE >= length) {\n            break;\n        }\n\n        v_src = svld1_u32(src + i + SV_INT32_SIZE);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    svfloat32_t v_epsilon = svdup_n_f32(epsilon);\n    svint8_t v_zero = svdup_n_s8(0);\n    svint8_t v_one = svdup_n_s8(1);\n    svint8_t v_neg_one = svdup_n_s8(-1);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        svfloat32_t va = svld1q_f32(&A[i]);\n        svfloat32_t vb = svld1q_f32(&B[i]);\n\n        svfloat32_t diff = svsubq_f32(va, vb);\n        svfloat32_t abs_diff = svabsq_f32(diff);\n\n        svbool_t mask_greater = svcmpgtq_f32(abs_diff, v_epsilon);\n        svbool_t mask_less = svcmpltq_f32(abs_diff, v_epsilon);\n\n        svint8_t result = svsel(mask_greater, v_one, svsel(mask_less, v_neg_one, v_zero));\n        svst1q_s8(&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    svfloat32_t v_epsilon = svdup_n_f32(epsilon);\n    svint8_t v_zero = svdup_n_s8(0);\n    svint8_t v_one = svdup_n_s8(1);\n    svint8_t v_neg_one = svdup_n_s8(-1);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        svfloat32_t va = svld1q_f32(&A[i]);\n        svfloat32_t vb = svld1q_f32(&B[i]);\n\n        svfloat32_t diff = svsub_f32(va, vb);\n        svfloat32_t abs_diff = svabs_f32(diff);\n\n        svbool_t mask_greater = svcmpgt_f32(abs_diff, v_epsilon);\n        svbool_t mask_less = svcmplt_f32(abs_diff, v_epsilon);\n\n        svint8_t result = svsel(mask_greater, v_one, svsel(mask_less, v_neg_one, v_zero));\n        svst1q_s8(&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    svfloat32_t v_epsilon = svdup_n_f32(epsilon);\n    svbool_t mask;\n\n    for (size_t i = 0; i < length; i += SV_INT32_ELEMS) {\n        svfloat32_t va = svld1_u32((uint32_t*)&A[i]);\n        svfloat32_t vb = svld1_u32((uint32_t*)&B[i]);\n\n        svfloat32_t diff = svsub_f32(va, vb);\n        svfloat32_t abs_diff = svabs_f32(diff);\n\n        mask = svcmpgt_f32(abs_diff, v_epsilon);\n        svst1_u8((uint8_t*)&out[i], svreinterpret_u8(mask));\n\n        mask = svcmplt_f32(abs_diff, v_epsilon);\n        svst1_u8((uint8_t*)&out[i], svreinterpret_u8(svand_b(mask, svnot_b(svreinterpret_b(mask)))));\n\n        mask = svcmpeq_f32(abs_diff, v_epsilon);\n        svst1_u8((uint8_t*)&out[i], svreinterpret_u8(svand_b(mask, svnot_b(svreinterpret_b(mask)))));\n    }\n}\n```"}
{"task_id": "SimdBench_49_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    svfloat32_t v_epsilon = svdup_n_f32(epsilon);\n    svint8_t v_zero = svdup_n_s8(0);\n    svint8_t v_one = svdup_n_s8(1);\n    svint8_t v_neg_one = svdup_n_s8(-1);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        svfloat32_t va = svld1q_f32(&A[i]);\n        svfloat32_t vb = svld1q_f32(&B[i]);\n\n        svfloat32_t diff = svsubq_f32(va, vb);\n        svfloat32_t abs_diff = svabsq_f32(diff);\n\n        svbool_t mask_gt = svcmpgtq_f32(abs_diff, v_epsilon);\n        svbool_t mask_lt = svcmpltq_f32(abs_diff, v_epsilon);\n\n        svint8_t result = svsel(mask_gt, v_one, svsel(mask_lt, v_neg_one, v_zero));\n        svst1q_s8(&out[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (fabsf(A[i] - B[i]) > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (fabsf(A[i] - B[i]) < fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    svfloat32_t v_epsilon = svdup_n_f32(epsilon);\n    svint8_t v_zero = svdup_n_s8(0);\n    svint8_t v_one = svdup_n_s8(1);\n    svint8_t v_neg_one = svdup_n_s8(-1);\n\n    for (size_t i = 0; i < length; i += SV_INT32_ELEMS) {\n        svfloat32_t va = svld1_f32(&A[i]);\n        svfloat32_t vb = svld1_f32(&B[i]);\n\n        svfloat32_t diff = svsub_f32(va, vb);\n        svfloat32_t abs_diff = svabs_f32(diff);\n\n        svbool_t gt_mask = svcmpgt_f32(abs_diff, v_epsilon);\n        svbool_t lt_mask = svcmplt_f32(abs_diff, v_epsilon);\n\n        svint8_t result = svsel(gt_mask, v_one, svsel(lt_mask, v_neg_one, v_zero));\n        svst1_s8(&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_50_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    size_t i = 0;\n\n    while (i + 16 <= size) {\n        svfloat32_t va = svld1(pg, A + i);\n        svint32_t vb = svclamps(pg, svreinterpret_f32_u32(va), INT32_MIN, INT32_MAX);\n        svst1(pg, B + i, vb);\n        i += 16;\n    }\n\n    for (; i < size; ++i) {\n        B[i] = A[i] > INT32_MAX ? INT32_MAX : (A[i] < INT32_MIN ? INT32_MIN : static_cast<int32_t>(A[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_50_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    svuint32_t max_val = svdup_u32(INT32_MAX);\n    svint32_t min_val = svdup_s32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += SV_INT32_SIZE) {\n        svfloat32_t vec_A = svld1_f32(&A[i]);\n        svint32_t vec_B = svclamps_f32_z(vec_A, min_val, max_val);\n        svst1_s32(&B[i], vec_B);\n    }\n}\n```"}
{"task_id": "SimdBench_50_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    svfloat32_t va;\n    svint32_t vb;\n    svbool_t pg;\n\n    for (size_t i = 0; i < size; i += SV_INT32_SIZE) {\n        va = svld1_f32(A + i);\n        pg = svcgt_f32(va, svdup_n_f32(INT32_MAX));\n        va = svsel(pg, svdup_n_f32(INT32_MAX), va);\n        pg = svclt_f32(va, svdup_n_f32(INT32_MIN));\n        va = svsel(pg, svdup_n_f32(INT32_MIN), va);\n        vb = svcvtfz_s32_f32(va);\n        svst1_s32(B + i, vb);\n    }\n}\n```"}
{"task_id": "SimdBench_50_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    svuint32_t max_val = svdup_u32(INT32_MAX);\n    svint32_t min_val = svdup_s32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += SV_INT32_SIZE) {\n        svfloat32_t vec_A = svld1_f32(&A[i]);\n        svint32_t vec_B = svclamps_f32(vec_A, min_val, max_val);\n        svst1_s32(&B[i], vec_B);\n    }\n}\n```"}
{"task_id": "SimdBench_50_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    svfloat32_t va;\n    svint32_t vb;\n    svbool_t pg;\n\n    for (size_t i = 0; i < size; i += SV_INT32_SIZE) {\n        va = svld1_f32(A + i);\n        pg = svlt_f32(va, svdup_n_f32(INT32_MIN));\n        va = svmaxnm_f32(va, svdup_n_f32(INT32_MIN), pg);\n        pg = svgt_f32(va, svdup_n_f32(INT32_MAX));\n        va = svminnm_f32(va, svdup_n_f32(INT32_MAX), pg);\n        vb = svcvtfz_s32_f32(va);\n        svst1_s32(B + i, vb);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    svuint32_t va;\n    svfloat32_t vb, vc;\n    size_t i = 0;\n\n    while (i + 4 <= size) {\n        va = svld1_u32(&A[i]);\n        vb = svreinterpret_f32_u32(va);\n        vc = svmul_f32(vb, svdup_n_f32(scale));\n        svst1_f32(&B[i], vc);\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    svuint32_t vec_A;\n    svfloat32_t vec_B, vec_scale;\n    size_t i = 0;\n\n    while (i + 16 <= size) {\n        vec_A = svld1_u32(&A[i]);\n        vec_scale = svdup_n_f32(scale / 255.0f);\n        vec_B = svreinterpret_f32_u32(vec_A);\n        vec_B = svmul_f32(vec_B, vec_scale);\n        svst1_f32(&B[i], vec_B);\n        i += 16;\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * (scale / 255.0f);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    svuint32_t va;\n    svfloat32_t vb, vc;\n    size_t i = 0;\n\n    while (i + 4 <= size) {\n        va = svld1_u32(&A[i]);\n        vb = svreinterpret_f32_u32(va);\n        vc = svmul_f32(vb, svdup_n_f32(scale));\n        svst1_f32(&B[i], vc);\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    svuint32_t vec_A;\n    svfloat32_t vec_B;\n    svbool_t mask;\n\n    for (size_t i = 0; i < size; i += SV_INT32_SIZE) {\n        vec_A = svld1_u32(A + i);\n        vec_B = svreinterpret_f32_u32(vec_A);\n        vec_B = svmul_f32(vec_B, svdup_n_f32(scale));\n        svst1_f32(B + i, vec_B);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    svuint32_t vec_A;\n    svfloat32_t vec_B, vec_scale;\n    size_t i = 0;\n\n    while (i + 16 <= size) {\n        vec_A = svld1_u32(A + i);\n        vec_scale = svdup_n_f32(scale / 255.0f);\n        vec_B = svreinterpret_f32_u32(svshll_n_u32(vec_A, 8));\n        vec_B = svmul_f32(vec_B, vec_scale);\n        svst1_f32(B + i, vec_B);\n        i += 16;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * (scale / 255.0f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    svfloat32_t va, vb;\n    svint32_t vc;\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        va = svld1_f32(A + i);\n        vb = svld1_f32(B + i);\n\n        // Perform element-wise division and round to nearest integer\n        vc = svrndra_f32(va, vb);\n\n        // Store the result\n        svst1_s32(C + i, vc);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    svfloat32_t va, vb;\n    svint32_t vc;\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        va = svld1_f32(A + i);\n        vb = svld1_f32(B + i);\n\n        // Perform element-wise division and round to nearest integer\n        vc = svrndra_f32(va, vb);\n\n        // Store the results\n        svst1_s32(C + i, vc);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    svfloat32_t va, vb;\n    svint32_t vc;\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        va = svld1_f32(&A[i]);\n        vb = svld1_f32(&B[i]);\n\n        // Perform division and round to nearest integer\n        vc = svrndi_f32(va / vb);\n\n        svst1_s32(&C[i], vc);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)round(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    svfloat32_t va, vb;\n    svint32_t vc;\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        va = svld1_f32(A + i);\n        vb = svld1_f32(B + i);\n\n        // Perform element-wise division and round to nearest integer\n        vc = svrndra_f32(va, vb);\n\n        // Store the results\n        svst1_s32(C + i, vc);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    svfloat32_t va, vb;\n    svint32_t vc;\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        va = svld1_f32(A + i);\n        vb = svld1_f32(B + i);\n\n        // Perform element-wise division and round to nearest integer\n        vc = svrndra_f32(va, vb);\n\n        // Store the results\n        svst1_s32(C + i, vc);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    svfloat64_t va;\n    svfloat32_t vb;\n    svbool_t mask;\n\n    for (size_t i = 0; i < size; i += SV_INT64_ELEMS) {\n        va = svld1_f64(A + i);\n\n        // Check for overflow/underflow\n        mask = svcgt_f64(va, svdup_n_f64(FLT_MAX));\n        va = svsel(mask, svdup_n_f64(FLT_MAX), va);\n        mask = svclt_f64(va, svdup_n_f64(-FLT_MAX));\n        va = svsel(mask, svdup_n_f64(-FLT_MAX), va);\n\n        vb = svfcvtzs_f32_f64(va);\n        svst1_f32(B + i, vb);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    svfloat32_t v_max = svcvt_f32_f64(svrnd_nz_f64(svset1_f64(FLT_MAX)));\n    svfloat32_t v_min = svcvt_f32_f64(svrnd_nz_f64(svset1_f64(-FLT_MAX)));\n\n    for (size_t i = 0; i < size; i += SVVectorSize / sizeof(double)) {\n        svfloat64_t va = svld1_f64(A + i);\n        svbool_t mask_gt = svcmpgt_f64(va, svset1_f64(FLT_MAX));\n        svbool_t mask_lt = svcmplt_f64(va, svset1_f64(-FLT_MAX));\n\n        svfloat32_t vb = svcvt_f32_f64(svsel(mask_gt | mask_lt, svset1_f64(FLT_MAX), svsel(mask_lt, svset1_f64(-FLT_MAX), va)));\n        svst1_f32(B + i * 2, vb);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    svlabel_t label = svptrue_b32();\n    size_t i = 0;\n\n    while (i + 16 <= size) {\n        svfloat64_t va = svld1_f64_z(label, A + i);\n        svfloat32_t vb = svdup_n_f32(FLT_MAX);\n        svfloat32_t vc = svdup_n_f32(-FLT_MAX);\n\n        svbool_t mask_gt = svgt_f64(va, vb);\n        svbool_t mask_lt = svlt_f64(va, vc);\n\n        vb = svsel(mask_gt, vb, svreinterpret_f32(va));\n        vc = svsel(mask_lt, vc, svreinterpret_f32(va));\n\n        svst1_f32(B + i, vb);\n        svst1_f32(B + i + 8, vc);\n\n        i += 16;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    svlabel_t label;\n    svuint32_t mask;\n    svfloat32_t max_val = svdup_n_f32(FLT_MAX);\n    svfloat32_t min_val = svdup_n_f32(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += SV_INT32_ELEMS) {\n        svfloat64_t vec_A = svld1_f64(A + i);\n        svfloat32_t vec_B = svreinterpret_f32_f64(vec_A);\n\n        mask = svcmpgt_f32(vec_B, max_val);\n        vec_B = svselect(mask, max_val, vec_B);\n\n        mask = svcmplt_f32(vec_B, min_val);\n        vec_B = svselect(mask, min_val, vec_B);\n\n        svst1_f32(B + i, vec_B);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    size_t i = 0;\n\n    while (i + 16 <= size) {\n        svfloat64_t va = svld1_f64(pg, A + i);\n        svfloat32_t vb = svdup_n_f32(FLT_MAX);\n        svfloat32_t vn = svdup_n_f32(-FLT_MAX);\n\n        svfloat32_t vc = svreinterpret_f32_f64(va);\n        vc = svmaxnmvq_f32(vb, vc);\n        vc = svminnmvq_f32(vn, vc);\n\n        svst1_f32(pg, B + i, vc);\n        i += 16;\n    }\n\n    for (; i < size; ++i) {\n        double a = A[i];\n        float b = (a > FLT_MAX) ? FLT_MAX : ((a < -FLT_MAX) ? -FLT_MAX : (float)a);\n        B[i] = b;\n    }\n}\n```"}
{"task_id": "SimdBench_54_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    svbool_t pg = svptrue_b16();\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        svint16x8_t va = svld1_x8(pg, A + i);\n        svint32x4_t vb = svreinterpret_u32_u16(va);\n        svst1_x4(pg, B + i, vb);\n        i += 8;\n    }\n\n    if (i < size) {\n        svint16x8_t va = svld1_x8(svptrue_e16(size - i), A + i);\n        svint32x4_t vb = svreinterpret_u32_u16(va);\n        svst1_x4(svptrue_e32(size - i), B + i, vb);\n    }\n}\n```"}
{"task_id": "SimdBench_54_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    svbool_t pg = svptrue_b16();\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        svint16x8_t va = svld1_x8(pg, A + i);\n        svint32x4_t vb = svreinterpret_u32_u16(svextw_z_h(pg, va));\n        svst1_x4(pg, B + i, vb);\n        i += 8;\n    }\n\n    if (i < size) {\n        svint16x8_t va = svld1_x8(svptrue_e16(size - i), A + i);\n        svint32x4_t vb = svreinterpret_u32_u16(svextw_z_h(svptrue_e16(size - i), va));\n        svst1_x4(svptrue_e16(size - i), B + i, vb);\n    }\n}\n```"}
{"task_id": "SimdBench_54_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    svuint16_t vec_A;\n    svint32_t vec_B;\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        vec_A = svld1_u16(&A[i]);\n        vec_B = svreinterpret_u32_u16(vec_A);\n        svst1_s32(&B[i], vec_B);\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    svuint16_t va;\n    svint32_t vb;\n    size_t i = 0;\n\n    while (i + 8 <= size) {\n        va = svld1_u16(&A[i]);\n        vb = svreinterpret_u32_u16(va);\n        svst1_s32(&B[i], vb);\n        i += 8;\n    }\n\n    if (i < size) {\n        va = svld1_u16(&A[i]);\n        vb = svreinterpret_u32_u16(va);\n        svst1_s32(&B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_54_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    svuint16_t vec_A;\n    svint32_t vec_B;\n    size_t i = 0;\n\n    while (i + SV_INT16_SIZE <= size) {\n        vec_A = svld1_u16(A + i);\n        vec_B = svreinterpret_s32_u32(svuzp1_u16(vec_A, vec_A));\n        svst1_s32(B + i, vec_B);\n        i += SV_INT16_SIZE;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        B[i] = A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    svfloat32_t v_min_val = svdup_n_f32(min_val);\n    svfloat32_t v_max_val = svdup_n_f32(max_val);\n    svfloat32_t v_one = svdup_n_f32(1.0f);\n    svfloat32_t v_zero = svdup_n_f32(0.0f);\n\n    size_t i = 0;\n    for (; i <= size - 16; i += 16) {\n        svfloat32_t va = svld1q_f32(&A[i]);\n        svint32x4_t vcontrol = svld1q_s32(&control[i]);\n\n        svbool_t mask = sveor_z_f32(svptrue_b32(), svreinterpret_u32_s32(vcontrol), svreinterpret_u32_s32(svones_s32()));\n\n        svfloat32_t vnormalized = svdiv_f32(va, svsub_f32(v_max_val, v_min_val));\n        vnormalized = svminnmv_f32(vnormalized, vone);\n        vnormalized = svmaxnmv_f32(vnormalized, vzero);\n\n        svst1q_f32(&B[i], svsel(mask, vnormalized, va));\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    svfloat32_t v_min_val = svdup_n_f32(min_val);\n    svfloat32_t v_max_val = svdup_n_f32(max_val);\n    svfloat32_t v_zero = svdup_n_f32(0.0f);\n    svfloat32_t v_one = svdup_n_f32(1.0f);\n\n    size_t i = 0;\n    for (; i <= size - 16; i += 16) {\n        svfloat32_t va = svld1q_f32(&A[i]);\n        svint32x4_t vcontrol = svld1q_s32(&control[i]);\n\n        svbool_t mask = svcmpgt_z_f32(vcontrol, svdup_n_f32(0.0f));\n\n        svfloat32_t vdiff = svsub_f32(va, v_min_val);\n        svfloat32_t vnorm = svdiv_f32(vdiff, svsub_f32(v_max_val, v_min_val));\n        vnorm = svmax_f32(vnorm, v_zero);\n        vnorm = svmin_f32(vnorm, v_one);\n\n        svst1q_f32(&B[i], svsel(mask, vnorm, va));\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = A[i];\n            val = (val - min_val) / (max_val - min_val);\n            val = fmaxf(val, 0.0f);\n            val = fminf(val, 1.0f);\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    svfloat32_t v_min_val = svdup_n_f32(min_val);\n    svfloat32_t v_max_val = svdup_n_f32(max_val);\n    svfloat32_t v_zero = svdup_n_f32(0.0f);\n    svfloat32_t v_one = svdup_n_f32(1.0f);\n\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        svfloat32_t va = svld1_u32((uint32_t*)&A[i]);\n        svint32x4_t vcontrol = svld1_u32((uint32_t*)&control[i]);\n\n        svbool_t mask = sveor_z_f32(svptrue_b32(), svle_z_f32(vcontrol, svdup_n_f32(0.0f)));\n\n        svfloat32_t vnormalized = svdiv_f32(svsub_f32(va, v_min_val), svsub_f32(v_max_val, v_min_val));\n        vnormalized = svminnmv_f32(vnormalized, v_one);\n        vnormalized = svmaxnmv_f32(vnormalized, v_zero);\n\n        svst1_u32((uint32_t*)&B[i], svsel(mask, vnormalized, va));\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = A[i];\n            val = (val - min_val) / (max_val - min_val);\n            val = val < 0.0f ? 0.0f : val;\n            val = val > 1.0f ? 1.0f : val;\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    svfloat32_t v_min_val = svdup_n_f32(min_val);\n    svfloat32_t v_max_val = svdup_n_f32(max_val);\n    svfloat32_t v_zero = svdup_n_f32(0.0f);\n    svfloat32_t v_one = svdup_n_f32(1.0f);\n\n    for (size_t i = 0; i < size; i += SV_INT32_ELEMENTS) {\n        svint32x4_t v_control = svld1_u32(control + i);\n        svfloat32_t v_A = svld1_f32(A + i);\n\n        svbool_t mask = sveor_z_f32(svptrue_b32(), svle_z_f32(v_control, svdup_n_f32(0.0f)));\n\n        svfloat32_t v_normalized = svdiv_f32(svsub_f32(v_A, v_min_val), svsub_f32(v_max_val, v_min_val));\n        v_normalized = svmin_f32(v_normalized, v_one);\n        v_normalized = svmax_f32(v_normalized, v_zero);\n\n        svst1_f32(B + i, svsel(mask, v_normalized, v_A));\n    }\n}\n```"}
{"task_id": "SimdBench_55_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    svfloat32_t v_min_val = svdup_n_f32(min_val);\n    svfloat32_t v_max_val = svdup_n_f32(max_val);\n    svfloat32_t v_one = svdup_n_f32(1.0f);\n    svfloat32_t v_zero = svdup_n_f32(0.0f);\n\n    size_t i = 0;\n    for (; i <= size - 16; i += 16) {\n        svfloat32_t va = svld1q_f32(&A[i]);\n        svint32x4_t vcontrol = svld1q_s32(&control[i]);\n\n        svbool_t mask = svcmpgt_z_s32(vcontrol, svdup_n_s32(0));\n\n        svfloat32_t vdiff = svsub_f32(va, v_min_val);\n        svfloat32_t vnorm = svdiv_f32(vdiff, svsub_f32(v_max_val, v_min_val));\n        vnorm = svmin_f32(vnorm, v_one);\n        vnorm = svmax_f32(vnorm, v_zero);\n\n        svst1q_f32(&B[i], svsel(mask, vnorm, va));\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = A[i];\n            val = (val - min_val) / (max_val - min_val);\n            val = fmaxf(fminf(val, 1.0f), 0.0f);\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < size; i += svcntw()) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vf = svreinterpret_f32_u32(va);\n        svfloat32_t vsqrtf = svsqrt_f32(vf);\n        svst1_f32(pg, &B[i], vsqrtf);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < size; i += svcntw()) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vb = svreinterpret_f32_u32(va);\n        svfloat32_t vc = svsqrt_f32(vb);\n        svst1_f32(pg, &B[i], vc);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    size_t vl = svcntp_b32(pg);\n\n    for (size_t i = 0; i < size; i += vl) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vf = svreinterpret_f32_u32(va);\n        svfloat32_t vsqrt = svsqrt_f32(vf);\n        svst1_f32(pg, &B[i], vsqrt);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    size_t vl = svcntp_b32(pg);\n\n    for (size_t i = 0; i < size; i += vl) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vf = svreinterpret_f32_u32(va);\n        svfloat32_t vsqrtf = svsqrt_f32(vf);\n        svst1_f32(pg, &B[i], vsqrtf);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    size_t vl = svcntp_b32(pg);\n\n    for (size_t i = 0; i < size; i += vl) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vf = svreinterpret_f32_u32(va);\n        svfloat32_t vsqrtf = svsqrt_f32(vf);\n        svst1_f32(pg, &B[i], vsqrtf);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < size; i += svcntw()) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vb = svreinterpret_f32_u32(va);\n        svst1_f32(pg, &B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svwhilelt_b32(size);\n    svuint32_t va = svld1_u32(pg, A);\n    svfloat32_t vb = svreinterpret_f32_u32(va);\n    svst1_f32(pg, B, vb);\n}\n```"}
{"task_id": "SimdBench_57_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < size; i += svcntw()) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vb = svreinterpret_f32_u32(va);\n        svst1_f32(pg, &B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < size; i += svcntw()) {\n        svuint32_t va = svld1_u32(pg, &A[i]);\n        svfloat32_t vb = svreinterpret_f32_u32(va);\n        svst1_f32(pg, &B[i], vb);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    svbool_t pg = svptrue_b32();\n    size_t vl = svcntp_b32(pg);\n\n    while (size >= vl) {\n        svuint32_t va = svld1_u32(pg, A);\n        svfloat32_t vb = svreinterpret_f32_u32(va);\n        svst1_f32(pg, B, vb);\n\n        A += vl;\n        B += vl;\n        size -= vl;\n    }\n\n    if (size > 0) {\n        svuint32_t va = svld1_u32_z(pg, A, svdup_imm_b32(size));\n        svfloat32_t vb = svreinterpret_f32_u32(va);\n        svst1_f32_z(pg, B, vb, svdup_imm_b32(size));\n    }\n}\n```"}
{"task_id": "SimdBench_58_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    svbool_t p = svptrue_b32();\n    svuint32_t sv_A = svld1_u32(A);\n    svuint32_t sv_A_prev = svdup_u32(0);\n    svfloat32_t sv_diff = svdup_f32(0.0f);\n\n    for (size_t i = 1; i < size; ++i) {\n        svuint32_t sv_cond = svld1_u32(cond + i);\n        svuint32_t sv_A_curr = svld1_u32(A + i);\n        svfloat32_t sv_diff_temp = svsub_f32(sv_reinterpret_f32_u32(sv_A_curr), sv_reinterpret_f32_u32(sv_A_prev));\n\n        sv_diff = svsel(p & svreinterpret_b32_u32(sv_cond), sv_diff_temp, sv_diff);\n        svst1_f32(diff + i, sv_diff);\n\n        sv_A_prev = sv_A_curr;\n    }\n\n    diff[0] = 0.0f;\n}\n```"}
{"task_id": "SimdBench_58_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    svbool_t p = svptrue_b32();\n    svuint32_t va = svld1_u32(A);\n    svuint32_t vb = svld1_u32(A + 1);\n    svfloat32_t vdiff = svdup_f32(0.0f);\n\n    for (size_t i = 1; i < size; ++i) {\n        svuint32_t vc = svld1_u32(&cond[i]);\n        svuint32_t vd = svsub_u32(vb, va);\n        svfloat32_t vf = svreinterpret_f32_u32(vd);\n        vdiff = svsel(p, vf, vdiff);\n        va = vb;\n        vb = svld1_u32(A + i + 1);\n    }\n\n    svst1_f32(diff, vdiff);\n}\n```"}
{"task_id": "SimdBench_58_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    svbool_t p = svptrue_b32();\n    svuint32_t svA = svld1_u32(A);\n    svuint32_t svA_prev = svdup_u32(0);\n    svfloat32_t svdiff = svdup_f32(0.0f);\n\n    for (size_t i = 1; i < size; ++i) {\n        svuint32_t svA_next = svld1_lane_u32(&A[i], svA, 0);\n        svdiff = svsub_f32(svdiff, svreinterpret_f32(svsub_u32(svA_next, svA_prev)));\n        svA_prev = svA_next;\n        svdiff = svsel(p, svdiff, svdup_f32(0.0f), !svld1_lane_b32(&cond[i], svptrue_b32(), 0));\n        svst1_f32(&diff[i-1], svdiff);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    svbool_t p = svptrue_b32();\n    svuint32_t svA = svld1_u32(A);\n    svuint32_t svA_prev = svdup_u32(0);\n    svfloat32_t svdiff = svdup_f32(0.0f);\n\n    for (size_t i = 1; i < size; ++i) {\n        svuint32_t svA_next = svld1_lane_u32(&A[i], svA, 0);\n        svdiff = svsub_f32(svdiff, svreinterpret_f32(svsub_u32(svA_next, svA_prev)));\n        svA_prev = svA_next;\n        svdiff = svsel(p, svdiff, svdup_f32(0.0f), !cond[i]);\n        svst1_f32(&diff[i-1], svdiff);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    svbool_t p = svptrue_b32();\n    svuint32_t va = svld1_u32(A);\n    svuint32_t vb = svld1_u32(A + 1);\n    svuint32_t vdiff = svsub_u32(vb, va);\n    svfloat32_t vdiff_f = svreinterpret_f32_u32(vdiff);\n    svbool_t vcond = svld1_u8((const uint8_t*)cond);\n    svst1_f32(diff + 1, svsel_z(p, vcond, vdiff_f, svdup_n_f32(0.0f)));\n    diff[0] = 0.0f;\n}\n```"}
{"task_id": "SimdBench_59_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svuint32_t vsrc = svld1_u32(pg, &src[i]);\n        svuint64_t vdst = svreinterpret_u64_u32(svreinterpret_u32_u8(svdup_n_u8(0xFF)));\n        svst1_u64(pg, &dst[i], vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svuint32_t vsrc = svld1_u32(pg, &src[i]);\n        svuint64_t vdst = svreinterpret_u64_u32(svorr_x32(vsrc, svdup_n_u32(0xFFFFFFFF)));\n        svst1_u64(pg, &dst[i], vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svuint32_t vsrc = svld1_u32(pg, &src[i]);\n        svuint64_t vdst = svreinterpret_u64_u32(svreinterpret_u32_u8(svdup_n_u8(0xFF)));\n        svst1_u64(pg, &dst[i], vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svuint32_t vsrc = svld1_u32(pg, &src[i]);\n        svuint64_t vdst = svreinterpret_u64_u32(svreinterpret_u32_u8(svdup_n_u8(0xFF)));\n        svst1_u64(pg, &dst[i], vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    svbool_t pg = svptrue_b32();\n    for (size_t i = 0; i < length; i += svcntw()) {\n        svuint32_t vsrc = svld1_u32(pg, &src[i]);\n        svuint64_t vdst = svreinterpret_u64_u32(svorr_u32(vsrc, svdup_u32(0xFFFFFFFF)));\n        svst1_u64(pg, &dst[i], vdst);\n    }\n}\n```"}
{"task_id": "SimdBench_60_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    svfloat64_t sum = svdup_f64(0.0);\n    svuint32_t idx = svdup_u32(0);\n\n    for (size_t i = 0; i < length; i += SV_INT64_ELEMS) {\n        svuint32_t mask = svle_u32(idx, svld1_u32(index + i));\n        svfloat64_t selected_values = svld1_gather_f64(mask, vec, idx);\n        sum = svadd_f64(sum, selected_values);\n        idx = svadd_u32(idx, svdup_u32(SV_INT64_ELEMS));\n    }\n\n    return svget_f64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_60_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    svfloat64_t sum = svdup_f64(0.0);\n    for (size_t i = 0; i < length; i += SV_INT64_ELEMS) {\n        svuint32_t idx_vec = svreinterpret_u32(svld1_u16(index + i));\n        svbool_t mask = svle_u32(idx_vec, svdup_u32(length - 1));\n        svfloat64_t selected_values = svld1_f64(vec + svuzp1_u32(idx_vec, svuzp2_u32(idx_vec))[0]);\n        sum = svadd_f64(sum, svsel(mask, selected_values, svdup_f64(0.0)));\n    }\n    return svget_f64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_60_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    svfloat64_t sum = svdup_f64(0.0);\n    for (size_t i = 0; i < length; i += SV_INT64_ELEMS) {\n        svuint32_t idx_vec = svreinterpret_u32(svld1_u16(index + i));\n        svbool_t mask = svle_u32(idx_vec, svdup_u32(length - 1));\n        svfloat64_t selected_vals = svld1_gather_offset_f64(vec, idx_vec, mask);\n        sum = svadd_f64(sum, selected_vals);\n    }\n    return svget_f64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_60_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    svfloat64_t sum = svdup_f64(0.0);\n    svuint32_t idx_mask = svdup_u32(length - 1);\n\n    for (size_t i = 0; i < length; i += SV_INT64_ELEMS) {\n        svuint32_t idx_vec = svld1_u32(&index[i]);\n        svuint32_t valid_idx = svclz_u32(idx_vec) & idx_mask;\n        svuint32_t valid_idx_shr = svshl_n_u32(valid_idx, 3); // Shift right by 3 to get byte offset\n\n        svfloat64_t vec_vec = svld1_f64(&vec[valid_idx_shr >> 3]);\n        sum = svadd_f64(sum, vec_vec);\n    }\n\n    return svget_f64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_60_SVE", "completion": "```c\n#include <arm_sve.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    svfloat64_t sum = svdup_f64(0.0);\n    for (size_t i = 0; i < length; i += SV_INT64_ELEMS) {\n        svuint32_t idx_vec = svreinterpret_u32(svld1_u16(index + i));\n        svbool_t mask = svle_u32(idx_vec, svdup_u32(length - 1));\n        svfloat64_t selected_values = svld1_gather_offset_f64(vec, idx_vec, mask);\n        sum = svadd_f64(sum, selected_values);\n    }\n    return svget_f64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_61_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svbool_t ptrue = svptrue_b64();\n\n    size_t output_size = input_size - kernel_size + 1;\n    for (size_t i = 0; i < output_size; ++i) {\n        svfloat64_t vsum = vzero;\n        for (size_t j = 0; j < kernel_size; ++j) {\n            svfloat64_t vin = svld1_f64(input + i + j * input_size);\n            svfloat64_t vker = svld1_f64(kernel + j * kernel_size);\n            vsum = svadd_f64(vsum, svmul_f64(vin, vker));\n        }\n        svfloat64_t vrelu = svmax_f64(vsum, vzero);\n        svst1_f64(output + i, vrelu);\n    }\n}\n```"}
{"task_id": "SimdBench_61_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    svfloat64_t vzero = svdup_f64(0.0);\n    svbool_t ptrue = svptrue_b64();\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            svfloat64_t sum = vzero;\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    svfloat64_t vinput = svld1_f64(input + (i + ki) * input_size + j + kj);\n                    svfloat64_t vkernel = svld1_f64(kernel + ki * kernel_size + kj);\n                    sum = svadd_f64(sum, svmul_f64(vinput, vkernel));\n                }\n            }\n            svfloat64_t vrelu = svmax_f64(sum, vzero);\n            svst1_f64(output + i * (input_size - kernel_size + 1) + j, vrelu);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    svfloat64_t v_input, v_kernel, v_output;\n    svuint32_t v_idx;\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            v_output = svdup_f64(0.0);\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    v_idx = svdup_u32(ki * kernel_size + kj);\n                    v_input = svld1_f64(input + i + ki * input_size + j + kj);\n                    v_kernel = svld1_f64(kernel + ki * kernel_size + kj);\n                    v_output = svmla_f64(v_output, v_input, v_kernel);\n                }\n            }\n\n            v_output = svmax_f64(v_output, svdup_f64(0.0));\n            svst1_f64(output + i * (input_size - kernel_size + 1) + j, v_output);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    svfloat64_t v_input, v_kernel, v_output;\n    svbool_t p;\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            v_output = svdup_f64(0.0);\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    v_input = svld1_f64(input + i + ki * input_size + j + kj);\n                    v_kernel = svld1_f64(kernel + ki * kernel_size + kj);\n                    v_output = svmla_f64(v_output, v_input, v_kernel);\n                }\n            }\n\n            v_output = svmax_f64(v_output, svdup_f64(0.0));\n            svst1_f64(output + i * (input_size - kernel_size + 1) + j, v_output);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SVE", "completion": "```c\n#include <arm_sve.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    svfloat64_t v_input, v_kernel, v_output;\n    svbool_t p;\n\n    for (size_t i = 0; i <= input_size - kernel_size; ++i) {\n        for (size_t j = 0; j <= input_size - kernel_size; ++j) {\n            v_output = svdup_f64(0.0);\n\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; ++kj) {\n                    v_input = svld1_f64(input + i + ki * input_size + j + kj);\n                    v_kernel = svld1_f64(kernel + ki * kernel_size + kj);\n                    v_output = svmla_f64(v_output, v_input, v_kernel);\n                }\n            }\n\n            v_output = svmax_f64(v_output, svdup_f64(0.0));\n            svst1_f64(output + i * (input_size - kernel_size + 1) + j, v_output);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    int n = numbers.size();\n    for (int i = 0; i + 7 < n; i += 8) {\n        svfloat64_t v_a = svld1_f64(numbers.data() + i);\n        for (int j = i + 1; j + 7 < n; j += 8) {\n            svfloat64_t v_b = svld1_f64(numbers.data() + j);\n            svfloat64_t v_diff = svabs_f64(svsub_f64(v_a, v_b));\n            svbool_t mask = svcmpge_f64(v_diff, v_threshold);\n            if (svfirst_m_b64(mask) != SV_ALL_FALSE_B64) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    int n = numbers.size();\n    for (int i = 0; i < n - 1; i += 4) {\n        svfloat64_t v_a = svld1_f64(numbers.data() + i);\n        for (int j = i + 1; j < n; j += 4) {\n            svfloat64_t v_b = svld1_f64(numbers.data() + j);\n            svfloat64_t v_diff = svabs_f64(svsub_f64(v_a, v_b));\n            svbool_t mask = svcmpge_f64(v_diff, v_threshold);\n            if (svfirst_m_b64(mask) != SV_ALL_FALSE_B64) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    int n = numbers.size();\n    for (int i = 0; i + 7 < n; i += 8) {\n        svfloat64_t v_a = svld1_f64(numbers.data() + i);\n        for (int j = i + 1; j + 7 < n; j += 8) {\n            svfloat64_t v_b = svld1_f64(numbers.data() + j);\n            svfloat64_t v_diff = svabs_f64(svsub_f64(v_a, v_b));\n            svbool_t mask = svcmpge_f64(v_diff, v_threshold);\n            if (svfirst_m_b64(mask) != SV_ALL_FALSE_B64) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    int n = numbers.size();\n    for (int i = 0; i + 7 < n; i += 8) {\n        svfloat64_t v_a = svld1_f64(numbers.data() + i);\n        for (int j = i + 1; j + 7 < n; j += 8) {\n            svfloat64_t v_b = svld1_f64(numbers.data() + j);\n            svfloat64_t v_diff = svabs_f64(svsub_f64(v_a, v_b));\n            svbool_t mask = svcmpge_f64(v_diff, v_threshold);\n            if (svfirst_m_b64(mask) != SV_ALL_FALSE_B64) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    svfloat64_t v_threshold = svdup_f64(threshold);\n    int n = numbers.size();\n    for (int i = 0; i < n - 1; i += 4) {\n        svfloat64_t v_a = svld1_f64(numbers.data() + i);\n        for (int j = i + 1; j < n; j += 4) {\n            svfloat64_t v_b = svld1_f64(numbers.data() + j);\n            svfloat64_t v_diff = svsub_f64(v_a, v_b);\n            svfloat64_t v_abs_diff = svabs_f64(v_diff);\n            svbool_t mask = svcmpge_f64(v_abs_diff, v_threshold);\n            if (svfirst_m_b64(mask) != SV_ALL_FALSE_B64) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    svint32_t balance = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < operations.size(); i += SV_INT32_SIZE) {\n        svint32_t ops = svld1_u32(&operations[i]);\n        balance = svadd_s32(balance, ops);\n        mask = svor(mask, svtst_s32(balance, svdup_n_s32(0)));\n    }\n\n    return svptest_any_p(mask);\n}\n```"}
{"task_id": "SimdBench_63_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    svint32_t balance = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < operations.size(); i += SV_INT32_SIZE) {\n        svint32_t ops = svld1_u32(&operations[i]);\n        balance = svadd_s32(balance, ops);\n\n        svbool_t below_zero = svtstlt_s32(mask, balance, svdup_n_s32(0));\n        if (svptest_any(below_zero)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    svint32_t balance = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < operations.size(); i += SV_INT32_SIZE) {\n        size_t count = std::min(SV_INT32_SIZE, operations.size() - i);\n        svint32_t ops = svld1_s32(&operations[i]);\n        balance = svadd_s32(balance, ops);\n\n        svbool_t below_zero = svtstlt_s32(mask, balance, svdup_n_s32(0));\n        if (svptest_any(below_zero)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    svint32_t balance = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < operations.size(); i += SV_INT32_SIZE) {\n        size_t count = std::min(SV_INT32_SIZE, operations.size() - i);\n        svint32_t ops = svld1(count, reinterpret_cast<const int32_t*>(&operations[i]));\n        balance = svadd(balance, ops);\n\n        svbool_t below_zero = svtst_gt(balance, svdup_n_s32(0));\n        if (svany_true(below_zero)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    svint32_t balance = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < operations.size(); i += SV_INT32_SIZE) {\n        svint32_t ops = svld1_u32(&operations[i]);\n        balance = svadd_s32(balance, ops);\n\n        svbool_t below_zero = svtstlt_s32(mask, balance, svdup_n_s32(0));\n        if (svptest_any(below_zero)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SVE", "completion": "```c++\n#include <arm_sve.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    svfloat64_t sum = svdup_f64(0.0);\n    svuint32_t indices = svindex_u32(numbers.size());\n    svfloat64_t values = svld1_f64(numbers.data(), indices);\n\n    // Calculate the mean\n    sum = svadd_f64(sum, values);\n    sum = svdiv_f64(sum, svdup_f64(static_cast<double>(numbers.size())));\n    svfloat64_t mean = sum;\n\n    // Calculate the absolute differences from the mean\n    svfloat64_t diff = svsub_f64(values, mean);\n    diff = svabs_f64(diff);\n\n    // Sum the absolute differences\n    sum = svdup_f64(0.0);\n    sum = svadd_f64(sum, diff);\n\n    // Calculate the mean of the absolute differences\n    sum = svdiv_f64(sum, svdup_f64(static_cast<double>(numbers.size())));\n\n    return svget_f64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_64_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cmath>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    svfloat64_t sum = svdup_f64(0.0);\n    for (size_t i = 0; i < n; i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(numbers.data() + i);\n        sum = svadd_f64(sum, vec);\n    }\n    double total_sum = svreduce_add_f64(sum);\n\n    svfloat64_t mean = svdup_f64(total_sum / n);\n    svfloat64_t diff_sum = svdup_f64(0.0);\n    for (size_t i = 0; i < n; i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(numbers.data() + i);\n        svfloat64_t diff = svabs_f64(svsub_f64(vec, mean));\n        diff_sum = svadd_f64(diff_sum, diff);\n    }\n    double total_diff_sum = svreduce_add_f64(diff_sum);\n\n    return total_diff_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    svfloat64_t sum = svdup_f64(0.0);\n    size_t n = numbers.size();\n\n    // Sum all elements in the vector\n    for (size_t i = 0; i < n; i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(numbers.data() + i);\n        sum = svadd_f64(sum, vec);\n    }\n\n    double total_sum = svget_f64(sum, 0);\n    double mean = total_sum / n;\n\n    svfloat64_t diff_sum = svdup_f64(0.0);\n\n    // Calculate the sum of absolute differences from the mean\n    for (size_t i = 0; i < n; i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(numbers.data() + i);\n        svfloat64_t diff = svabs_f64(svsub_f64(vec, svdup_f64(mean)));\n        diff_sum = svadd_f64(diff_sum, diff);\n    }\n\n    double total_diff_sum = svget_f64(diff_sum, 0);\n    double mad = total_diff_sum / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(const std::vector<double>& numbers) {\n    svfloat64_t sum = svdup_f64(0.0);\n    svuint32_t indices = svindex_u32(numbers.size());\n    svbool_t mask = svle_u32(indices, svdup_u32(numbers.size() - 1));\n\n    for (size_t i = 0; i < numbers.size(); i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(&numbers[i]);\n        sum = svadd_f64(sum, vec);\n    }\n\n    double total_sum = svget_f64(sum, 0);\n    double mean = total_sum / numbers.size();\n\n    svfloat64_t diff_sum = svdup_f64(0.0);\n\n    for (size_t i = 0; i < numbers.size(); i += SV_INT64_ELEMS) {\n        svfloat64_t vec = svld1_f64(&numbers[i]);\n        svfloat64_t diff = svabs_f64(svsub_f64(vec, svdup_f64(mean)));\n        diff_sum = svadd_f64(diff_sum, diff);\n    }\n\n    double total_diff_sum = svget_f64(diff_sum, 0);\n    return total_diff_sum / numbers.size();\n}\n```"}
{"task_id": "SimdBench_64_SVE", "completion": "```c++\n#include <arm_sve.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    svfloat64_t sum = svdup_f64(0.0);\n    svuint32_t indices = svindex_u32(numbers.size());\n    svfloat64_t values = svld1_f64(numbers.data(), indices);\n\n    // Calculate the mean\n    sum = svadd_f64(sum, values);\n    sum = svdiv_f64(sum, svdup_f64(static_cast<double>(numbers.size())));\n    svfloat64_t mean = sum;\n\n    // Calculate the absolute differences from the mean\n    svfloat64_t diff = svsub_f64(values, mean);\n    diff = svabs_f64(diff);\n\n    // Sum the absolute differences\n    sum = svdup_f64(0.0);\n    sum = svadd_f64(sum, diff);\n\n    // Calculate the mean of the absolute differences\n    sum = svdiv_f64(sum, svdup_f64(static_cast<double>(numbers.size())));\n\n    return svget_f64(sum, 0);\n}\n```"}
{"task_id": "SimdBench_65_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t total_size = n * 2 - 1;\n    std::vector<int> result(total_size);\n\n    svint32x8_t v_delimiter = svdup_n_s32(delimiter);\n    svbool_t mask = svptrue_b32();\n\n    size_t i = 0;\n    while (i + 7 < n) {\n        svint32x8_t v_numbers = svld1_u32(numbers.data() + i);\n        svst1_u32(result.data() + i * 2, v_numbers);\n        svst1_u32(result.data() + i * 2 + 1, v_delimiter);\n        i += 8;\n    }\n\n    while (i < n) {\n        result[i * 2] = numbers[i];\n        result[i * 2 + 1] = delimiter;\n        i++;\n    }\n\n    // Remove the last delimiter\n    result.pop_back();\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t result_size = n * 2 - 1;\n    std::vector<int> result(result_size);\n\n    svint32x8_t v_delimiter = svdup_n_s32(delimiter);\n    svbool_t p = svptrue_b32();\n\n    size_t i = 0;\n    for (; i + 7 < n; i += 8) {\n        svint32x8_t v_numbers = svld1_u32(numbers.data() + i);\n        svst1_u32(result.data() + i * 2, svzip1(v_numbers, v_delimiter));\n        svst1_u32(result.data() + i * 2 + 8, svzip2(v_numbers, v_delimiter));\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n        result[i * 2 + 1] = delimiter;\n    }\n\n    // Remove the last delimiter\n    result.pop_back();\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t output_size = n * 2 - 1;\n    std::vector<int> result(output_size);\n\n    svint32x4_t v_delimiter = svdup_n_s32(delimiter);\n    svbool_t mask = svptrue_b32();\n\n    size_t i = 0;\n    for (; i + 7 < n; i += 8) {\n        svint32x4_t v_numbers1 = svld1_s32(numbers.data() + i);\n        svint32x4_t v_numbers2 = svld1_s32(numbers.data() + i + 4);\n        svst1_s32(result.data() + i * 2, svzip1(v_numbers1, v_delimiter));\n        svst1_s32(result.data() + i * 2 + 8, svzip2(v_numbers1, v_delimiter));\n        svst1_s32(result.data() + i * 2 + 16, svzip1(v_numbers2, v_delimiter));\n        svst1_s32(result.data() + i * 2 + 24, svzip2(v_numbers2, v_delimiter));\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != n - 1) {\n            result[i * 2 + 1] = delimiter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t output_size = n * 2 - 1;\n    std::vector<int> result(output_size);\n\n    svint32x8_t v_delimiter = svdup_n_s32(delimiter);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < n; ++i) {\n        svint32x8_t v_number = svdup_n_s32(numbers[i]);\n        svst1q_lane_s32(&result[i * 2], v_number, 0);\n        if (i + 1 < n) {\n            svst1q_lane_s32(&result[i * 2 + 1], v_delimiter, 0);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(const std::vector<int>& numbers, int delimiter) {\n    size_t n = numbers.size();\n    if (n == 0) return {};\n\n    size_t output_size = n * 2 - 1;\n    std::vector<int> result(output_size);\n\n    svint32x8_t v_delimiter = svdup_n_s32(delimiter);\n    svbool_t p = svptrue_b32();\n\n    size_t i = 0;\n    while (i + 7 < n) {\n        svint32x8_t v_numbers = svld1_u32(numbers.data() + i);\n        svst1_u32(result.data() + i * 2, v_numbers);\n        svst1_u32(result.data() + i * 2 + 1, v_delimiter);\n        i += 8;\n    }\n\n    for (; i < n; ++i) {\n        result[i * 2] = numbers[i];\n        if (i != n - 1) {\n            result[i * 2 + 1] = delimiter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    svint16_t nums = svld1(svptrue_b16(), numbers.data());\n    svbool_t mask = svptrue_b16();\n\n    while (!svptest_any(mask)) {\n        sum += svaddv_s32(svreinterpret_u32_s16(nums));\n        product *= svpmulav_s32(svreinterpret_u32_s16(nums), svdup_n_s32(product));\n\n        mask = svsliding_window_and(mask, svptrue_b16(), 1);\n        nums = svld1(mask, &numbers[svfirst_m_b16(mask)]);\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    svint16_t nums = svld1(svptrue_b16(), numbers.data());\n    svint64_t sum = svaddv_s64(nums);\n    svint64_t product = svmulv_s64(nums);\n\n    if (svcntw_b16() == 0) {\n        sum = svdup_n_s64(0);\n        product = svdup_n_s64(1);\n    }\n\n    return {sum.val[0], product.val[0]};\n}\n```"}
{"task_id": "SimdBench_66_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    svint16_t vec = svld1_u16(numbers.data());\n    svint64_t sum = svaddlv_s16(vec);\n    svint64_t product = svpmulav_s16(svreinterpret_s16_m_f32(svdup_n_f32(1.0f)), vec);\n    return {svget_lane_s64(sum, 0), svget_lane_s64(product, 0)};\n}\n```"}
{"task_id": "SimdBench_66_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<int64_t, int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    svint16_t vec = svld1_u16(numbers.data());\n    svbool_t mask = svptrue_b16();\n    svint64_t sum = svaddlv_s16(vec);\n    svint64_t product = svmul_lv_s16(svone_s16(), vec);\n\n    if (svptest_any(mask, svcmpeq_z_s16(sum, svzero_s16()))) {\n        sum = svdup_n_s64(0);\n    }\n\n    if (svptest_any(mask, svcmpeq_z_s16(product, svzero_s16()))) {\n        product = svdup_n_s64(1);\n    }\n\n    return {svgetlane_s64(sum, 0), svgetlane_s64(product, 0)};\n}\n```"}
{"task_id": "SimdBench_66_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(const std::vector<int16_t>& numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    svbool_t p = svptrue_b16();\n    svint16_t vnumbers = svld1(p, numbers.data());\n    svint64_t vsum = svdup_n_s64(0);\n    svint64_t vproduct = svdup_n_s64(1);\n\n    for (size_t i = 0; i < numbers.size(); i += SV_INT16_SIZE) {\n        vnumbers = svld1(p, &numbers[i]);\n        vsum = svaddlv_s16(vsum, vnumbers);\n        vproduct = svmul_lv_s16(vproduct, vnumbers);\n    }\n\n    sum = svgetlane_s64(vsum, 0);\n    product = svgetlane_s64(vproduct, 0);\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    svint32_t* input = reinterpret_cast<svint32_t*>(numbers.data());\n    svint32_t* output = reinterpret_cast<svint32_t*>(result.data());\n    \n    svint32_t max_val = svdup_n_s32(INT_MIN);\n    \n    for (int i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vec = svld1_u32(input + i);\n        max_val = svmax_s32(max_val, vec);\n        svst1_u32(output + i, max_val);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    svint32_t* input = reinterpret_cast<svint32_t*>(const_cast<int*>(&numbers[0]));\n    svint32_t* output = reinterpret_cast<svint32_t*>(result.data());\n\n    for (int i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vec = svld1_u32(input + i);\n        svint32_t max_vec = vec;\n        for (int j = 1; j * SV_INT32_SIZE <= n - i; ++j) {\n            svint32_t next_vec = svld1_u32(input + i + j * SV_INT32_SIZE);\n            max_vec = svmax_s32(max_vec, next_vec);\n        }\n        svst1_u32(output + i / SV_INT32_SIZE, max_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    svint32_t* input = reinterpret_cast<svint32_t*>(const_cast<int*>(&numbers[0]));\n    svint32_t* output = reinterpret_cast<svint32_t*>(result.data());\n\n    for (int i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vec = svld1_u32(input + i);\n        svint32_t max_vec = vec;\n        for (int j = i + SV_INT32_SIZE; j < n && j - i < SV_INT32_SIZE; j++) {\n            svint32_t val = vld1_lane_s32(&numbers[j], svdup_n_s32(0), 0);\n            max_vec = vmax_s32(max_vec, val);\n        }\n        svst1_u32(output + i, max_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    if (n == 0) return {};\n\n    std::vector<int> result(n);\n    svint32_t max_vec = svdup_n_s32(numbers[0]);\n\n    for (int i = 1; i < n; ++i) {\n        svint32_t current_vec = svdup_n_s32(numbers[i]);\n        max_vec = svmax_s32(max_vec, current_vec);\n        result[i] = svget_lane_s32(max_vec, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(const std::vector<int>& numbers) {\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    if (n == 0) return result;\n    \n    svint32_t max_vec = svdup_n_s32(numbers[0]);\n    result[0] = numbers[0];\n    \n    for (int i = 1; i < n; ++i) {\n        svint32_t current_vec = svdup_n_s32(numbers[i]);\n        max_vec = svmax_s32(max_vec, current_vec);\n        result[i] = svget_lane_s32(max_vec, 0);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string string_xor_simd(const std::string& a, const std::string& b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string result(len, '0');\n\n    svbool_t pg = svptrue_b8();\n    svuint8_t va = svld1_u8(reinterpret_cast<const uint8_t*>(a.c_str()));\n    svuint8_t vb = svld1_u8(reinterpret_cast<const uint8_t*>(b.c_str()));\n\n    for (size_t i = 0; i < len; i += 16) {\n        svuint8_t vresult = svorn_x(pg, va, vb);\n        svst1_u8(reinterpret_cast<uint8_t*>(&result[i]), vresult);\n\n        va = svld1_u8(reinterpret_cast<const uint8_t*>(&a[i + 16]));\n        vb = svld1_u8(reinterpret_cast<const uint8_t*>(&b[i + 16]));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string result(len, '0');\n\n    svbool_t pg = svptrue_b8();\n    svuint8_t va = svld1_u8(reinterpret_cast<const uint8_t*>(a.c_str()));\n    svuint8_t vb = svld1_u8(reinterpret_cast<const uint8_t*>(b.c_str()));\n\n    for (size_t i = 0; i < len; i += SVVectorSize) {\n        svuint8_t vresult = svorn_x(pg, va, vb);\n        svst1_u8(reinterpret_cast<uint8_t*>(&result[i]), vresult);\n        va = svld1_u8(reinterpret_cast<const uint8_t*>(&a[i + SVVectorSize]));\n        vb = svld1_u8(reinterpret_cast<const uint8_t*>(&b[i + SVVectorSize]));\n    }\n\n    return result;\n}\n```\n\nNote: The `SVVectorSize` macro should be defined based on the specific SVE vector length you want to use. For example, if you want to use 256-bit vectors, you can define `SVVectorSize` as 32 (since each byte is 8 bits)."}
{"task_id": "SimdBench_68_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = std::min(a.size(), b.size());\n    std::string result(len, '0');\n\n    svbool_t pg = svptrue_b8();\n    svuint8_t va = svld1_u8(reinterpret_cast<const uint8_t*>(a.c_str()));\n    svuint8_t vb = svld1_u8(reinterpret_cast<const uint8_t*>(b.c_str()));\n\n    for (size_t i = 0; i < len; i += 16) {\n        svuint8_t vresult = svorn_x(pg, va, vb);\n        svst1_u8(reinterpret_cast<uint8_t*>(&result[i]), vresult);\n\n        va = svld1_u8(reinterpret_cast<const uint8_t*>(&a[i + 16]));\n        vb = svld1_u8(reinterpret_cast<const uint8_t*>(&b[i + 16]));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n\n    svbool_t p = svptrue_b8();\n\n    for (size_t i = 0; i < len; i += SV_INT8_SIZE) {\n        svuint8_t va = svld1_u8(a.data() + i);\n        svuint8_t vb = svld1_u8(b.data() + i);\n        svuint8_t vr = svorn_u8(va, vb);\n        svst1_u8(result.data() + i, vr);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must have the same length\");\n    }\n\n    std::string result(len, '0');\n    svbool_t pg = svptrue_b8();\n\n    for (size_t i = 0; i < len; i += SV_INT8_SIZE) {\n        svuint8_t va = svld1_u8(a.data() + i);\n        svuint8_t vb = svld1_u8(b.data() + i);\n        svuint8_t vr = svorn_u8(va, vb);\n        svst1_u8(result.data() + i, vr);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_SVE", "completion": "```cpp\n\n```"}
{"task_id": "SimdBench_69_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    svuint8_t max_len = svdup_u8(0);\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < strings.size(); ++i) {\n        svuint8_t len = svdup_u8(strings[i].length());\n        svbool_t mask = svcmpgt_b8(len, max_len);\n        max_len = svsel(mask, len, max_len);\n        max_index = svfirst_m_b8(mask);\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SVE", "completion": "```cpp\n\n```"}
{"task_id": "SimdBench_69_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    svuint8_t max_len = svdup_u8(0);\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < strings.size(); ++i) {\n        svuint8_t len = svdup_u8(strings[i].length());\n        svbool_t mask = svcmpgt_b8(len, max_len);\n        max_len = svmul_b8(mask, len, max_len);\n        max_index = svfirst_m_b8(mask, svreinterpret_u32_u8(svadd_z_u32(svmul_n_u32(i, 4), svreinterpret_u32_u8(max_len))));\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string>& strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    svbool_t p = svptrue_b8();\n    size_t max_len = 0;\n    const char* longest_str = nullptr;\n\n    for (const auto& str : strings) {\n        size_t len = str.size();\n        if (len > max_len) {\n            max_len = len;\n            longest_str = str.c_str();\n        }\n    }\n\n    return std::string(longest_str, max_len);\n}\n```"}
{"task_id": "SimdBench_70_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    svbool_t p = svcntp_b8(a, b);\n    while (!svptest_any(p)) {\n        svuint64_t remainder = svsub_u64(a, svmul_u64(b, svdiv_u64(a, b)));\n        a = b;\n        b = remainder;\n        p = svcntp_b8(a, b);\n    }\n    return a;\n}\n```"}
{"task_id": "SimdBench_70_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    svbool_t p = svcntp_b64(a, b);\n    while (!svptest_any(p)) {\n        uint64_t min_val = svmin_u64(a, b);\n        uint64_t max_val = svmax_u64(a, b);\n        a = max_val;\n        b = min_val;\n        p = svcntp_b64(a, b);\n    }\n    return b;\n}\n```"}
{"task_id": "SimdBench_70_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    svbool_t p = svptrue_b64();\n    svuint64_t va = svdup_u64(a);\n    svuint64_t vb = svdup_u64(b);\n\n    while (!svallb(p)) {\n        svuint64_t vrem = svmod_u64(va, vb);\n        p = svcmpne_z_b64(vrem, svzextw_u64(svptrue_b32(), svreinterpret_u32(vrem)));\n        va = vb;\n        vb = vrem;\n    }\n\n    return svfirst_u64(vb);\n}\n```"}
{"task_id": "SimdBench_70_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    svbool_t p = svcntp_b64(a, b);\n    while (!svptest_any(p)) {\n        uint64_t min_val = svmin_u64(a, b);\n        uint64_t max_val = svmax_u64(a, b);\n        a = max_val;\n        b = min_val;\n        p = svcntp_b64(a, b);\n    }\n    return b;\n}\n```"}
{"task_id": "SimdBench_70_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    svbool_t p = svcntp_b64(a, b);\n    while (!svptest_any(p)) {\n        svuint64_t remainder = svmod_u64(a, b);\n        a = b;\n        b = remainder;\n        p = svcntp_b64(a, b);\n    }\n    return b;\n}\n```"}
{"task_id": "SimdBench_71_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    svfloat32_t vec = svld1_f32(numbers.data());\n    svfloat32_t min_diff = svdup_n_f32(INFINITY);\n    svuint32x4_t idx_min_diff = svdup_n_u32(0);\n\n    for (size_t i = 0; i + 4 <= numbers.size(); i += 4) {\n        svfloat32_t diff = svabs_f32(svsub_f32(vec, svext_f32(vec, svdup_n_f32(numbers[i]), 1)));\n        svbool_t mask = svcmplt_f32(diff, min_diff);\n        min_diff = svsel(mask, diff, min_diff);\n        idx_min_diff = svsel(mask, svreinterpret_u32_x4(svindex_f32(0, 4)), idx_min_diff);\n    }\n\n    float min_val = INFINITY;\n    uint32_t idx = 0;\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        if (fabsf(numbers[i] - numbers[idx_min_diff[0]]) < min_val) {\n            min_val = fabsf(numbers[i] - numbers[idx_min_diff[0]]);\n            idx = i;\n        }\n    }\n\n    return {numbers[idx], numbers[idx_min_diff[0]]};\n}\n```"}
{"task_id": "SimdBench_71_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    svfloat32_t vec = svld1_f32(numbers.data());\n    svfloat32_t min_diff = svdup_n_f32(INFINITY);\n    svuint32x4_t indices = svdup_n_u32(0);\n\n    for (size_t i = 0; i + 4 <= numbers.size(); i += 4) {\n        svfloat32_t diff = svabs_f32(svsub_f32(vec, svld1_f32(&numbers[i + 4])));\n        svbool_t mask = svcmple_f32(diff, min_diff);\n        min_diff = svsel(mask, diff, min_diff);\n        indices = svsel(mask, svdup_n_u32(i + 4), indices);\n    }\n\n    float min_val = INFINITY;\n    size_t min_index = 0;\n    for (size_t i = 0; i < numbers.size(); ++i) {\n        if (fabsf(numbers[i] - numbers[indices[min_index]]) < fabsf(min_val - numbers[indices[min_index]])) {\n            min_val = numbers[i];\n            min_index = i;\n        }\n    }\n\n    return {min_val, numbers[min_index]};\n}\n```"}
{"task_id": "SimdBench_71_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    svfloat32_t vec = svld1_f32(numbers.data());\n    svfloat32_t min_diff = svdup_n_f32(INFINITY);\n    svuint32x4_t idx_min_diff = svdup_n_u32(0);\n\n    for (size_t i = 0; i < numbers.size() - 1; i += 4) {\n        svfloat32_t vec_i = svld1q_lane_f32(&numbers[i], vec, 0);\n        svfloat32_t diff = svabs_f32(svsub_f32(vec_i, svext_f32(vec_i, vec_i, 1)));\n        svbool_t mask = svcmple_f32(diff, min_diff);\n        min_diff = svsel(mask, diff, min_diff);\n        idx_min_diff = svsel(mask, svreinterpret_u32_u64(svptrue_b32()), idx_min_diff);\n    }\n\n    size_t idx1 = svfirst_m_b32(svptrue_b32(), idx_min_diff);\n    size_t idx2 = idx1 + 1;\n\n    return {numbers[idx1], numbers[idx2]};\n}\n```"}
{"task_id": "SimdBench_71_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    svfloat32_t vec = svld1_f32(numbers.data());\n    svfloat32_t min_diff = svdup_n_f32(INFINITY);\n    svuint32x4_t indices = svdup_u32(0);\n\n    for (size_t i = 0; i + 4 <= numbers.size(); i += 4) {\n        svfloat32_t diff = svabs_f32(svsub_f32(vec, svext_f32(vec, svdup_n_f32(numbers[i]), 1)));\n        svbool_t mask = svcmple_f32(diff, min_diff);\n        min_diff = svsel(mask, diff, min_diff);\n        indices = svsel(mask, svadd_x_u32(svreinterpret_u32_u8(svptrue_b8()), svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(indices), svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuzp1_u32(svreinterpret_u32_u8(svuz"}
{"task_id": "SimdBench_71_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::pair<float, float> find_closest_elements_simd(const std::vector<float>& numbers) {\n    svfloat32_t vec = svld1_f32(numbers.data());\n    svfloat32_t min_diff = svdup_n_f32(INFINITY);\n    svuint32x4_t indices = svdup_u32(0);\n\n    for (size_t i = 0; i + 4 <= numbers.size(); i += 4) {\n        svfloat32_t diff = svabs_f32(svsub_f32(vec, svext_f32(vec, svdup_n_f32(numbers[i]), 1)));\n        svbool_t mask = svcmplt_f32(diff, min_diff);\n        min_diff = svsel(mask, diff, min_diff);\n        indices = svsel(mask, svuzp1_u32(svuzp1_u32(svreinterpret_u32_f32(vec), svreinterpret_u32_f32(svext_f32(vec, svdup_n_f32(numbers[i]), 1)))), indices);\n    }\n\n    float min_diff_val = svget_f32(min_diff, 0);\n    size_t idx1 = svget_u32(indices, 0);\n    size_t idx2 = svget_u32(indices, 1);\n\n    if (idx1 > idx2) {\n        std::swap(idx1, idx2);\n    }\n\n    return {numbers[idx1], numbers[idx2]};\n}\n```"}
{"task_id": "SimdBench_72_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    int n = numbers.size();\n    if (n < 2) {\n        return numbers;\n    }\n\n    svfloat32_t min_val = svdup_n_f32(FLT_MAX);\n    svfloat32_t max_val = svdup_n_f32(-FLT_MAX);\n\n    // Find min and max values in parallel\n    for (int i = 0; i + 16 <= n; i += 16) {\n        svfloat32_t vec = svld1q_f32(numbers.data() + i);\n        min_val = svminnmv_f32(min_val, vec);\n        max_val = svmaxnmv_f32(max_val, vec);\n    }\n    for (; i < n; ++i) {\n        min_val = svminnm_f32(min_val, svdup_n_f32(numbers[i]));\n        max_val = svmaxnm_f32(max_val, svdup_n_f32(numbers[i]));\n    }\n\n    float min_val_scalar = svget_f32(min_val, 0);\n    float max_val_scalar = svget_f32(max_val, 0);\n\n    std::vector<float> result(n);\n    for (int i = 0; i + 16 <= n; i += 16) {\n        svfloat32_t vec = svld1q_f32(numbers.data() + i);\n        svfloat32_t scaled_vec = svcmulq_f32(svsubq_f32(vec, svdup_n_f32(min_val_scalar)), svrsqrteq_f32(svsubq_f32(svdup_n_f32(max_val_scalar - min_val_scalar), svdup_n_f32(1.0f))));\n        svst1q_f32(result.data() + i, scaled_vec);\n    }\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val_scalar) / (max_val_scalar - min_val_scalar);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    int n = numbers.size();\n    if (n < 2) {\n        return numbers;\n    }\n\n    svfloat32_t v_min = svdup_n_f32(FLT_MAX);\n    svfloat32_t v_max = svdup_n_f32(-FLT_MAX);\n\n    // Find min and max in parallel\n    for (int i = 0; i + 16 <= n; i += 16) {\n        svfloat32_t v = svld1q_f32(numbers.data() + i);\n        v_min = svminnmv_f32(v_min, v);\n        v_max = svmaxnmv_f32(v_max, v);\n    }\n    for (; i < n; ++i) {\n        v_min = svminnm_f32(v_min, svdup_n_f32(numbers[i]));\n        v_max = svmaxnm_f32(v_max, svdup_n_f32(numbers[i]));\n    }\n\n    float min_val = svgetlane_f32(v_min, 0);\n    float max_val = svgetlane_f32(v_max, 0);\n\n    if (min_val == max_val) {\n        return numbers;\n    }\n\n    std::vector<float> result(n);\n    svfloat32_t v_range = svsub_f32(v_max, v_min);\n    svfloat32_t v_scale = svdiv_f32(svsetvl_f32(n), v_range);\n\n    for (int i = 0; i + 16 <= n; i += 16) {\n        svfloat32_t v = svld1q_f32(numbers.data() + i);\n        v = svsub_f32(v, v_min);\n        v = svmul_f32(v, v_scale);\n        svst1q_f32(result.data() + i, v);\n    }\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) * (n / (max_val - min_val));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    svfloat32_t v_numbers = svld1_f32(numbers.data());\n    int64_t n = numbers.size();\n    svint32_t v_indices = svdup_n_s32(0);\n    svbool_t mask = svwhilelt_b32(v_indices, svdup_n_s32(n));\n\n    svfloat32_t v_min = svminv_u32(mask, v_numbers);\n    svfloat32_t v_max = svmaxv_u32(mask, v_numbers);\n\n    svfloat32_t v_range = svsub_f32(v_max, v_min);\n    svfloat32_t v_result = svdiv_f32(svsub_f32(v_numbers, v_min), v_range);\n\n    std::vector<float> result(n);\n    svst1_f32(result.data(), v_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    int n = numbers.size();\n    if (n < 2) return numbers;\n\n    svfloat32_t* data = reinterpret_cast<svfloat32_t*>(const_cast<float*>(&numbers[0]));\n    int64_t num_elements = n / 8;\n    int remainder = n % 8;\n\n    svfloat32_t min_val = svdup_n_f32(FLT_MAX);\n    svfloat32_t max_val = svdup_n_f32(-FLT_MAX);\n\n    // Find min and max values in parallel\n    for (int i = 0; i < num_elements; ++i) {\n        svfloat32_t vec = data[i];\n        min_val = svmin_f32(min_val, vec);\n        max_val = svmax_f32(max_val, vec);\n    }\n\n    // Handle remaining elements\n    for (int i = 0; i < remainder; ++i) {\n        float val = numbers[num_elements * 8 + i];\n        min_val = svmin_f32(min_val, svdup_n_f32(val));\n        max_val = svmax_f32(max_val, svdup_n_f32(val));\n    }\n\n    float min_val_scalar = svreduce_min_f32(min_val);\n    float max_val_scalar = svreduce_max_f32(max_val);\n\n    std::vector<float> result(n);\n    svfloat32_t* result_data = reinterpret_cast<svfloat32_t*>(&result[0]);\n\n    // Apply linear transformation in parallel\n    for (int i = 0; i < num_elements; ++i) {\n        svfloat32_t vec = data[i];\n        svfloat32_t scaled_vec = svsub_f32(svdiv_f32(svreinterpret_f32_u32(svreinterpret_u32_f32(vec)), svdup_n_f32(max_val_scalar - min_val_scalar)), svdup_n_f32(min_val_scalar / (max_val_scalar - min_val_scalar)));\n        result_data[i] = scaled_vec;\n    }\n\n    // Handle remaining elements\n    for (int i = 0; i < remainder; ++i) {\n        float val = numbers[num_elements * 8 + i];\n        float scaled_val = (val - min_val_scalar) / (max_val_scalar - min_val_scalar);\n        result[num_elements * 8 + i] = scaled_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(const std::vector<float>& numbers) {\n    svfloat32_t v_numbers = svld1_f32(numbers.data());\n    svfloat32_t vmin = svminv_f32(v_numbers);\n    svfloat32_t vmax = svmaxv_f32(v_numbers);\n    svfloat32_t range = svsub_f32(vmax, vmin);\n    svfloat32_t result = svdiv_f32(svsub_f32(v_numbers, vmin), range);\n    return {svst1_f32(result)};\n}\n```"}
{"task_id": "SimdBench_73_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    svbool_t p = svptrue_b64();\n    svuint64_t v = svdup_u64(n);\n    svuint64_t divisors = svset_u64(0);\n\n    for (int i = 2; i <= n / 2; i++) {\n        svuint64_t divisor = svdup_u64(i);\n        svuint64_t remainder = svmod_u64(v, divisor);\n        svbool_t is_divisor = svcmple_z_b64(svreinterpret_u64_u8(svreinterpret_u8_u64(remainder)), svfalse_b64());\n        divisors = svsel(is_divisor, divisor, divisors);\n    }\n\n    return svmax_u64(divisors);\n}\n```"}
{"task_id": "SimdBench_73_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    svbool_t p = svptrue_b64();\n    uint64_t max_divisor = 0;\n    for (int i = 2; i <= n / 2; i++) {\n        svuint64_t divisors = svdup_u64(i);\n        svbool_t mask = svcmplt_z_u64(divisors, n);\n        svuint64_t valid_divisors = svsel(mask, divisors, svzero_u64());\n        svuint64_t remainders = svmod_u64(n, valid_divisors);\n        svbool_t is_divisible = svcmpeq_z_u64(remainders, svzero_u64());\n        svuint64_t current_max = svmax_u64(svsel(is_divisible, valid_divisors, svzero_u64()), svzero_u64());\n        max_divisor = svmax_u64(max_divisor, current_max);\n    }\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    svbool_t p = svptrue_b64();\n    svuint64_t v = svdup_u64(n);\n    svuint64_t divisors = svset_u64(0);\n\n    for (int i = 2; i <= n / 2; i++) {\n        svuint64_t divisor = svdup_u64(i);\n        svbool_t mask = svcmple_u64(v, divisor);\n        svuint64_t remainder = svsub_u64(v, svand_u64(v, svcmplt_u64(v, divisor)));\n        svuint64_t new_divisors = svsel(mask, divisor, divisors);\n        divisors = new_divisors;\n    }\n\n    return svmax_u64(divisors);\n}\n```"}
{"task_id": "SimdBench_73_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    svbool_t p = svptrue_b64();\n    uint64_t max_divisor = 0;\n    for (int i = 2; i <= n / 2; i++) {\n        svuint64_t divisors = svdup_u64(i);\n        svbool_t mask = svcmplt_u64(divisors, svreinterpret_u64(n));\n        svuint64_t valid_divisors = svsel(mask, divisors, svzero_u64());\n        svuint64_t remainders = svmod_u64(valid_divisors, svreinterpret_u64(n));\n        svbool_t is_zero = sveor_z(p, remainders, svzero_u64());\n        if (svfirst_m(is_zero) != SV_ALL_FALSE) {\n            max_divisor = i;\n        }\n    }\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    svbool_t p = svptrue_b64();\n    uint64_t max_divisor = 0;\n    for (int i = 2; i <= n / 2; i++) {\n        svuint64_t divisors = svdup_u64(i);\n        svuint64_t remainders = svmod_u64(n, divisors);\n        svbool_t is_divisible = svcmple_u64(remainders, svdup_u64(0));\n        if (svany(is_divisible)) {\n            max_divisor = i;\n        }\n    }\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_74_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    svint32_t v_n = svdup_n_s32(n);\n    svbool_t p = svcntp_u32(svld1_s32(&n), 1);\n\n    while (!svallb(p)) {\n        int divisor = 2;\n        svint32_t v_divisor = svdup_n_s32(divisor);\n        svbool_t divisible = svcmple_s32(v_n, v_divisor);\n\n        while (svany(divisible)) {\n            if (svfirst_m_b(divisible)) {\n                factors.push_back(divisor);\n                v_n = svsub_s32(v_n, svand_s32(v_n, v_divisor));\n            }\n            v_divisor = svadd_s32(v_divisor, svdup_n_s32(1));\n            divisible = svcmple_s32(v_n, v_divisor);\n        }\n\n        p = svcntp_u32(svld1_s32(&divisor), 1);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    svint32_t v_n = svdup_n_s32(n);\n    svbool_t p = svcntp_u32(svptrue_b32(), v_n);\n\n    while (!svptest_any(p)) {\n        svuint32_t v_i = svld1_u32(&factors.back());\n        svuint32_t v_div = svdup_n_u32(v_i);\n        svuint32_t v_mod = svmodu_x_u32(v_n, v_div);\n        p = svcmpeq_z_u32(svcmov_u32(svptrue_b32(), v_mod, svdup_n_u32(0)), svdup_n_u32(0));\n        if (svptest_any(p)) {\n            factors.push_back(v_i);\n            v_n = svdiv_x_u32(v_n, v_div);\n        } else {\n            factors.push_back(v_n[0]);\n            break;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    svint32_t v_n = svdup_n_s32(n);\n    svbool_t p = svcntp_u32(svptrue_b32(), v_n);\n\n    while (!svptest_any(p)) {\n        svuint32_t v_i = svld1_u32(&factors.back());\n        svuint32_t v_div = svdup_n_u32(factors.back());\n        svuint32_t v_mod = svmodu_u32(v_n, v_div);\n        p = svcmpne_u32(v_mod, svdup_n_u32(0));\n        if (svptest_any(p)) {\n            factors.push_back(factors.back() + 1);\n        } else {\n            factors.pop_back();\n            factors.push_back(factors.back() + 1);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    svint32_t v_n = svdup_n_s32(n);\n    svbool_t p = svcntp_u32(svld1_u32(nullptr), 1);\n\n    while (!svany(p)) {\n        svuint32_t v_i = svld1_u32(nullptr);\n        svuint32_t v_div = svdiv_u32(v_n, v_i);\n        svbool_t v_is_prime = svcmple_u32(v_i, v_n);\n        svbool_t v_is_factor = svcmpeq_u32(v_n, v_div);\n        p = svand_b32(p, svnot_b32(v_is_prime));\n        if (svany(v_is_factor)) {\n            factors.push_back(svextract_u32(v_i, 0, 1));\n            v_n = svsub_z_u32(v_n, v_div);\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    svint32_t v_n = svdup_n_s32(n);\n    svbool_t p = svcntp_u32(svptrue_b32(), v_n);\n\n    while (!svptest_any(p)) {\n        int factor = svfirst_m_b32(p, v_n);\n        factors.push_back(factor);\n        v_n = svdiv_s32(v_n, svdup_n_s32(factor));\n        p = svcntp_u32(svptrue_b32(), v_n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svbool_t mask;\n\n    for (size_t i = 0; i < n; i += SV_INT32_ELEMS) {\n        svfloat32_t vin = svld1_u32(l.data() + i);\n        mask = svgtz_f32(vin, vzero);\n        svst1_u32(result.data() + i, svsel(mask, vin, vzero));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svbool_t mask;\n\n    for (size_t i = 0; i < n; i += SV_INT32_ELEMS) {\n        svfloat32_t vin = svld1_f32(l.data() + i);\n        mask = svgt_f32(vin, vzero);\n        svst1_f32(result.data() + i, svsel(mask, vin, vzero));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svbool_t mask;\n\n    for (size_t i = 0; i < n; i += SV_INT32_ELEMS) {\n        svfloat32_t vin = svld1_u32(l.data() + i);\n        mask = svgtz_f32(vin, vzero);\n        svst1_u32(result.data() + i, svsel(mask, vin, vzero));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    std::vector<float> result;\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svbool_t mask;\n\n    for (size_t i = 0; i < n; i += SV_INT32_ELEMS) {\n        svfloat32_t vin = svld1_f32(l.data() + i);\n        mask = svgt_f32(vin, vzero);\n        svfloat32_t vout = svsel(mask, vin, vzero);\n        size_t count = svcntb(mask);\n        result.insert(result.end(), reinterpret_cast<const float*>(&vout), reinterpret_cast<const float*>(&vout) + count);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svbool_t mask;\n\n    for (size_t i = 0; i < n; i += SV_INT32_ELEMS) {\n        svfloat32_t vin = svld1_f32(l.data() + i);\n        mask = svgtz_f32(vin, vzero);\n        svst1_f32(result.data() + i, svsel(mask, vin, vzero));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    svint64_t vlimit = svdup_n_s64(limit);\n    svuint64_t vstep = svdup_n_u64(6);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        svuint64_t vi = svdup_n_u64(i);\n        svuint64_t vm = svadd_x_u64(vi, vstep);\n        svuint64_t vr = svand_x_u64(svand_x_u64(svmod_x_u64(n, vi), svmod_x_u64(n, vm)), svdup_n_u64(n - 1));\n        if (!svany_true_z(sve_true(), vr)) continue;\n        return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    svint64_t vlimit = svdup_n_s64(limit);\n    svuint64_t vstep = svdup_n_u64(6);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        svuint64_t vi = svdup_n_u64(i);\n        svuint64_t vm = svadd_x_u64(vi, vstep);\n        svuint64_t vr = svand_x_u64(svand_x_u64(svmod_x_u64(n, vi), svmod_x_u64(n, vm)), svdup_n_u64(1));\n        if (!svany_true_b8(sveor_z_b8(zero_b8(), vr))) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    svint64_t vlimit = svdup_n_s64(limit);\n    svuint64_t vstep = svdup_n_u64(6);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        svuint64_t vi = svdup_n_u64(i);\n        svuint64_t vm = svadd_x_u64(vi, vstep);\n        svuint64_t vr = svand_x_u64(svnot_x_u64(sveor_x_u64(vi, vm)), svle_x_u64(vr, vlimit));\n        if (!svany_true_b8(vr)) continue;\n\n        svuint64_t vn = svdiv_x_u64(n, vi);\n        svuint64_t vmn = svdiv_x_u64(n, vm);\n        svuint64_t vr2 = svand_x_u64(svnot_x_u64(sveor_x_u64(vn, vmn)), svle_x_u64(vr2, vlimit));\n        if (svany_true_b8(vr2)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    svint64_t vlimit = svdup_n_s64(limit);\n    svuint64_t vstep = svdup_n_u64(6);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        svuint64_t vi = svdup_n_u64(i);\n        svuint64_t vm1 = svadd_x_u64(vi, svdup_n_u64(1));\n        svuint64_t vm2 = svadd_x_u64(vi, svdup_n_u64(5));\n\n        svbool_t cond1 = svle_x_u64(vm1, vlimit);\n        svbool_t cond2 = svle_x_u64(vm2, vlimit);\n\n        svuint64_t vm1_mod_n = svmul_x_u64(vm1, svreinterpret_u64(svdup_n_s64(n)));\n        svuint64_t vm2_mod_n = svmul_x_u64(vm2, svreinterpret_u64(svdup_n_s64(n)));\n\n        svuint64_t remainder1 = svmod_x_u64(vm1_mod_n, svreinterpret_u64(svdup_n_s64(n)));\n        svuint64_t remainder2 = svmod_x_u64(vm2_mod_n, svreinterpret_u64(svdup_n_s64(n)));\n\n        svbool_t is_not_prime1 = sveor_z_b64(cond1, svceq_z_u64(remainder1, svdup_n_u64(1)));\n        svbool_t is_not_prime2 = sveor_z_b64(cond2, svceq_z_u64(remainder2, svdup_n_u64(1)));\n\n        if (svany_true(is_not_prime1) || svany_true(is_not_prime2)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = sqrt(n);\n    svint64_t vlimit = svdup_n_s64(limit);\n    svuint64_t vmask = svcreate_mask_64(limit / 64 + 1);\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        svint64_t vi = svdup_n_s64(i);\n        svint64_t vi2 = svadd_n_s64(vi, 2);\n        svint64_t vi4 = svadd_n_s64(vi, 4);\n\n        svbool_t cond1 = svtst_m_b64(vmask, svcmpgt_z_s64(svmul_n_s64(vi, n), vlimit));\n        svbool_t cond2 = svtst_m_b64(vmask, svcmpgt_z_s64(svmul_n_s64(vi2, n), vlimit));\n        svbool_t cond3 = svtst_m_b64(vmask, svcmpgt_z_s64(svmul_n_s64(vi4, n), vlimit));\n\n        if (svany_true(cond1)) return false;\n        if (svany_true(cond2)) return false;\n        if (svany_true(cond3)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string solve_simd(const std::string &s) {\n    size_t len = s.length();\n    if (len == 0) return s;\n\n    svbool_t p = svptrue_b8();\n    svuint8_t vec = svld1_u8(s.data());\n    svuint8_t mask = svcmple_u8(vec, svdup_u8('9'));\n    mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('0')));\n    mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('@')));\n    mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('[')));\n    mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('^')));\n    mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('`')));\n    mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('{')));\n    mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('~')));\n\n    svuint8_t result = svand_u8(vec, svnot_u8(mask));\n    result = svor_u8(result, svshl_n_u8(svand_u8(vec, mask), 32));\n\n    std::string res(len, '\\0');\n    svst1_u8(res.data(), result);\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.length();\n    svbool_t p = svptrue_b8();\n    svuint8_t vec = svld1_u8(s.data());\n    svuint8_t mask = svcmple_u8(vec, svdup_u8('9'));\n    svuint8_t result = vec;\n\n    while (svptest_any(p, mask)) {\n        svuint8_t lower_mask = svcmpgt_u8(vec, svdup_u8('Z'));\n        svuint8_t upper_mask = svcmplt_u8(vec, svdup_u8('a'));\n\n        svuint8_t lower_to_upper = svsub_u8(svadd_u8(vec, svdup_u8(32)), svand_u8(lower_mask, svdup_u8(32)));\n        svuint8_t upper_to_lower = svsub_u8(svadd_u8(vec, svdup_u8(-32)), svand_u8(upper_mask, svdup_u8(32)));\n\n        result = svsel_u8(lower_mask, lower_to_upper, svsel_u8(upper_mask, upper_to_lower, vec));\n\n        vec = svld1_u8(&s[len - 16]);\n        mask = svcmple_u8(vec, svdup_u8('9'));\n        len -= 16;\n    }\n\n    if (len > 0) {\n        svuint8_t remaining = svld1_u8(&s[len - 16]);\n        svuint8_t lower_mask = svcmpgt_u8(remaining, svdup_u8('Z'));\n        svuint8_t upper_mask = svcmplt_u8(remaining, svdup_u8('a'));\n\n        svuint8_t lower_to_upper = svsub_u8(svadd_u8(remaining, svdup_u8(32)), svand_u8(lower_mask, svdup_u8(32)));\n        svuint8_t upper_to_lower = svsub_u8(svadd_u8(remaining, svdup_u8(-32)), svand_u8(upper_mask, svdup_u8(32)));\n\n        result = svsel_u8(lower_mask, lower_to_upper, svsel_u8(upper_mask, upper_to_lower, remaining));\n    }\n\n    char *output = new char[s.length() + 1];\n    svst1_u8(output, result);\n    output[s.length()] = '\\0';\n\n    std::string res(output);\n    delete[] output;\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string solve_simd(const std::string &s) {\n    size_t len = s.length();\n    if (len == 0) return s;\n\n    svbool_t p = svptrue_b8();\n    svuint8_t vec_s = svld1_u8(s.data());\n    svuint8_t result[len];\n\n    for (size_t i = 0; i < len; i += 16) {\n        svuint8_t mask = svcmple_u8_z(p, svdup_u8('z'), svdup_u8('a'));\n        svuint8_t upper = svclru_u8(mask, svdup_u8('A'), svsub_u8(svadd_u8(vec_s, svdup_u8(32)), svdup_u8('a')));\n        svuint8_t lower = svclrl_u8(mask, svdup_u8('a'), svsub_u8(svadd_u8(vec_s, svdup_u8(-32)), svdup_u8('A')));\n        svuint8_t new_vec_s = svsel_u8(mask, upper, lower);\n        svst1_u8(result + i, new_vec_s);\n        vec_s = svld1_u8(s.data() + i + 16);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 16; i < len; ++i) {\n        if ((result[i] >= 'a' && result[i] <= 'z') || (result[i] >= 'A' && result[i] <= 'Z')) {\n            result[i] ^= 0x20;\n        }\n    }\n\n    return std::string(reinterpret_cast<char*>(result), len);\n}\n```"}
{"task_id": "SimdBench_77_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return s;\n\n    svbool_t mask = svptrue_b8();\n    svuint8_t vec = svld1_u8(s.data());\n    svuint8_t result = vec;\n\n    for (size_t i = 0; i < len; i += 16) {\n        svuint8_t vec = svld1_u8(s.data() + i);\n        svuint8_t mask = svcmpne_u8(vec, svdup_u8(0));\n        svuint8_t lower_mask = svand_u8(mask, svcmple_u8(vec, svdup_u8('z')));\n        svuint8_t upper_mask = svand_u8(mask, svcmpge_u8(vec, svdup_u8('A')));\n\n        svuint8_t lower_to_upper = svsub_u8(svadd_u8(vec, svdup_u8('A' - 'a')), svdup_u8('a'));\n        svuint8_t upper_to_lower = svsub_u8(svadd_u8(vec, svdup_u8('a' - 'A')), svdup_u8('A'));\n\n        result = svsel_u8(lower_mask, lower_to_upper, svsel_u8(upper_mask, upper_to_lower, vec));\n\n        svst1_u8(result.svdata(), result);\n    }\n\n    return std::string(reinterpret_cast<char*>(result.svdata()), len);\n}\n```"}
{"task_id": "SimdBench_77_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string solve_simd(const std::string &s) {\n    size_t len = s.length();\n    if (len == 0) return s;\n\n    svbool_t p = svptrue_b8();\n    svuint8_t vec = svld1_u8(s.data());\n    svuint8_t result[len];\n\n    for (size_t i = 0; i < len; i += 16) {\n        svuint8_t mask = svcmple_u8(vec, svdup_u8('9'));\n        mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('0')));\n        mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('Z')));\n        mask = svbic_u8(mask, svcmple_u8(vec, svdup_u8('a')));\n\n        svuint8_t upper = svadd_u8(svsub_u8(vec, svdup_u8('a')), svdup_u8('A'));\n        svuint8_t lower = svadd_u8(svsub_u8(vec, svdup_u8('A')), svdup_u8('a'));\n\n        svuint8_t new_vec = svsel(mask, upper, lower);\n        svst1_u8(result + i, new_vec);\n\n        vec = svld1_u8(s.data() + i + 16);\n    }\n\n    std::string res(result, len);\n    std::reverse(res.begin(), res.end());\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_78_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    svint32_t v = svld1_lane(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), 0);\n    svbool_t mask = svcmple_z_s32(v, svdup_n_s32(3));\n    svint32_t sorted = svsort_s32(mask, v);\n    svst1_lane(reinterpret_cast<int32_t*>(l.data()), sorted, 0);\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    svint32_t v = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), l.size());\n    svbool_t mask = svcmple_z_s32(svptrue_b32(), svdup_imm_s32(0), svmodu_z_s32(svptrue_b32(), svindex_u32(l.size()), svdup_imm_u32(3)));\n    svint32_t sorted = svsort_s32(mask, v);\n    svst1(svptrue_b32(), reinterpret_cast<int32_t*>(l.data()), sorted);\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    svint32_t *input = reinterpret_cast<svint32_t*>(l.data());\n    size_t n = l.size();\n    size_t num_vectors = (n + 7) / 8;\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        svint32_t vec = svld1_lane_z(svptrue_b32(), input + i * 8, 0);\n        svbool_t mask = svcmple_z(svptrue_b32(), svdup_n_u32(3), svmodu_z(svptrue_b32(), svindex_u32(0, 8), svdup_n_u32(3)));\n        svint32_t sorted_vec = svsortb(mask, vec);\n        svst1_lane_z(svptrue_b32(), input + i * 8, sorted_vec, 0);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    svint32_t v = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), l.size());\n    svbool_t mask = svcmple_z_s32(svptrue_b32(), svdup_n_s32(0), svmodu_z_s32(svptrue_b32(), svindex_u32(l.size()), svdup_n_u32(3)));\n    svint32_t sorted = svsort_s32(mask, v);\n    svst1(svptrue_b32(), reinterpret_cast<int32_t*>(l.data()), sorted);\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    svint32_t *input = reinterpret_cast<svint32_t*>(l.data());\n    size_t n = l.size();\n    size_t num_elements = n / 3;\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        svint32_t vec = svld1_lane_z(svptrue_b32(), input + i * 3, 0);\n        svint32_t sorted_vec = svsort32(vec);\n        svst1_lane_z(svptrue_b32(), input + i * 3, sorted_vec, 0);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    svfloat32_t vec = svld1_f32(l.data());\n    int64_t size = l.size();\n    while (size > 8) {\n        svfloat32_t vec2 = svld1_f32(l.data() + size - 8);\n        vec = svmaxnmv_f32(vec, vec2);\n        size -= 8;\n    }\n    float result[8];\n    svst1_f32(result, vec);\n    float max_val = result[0];\n    for (int i = 1; i < 8; ++i) {\n        if (result[i] > max_val) {\n            max_val = result[i];\n        }\n    }\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    svfloat32_t vec = svld1_f32(l.data());\n    svbool_t mask = svcmple_f32(vec, svdup_n_f32(-FLT_MAX));\n    float32_t max_val = svmaxnmvq_f32(mask, vec);\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    svfloat32_t vec = svld1_f32(l.data());\n    uint64_t num_elements = l.size();\n    while (num_elements > 1) {\n        svuint32_t mask = svcmpgt_m_u32(svindex_u32(num_elements), svdup_n_u32(1));\n        vec = svmaxnm_f32(vec, svsel(mask, vec, svld1_f32(&l[num_elements / 2])));\n        num_elements = (num_elements + 1) / 2;\n    }\n    return svget_f32(vec, 0);\n}\n```"}
{"task_id": "SimdBench_79_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    svfloat32_t vec = svld1_f32(l.data());\n    svbool_t mask = svcmpgt_b32(vec, svdup_n_f32(-FLT_MAX));\n    float32_t max_val = svmaxnmvq_f32(mask, vec);\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    svfloat32_t vec = svld1_f32(l.data());\n    svbool_t mask = svcmpgt_f32(vec, svdup_n_f32(-FLT_MAX));\n    float32_t max_val = svmaxnmvq_f32(mask, vec);\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_SVE", "completion": "```c\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v_n = svdup_u32(n);\n    svuint32_t v_i = svdup_u32(0);\n\n    while (!svallzero_b32(p)) {\n        svuint32_t v_div_11 = svdiv_u32(v_i, svconst_u32(11));\n        svuint32_t v_div_13 = svdiv_u32(v_i, svconst_u32(13));\n        svuint32_t v_mod_11 = svmod_u32(v_i, svconst_u32(11));\n        svuint32_t v_mod_13 = svmod_u32(v_i, svconst_u32(13));\n\n        svbool_t v_is_div_11 = sveor_z_u32(p, svceqz_u32(v_mod_11, svconst_u32(0)));\n        svbool_t v_is_div_13 = sveor_z_u32(p, svceqz_u32(v_mod_13, svconst_u32(0)));\n\n        svbool_t v_is_fizz_buzz = sveor_z_u32(p, svand_z_u32(v_is_div_11, v_is_div_13));\n\n        count += svsad_u32(svptrue_b32(), v_is_fizz_buzz, svdup_u32(0));\n\n        v_i = svadd_u32(v_i, svconst_u32(1));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SVE", "completion": "```c\n#include <arm_sve.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v_n = svdup_u32(n);\n    svuint32_t v_i = svdup_u32(0);\n\n    while (!svallzero_b32(p)) {\n        svuint32_t v_div_11 = svdiv_u32(v_i, svdup_u32(11));\n        svuint32_t v_div_13 = svdiv_u32(v_i, svdup_u32(13));\n        svuint32_t v_mod_11 = svmod_u32(v_i, svdup_u32(11));\n        svuint32_t v_mod_13 = svmod_u32(v_i, svdup_u32(13));\n\n        svbool_t v_is_fizz = svcmple_u32(v_mod_11, svdup_u32(0));\n        svbool_t v_is_buzz = svcmple_u32(v_mod_13, svdup_u32(0));\n        svbool_t v_is_fizzbuzz = svand_b32(v_is_fizz, v_is_buzz);\n\n        svuint32_t v_count_fizz = svreinterpret_u32(svpcount_m_b32(v_is_fizz, svone_u32()));\n        svuint32_t v_count_buzz = svreinterpret_u32(svpcount_m_b32(v_is_buzz, svone_u32()));\n        svuint32_t v_count_fizzbuzz = svreinterpret_u32(svpcount_m_b32(v_is_fizzbuzz, svone_u32()));\n\n        count += svgetlane_u32(v_count_fizz, 0);\n        count += svgetlane_u32(v_count_buzz, 0);\n        count -= svgetlane_u32(v_count_fizzbuzz, 0);\n\n        v_i = svadd_u32(v_i, svdup_u32(1));\n        p = svcmpgt_u32(v_i, v_n);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SVE", "completion": "```c\n#include <arm_sve.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v_n = svdup_u32(n);\n    svuint32_t v_i = svdup_u32(0);\n\n    while (!svcmpeq_z(p, v_i, v_n)) {\n        svuint32_t v_div_11 = svudiv_u32(v_i, svdup_u32(11));\n        svuint32_t v_div_13 = svudiv_u32(v_i, svdup_u32(13));\n        svuint32_t v_mod_11 = svurem_u32(v_i, svdup_u32(11));\n        svuint32_t v_mod_13 = svurem_u32(v_i, svdup_u32(13));\n\n        svbool_t v_is_div_11 = svceq_z(p, v_mod_11, svdup_u32(0));\n        svbool_t v_is_div_13 = svceq_z(p, v_mod_13, svdup_u32(0));\n        svbool_t v_is_fizz_buzz = svor(p, v_is_div_11, v_is_div_13);\n\n        svuint32_t v_digit_7 = svdup_u32('7');\n        svuint32_t v_count_7 = svcnt_u32(v_is_fizz_buzz & svceq_z(p, v_i, svdup_u32('7')));\n        count += svgetlane_u32(v_count_7, 0);\n\n        v_i = svadd_u32(v_i, svdup_u32(1));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SVE", "completion": "```c\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v_n = svdup_u32(n);\n    svuint32_t v_i = svdup_u32(0);\n    svuint32_t v_7 = svdup_u32('7');\n    svuint32_t v_11 = svdup_u32(11);\n    svuint32_t v_13 = svdup_u32(13);\n\n    while (svanyTrue_b32(p)) {\n        svuint32_t v_div_11 = svdiv_u32(v_i, v_11);\n        svuint32_t v_div_13 = svdiv_u32(v_i, v_13);\n        svuint32_t v_mod_11 = svmod_u32(v_i, v_11);\n        svuint32_t v_mod_13 = svmod_u32(v_i, v_13);\n        svuint32_t v_or = svor_u32(v_mod_11, v_mod_13);\n        svuint32_t v_eq_0 = sveqz_u32(v_or);\n        svuint32_t v_and = svand_u32(v_eq_0, svcmpgt_u32(v_i, v_n));\n        svuint32_t v_not = svnot_u32(v_and);\n        p = svand_b32(p, v_not);\n        svuint32_t v_count = svadd_u32(svpopcntb_u32(sveor_u32(svclrsb_u32(v_i), v_7)), v_count);\n        v_i = svadd_u32(v_i, svdup_u32(1));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SVE", "completion": "```c\n#include <arm_sve.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v_n = svdup_u32(n);\n    svuint32_t v_i = svdup_u32(0);\n\n    while (svany_true_b32(p)) {\n        svuint32_t v_div_11 = svdiv_u32(v_i, svdup_u32(11));\n        svuint32_t v_div_13 = svdiv_u32(v_i, svdup_u32(13));\n        svuint32_t v_mod_11 = svmod_u32(v_i, svdup_u32(11));\n        svuint32_t v_mod_13 = svmod_u32(v_i, svdup_u32(13));\n\n        svbool_t v_is_div_11 = svcmpeq_u32(v_mod_11, svdup_u32(0));\n        svbool_t v_is_div_13 = svcmpeq_u32(v_mod_13, svdup_u32(0));\n        svbool_t v_is_fizz_buzz = svor_b32(v_is_div_11, v_is_div_13);\n\n        svuint32_t v_count = svadd_u32(svreinterpret_u32_z(p), svreinterpret_u32_z(v_is_fizz_buzz));\n        count += svsum_u32(v_count);\n\n        v_i = svadd_u32(v_i, svdup_u32(1));\n        p = svlt_u32(v_i, v_n);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svbool_t ptrue = svptrue_b32();\n\n    for (size_t i = 0; i < n; i += 8) {\n        size_t count = std::min<size_t>(n - i, 8);\n        svfloat32_t vl = svld1_u32(l.data() + i);\n\n        // Extract even elements\n        svuint32x4_t veven_indices = svuzpq_u32(svindex_u32(count, 2));\n        svfloat32_t veven = svextw_u32(vl, vzero, veven_indices);\n\n        // Sort even elements\n        svfloat32_t vsorted = sveclrsort_f32(veven, ptrue);\n\n        // Store sorted even elements back into result\n        svst1_u32(result.data() + i, vsorted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    svfloat32_t *vec_l = reinterpret_cast<svfloat32_t*>(l.data());\n    svbool_t mask = svindex_b32(n / 2);\n    svfloat32_t evens = svld1(mask, vec_l);\n    svsort32(evens);\n    svst1(mask, evens, vec_l);\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(const std::vector<float>& l) {\n    size_t n = l.size();\n    std::vector<float> result(n);\n\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    svbool_t ptrue = svptrue_b32();\n\n    for (size_t i = 0; i < n; i += 8) {\n        size_t count = std::min<size_t>(n - i, 8);\n        svfloat32_t vl = svld1(count, &l[i]);\n\n        // Extract even elements\n        svfloat32_t veven = svext_x_u32(vl, vzero, 1);\n\n        // Sort even elements\n        svfloat32_t vsorted = sveclrsort_f32(ptrue, veven);\n\n        // Store sorted even elements back into result\n        svst1(count, &result[i], vsorted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    svfloat32_t v = svld1_f32(l.data());\n    svbool_t mask = svcreate_mask_e32(0xAAAAAAAA);\n    svfloat32_t evens = svsel(mask, v, svdup_n_f32(0));\n    svqsort_f32(evens);\n    svst1_f32(l.data(), svsel(mask, evens, v));\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    svfloat32_t v = svld1_f32(l.data());\n    svuint32_t mask = svdup_u32(0xAAAAAAAA);\n    svbool_t is_even = svand_b32(mask, svindex_u32(svget_lanes(v), 0));\n    svfloat32_t evens = svsel(is_even, v, svdup_n_f32(0.0f));\n    svsort32(evens);\n    svst1_f32(l.data(), svsel(is_even, evens, v));\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t vec = svld1(svpattern_b(0x3F), reinterpret_cast<const int32_t*>(l.data()));\n    svint32_t sum = svdup_i32(0);\n    bool result = false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        svint32_t v = svdup_i32(l[i]);\n        for (size_t j = i + 1; j < l.size(); ++j) {\n            svint32_t w = svdup_i32(l[j]);\n            svint32_t x = svadd(v, w);\n            for (size_t k = j + 1; k < l.size(); ++k) {\n                svint32_t y = svdup_i32(l[k]);\n                svint32_t z = svadd(x, y);\n                if (svcmpeq(z, sum)) {\n                    result = true;\n                    break;\n                }\n            }\n            if (result) break;\n        }\n        if (result) break;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t v = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), l.size());\n    svint32_t sum = svdup_n_s32(0);\n    bool result = false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        for (size_t j = i + 1; j < l.size(); ++j) {\n            for (size_t k = j + 1; k < l.size(); ++k) {\n                svint32_t v_i = svdup_n_s32(l[i]);\n                svint32_t v_j = svdup_n_s32(l[j]);\n                svint32_t v_k = svdup_n_s32(l[k]);\n                svint32_t v_sum = svadd3(v_i, v_j, v_k);\n                result |= svcmpeq_z_s32(svadd(sum, v_sum), svdup_n_s32(0));\n                if (result) return true;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t v = svld1(svpattern_b(0x3F), reinterpret_cast<const int32_t*>(l.data()));\n    svint32_t sum = svdup_i32(0);\n    bool result = false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        svint32_t vi = svdup_i32(l[i]);\n        svint32_t diff = svsub_z(svpattern_b(0x3F), v, vi);\n        svint32_t mask = svtst(diff, v);\n        result |= svany(mask);\n        v = svadd_z(svpattern_b(0x3F), v, vi);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t v = svld1(svpattern_b(0x3F), reinterpret_cast<const int32_t*>(l.data()));\n    int n = l.size();\n    for (int i = 0; i < n - 2; ++i) {\n        svint32_t vi = svdup_n_s32(l[i]);\n        for (int j = i + 1; j < n - 1; ++j) {\n            svint32_t vj = svdup_n_s32(l[j]);\n            svint32_t vk = svsub_z_s32(v, svadd_z_s32(vi, vj));\n            for (int k = j + 1; k < n; ++k) {\n                if (svcmpeq(svduw_z_s32(vk, svdup_n_u32(l[k])), svuw_dup_n_u32(0))) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool triples_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t v = svld1(svpattern_b(0x55), reinterpret_cast<const int32_t*>(l.data()));\n    svint32_t sum = svdup_i32(0);\n    bool result = false;\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        svint32_t vi = svdup_i32(l[i]);\n        svint32_t diff = svsub_z(svpattern_b(0xFF), vi, sum);\n        svbool_t match = svcmpeq_z(svpattern_b(0xFF), diff, v);\n        if (svfirst_m(svpattern_b(0xFF), match)) {\n            result = true;\n            break;\n        }\n        sum = svadd(sum, vi);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    svint32_t *in = reinterpret_cast<svint32_t*>(l.data());\n    svint32_t *out = reinterpret_cast<svint32_t*>(result.data());\n\n    for (size_t i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vec_in = svld1_u32(in + i);\n        svint32_t vec_out = svadd_i32(vec_in, svdup_n_s32(1));\n        svst1_u32(out + i, vec_out);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n\n    svint32_t *in = reinterpret_cast<svint32_t*>(l.data());\n    svint32_t *out = reinterpret_cast<svint32_t*>(result.data());\n\n    for (int i = 0; i < n; i += svcntw()) {\n        svint32_t vec_in = svld1(svptrue_b32(), in + i);\n        svint32_t vec_out = svadd_z(svptrue_b32(), vec_in, svdup_n_s32(1));\n        svst1(svptrue_b32(), out + i, vec_out);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    svint32_t *in = reinterpret_cast<svint32_t*>(l.data());\n    svint32_t *out = reinterpret_cast<svint32_t*>(result.data());\n\n    for (size_t i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vec_in = svld1_u32(in + i);\n        svint32_t vec_out = svadd_i32(vec_in, svdup_n_s32(1));\n        svst1_u32(out + i, vec_out);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    std::vector<int> result(n);\n\n    svint32_t *in = reinterpret_cast<svint32_t*>(l.data());\n    svint32_t *out = reinterpret_cast<svint32_t*>(result.data());\n\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        svint32_t vec_in = svld1_u32(in + i);\n        svint32_t vec_out = svadd_i32(vec_in, svdup_n_s32(1));\n        svst1_u32(out + i, vec_out);\n    }\n\n    for (; i < n; ++i) {\n        out[i] = in[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result(n);\n    svint32_t ones = svdup_n_s32(1);\n\n    for (int i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vec = svld1_u32(l.data() + i);\n        svst1_u32(result.data() + i, svadd_s32(vec, ones));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t vec = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), l.size());\n    svint32_t neg_vec = svneg_z(svptrue_b32(), vec);\n    svbool_t mask = svor_u32(svptrue_b32(), svfind_first(svcmpeq_z(svptrue_b32(), vec, neg_vec)), svone_b32());\n    return svptest_any(mask);\n}\n```"}
{"task_id": "SimdBench_84_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t vec = svld1(svpattern_b(0x55), reinterpret_cast<const int32_t*>(l.data()));\n    svint32_t mask = svcmple_z_s32(vec, svdup_n_s32(0));\n    svuint32_t idx = svfind_first(mask);\n    while (idx != SV_ALL_ONES_U32) {\n        svint32_t neg_vec = svneg_s32(svindex_s32(vec, idx));\n        svint32_t found = svany_true_z_s32(sveor_z_s32(mask, svcmpne_z_s32(vec, neg_vec)));\n        if (found) {\n            return true;\n        }\n        mask = svsetbit_u32(mask, idx, 0);\n        idx = svfind_first(mask);\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t vec = svld1(svpattern_b(0x55), reinterpret_cast<const int32_t*>(l.data()));\n    svint32_t mask = svcmple_z_i32(vec, svdup_n_s32(0));\n    svuint32_t indices = svreinterpret_u32(mask);\n    svuint32_t count = svcntp_u32(indices);\n    return count > 0;\n}\n```"}
{"task_id": "SimdBench_84_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t vec = svld1(svpattern_b(0x55), reinterpret_cast<const int32_t*>(l.data()));\n    svint32_t mask = svcmple_z(svabs(vec), svdup_i32(0));\n    svuint32_t cnt = svcntp_u(mask);\n    return cnt > 0;\n}\n```"}
{"task_id": "SimdBench_84_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    svint32_t vec = svld1(svpattern_b(0x55), reinterpret_cast<const int32_t*>(l.data()));\n    svint32_t mask = svcmple_z_s32(vec, svdup_n_s32(0));\n    svuint32_t idx = svfind_first(mask);\n    while (idx != SV_ALL_ONES_U32) {\n        svint32_t neg_vec = svneg_s32(svindex_s32(vec, idx));\n        svint32_t found = svinnelem_s32(neg_vec, vec);\n        if (found != 0) {\n            return true;\n        }\n        mask = svcmple_z_s32(vec, svdup_n_s32(0));\n        idx = svfind_first(mask);\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    svuint8_t digits = svdup_u8('0');\n    svbool_t mask = svptrue_b8();\n    svint64_t remainder = svset_i64(x);\n    svint64_t quotient = svzero_i64();\n\n    while (!svallzero_b8(mask)) {\n        svint64_t div_result = svdiv_s64(remainder, svset_i64(base));\n        svint64_t mod_result = svmod_s64(remainder, svset_i64(base));\n\n        svuint8_t digit = svreinterpret_u8(svadd_s64(mod_result, svreinterpret_s64(digits)));\n        svst1_u8(digit.ptrue(), digit, &digits);\n\n        remainder = svreinterpret_s64(div_result);\n    }\n\n    std::string result(reinterpret_cast<char*>(digits.ptrue()), digits.length());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    svuint8_t digits = svdup_u8('0');\n    svbool_t mask = svptrue_b8();\n    svint64_t remainder = svset_i64(x);\n    svint64_t quotient = svzero_i64();\n\n    while (!svallzero_b8(mask)) {\n        quotient = svdiv_i64(remainder, svset_i64(base));\n        remainder = svrem_i64(remainder, svset_i64(base));\n        svuint8_t new_digit = svreinterpret_u8(svsub_i64(remainder, svcmov_i64(sveor_z_b8(mask, svcmpgt_i64(quotient, svzero_i64())), svset_i64(0), svset_i64(1))));\n        digits = svsel_b8(mask, new_digit, digits);\n        mask = svandnot_b8(mask, svcmpeq_i64(quotient, svzero_i64()));\n    }\n\n    char result[20];\n    svstrmva(result, digits);\n    return std::string(result);\n}\n```"}
{"task_id": "SimdBench_85_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    svint8_t digits = svdup_n_b8('0');\n    svuint8_t remainder;\n    svbool_t mask;\n\n    while (x != 0) {\n        remainder = svreinterpret_u8_mins(svdiv_n_s64(x, base));\n        x = svreinterpret_s64_mins(svmul_n_s64(x, base));\n        mask = sveor_z_b8(mask, svcmpgt_z_b8(remainder, svdup_n_b8('9')));\n        digits = svadd_m_b8(digits, mask, svsub_n_b8(remainder, '0'), svreinterpret_b8_u8(mask));\n    }\n\n    char result[20];\n    std::memcpy(result, reinterpret_cast<char*>(svreinterpret_u8_mins(digits)), 20);\n    return std::string(result);\n}\n```"}
{"task_id": "SimdBench_85_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    svuint8_t digits = svdup_u8('0');\n    svbool_t mask = svptrue_b8();\n    svint64_t remainder = svset_s64(x);\n    svint64_t quotient = svset_s64(0);\n\n    while (!svany_true_p(svcmpne_z(mask, svfalse_b8()))) {\n        svint64_t div_result = svdiv_s64(remainder, svset_s64(base));\n        svint64_t mod_result = svmod_s64(remainder, svset_s64(base));\n\n        svuint8_t digit = svreinterpret_u8_u64(mod_result);\n        svst1_u8(digits + svfirst_m(mask), digit);\n\n        remainder = svreinterpret_s64_u64(div_result);\n        quotient = svreinterpret_s64_u64(div_result);\n\n        mask = svandnot_b8(mask, svcmpeq_z(svreinterpret_u8_s64(quotient), svzero_u8()));\n    }\n\n    char result[20];\n    svstrmbyt_u8(result, digits, svcountb_m(mask));\n    return std::string(result);\n}\n```"}
{"task_id": "SimdBench_85_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    svint64_t num = svdup_n_s64(x);\n    svuint8_t digits = svdup_n_u8('0');\n    svbool_t mask = svtrue_b8();\n\n    while (!svallzero_z(mask)) {\n        svuint8_t remainder = svmodu(num, svdup_n_u64(base));\n        num = svsdiv(num, svdup_n_u64(base));\n        svst1(&digits[0], remainder);\n        mask = svcmpne_u64(num, svdup_n_u64(0));\n    }\n\n    std::string result((char*)&digits[0]);\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    svuint8_t* data = reinterpret_cast<svuint8_t*>(const_cast<char*>(text.c_str()));\n    size_t vec_size = svcntb();\n\n    for (size_t i = 0; i < len / vec_size; ++i) {\n        svuint8_t vec = svld1_u8(data + i * vec_size);\n        svuint8_t rev_vec = svrev64_u8(svrev32_u8(svrev16_u8(vec)));\n        if (!svcmpeq_z_u8(vec, rev_vec)) return false;\n    }\n\n    // Handle remaining elements\n    for (size_t i = len - len % vec_size; i < len; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    svuint8_t* data = reinterpret_cast<svuint8_t*>(const_cast<char*>(text.c_str()));\n    size_t num_elements = len / sizeof(svuint8_t);\n\n    for (size_t i = 0; i < num_elements / 2; ++i) {\n        svuint8_t vec1 = svld1_u8(data + i);\n        svuint8_t vec2 = svld1_u8(data + len - i - 1);\n        svuint8_t cmp = sveor_u8(vec1, vec2);\n        if (!svptest_any_p(svptrue_b8(), cmp)) {\n            return false;\n        }\n    }\n\n    // Handle remaining elements if length is odd\n    if (len % sizeof(svuint8_t) != 0) {\n        size_t offset = num_elements * sizeof(svuint8_t);\n        char middle_char = text[offset];\n        if (middle_char != text[len - offset - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nbool is_palindrome_simd(const std::string& text) {\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    svuint8_t* data = reinterpret_cast<svuint8_t*>(const_cast<char*>(text.c_str()));\n    size_t vec_size = svcntb();\n\n    for (size_t i = 0; i < len / vec_size; ++i) {\n        svuint8_t vec1 = svld1_u8(data + i * vec_size);\n        svuint8_t vec2 = svld1_u8(data + len - (i + 1) * vec_size);\n        svuint8_t mask = sveor_u8(vec1, svrev64_u8(svrev32_u8(vec2)));\n        if (!svptest_any_p(svptrue_b8(), svcmpeq_z_u8(mask, svdup_u8(0)))) {\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = len - len % vec_size; i < len; ++i) {\n        if (text[i] != text[len - 1 - i]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nbool is_palindrome_simd(const std::string& text) {\n    svint32_t v0 = svdup_i32(0);\n    svint32_t v1 = svdup_i32(0);\n    svint32_t v2 = svdup_i32(0);\n    svint32_t v3 = svdup_i32(0);\n\n    int len = text.length();\n    int half_len = len / 2;\n\n    for (int i = 0; i < half_len; ++i) {\n        svuint8_t v4 = svld1_u8(text.data() + i);\n        svuint8_t v5 = svld1_u8(text.data() + len - i - 1);\n\n        svuint8_t v6 = svcmpgt_b8(v4, v5);\n        svuint32_t v7 = svreinterpret_u32_u8(v6);\n\n        v0 = svaddlv_u32(v0, v7);\n    }\n\n    return svget_lane_u32(v0, 0) == 0;\n}\n```"}
{"task_id": "SimdBench_86_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cstring>\n\nbool is_palindrome_simd(const std::string& text) {\n    svuint8_t* data = reinterpret_cast<svuint8_t*>(const_cast<char*>(text.c_str()));\n    size_t len = text.length();\n    size_t half_len = len / 2;\n\n    for (size_t i = 0; i < half_len; ++i) {\n        svuint8_t vec1 = svld1_u8(data + i);\n        svuint8_t vec2 = svld1_u8(data + len - i - 1);\n\n        svuint8_t cmp = sveor_u8(vec1, vec2);\n        if (!svptest_any_p(svptrue_b8(), cmp)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::string decode_shift_simd(std::string s) {\n    svuint8_t *input = reinterpret_cast<svuint8_t*>(s.data());\n    svuint8_t *output = reinterpret_cast<svuint8_t*>(new char[s.size()]);\n    size_t n = s.size();\n    \n    for (size_t i = 0; i < n; i += SV_INT8_SIZE) {\n        svuint8_t vec = svld1_u8(input + i);\n        svuint8_t shifted = svsub_u8(svadd_u8(vec, svdup_u8(5)), svdup_u8('a'));\n        shifted = svmod_u8(shifted, svdup_u8(26));\n        shifted = svadd_u8(shifted, svdup_u8('a'));\n        svst1_u8(output + i, shifted);\n    }\n\n    std::string result(reinterpret_cast<char*>(output), s.size());\n    delete[] output;\n    return result;\n}\n```"}
{"task_id": "SimdBench_87_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::string decode_shift_simd(std::string s) {\n    svuint8_t *input = reinterpret_cast<svuint8_t*>(const_cast<char*>(s.c_str()));\n    size_t len = s.length();\n    size_t num_elements = len / 16;\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        svuint8_t vec = svld1_u8(input + i * 16);\n        svuint8_t shifted = svsub_u8(svadd_u8(vec, svdup_u8(5)), svdup_u8('a'));\n        shifted = svmod_u8(shifted, svdup_u8(26));\n        shifted = svadd_u8(shifted, svdup_u8('a'));\n        svst1_u8(input + i * 16, shifted);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_elements * 16; i < len; ++i) {\n        char c = s[i];\n        c = ((c - 'a' - 5 + 26) % 26) + 'a';\n        s[i] = c;\n    }\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_87_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::string decode_shift_simd(std::string s) {\n    svuint8_t *in = reinterpret_cast<svuint8_t*>(s.data());\n    svuint8_t *out = reinterpret_cast<svuint8_t*>(new char[s.size()]);\n    size_t n = s.size();\n    \n    for (size_t i = 0; i < n; i += SV_INT8_SIZE) {\n        svuint8_t vec_in = svld1_u8(in + i);\n        svuint8_t vec_out = svsub_u8(svadd_u8(vec_in, svdup_u8(5)), svdup_u8('a'));\n        vec_out = svmod_u8(svadd_u8(vec_out, svdup_u8(26)), svdup_u8(26));\n        vec_out = svadd_u8(vec_out, svdup_u8('a'));\n        svst1_u8(out + i, vec_out);\n    }\n\n    std::string result(reinterpret_cast<char*>(out), n);\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_87_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::string decode_shift_simd(std::string s) {\n    svuint8_t *input = reinterpret_cast<svuint8_t*>(s.data());\n    size_t len = s.length();\n    size_t num_elements = len / 16;\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        svuint8_t vec = svld1_u8(input + i * 16);\n        svuint8_t shifted = svsub_u8(svadd_u8(vec, svdup_u8(5)), svdup_u8('a'));\n        shifted = svmod_u8(shifted, svdup_u8(26));\n        shifted = svadd_u8(shifted, svdup_u8('a'));\n        svst1_u8(input + i * 16, shifted);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_elements * 16; i < len; ++i) {\n        uint8_t w = ((uint8_t)s[i] - 5 - 'a' + 26) % 26 + 'a';\n        s[i] = (char)w;\n    }\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_87_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::string decode_shift_simd(std::string s) {\n    size_t len = s.size();\n    svuint8_t *in = reinterpret_cast<svuint8_t *>(const_cast<char *>(s.c_str()));\n    svuint8_t *out = reinterpret_cast<svuint8_t *>(new char[len]);\n\n    svbool_t p = svptrue_b8();\n\n    for (size_t i = 0; i < len; i += SV_VL) {\n        svuint8_t vec_in = svld1_u8(p, &in[i]);\n        svuint8_t vec_out = svsub_u8(vec_in, svdup_u8(5));\n        svuint8_t vec_mask = svcmpgt_u8(vec_out, svdup_u8('z'));\n        vec_out = svselect(vec_mask, svadd_u8(vec_out, svdup_u8(26)), vec_out);\n        svst1_u8(p, &out[i], vec_out);\n    }\n\n    std::string result(reinterpret_cast<char *>(out), len);\n    delete[] out;\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    svint32_t vec_l = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), l.size());\n    svint32_t vec_t = svdup_n_s32(t);\n    svbool_t mask = svtstle(vec_l, vec_t);\n    return sve_all(mask);\n}\n```"}
{"task_id": "SimdBench_88_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    svint32_t vthreshold = svdup_n_s32(t);\n    svbool_t vall_below = svcntp_m_b32(svle_s32_z(svptrue_b32(), svld1_s32(l.data()), vthreshold));\n    return svptest_any(vall_below);\n}\n```"}
{"task_id": "SimdBench_88_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    svint32_t vec_l = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), l.size());\n    svint32_t vec_t = svdup_n_s32(t);\n    svbool_t mask = svtstle(vec_l, vec_t);\n    return svall(mask);\n}\n```"}
{"task_id": "SimdBench_88_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    svint32_t vec_l = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(l.data()), l.size());\n    svint32_t vec_t = svdup_n_s32(t);\n    svbool_t mask = svtstle(vec_l, vec_t);\n    return svall(mask);\n}\n```"}
{"task_id": "SimdBench_88_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    svint32_t vec_l = svld1(svptrue_b32(), l.data());\n    svint32_t vec_t = svdup_n_s32(t);\n    svbool_t mask = svlt_z_s32(vec_l, vec_t);\n    return svall(mask);\n}\n```"}
{"task_id": "SimdBench_89_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (size_t i = 0; i < brackets.size(); i += 16) {\n        size_t count = std::min(brackets.size() - i, 16u);\n        svuint8_t chars = svld1_u8(reinterpret_cast<const uint8_t*>(&brackets[i]));\n\n        svuint8_t open_chars = svcmplt_u8(chars, svdup_u8('>'));\n\n        open_mask = svor_u8(open_mask, open_chars);\n\n        svuint8_t close_chars = svcmpgt_u8(chars, svdup_u8('<'));\n\n        close_mask = svor_u8(close_mask, close_chars);\n    }\n\n    return svall_gt(svand_u8(open_mask, close_mask), svdup_u8(0));\n}\n```"}
{"task_id": "SimdBench_89_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (size_t i = 0; i < brackets.size(); i += 16) {\n        size_t count = std::min(brackets.size() - i, 16u);\n        svuint8_t chars = svld1_u8(&brackets[i]);\n\n        svuint8_t open_chars = svcmplt_z_u8(chars, svdup_u8('>'));\n\n        open_mask = svor_u8(open_mask, open_chars);\n\n        svuint8_t close_chars = svcmpgt_z_u8(chars, svdup_u8('<'));\n\n        close_mask = svor_u8(close_mask, close_chars);\n    }\n\n    return svall_gt(svand_u8(open_mask, close_mask), svdup_u8(0));\n}\n```"}
{"task_id": "SimdBench_89_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (size_t i = 0; i < brackets.size(); i += 16) {\n        size_t count = std::min(brackets.size() - i, size_t(16));\n        svuint8_t chars = svld1_u8(reinterpret_cast<const uint8_t*>(&brackets[i]));\n\n        svuint8_t open_chars = svcmplt_u8(chars, svdup_u8('>'));\n\n        open_mask = svor_u8(open_mask, open_chars);\n\n        svuint8_t close_chars = svcmpeq_u8(chars, svdup_u8('>'));\n        close_mask = svor_u8(close_mask, close_chars);\n\n        svuint8_t balance = svsub_u8(open_mask, close_mask);\n        if (!svall_gt_z_u8(balance, svdup_u8(0))) {\n            return false;\n        }\n    }\n\n    return svall_gt_z_u8(svsub_u8(open_mask, close_mask), svdup_u8(0));\n}\n```"}
{"task_id": "SimdBench_89_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (size_t i = 0; i < brackets.size(); i += 16) {\n        size_t count = std::min(brackets.size() - i, 16u);\n        svuint8_t chars = svld1_u8(reinterpret_cast<const uint8_t*>(&brackets[i]));\n\n        svuint8_t open_chars = svcmple_u8(chars, svdup_u8('<'));\n        svuint8_t close_chars = svcmpgt_u8(chars, svdup_u8('>'));\n\n        open_mask = svor_u8(open_mask, open_chars);\n        close_mask = svor_u8(close_mask, close_chars);\n    }\n\n    return svpopcnt_u32(svreinterpret_u32_u8(open_mask)) == svpopcnt_u32(svreinterpret_u32_u8(close_mask));\n}\n```"}
{"task_id": "SimdBench_89_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (size_t i = 0; i < brackets.size(); i += 16) {\n        size_t count = std::min(brackets.size() - i, 16u);\n        svuint8_t chars = svld1_u8(reinterpret_cast<const uint8_t*>(&brackets[i]));\n\n        svuint8_t open_chars = svcmple_u8(chars, svdup_u8('<'));\n        svuint8_t close_chars = svcmpgt_u8(chars, svdup_u8('>'));\n\n        open_mask = svor_u8(open_mask, open_chars);\n        close_mask = svor_u8(close_mask, close_chars);\n    }\n\n    return svpopcnt_u32(svreinterpret_u32_u8(open_mask)) == svpopcnt_u32(svreinterpret_u32_u8(close_mask));\n}\n```"}
{"task_id": "SimdBench_90_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    svint32_t open_mask = svdup_n_s32(0);\n    svint32_t close_mask = svdup_n_s32(0);\n\n    for (int i = 0; i < n; i += 16) {\n        svuint8_t vec = svld1_u8(&brackets[i]);\n        svuint8_t open_vec = svcmplt_z_u8(vec, svdup_n_u8(')'));\n        svuint8_t close_vec = svcmpgt_z_u8(vec, svdup_n_u8('('));\n\n        open_mask = svor_b32(open_mask, svreinterpret_u32_u8(open_vec));\n        close_mask = svor_b32(close_mask, svreinterpret_u32_u8(close_vec));\n    }\n\n    return svany_true_b32(sveor_b32(open_mask, close_mask)) == false;\n}\n```"}
{"task_id": "SimdBench_90_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n % 2 != 0) return false;\n\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (int i = 0; i < n; i += 16) {\n        svuint8_t chars = svld1_u8(brackets.data() + i);\n        svuint8_t open_chars = svcmple_u8(chars, svdup_u8('('));\n        svuint8_t close_chars = svcmpgt_u8(chars, svdup_u8(')'));\n\n        open_mask = svor_u8(open_mask, open_chars);\n        close_mask = svor_u8(close_mask, close_chars);\n    }\n\n    svuint32_t open_count = svcntb_u32(svreinterpret_u32_u8(open_mask));\n    svuint32_t close_count = svcntb_u32(svreinterpret_u32_u8(close_mask));\n\n    return svcmpeq_z_u32(sveor_u32(open_count, close_count), svdup_u32(0));\n}\n```"}
{"task_id": "SimdBench_90_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (size_t i = 0; i < brackets.size(); i += 16) {\n        size_t count = std::min(brackets.size() - i, 16u);\n        svuint8_t chars = svld1_u8(reinterpret_cast<const uint8_t*>(&brackets[i]));\n\n        svuint8_t open_chars = svcmplt_u8(chars, svdup_u8(')'));\n        svuint8_t close_chars = svcmpgt_u8(chars, svdup_u8('('));\n\n        open_mask = svor_u8(open_mask, open_chars);\n        close_mask = svor_u8(close_mask, close_chars);\n    }\n\n    return svall_gt(svand_u8(open_mask, close_mask), svdup_u8(0));\n}\n```"}
{"task_id": "SimdBench_90_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int n = brackets.size();\n    if (n % 2 != 0) return false;\n\n    svuint8_t open_mask = svdup_u8(0x28); // ASCII value of '('\n    svuint8_t close_mask = svdup_u8(0x29); // ASCII value of ')'\n    svint32_t balance = svdup_n_s32(0);\n\n    for (int i = 0; i < n; i += 16) {\n        svuint8_t vec = svld1_u8(brackets.data() + i);\n        svuint8_t open_vec = svand_u8(vec, open_mask);\n        svuint8_t close_vec = svand_u8(vec, close_mask);\n\n        svint32_t open_count = svtbl_s32(svreinterpret_u32(open_vec), svdup_n_u32(1));\n        svint32_t close_count = svtbl_s32(svreinterpret_u32(close_vec), svdup_n_u32(1));\n\n        balance = svadd_s32(balance, svsub_s32(open_count, close_count));\n    }\n\n    return svget_lane_s32(balance, 0) == 0;\n}\n```"}
{"task_id": "SimdBench_90_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    svuint8_t open_mask = svdup_u8(0);\n    svuint8_t close_mask = svdup_u8(0);\n\n    for (size_t i = 0; i < brackets.size(); i += 16) {\n        size_t count = std::min(brackets.size() - i, 16u);\n        svuint8_t chars = svld1_u8(reinterpret_cast<const uint8_t*>(&brackets[i]));\n\n        svuint8_t open_chars = svcmplt_u8(chars, svdup_u8(')'));\n        svuint8_t close_chars = svcmpgt_u8(chars, svdup_u8('('));\n\n        open_mask = svor_u8(open_mask, open_chars);\n        close_mask = svor_u8(close_mask, close_chars);\n    }\n\n    return svall_gt(svand_u8(open_mask, close_mask), svdup_u8(0));\n}\n```"}
{"task_id": "SimdBench_91_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    svfloat32_t* xs_ptr = reinterpret_cast<svfloat32_t*>(const_cast<float*>(&xs[0]));\n    size_t num_elements = n / 4;\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        svfloat32_t coeffs = svld1q_f32(xs_ptr + i * 4);\n        svfloat32_t indices = svdup_n_f32(i * 4);\n        svfloat32_t derivatives = svmlaq_z_f32(svzero_f32(), coeffs, indices);\n        svst1q_f32(result.data() + i * 4, derivatives);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_elements * 4; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    svfloat32_t* xs_ptr = reinterpret_cast<svfloat32_t*>(const_cast<float*>(&xs[0]));\n    size_t num_elements = n / 4;\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        svfloat32_t coeffs = svld1q_f32(xs_ptr + i * 4);\n        svfloat32_t indices = svdup_n_f32(i * 4);\n        svfloat32_t derivatives = svmlaq_z_f32(svzero_f32(), coeffs, indices);\n        svst1q_f32(result.data() + i * 4, derivatives);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_elements * 4; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    svfloat32_t* xs_ptr = reinterpret_cast<svfloat32_t*>(const_cast<float*>(&xs[0]));\n    size_t num_elements = n / 4;\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        svfloat32_t coeffs = svld1q_f32(xs_ptr + i * 4);\n        svfloat32_t indices = svdup_n_f32(i + 1);\n        svfloat32_t derivatives = svmulq(coeffs, indices);\n        svst1q_f32(result.data() + i * 4, derivatives);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_elements * 4; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    svfloat32_t* xs_ptr = reinterpret_cast<svfloat32_t*>(const_cast<float*>(&xs[0]));\n    size_t num_elements = n / 4;\n    std::vector<float> result(n - 1);\n\n    for (size_t i = 0; i < num_elements; ++i) {\n        svfloat32_t coeffs = svld1q_f32(xs_ptr + i * 4);\n        svfloat32_t indices = svdup_n_f32(i * 4);\n        svfloat32_t derivatives = svmlaq_z_f32(svzero_f32(), coeffs, indices);\n        svst1q_f32(result.data() + i * 4, derivatives);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_elements * 4; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(const std::vector<float>& xs) {\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    svfloat32_t* xs_ptr = reinterpret_cast<svfloat32_t*>(const_cast<float*>(&xs[0]));\n    svint32_t indices = svdup_n_s32(1);\n    svfloat32_t result[n - 1];\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        svfloat32_t coeff = xs_ptr[i];\n        svfloat32_t index = svreinterpret_f32_u32(svadd_s32(indices, svdup_n_s32(i)));\n        result[i] = svmul_f32(coeff, index);\n    }\n\n    return std::vector<float>(result, result + n - 1);\n}\n```"}
{"task_id": "SimdBench_92_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    svuint8_t vowels = svdup_u8('aeiou');\n    svuint8_t y = svdup_u8('y');\n\n    for (size_t i = 0; i < s.size(); i += 16) {\n        size_t remaining = std::min(s.size() - i, 16U);\n        svuint8_t chars = svld1_u8(&s[i]);\n        svuint8_t lower_chars = svreinterpret_u8(svclz_u8(chars));\n        svuint8_t match = svcmple_u8(lower_chars, vowels);\n        if (remaining == 16) {\n            match = svand_b8(match, svcmpne_u8(lower_chars, y));\n        } else {\n            svuint8_t last_char = svld1_lane_u8(&s[i + remaining - 1], svdup_u8(0), remaining - 1);\n            last_char = svreinterpret_u8(svclz_u8(last_char));\n            match = svand_b8(match, svcmpne_u8(last_char, y));\n        }\n        count += svcntb(mask & match);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    svuint8_t vowels = svdup_u8('a');\n    svuint8_t y = svdup_u8('y');\n\n    for (size_t i = 0; i < s.size(); i += 16) {\n        size_t len = std::min(size_t(16), s.size() - i);\n        svuint8_t chars = svld1_u8(s.data() + i);\n\n        svuint8_t eq_a = sveor_u8(svand_u8(chars, svdup_u8(0x5F)), vowels); // Case insensitive comparison with 'a'\n        svuint8_t eq_e = sveor_u8(svand_u8(chars, svdup_u8(0x5F)), svdup_u8('E'));\n        svuint8_t eq_i = sveor_u8(svand_u8(chars, svdup_u8(0x5F)), svdup_u8('I'));\n        svuint8_t eq_o = sveor_u8(svand_u8(chars, svdup_u8(0x5F)), svdup_u8('O'));\n        svuint8_t eq_u = sveor_u8(svand_u8(chars, svdup_u8(0x5F)), svdup_u8('U'));\n\n        svuint8_t eq_y = sveor_u8(svand_u8(chars, svdup_u8(0x5F)), y);\n\n        svuint8_t sum = svaddb_u8(eq_a, eq_e);\n        sum = svaddb_u8(sum, eq_i);\n        sum = svaddb_u8(sum, eq_o);\n        sum = svaddb_u8(sum, eq_u);\n\n        if (i + len == s.size()) {\n            svuint8_t last_char = svld1_lane_u8(s.data() + i + len - 1, svdup_u8(0), 0);\n            svuint8_t eq_last_y = sveor_u8(svand_u8(last_char, svdup_u8(0x5F)), y);\n            sum = svaddb_u8(sum, eq_last_y);\n        }\n\n        count += svcntb_u8(sum);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    svuint8_t vowels = svdup_u8('aeiou');\n    svuint8_t y = svdup_u8('y');\n\n    for (size_t i = 0; i < s.size(); i += 16) {\n        size_t len = std::min(size_t(16), s.size() - i);\n        svuint8_t chars = svld1_u8(s.data() + i);\n\n        svuint8_t matches = svcmple_u8(chars, vowels);\n        count += svcntb(mask & matches);\n\n        if (len > 1 && s[i + len - 1] == 'y') {\n            matches = svcmpeq_u8(svlast1_u8(chars), y);\n            count += svcntb(mask & matches);\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    svuint8_t vowels = svdup_u8('a');\n    svuint8_t y = svdup_u8('y');\n\n    for (size_t i = 0; i < s.size(); i += 16) {\n        size_t len = std::min(size_t(16), s.size() - i);\n        svuint8_t chars = svld1_u8(s.data() + i);\n\n        svbool_t is_vowel = svor_b8(svcmpeq_u8(chars, vowels), svcmpequ_u8(chars, y));\n        if (i + len == s.size()) {\n            is_vowel = svand_b8(is_vowel, svcmpgt_b8(svsub_z_u8(svptrue_b8(), svclz_u8(svreinterpret_u8(svadd_z_u8(svreinterpret_u8(svptrue_b8()), svdup_u8(len)))), svdup_u8(4)));\n        }\n\n        count += svpopcnt_b8(is_vowel);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nint vowels_count_simd(const std::string& s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    svuint8_t vowels = svdup_u8('aeiou');\n    svuint8_t y = svdup_u8('y');\n\n    for (size_t i = 0; i < s.size(); i += SV_INT8_SIZE) {\n        size_t len = std::min(SV_INT8_SIZE, s.size() - i);\n        svuint8_t chars = svld1_u8(s.data() + i);\n\n        svbool_t match = svcmpib_eq_u8(mask, chars, vowels);\n        count += svcntb(mask, match);\n\n        if (len > 1 && s[i + len - 1] == 'y') {\n            match = svcmpib_eq_u8(mask, chars, y);\n            count += svcntb(mask, match);\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    svuint8_t sum = svdup_u8(0);\n    size_t len = s.size();\n    size_t i = 0;\n\n    while (i + 16 <= len) {\n        svuint8_t chars = svld1_u8(s.data() + i);\n        svbool_t is_upper = svcmpgt_b8(chars, svdup_u8('a'));\n        svuint8_t upper_chars = svand_u8(chars, svreinterpret_u8(is_upper));\n        sum = svadd_u8(sum, svreinterpret_u8(svsum_u8(upper_chars)));\n        i += 16;\n    }\n\n    while (i < len) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum = svadd_u8(sum, svdup_u8(static_cast<uint8_t>(s[i])));\n        }\n        i++;\n    }\n\n    return static_cast<int>(svget_lane_u32(sum, 0));\n}\n```"}
{"task_id": "SimdBench_93_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint digitSum_simd(const std::string& s) {\n    svuint8_t sum = svdup_u8(0);\n    svbool_t mask = svfalse_b8();\n    \n    for (size_t i = 0; i < s.size(); i += SV_INT8_SIZE) {\n        size_t len = std::min(SV_INT8_SIZE, s.size() - i);\n        svuint8_t chars = svld1_u8(s.data() + i);\n        \n        // Create a mask for uppercase letters\n        svbool_t is_upper = svcmpgt_u8(chars, svdup_u8('a'));\n        is_upper = svand_b8(is_upper, svcmplt_u8(chars, svdup_u8('Z')));\n        \n        // Sum the ASCII values of uppercase letters\n        sum = svaddb_u8(sum, svsel_b8(mask, sum, svreinterpret_u8(svadd_u16(svreinterpret_u16(svuzp2_u8(chars, chars)), svuzp2_u8(chars, chars))))));\n        mask = svor_b8(mask, is_upper);\n    }\n    \n    return svfirst_u8(sum);\n}\n```"}
{"task_id": "SimdBench_93_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nint digitSum_simd(const std::string& s) {\n    svuint8_t vec = svdup_u8(0);\n    int sum = 0;\n\n    for (size_t i = 0; i < s.size(); i += SV_INT8_SIZE) {\n        size_t remaining = s.size() - i;\n        size_t count = std::min(SV_INT8_SIZE, remaining);\n\n        svuint8_t chars = svld1_u8(s.data() + i);\n        svbool_t mask = svcmpgt_b8(chars, svdup_u8('Z'));\n        chars = svand_u8(chars, svcmple_b8(chars, svdup_u8('A')));\n        chars = svandnot_u8(chars, mask);\n\n        vec = svaddlv_u8(vec, chars);\n    }\n\n    svuint32_t result = svreinterpret_u32_u8(vec);\n    sum += svgetlane_u32(result, 0);\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint digitSum_simd(const std::string& s) {\n    svuint8_t vec = svdup_u8(0);\n    int sum = 0;\n\n    for (size_t i = 0; i < s.size(); i += SV_INT8_SIZE) {\n        size_t remaining = std::min(SV_INT8_SIZE, s.size() - i);\n        svuint8_t chars = svld1_u8(s.data() + i);\n\n        svbool_t is_upper = svcmpgt_b8(chars, svdup_u8('a'));\n        is_upper = svand_b8(is_upper, svcmpge_b8(chars, svdup_u8('A')));\n\n        svuint8_t upper_chars = svsel_b8(is_upper, chars, svdup_u8(0));\n        vec = svaddlv_u8(vec, upper_chars);\n    }\n\n    sum = svreinterpret_u32_u8(vec)[0];\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint digitSum_simd(const std::string& s) {\n    svuint8_t sum = svdup_u8(0);\n    svbool_t mask = svfalse_b8();\n    size_t i = 0;\n\n    while (i + 16 <= s.size()) {\n        svuint8_t chars = svld1_u8(s.data() + i);\n        svuint8_t upper_mask = svcmpgt_u8(chars, svdup_u8('a'));\n        svuint8_t lower_mask = svcmplt_u8(chars, svdup_u8('z'));\n        svuint8_t valid_chars = svand_u8(svand_u8(upper_mask, lower_mask), svnot_u8(mask));\n        sum = svadd_u8(sum, svreinterpret_u8(svreinterpret_b8(valid_chars)));\n        mask = svor_u8(mask, valid_chars);\n        i += 16;\n    }\n\n    while (i < s.size()) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum = svadd_u8(sum, svdup_u8(static_cast<uint8_t>(s[i])));\n        }\n        i++;\n    }\n\n    return svget_lane_u32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_94_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    svint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svcmpge_z_s32(vec, svdup_n_s32(0));\n    svuint32_t indices = svindex_u32(arr.size(), 1);\n    svuint32_t min_indices = svundef_u32();\n    svint32_t min_values = svundef_s32();\n    svbool_t min_mask = svfalse_b();\n\n    for (size_t i = 0; i < arr.size(); i += SV_INT32_SIZE) {\n        svint32_t sub_vec = svld1_u32(arr.data() + i);\n        svbool_t sub_mask = svcmpge_z_s32(sub_vec, svdup_n_s32(0));\n        svuint32_t sub_indices = svindex_u32(SV_INT32_SIZE, 1);\n        svuint32_t sub_min_indices = svundef_u32();\n        svint32_t sub_min_values = svundef_s32();\n        svbool_t sub_min_mask = svfalse_b();\n\n        for (size_t j = 0; j < SV_INT32_SIZE; ++j) {\n            if (svget(sub_mask, j) && svget(sub_vec, j) % 2 == 0) {\n                if (sub_min_mask == svfalse_b() || svget(sub_vec, j) < svget(sub_min_values, 0)) {\n                    sub_min_indices = svset_u32(sub_indices, j);\n                    sub_min_values = svset_s32(sub_min_values, svget(sub_vec, j));\n                    sub_min_mask = svtrue_b();\n                } else if (svget(sub_vec, j) == svget(sub_min_values, 0)) {\n                    sub_min_indices = svset_u32(sub_min_indices, svmin(svget(sub_min_indices, 0), svget(sub_indices, j)));\n                }\n            }\n        }\n\n        if (sub_min_mask != svfalse_b()) {\n            if (min_mask == svfalse_b() || svget(sub_min_values, 0) < svget(min_values, 0)) {\n                min_indices = sub_min_indices;\n                min_values = sub_min_values;\n                min_mask = sub_min_mask;\n            } else if (svget(sub_min_values, 0) == svget(min_values, 0)) {\n                min_indices = svset_u32(min_indices, svmin(svget(min_indices, 0), svget(sub_min_indices, 0)));\n            }\n        }\n    }\n\n    if (min_mask == svfalse_b()) {\n        return {};\n    }\n\n    int min_value = svget(min_values, 0);\n    int min_index = svget(min_indices, 0);\n\n    return {min_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    svint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svcmpge_z_s32(vec, svdup_n_s32(0));\n    svuint32_t indices = svindex_u32(arr.size(), 1);\n    svuint32_t min_indices = svundef_u32();\n    svint32_t min_values = svundef_s32();\n    svuint32_t min_index = svundef_u32();\n    svint32_t min_value = svundef_s32();\n\n    for (size_t i = 0; i < arr.size(); i += sve_cntb(mask)) {\n        svint32_t sub_vec = svld1_u32(arr.data() + i);\n        svbool_t sub_mask = svcmpge_z_s32(sub_vec, svdup_n_s32(0));\n        svuint32_t sub_indices = svindex_u32(sve_cntb(sub_mask), 1);\n        svuint32_t sub_min_indices = svundef_u32();\n        svint32_t sub_min_values = svundef_s32();\n        svuint32_t sub_min_index = svundef_u32();\n        svint32_t sub_min_value = svundef_s32();\n\n        if (svany(sub_mask)) {\n            sub_min_values = svmin_u32(sub_min_values, svsel(sub_mask, sub_vec, svundef_s32()));\n            sub_min_indices = svsel(sub_mask, sub_indices, svundef_u32());\n            sub_min_index = svfirst_m_b(sub_mask, sub_min_indices);\n            sub_min_value = svfirst_m_b(sub_mask, sub_min_values);\n        }\n\n        if (svany(min_mask)) {\n            min_values = svmin_u32(min_values, svsel(min_mask, sub_min_values, svundef_s32()));\n            min_indices = svsel(min_mask, sub_min_indices, svundef_u32());\n            min_index = svfirst_m_b(min_mask, sub_min_indices);\n            min_value = svfirst_m_b(min_mask, sub_min_values);\n        } else {\n            min_values = sub_min_values;\n            min_indices = sub_min_indices;\n            min_index = sub_min_index;\n            min_value = sub_min_value;\n        }\n    }\n\n    if (svany(min_mask)) {\n        return {min_value[0], min_index[0]};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    svint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svcmpge_z_s32(vec, svdup_n_s32(0));\n    svuint32_t indices = svindex_u32(svget_lanes(mask), svget_lanes(mask));\n    svint32_t min_val = svminv_u32(svand_b32(mask, vec));\n    svuint32_t min_idx = svfirst_m_b32(mask, svcmpeq_u32(vec, min_val));\n    if (svpopcnt_b32(mask) == 0) {\n        return {};\n    }\n    return {min_val[0], min_idx};\n}\n```"}
{"task_id": "SimdBench_94_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    svint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svcmpge_z_s32(vec, svdup_n_s32(0));\n    svuint32_t indices = svindex_u32(arr.size(), 0);\n    svuint32_t min_indices = svdup_n_u32(arr.size());\n    svint32_t min_values = svdup_n_s32(INT_MAX);\n\n    for (size_t i = 0; i < arr.size(); i += SV_INT32_SIZE) {\n        svint32_t sub_vec = svld1_u32(&arr[i]);\n        svbool_t sub_mask = svcmpge_z_s32(sub_vec, svdup_n_s32(0));\n        svuint32_t sub_indices = svindex_u32(SV_INT32_SIZE, i);\n        svuint32_t sub_min_indices = svdup_n_u32(i);\n        svint32_t sub_min_values = svdup_n_s32(INT_MAX);\n\n        svbool_t even_mask = svand_b32(sub_mask, svcmpeq_z_s32(svand_b32(sub_vec, svdup_n_s32(1)), svfalse_b32()));\n        svuint32_t even_indices = svsel_b32(even_mask, sub_indices, min_indices);\n        svint32_t even_values = svsel_b32(even_mask, sub_vec, min_values);\n\n        svuint32_t new_min_indices = svmin_u32(even_indices, min_indices);\n        svint32_t new_min_values = svmin_s32(even_values, min_values);\n\n        min_indices = new_min_indices;\n        min_values = new_min_values;\n    }\n\n    int min_value = svfirst_m_b32(min_values != svdup_n_s32(INT_MAX), min_values);\n    int min_index = svfirst_m_b32(min_values != svdup_n_s32(INT_MAX), min_indices);\n\n    if (min_value == INT_MAX) {\n        return {};\n    } else {\n        return {min_value, min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(const std::vector<int>& arr) {\n    svint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svcmpge_z_s32(vec, svdup_n_s32(0));\n    svuint32_t indices = svindex_u32(arr.size());\n    svuint32_t min_indices = svselect(mask, indices, svdup_n_u32(arr.size()));\n    svint32_t min_values = svselect(mask, vec, svdup_n_s32(-1));\n    svint32x2_t min_pair = svmin_m_s32(min_values, min_indices);\n    int min_value = svgetlane_s32(min_pair, 0);\n    int min_index = svgetlane_s32(min_pair, 1);\n    if (min_value == -1) {\n        return {};\n    } else {\n        return {min_value, min_index};\n    }\n}\n```"}
{"task_id": "SimdBench_95_SVE", "completion": "```c++\nint search_simd(std::vector<int> lst) {\n    int max_val = -1;\n    svbool_t p = svptrue_b32();\n    svuint32_t freq[256] = {0};\n    svuint32_t values[256] = {0};\n\n    // Count frequencies\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        svuint32_t vec = svld1_u32(lst.data() + i);\n        svuint32_t mask = svle_u32(vec, svdup_u32(255));\n        vec = svand_u32(vec, mask);\n        svaddv_u32(freq, vec);\n    }\n\n    // Find the greatest integer with frequency >= value\n    for (int i = 0; i < 256; ++i) {\n        if (freq[i] >= i && i > max_val) {\n            max_val = i;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SVE", "completion": "```c++\nint search_simd(std::vector<int> lst) {\n    int max_val = -1;\n    svuint32_t freq[256] = {0};\n    svuint32_t values[256] = {0};\n\n    // Count frequencies\n    for (size_t i = 0; i < lst.size(); i += 16) {\n        svuint32_t vec = svld1_u32(lst.data() + i);\n        svuint32_t mask = svcmpgt_u32(vec, svdup_u32(0));\n        svst1_u32(freq, svadd_u32(freq, svand_u32(vec, mask)));\n    }\n\n    // Find the greatest integer with frequency >= value\n    for (int i = 0; i < 256; ++i) {\n        if (freq[i] >= i && i > max_val) {\n            max_val = i;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nint search_simd(const std::vector<int>& lst) {\n    int max_val = -1;\n    svuint32_t freq[256] = {0};\n    svuint32_t values[256];\n    \n    // Count frequencies of each value\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        svuint32_t vec = svld1_u32(&lst[i]);\n        svst1_u32(freq, vec);\n    }\n    \n    // Find the greatest integer with frequency >= its value\n    for (int i = 1; i <= 255; ++i) {\n        if (freq[i] >= i && i > max_val) {\n            max_val = i;\n        }\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint search_simd(std::vector<int> lst) {\n    int max_val = -1;\n    svbool_t p = svptrue_b32();\n    svuint32_t freqs[256] = {0};\n    svuint32_t counts[256] = {0};\n\n    // Count frequencies\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        svuint32_t vec = svld1_u32(lst.data() + i);\n        svuint32_t mask = svle_u32(vec, svdup_u32(255));\n        vec = svand_u32(vec, mask);\n        for (int j = 0; j < 8; ++j) {\n            freqs[lst[i + j]]++;\n        }\n    }\n\n    // Find the greatest integer with frequency >= value\n    for (int i = 0; i < 256; ++i) {\n        if (freqs[i] >= i && i > max_val) {\n            max_val = i;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SVE", "completion": "```c++\nint search_simd(std::vector<int> lst) {\n    int max_val = -1;\n    svbool_t p = svptrue_b32();\n    svuint32_t freq[256] = {0};\n    svuint32_t values[256] = {0};\n\n    // Count frequencies\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        svuint32_t vec = svld1_u32(lst.data() + i);\n        svuint32_t mask = svle_u32(vec, svdup_u32(255));\n        vec = svand_u32(vec, mask);\n        svaddv_u32(freq, vec);\n    }\n\n    // Find the greatest integer with frequency >= value\n    for (int i = 0; i < 256; ++i) {\n        if (freq[i] >= i && i > max_val) {\n            max_val = i;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_96_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    svint32_t vq = svld1(svptrue_b32(), q.data());\n    svint32_t vsum = svaddv_u32(vq);\n    svint32_t vrev = svrev32_v_u32(vq);\n    svbool_t vb = sveor_z_b32(svptrue_b32(), vq, vrev);\n\n    return svallany_m_b32(vb) && vsum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    svint32_t vq = svld1(svptrue_b32(), q.data());\n    svint32_t vsum = svaddv_s32(vq);\n    bool is_balanced = svall_gt_z(svcmpgt_e32_m(svptrue_b32(), vq, svrev32_v(vq)), 0);\n    return is_balanced && vsum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    svint32_t vq = svld1(svptrue_b32(), q.data());\n    svint32_t vsum = svaddv_u32(vq);\n    bool is_balanced = svall_gt_z(svcmpgt_e32_m(svptrue_b32(), vq, svrev32(vq)), 0);\n    return is_balanced && vsum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    svint32_t vq = svld1(svptrue_b32(), q.data());\n    svint32_t vsum = svaddv_u32(vq);\n    bool is_balanced = svall_gt_z(svcmpgt_e32_b32(vq, svrev32(vq)), svfalse_b32());\n    return is_balanced && vsum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    svint32_t vq = svld1(svptrue_b32(), q.data());\n    svint32_t vsum = svaddv_s32(vq);\n    svbool_t vb = sveor_z(svptrue_b32(), vq, svrev32_v(vq));\n    \n    return svallb_b32(vb) && vsum <= w;\n}\n```"}
{"task_id": "SimdBench_97_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    svbool_t p = svptrue_b32();\n    svuint32_t v_arr = svld1_u32(arr.data());\n    svuint32_t v_rev = svld1_u32(arr.data() + n - 1);\n    int count = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        svuint32_t eq = sveor_u32(v_arr, v_rev);\n        count += svcntp_u32(eq, p);\n        v_arr = svextw_u32(v_arr, v_arr, 1);\n        v_rev = svextw_u32(v_rev, v_rev, -1);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_97_SVE", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int changes = 0;\n    svbool_t p = svptrue_b32();\n\n    for (int i = 0; i < n / 2; ++i) {\n        svuint32_t v1 = svld1_u32(arr.data() + i);\n        svuint32_t v2 = svld1_u32(arr.data() + n - i - 1);\n        svuint32_t eq = sveor_u32(v1, v2);\n        changes += svcntb_z(p, eq);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    svbool_t p = svptrue_b32();\n    svuint32_t v_arr = svld1_u32(arr.data());\n    svuint32_t v_rev = svld1_u32(arr.data() + n - 1);\n    int count = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        svuint32_t eq = sveor_u32(v_arr, v_rev);\n        count += svcntb_z(p, eq);\n        v_arr = svextw_u32(v_arr, v_arr, 1);\n        v_rev = svextw_u32(v_rev, v_rev, -1);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_97_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    svint32_t *sv_arr = reinterpret_cast<svint32_t*>(arr.data());\n    int changes = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        svbool_t mask = svcmpgt_z(svptrue_b32(), sv_arr[i], sv_arr[n - i - 1]);\n        changes += svcntb(mask);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    svbool_t p = svptrue_b32();\n    svuint32_t v_arr = svld1_u32(p, arr.data());\n    svuint32_t v_rev = svld1_u32(p, arr.data() + n - 1);\n    int count = 0;\n\n    for (int i = 0; i < n / 2; ++i) {\n        svuint32_t eq = sveor_u32(v_arr[i], v_rev[n - 1 - i]);\n        count += !svptest_any(p, eq);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_98_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    svbool_t p1 = svptrue_b8();\n    svbool_t p2 = svptrue_b8();\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        sum1 += lst1[i].length();\n    }\n\n    for (size_t i = 0; i < lst2.size(); ++i) {\n        sum2 += lst2[i].length();\n    }\n\n    if (sum1 < sum2) {\n        return lst1;\n    } else if (sum2 < sum1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    svbool_t p1 = svptrue_b8();\n    svbool_t p2 = svptrue_b8();\n\n    for (const auto& str : lst1) {\n        sum1 += str.size();\n    }\n\n    for (const auto& str : lst2) {\n        sum2 += str.size();\n    }\n\n    if (sum1 <= sum2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    svbool_t p;\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        sum1 += lst1[i].length();\n    }\n\n    for (size_t i = 0; i < lst2.size(); ++i) {\n        sum2 += lst2[i].length();\n    }\n\n    if (sum1 < sum2) {\n        return lst1;\n    } else if (sum2 < sum1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    svbool_t p;\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        sum1 += lst1[i].length();\n    }\n\n    for (size_t i = 0; i < lst2.size(); ++i) {\n        sum2 += lst2[i].length();\n    }\n\n    if (sum1 < sum2) {\n        return lst1;\n    } else if (sum2 < sum1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t sum1 = 0, sum2 = 0;\n    svbool_t p;\n\n    for (size_t i = 0; i < lst1.size(); ++i) {\n        sum1 += lst1[i].length();\n    }\n\n    for (size_t i = 0; i < lst2.size(); ++i) {\n        sum2 += lst2[i].length();\n    }\n\n    if (sum1 < sum2) {\n        return lst1;\n    } else if (sum2 < sum1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    svuint64_t v_x = svdup_u64(x);\n    svuint64_t v_n = svdup_u64(n);\n    svuint64_t v_one = svdup_u64(1);\n\n    for (size_t i = 0; i < SV_INT64_ELT_SIZE; ++i) {\n        svuint64_t v_result = v_one;\n        for (size_t j = 0; j < 64; ++j) {\n            v_result = svmul_u64(v_result, v_n);\n            if (svcmpeq_z_u64(svcmpgt_z_u64(v_result, v_x), 0)) {\n                return false;\n            }\n            if (svcmpeq_z_u64(v_result, v_x), 0)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    svuint64_t v_x = svdup_u64(x);\n    svuint64_t v_n = svdup_u64(n);\n    svuint64_t v_one = svdup_u64(1);\n\n    for (size_t i = 0; i < svcntw(); ++i) {\n        if (svany_true_b64(svcmpgt_u64(v_x, v_one))) {\n            v_x = svudiv_u64(v_x, v_n);\n        } else {\n            break;\n        }\n    }\n\n    return svall_true_b64(svcmpeq_u64(v_x, v_one));\n}\n```"}
{"task_id": "SimdBench_99_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    svuint64_t v_x = svdup_u64(x);\n    svuint64_t v_n = svdup_u64(n);\n    svuint64_t v_one = svdup_u64(1);\n\n    for (size_t i = 0; i < svcntw(); ++i) {\n        if (svany_true_b64(svcmpgt_u64(v_x, v_one))) {\n            v_x = svmul_u64(v_x, v_n);\n        } else {\n            return false;\n        }\n    }\n\n    return svcmpeq_u64(v_x, v_one);\n}\n```"}
{"task_id": "SimdBench_99_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    svuint64_t v_x = svdup_u64(x);\n    svuint64_t v_n = svdup_u64(n);\n    svuint64_t v_one = svdup_u64(1);\n\n    for (size_t i = 0; i < SV_INT64_ELT_SIZE; ++i) {\n        svuint64_t v_result = v_one;\n        for (size_t j = 0; j < 64; ++j) {\n            v_result = svmul_u64(v_result, v_n);\n            if (svcmpeq_z_u64(svcmpgt_z_u64(v_result, v_x), 0)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SVE", "completion": "```c\n#include <arm_sve.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    svuint64_t v_x = svdup_u64(x);\n    svuint64_t v_n = svdup_u64(n);\n    svuint64_t v_one = svdup_u64(1);\n\n    for (size_t i = 0; i < SV_INT64_ELEMS; ++i) {\n        if (v_x[i] == v_one || v_x[i] % v_n[i] != 0) {\n            return false;\n        }\n        v_x[i] /= v_n[i];\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_100_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint hex_key_simd(const std::string & num) {\n    svbool_t mask = svptrue_b8();\n    int count = 0;\n\n    for (size_t i = 0; i < num.size(); i += SV_INT8_SIZE) {\n        svuint8_t vec = svld1_u8(num.data() + i);\n        svuint8_t masked_vec = svand_u8(vec, svdup_u8(0x0F));\n        svuint8_t cmp_vec = sveor_u8(masked_vec, svdup_u8('2'));\n        cmp_vec = sveor_u8(cmp_vec, svdup_u8('3'));\n        cmp_vec = sveor_u8(cmp_vec, svdup_u8('5'));\n        cmp_vec = sveor_u8(cmp_vec, svdup_u8('7'));\n        cmp_vec = sveor_u8(cmp_vec, svdup_u8('B'));\n        cmp_vec = sveor_u8(cmp_vec, svdup_u8('D'));\n\n        svuint8_t eq_vec = sveor_u8(cmp_vec, svdup_u8(0xFF));\n        svuint8_t cnt_vec = svcntb_u8(eq_vec);\n\n        count += svaddv_u8(cnt_vec);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint hex_key_simd(const std::string & num) {\n    svbool_t mask = svptrue_b8();\n    int count = 0;\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        svuint8_t vec = svld1_u8(num.data() + i);\n        svuint8_t mask_primes = svcmplt_u8(vec, svdup_u8('F'));\n        mask_primes = svtst(mask_primes, svcmpeq_u8(vec, svdup_u8('2')));\n        mask_primes = svtst(mask_primes, svcmpeq_u8(vec, svdup_u8('3')));\n        mask_primes = svtst(mask_primes, svcmpeq_u8(vec, svdup_u8('5')));\n        mask_primes = svtst(mask_primes, svcmpeq_u8(vec, svdup_u8('7')));\n        mask_primes = svtst(mask_primes, svcmpeq_u8(vec, svdup_u8('B')));\n        mask_primes = svtst(mask_primes, svcmpeq_u8(vec, svdup_u8('D')));\n        count += svcntb(mask_primes);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SVE", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    svbool_t mask = svptrue_b8();\n    int count = 0;\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        svuint8_t vec = svld1_u8(num.data() + i);\n        svuint8_t masked_vec = svand_u8(vec, svdup_u8(0x0F));\n\n        svuint8_t eq_2 = sveq_u8(masked_vec, svdup_u8('2'));\n        svuint8_t eq_3 = sveq_u8(masked_vec, svdup_u8('3'));\n        svuint8_t eq_5 = sveq_u8(masked_vec, svdup_u8('5'));\n        svuint8_t eq_7 = sveq_u8(masked_vec, svdup_u8('7'));\n        svuint8_t eq_B = sveq_u8(masked_vec, svdup_u8('B'));\n        svuint8_t eq_D = sveq_u8(masked_vec, svdup_u8('D'));\n\n        svuint8_t sum = svadd_u8(eq_2, eq_3);\n        sum = svadd_u8(sum, eq_5);\n        sum = svadd_u8(sum, eq_7);\n        sum = svadd_u8(sum, eq_B);\n        sum = svadd_u8(sum, eq_D);\n\n        uint8_t result[16];\n        svst1_u8(result, sum);\n\n        for (int j = 0; j < 16; ++j) {\n            if (result[j] != 0) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint hex_key_simd(const std::string & num) {\n    svbool_t mask = svptrue_b8();\n    int count = 0;\n\n    for (size_t i = 0; i < num.size(); i += 16) {\n        svuint8_t vec = svld1_u8(num.data() + i);\n        svuint8_t masked_vec = svcmple_u8(vec, svdup_u8('F'));\n        svuint8_t prime_mask = svcmpequ_u8(svselect(masked_vec, vec, svdup_u8('2')), svdup_u8('2')) |\n                               svcmpequ_u8(svselect(masked_vec, vec, svdup_u8('3')), svdup_u8('3')) |\n                               svcmpequ_u8(svselect(masked_vec, vec, svdup_u8('5')), svdup_u8('5')) |\n                               svcmpequ_u8(svselect(masked_vec, vec, svdup_u8('7')), svdup_u8('7')) |\n                               svcmpequ_u8(svselect(masked_vec, vec, svdup_u8('B')), svdup_u8('B')) |\n                               svcmpequ_u8(svselect(masked_vec, vec, svdup_u8('D')), svdup_u8('D'));\n\n        count += svcntb_u8(prime_mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SVE", "completion": "```cpp\nint hex_key_simd(const std::string & num) {\n    svbool_t mask = svptrue_b8();\n    svuint8_t hex_chars = svdup_u8('0');\n    svuint8_t primes = svdup_u8(0x2357BD);\n\n    int count = 0;\n    size_t len = num.size();\n\n    for (size_t i = 0; i < len; i += 16) {\n        size_t chunk_size = std::min(len - i, size_t(16));\n        svuint8_t chars = svld1_u8(num.data() + i);\n\n        svuint8_t eq_mask = svcmpequ_p_u8(mask, chars, hex_chars);\n        svuint8_t prime_mask = svcmpequ_p_u8(mask, chars, primes);\n\n        svuint8_t valid_mask = svand_p_u8(eq_mask, prime_mask);\n        count += svcntb_u8(valid_mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_happy_simd(const std::string & s) {\n    svuint8_t mask = svdup_u8(0xFF);\n    svuint8_t zero = svdup_u8(0);\n\n    if (s.length() < 3) {\n        return false;\n    }\n\n    size_t len = s.length();\n    size_t num_vectors = (len + 2) / 3;\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        size_t start = i * 3;\n        size_t end = std::min(start + 3, len);\n\n        svuint8_t vec1 = svld1_u8(s.data() + start);\n        svuint8_t vec2 = svld1_u8(s.data() + start + 1);\n        svuint8_t vec3 = svld1_u8(s.data() + start + 2);\n\n        svuint8_t eq1 = sveor_u8(vec1, vec2);\n        svuint8_t eq2 = sveor_u8(vec2, vec3);\n        svuint8_t eq3 = sveor_u8(vec1, vec3);\n\n        svuint8_t all_eq = svand_u8(eq1, eq2);\n        all_eq = svand_u8(all_eq, eq3);\n\n        if (svany_true_p(all_eq)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_happy_simd(const std::string & s) {\n    svuint8_t *svs = reinterpret_cast<svuint8_t *>(const_cast<char *>(s.c_str()));\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    for (size_t i = 0; i <= len - 3; i += 16) {\n        svuint8_t v0 = svld1_u8(svs + i);\n        svuint8_t v1 = svld1_u8(svs + i + 1);\n        svuint8_t v2 = svld1_u8(svs + i + 2);\n\n        svuint8_t v01 = svor_u8(v0, v1);\n        svuint8_t v12 = svor_u8(v1, v2);\n        svuint8_t v02 = svor_u8(v0, v2);\n\n        svuint8_t v012 = svor_u8(v01, v12);\n        svuint8_t v012_not = svnot_u8(v012);\n\n        if (!svptest_any_z(v012_not)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_happy_simd(const std::string & s) {\n    svuint8_t mask = svdup_u8(0xFF);\n    svuint8_t zero = svdup_u8(0);\n\n    int n = s.size();\n    if (n < 3) return false;\n\n    for (int i = 2; i < n; ++i) {\n        svuint8_t vec1 = svld1_u8(s.data() + i - 2);\n        svuint8_t vec2 = svld1_u8(s.data() + i - 1);\n        svuint8_t vec3 = svld1_u8(s.data() + i);\n\n        svuint8_t eq1 = sveor_u8(vec1, vec2);\n        svuint8_t eq2 = sveor_u8(eq1, vec3);\n\n        if (svany_true_p(svand_u8(eq2, mask))) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_happy_simd(const std::string & s) {\n    svuint8_t *svs = reinterpret_cast<svuint8_t *>(const_cast<char *>(s.c_str()));\n    size_t len = s.length();\n\n    if (len < 3) {\n        return false;\n    }\n\n    for (size_t i = 0; i <= len - 3; ++i) {\n        svuint8_t v1 = svld1_u8(&svs[i]);\n        svuint8_t v2 = svld1_u8(&svs[i + 1]);\n        svuint8_t v3 = svld1_u8(&svs[i + 2]);\n\n        svuint8_t v4 = svor_u8(v1, v2);\n        svuint8_t v5 = svor_u8(v4, v3);\n\n        svbool_t mask = svcmpne_b8(v5, svdup_u8(0));\n        if (!svptest_any(mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_happy_simd(const std::string & s) {\n    svuint8_t mask = svdup_u8(0xFF);\n    svuint8_t zero = svdup_u8(0);\n\n    int len = s.length();\n    if (len < 3) {\n        return false;\n    }\n\n    for (int i = 2; i < len; ++i) {\n        svuint8_t vec1 = svld1_u8(s.data() + i - 2);\n        svuint8_t vec2 = svld1_u8(s.data() + i - 1);\n        svuint8_t vec3 = svld1_u8(s.data() + i);\n\n        svuint8_t eq1 = svcmpeq_u8(vec1, vec2);\n        svuint8_t eq2 = svcmpeq_u8(vec2, vec3);\n        svuint8_t eq3 = svcmpeq_u8(vec1, vec3);\n\n        svuint8_t any_eq = sveor_u8(eq1, eq2);\n        any_eq = sveor_u8(any_eq, eq3);\n\n        if (!svptest_any_p(svptrue_b8(), any_eq)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool prime_length_simd(const std::string & str) {\n    svuint32_t len = svdup_u32(str.length());\n    svuint32_t one = svdup_u32(1);\n    svuint32_t two = svdup_u32(2);\n    svuint32_t three = svdup_u32(3);\n\n    // Check if length is less than 2\n    svbool_t lt_two = svcmple_u32(len, two);\n    if (svptest_any(lt_two)) return false;\n\n    // Check if length is divisible by 2\n    svbool_t div_by_two = svcmpeq_u32(svand_u32(len, two), one);\n    if (svptest_any(div_by_two)) return false;\n\n    // Check if length is divisible by 3\n    svbool_t div_by_three = svcmpeq_u32(svand_u32(len, three), one);\n    if (svptest_any(div_by_three)) return false;\n\n    // Check divisibility by other numbers up to sqrt(len)\n    uint32_t max_divisor = static_cast<uint32_t>(std::sqrt(str.length()));\n    svuint32_t divisors = svdup_u32(max_divisor);\n    svuint32_t step = svdup_u32(2);\n    svuint32_t zero = svdup_u32(0);\n\n    while (!svallzero(divisors)) {\n        svuint32_t remainder = svmod_u32(len, divisors);\n        svbool_t is_zero = svcmpeq_u32(remainder, zero);\n        if (svptest_any(is_zero)) return false;\n        divisors = svsub_u32(divisors, step);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool prime_length_simd(const std::string & str) {\n    svuint32_t len = svdup_u32(str.length());\n    svbool_t is_prime = svtrue_b32();\n    \n    for (int i = 2; i * i <= str.length(); ++i) {\n        svuint32_t divisor = svdup_u32(i);\n        svuint32_t remainder = svmod_u32(len, divisor);\n        is_prime = svand_b32(is_prime, svnot_b32(sveor_z_b32(svfalse_b32(), svcmpne_u32(remainder, svzero_u32()))));\n    }\n    \n    return svany_true_b32(is_prime);\n}\n```"}
{"task_id": "SimdBench_102_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool prime_length_simd(const std::string & str) {\n    svuint32_t len = svdup_u32(str.length());\n    svuint32_t two = svdup_u32(2);\n    svuint32_t one = svdup_u32(1);\n\n    svuint32_t mask = svcmpgt_u32(len, one);\n    svuint32_t result = svand_u32(mask, svcmple_u32(len, two));\n\n    for (size_t i = 3; i * i <= str.length(); i += 2) {\n        svuint32_t divisor = svdup_u32(i);\n        svuint32_t remainder = svmod_u32(len, divisor);\n        result = svand_u32(result, svcmpeq_u32(remainder, one));\n    }\n\n    return svany_true_z(result);\n}\n```"}
{"task_id": "SimdBench_102_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool prime_length_simd(const std::string & str) {\n    svuint32_t len = svdup_u32(str.length());\n    svuint32_t two = svdup_u32(2);\n    svuint32_t one = svdup_u32(1);\n\n    svbool_t is_prime = svcmpgt_z_u32(len, one);\n    is_prime = svtst_z(is_prime, svcmpeq_z_u32(len, two));\n\n    for (int i = 3; i * i <= str.length(); i += 2) {\n        svuint32_t divisor = svdup_u32(i);\n        svuint32_t remainder = svmod_u32(len, divisor);\n        is_prime = svand_z(is_prime, svneq_z_u32(remainder, zero));\n    }\n\n    return svany_true(is_prime);\n}\n```"}
{"task_id": "SimdBench_102_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool prime_length_simd(const std::string & str) {\n    svuint32_t len = svdup_u32(str.length());\n    svbool_t is_prime = svtrue_b32();\n    \n    for (int i = 2; i * i <= str.length(); ++i) {\n        svuint32_t divisor = svdup_u32(i);\n        svuint32_t remainder = svmod_u32(len, divisor);\n        is_prime = svand_b32(is_prime, svnot_b32(sveor_z_b32(svfalse_b32(), svcmpne_u32(remainder, svzero_u32()))));\n    }\n    \n    return svany_true_b32(is_prime);\n}\n```"}
{"task_id": "SimdBench_103_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    svuint8_t sum = svdup_u8(0);\n    while (N != 0) {\n        svuint8_t digits = svreinterpret_u8_u64(svcreate_u64(N));\n        N /= 10;\n        sum = svaddb_u8(sum, digits);\n    }\n    uint32_t result = svsum_u32(svptrue_b8(), svreinterpret_u32_u8(sum));\n    std::bitset<32> bits(result);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_103_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    svuint8_t sum = svdup_u8(0);\n    while (N != 0) {\n        svuint8_t digits = svreinterpret_u8_u32(svuzp2_u32(svreinterpret_u32_u8(svptrue_b8()), svreinterpret_u32_u8(svptrue_b8())));\n        sum = svaddb_u8(sum, svreinterpret_u8_u32(digits));\n        N /= 10;\n    }\n    std::string result;\n    for (int i = 0; i < 8; ++i) {\n        result += std::to_string((sum[i] >> 7) & 1);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_103_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    svuint8_t sum = svdup_u8(0);\n    while (N != 0) {\n        svuint8_t digits = svreinterpret_u8_u32(svreinterpret_u32_u8(svcreate_u8(N % 10)));\n        sum = svaddb_u8(sum, digits);\n        N /= 10;\n    }\n    std::string result;\n    for (int i = 7; i >= 0; --i) {\n        if ((sum[i / 8] >> (i % 8)) & 1) {\n            result += '1';\n        } else {\n            result += '0';\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_103_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    svuint8_t sum = svdup_u8(0);\n    while (N != 0) {\n        svuint8_t digits = svreinterpret_u8_u64(svcreate_u64(N));\n        sum = svaddb_u8(sum, digits);\n        N /= 10;\n    }\n    uint8_t result[9];\n    svst1_u8(result, sum);\n    std::string binary_str = std::bitset<8>(result[0]).to_string();\n    for (int i = 1; i < 9; ++i) {\n        binary_str += std::bitset<8>(result[i]).to_string();\n    }\n    // Remove leading zeros\n    size_t first_one = binary_str.find('1');\n    if (first_one != std::string::npos) {\n        binary_str = binary_str.substr(first_one);\n    } else {\n        binary_str = \"0\";\n    }\n    return binary_str;\n}\n```"}
{"task_id": "SimdBench_103_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N) {\n    svuint8_t sum = svdup_u8(0);\n    while (N != 0) {\n        svuint8_t digits = svreinterpret_u8_u64(svand_u64(N, svdup_u64(0xFF)));\n        sum = svaddb_u8(sum, digits);\n        N >>= 8;\n    }\n    std::string result;\n    for (int i = 0; i < 8; ++i) {\n        if (svget_u8(sum, i)) {\n            result += '1';\n        } else {\n            result += '0';\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_SVE", "completion": "```cpp\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t vlst = svld1_u32(lst.data());\n    svuint32_t vindices = svdup_u32(1);\n    svuint32_t vstep = svdup_u32(2);\n    svuint32_t vend = svdup_u32(lst.size());\n\n    while (!svcmpeq_z(p, vend, vindices)) {\n        svuint32_t veven = svand_u32(vlst, svdup_u32(1));\n        svuint32_t vadd = svcmov_u32(veven, vindices, svdup_u32(0), svdup_u32(1));\n        sum += svsum_u32(vadd);\n        vindices = svadd_u32(vindices, vstep);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v = svld1_u32(lst.data());\n    svuint32_t mask = svdup_u32(0xAAAAAAAA);\n    v = svand_u32(v, mask);\n    v = svaddlv_u32(v);\n    return svget_lane_u32(v, 0);\n}\n```"}
{"task_id": "SimdBench_104_SVE", "completion": "```c++\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v = svld1_u32(lst.data());\n    for(size_t i = 2; i < lst.size(); i += 4){\n        svuint32_t v_next = svld1_u32(lst.data() + i);\n        svuint32_t mask = svcmplt_z_u32(v, v_next);\n        v = svadd_u32(v, svsel(mask, v_next, v));\n    }\n    for(int i = 0; i < lst.size(); i++){\n        if(i % 2 != 0 && lst[i] % 2 == 0){\n            sum += lst[i];\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t v = svld1_u32(lst.data());\n    svuint32_t mask = svdup_u32(0xAAAAAAAA);\n    v = svand_u32(v, mask);\n    v = svaddlv_u32(v);\n    sum += svgetlane_u32(v, 0);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SVE", "completion": "```c++\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t vlst = svld1_u32(lst.data());\n    svuint32_t veven = svand_u32(vlst, svdup_u32(0xAAAAAAAA));\n    svuint32_t vodd = svand_u32(vlst, svdup_u32(0x55555555));\n    svuint32_t vsum = svaddlv_u32(veven);\n    sum += svgetlane_u32(vsum, 0);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    svuint8_t *input = reinterpret_cast<svuint8_t *>(const_cast<char *>(s.c_str()));\n    svuint8_t *output = new svuint8_t[len];\n\n    svbool_t mask = svptrue_b8();\n\n    for (size_t i = 0; i < len; i += SV_INT8_SIZE) {\n        svuint8_t vec = svld1(mask, input + i);\n        svuint8_t shifted = svaddb_u8(vec, svdup_n_u8(2 * 2));\n        svst1(mask, output + i, shifted);\n    }\n\n    std::string result(reinterpret_cast<char *>(output), len);\n    delete[] output;\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    // Create a mask for the ASCII values of 'a' to 'z'\n    svbool_t mask = svcmple_b8(svdup_u8('z'), svdup_u8('a'));\n\n    // Convert the input string to a vector of characters\n    svuint8_t vec_in = svld1_u8(s.data());\n\n    // Rotate the alphabet by 2 * 2 = 4 positions\n    svuint8_t vec_rot = svaddb_u8(vec_in, svdup_u8(4));\n\n    // Wrap around the alphabet if necessary\n    svuint8_t vec_wrap = svminub_u8(vec_rot, svdup_u8('z'));\n    svuint8_t vec_sub = svsubb_u8(svdup_u8('z'), svdup_u8('a'));\n    svuint8_t vec_add = svaddb_u8(vec_wrap, vec_sub);\n    svuint8_t vec_or = svor_u8(mask, vec_add);\n\n    // Store the result back into a string\n    std::string result(len, '\\0');\n    svst1_u8(result.data(), vec_or);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    // Create a mask for the ASCII values of 'a' to 'z'\n    svbool_t mask = svcmple_b8(svdup_u8('z'), svdup_u8('a'));\n\n    // Convert the input string to a vector of characters\n    svuint8_t vec_in = svld1_u8(s.data());\n\n    // Rotate the alphabet by 2 * 2 = 4 positions\n    svuint8_t vec_rot = svaddb_u8(vec_in, svdup_u8(4));\n\n    // Wrap around the alphabet if necessary\n    svuint8_t vec_wrap = svsubb_u8(vec_rot, svdup_u8(26));\n    svuint8_t vec_masked = svandnot_z(mask, vec_rot, vec_wrap);\n    svuint8_t vec_result = svor_z(mask, vec_masked, vec_rot);\n\n    // Convert the result back to a string\n    std::string result(len, '\\0');\n    svst1_u8(result.data(), vec_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    // Create a mask for the last character\n    svbool_t mask = svptrue_b8();\n\n    // Load the input string into a scalable vector register\n    svuint8_t vec_in = svld1_u8(s.data());\n\n    // Define the rotation value\n    uint8_t rotate = 4; // 2 * 2\n\n    // Perform the encryption\n    svuint8_t vec_out = svaddb_u8(vec_in, svdup_n_u8(rotate));\n\n    // Wrap around characters beyond 'z'\n    vec_out = svminub_u8(vec_out, svdup_n_u8('z'));\n    vec_out = svmaxub_u8(vec_out, svdup_n_u8('a'));\n\n    // Store the result back to the output string\n    svst1_u8(s.data(), vec_out);\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_105_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.length();\n    if (len == 0) return \"\";\n\n    // Create a vector mask for the entire string\n    svbool_t mask = svptrue_b8();\n\n    // Convert the string to a vector of characters\n    svuint8_t vec_str = svld1_u8(s.data(), mask);\n\n    // Define the rotation value\n    svint32_t rot_val = svdup_n_s32(4); // 2 * 2\n\n    // Perform the encryption\n    svuint8_t vec_enc = svaddb_u8(vec_str, svreinterpret_u8(rot_val), mask);\n    vec_enc = svclrbits_u8(vec_enc, mask, 0x60); // Clear bits for non-alphabetic characters\n\n    // Convert the vector back to a string\n    std::string enc_str(len, ' ');\n    svst1_u8(enc_str.data(), vec_enc, mask);\n\n    return enc_str;\n}\n```"}
{"task_id": "SimdBench_106_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    svbool_t p = svptrue_b8();\n    svuint8_t msg = svld1_u8(message.data());\n    svuint8_t result = svdup_u8(0);\n\n    for (size_t i = 0; i < message.size(); i += 16) {\n        svuint8_t chunk = svld1_u8(message.data() + i);\n        svuint8_t swapped_case = svorr_u8(svandnot_u8(chunk, svdup_u8(0x20)), svand_u8(chunk, svdup_u8(0x20)));\n        svuint8_t vowels = svcmple_u8(swapped_case, svdup_u8('u')) & svcmpge_u8(swapped_case, svdup_u8('a'));\n        svuint8_t shifted_vowels = svadd_u8(svu8(sub(swapped_case, svdup_u8('a')), svdup_u8(2)), svdup_u8('a'));\n        svuint8_t encoded_chunk = svsel_u8(vowels, shifted_vowels, swapped_case);\n        result = svstl1_u8(result, encoded_chunk);\n    }\n\n    return std::string(reinterpret_cast<char*>(result.base()), message.size());\n}\n```"}
{"task_id": "SimdBench_106_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t n = message.size();\n    std::string result(n, '\\0');\n    svbool_t p = svptrue_b8();\n\n    for (size_t i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32x4_t v_in = svld1_u32(&message[i]);\n        svint32x4_t v_out = v_in;\n\n        // Swap case\n        v_out = svsel(p, svcmplt_z_i32(v_in, 'a'), svsub_z_i32(svadd_z_i32(v_in, 32), svcmplt_z_i32(v_in, 'A')));\n\n        // Replace vowels\n        svint32x4_t vowels = svdup_n_i32(0x65736f6c); // 'els'\n        svint32x4_t replacements = svdup_n_i32(0x67756e69); // 'ing'\n        svint32x4_t mask = svcmpeq_xsv32(vowels, v_andnot_z_i32(svcmplt_z_i32(v_in, 0x61), svcmpgt_z_i32(v_in, 0x5a)));\n        v_out = svsel(mask, v_or_xsv32(v_out, replacements), v_out);\n\n        svst1_u32(&result[i], v_out);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    svuint8_t msg = svld1_u8(message.data());\n    svbool_t is_vowel = sveor_b8(svcmpne_u8(msg, 'a'), svcmpne_u8(msg, 'e'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'i'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'o'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'u'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'A'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'E'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'I'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'O'));\n    is_vowel = sveor_b8(is_vowel, svcmpne_u8(msg, 'U'));\n\n    svuint8_t swapped_case = svsub_u8(sveor_u8(msg, 0x20), svandnot_u8(msg, 0x20));\n    svuint8_t shifted_vowels = svadd_u8(swapped_case, svdup_u8(2));\n\n    svuint8_t result = svsel_b8(is_vowel, shifted_vowels, swapped_case);\n\n    return std::string(reinterpret_cast<char*>(result.base()), message.size());\n}\n```"}
{"task_id": "SimdBench_106_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t n = message.size();\n    std::string result(n, '\\0');\n    svbool_t p = svptrue_b8();\n\n    for (size_t i = 0; i < n; i += SV_MAX_SIZE) {\n        size_t chunk_size = std::min(SV_MAX_SIZE, n - i);\n        svuint8x16_t msg_vec = svld1_u8(message.data() + i);\n\n        // Swap case\n        svuint8x16_t swapped_case = svbic_u8(msg_vec, svdup_u8(0x20));\n\n        // Replace vowels with the letter that appears 2 places ahead\n        svuint8x16_t mask_vowels = svcmple_u8(swapped_case, svdup_u8('z'));\n        svuint8x16_t mask_upper = svcmpgt_u8(swapped_case, svdup_u8('Z'));\n        svuint8x16_t mask_lower = svcmpgt_u8(swapped_case, svdup_u8('a'));\n\n        svuint8x16_t shifted_vowels = svadd_u8(swapped_case, svdup_u8(2));\n        shifted_vowels = svsub_u8(shifted_vowels, svdup_u8('y' - 'a' + 1));\n        shifted_vowels = svsub_u8(shifted_vowels, svdup_u8('Y' - 'A' + 1));\n\n        svuint8x16_t result_vec = svsel(mask_vowels, shifted_vowels, swapped_case);\n\n        // Store the result\n        svst1_u8(result.data() + i, result_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message) {\n    size_t len = message.length();\n    svbool_t p = svptrue_b8();\n    svuint8_t msg = svld1_u8(message.data());\n    \n    // Swap case\n    svuint8_t swapped_case = svorr_u8(svandnot_u8(msg, svdup_u8(0x20)), svand_u8(msg, svdup_u8(0x20)));\n    \n    // Replace vowels with the letter that appears 2 places ahead\n    svuint8_t vowels = svdup_u8('aeiouAEIOU');\n    svuint8_t replacements = svdup_u8('cgikmoCGIKMO');\n    svuint8_t result = svsub_u8(swapped_case, svreinterpret_u8(svpfalse_b8()));\n    \n    for (size_t i = 0; i < len; i += 16) {\n        svuint8_t mask = svcmplt_z_u8(vowels, svld1_u8(&message[i]));\n        svuint8_t replaced_vowels = svadd_u8(replacements, svreinterpret_u8(mask));\n        result = svsel_u8(mask, replaced_vowels, result);\n    }\n    \n    return std::string(reinterpret_cast<char*>(svstl_u8(result)), len);\n}\n```"}
{"task_id": "SimdBench_107_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    svuint8_t lower_mask = svdup_u8(0x20);\n    svuint8_t upper_mask = svdup_u8(0x20);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        svuint8_t key_vec = svld1_u8(key.data(), sv_dup_imm_u8(key.size()));\n\n        svuint8_t lower_check = svand_u8(svsub_u8(key_vec, lower_mask), svdup_u8(0x20));\n        svuint8_t upper_check = svand_u8(svsub_u8(key_vec, upper_mask), svdup_u8(0x20));\n\n        if (!svalltrue_b8(sveor_u8(lower_check, key_vec)) && !svalltrue_b8(sveor_u8(upper_check, key_vec))) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    svbool_t all_lower = svptrue_b8();\n    svbool_t all_upper = svptrue_b8();\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        svuint8x16_t key_vec = svld1_u8(key.data());\n        svbool_t is_lower = sveor_p8(all_lower, svcmpgt_b8(key_vec, svdup_u8('Z')));\n        svbool_t is_upper = sveor_p8(all_upper, svcmpgt_b8(svsub_u8(key_vec, svdup_u8('a')), svdup_u8('z')));\n\n        all_lower = svand_p8(all_lower, is_lower);\n        all_upper = svand_p8(all_upper, is_upper);\n\n        if (!svptest_any_p8(all_lower) && !svptest_any_p8(all_upper)) {\n            return false;\n        }\n    }\n\n    return svptest_any_p8(all_lower) || svptest_any_p8(all_upper);\n}\n```"}
{"task_id": "SimdBench_107_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    svbool_t all_lower = svptrue_b8();\n    svbool_t all_upper = svptrue_b8();\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        svuint8x16_t key_data = svld1_u8(key.data());\n        svuint8x16_t mask_lower = sveor_u8(sveor_u8(svand_u8(key_data, svdup_u8(0x7F)), svdup_u8(0x20)), svdup_u8(0x20));\n        svuint8x16_t mask_upper = sveor_u8(sveor_u8(svand_u8(key_data, svdup_u8(0x7F)), svdup_u8(0x20)), svdup_u8(0x20));\n\n        all_lower = svand(all_lower, svcmpne(mask_lower, svdup_u8(0)));\n        all_upper = svand(all_upper, svcmpne(mask_upper, svdup_u8(0)));\n    }\n\n    return svptest_any(all_lower) || svptest_any(all_upper);\n}\n```"}
{"task_id": "SimdBench_107_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n#include <map>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    svuint8_t lower_mask = svdup_u8(0x20);\n    svuint8_t upper_mask = svdup_u8(0x20);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        svuint8_t key_vec = svld1_u8(key.c_str(), sv_dup_imm_u32(key.size()));\n\n        svuint8_t lower_check = svand_u8(key_vec, lower_mask);\n        svuint8_t upper_check = svand_u8(key_vec, upper_mask);\n\n        if (!svalltrue_b8(sveor_u8(lower_check, svdup_u8(0)))) {\n            upper_mask = svdup_u8(0);\n        }\n        if (!svalltrue_b8(sveor_u8(upper_check, svdup_u8(0)))) {\n            lower_mask = svdup_u8(0);\n        }\n\n        if (!(lower_mask | upper_mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nbool check_dict_case_simd(const std::map<std::string, std::string>& dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    svbool_t all_lower = svptrue_b8();\n    svbool_t all_upper = svptrue_b8();\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        svuint8x16_t key_vec = svld1_u8(key.data());\n        svuint8x16_t lower_mask = svcmplt_u8(key_vec, svdup_u8('a'));\n        svuint8x16_t upper_mask = svcmpgt_u8(key_vec, svdup_u8('Z'));\n\n        all_lower = svand(all_lower, svnot(lower_mask));\n        all_upper = svand(all_upper, svnot(upper_mask));\n\n        if (!svptest_any(all_lower) && !svptest_any(all_upper)) {\n            return false;\n        }\n    }\n\n    return svptest_any(all_lower) || svptest_any(all_upper);\n}\n```"}
{"task_id": "SimdBench_108_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    svbool_t p = svptrue_b32();\n    svuint32_t sieve = svdup_u32(0);\n    svuint32_t limit = svdup_u32(n);\n\n    for (svuint32_t i = 2; i < n; i++) {\n        svuint32_t mask = sveor_z_u32(p, svcmpgt_u32(i, sieve));\n        sieve = svadd_z_u32(sieve, mask, i);\n    }\n\n    for (svuint32_t i = 2; i < n; i++) {\n        if (sveany_gt_z_u32(sveor_z_u32(p, svcmpeq_u32(sieve[i], 0)))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    svbool_t p = svptrue_b32();\n    svuint32_t v = svdup_u32(2);\n    svuint32_t limit = svdup_u32(n);\n\n    while (svany_true_z(p)) {\n        svuint32_t mask = svcmple_u32(v, limit);\n        svuint32_t next_prime = svadd_u32(v, svld1_u32(primes.data(), svcntb(mask)));\n        primes.push_back(next_prime[0]);\n        v = svadd_u32(v, svld1_u32(primes.data(), svcntb(mask)));\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    svbool_t p = svptrue_b32();\n    svuint32_t v = svdup_u32(2);\n    svuint32_t limit = svdup_u32(n);\n\n    while (!svallb(p)) {\n        svuint32_t mask = svcmple_u32(v, limit);\n        svuint32_t next = svadd_u32(v, svld1_u32(&primes[0]));\n        svst1_u32(&primes[0], next);\n        v = svsub_u32(next, svand_u32(mask, v));\n        p = svcmpne_u32(v, svzero_u32());\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    svbool_t p = svptrue_b32();\n    svuint32_t sieve = svdup_u32(0);\n    svuint32_t limit = svdup_u32(n);\n\n    for (int i = 2; i < n; ++i) {\n        svuint32_t idx = svdup_u32(i);\n        svuint32_t mask = sveor_z_u32(p, sieve[idx], p);\n        sieve = svadd_z_u32(sieve, mask, p);\n        p = svandnot_z_u32(p, mask, p);\n    }\n\n    for (int i = 2; i < n; ++i) {\n        if (svgetlane_u32(p, i - 2)) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    svbool_t p = svptrue_b32();\n    svuint32_t sieve = svdup_u32(0);\n    svuint32_t limit = svdup_u32(n);\n\n    for (svuint32_t i = 2; i < limit; i++) {\n        svuint32_t multiple = svmul_u32(i, svindex_u32(0, n - i));\n        sieve = svadd_u32(sieve, multiple);\n    }\n\n    for (svuint32_t i = 2; i < limit; i++) {\n        if (svanyTrue_b32(svand_b32(p, svcmpne_u32(sieve, svdup_u32(i))))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SVE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    size_t i = 0;\n\n    while (i + SV_INT32_SIZE <= s.size()) {\n        svuint32_t vec = svld1_u32((const uint32_t *)&s[i]);\n        svuint32_t upper_mask = svcmpgt_u32(vec, svdup_u32('A')) & svcmple_u32(vec, svdup_u32('Z'));\n        svuint32_t vowel_mask = upper_mask & svcreate_u32(0x000000F6); // 'AEIOU'\n        count += svcntb_z(mask & vowel_mask);\n        i += SV_INT32_SIZE;\n    }\n\n    // Handle remaining elements\n    while (i < s.size() && i % 4 == 0) {\n        if ((s[i] >= 'A' && s[i] <= 'Z') && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            count++;\n        }\n        i++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SVE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    size_t i = 0;\n\n    while (i + 16 <= s.size()) {\n        svuint8_t vec = svld1_u8(s.data() + i);\n        svuint8_t upper_mask = svcmpgt_u8(vec, svdup_u8('A'));\n        upper_mask = svand_u8(upper_mask, svcmpge_u8(vec, svdup_u8('Z')));\n        upper_mask = svand_u8(upper_mask, svcmpeq_u8(svand_u8(vec, svdup_u8(0x20)), svdup_u8(0)));\n        count += svcntb(mask, upper_mask);\n        i += 16;\n    }\n\n    // Handle remaining elements\n    while (i < s.size()) {\n        if (s[i] >= 'A' && s[i] <= 'Z' && !(s[i] & 0x20)) {\n            count++;\n        }\n        i++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SVE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    size_t i = 0;\n\n    while (i + SV_INT64_SIZE <= s.size()) {\n        svuint8_t vec = svld1_u8(s.data() + i);\n        svuint8_t upper_mask = svcmpgt_u8(vec, svdup_u8('A'));\n        upper_mask = svand_u8(upper_mask, svcmpge_u8(vec, svdup_u8('Z')));\n        svuint8_t vowel_mask = svor_u8(svand_u8(upper_mask, svcmpeq_u8(vec, svdup_u8('A'))),\n                                      svand_u8(upper_mask, svcmpeq_u8(vec, svdup_u8('E'))));\n        vowel_mask = svand_u8(vowel_mask, svand_u8(upper_mask, svcmpeq_u8(vec, svdup_u8('I'))));\n        vowel_mask = svand_u8(vowel_mask, svand_u8(upper_mask, svcmpeq_u8(vec, svdup_u8('O'))));\n        vowel_mask = svand_u8(vowel_mask, svand_u8(upper_mask, svcmpeq_u8(vec, svdup_u8('U'))));\n\n        count += svcntb(mask & vowel_mask);\n\n        i += SV_INT64_SIZE;\n    }\n\n    // Handle remaining elements\n    for (; i < s.size(); ++i) {\n        if ((i % 2 == 0) && isupper(s[i]) && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    size_t i = 0;\n\n    while (i + SV_ELT_SIZE_B(svuint8_t) <= s.size()) {\n        svuint8_t vec = svld1_u8(&s[i]);\n        svuint8_t upper_mask = sveor_u8(vec, svdup_u8('a' ^ 'A'));\n        svuint8_t vowel_mask = svand_u8(upper_mask, svdup_u8('AEIOU' | ('aeiou' << 8)));\n        svuint32_t result = svcntb_z(mask, svreinterpret_u32(vowel_mask));\n        count += svget_lane_u32(result, 0);\n        i += SV_ELT_SIZE_B(svuint8_t);\n    }\n\n    // Handle remaining elements\n    for (; i < s.size(); ++i) {\n        if ((i % 2 == 0) && (s[i] >= 'A' && s[i] <= 'Z') && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SVE", "completion": "```cpp\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    svbool_t mask = svptrue_b8();\n    size_t i = 0;\n\n    while (i + 16 <= s.size()) {\n        svuint8_t chars = svld1_u8(s.data() + i);\n        svuint8_t vowels = svcmple_u8(chars, svdup_u8('U'));\n        svuint8_t upper = svand_u8(svcmplt_u8(chars, svdup_u8('a')), svcmple_u8(chars, svdup_u8('Z')));\n        svuint8_t result = svand_u8(vowels, upper);\n        count += svcntb_z(mask, result);\n        i += 16;\n    }\n\n    // Handle remaining characters\n    while (i < s.size()) {\n        if (s[i] >= 'A' && s[i] <= 'Z' && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            count++;\n        }\n        i++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    svint32_t levels = svdup_n_s32(n);\n    svbool_t mask = svle_u32(svindex_s32(n), svdup_n_s32(n - 1));\n    svint32_t increment = svadd_z_s32(mask, svdup_n_s32(2), svdup_n_s32(1));\n    levels = svadd_z_s32(mask, levels, increment);\n    std::vector<int> result(levels.base_addr(), levels.base_addr() + n);\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    svint32_t levels = svdup_n_s32(n);\n    svbool_t mask = svle_u32(levels, svdup_n_u32(n));\n    svint32_t odd_even_mask = svreinterpret_s32_u32(svadd_x_u32(svmul_n_u32(mask, svdup_n_u32(2)), svdup_n_u32(1)));\n    svint32_t result = svadd_x_s32(levels, odd_even_mask);\n    std::vector<int> output(n);\n    svst1(result.base_addr(), result, svptrue_b32());\n    return output;\n}\n```"}
{"task_id": "SimdBench_110_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    svint32_t ones = svdup_n_s32(1);\n    svbool_t mask = svle_u32_z(svptrue_b32(), svdup_n_u32(n), svdup_n_u32(0));\n    svuint32_t count = svdup_n_u32(n);\n\n    for (int i = 0; i < n; i += 8) {\n        svuint32_t indices = svindex_u32(8);\n        svuint32_t parity = svand_u32(svadd_u32(count, indices), ones);\n        svuint32_t next_stones = svadd_u32(count, svcmov_u32(parity, svadd_u32(indices, ones), svsub_u32(indices, ones)));\n        svst1_u32(result.data() + i, next_stones);\n        count = svadd_u32(count, svdup_n_u32(8));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    svint32_t ones = svdup_n_s32(1);\n    svint32_t two = svdup_n_s32(2);\n    svbool_t mask = svle_u32(svptrue_b32(), svdup_n_u32(n));\n\n    for (int i = 0; i < n; i += 8) {\n        svint32_t indices = svld1_u32(mask, reinterpret_cast<const uint32_t*>(&i));\n        svint32_t current_level = svadd_z_s32(svfalse_b32(), svreinterpret_u32(indices), ones);\n        svint32_t next_level = svadd_z_s32(svfalse_b32(), svreinterpret_u32(current_level), svselect(svand_z_b32(mask, svmod_z_s32(svreinterpret_u32(current_level), two)), ones, two));\n        svst1_u32(mask, reinterpret_cast<uint32_t*>(&result[i]), svreinterpret_u32(next_level));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    svint32_t levels = svdup_n_s32(n);\n    svbool_t mask = svle_u32(svindex_s32(n), svdup_n_s32(n));\n    svint32_t next_odd = svadd_z_s32(mask, levels, svdup_n_s32(1));\n    svint32_t next_even = svadd_z_s32(~mask, levels, svdup_n_s32(0));\n    svint32_t result = svtbl_s32(next_odd, svreinterpret_u32_u8(svptrue_b32()));\n    return std::vector<int>(result.base_addr, result.base_addr + n);\n}\n```"}
{"task_id": "SimdBench_111_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cmath>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    svint64_t sum = svdup_n_s64(0);\n    svuint64_t count = svdup_n_u64(m - n + 1);\n\n    for (int64_t i = n; i <= m; i += 8) {\n        svint64_t values = svld1_s64(&i);\n        sum = svadd_s64(sum, values);\n    }\n\n    svint64_t avg = svdiv_s64(sum, count);\n    int64_t result = std::round(svget_lane_s64(avg, 0));\n\n    return std::bitset<64>(result).to_string();\n}\n```"}
{"task_id": "SimdBench_111_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cmath>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    svint64_t sum = svdup_n_s64(0);\n    svuint64_t count = svdup_n_u64(m - n + 1);\n\n    for (int64_t i = n; i <= m; i += 8) {\n        svint64_t vec = svld1_s64(&i);\n        sum = svadd_s64(sum, vec);\n    }\n\n    svint64_t avg = svtudiv_s64(sum, count);\n    int64_t result = std::round(static_cast<double>(avg[0]));\n\n    std::bitset<64> bits(result);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_111_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cmath>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    svint64_t sum = svdup_n_s64(0);\n    svuint64_t count = svdup_n_u64(m - n + 1);\n\n    for (int64_t i = n; i <= m; i += 8) {\n        svint64_t vec = svld1_s64(&i);\n        sum = svadd_s64(sum, vec);\n    }\n\n    svint64_t avg = svtudiv_s64(sum, count);\n    int64_t result = std::round(static_cast<double>(svgetlane(avg, 0)));\n\n    return std::bitset<64>(result).to_string().substr(64 - std::ceil(std::log2(result + 1)));\n}\n```"}
{"task_id": "SimdBench_111_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cmath>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    svint64_t sum = svdup_n_s64(0);\n    svuint64_t count = svdup_n_u64(m - n + 1);\n\n    for (int64_t i = n; i <= m; i += 8) {\n        svint64_t values = svld1_s64(&i);\n        sum = svadd_s64(sum, values);\n    }\n\n    svint64_t avg = svdiv_s64(sum, count);\n    int64_t result = std::round(svget_lane_s64(avg, 0));\n\n    std::bitset<64> bits(result);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_111_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <cmath>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) {\n        return \"-1\";\n    }\n\n    svint64_t sum = svdup_n_s64(0);\n    svuint64_t count = svdup_n_u64(m - n + 1);\n\n    for (int64_t i = n; i <= m; i += 8) {\n        svint64_t values = svld1_s64(&i);\n        sum = svadd_s64(sum, values);\n    }\n\n    svint64_t avg = svtudiv_s64(sum, count);\n    int64_t result = std::round(svgetlane(avg, 0));\n\n    return std::bitset<64>(result).to_string();\n}\n```"}
{"task_id": "SimdBench_112_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    svint32_t indices = svdup_n_s32(1);\n    svint32_t result = svdup_n_s32(0);\n    svbool_t mask = svle_u32(indices, svdup_n_u32(n));\n\n    while (svptest_any(mask)) {\n        svint32_t is_even = svand_b32(svmodu_u32(indices, svdup_n_u32(2)), svtrue_b32());\n        svint32_t factorial = svfactorial_s32(indices);\n        svint32_t sum = svsum_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s32(svduadd_s32(svsub_s32(svadd_s3"}
{"task_id": "SimdBench_112_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    svint32_t indices = svdup_n_s32(1);\n    svint32_t result = svdup_n_s32(0);\n    svbool_t mask = svle_u32(indices, svdup_n_u32(n));\n\n    while (svptest_any(mask)) {\n        svint32_t even_mask = svceqz_z_s32(svmodu_s32(indices, svdup_n_s32(2)), svtrue());\n        svint32_t odd_sum = svaddv_s32(svselect(even_mask, svdup_n_s32(0), svsum_s32(svspan(indices))));\n        svint32_t factorial = svfactorial_s32(svselect(even_mask, svdup_n_s32(1), svspan(indices)));\n        svint32_t mod_factorial = svmodu_s32(factorial, svdup_n_s32(10000));\n        result = svadd(result, svselect(even_mask, mod_factorial, odd_sum));\n        indices = svadd(indices, svdup_n_s32(1));\n        mask = svle_u32(indices, svdup_n_u32(n));\n    }\n\n    std::vector<int> output(n);\n    svst1_u32(output.data(), result);\n    return output;\n}\n```"}
{"task_id": "SimdBench_112_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    svint32_t result = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n    int i = 1;\n\n    while (svptest_any(mask)) {\n        svuint32_t indices = svindex_u32(i, n);\n        svuint32_t even_mask = svcmple_u32(indices, svdup_n_u32(n / 2));\n        svuint32_t odd_mask = svcmplt_u32(indices, svdup_n_u32(n / 2));\n\n        svint32_t factorials = svdup_n_s32(1);\n        svint32_t sums = svdup_n_s32(0);\n\n        for (int j = 2; j <= n; ++j) {\n            svint32_t j_vec = svdup_n_s32(j);\n            factorials = svmla_e32(factorials, svdup_n_s32(j), factorials, even_mask);\n            sums = svmla_e32(sums, svdup_n_s32(j), sums, odd_mask);\n        }\n\n        result = svsel_e32(even_mask, factorials, sums);\n        result = svmod_s32(result, svdup_n_s32(10000));\n\n        mask = svcmpgt_b32(svadd_z_s32(svindex_u32(i, n), svdup_n_u32(1)), svdup_n_u32(n));\n        i += svcntb(mask);\n    }\n\n    return std::vector<int>(result.base_addr, result.base_addr + n);\n}\n```"}
{"task_id": "SimdBench_112_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    svint32_t result = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (int i = 1; i <= n; ++i) {\n        svint32_t idx = svdup_n_s32(i);\n        svint32_t mod = svdup_n_s32(10000);\n\n        if (svptest_any(mask, svcmple_u32(idx, svdup_n_u32(1)))) {\n            result = svadd_z_s32(result, svdup_n_s32(i), mask);\n        } else {\n            svint32_t factorial = svdup_n_s32(1);\n            for (int j = 1; j <= i; ++j) {\n                factorial = svmul_z_s32(factorial, svdup_n_s32(j), mask);\n            }\n            result = svadd_z_s32(result, svmod_z_s32(factorial, mod, mask), mask);\n        }\n    }\n\n    std::vector<int> vec(n);\n    svst1_u32(vec.data(), result);\n    return vec;\n}\n```"}
{"task_id": "SimdBench_112_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    svint32_t result = svdup_n_s32(0);\n    svuint32_t indices = svindex_u32(n);\n\n    for (int i = 1; i <= n; ++i) {\n        svbool_t is_even = svcmpgt_b32(svconst_i32(i), svconst_i32(1));\n        svuint32_t current_index = svreinterpret_u32(is_even);\n        svuint32_t current_value = svreinterpret_u32(svadd_z_u32(current_index, svconst_u32(1)));\n\n        if (svptest_any(is_even)) {\n            svuint32_t factorial = svdup_n_u32(1);\n            for (int j = 1; j <= i; ++j) {\n                factorial = svmul_u32(factorial, svreinterpret_u32(svadd_z_u32(svreinterpret_u32(svconst_u32(j)), svconst_u32(1))));\n            }\n            result = svadd_z_s32(result, svreinterpret_s32(svmodu_z_u32(factorial, svconst_u32(10000))));\n        } else {\n            svuint32_t sum = svdup_n_u32(0);\n            for (int j = 1; j <= i; ++j) {\n                sum = svadd_z_u32(sum, svreinterpret_u32(svadd_z_u32(svreinterpret_u32(svconst_u32(j)), svconst_u32(1))));\n            }\n            result = svadd_z_s32(result, svreinterpret_s32(svmodu_z_u32(sum, svconst_u32(10000))));\n        }\n    }\n\n    std::vector<int> output(n);\n    svst1_u32(output.data(), result);\n    return output;\n}\n```"}
{"task_id": "SimdBench_113_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    svbool_t p = svptrue_b32();\n    svuint32_t count_even = svdup_u32(0);\n    svuint32_t count_odd = svdup_u32(0);\n\n    for (int i = 1; i <= n; ++i) {\n        svuint32_t num = svdup_u32(i);\n        svuint32_t reversed_num = svrev32(num);\n        svbool_t is_palindrome = svcmpeq_u32(num, reversed_num);\n\n        if (svany(is_palindrome)) {\n            svuint32_t is_even = svand_u32(svreinterpret_u32(svptrue_b32()), svreinterpret_u32(is_palindrome));\n            count_even = svadd_u32(count_even, is_even);\n            count_odd = svadd_u32(count_odd, svsub_z_u32(p, is_even));\n        }\n    }\n\n    std::vector<int> result;\n    result.push_back(svgetlane_u32(count_even, 0));\n    result.push_back(svgetlane_u32(count_odd, 0));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    svbool_t p = svptrue_b32();\n    svuint32_t count_even = svdup_u32(0);\n    svuint32_t count_odd = svdup_u32(0);\n\n    for (int i = 1; i <= n; ++i) {\n        svuint32_t num = svdup_u32(i);\n        svuint32_t reversed_num = svrev32(num);\n        svbool_t is_palindrome = svcmpeq_u32(num, reversed_num);\n\n        if (is_palindrome) {\n            svuint32_t is_even = svand_u32(svreinterpret_u32(svptrue_b32()), svreinterpret_u32(svcmpgt_u32(num, svdup_u32(0))));\n            count_even = svadd_u32(count_even, svselect(is_even, svone_u32(), svzero_u32()));\n            count_odd = svadd_u32(count_odd, svselect(!is_even, svone_u32(), svzero_u32()));\n        }\n    }\n\n    int even = svfirst_m_b32(p, count_even);\n    int odd = svfirst_m_b32(p, count_odd);\n\n    return {even, odd};\n}\n```"}
{"task_id": "SimdBench_113_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    svint32_t count_even = svdup_n_s32(0);\n    svint32_t count_odd = svdup_n_s32(0);\n\n    for (int i = 1; i <= n; ++i) {\n        if (is_palindrome(i)) {\n            if (i % 2 == 0) {\n                count_even = svadd(count_even, svdup_n_s32(1));\n            } else {\n                count_odd = svadd(count_odd, svdup_n_s32(1));\n            }\n        }\n    }\n\n    std::vector<int> result;\n    int even_value = svfirst(count_even);\n    int odd_value = svfirst(count_odd);\n    result.push_back(even_value);\n    result.push_back(odd_value);\n\n    return result;\n}\n\nbool is_palindrome(int num) {\n    // Helper function to check if a number is a palindrome\n    // Implementation details depend on the specific requirements\n}\n```"}
{"task_id": "SimdBench_113_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    svint32_t count = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (int i = 1; i <= n; ++i) {\n        svint32_t num = svdup_n_s32(i);\n        svint32_t rev_num = svrev32(num);\n        svbool_t is_palindrome = svcmpeq_z(mask, num, rev_num);\n\n        if (svany(is_palindrome)) {\n            svint32_t parity = svand_z(mask, num, svdup_n_s32(1));\n            count = svadd(count, svreinterpret_u32(parity));\n        }\n    }\n\n    int even_count = svgetlane(count, 0);\n    int odd_count = svgetlane(count, 1);\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    svint32_t count_even = svdup_n_s32(0);\n    svint32_t count_odd = svdup_n_s32(0);\n\n    for (int i = 1; i <= n; ++i) {\n        svbool_t is_palindrome = svfalse_b32();\n        int num = i;\n        while (num > 0) {\n            int digit = num % 10;\n            if (digit != num / 10) {\n                break;\n            }\n            num /= 10;\n        }\n        if (num == 0) {\n            is_palindrome = svtrue_b32();\n        }\n\n        svint32_t increment = svdup_n_s32(is_palindrome ? 1 : 0);\n        count_even = svadd(count_even, svcmov(is_palindrome, increment, svdup_n_s32(0)));\n        count_odd = svadd(count_odd, svcmov(!is_palindrome, increment, svdup_n_s32(0)));\n    }\n\n    std::vector<int> result(2);\n    result[0] = svgetlane(count_even, 0);\n    result[1] = svgetlane(count_odd, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    for (size_t i = 0; i < nums.size(); i += SV_INT32_SIZE) {\n        svint32_t v = svld1_u32(nums.data() + i);\n        svuint32_t abs_v = svabs_u32(v);\n        svuint32_t sum = svaddv_u32(abs_v);\n        svbool_t mask = svcmpgt_z_u32(sum, svdup_u32(0));\n        count += svcntb(mask);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    svbool_t mask = svptrue_b32();\n    svuint32_t sum = svdup_u32(0);\n    svuint32_t nums = svld1_u32(n.data());\n\n    while (!svptest_any(mask)) {\n        svuint32_t abs_nums = svabs_u32(nums);\n        svuint32_t digits = svclz_u32(abs_nums);\n        svuint32_t valid_digits = svsub_u32(digits, svdup_u32(1));\n        svuint32_t digit_sum = svtbl_u32(svreinterpret_u32(abs_nums), valid_digits);\n        sum = svadd_u32(sum, digit_sum);\n\n        mask = svcmpgt_z_u32(sum, svdup_u32(0));\n        nums = svld1_u32(&n[count * 8]);\n        count += 8;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t ptrue = svptrue_b32();\n    svuint32_t sum_digits = svdup_u32(0);\n    svuint32_t num_vec = svld1_u32(nums.data(), sv_dup_imm_u32(nums.size()));\n\n    for (size_t i = 0; i < nums.size(); i += SV_INT32_SIZE) {\n        svuint32_t abs_num = svabs_u32(num_vec);\n        svuint32_t digits = svclz_u32(abs_num);\n        svuint32_t mask = svcmov_u32(ptrue, svcmpgt_z_u32(digits, svdup_u32(0)), svdup_u32(0));\n        sum_digits = svadd_u32(sum_digits, svand_u32(mask, svreinterpret_u32(svreinterpret_f32(abs_num))));\n    }\n\n    count = svsum_u32(sum_digits);\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t ptrue = svptrue_b32();\n    svuint32_t sum_digits = svdup_u32(0);\n    svint32_t nums_vec = svld1_s32(nums.data(), sv_dup_imm_u32(nums.size()));\n\n    for (size_t i = 0; i < nums.size(); i += SV_INT32_SIZE) {\n        svint32_t num = svld1_s32(&nums[i], sv_dup_imm_u32(SV_INT32_SIZE));\n        svuint32_t abs_num = svabs_s32(num);\n        svuint32_t digits = svclz_u32(abs_num) >> 5;\n        svuint32_t digit_sum = svaddlv_u32(digits);\n        sum_digits = svadd_u32(sum_digits, digit_sum);\n    }\n\n    svuint32_t non_zero = svcmpne_u32(sum_digits, svdup_u32(0), ptrue);\n    count = svcntb_u32(non_zero);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nint count_nums_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t sum_digits = svdup_u32(0);\n    svuint32_t num_vec = svld1_u32(nums.data());\n\n    for (size_t i = 0; i < nums.size(); i += SV_INT32_SIZE) {\n        svuint32_t abs_num = svabs_u32(num_vec);\n        svuint32_t digits = svclz_u32(abs_num);\n        svuint32_t valid = svcmple_u32(digits, svdup_u32(SV_INT32_SIZE));\n        svuint32_t digit_sum = svaddlv_u32(svand_u32(abs_num, svdup_u32(0x0F)));\n        sum_digits = svadd_u32(sum_digits, svsel(valid, digit_sum, svdup_u32(0)));\n\n        if (svptest_any(p, svcmpgt_z_u32(sum_digits, svdup_u32(0)))) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    svint32_t *sv_arr = reinterpret_cast<svint32_t*>(arr.data());\n    svint32_t zero = svdup_n_s32(0);\n    svint32_t one = svdup_n_s32(1);\n\n    for (int i = 0; i < n; ++i) {\n        svint32_t shifted = svext_x_s32(sv_arr[i], sv_arr[i], n - i);\n        svint32_t sorted = svtbl_z_s32(shifted, svindex_u32(n));\n        svint32_t mask = svcmpge_z_s32(sorted, shifted);\n        if (!svall_true(mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    svint32_t *sv_arr = reinterpret_cast<svint32_t*>(arr.data());\n    svint32_t *sv_end = sv_arr + (n / 16);\n\n    while (sv_arr != sv_end) {\n        svint32_t v0 = svld1_u32(sv_arr);\n        svint32_t v1 = svld1_u32(sv_arr + 16);\n        svint32_t v2 = svld1_u32(sv_arr + 32);\n        svint32_t v3 = svld1_u32(sv_arr + 48);\n        svint32_t v4 = svld1_u32(sv_arr + 64);\n        svint32_t v5 = svld1_u32(sv_arr + 80);\n        svint32_t v6 = svld1_u32(sv_arr + 96);\n        svint32_t v7 = svld1_u32(sv_arr + 112);\n        svint32_t v8 = svld1_u32(sv_arr + 128);\n        svint32_t v9 = svld1_u32(sv_arr + 144);\n        svint32_t va = svld1_u32(sv_arr + 160);\n        svint32_t vb = svld1_u32(sv_arr + 176);\n        svint32_t vc = svld1_u32(sv_arr + 192);\n        svint32_t vd = svld1_u32(sv_arr + 208);\n\n        svint32_t v0_shifted = svextw_n_u32(v0, 32);\n        svint32_t v1_shifted = svextw_n_u32(v1, 32);\n        svint32_t v2_shifted = svextw_n_u32(v2, 32);\n        svint32_t v3_shifted = svextw_n_u32(v3, 32);\n        svint32_t v4_shifted = svextw_n_u32(v4, 32);\n        svint32_t v5_shifted = svextw_n_u32(v5, 32);\n        svint32_t v6_shifted = svextw_n_u32(v6, 32);\n        svint32_t v7_shifted = svextw_n_u32(v7, 32);\n        svint32_t v8_shifted = svextw_n_u32(v8, 32);\n        svint32_t v9_shifted = svextw_n_u32(v9, 32);\n        svint32_t va_shifted = svextw_n_u32(va, 32);\n        svint32_t vb_shifted = svextw_n_u32(vb, 32);\n        svint32_t vc_shifted = svextw_n_u32(vc, 32);\n        svint32_t vd_shifted = svextw_n_u32(vd, 32);\n\n        svbool_t mask0 = svlt_z_s32(v0, v0_shifted);\n        svbool_t mask1 = svlt_z_s32(v1, v1_shifted);\n        svbool_t mask2 = svlt_z_s32(v2, v2_shifted);\n        svbool_t mask3 = svlt_z_s32(v3, v3_shifted);\n        svbool_t mask4 = svlt_z_s32(v4, v4_shifted);\n        svbool_t mask5 = svlt_z_s32(v5, v5_shifted);\n        svbool_t mask6 = svlt_z_s32(v6, v6_shifted);\n        svbool_t mask7 = svlt_z_s32(v7, v7_shifted);\n        svbool_t mask8 = svlt_z_s32(v8, v8_shifted);\n        svbool_t mask9 = svlt_z_s32(v9, v9_shifted);\n        svbool_t maska = svlt_z_s32(va, va_shifted);\n        svbool_t maskb = svlt_z_s32(vb, vb_shifted);\n        svbool_t maskc = svlt_z_s32(vc, vc_shifted);\n        svbool_t maskd = svlt_z_s32(vd, vd_shifted);\n\n        if (!svall(mask0) && !svall(mask1) && !svall(mask2) && !svall(mask3) &&\n            !svall(mask4) && !svall(mask5) && !svall(mask6) && !svall(mask7) &&\n            !svall(mask8) && !svall(mask9) && !svall(maska) && !svall(maskb) &&\n            !svall(maskc) && !svall(maskd)) {\n            return true;\n        }\n\n        svst1_u32(sv_arr, v0_shifted);\n        svst1_u32(sv_arr + 16, v1_shifted);\n        svst1_u32(sv_arr + 32, v2_shifted);\n        svst1_u32(sv_arr + 48, v3_shifted);\n        svst1_u32(sv_arr + 64, v4_shifted);\n        svst1_u32(sv_arr + 80, v5_shifted);\n        svst1_u32(sv_arr + 96, v6_shifted);\n        svst1_u32(sv_arr + 112, v7_shifted);\n        svst1_u32(sv_arr + 128, v8_shifted);\n        svst1_u32(sv_arr + 144, v9_shifted);\n        svst1_u32(sv_arr + 160, va_shifted);\n        svst1_u32(sv_arr + 176, vb_shifted);\n        svst1_u32(sv_arr + 192, vc_shifted);\n        svst1_u32(sv_arr + 208, vd_shifted);\n\n        sv_arr += 216;\n    }\n\n    // Handle remaining elements\n    for (int i = 0; i < n % 216; ++i) {\n        int shifted = arr[i];\n        for (int j = i; j > 0; --j) {\n            arr[j] = arr[j - 1];\n        }\n        arr[0] = shifted;\n\n        bool sorted = true;\n        for (int j = 1; j < n; ++j) {\n            if (arr[j] < arr[j - 1]) {\n                sorted = false;\n                break;\n            }\n        }\n        if (sorted) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    svint32_t *sv_arr = reinterpret_cast<svint32_t*>(arr.data());\n    svint32_t zero = svdup_n_s32(0);\n    svint32_t ones = svdup_n_s32(1);\n\n    for (int i = 0; i < n; ++i) {\n        svint32_t shifted = svext_n_s32(sv_arr[i], sv_arr[i], 1);\n        svint32_t sorted = svtst(shifted, sv_gt(shifted, sv_arr[i]));\n        if (!svptest_any(sorted)) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    svint32_t *sv_arr = reinterpret_cast<svint32_t*>(arr.data());\n    svint32_t *sv_end = sv_arr + n;\n\n    svint32_t min_val = sv_min(svld1(n, sv_arr));\n    svint32_t max_val = sv_max(svld1(n, sv_arr));\n\n    if (max_val - min_val >= n) {\n        return false;\n    }\n\n    svint32_t sorted_arr[n];\n    for (int i = 0; i < n; ++i) {\n        sorted_arr[i] = arr[(i + 1) % n];\n    }\n\n    svint32_t *sv_sorted_arr = reinterpret_cast<svint32_t*>(sorted_arr);\n\n    svbool_t is_sorted = svcmple_z_z(svld1(n, sv_arr), svld1(n, sv_sorted_arr));\n    return sve_all(is_sorted);\n}\n```"}
{"task_id": "SimdBench_115_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return true;\n    }\n\n    svint32_t *sv_arr = reinterpret_cast<svint32_t*>(arr.data());\n    svint32_t zero = svdup_n_s32(0);\n    svint32_t ones = svdup_n_s32(1);\n\n    for (int i = 0; i < n; ++i) {\n        svint32_t shifted = svext_r_s32(sv_arr[i], sv_arr[i], 1);\n        svint32_t sorted = svtbl_z_s32(shifted, svindex_s32(n, 0));\n        svint32_t mask = svcmpge_z_s32(sorted, shifted);\n        if (!svallb(mask)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_116_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    svbool_t mask = svcmple_z_s32(svptrue_b32(), svreinterpret_u32_u8(svld1_lane_u8((uint8_t*)lst1.data(), svdup_n_u8(0), 0)), svdup_n_s32(0));\n    int count = svcntb(mask);\n    if (count == lst1.size()) {\n        return \"YES\";\n    }\n    mask = svcmple_z_s32(svptrue_b32(), svreinterpret_u32_u8(svld1_lane_u8((uint8_t*)lst2.data(), svdup_n_u8(0), 0)), svdup_n_s32(0));\n    count = svcntb(mask);\n    if (count >= lst1.size() - count) {\n        return \"YES\";\n    }\n    return \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    svbool_t mask = svcmple_z_s32(svptrue_b32(), svreinterpret_u32_u8(svld1_lane_u8(lst1.data(), svdup_u8(0), 0)), svreinterpret_u32_u8(svld1_lane_u8(lst2.data(), svdup_u8(0), 0)));\n    if (svcntb(mask) == lst1.size()) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    svbool_t mask = svle(svdup_n_b8(1), svreinterpret_u8_z(svptrue_b8(), svld1_lane_u8(&lst1[0], svdup_u8(0), 0)));\n    if (svcntb(mask) == lst1.size()) {\n        return \"YES\";\n    }\n    mask = svle(svdup_n_b8(1), svreinterpret_u8_z(svptrue_b8(), svld1_lane_u8(&lst2[0], svdup_u8(0), 0)));\n    if (svcntb(mask) == lst2.size()) {\n        return \"NO\";\n    }\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    svbool_t mask = svptrue_b32();\n    svint32_t v_lst1 = svld1_s32(mask, lst1.data());\n    svint32_t v_lst2 = svld1_s32(mask, lst2.data());\n\n    svint32_t v_even_mask = svcmple_s32(v_lst1, svdup_imm_s32(0));\n    svint32_t v_odd_mask = svcmplt_s32(v_lst1, svdup_imm_s32(0));\n\n    svint32_t v_even_count = svtbl_s32(svones_s32(), v_even_mask);\n    svint32_t v_odd_count = svtbl_s32(svones_s32(), v_odd_mask);\n\n    if (svgetlane_u32(v_even_count, 0) == lst1.size()) {\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    svbool_t mask = svle(svdup_n_b8(0), svreinterpret_u8_z(svptrue_b8(), svld1_lane_u8(lst1.data(), svdup_u8(0), 0)));\n    if (svcntb(mask) == lst1.size()) {\n        return \"YES\";\n    }\n    mask = svle(svdup_n_b8(0), svreinterpret_u8_z(svptrue_b8(), svld1_lane_u8(lst2.data(), svdup_u8(0), 0)));\n    if (svcntb(mask) == lst2.size()) {\n        return \"NO\";\n    }\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_117_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        svbool_t mask = svfalse_b8();\n        svuint8_t vec = svdup_u8(0);\n        for (char c : str) {\n            uint8_t digit = c - '0';\n            if (digit % 2 != 0) {\n                mask = svmul_b8(mask, svtrue_b8());\n                vec = svadd_x_u8(vec, svreinterpret_u8_u32(svsetlane_u32(digit, svdup_u32(0), 0)));\n            }\n        }\n        int count = svget_lane_u32(svreinterpret_u32_u8(vec), 0);\n        std::string new_str = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(new_str);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    svbool_t p = svptrue_b8();\n    for (const auto& str : lst) {\n        size_t len = str.size();\n        svuint8_t vec = svld1_u8(str.data());\n        svuint8_t mask = svcmple_u8(vec, svdup_u8('9'));\n        svuint8_t odd_mask = svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('1')), svdup_u8('0')),\n                                     svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('3')), svdup_u8('0')),\n                                              svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('5')), svdup_u8('0')),\n                                                       svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('7')), svdup_u8('0')),\n                                                                svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('9')), svdup_u8('0')))))));\n        svuint32_t count = svcntb_u32(odd_mask);\n        std::string s = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(s);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        svbool_t mask = svfalse_b8();\n        svuint8_t vec = svdup_u8(0);\n        for (size_t i = 0; i < str.size(); ++i) {\n            uint8_t digit = str[i] - '0';\n            if (digit % 2 != 0) {\n                mask = svmul_b8(mask, svtrue_b8());\n                vec = svadd_x_u8(vec, svreinterpret_u8_u64(svmov_n_u64(digit)));\n            }\n        }\n        size_t count = svpopcntb(mask);\n        std::string temp = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(temp);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    svbool_t p = svptrue_b8();\n    for (const auto& str : lst) {\n        svuint8_t vec = svld1_u8(str.data());\n        svuint8_t mask = svcmple_u8(vec, svdup_u8('9'));\n        svuint8_t odd_mask = svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('1')), svdup_u8('0')),\n                                     svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('3')), svdup_u8('0')),\n                                              svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('5')), svdup_u8('0')),\n                                                       svand_u8(svcmpne_u8(svand_u8(vec, svdup_u8('7')), svdup_u8('0'))))));\n        svuint32_t count = svcntb_u32(odd_mask);\n        std::string s = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(count) + \"ng \" + std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\";\n        result.push_back(s);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    svbool_t p = svptrue_b8();\n    for (const auto & str : lst) {\n        svuint8_t vec = svld1_u8(str.data());\n        svuint8_t mask = svcmplt_u8(vec, svdup_u8('0'));\n        svuint8_t count = svcntb(mask);\n        std::string s = \"the number of odd elements \";\n        s += std::to_string(count);\n        s += \"n the str\";\n        s += std::to_string(count);\n        s += \"ng \";\n        s += std::to_string(count);\n        s += \" of the \";\n        s += std::to_string(count);\n        s += \"nput.\";\n        result.push_back(s);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SVE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    if (n == 0) return 0;\n\n    svint64_t v_min_sum = svdup_n_s64(nums[0]);\n    svint64_t v_current_sum = svdup_n_s64(nums[0]);\n\n    for (size_t i = 1; i < n; ++i) {\n        svint64_t v_num = svdup_n_s64(nums[i]);\n        v_current_sum = svmax_s64(v_current_sum, svadd_s64(v_current_sum, v_num));\n        v_min_sum = svmin_s64(v_min_sum, v_current_sum);\n    }\n\n    return svgetlane_s64(v_min_sum, 0);\n}\n```"}
{"task_id": "SimdBench_118_SVE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    if (n == 0) return 0;\n\n    svint64_t v_min_sum = svdup_n_s64(nums[0]);\n    svint64_t v_current_sum = svdup_n_s64(nums[0]);\n\n    for (size_t i = 1; i < n; ++i) {\n        svint64_t v_num = svdup_n_s64(nums[i]);\n        v_current_sum = svmax_s64(v_current_sum, svadd_s64(v_current_sum, v_num));\n        v_min_sum = svmin_s64(v_min_sum, v_current_sum);\n    }\n\n    return svgetlane_s64(v_min_sum, 0);\n}\n```"}
{"task_id": "SimdBench_118_SVE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    if (n == 0) return 0;\n\n    svint64_t v_min_sum = svdup_n_s64(nums[0]);\n    svint64_t v_current_sum = svdup_n_s64(nums[0]);\n\n    for (size_t i = 1; i < n; ++i) {\n        svint64_t v_num = svdup_n_s64(nums[i]);\n        v_current_sum = svmax_s64(v_current_sum, svadd_s64(v_current_sum, v_num));\n        v_min_sum = svmin_s64(v_min_sum, v_current_sum);\n    }\n\n    return svgetlane_s64(v_min_sum, 0);\n}\n```"}
{"task_id": "SimdBench_118_SVE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    if (n == 0) return 0;\n\n    svint64_t v_min_sum = svdup_n_s64(nums[0]);\n    svint64_t v_current_sum = svdup_n_s64(nums[0]);\n\n    for (size_t i = 1; i < n; ++i) {\n        svint64_t v_num = svdup_n_s64(nums[i]);\n        v_current_sum = svmax_s64(v_current_sum, svadd_s64(v_current_sum, v_num));\n        v_min_sum = svmin_s64(v_min_sum, v_current_sum);\n    }\n\n    return svgetlane_s64(v_min_sum, 0);\n}\n```"}
{"task_id": "SimdBench_118_SVE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t min_sum = INT64_MAX;\n    svint64_t v_min_sum = svdup_n_s64(INT64_MAX);\n    svint64_t v_nums = svld1_s64(nums.data());\n    svint64_t v_current_sum = svdup_n_s64(0);\n\n    for (size_t i = 0; i < nums.size(); ++i) {\n        v_current_sum = svmax_s64(v_current_sum, svadd_s64(v_current_sum, v_nums));\n        v_min_sum = svmin_s64(v_min_sum, v_current_sum);\n    }\n\n    return svgetlane_s64(v_min_sum, 0);\n}\n```"}
{"task_id": "SimdBench_119_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    svbool_t p = svptrue_b32();\n    uint32_t total_water = 0;\n    size_t num_wells = grid.size();\n    size_t well_width = grid[0].size();\n\n    for (size_t i = 0; i < num_wells; i += 8) {\n        svuint32_t sum = svdup_u32(0);\n        for (size_t j = 0; j < well_width; j += 32) {\n            svuint32_t vec = svld1_u32(&grid[i][j]);\n            sum = svadd_u32(sum, svreinterpret_u32(svpcount_m(p, vec)));\n        }\n        total_water += svsum_u32(sum);\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    svbool_t p = svptrue_b32();\n    uint32_t total_water = 0;\n    size_t num_wells = grid.size();\n    size_t well_length = grid[0].size();\n\n    for (size_t i = 0; i < num_wells; i += SV_INT32_SIZE) {\n        svuint32_t sum = svdup_u32(0);\n        for (size_t j = 0; j < well_length; j += SV_INT32_SIZE) {\n            svuint32_t well_row = svld1_u32(&grid[i][j]);\n            sum = svadd_u32(sum, well_row);\n        }\n        total_water += svsum_u32(sum);\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_SVE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    svbool_t p = svptrue_b32();\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    for (int i = 0; i < rows; i += 8) {\n        svuint32_t sum = svdup_u32(0);\n        for (int j = 0; j < cols; j++) {\n            svuint32_t vec = svld1_u32(&grid[i][j]);\n            sum = svadd_u32(sum, vec);\n        }\n        total_water += svreinterpret_u32_u8(svclz_u32(svreinterpret_u32_u8(sum)));\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_SVE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    svbool_t p = svptrue_b8();\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    for (int i = 0; i < rows; i += 16) {\n        svuint8_t sum = svdup_u8(0);\n        for (int j = 0; j < cols; j += 16) {\n            svuint8_t vec = svld1_u8(&grid[i][j]);\n            sum = svaddb_u8(sum, vec);\n        }\n        svuint32_t count = svreinterpret_u32_u8(svclz_u8(svreinterpret_u8_u32(sum)));\n        total_water += svgetlane_u32(count, 0);\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_SVE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    svbool_t p = svptrue_b8();\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    for (int i = 0; i < rows; i += 16) {\n        svuint8_t sum = svdup_u8(0);\n        for (int j = 0; j < cols; j += 16) {\n            svuint8_t col_data = svld1_u8(&grid[i][j]);\n            sum = svadd_u8(sum, col_data);\n        }\n        svuint32_t sum_vec = svreinterpret_u32_u8(sum);\n        uint32_t sum_val = svgetlane_u32(sum_vec, 0);\n        total_water += (sum_val + capacity - 1) / capacity;\n    }\n\n    return total_water;\n}\n```"}
{"task_id": "SimdBench_120_SVE", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    svbool_t p = svptrue_b16();\n    svuint16_t v = svld1_u16(lst.data());\n    svuint16_t mask = svcmple_z_u16(v, svdup_u16(1));\n    svuint16_t odd_elements = svand_u16(v, svreinterpret_u16(mask));\n    svuint16_t even_positions = svand_u16(svreinterpret_u16(svptrue_e16()), svreinterpret_u16(svmul_n_u16(svreinterpret_u16(svptrue_e16()), svdup_u16(2))));\n    svuint16_t result = svand_u16(odd_elements, even_positions);\n    sum += svsum_u16(result);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SVE", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    svbool_t p = svptrue_b16();\n    svuint16_t v = svld1_u16(lst.data());\n    svuint16_t mask = svcmple_z_u16(v, svdup_u16(1));\n    svuint16_t odd_elements = svand_u16(v, svreinterpret_u16(mask));\n    svuint16_t even_positions = svand_u16(svreinterpret_u16(svptrue_e16()), svreinterpret_u16(svclrsb_x_u16(svreinterpret_u16(svadd_n_u16(svreinterpret_u16(svptrue_e16()), svreinterpret_u16(svptrue_e16()))))));\n    svuint16_t result = svand_u16(odd_elements, even_positions);\n    sum += svsum_u16(result);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SVE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    svbool_t p = svptrue_b16();\n    svuint16_t vsum = svdup_u16(0);\n    size_t n = lst.size();\n\n    for (size_t i = 0; i + 16 <= n; i += 16) {\n        svuint16_t vec = svld1_u16(lst.data() + i);\n        svuint16_t mask = svand_u16(sveor_u16(vec, svadd_x_u16(vec, svdup_u16(1))), svdup_u16(1));\n        svuint16_t even_mask = svand_u16(mask, svreinterpret_u16(svclrsb_x_b16(sveor_u16(svclrsb_x_b16(svreinterpret_u16(vec)), svreinterpret_u16(svclrsb_x_b16(svreinterpret_u16(vec), svreinterpret_u16(svclrsb_x_b16(svreinterpret_u16(vec))))))))));\n        vsum = svaddlv_u16(vsum, svand_u16(vec, even_mask));\n    }\n\n    for (size_t i = n - n % 16; i < n; ++i) {\n        if ((lst[i] & 1) && (i % 2 == 0)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum + svgetlane_u32(svreinterpret_u32(vsum), 0);\n}\n```"}
{"task_id": "SimdBench_120_SVE", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    svbool_t p = svptrue_b16();\n    svuint16_t v = svld1_u16(lst.data());\n    svuint16_t mask = svcmple_z_u16(v, svdup_u16(1));\n    svuint16_t odd_elements = svand_u16(v, svreinterpret_u16(mask));\n    svuint16_t even_positions = svand_u16(svadd_x_u16(svreinterpret_u16(p), svdup_u16(1)), svdup_u16(2));\n    svuint16_t result = svand_u16(odd_elements, even_positions);\n    sum += svsum_u16(result);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SVE", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t sum = 0;\n    svbool_t p = svptrue_b16();\n    svuint16_t v = svld1_u16(lst.data());\n    svuint16_t mask = svcmple_z_u16(v, svdup_u16(1));\n    svuint16_t odd_elements = svand_u16(v, svreinterpret_u16(mask));\n    svuint16_t even_positions = svand_u16(svadd_x_u16(svreinterpret_u16(p), svdup_u16(1)), svdup_u16(2));\n    svuint16_t result = svand_u16(odd_elements, even_positions);\n    sum += svsum_u16(result);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SVE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k) {\n    svint32_t sum = svdup_n_s32(0);\n    int i = 0;\n    for (; i + 16 <= k; i += 16) {\n        svint32_t vec = svld1_s32(&arr[i]);\n        svint32_t abs_vec = svabs_s32(vec);\n        svbool_t mask = svtstle_u32(abs_vec, svdup_n_u32(99));\n        svint32_t filtered_vec = svsel(mask, vec, svdup_n_s32(0));\n        sum = svadd_s32(sum, svsum_s32(filtered_vec));\n    }\n    for (; i < k; ++i) {\n        if (std::abs(arr[i]) <= 99) {\n            sum = svadd_s32(sum, svdup_n_s32(arr[i]));\n        }\n    }\n    return svgetlane_s32(svreinterpret_s32_u32(sum), 0);\n}\n```"}
{"task_id": "SimdBench_121_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    svuint32_t vec_sum = svdup_u32(0);\n    for (int i = 0; i < k; ++i) {\n        svuint32_t vec_val = svreinterpret_u32_u8(sv_dup_n_u8(arr[i]));\n        svuint32_t mask = svcmple_u32(vec_val, svdup_u32(99));\n        vec_sum = svadd_u32(vec_sum, svand_u32(vec_val, mask));\n    }\n    uint32_t result;\n    svst1_lane_u32(&result, vec_sum, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    svuint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svle_u32(vec, svdup_u32(99));\n    svuint32_t filtered = svsel(mask, vec, svzero_u32());\n    uint32_t sum = svaddv_u32(filtered);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    svuint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svle_u32(vec, svdup_u32(99));\n    svuint32_t filtered = svsel(mask, vec, svzero_u32());\n    uint32_t sum = svaddv_u32(filtered);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    svuint32_t vec = svld1_u32(arr.data());\n    svbool_t mask = svle_u32(vec, svdup_u32(99));\n    svuint32_t filtered = svsel(mask, vec, svdup_u32(0));\n    uint32_t sum = svaddv_u32(filtered);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    svint32_t vec = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(lst.data()));\n    svint32_t next_vec = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(lst.data()) + 1);\n    svbool_t is_sorted = svle(vec, next_vec);\n\n    for (size_t i = 2; i < lst.size(); ++i) {\n        vec = next_vec;\n        next_vec = svld1(svptrue_b32(), reinterpret_cast<const int32_t*>(lst.data()) + i);\n        is_sorted &= svle(vec, next_vec);\n    }\n\n    return svall(is_sorted);\n}\n```"}
{"task_id": "SimdBench_122_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    svint32_t vec = svld1_u32(lst.data());\n    svint32_t dup = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 1; i < lst.size(); ++i) {\n        svint32_t next_vec = svld1_u32(&lst[i]);\n        svbool_t cmp = svcmpgt_z_s32(mask, vec, next_vec);\n        if (svptest_any(cmp)) {\n            return false;\n        }\n        dup = svdup_n_s32(svmaxv_s32(dup, vec));\n        vec = next_vec;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    svint32_t vec = svld1_u32(lst.data());\n    svint32_t dup_vec = svdup_n_s32(0);\n    svuint32_t mask = svcmple_z_s32(vec, svld1_u32(lst.data() + 1));\n    svuint32_t dup_mask = svtst(mask, dup_vec);\n    if (svany(dup_mask)) {\n        return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    svint32_t vec = svld1_u32(lst.data());\n    svint32_t dup = svdup_n_s32(0);\n    svbool_t mask = svtrue_b32();\n\n    for (size_t i = 1; i < lst.size(); ++i) {\n        svint32_t next_vec = svld1_u32(&lst[i]);\n        svbool_t cmp = svcmpgt_z_s32(mask, vec, next_vec);\n        if (svfirst_m_b32(cmp) != SV_ALL_FALSE_B32) {\n            return false;\n        }\n        svint32_t dup_next = svdup_n_s32(lst[i]);\n        svbool_t dup_mask = svelt_leq_z_s32(mask, dup, dup_next);\n        if (svfirst_m_b32(dup_mask) != SV_ALL_FALSE_B32) {\n            return false;\n        }\n        dup = dup_next;\n        vec = next_vec;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nbool is_sorted_simd(const std::vector<int>& lst) {\n    svint32_t vec = svld1_u32(lst.data());\n    svint32_t vec_next = svld1_u32(lst.data() + 1);\n    svbool_t is_sorted = svle(vec, vec_next);\n\n    for (size_t i = 2; i < lst.size(); ++i) {\n        vec = svld1_u32(lst.data() + i - 1);\n        vec_next = svld1_u32(lst.data() + i);\n        is_sorted &= svle(vec, vec_next);\n    }\n\n    return svall(is_sorted);\n}\n```"}
{"task_id": "SimdBench_123_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    svint32_t sum = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < arr.size(); i += SV_INT32_SIZE) {\n        size_t count = std::min(SV_INT32_SIZE, arr.size() - i);\n        svint32_t vec = svld1_u32(arr.data() + i);\n\n        svint32_t abs_vec = svabs_s32(vec);\n        sum = svadd_s32(sum, abs_vec);\n\n        svint32x4_t sign_vec = svsign_s32(vec);\n        svint32_t sign_product = svpmul_z_s32(mask, sign_vec, sign_vec);\n        sign_product = svreinterpret_s32_u32(svclz_s32(sign_product));\n        sign_product = svsub_n_s32(sign_product, 31);\n        sign_product = svand_n_s32(sign_product, 1);\n        sum = svsub_s32(sum, svreinterpret_s32_u32(sign_product));\n    }\n\n    int result = svgetlane_s32(sum, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    svint32_t sum = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < arr.size(); i += SV_INT32_SIZE) {\n        size_t count = std::min(SV_INT32_SIZE, arr.size() - i);\n        svint32_t vec = svld1_s32(arr.data() + i);\n\n        svint32_t abs_vec = svabs_s32(vec);\n        sum = svadd_s32(sum, abs_vec);\n\n        svint32x4_t sign_vec = svsign_s32(vec);\n        svint32_t sign_product = svpmul_s32(svpmul_s32(sign_vec[0], sign_vec[1]), svpmul_s32(sign_vec[2], sign_vec[3]));\n        sum = svsub_s32(sum, svcmov_s32(mask, sign_product, sum));\n    }\n\n    int result = svgetlane_s32(sum, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    svint32_t sum_magnitudes = svdup_n_s32(0);\n    svint32_t product_signs = svdup_n_s32(1);\n\n    size_t n = arr.size();\n    size_t i = 0;\n\n    for (; i + 15 < n; i += 16) {\n        svint32_t vec = svld1_u32(arr.data() + i);\n        svint32_t abs_vec = svabs_s32(vec);\n        sum_magnitudes = svadd_s32(sum_magnitudes, svreinterpret_s32(svclz_s32(abs_vec)));\n\n        svint32x4_t sign_vec = svsign_s32(vec);\n        svint32_t sign_product = svpmul_s32(sign_vec, sign_vec);\n        product_signs = svand_s32(product_signs, svreinterpret_s32(sign_product));\n    }\n\n    for (; i < n; ++i) {\n        int val = arr[i];\n        sum_magnitudes = svadd_s32(sum_magnitudes, svdup_n_s32(val > 0 ? 1 : (val < 0 ? -1 : 0)));\n        product_signs = svand_s32(product_signs, svdup_n_s32(val != 0 ? (val > 0 ? 1 : -1) : 0));\n    }\n\n    int result = svget_lane_s32(sum_magnitudes, 0);\n    result *= svget_lane_s32(product_signs, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    svint32_t sum = svdup_n_s32(0);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < arr.size(); i += SV_INT32_SIZE) {\n        size_t count = std::min(SV_INT32_SIZE, arr.size() - i);\n        svint32_t vec = svld1_s32(arr.data() + i);\n\n        svint32_t abs_vec = svabs_s32(vec);\n        sum = svadd_s32(sum, abs_vec);\n\n        svint32x4_t sign_vec = svsign_s32(vec);\n        svint32_t sign_product = svpmul_z_s32(mask, sign_vec, sign_vec);\n        sign_product = svreinterpret_u32_s32(sign_product);\n        sign_product = svand_s32(svreinterpret_u32_s32(sum), svreinterpret_u32_s32(sign_product));\n        sum = svreinterpret_s32_u32(sign_product);\n    }\n\n    int result = svgetlane_s32(sum, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_SVE", "completion": "```c++\nint prod_signs_simd(const std::vector<int>& arr) {\n    if (arr.empty()) {\n        return -32768;\n    }\n\n    svint16_t sum = svdupw_z_s16(0);\n    svbool_t mask = svptrue_b16();\n\n    for (size_t i = 0; i < arr.size(); i += SV_INT16_SIZE) {\n        size_t count = std::min(SV_INT16_SIZE, arr.size() - i);\n        svint16_t vec = svld1_u16(arr.data() + i);\n\n        sum = svaddw_m(mask, sum, svabs_m(mask, vec));\n    }\n\n    int result = svget_lane_s16(sum, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SVE code here\n}\n```"}
{"task_id": "SimdBench_124_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SVE code here\n}\n```"}
{"task_id": "SimdBench_124_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SVE code here\n}\n```"}
{"task_id": "SimdBench_124_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SVE implementation here\n}\n```"}
{"task_id": "SimdBench_124_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    // Your SVE code here\n}\n```"}
{"task_id": "SimdBench_125_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t digits_simd(uint64_t n) {\n    svbool_t mask = svptrue_b8();\n    uint64x2_t prod = svdup_u64(1);\n    while (n != 0) {\n        uint64x2_t digit = svreinterpret_u64_u8(svclz_u8(svdup_u8(n & 0xFF)));\n        digit = svsub_u64(svdup_u64(8), digit);\n        digit = svand_u64(digit, svreinterpret_u64_u8(svptrue_b8()));\n        prod = svmul_u64(prod, digit);\n        n >>= 8;\n    }\n    return svgetlane_u64(prod, 0);\n}\n```"}
{"task_id": "SimdBench_125_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t digits_simd(uint64_t n) {\n    svbool_t mask = svptrue_b8();\n    uint64x2_t prod = svdup_u64(1);\n    while (n != 0) {\n        uint64x2_t digit = svreinterpret_u64_u8(svclz_u8(svdup_u8(n % 10)));\n        digit = svand_u64(digit, svreinterpret_u64_u8(svptrue_e8()));\n        digit = svcmov_u64(mask, digit, svreinterpret_u64_u8(svptrue_e8()));\n        prod = svmul_u64(prod, digit);\n        n /= 10;\n    }\n    return svgetlane_u64(prod, 0);\n}\n```"}
{"task_id": "SimdBench_125_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t digits_simd(uint64_t n) {\n    svbool_t mask = svptrue_b8();\n    svuint8_t digits = svreinterpret_u8_u64(svset_lane_u64(n, svdup_u64(0), 0));\n    svuint8_t odd_digits = svand_u8(digits, svcmplt_u8(digits, svdup_u8(10)));\n    svuint8_t odd_mask = svcmpne_u8(odd_digits, svdup_u8(0));\n    svuint8_t product = svdup_u8(1);\n    while (!svptest_any(mask)) {\n        svuint8_t digit = svgetlane_u8(digits, 0);\n        if (digit % 2 != 0) {\n            product = svmul_u8(product, svdup_u8(digit));\n        }\n        digits = svextw_u8_u8(digits, svdup_u8(0), 7);\n    }\n    return svgetlane_u8(product, 0);\n}\n```"}
{"task_id": "SimdBench_125_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t digits_simd(uint64_t n) {\n    svbool_t mask = svptrue_b8();\n    uint64x2_t prod = svdup_u64(1);\n    while (n != 0) {\n        uint64x2_t digit = svreinterpret_u64_u8(svclz_u8(svdup_u8(n & 0xFF)));\n        digit = svand_u64(digit, svdup_u64(0x0F));\n        mask = svcmpne_u64(digit, svdup_u64(0));\n        prod = svmla_u64(prod, digit, mask);\n        n >>= 8;\n    }\n    return svgetlane_u64(prod, 0);\n}\n```"}
{"task_id": "SimdBench_125_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t digits_simd(uint64_t n) {\n    svbool_t p = svptrue_b8();\n    uint64x2_t prod = svdup_u64(1);\n    while (n != 0) {\n        uint64x2_t digit = svreinterpret_u64_u8(svclz_u8(svdup_u8(n & 0xFF)));\n        digit = svsub_u64(svdup_u64(8), digit);\n        digit = svand_u64(digit, svdup_u64(1));\n        prod = svmla_u64(prod, digit, svreinterpret_u64_u8(svdup_u8(n & 0xFF)));\n        n >>= 8;\n    }\n    return svgetlane_u64(prod, 0);\n}\n```"}
{"task_id": "SimdBench_126_SVE", "completion": "```c++\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    svfloat32_t sum = svdup_n_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        svfloat32_t vec = svld1_f32(lst.data() + i);\n        svfloat32_t ceil_vec = svei32_u_f32(vec);\n        svfloat32_t squares = svmul_f32(ceil_vec, ceil_vec);\n        sum = svadd_f32(sum, squares);\n    }\n    for (; i < n; ++i) {\n        float val = lst[i];\n        val = ceil(val);\n        sum = svadd_f32(sum, svdup_n_f32(val * val));\n    }\n    uint64_t result = 0;\n    svst1_lane_u64(&result, svreinterpret_u64_f32(sum), 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    uint64_t sum = 0;\n    svfloat32_t vsum = svdup_n_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i + 8 <= n) {\n        svfloat32_t vec = svld1_f32(lst.data() + i);\n        svfloat32_t ceil_vec = svcvteq_f32_u32(svcvtmq_u32_f32(svabs_f32(vec)));\n        svfloat32_t squares = svmul_f32(ceil_vec, ceil_vec);\n        vsum = svadd_f32(vsum, squares);\n        i += 8;\n    }\n\n    float32_t remaining[n % 8];\n    memcpy(remaining, lst.data() + i, sizeof(float) * (n % 8));\n    for (size_t j = 0; j < n % 8; ++j) {\n        float ceil_val = ceil(remaining[j]);\n        sum += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    float32_t result[1];\n    svst1_f32(result, vsum);\n    sum += static_cast<uint64_t>(result[0]);\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    svfloat32_t sum = svdup_n_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i + 8 <= n) {\n        svfloat32_t vec = svld1_f32(lst.data() + i);\n        svfloat32_t ceil_vec = svcvteq_f32_u32(svcvtmq_u32_f32(svabs_f32(vec)));\n        svfloat32_t squares = svmul_f32(ceil_vec, ceil_vec);\n        sum = svadd_f32(sum, squares);\n        i += 8;\n    }\n\n    while (i < n) {\n        float val = lst[i];\n        sum = svadd_f32(sum, svdup_n_f32(val * val));\n        i++;\n    }\n\n    uint64_t result = 0;\n    svst1_u64(&result, svreinterpret_u64_f32(sum));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    svfloat32_t sum = svdup_n_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i + 8 <= n) {\n        svfloat32_t vec = svld1_f32(&lst[i]);\n        svfloat32_t ceil_vec = svcvta_u32_f32(vec);\n        svfloat32_t squares = svmul_f32(ceil_vec, ceil_vec);\n        sum = svadd_f32(sum, squares);\n        i += 8;\n    }\n\n    while (i < n) {\n        float val = lst[i];\n        uint32_t ceil_val = static_cast<uint32_t>(std::ceil(val));\n        sum = svadd_f32(sum, svdup_n_f32(static_cast<float>(ceil_val * ceil_val)));\n        ++i;\n    }\n\n    return static_cast<uint64_t>(svreinterpret_u64_f32(svget_lane_f32(sum, 0)));\n}\n```"}
{"task_id": "SimdBench_126_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    svfloat32_t sum = svdup_n_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i + 8 <= n) {\n        svfloat32_t vec = svld1_u32((const uint32_t *)&lst[i]);\n        svfloat32_t ceil_vec = sveilp_u32(vec);\n        svfloat32_t squares = svmul_f32(ceil_vec, ceil_vec);\n        sum = svadd_f32(sum, squares);\n        i += 8;\n    }\n\n    while (i < n) {\n        float val = lst[i];\n        val = ceil(val);\n        sum = svadd_f32(sum, svdup_n_f32(val * val));\n        i++;\n    }\n\n    return static_cast<uint64_t>(svget_lane_f32(sum, 0));\n}\n```"}
{"task_id": "SimdBench_127_SVE", "completion": "```c++\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) {\n        return -1;\n    }\n\n    svint32_t v_arr = svld1_u32(arr.data());\n    svbool_t mask = svcmple_z_s32(n - 1, svld1_u32(&arr[1]));\n    svuint32_t indices = svindex_u32(n);\n    svuint32_t max_index = svdup_u32(-1);\n\n    for (int i = 0; i < n - 1; ++i) {\n        svbool_t current_mask = svcmple_z_s32(i, svld1_u32(&arr[i + 1]));\n        svuint32_t current_indices = svindex_u32(n);\n        svuint32_t current_max_index = svdup_u32(i);\n\n        svuint32_t selected_indices = svsel(current_mask, current_indices, max_index);\n        max_index = svmax_u32(max_index, selected_indices);\n    }\n\n    return svfirst_u32(max_index);\n}\n```"}
{"task_id": "SimdBench_127_SVE", "completion": "```c++\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    svint32_t v = svld1_u32(arr.data());\n    svint32_t vl = svdup_n_u32(n);\n    svbool_t mask = svcmple_z_s32(vl, svadd_z_s32(vl, svconst_i32(1)));\n\n    for (int i = 1; i < n; ++i) {\n        svint32_t v_next = svld1_u32(arr.data() + i);\n        svbool_t cmp_mask = svcmplt_z_s32(v, v_next);\n        mask = svand_b(mask, cmp_mask);\n        v = v_next;\n    }\n\n    int result = -1;\n    for (int i = 0; i < n; ++i) {\n        if (svget1_b(mask, i)) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SVE", "completion": "```c++\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    svint32_t v_prev = svld1_lane_s32(arr.data(), svdup_n_s32(INT_MIN), 0);\n    int idx = -1;\n\n    for (int i = 1; i < n; ++i) {\n        svint32_t v_curr = svld1_lane_s32(arr.data() + i, svdup_n_s32(INT_MIN), 0);\n        svbool_t mask = svle_s32(v_prev, v_curr);\n        if (svptest_any(mask)) {\n            idx = i;\n        }\n        v_prev = v_curr;\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_127_SVE", "completion": "```c++\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    svint32_t v_prev = svld1_lane_s32(arr.data(), svdup_n_s32(INT_MIN), 0);\n    int idx = -1;\n\n    for (int i = 1; i < n; ++i) {\n        svint32_t v_curr = svld1_lane_s32(arr.data() + i, svdup_n_s32(INT_MIN), 0);\n\n        svbool_t mask = svle_s32(v_prev, v_curr);\n        if (svptest_any(mask)) {\n            idx = i;\n        }\n\n        v_prev = v_curr;\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_127_SVE", "completion": "```c++\nint can_arrange_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return -1;\n\n    svint32_t v0 = svld1_u32(arr.data());\n    svint32_t v1 = svld1_u32(arr.data() + 1);\n    svbool_t mask = svcmple_z_s32(v0, v1);\n\n    int idx = -1;\n    for (int i = 1; i < n; ++i) {\n        if (!svptest_any(mask)) break;\n        idx = i;\n        v0 = v1;\n        v1 = svld1_u32(arr.data() + i + 1);\n        mask = svcmple_z_s32(v0, v1);\n    }\n\n    return idx;\n}\n```"}
{"task_id": "SimdBench_128_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    svint32_t vec = svld1_lane_u32(reinterpret_cast<const uint32_t*>(lst.data()), svdup_n_u32(0), 0);\n    int32_t max_neg = INT_MIN;\n    int32_t min_pos = INT_MAX;\n\n    for (size_t i = 1; i < lst.size(); ++i) {\n        svint32_t current_vec = svld1_lane_u32(reinterpret_cast<const uint32_t*>(&lst[i]), vec, 0);\n\n        svbool_t is_negative = svcmpgt_z_s32(current_vec, svdup_n_s32(0));\n        svbool_t is_positive = svcmplt_z_s32(current_vec, svdup_n_s32(0));\n\n        max_neg = svmax_s32(max_neg, svsel(is_negative, current_vec, svdup_n_s32(max_neg)));\n        min_pos = svmin_s32(min_pos, svsel(is_positive, current_vec, svdup_n_s32(min_pos)));\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    svint32_t vec = svld1_lane_u32(reinterpret_cast<const uint32_t*>(lst.data()), svdup_n_u32(0), 0);\n    int32_t max_neg = INT_MIN;\n    int32_t min_pos = INT_MAX;\n\n    for (size_t i = 1; i < lst.size(); ++i) {\n        svint32_t new_vec = svld1_lane_u32(reinterpret_cast<const uint32_t*>(&lst[i]), vec, i % 8);\n\n        svbool_t is_negative = svcmpgt_z_s32(svreinterpret_s32_u32(new_vec), svzero_s32());\n        svbool_t is_positive = svcmplt_z_s32(svreinterpret_s32_u32(new_vec), svzero_s32());\n\n        max_neg = svmax_s32(max_neg, svsel(is_negative, svreinterpret_s32_u32(new_vec), svdup_n_s32(max_neg)));\n        min_pos = svmin_s32(min_pos, svsel(is_positive, svreinterpret_s32_u32(new_vec), svdup_n_s32(min_pos)));\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    svint32_t vec = svld1_lane_s32(lst.data(), svdup_n_s32(0), 0);\n    int32_t max_neg = INT_MIN;\n    int32_t min_pos = INT_MAX;\n\n    for (size_t i = 1; i < lst.size(); ++i) {\n        svint32_t lane = svld1_lane_s32(lst.data() + i, vec, i % 8);\n        if (svany_gt_z(svmul_n_s32(lane, svdup_n_s32(-1)), svzero_s32())) {\n            max_neg = svmax_s32(max_neg, svgetlane_s32(lane, 0));\n        }\n        if (svany_gt_z(lane, svzero_s32())) {\n            min_pos = svmin_s32(min_pos, svgetlane_s32(lane, 0));\n        }\n    }\n\n    return {max_neg == INT_MIN ? 0 : max_neg, min_pos == INT_MAX ? 0 : min_pos};\n}\n```"}
{"task_id": "SimdBench_128_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    svint32_t vec = svld1_lane_s32(lst.data(), svdup_n_s32(0), 0);\n    int32_t max_neg = INT_MIN;\n    int32_t min_pos = INT_MAX;\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        svint32_t v = svld1_u32(lst.data() + i);\n        svbool_t mask_neg = svcmpgt_z_s32(v, svdup_n_s32(0));\n        svbool_t mask_pos = svcmple_z_s32(v, svdup_n_s32(0));\n\n        max_neg = svmax_s32(max_neg, svsel(mask_neg, v, svdup_n_s32(INT_MIN)));\n        min_pos = svmin_s32(min_pos, svsel(mask_pos, v, svdup_n_s32(INT_MAX)));\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    if (lst.empty()) {\n        return {0, 0};\n    }\n\n    svint32_t vec = svld1_lane_u32(reinterpret_cast<const uint32_t*>(lst.data()), svdup_n_u32(0), 0);\n    int32_t max_neg = INT_MIN;\n    int32_t min_pos = INT_MAX;\n\n    for (size_t i = 1; i < lst.size(); ++i) {\n        svint32_t current_vec = svld1_lane_u32(reinterpret_cast<const uint32_t*>(&lst[i]), vec, i % 8);\n\n        svbool_t is_negative = svcmpgt_z_s32(current_vec, svdup_n_s32(0));\n        svbool_t is_positive = svcmplt_z_s32(current_vec, svdup_n_s32(0));\n\n        max_neg = svmax_s32(max_neg, svsel(is_negative, current_vec, svdup_n_s32(max_neg)));\n        min_pos = svmin_s32(min_pos, svsel(is_positive, current_vec, svdup_n_s32(min_pos)));\n    }\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_129_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    svuint32_t indices = svindex_u32(n);\n    svuint32_t factorials[64];\n    uint64_t result = 1;\n\n    for (int i = 0; i < n; i++) {\n        svuint32_t factorial = svdup_u32(1);\n        for (int j = 1; j <= indices[i]; j++) {\n            factorial = svmul_u32(factorial, svdup_u32(j));\n        }\n        factorials[i] = svfirst_u32(factorial);\n    }\n\n    for (int i = 0; i < n; i++) {\n        result *= factorials[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    svbool_t p = svptrue_b32();\n    uint64_t result = 1;\n    for (int i = 1; i <= n; i++) {\n        svuint32_t vec_i = svdup_u32(i);\n        svuint32_t vec_fact = svdup_u32(result);\n        for (int j = 1; j < i; j++) {\n            vec_fact = svmul_u32(vec_fact, svdup_u32(j));\n        }\n        result *= svfirst_u32(svreinterpret_u32_u64(vec_fact));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    svbool_t p = svptrue_b32();\n    uint64_t result = 1;\n    for (uint64_t i = 1; i <= n; i++) {\n        svuint32_t vec_i = svdup_u32(i);\n        svuint32_t vec_fact = svdup_u32(result);\n        for (int j = 1; j < i; j++) {\n            vec_fact = svmul_u32(vec_fact, svdup_u32(j));\n        }\n        result *= svreinterpret_u64_u32(svpadup_z_u32(p, vec_fact))[0];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    svbool_t p = svptrue_b32();\n    uint64_t result = 1;\n    for (int i = 1; i <= n; i++) {\n        svuint32_t vec_i = svdup_u32(i);\n        svuint32_t vec_fact = svdup_u32(result);\n        svuint32_t vec_mul = svmul_u32(vec_fact, vec_i);\n        result = svreinterpret_u64(svdup_lane_u32(svgetlane_u32(vec_mul, 0), 0));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    svuint32_t indices = svindex_u32(n);\n    svuint32_t factorials[64];\n    uint64_t result = 1;\n\n    for (int i = 0; i < n; i++) {\n        svuint32_t current_index = svdup_u32(i + 1);\n        svuint32_t factorial = svdup_u32(1);\n\n        for (int j = 1; j <= i; j++) {\n            factorial = svmul_u32(factorial, svdup_u32(j));\n        }\n\n        factorials[i] = svgetlane_u32(factorial, 0);\n    }\n\n    for (int i = 0; i < n; i++) {\n        result *= factorials[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i + 8 <= n) {\n        svint32x4_t v = svld1_u32(lst.data() + i);\n        svint32x4_t v_sq = svmul_x(v, v);\n        svint32x4_t v_cubed = svmul_x(svmul_x(v, v), v);\n\n        svuint32x4_t mask_3 = svcmple_z_u32(svreinterpret_u32_s32(svmod_s32(svreinterpret_s32_u32(i), svdup_n_u32(3))), svdup_n_u32(0));\n        svuint32x4_t mask_4_not_3 = svcmple_z_u32(svreinterpret_u32_s32(svmod_s32(svreinterpret_s32_u32(i), svdup_n_u32(4))), svdup_n_u32(0));\n        mask_4_not_3 = svbic_u32(mask_4_not_3, mask_3);\n\n        v_sq = svsel(mask_3, v_sq, v);\n        v_cubed = svsel(mask_4_not_3, v_cubed, v);\n\n        sum += svaddv_s32(svadd_s32(svadd_s32(svgetlane_s32(v_sq, 0), svgetlane_s32(v_sq, 1)), svgetlane_s32(v_sq, 2)), svgetlane_s32(v_sq, 3)));\n        sum += svaddv_s32(svadd_s32(svadd_s32(svgetlane_s32(v_cubed, 0), svgetlane_s32(v_cubed, 1)), svgetlane_s32(v_cubed, 2)), svgetlane_s32(v_cubed, 3)));\n\n        i += 8;\n    }\n\n    while (i < n) {\n        int val = lst[i];\n        if (i % 3 == 0) {\n            sum += val * val;\n        } else if (i % 4 == 0) {\n            sum += val * val * val;\n        }\n        i++;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i + 8 <= n) {\n        svint32x4_t v = svld1_u32(lst.data() + i);\n        svint32x4_t v3 = svdup_n_s32(3);\n        svint32x4_t v4 = svdup_n_s32(4);\n\n        svuint32x4_t idx = svindex_u32(i, 8);\n        svbool_t m3 = svcmple_z_u32(idx, v3);\n        svbool_t m4 = svcmple_z_u32(idx, v4);\n        svbool_t m_not3 = svnot(m3);\n        svbool_t m_and = svand(m4, m_not3);\n\n        svint32x4_t v_squared = svmul_s32(v, v);\n        svint32x4_t v_cubed = svmul_s32(svmul_s32(v, v), v);\n        svint32x4_t v_result = svsel(m_and, v_cubed, svsel(m3, v_squared, v));\n\n        sum += svaddv_s32(v_result);\n        i += 8;\n    }\n\n    while (i < n) {\n        int val = lst[i];\n        if (i % 3 == 0) {\n            val *= val;\n        } else if (i % 4 == 0) {\n            val *= val * val;\n        }\n        sum += val;\n        ++i;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SVE", "completion": "```c++\nint sum_squares_simd(const std::vector<int> & lst){\n    int size = lst.size();\n    svint32_t *input = reinterpret_cast<svint32_t*>(const_cast<int*>(&lst[0]));\n    svint32_t result = svdup_n_s32(0);\n    \n    for(int i = 0; i < size; i += 8){\n        svint32_t vec = svld1_u32(input + i);\n        \n        svbool_t mask3 = svcmple_z_s32(svptrue_b32(), svindex_s32(i, 8), svdup_n_s32(3));\n        svint32_t squares = svmul_s32(vec, vec);\n        result = svadd(result, svsel(mask3, squares, result));\n        \n        svbool_t mask4_not_3 = svcmple_z_s32(svptrue_b32(), svindex_s32(i, 8), svdup_n_s32(4));\n        svbool_t mask4_and_not_3 = svandnot(mask4_not_3, mask3);\n        svint32_t cubes = svmul_s32(svmul_s32(vec, vec), vec);\n        result = svadd(result, svsel(mask4_and_not_3, cubes, result));\n    }\n    \n    return svget_lane_s32(result, 0);\n}\n```"}
{"task_id": "SimdBench_130_SVE", "completion": "```c++\n#include <arm_sve.h>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int size = lst.size();\n    svint32_t sum = svdup_n_s32(0);\n    \n    for (int i = 0; i < size; i += 16) {\n        svint32_t vec = svld1_u32(lst.data() + i);\n        \n        svbool_t mask3 = svcmple_z_s32(svptrue_b32(), svdup_n_s32(i), svdup_n_s32(i + 15));\n        svbool_t mask4 = svcmple_z_s32(svptrue_b32(), svdup_n_s32(i), svdup_n_s32(i + 12));\n        \n        svint32_t squared = svmul_s32(vec, vec);\n        svint32_t cubed = svmul_s32(svmul_s32(vec, vec), vec);\n        \n        sum = svadd_s32(sum, svsel(mask3, squared, svdup_n_s32(0)));\n        sum = svadd_s32(sum, svsel(svand_b(mask4, svnot_b(mask3)), cubed, svdup_n_s32(0)));\n    }\n    \n    return svgetlane_s32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_130_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint sum_squares_simd(const std::vector<int>& lst) {\n    int sum = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t vidx = svindex_u32(lst.size());\n    svuint32_t vmask3 = svdup_u32(3);\n    svuint32_t vmask4 = svdup_u32(4);\n\n    for (size_t i = 0; i < lst.size(); i += SV_INT32_SIZE) {\n        svint32_t vin = svld1_s32(&lst[i]);\n        svuint32_t vmod3 = svand_u32(svmod_u32(vidx, vmask3), vmask3);\n        svuint32_t vmod4 = svand_u32(svmod_u32(vidx, vmask4), vmask4);\n        svuint32_t vcond = sveor_u32(vmod3, vmod4);\n        svint32_t vsqr = svcmulq_s32(vin, vin);\n        svint32_t vcub = svcmulq_s32(svcmulq_s32(vin, vin), vin);\n        svint32_t vres = svsel_s32(p, svcmulq_s32(vin, vin), svcmulq_s32(svcmulq_s32(vin, vin), vin));\n        sum += svaddv_s32(vres);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_SVE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t vnums = svld1_u32(nums.data());\n    svuint32_t vones = svdup_u32(1);\n    svuint32_t vmult10 = svdup_u32(10);\n\n    for (size_t i = 0; i < nums.size(); i += 16) {\n        svuint32_t vnums_part = svld1_u32(&nums[i]);\n        svuint32_t vgtr10 = svcmpgt_u32(vnums_part, vmult10);\n        svuint32_t vfirst_odd = svand_u32(svshrn_n_u32(vnums_part, 3), vones);\n        svuint32_t vlast_odd = svand_u32(svmod_u32(vnums_part, 10), vones);\n        svuint32_t vresult = svand_u32(svand_u32(vgtr10, vfirst_odd), vlast_odd);\n        count += svtbl1_u32(svptrue_b32(), vresult);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SVE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t vnums = svld1_u32(nums.data());\n    svuint32_t vones = svdup_u32(1);\n    svuint32_t vmult10 = svdup_u32(10);\n    svuint32_t vmult100 = svdup_u32(100);\n\n    for (size_t i = 0; i < nums.size(); i += SV_INT32_SIZE) {\n        svuint32_t vgt10 = svcmpgt_u32(vnums, vmult10);\n        svuint32_t vfirst_odd = svand_u32(svshrn_n_u64(svreinterpret_u64_s32(vnums), 31), vones);\n        svuint32_t vlast_odd = svand_u32(svshrn_n_u64(svreinterpret_u64_s32(vnums), 0), vones);\n        svuint32_t vresult = svand_u32(svand_u32(vgt10, vfirst_odd), vlast_odd);\n        count += svtbl1_u32(svptrue_b32(), vresult);\n        vnums = svld1_u32(&nums[i + SV_INT32_SIZE]);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    svbool_t mask = svptrue_b32();\n    svuint32_t vec_nums = svld1_u32(nums.data());\n    svuint32_t vec_threshold = svdup_u32(10);\n    svuint32_t vec_first_digit = svclz_u32(vec_nums) >> 28;\n    svuint32_t vec_last_digit = svand_u32(svclz_u32(svrev32_u32(vec_nums)), 0x0F);\n    svuint32_t vec_odd_digits = svor_u32(svor_u32(svshl_n_u32(svclz_u32(svrev32_u32(vec_nums)), 26), svshl_n_u32(svclz_u32(svrev32_u32(vec_nums)), 22)), svshl_n_u32(svclz_u32(svrev32_u32(vec_nums)), 18));\n    svuint32_t vec_result = svand_u32(svand_u32(svgt_u32(vec_nums, vec_threshold), svand_u32(vec_first_digit, vec_last_digit)), vec_odd_digits);\n    while (!svall(mask)) {\n        if (svget_u32(vec_result, svfirst_p(mask))) {\n            count++;\n        }\n        mask = svextb(mask, svreinterpret_u8(mask), 1);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SVE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t p = svptrue_b32();\n    svuint32_t vnums = svld1_u32(nums.data());\n    for (size_t i = 0; i < nums.size(); i += SV_INT32_SIZE) {\n        svuint32_t mask = svcmpgt_u32(vnums, svdup_u32(10));\n        svuint32_t first_digit = svand_u32(svshrq_n_u32(vnums, 28), svdup_u32(0x0F));\n        svuint32_t last_digit = svand_u32(vnums, svdup_u32(0x0F));\n        svuint32_t odd_first_last = svand_u32(first_digit, svdup_u32(0x01));\n        svuint32_t odd_first_last_mask = svand_u32(last_digit, svdup_u32(0x01));\n        svuint32_t result = svand_u32(mask, svand_u32(odd_first_last, odd_first_last_mask));\n        count += svcntb_u32(result);\n        if (i + SV_INT32_SIZE < nums.size()) {\n            vnums = svld1_u32(&nums[i + SV_INT32_SIZE]);\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nint specialFilter_simd(const std::vector<int>& nums) {\n    int count = 0;\n    svbool_t mask = svptrue_b32();\n    svuint32_t vec_nums = svld1_u32(nums.data());\n    svuint32_t vec_threshold = svdup_u32(10);\n    svuint32_t vec_odd_digits = svdup_u32(0x55555555); // Binary representation of odd digits (1, 3, 5, 7, 9)\n\n    for (size_t i = 0; i < nums.size(); i += SV_INT32_SIZE) {\n        svuint32_t vec_num = svld1_u32(&nums[i]);\n        svuint32_t vec_gt_10 = svcmpgt_u32(vec_num, vec_threshold);\n\n        svuint32_t vec_first_digit = svclz_u32(vec_num) >> 28;\n        svuint32_t vec_last_digit = svand_u32(svclz_u32(svrev32_u32(vec_num)), vec_odd_digits);\n        svuint32_t vec_odd_first_last = svand_u32(vec_first_digit, vec_last_digit);\n\n        svuint32_t vec_result = svand_u32(svand_u32(vec_gt_10, vec_odd_first_last), svreinterpret_u32(mask));\n        count += svcntw_u32(vec_result);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    svbool_t p = svptrue_b32();\n    uint32_t v_n = (uint32_t)n;\n    svuint32_t v_i = svdup_u32(0);\n    svuint32_t v_a = svdup_u32(0);\n    svuint32_t v_sum = svdup_u32(0);\n    svuint32_t v_count = svdup_u32(0);\n\n    while (svany_true_b32(p)) {\n        v_i = svadd_z_u32(p, v_i, svconst_u32(1));\n        v_a = svsub_z_u32(p, svmul_z_u32(p, v_i, v_i), v_i);\n        v_sum = svadd_z_u32(p, v_sum, v_a);\n        svuint32_t v_mod = svmod_z_u32(p, v_sum, svconst_u32(3));\n        svuint32_t v_mask = svcmpeq_z_u32(p, v_mod, svconst_u32(0));\n        v_count = svadd_z_u32(p, v_count, svreinterpret_u32(svpopcntb_z_u8(p, svreinterpret_u8(v_mask))));\n    }\n\n    return svgetlane_u64(v_count, 0);\n}\n```"}
{"task_id": "SimdBench_132_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    svbool_t p = svptrue_b32();\n    uint32_t v_n = (uint32_t)n;\n    svuint32_t v_i = svdup_u32(0);\n    svuint32_t v_a = svdup_u32(0);\n    svuint32_t v_count = svdup_u32(0);\n\n    while (svany_true_b32(p)) {\n        v_i = svadd_z_u32(p, v_i, svconst_u32(1));\n        v_a = svsub_z_u32(p, svmul_z_u32(p, v_i, v_i), v_i);\n        svuint32_t v_sum = svadd_z_u32(p, v_a, svadd_z_u32(p, v_a, svld1_u32(&v_a[1])));\n        svuint32_t v_mod = svmod_z_u32(p, v_sum, svconst_u32(3));\n        svuint32_t v_mask = sveor_z_u32(p, v_mod, svconst_u32(0));\n        v_count = svadd_z_u32(p, v_count, svreinterpret_u32(sveor_z_b32(p, svfalse_b32(), v_mask)));\n    }\n\n    return (uint64_t)v_count[0];\n}\n```"}
{"task_id": "SimdBench_132_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    svbool_t p = svptrue_b32();\n    uint32_t v_n = (uint32_t)n;\n    svuint32_t v_i = svdup_u32(0);\n    svuint32_t v_a = svdup_u32(0);\n    svuint32_t v_count = svdup_u32(0);\n\n    while (svany_true_b32(p)) {\n        v_i = svadd_z_u32(p, v_i, svconst_u32(1));\n        v_a = svsub_z_u32(p, svmul_z_u32(p, v_i, v_i), v_i);\n        v_a = svadd_z_u32(p, v_a, svconst_u32(1));\n\n        svuint32_t v_j = svdup_u32(0);\n        while (svany_true_b32(p)) {\n            v_j = svadd_z_u32(p, v_j, svconst_u32(1));\n            if (svlt_z_u32(p, v_j, v_i)) {\n                svuint32_t v_k = svdup_u32(0);\n                while (svany_true_b32(p)) {\n                    v_k = svadd_z_u32(p, v_k, svconst_u32(1));\n                    if (svlt_z_u32(p, v_k, v_j)) {\n                        svuint32_t v_sum = svadd_z_u32(p, v_a, svadd_z_u32(p, svld1_u32(v_j), svld1_u32(v_k)));\n                        if (svmod_z_u32(p, v_sum, svconst_u32(3)) == svconst_u32(0)) {\n                            v_count = svadd_z_u32(p, v_count, svconst_u32(1));\n                        }\n                    } else {\n                        break;\n                    }\n                }\n            } else {\n                break;\n            }\n        }\n    }\n\n    return svfirst_u32(v_count);\n}\n```"}
{"task_id": "SimdBench_132_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    svbool_t p = svptrue_b32();\n    uint64_t count = 0;\n\n    // Create the vector a\n    svint32_t a = svdup_n_s32(1);\n    for (uint64_t i = 1; i < n; i++) {\n        a = svadd_x(svmul_n_s32(a, 2), svsub_x(svadd_x(a, 1), svdup_n_s32(i)));\n    }\n\n    // Compute all possible triples and check if their sum is a multiple of 3\n    for (uint64_t i = 0; i < n - 2; i++) {\n        svint32_t ai = svindex_s32(n, i);\n        for (uint64_t j = i + 1; j < n - 1; j++) {\n            svint32_t aj = svindex_s32(n, j);\n            for (uint64_t k = j + 1; k < n; k++) {\n                svint32_t ak = svindex_s32(n, k);\n                svint32_t sum = svadd_x(svadd_x(ai, aj), ak);\n                if (svany_true_p(p, svcmple_z_s32(sum, svdup_n_s32(0)))) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SVE", "completion": "```c\n#include <arm_sve.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    svint32_t v_n = svdupw_z_s32(n);\n    svint32_t v_i = svdupw_z_s32(1);\n    svint32_t v_a = svcmulq_lane_s32(v_i, v_i, 0);\n    v_a = svsubq_lane_s32(v_a, v_i, 0);\n    v_a = svaddq_lane_s32(v_a, v_i, 0);\n\n    uint64_t count = 0;\n    for (uint64_t i = 1; i < n; i++) {\n        v_i = svaddq_lane_s32(v_i, v_i, 0);\n        v_a = svcmulq_lane_s32(v_i, v_i, 0);\n        v_a = svsubq_lane_s32(v_a, v_i, 0);\n        v_a = svaddq_lane_s32(v_a, v_i, 0);\n\n        svint32_t v_j = svaddq_lane_s32(v_i, v_i, 0);\n        v_j = svaddq_lane_s32(v_j, v_i, 0);\n        v_j = svaddq_lane_s32(v_j, v_i, 0);\n\n        for (uint64_t j = i + 1; j < n; j++) {\n            v_j = svaddq_lane_s32(v_j, v_i, 0);\n            svint32_t v_k = svaddq_lane_s32(v_j, v_i, 0);\n            v_k = svaddq_lane_s32(v_k, v_i, 0);\n\n            for (uint64_t k = j + 1; k < n; k++) {\n                v_k = svaddq_lane_s32(v_k, v_i, 0);\n                svint32_t sum = svaddq_lane_s32(v_a, v_j, 0);\n                sum = svaddq_lane_s32(sum, v_k, 0);\n                if (svmodq_lane_u32(sum, 3) == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_SVE", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    svbool_t p = svcmpgt_b32(svptrue_b32(), svdup_n_u32(n), svdup_n_u32(1));\n    svuint32_t np = svcmple_z_u32(p, svdup_n_u32(n), svdup_n_u32(1));\n    svuint32_t result = svsel_z_u32(np, svdup_n_u32(x), svdup_n_u32(y));\n    return svfirst_m_b32(result);\n}\n```"}
{"task_id": "SimdBench_133_SVE", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    svbool_t p = svcmple_u32_z(svptrue_b32(), svdup_u32(n), svdup_u32(2));\n    svuint32_t factors = svclz_u32(svand_u32(svnot_u32(p), svslct_u32(svdup_u32(x), svdup_u32(y), svcmpeq_u32_z(svptrue_b32(), svmod_u32(svduiw_u32(svld1_u32((uint32_t[]){n}), 0), svdup_u32(i)), svdup_u32(1))))));\n    return svfirst_m_b32(factors);\n}\n```"}
{"task_id": "SimdBench_133_SVE", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    svbool_t p = svcmpgt_b32(svptrue_b32(), svdup_n_u32(n), svdup_n_u32(1));\n    svuint32_t mask = svreinterpret_u32(p);\n    return (mask & x) | (~mask & y);\n}\n```"}
{"task_id": "SimdBench_133_SVE", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    svbool_t p = svcmpgt_b32(svptrue_b32(), svdup_n_u32(n), svdup_n_u32(1));\n    svbool_t q = svcmple_b32(p, svdup_n_u32(n), svdup_n_u32(n / 2 + 1));\n    svuint32_t mask = svreinterpret_u32(q);\n    return (mask & x) | (~mask & y);\n}\n```"}
{"task_id": "SimdBench_133_SVE", "completion": "```c\nint x_or_y_simd(int n, int x, int y) {\n    svbool_t p = svcmple_u32_z(svptrue_b32(), svdup_u32(n), svdup_u32(2));\n    for (int i = 3; i * i <= n; i += 2) {\n        svbool_t q = svcmple_u32_z(p, svdup_u32(i), svdup_u32(n / i));\n        p = svand_z(p, q);\n    }\n    return p ? x : y;\n}\n```"}
{"task_id": "SimdBench_134_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    svfloat32_t vzero = svdup_n_f32(0.0f);\n    uint64_t sum = 0;\n\n    size_t n = lst.size();\n    size_t i = 0;\n    for (; i <= n - 16; i += 16) {\n        svfloat32_t vin = svld1q_f32(lst.data() + i);\n        svbool_t is_odd = svcmple_f32(vin, svdup_n_f32(0.0f));\n        svbool_t is_int = svcmpge_f32(svabs_f32(vin), svfloor_f32(svabs_f32(vin)));\n        svbool_t mask = svand_b32(is_odd, is_int);\n\n        svfloat32_t vabs = svabs_f32(vin);\n        svfloat32_t vsqr = svmul_f32(vabs, vabs);\n        svfloat32_t vsum = svaddvq_f32(vsqr);\n\n        sum += svreinterpret_u64_f32(vsum)[0];\n    }\n\n    for (; i < n; ++i) {\n        if (lst[i] > 0 && fmod(lst[i], 2.0f) == 0) {\n            sum += static_cast<uint64_t>(pow(lst[i], 2));\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    svfloat32_t vsum = svdup_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i < n) {\n        size_t chunk_size = std::min(n - i, SV_INT32_ELT_SIZE);\n        svint32_t indices = svindex(chunk_size);\n        svbool_t mask = svle(indices, svdup_u32(chunk_size - 1));\n        svfloat32_t vdata = svld1(mask, reinterpret_cast<const float*>(&lst[i]));\n        svfloat32_t vabs = svabs(vdata);\n        svbool_t is_odd = svand(svcmpne(svfloor(vabs), vabs), svcmpgt(vabs, svdup_f32(0.0f)));\n        svfloat32_t vsquares = svmul(vabs, vabs);\n        vsum = svadd(vsum, svsel(is_odd, vsquares, svdup_f32(0.0f)));\n        i += chunk_size;\n    }\n\n    uint64_t result = 0;\n    svst1(reinterpret_cast<float*>(&result), vsum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t double_the_difference_simd(const std::vector<float>& lst) {\n    svfloat32_t sum = svdup_f32(0.0f);\n    svbool_t mask = svptrue_b32();\n\n    for (size_t i = 0; i < lst.size(); i += SV_INT32_ELEMS) {\n        svfloat32_t vec = svld1_f32(lst.data() + i);\n        svbool_t is_odd = svcmple_f32(svabs_f32(vec), svdup_f32(1.0f));\n        svbool_t is_positive = svcmpge_f32(vec, svzero_f32());\n        svbool_t is_integer = svcmpeq_f32(svmod_f32(vec, svdup_f32(2.0f)), svzero_f32());\n\n        mask = svtbl_b32(mask, svand_b32(is_odd, svand_b32(is_positive, is_integer)));\n\n        sum = svadd_f32(sum, svmul_f32(vec, vec), mask);\n    }\n\n    return static_cast<uint64_t>(svsum_f32(sum));\n}\n```"}
{"task_id": "SimdBench_134_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    svfloat32_t vsum = svdup_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i < n) {\n        size_t chunk_size = std::min(n - i, SV_INT32_ELT_SIZE);\n        svint32_t vidx = svindex_s32(chunk_size);\n        svbool_t vmask = sveor_z_b32(svptrue_b32(), svle_z_s32(vidx, svdup_s32(chunk_size - 1)));\n        svfloat32_t vdata = svld1_f32(lst.data() + i);\n\n        svbool_t vcond = svandnot_z_b32(vmask, svtst_m_b32(vdata, svdup_f32(0.0f), SV_ALL_ONE_B32));\n        vcond = svandnot_z_b32(vcond, svgtz_m_b32(vdata, svdup_f32(0.0f), SV_ALL_ONE_B32));\n        vcond = svandnot_z_b32(vcond, svand_z_b32(svfloor_m_b32(vdata, vdata, SV_ALL_ONE_B32), svceq_m_b32(vdata, svfloor_m_b32(vdata, vdata, SV_ALL_ONE_B32), SV_ALL_ONE_B32)));\n\n        svfloat32_t vsquares = svmul_f32(vdata, vdata);\n        vsum = svaddm_f32(vsum, vsquares, vcond);\n\n        i += chunk_size;\n    }\n\n    return static_cast<uint64_t>(svreinterpret_u64_f32(vsum)[0]);\n}\n```"}
{"task_id": "SimdBench_134_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    svfloat32_t sum = svdup_n_f32(0.0f);\n    size_t n = lst.size();\n    size_t i = 0;\n\n    while (i < n) {\n        size_t chunk_size = std::min(n - i, SV_INT32_ELT_SIZE);\n        svint32_t indices = svindex(chunk_size);\n        svbool_t mask = svle(indices, svdup_n_s32(chunk_size - 1));\n        svfloat32_t values = svld1_u32(reinterpret_cast<const uint32_t*>(&lst[i]));\n        svfloat32_t abs_values = svabs(values);\n        svfloat32_t int_values = svfloor(abs_values);\n        svbool_t is_int = sveor(svcmpne_f32(abs_values, int_values), svfalse());\n        svbool_t is_odd = svand(is_int, svmod(int_values, svdup_n_f32(2.0f)));\n        svfloat32_t squares = svmul_f32(int_values, int_values);\n        sum = svadd_f32(sum, svsel(is_odd, squares, svdup_n_f32(0.0f)));\n        i += chunk_size;\n    }\n\n    return static_cast<uint64_t>(svsum_f32(sum));\n}\n```"}
{"task_id": "SimdBench_135_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    svint32_t* pGame = reinterpret_cast<svint32_t*>(const_cast<int*>(&game[0]));\n    svint32_t* pGuess = reinterpret_cast<svint32_t*>(const_cast<int*>(&guess[0]));\n    svint32_t* pResult = reinterpret_cast<svint32_t*>(&result[0]);\n\n    for (int i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vGame = svld1_u32(pGame + i);\n        svint32_t vGuess = svld1_u32(pGuess + i);\n        svint32_t vDiff = svabs_s32(svsub_s32(vGame, vGuess));\n        svst1_u32(pResult + i, vDiff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SVE", "completion": "```cpp\n#include <arm_sve.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    svint32_t* pGame = reinterpret_cast<svint32_t*>(const_cast<int*>(&game[0]));\n    svint32_t* pGuess = reinterpret_cast<svint32_t*>(const_cast<int*>(&guess[0]));\n    svint32_t* pResult = reinterpret_cast<svint32_t*>(&result[0]);\n\n    for (size_t i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vGame = svld1_u32(pGame + i);\n        svint32_t vGuess = svld1_u32(pGuess + i);\n        svint32_t vDiff = svabs_s32(svreinterpret_s32_u32(svsub_u32(vGuess, vGame)));\n        svst1_u32(pResult + i, vDiff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    int n = game.size();\n    std::vector<int> result(n);\n\n    svint32_t* pGame = reinterpret_cast<svint32_t*>(const_cast<int*>(&game[0]));\n    svint32_t* pGuess = reinterpret_cast<svint32_t*>(const_cast<int*>(&guess[0]));\n    svint32_t* pResult = reinterpret_cast<svint32_t*>(&result[0]);\n\n    for (int i = 0; i < n; i += SV_INT32_SIZE) {\n        svint32_t vGame = svld1_u32(pGame + i);\n        svint32_t vGuess = svld1_u32(pGuess + i);\n        svint32_t vDiff = svabs_s32(svsub_s32(vGame, vGuess));\n        svst1_u32(pResult + i, vDiff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    svint32x4_t v_game, v_guess, v_diff, v_zero;\n    svbool_t mask;\n\n    for (size_t i = 0; i < n; i += 4) {\n        v_game = svld1_u32(game.data() + i);\n        v_guess = svld1_u32(guess.data() + i);\n        v_diff = svabs_s32(svreinterpret_s32_u32(svsub_s32(v_game, v_guess)));\n        v_zero = svdup_n_s32(0);\n        mask = svcmplt_z_s32(v_diff, v_zero);\n        v_diff = svsel(mask, v_zero, v_diff);\n        svst1_u32(result.data() + i, v_diff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SVE", "completion": "```cpp\n#include <arm_sve.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n);\n\n    svint32x8_t v_game, v_guess, v_diff, v_zero;\n    svbool_t mask;\n\n    for (size_t i = 0; i < n; i += 8) {\n        v_game = svld1_u32(game.data() + i);\n        v_guess = svld1_u32(guess.data() + i);\n        v_diff = svabs_s32(svreinterpret_s32_u32(svsub_s32(v_game, v_guess)));\n        v_zero = svdup_n_s32(0);\n        mask = svcmplt_z_s32(v_diff, v_zero);\n        v_diff = svsel(mask, v_zero, v_diff);\n        svst1_u32(result.data() + i, v_diff);\n    }\n\n    return result;\n}\n```"}

{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 mask values (bools)\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        // Convert boolean mask to integer mask (0 or -1)\n        __m256i mask_int = _mm256_cvtepi8_epi64(mask_vec);\n\n        // Load 4 source values\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        // Load 4 destination values\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)&dst[i]);\n\n        // Blend source and destination based on mask\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_int);\n\n        // Store the result back to destination\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < length; i += 4) {\n        // Load 4 mask values (booleans)\n        __m256i mask_vec = _mm256_set_epi64x(mask[i + 3] ? -1 : 0,\n                                              mask[i + 2] ? -1 : 0,\n                                              mask[i + 1] ? -1 : 0,\n                                              mask[i] ? -1 : 0);\n\n        // Load 4 source values\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Load 4 destination values\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)&dst[i]);\n\n        // Blend source and destination based on the mask\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        // Store the result back to the destination array\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < length; i += 4) {\n        // Load 4 mask values (booleans)\n        __m256i mask_vec = _mm256_set_epi64x(mask[i + 3] ? -1 : 0,\n                                              mask[i + 2] ? -1 : 0,\n                                              mask[i + 1] ? -1 : 0,\n                                              mask[i] ? -1 : 0);\n\n        // Load 4 source values\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Load 4 destination values\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)&dst[i]);\n\n        // Blend source and destination based on the mask\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        // Store the result back to the destination array\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < length; i += 4) {\n        // Load 4 mask values (bools) into an AVX register\n        __m256i mask_vec = _mm256_cvtepi8_epi64(_mm_loadu_si128((__m128i*)&mask[i])); // Extend bools to 64-bit integers\n        mask_vec = _mm256_and_si256(mask_vec, _mm256_set1_epi64x(1)); // Ensure mask is 0 or 1\n\n        // Load 4 source and destination values\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)&dst[i]);\n\n        // Blend based on mask: if mask is true, take from src; otherwise, keep dst\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        // Store the result back to dst\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Process remaining elements (if any) with scalar code\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < length; i += 4) {\n        // Load 4 mask values (bools)\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        // Convert boolean mask to integer mask\n        __m256i mask_int = _mm256_cvtepi8_epi64(mask_vec);\n\n        // Load 4 src and dst values\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i dst_vec = _mm256_loadu_si256((__m256i*)&dst[i]);\n\n        // Blend based on mask\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_int);\n\n        // Store the result back to dst\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vseven = _mm256_set1_epi32(7);\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vthree = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 integers from src into a SIMD register\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Compute element * scale\n        __m256i vmul = _mm256_mullo_epi32(vsrc, vscale);\n\n        // tmp1 = (element * scale) >> 3\n        __m256i vtmp1 = _mm256_srai_epi32(_mm256_slli_epi32(vmul, 32 - 3), 32 - 3);\n\n        // tmp2 = (element * scale) << 3\n        __m256i vtmp2 = _mm256_slli_epi32(vmul, 3);\n\n        // tmp1 * tmp2\n        __m256i vtmp1tmp2 = _mm256_mullo_epi32(vtmp1, vtmp2);\n\n        // result = (element * scale + tmp1 * tmp2) / 7\n        __m256i vresult = _mm256_add_epi32(vmul, vtmp1tmp2);\n        vresult = _mm256_div_epi32(vresult, vseven);\n\n        // Store the result back to dst\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_divisor = _mm256_set1_epi32(7);\n    __m256i v_shift_right = _mm256_set1_epi32(3);\n    __m256i v_shift_left = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        // Load 8 integers from src\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Compute element * scale\n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n\n        // Compute tmp1: (element * scale) >> 3\n        __m256i v_tmp1 = _mm256_srai_epi32(_mm256_slli_epi32(v_scaled, 32 - 3), 32 - 3);\n\n        // Compute tmp2: (element * scale) << 3\n        __m256i v_tmp2 = _mm256_slli_epi32(v_scaled, 3);\n\n        // Compute tmp1 * tmp2\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n\n        // Compute final result: (element * scale + tmp1 * tmp2) / 7\n        __m256i v_result = _mm256_add_epi32(v_scaled, v_product);\n        v_result = _mm256_div_epi32(v_result, v_divisor);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (length / 8) * 8; i < length; ++i) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i vseven = _mm256_set1_epi32(7);\n    __m256i vscale = _mm256_set1_epi32(scale);\n    __m256i vthree = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 elements from src\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Compute element * scale\n        __m256i vmul = _mm256_mullo_epi32(vsrc, vscale);\n\n        // Calculate tmp1: (element * scale) >> 3\n        __m256i vtmp1 = _mm256_srai_epi32(_mm256_slli_epi32(vmul, 32 - 3), 32 - 3);\n\n        // Calculate tmp2: (element * scale) << 3\n        __m256i vtmp2 = _mm256_slli_epi32(vmul, 3);\n\n        // Compute tmp1 * tmp2\n        __m256i vtmp1tmp2 = _mm256_mullo_epi32(vtmp1, vtmp2);\n\n        // Compute final result: (vmul + vtmp1tmp2) / 7\n        __m256i vresult = _mm256_add_epi32(vmul, vtmp1tmp2);\n        vresult = _mm256_div_epi32(vresult, vseven);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_divisor = _mm256_set1_epi32(7);\n    __m256i v_shift_right = _mm256_set1_epi32(3);\n    __m256i v_shift_left = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        // Load 8 elements from src\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Multiply each element by scale\n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n\n        // Calculate tmp1: (element * scale) >> 3\n        __m256i v_tmp1 = _mm256_srai_epi32(_mm256_slli_epi32(v_scaled, 32 - 3), 32 - 3);\n\n        // Calculate tmp2: (element * scale) << 3\n        __m256i v_tmp2 = _mm256_slli_epi32(v_scaled, 3);\n\n        // Compute tmp1 * tmp2\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n\n        // Compute final result: (element * scale + tmp1 * tmp2) / 7\n        __m256i v_result = _mm256_add_epi32(v_scaled, v_product);\n        v_result = _mm256_div_epi32(v_result, v_divisor);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (length / 8) * 8; i < length; ++i) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i v_scale = _mm256_set1_epi32(scale);\n    __m256i v_divisor = _mm256_set1_epi32(7);\n    __m256i v_shift_right = _mm256_set1_epi32(3);\n    __m256i v_shift_left = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        // Load 8 integers from src\n        __m256i v_src = _mm256_loadu_si256((__m256i const *)(src + i));\n\n        // Compute element * scale\n        __m256i v_scaled = _mm256_mullo_epi32(v_src, v_scale);\n\n        // Calculate tmp1: (element * scale) >> 3\n        __m256i v_tmp1 = _mm256_srai_epi32(v_scaled, 3);\n\n        // Calculate tmp2: (element * scale) << 3\n        __m256i v_tmp2 = _mm256_slli_epi32(v_scaled, 3);\n\n        // Compute tmp1 * tmp2\n        __m256i v_tmp1_tmp2 = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n\n        // Compute final result: (element * scale + tmp1 * tmp2) / 7\n        __m256i v_result = _mm256_add_epi32(v_scaled, v_tmp1_tmp2);\n        v_result = _mm256_div_epi32(v_result, v_divisor);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements (if any)\n    for (size_t i = (length / 8) * 8; i < length; ++i) {\n        int scaled = src[i] * scale;\n        int tmp1 = scaled >> 3;\n        int tmp2 = scaled << 3;\n        dst[i] = (scaled + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d vec;\n\n    // Process 4 rows at a time using AVX\n    for (; i + 3 < rows; i += 4) {\n        // Load 4 elements from src with the given stride\n        vec = _mm256_set_pd(src[(i + 3) * cols + stride],\n                            src[(i + 2) * cols + stride],\n                            src[(i + 1) * cols + stride],\n                            src[i * cols + stride]);\n\n        // Store the 4 elements into dst\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    // Process remaining rows (if any)\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d vec;\n\n    // Process 4 elements at a time using AVX\n    for (; i + 3 < rows; i += 4) {\n        // Gather indices for the current 4 rows\n        __m128i indices = _mm_set_epi64x((i + 3) * cols + stride, (i + 2) * cols + stride,\n                                         (i + 1) * cols + stride, i * cols + stride);\n\n        // Load 4 doubles from src using the calculated indices\n        vec = _mm256_i64gather_pd(src, indices, 8);\n\n        // Store the loaded values into dst\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    // Handle remaining rows (if any) with scalar code\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d vec;\n\n    // Process 4 elements at a time using AVX\n    for (; i + 3 < rows; i += 4) {\n        // Gather indices for the current 4 rows\n        __m256i indices = _mm256_set_epi64x((i + 3) * cols + stride, \n                                            (i + 2) * cols + stride, \n                                            (i + 1) * cols + stride, \n                                            i * cols + stride);\n\n        // Load 4 doubles from src using the computed indices\n        vec = _mm256_i64gather_pd(src, indices, 8);\n\n        // Store the loaded values into dst\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    // Process remaining rows (if any) with scalar code\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d vec;\n\n    // Process 4 rows at a time using AVX\n    for (; i + 3 < rows; i += 4) {\n        // Gather indices for the current 4 rows\n        size_t idx0 = (i + 0) * cols + stride;\n        size_t idx1 = (i + 1) * cols + stride;\n        size_t idx2 = (i + 2) * cols + stride;\n        size_t idx3 = (i + 3) * cols + stride;\n\n        // Load 4 doubles from src using the calculated indices\n        vec = _mm256_set_pd(src[idx3], src[idx2], src[idx1], src[idx0]);\n\n        // Store the loaded values into dst at positions i to i+3\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    // Process remaining rows (if any) using scalar code\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d vec;\n\n    // Process 4 rows at a time using AVX\n    for (; i + 3 < rows; i += 4) {\n        // Load 4 elements from src with stride\n        vec = _mm256_set_pd(src[(i + 3) * cols + stride],\n                            src[(i + 2) * cols + stride],\n                            src[(i + 1) * cols + stride],\n                            src[i * cols + stride]);\n\n        // Store 4 elements into dst\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    // Process remaining rows (if any)\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Gather operation: Copy values from src to dst using indices as lookups\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 gathered = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], gathered);\n    }\n\n    // Handle remaining elements for gather\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: Copy values from src to dst using indices as write locations\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 values = _mm256_loadu_ps(&src[i]);\n\n        int index0 = _mm256_extract_epi32(idx, 0);\n        dst[index0] = _mm256_cvtss_f32(values);\n\n        int index1 = _mm256_extract_epi32(idx, 1);\n        dst[index1] = _mm256_cvtss_f32(_mm256_shuffle_ps(values, values, _MM_SHUFFLE(1, 1, 1, 1)));\n\n        int index2 = _mm256_extract_epi32(idx, 2);\n        dst[index2] = _mm256_cvtss_f32(_mm256_shuffle_ps(values, values, _MM_SHUFFLE(2, 2, 2, 2)));\n\n        int index3 = _mm256_extract_epi32(idx, 3);\n        dst[index3] = _mm256_cvtss_f32(_mm256_shuffle_ps(values, values, _MM_SHUFFLE(3, 3, 3, 3)));\n\n        int index4 = _mm256_extract_epi32(idx, 4);\n        dst[index4] = _mm256_cvtss_f32(_mm256_shuffle_ps(values, values, _MM_SHUFFLE(4, 4, 4, 4)));\n\n        int index5 = _mm256_extract_epi32(idx, 5);\n        dst[index5] = _mm256_cvtss_f32(_mm256_shuffle_ps(values, values, _MM_SHUFFLE(5, 5, 5, 5)));\n\n        int index6 = _mm256_extract_epi32(idx, 6);\n        dst[index6] = _mm256_cvtss_f32(_mm256_shuffle_ps(values, values, _MM_SHUFFLE(6, 6, 6, 6)));\n\n        int index7 = _mm256_extract_epi32(idx, 7);\n        dst[index7] = _mm256_cvtss_f32(_mm256_shuffle_ps(values, values, _MM_SHUFFLE(7, 7, 7, 7)));\n    }\n\n    // Handle remaining elements for scatter\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Gather operation: dst[i] = src[indices[i]]\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 gathered = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], gathered);\n    }\n\n    // Handle remaining elements for gather\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: dst[indices[i]] = src[i]\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 src_vals = _mm256_loadu_ps(&src[i]);\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n\n        // Scatter values one by one since AVX2 does not support scatter directly\n        for (int j = 0; j < 8; ++j) {\n            int index = _mm256_extract_epi32(idx, j);\n            dst[index] = _mm256_cvtss_f32(_mm256_permutevar8x32_ps(src_vals, _mm256_set1_epi32(j)));\n        }\n    }\n\n    // Handle remaining elements for scatter\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Gather operation: dst[i] = src[indices[i]]\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(indices + i));\n        __m256 gathered = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(dst + i, gathered);\n    }\n\n    // Handle remaining elements for gather\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: dst[indices[i]] = src[i]\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 src_vals = _mm256_loadu_ps(src + i);\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(indices + i));\n\n        // Scatter is not directly supported in AVX2, so we emulate it\n        int32_t indices_arr[8];\n        float src_arr[8];\n        _mm256_storeu_si256((__m256i*)indices_arr, idx);\n        _mm256_storeu_ps(src_arr, src_vals);\n\n        for (int j = 0; j < 8; ++j) {\n            dst[indices_arr[j]] = src_arr[j];\n        }\n    }\n\n    // Handle remaining elements for scatter\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Gather operation\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 gathered = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], gathered);\n    }\n\n    // Handle remaining elements for gather\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 values = _mm256_loadu_ps(&src[i]);\n        _mm256_i32scatter_ps(dst, idx, values, 4);\n    }\n\n    // Handle remaining elements for scatter\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n\n    // Gather operation: Copy values from src to dst using indices as lookups\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 gathered = _mm256_i32gather_ps(src, idx, 4);\n        _mm256_storeu_ps(&dst[i], gathered);\n    }\n\n    // Handle remaining elements for gather\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n    }\n\n    // Scatter operation: Copy values from src to dst using indices as write locations\n    i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i idx = _mm256_loadu_si256((__m256i*)&indices[i]);\n        __m256 values = _mm256_loadu_ps(&src[i]);\n        _mm256_i32scatter_ps(dst, idx, values, 4);\n    }\n\n    // Handle remaining elements for scatter\n    for (; i < length; ++i) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure the range is valid\n    if (start >= end || arr == nullptr) return;\n\n    // Align start and end to 16-byte boundaries for AVX2 processing\n    uint64_t aligned_start = (start + 15) & ~15; // Round up to nearest 16-byte boundary\n    uint64_t aligned_end = (end + 1) & ~15;     // Round down to nearest 16-byte boundary\n\n    // Reverse elements outside the aligned range using scalar operations\n    uint64_t i = start, j = end;\n    while (i < aligned_start && i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n\n    // Reverse elements within the aligned range using AVX2 intrinsics\n    __m256i reverse_mask = _mm256_set_epi16(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14);\n    for (uint64_t front = aligned_start, back = aligned_end - 16; front < back; front += 16, back -= 16) {\n        __m256i front_data = _mm256_loadu_si256((__m256i*)(arr + front));\n        __m256i back_data = _mm256_loadu_si256((__m256i*)(arr + back));\n\n        // Reverse the order of elements in each vector\n        front_data = _mm256_permutevar8x32_epi32(front_data, reverse_mask);\n        back_data = _mm256_permutevar8x32_epi32(back_data, reverse_mask);\n\n        // Swap the two vectors\n        _mm256_storeu_si256((__m256i*)(arr + front), back_data);\n        _mm256_storeu_si256((__m256i*)(arr + back), front_data);\n    }\n\n    // Reverse any remaining elements outside the aligned range using scalar operations\n    i = aligned_start;\n    j = aligned_end - 1;\n    while (i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure the range is valid\n    if (start > end) return;\n\n    // Align the start and end indices to 16-byte boundaries for AVX2 processing\n    uint64_t aligned_start = (start + 7) & ~7; // Round up to nearest multiple of 8\n    uint64_t aligned_end = (end - 7) & ~7;    // Round down to nearest multiple of 8\n\n    // Reverse elements outside the aligned range using scalar operations\n    uint64_t i = start, j = end;\n    while (i < aligned_start && j >= aligned_end) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n\n    // Process the aligned range using AVX2 intrinsics\n    while (i < j) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[i]); // Load 16 elements from the start\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&arr[j - 15]); // Load 16 elements from the end\n\n        // Reverse the order of elements within each vector\n        vec1 = _mm256_permute4x64_epi64(vec1, 0x4E); // Swap 128-bit halves\n        vec2 = _mm256_permute4x64_epi64(vec2, 0x4E);\n\n        // Shuffle 16-bit elements within each 128-bit lane\n        vec1 = _mm256_shufflelo_epi16(vec1, 0x1B);\n        vec1 = _mm256_shufflehi_epi16(vec1, 0x1B);\n        vec2 = _mm256_shufflelo_epi16(vec2, 0x1B);\n        vec2 = _mm256_shufflehi_epi16(vec2, 0x1B);\n\n        // Store the reversed vectors back to memory\n        _mm256_storeu_si256((__m256i*)&arr[j - 15], vec1);\n        _mm256_storeu_si256((__m256i*)&arr[i], vec2);\n\n        i += 16;\n        j -= 16;\n    }\n\n    // Reverse any remaining elements in the unaligned tail using scalar operations\n    while (i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure the range is valid\n    if (start > end) return;\n\n    // Align the start and end indices to 16-byte boundaries for AVX2 processing\n    uint64_t aligned_start = (start + 7) & ~7; // Round up to nearest multiple of 8\n    uint64_t aligned_end = (end - 7) & ~7;    // Round down to nearest multiple of 8\n\n    // Reverse elements outside the aligned range using scalar code\n    uint64_t i = start, j = end;\n    while (i < aligned_start && i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n\n    // Process the aligned range using AVX2 intrinsics\n    while (i < aligned_end) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);       // Load 16 elements from the start\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&arr[j - 15]);   // Load 16 elements from the end\n\n        // Reverse the order of elements in each vector\n        vec1 = _mm256_permute4x64_epi64(vec1, 0x4E); // Swap 128-bit halves\n        vec2 = _mm256_permute4x64_epi64(vec2, 0x4E);\n\n        // Swap the two vectors\n        __m256i temp = vec1;\n        vec1 = vec2;\n        vec2 = temp;\n\n        // Store the reversed vectors back to memory\n        _mm256_storeu_si256((__m256i*)&arr[i], vec1);\n        _mm256_storeu_si256((__m256i*)&arr[j - 15], vec2);\n\n        i += 16;\n        j -= 16;\n    }\n\n    // Reverse any remaining elements using scalar code\n    while (i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure the range is valid\n    if (start >= end || arr == nullptr) return;\n\n    // Align the start and end indices to 16-byte boundaries for AVX2 processing\n    uint64_t aligned_start = (start + 15) & ~15; // Round up to nearest multiple of 16\n    uint64_t aligned_end = (end + 1) & ~15;     // Round down to nearest multiple of 16\n\n    // Reverse elements outside the aligned range using scalar operations\n    uint64_t i = start, j = end;\n    while (i < aligned_start && j >= aligned_end) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n\n    // Reverse elements within the aligned range using AVX2 intrinsics\n    while (i < j) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&arr[j - 15]);\n\n        // Reverse the order of 16-bit integers within each vector\n        __m256i perm1 = _mm256_permute4x64_epi64(vec1, 0b11100100);\n        __m256i perm2 = _mm256_permute4x64_epi64(vec2, 0b11100100);\n\n        __m256i shuffle1 = _mm256_shufflelo_epi16(perm1, 0b11100100);\n        __m256i shuffle2 = _mm256_shufflelo_epi16(perm2, 0b11100100);\n\n        shuffle1 = _mm256_shufflehi_epi16(shuffle1, 0b11100100);\n        shuffle2 = _mm256_shufflehi_epi16(shuffle2, 0b11100100);\n\n        // Swap the two reversed vectors\n        _mm256_storeu_si256((__m256i*)&arr[i], shuffle2);\n        _mm256_storeu_si256((__m256i*)&arr[j - 15], shuffle1);\n\n        i += 16;\n        j -= 16;\n    }\n\n    // Reverse any remaining elements that didn't fit into a full AVX2 vector\n    while (i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    // Ensure the range is valid\n    if (start >= end || arr == nullptr) return;\n\n    // Align the start and end indices to 16-byte boundaries for AVX2 processing\n    uint64_t aligned_start = (start + 15) & ~15; // Round up to nearest multiple of 16\n    uint64_t aligned_end = (end + 1) & ~15;     // Round down to nearest multiple of 16\n\n    // Reverse elements outside the aligned range using scalar operations\n    uint64_t i = start, j = end;\n    while (i < aligned_start && i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n\n    // Reverse elements within the aligned range using AVX2 intrinsics\n    while (i + 15 <= j) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&arr[j - 15]);\n\n        // Reverse the order of 16-bit integers within each vector\n        vec1 = _mm256_shufflelo_epi16(vec1, 0x1B); // Reverse lower 8 elements\n        vec1 = _mm256_shufflehi_epi16(vec1, 0x1B); // Reverse upper 8 elements\n        vec1 = _mm256_permute4x64_epi64(vec1, 0x4E); // Swap high and low 128-bit lanes\n\n        vec2 = _mm256_shufflelo_epi16(vec2, 0x1B); // Reverse lower 8 elements\n        vec2 = _mm256_shufflehi_epi16(vec2, 0x1B); // Reverse upper 8 elements\n        vec2 = _mm256_permute4x64_epi64(vec2, 0x4E); // Swap high and low 128-bit lanes\n\n        // Swap the two reversed vectors\n        _mm256_storeu_si256((__m256i*)&arr[i], vec2);\n        _mm256_storeu_si256((__m256i*)&arr[j - 15], vec1);\n\n        i += 16;\n        j -= 16;\n    }\n\n    // Reverse any remaining elements using scalar operations\n    while (i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t total_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i indices = _mm256_set_epi32(\n            i + 7, i + 6, i + 5, i + 4,\n            i + 3, i + 2, i + 1, i\n        );\n\n        __m256i values = _mm256_i32gather_epi32((const int *)(tensor + slice_offset), indices, 1);\n        _mm256_storeu_si256((__m256i *)(slice + i), values);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t row_stride = dim2 * dim3;\n    size_t slice_offset = slice_idx * dim2;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        const uint8_t *row_start = tensor + i * row_stride + slice_offset;\n        uint8_t *slice_row_start = slice + i * dim2;\n\n        size_t j = 0;\n        for (; j + 32 <= dim2; j += 32) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)(row_start + j));\n            _mm256_storeu_si256((__m256i*)(slice_row_start + j), vec);\n        }\n\n        for (; j < dim2; ++j) {\n            slice_row_start[j] = row_start[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t total_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i indices = _mm256_set_epi32(\n            i + 7, i + 6, i + 5, i + 4,\n            i + 3, i + 2, i + 1, i\n        );\n\n        __m256i values = _mm256_i32gather_epi32((const int *)(tensor + slice_offset), indices, 1);\n        _mm256_storeu_si256((__m256i *)(slice + i), values);\n    }\n\n    // Handle remaining elements if total_elements is not a multiple of 32\n    size_t remainder = total_elements % 32;\n    if (remainder > 0) {\n        for (size_t i = total_elements - remainder; i < total_elements; ++i) {\n            slice[i] = tensor[slice_offset + i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    size_t total_elements = dim1 * dim2;\n\n    for (size_t i = 0; i < total_elements; i += 32) {\n        __m256i indices = _mm256_set_epi32(\n            i + 7, i + 6, i + 5, i + 4,\n            i + 3, i + 2, i + 1, i\n        );\n\n        __m256i values = _mm256_i32gather_epi32((const int *)(tensor + slice_offset), indices, 1);\n        _mm256_storeu_si256((__m256i *)(slice + i), values);\n    }\n\n    // Handle remaining elements if total_elements is not a multiple of 32\n    size_t remainder = total_elements % 32;\n    if (remainder > 0) {\n        for (size_t i = total_elements - remainder; i < total_elements; ++i) {\n            slice[i] = tensor[slice_offset + i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t row_stride = dim2 * dim3;\n    size_t slice_offset = slice_idx * dim2;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        const uint8_t *row_start = tensor + i * row_stride + slice_offset;\n        uint8_t *slice_row = slice + i * dim2;\n\n        size_t j = 0;\n        for (; j + 32 <= dim2; j += 32) {\n            __m256i vec = _mm256_loadu_si256((const __m256i*)(row_start + j));\n            _mm256_storeu_si256((__m256i*)(slice_row + j), vec);\n        }\n\n        if (j + 16 <= dim2) {\n            __m128i vec = _mm_loadu_si128((const __m128i*)(row_start + j));\n            _mm_storeu_si128((__m128i*)(slice_row + j), vec);\n            j += 16;\n        }\n\n        for (; j < dim2; ++j) {\n            slice_row[j] = row_start[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_rows = (i + block_size > rows) ? (rows - i) : block_size;\n            size_t block_cols = (j + block_size > cols) ? (cols - j) : block_size;\n\n            for (size_t bi = 0; bi < block_rows; ++bi) {\n                for (size_t bj = 0; bj < block_cols; bj += 16) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const*)(src + (i + bi) * cols + j + bj));\n                    __m256i permuted = _mm256_permute4x64_epi64(vec, 0xD8);\n                    _mm256_storeu_si256((__m256i*)(dst + (j + bj) * rows + i + bi), permuted);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_rows = (i + block_size > rows) ? (rows - i) : block_size;\n            size_t block_cols = (j + block_size > cols) ? (cols - j) : block_size;\n\n            for (size_t bi = 0; bi < block_rows; ++bi) {\n                for (size_t bj = 0; bj < block_cols; bj += 16) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const*)(src + (i + bi) * cols + j + bj));\n                    __m256i permuted = _mm256_permute4x64_epi64(vec, 0xD8);\n                    _mm256_storeu_si256((__m256i*)(dst + (j + bj) * rows + i + bi), permuted);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_rows = (i + block_size > rows) ? (rows - i) : block_size;\n            size_t block_cols = (j + block_size > cols) ? (cols - j) : block_size;\n\n            for (size_t bi = 0; bi < block_rows; ++bi) {\n                for (size_t bj = 0; bj < block_cols; bj += 16) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const*)(src + (i + bi) * cols + j + bj));\n                    _mm256_storeu_si256((__m256i*)(dst + (j + bj) * rows + i + bi), vec);\n                }\n            }\n\n            // Handle remaining columns if block_cols is not a multiple of 16\n            for (size_t bi = 0; bi < block_rows; ++bi) {\n                for (size_t bj = (block_cols / 16) * 16; bj < block_cols; ++bj) {\n                    dst[(j + bj) * rows + i + bi] = src[(i + bi) * cols + j + bj];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_rows = (i + block_size > rows) ? (rows - i) : block_size;\n            size_t block_cols = (j + block_size > cols) ? (cols - j) : block_size;\n\n            for (size_t bi = 0; bi < block_rows; bi++) {\n                for (size_t bj = 0; bj < block_cols; bj += 16) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const*)(src + (i + bi) * cols + j + bj));\n                    __m256i permuted = _mm256_permute4x64_epi64(vec, 0xD8);\n                    _mm256_storeu_si256((__m256i*)(dst + (j + bj) * rows + i + bi), permuted);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            size_t block_rows = (i + block_size > rows) ? (rows - i) : block_size;\n            size_t block_cols = (j + block_size > cols) ? (cols - j) : block_size;\n\n            for (size_t bi = 0; bi < block_rows; ++bi) {\n                for (size_t bj = 0; bj < block_cols; bj += 16) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const*)(src + (i + bi) * cols + j + bj));\n                    __m256i permuted = _mm256_permute4x64_epi64(vec, 0xD8);\n                    _mm256_storeu_si256((__m256i*)(dst + (j + bj) * rows + i + bi), permuted);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 vec_sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 7 < dim; i += 8) {\n        __m256 diag_elements = _mm256_setzero_ps();\n        for (size_t j = 0; j < 8; ++j) {\n            size_t index = (i + j) * dim * dim + (i + j) * dim + (i + j);\n            diag_elements = _mm256_insert_ps(diag_elements, _mm_load_ss(&array[index]), j << 4);\n        }\n        vec_sum = _mm256_add_ps(vec_sum, diag_elements);\n    }\n\n    // Horizontal sum of the vector\n    float sums[8];\n    _mm256_storeu_ps(sums, vec_sum);\n    float total_sum = sums[0] + sums[1] + sums[2] + sums[3] + sums[4] + sums[5] + sums[6] + sums[7];\n\n    // Handle remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * dim * dim + i * dim + i;\n        total_sum += array[index];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 vec_sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 7 < dim; i += 8) {\n        __m256 diag_elements = _mm256_set_ps(\n            array[(i+7)*dim*dim + (i+7)*dim + (i+7)],\n            array[(i+6)*dim*dim + (i+6)*dim + (i+6)],\n            array[(i+5)*dim*dim + (i+5)*dim + (i+5)],\n            array[(i+4)*dim*dim + (i+4)*dim + (i+4)],\n            array[(i+3)*dim*dim + (i+3)*dim + (i+3)],\n            array[(i+2)*dim*dim + (i+2)*dim + (i+2)],\n            array[(i+1)*dim*dim + (i+1)*dim + (i+1)],\n            array[i*dim*dim + i*dim + i]\n        );\n        vec_sum = _mm256_add_ps(vec_sum, diag_elements);\n    }\n\n    // Horizontal sum of the vector\n    float temp[8];\n    _mm256_storeu_ps(temp, vec_sum);\n    float sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    // Process remaining elements\n    for (; i < dim; ++i) {\n        sum += array[i*dim*dim + i*dim + i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 vec_sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 7 < dim; i += 8) {\n        __m256 diag_elements = _mm256_set_ps(\n            array[(i+7)*dim*dim + (i+7)*dim + (i+7)],\n            array[(i+6)*dim*dim + (i+6)*dim + (i+6)],\n            array[(i+5)*dim*dim + (i+5)*dim + (i+5)],\n            array[(i+4)*dim*dim + (i+4)*dim + (i+4)],\n            array[(i+3)*dim*dim + (i+3)*dim + (i+3)],\n            array[(i+2)*dim*dim + (i+2)*dim + (i+2)],\n            array[(i+1)*dim*dim + (i+1)*dim + (i+1)],\n            array[i*dim*dim + i*dim + i]\n        );\n        vec_sum = _mm256_add_ps(vec_sum, diag_elements);\n    }\n\n    // Horizontal sum of the vector\n    float temp[8];\n    _mm256_storeu_ps(temp, vec_sum);\n    float sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    // Process remaining elements\n    for (; i < dim; ++i) {\n        sum += array[i*dim*dim + i*dim + i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 vec_sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 7 < dim; i += 8) {\n        __m256 diag_elements = _mm256_set_ps(\n            array[(i+7)*dim*dim + (i+7)*dim + (i+7)],\n            array[(i+6)*dim*dim + (i+6)*dim + (i+6)],\n            array[(i+5)*dim*dim + (i+5)*dim + (i+5)],\n            array[(i+4)*dim*dim + (i+4)*dim + (i+4)],\n            array[(i+3)*dim*dim + (i+3)*dim + (i+3)],\n            array[(i+2)*dim*dim + (i+2)*dim + (i+2)],\n            array[(i+1)*dim*dim + (i+1)*dim + (i+1)],\n            array[i*dim*dim + i*dim + i]\n        );\n        vec_sum = _mm256_add_ps(vec_sum, diag_elements);\n    }\n\n    // Horizontal sum of the vector\n    float temp[8];\n    _mm256_storeu_ps(temp, vec_sum);\n    float sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    // Process remaining elements\n    for (; i < dim; ++i) {\n        sum += array[i*dim*dim + i*dim + i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```cpp\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 vec_sum = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 7 < dim; i += 8) {\n        __m256 diag_elements = _mm256_setzero_ps();\n        for (size_t j = 0; j < dim; ++j) {\n            for (size_t k = 0; k < dim; ++k) {\n                if (j == k && j >= i && j < i + 8) {\n                    size_t index = j * dim * dim + j * dim + j;\n                    diag_elements = _mm256_add_ps(diag_elements, _mm256_loadu_ps(&array[index]));\n                }\n            }\n        }\n        vec_sum = _mm256_add_ps(vec_sum, diag_elements);\n    }\n\n    // Horizontal sum of the vector\n    float result[8];\n    _mm256_storeu_ps(result, vec_sum);\n    float final_sum = result[0] + result[1] + result[2] + result[3] +\n                      result[4] + result[5] + result[6] + result[7];\n\n    // Handle remaining elements\n    for (; i < dim; ++i) {\n        size_t index = i * dim * dim + i * dim + i;\n        final_sum += array[index];\n    }\n\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t total_elements = rows * cols;\n    size_t simd_aligned_elements = total_elements - (total_elements % simd_width);\n\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX\n    for (; i < simd_aligned_elements; i += simd_width) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]); // Load 4 doubles from src\n        __m256d v_cmp = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ); // Compare > threshold\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale); // Scale the values\n        __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_cmp); // Blend based on comparison\n        _mm256_storeu_pd(&dst[i], v_result); // Store 4 doubles to dst\n    }\n\n    // Process remaining elements (if any) using scalar code\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t total_elements = rows * cols;\n    size_t simd_limit = total_elements - (total_elements % simd_width);\n\n    size_t i = 0;\n    for (; i < simd_limit; i += simd_width) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]);\n        __m256d v_cmp = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n        __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_cmp);\n        _mm256_storeu_pd(&dst[i], v_result);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t total_elements = rows * cols;\n    size_t simd_limit = total_elements - (total_elements % simd_width);\n\n    size_t i = 0;\n    for (; i < simd_limit; i += simd_width) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]);\n        __m256d v_cmp = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n        __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_cmp);\n        _mm256_storeu_pd(&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t total_elements = rows * cols;\n    size_t simd_limit = total_elements - (total_elements % simd_width);\n\n    size_t i = 0;\n    for (; i < simd_limit; i += simd_width) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]);\n        __m256d v_cmp = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ);\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale);\n        __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_cmp);\n        _mm256_storeu_pd(&dst[i], v_result);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256d v_threshold = _mm256_set1_pd(threshold);\n    __m256d v_scale = _mm256_set1_pd(scale);\n\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t total_elements = rows * cols;\n    size_t simd_aligned_elements = total_elements - (total_elements % simd_width);\n\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX\n    for (; i < simd_aligned_elements; i += simd_width) {\n        __m256d v_src = _mm256_loadu_pd(&src[i]); // Load 4 doubles from src\n        __m256d v_cmp = _mm256_cmp_pd(v_src, v_threshold, _CMP_GT_OQ); // Compare > threshold\n        __m256d v_scaled = _mm256_mul_pd(v_src, v_scale); // Scale the values\n        __m256d v_result = _mm256_blendv_pd(v_src, v_scaled, v_cmp); // Blend based on comparison\n        _mm256_storeu_pd(&dst[i], v_result); // Store 4 doubles to dst\n    }\n\n    // Process remaining elements (if any) using scalar code\n    for (; i < total_elements; ++i) {\n        if (src[i] > threshold) {\n            dst[i] = src[i] * scale;\n        } else {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time (256 bits / 64 bits per double)\n    size_t aligned_cols = (cols / simd_width) * simd_width;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i] * cols;\n        size_t dst_row_idx = i * cols;\n\n        // Process aligned columns using AVX\n        size_t j = 0;\n        for (; j < aligned_cols; j += simd_width) {\n            __m256d vec = _mm256_loadu_pd(&src[src_row_idx + j]);\n            _mm256_storeu_pd(&dst[dst_row_idx + j], vec);\n        }\n\n        // Process remaining unaligned columns\n        for (; j < cols; ++j) {\n            dst[dst_row_idx + j] = src[src_row_idx + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time (256 bits / 64 bits per double)\n    size_t aligned_cols = (cols / simd_width) * simd_width;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i] * cols;\n        size_t dst_row_idx = i * cols;\n\n        // Process aligned columns using AVX\n        size_t j = 0;\n        for (; j < aligned_cols; j += simd_width) {\n            __m256d vec = _mm256_loadu_pd(&src[src_row_idx + j]);\n            _mm256_storeu_pd(&dst[dst_row_idx + j], vec);\n        }\n\n        // Process remaining columns (if any) without AVX\n        for (; j < cols; ++j) {\n            dst[dst_row_idx + j] = src[src_row_idx + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time (256 bits / 64 bits per double)\n    size_t aligned_cols = (cols / simd_width) * simd_width;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i] * cols;\n        size_t dst_row_idx = i * cols;\n\n        // Process aligned columns using AVX\n        size_t j = 0;\n        for (; j < aligned_cols; j += simd_width) {\n            __m256d vec = _mm256_loadu_pd(&src[src_row_idx + j]);\n            _mm256_storeu_pd(&dst[dst_row_idx + j], vec);\n        }\n\n        // Process remaining unaligned columns\n        for (; j < cols; ++j) {\n            dst[dst_row_idx + j] = src[src_row_idx + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time (256 bits / 64 bits per double)\n    size_t aligned_cols = (cols / simd_width) * simd_width;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i] * cols;\n        size_t dst_row_idx = i * cols;\n\n        // Process aligned columns using AVX\n        size_t j = 0;\n        for (; j < aligned_cols; j += simd_width) {\n            __m256d vec = _mm256_loadu_pd(&src[src_row_idx + j]);\n            _mm256_storeu_pd(&dst[dst_row_idx + j], vec);\n        }\n\n        // Process remaining columns (if any) without AVX\n        for (; j < cols; ++j) {\n            dst[dst_row_idx + j] = src[src_row_idx + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time (256 bits / 64 bits per double)\n    size_t aligned_cols = (cols / simd_width) * simd_width;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i] * cols;\n        size_t dst_row_idx = i * cols;\n\n        // Process aligned columns using AVX\n        size_t j = 0;\n        for (; j < aligned_cols; j += simd_width) {\n            __m256d vec = _mm256_loadu_pd(&src[src_row_idx + j]);\n            _mm256_storeu_pd(&dst[dst_row_idx + j], vec);\n        }\n\n        // Process remaining columns (if any) without AVX\n        for (; j < cols; ++j) {\n            dst[dst_row_idx + j] = src[src_row_idx + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    __m256i v_two = _mm256_set1_epi64x(2);\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Add src1 and src2 element-wise\n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n\n        // Add scalar to each element\n        v_sum = _mm256_add_epi64(v_sum, v_scalar);\n\n        // Divide by 2 (shift right by 1 for round-toward-zero)\n        __m256i v_result = _mm256_srli_epi64(_mm256_add_epi64(v_sum, v_two), 1);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = sum / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n    __m256i v_two = _mm256_set1_epi64x(2);\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Add src1 and src2\n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n\n        // Add scalar to each element\n        v_sum = _mm256_add_epi64(v_sum, v_scalar);\n\n        // Divide by 2 (shift right arithmetic)\n        __m256i v_result = _mm256_srai_epi64(_mm256_add_epi64(v_sum, v_two), 1);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = (sum >= 0) ? (sum / 2) : ((sum - 1) / 2);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    __m256i vtwo = _mm256_set1_epi64x(2);\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Add src1 and src2\n        __m256i vsum = _mm256_add_epi64(vsrc1, vsrc2);\n\n        // Add scalar to each element\n        vsum = _mm256_add_epi64(vsum, vscalar);\n\n        // Divide by 2 (shift right by 1 for round-toward-zero)\n        __m256i vresult = _mm256_srai_epi64(_mm256_add_epi64(vsum, vtwo), 1);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = (sum >= 0) ? (sum / 2) : ((sum - 1) / 2);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    __m256i vtwo = _mm256_set1_epi64x(2);\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Add src1 and src2\n        __m256i vsum = _mm256_add_epi64(vsrc1, vsrc2);\n\n        // Add scalar to each element\n        vsum = _mm256_add_epi64(vsum, vscalar);\n\n        // Divide by 2 (shift right by 1)\n        __m256i vresult = _mm256_srli_epi64(_mm256_add_epi64(vsum, vtwo), 1);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = sum / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n    __m256i vtwo = _mm256_set1_epi64x(2);\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Add src1 and src2 element-wise\n        __m256i vsum = _mm256_add_epi64(vsrc1, vsrc2);\n\n        // Add scalar to each element\n        vsum = _mm256_add_epi64(vsum, vscalar);\n\n        // Divide by 2 (shift right by 1 for round-toward-zero)\n        __m256i vresult = _mm256_srli_epi64(_mm256_add_epi64(vsum, vtwo), 1);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int64_t sum = src1[i] + src2[i] + scalar;\n        dst[i] = sum / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(INT32_MAX);\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 7 < length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const *)(src + i));\n\n        // Create a mask to select even indices (0, 2, 4, 6)\n        __m256i mask = _mm256_setr_epi32(0, INT32_MAX, 0, INT32_MAX, 0, INT32_MAX, 0, INT32_MAX);\n        __m256i even_elements = _mm256_blendv_epi8(_mm256_set1_epi32(INT32_MAX), vec, mask);\n\n        // Update the minimum value\n        min_vec = _mm256_min_epi32(min_vec, even_elements);\n    }\n\n    // Extract the minimum from the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i *)temp, min_vec);\n\n    int min_val = INT32_MAX;\n    for (int j = 0; j < 8; j++) {\n        if (temp[j] < min_val) {\n            min_val = temp[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (i % 2 == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    int min_val = 0x7FFFFFFF;\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Create a mask to select even indices (0, 2, 4, 6)\n        __m256i mask = _mm256_setr_epi32(0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF);\n        __m256i even_elements = _mm256_blendv_epi8(_mm256_set1_epi32(0x7FFFFFFF), vec, mask);\n\n        // Update the minimum value vector\n        min_vec = _mm256_min_epi32(min_vec, even_elements);\n    }\n\n    // Extract the minimum from the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, min_vec);\n    for (int j = 0; j < 8; j++) {\n        if (temp[j] < min_val) {\n            min_val = temp[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (i % 2 == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    int min_val = 0x7FFFFFFF;\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Mask to select even indices: 0xFF for even, 0x00 for odd\n        __m256i mask = _mm256_setr_epi32(0xFFFFFFFF, 0, 0xFFFFFFFF, 0, \n                                          0xFFFFFFFF, 0, 0xFFFFFFFF, 0);\n        __m256i masked_vec = _mm256_blendv_epi8(_mm256_set1_epi32(0x7FFFFFFF), vec, mask);\n\n        // Horizontal minimum across the vector\n        min_vec = _mm256_min_epi32(min_vec, masked_vec);\n    }\n\n    // Find the minimum value in the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, min_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] < min_val) {\n            min_val = temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (i % 2 == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    int min_val = 0x7FFFFFFF;\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i even_mask = _mm256_setr_epi32(0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n\n        // Set odd positions to max int so they don't affect the minimum\n        __m256i odd_mask = _mm256_setr_epi32(0, 0x7FFFFFFF, 0, 0x7FFFFFFF, 0, 0x7FFFFFFF, 0, 0x7FFFFFFF);\n        even_elements = _mm256_or_si256(even_elements, odd_mask);\n\n        min_vec = _mm256_min_epi32(min_vec, even_elements);\n    }\n\n    // Horizontal reduction to find the minimum in the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, min_vec);\n    for (int j = 0; j < 8; j++) {\n        if (temp[j] < min_val) {\n            min_val = temp[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (i % 2 == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) return -1;\n\n    __m256i min_vec = _mm256_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    int min_val = 0x7FFFFFFF;\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i even_mask = _mm256_setr_epi32(0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0, 0xFFFFFFFF, 0);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n        min_vec = _mm256_min_epi32(min_vec, even_elements);\n    }\n\n    // Extract minimum from the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, min_vec);\n    for (int j = 0; j < 8; j++) {\n        if (temp[j] != 0 && temp[j] < min_val) {\n            min_val = temp[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (i % 2 == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val == 0x7FFFFFFF ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Arithmetic right shift\n        __m256i arshift = _mm256_srai_epi32(vsrc, shift);\n\n        // Logical right shift (convert to unsigned, shift, then convert back to signed)\n        __m256i ulogical = _mm256_srli_epi32(_mm256_castsi256_si256(vsrc), shift);\n        __m256i lrshift = _mm256_castsi256_si256(ulogical);\n\n        // Add arithmetic and logical shifts\n        __m256i result = _mm256_add_epi32(arshift, lrshift);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int arshift = src[i] >> shift; // Arithmetic right shift\n        int lrshift = (int)((unsigned int)src[i] >> shift); // Logical right shift\n        dst[i] = arshift + lrshift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Arithmetic right shift\n        __m256i arshift = _mm256_srai_epi32(vsrc, shift);\n\n        // Logical right shift (convert to unsigned, shift, then convert back to signed)\n        __m256i ulogical = _mm256_srli_epi32(_mm256_castsi256_si256(vsrc), shift);\n\n        // Add arithmetic and logical shifts\n        __m256i result = _mm256_add_epi32(arshift, ulogical);\n\n        // Store the result into the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int arshift = src[i] >> shift; // Arithmetic right shift\n        int ulogical = (unsigned int)(src[i]) >> shift; // Logical right shift\n        dst[i] = arshift + ulogical;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Arithmetic right shift\n        __m256i arith_shifted = _mm256_srai_epi32(vsrc, shift);\n\n        // Logical right shift (convert to unsigned, shift, then convert back to signed)\n        __m256i logical_shifted = _mm256_srli_epi32(_mm256_castsi256_si256(vsrc), shift);\n\n        // Add arithmetic and logical shifts\n        __m256i result = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int arith_shifted = src[i] >> shift;\n        int logical_shifted = (unsigned int)(src[i]) >> shift;\n        dst[i] = arith_shifted + logical_shifted;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Perform arithmetic right shift\n        __m256i arshift = _mm256_srai_epi32(vsrc, shift);\n\n        // Convert to unsigned for logical right shift\n        __m256i vsrc_unsigned = _mm256_castsi256_si256(vsrc); // No-op cast to unsigned view\n        __m256i lrshift = _mm256_srli_epi32(vsrc_unsigned, shift);\n\n        // Add the results of arithmetic and logical shifts\n        __m256i result = _mm256_add_epi32(arshift, lrshift);\n\n        // Store the result into the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        int arshift_val = src[i] >> shift; // Arithmetic right shift\n        int lrshift_val = (unsigned int)(src[i]) >> shift; // Logical right shift\n        dst[i] = arshift_val + lrshift_val;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 integers from the source array\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Arithmetic right shift\n        __m256i arshift = _mm256_srai_epi32(vsrc, shift);\n\n        // Logical right shift (convert to unsigned, shift, then convert back to signed)\n        __m256i ulogical = _mm256_srli_epi32(_mm256_castsi256_si256(vsrc), shift);\n        __m256i lrshift = _mm256_castsi256_si256(ulogical);\n\n        // Add arithmetic and logical shifts\n        __m256i result = _mm256_add_epi32(arshift, lrshift);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int arshift = src[i] >> shift; // Arithmetic right shift\n        int lrshift = (int)((unsigned int)src[i] >> shift); // Logical right shift\n        dst[i] = arshift + lrshift;\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i vsum = _mm256_setzero_si256();\n    size_t i = 0;\n    size_t total_elements = rows * cols;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(matrix + i));\n        vsum = _mm256_add_epi32(vsum, v);\n    }\n\n    // Horizontal sum of the 8 packed integers in vsum\n    int64_t sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vsum);\n    for (int j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Add remaining elements that didn't fit into the AVX loop\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i vsum = _mm256_setzero_si256();\n    size_t i = 0;\n    size_t total_elements = rows * cols;\n\n    // Process 8 elements at a time\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(matrix + i));\n        vsum = _mm256_add_epi32(vsum, v);\n    }\n\n    // Horizontal sum of the AVX register\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    int64_t sum = temp[0] + temp[1] + temp[2] + temp[3] +\n                  temp[4] + temp[5] + temp[6] + temp[7];\n\n    // Add remaining elements\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i = 0;\n    size_t total_elements = rows * cols;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    // Horizontal sum of the vector\n    int64_t result = 0;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (size_t j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n\n    // Add remaining elements\n    for (; i < total_elements; ++i) {\n        result += matrix[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i vsum = _mm256_setzero_si256();\n    size_t i = 0;\n    size_t total_elements = rows * cols;\n\n    // Process 8 elements at a time\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(matrix + i));\n        vsum = _mm256_add_epi32(vsum, v);\n    }\n\n    // Horizontal sum of the vector\n    int64_t sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vsum);\n    for (size_t j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Add remaining elements\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i vsum = _mm256_setzero_si256();\n    size_t i = 0;\n    size_t total_elements = rows * cols;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < total_elements; i += 8) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(matrix + i));\n        vsum = _mm256_add_epi32(vsum, v);\n    }\n\n    // Horizontal sum of the 8 partial sums in vsum\n    int64_t sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vsum);\n    for (size_t j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    // Add remaining elements that didn't fit into the vectorized loop\n    for (; i < total_elements; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t i = 0;\n    __m256i max_vals = _mm256_set1_epi8(-128); // Initialize to lowest possible value\n    __m256i indices = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n    __m256i max_indices = _mm256_setzero_si256();\n    \n    // Process 32 elements at a time\n    for (; i + 31 < length; i += 32) {\n        __m256i vals = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(vals, max_vals);\n        max_vals = _mm256_max_epi8(max_vals, vals);\n        max_indices = _mm256_blendv_epi8(max_indices, _mm256_add_epi8(indices, _mm256_set1_epi8(i)), mask);\n    }\n\n    // Find the maximum value and its index in the vector\n    int8_t temp[32];\n    uint8_t idx_temp[32];\n    _mm256_storeu_si256((__m256i *)temp, max_vals);\n    _mm256_storeu_si256((__m256i *)idx_temp, max_indices);\n\n    int8_t max_val = -128;\n    size_t max_idx = 0;\n    for (int j = 0; j < 32; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n            max_idx = idx_temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t i = 0;\n    int8_t max_val = INT8_MIN;\n    size_t max_idx = 0;\n\n    // Process 32 elements at a time using AVX2\n    __m256i idx_vec = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n    __m256i max_vec = _mm256_set1_epi8(INT8_MIN);\n    __m256i max_idx_vec = _mm256_set1_epi8(-1);\n\n    for (; i + 31 < length; i += 32) {\n        __m256i src_vec = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp_mask = _mm256_cmpgt_epi8(src_vec, max_vec);\n\n        max_idx_vec = _mm256_blendv_epi8(max_idx_vec, idx_vec, cmp_mask);\n        max_vec = _mm256_max_epi8(max_vec, src_vec);\n\n        idx_vec = _mm256_add_epi8(idx_vec, _mm256_set1_epi8(32));\n    }\n\n    // Extract the maximum value and index from the vector\n    int8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, max_vec);\n    int8_t temp_idx[32];\n    _mm256_storeu_si256((__m256i*)temp_idx, max_idx_vec);\n\n    for (int j = 0; j < 32; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n            max_idx = i - 32 + temp_idx[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t i = 0;\n    __m256i max_vals = _mm256_set1_epi8(-128); // Initialize to minimum possible value\n    __m256i indices = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n    __m256i max_indices = _mm256_setzero_si256();\n    size_t max_index = 0;\n\n    // Process 32 elements at a time\n    for (; i + 31 < length; i += 32) {\n        __m256i vals = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i cmp = _mm256_cmpgt_epi8(vals, max_vals);\n        max_vals = _mm256_max_epi8(max_vals, vals);\n        max_indices = _mm256_blendv_epi8(max_indices, _mm256_add_epi8(indices, _mm256_set1_epi8(i)), cmp);\n    }\n\n    // Find the maximum value in the vector\n    int8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, max_vals);\n    int8_t max_val = -128;\n    for (int j = 0; j < 32; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n\n    // Find the corresponding index of the maximum value\n    _mm256_storeu_si256((__m256i*)temp, max_indices);\n    for (int j = 0; j < 32; ++j) {\n        if (temp[j] >= 0 && src[temp[j]] == max_val) {\n            max_index = temp[j];\n            break;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_vals = _mm256_set1_epi8(INT8_MIN);\n    __m256i indices = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n                                        15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n    __m256i increment = _mm256_set1_epi8(32);\n    __m256i max_indices = _mm256_set1_epi8(0);\n\n    size_t i = 0;\n    for (; i + 31 < length; i += 32) {\n        __m256i vals = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(vals, max_vals);\n        max_vals = _mm256_max_epi8(max_vals, vals);\n        max_indices = _mm256_blendv_epi8(max_indices, indices, mask);\n        indices = _mm256_add_epi8(indices, increment);\n    }\n\n    // Find the maximum value in the vector\n    int8_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, max_vals);\n    int8_t max_val = INT8_MIN;\n    for (int j = 0; j < 32; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n\n    // Find the corresponding index\n    size_t max_index = 0;\n    for (size_t j = 0; j < i; j += 32) {\n        for (int k = 0; k < 32 && j + k < length; ++k) {\n            if (src[j + k] == max_val) {\n                return j + k;\n            }\n        }\n    }\n\n    // Check remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```cpp\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_vals = _mm256_set1_epi8(INT8_MIN);\n    __m256i indices = _mm256_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n    __m256i increment = _mm256_set1_epi8(32);\n    __m256i max_indices = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 31 < length; i += 32) {\n        __m256i src_vals = _mm256_loadu_si256((const __m256i*)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(src_vals, max_vals);\n        max_vals = _mm256_blendv_epi8(max_vals, src_vals, mask);\n        max_indices = _mm256_blendv_epi8(max_indices, indices, mask);\n        indices = _mm256_add_epi8(indices, increment);\n    }\n\n    int8_t temp_max[32];\n    uint8_t temp_indices[32];\n    _mm256_storeu_si256((__m256i*)temp_max, max_vals);\n    _mm256_storeu_si256((__m256i*)temp_indices, max_indices);\n\n    int8_t max_val = INT8_MIN;\n    size_t max_index = 0;\n    for (size_t j = 0; j < 32; ++j) {\n        if (temp_max[j] > max_val) {\n            max_val = temp_max[j];\n            max_index = temp_indices[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi32(\n            i + 15, i + 14, i + 13, i + 12,\n            i + 11, i + 10, i + 9, i + 8,\n            i + 7, i + 6, i + 5, i + 4,\n            i + 3, i + 2, i + 1, i\n        );\n\n        __m256i and_result = _mm256_and_si256(indices, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n        _mm256_storeu_si256((__m256i*)&dst[i], blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi32(\n            i + 15, i + 14, i + 13, i + 12,\n            i + 11, i + 10, i + 9, i + 8,\n            i + 7, i + 6, i + 5, i + 4,\n            i + 3, i + 2, i + 1, i + 0\n        );\n\n        __m256i and_result = _mm256_and_si256(indices, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n        _mm256_storeu_si256((__m256i*)&dst[i], blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi32(\n            i + 15, i + 14, i + 13, i + 12,\n            i + 11, i + 10, i + 9, i + 8,\n            i + 7, i + 6, i + 5, i + 4,\n            i + 3, i + 2, i + 1, i + 0\n        );\n\n        __m256i and_result = _mm256_and_si256(indices, mask_vec);\n        __m256i cmp_result = _mm256_cmpgt_epi32(and_result, _mm256_setzero_si256());\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i blend_result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_result);\n        _mm256_storeu_si256((__m256i*)(dst + i), blend_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    // Process 16 elements at a time (256 bits / 16 bits per element)\n    for (; i + 16 <= length; i += 16) {\n        // Load 16 elements from src1 and src2\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Create index vector [i, i+1, ..., i+15]\n        __m256i indices = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8,\n                                            i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n\n        // Compute (index & mask) for each index\n        __m256i masked_indices = _mm256_and_si256(indices, mask_vec);\n\n        // Create a comparison mask where (masked_indices != 0)\n        __m256i cmp_mask = _mm256_cmpgt_epi32(masked_indices, _mm256_setzero_si256());\n\n        // Blend src1 and src2 based on the comparison mask\n        __m256i result = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_mask);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n\n    // Process 16 elements at a time (256 bits / 16 bits per element)\n    for (; i + 16 <= length; i += 16) {\n        // Load 16 elements from src1 and src2\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        // Create index vector [i, i+1, ..., i+15]\n        __m256i indices = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8,\n                                           i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n\n        // Compute bitwise AND of indices and mask\n        __m256i masked_indices = _mm256_and_si256(indices, mask_vec);\n\n        // Compare if the result is non-zero\n        __m256i cmp_mask = _mm256_cmpgt_epi32(masked_indices, _mm256_setzero_si256());\n\n        // Blend based on the comparison mask\n        __m256i blended = _mm256_blendv_epi8(src2_vec, src1_vec, cmp_mask);\n\n        // Store the result in dst\n        _mm256_storeu_si256((__m256i*)&dst[i], blended);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 uint32_t values from the source array\n        __m256i v = _mm256_loadu_si256((const __m256i*)(src + i));\n\n        // Compute population count for each of the 8 values\n        const __m256i lookup0 = _mm256_setr_epi8(\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n        );\n        const __m256i lookup1 = _mm256_setr_epi8(\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n            0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n        );\n\n        __m256i low_mask = _mm256_set1_epi8(0x0F);\n        __m256i lo = _mm256_and_si256(v, low_mask);\n        __m256i hi = _mm256_and_si256(_mm256_srli_epi16(v, 4), low_mask);\n\n        __m256i popcnt_lo = _mm256_shuffle_epi8(lookup0, lo);\n        __m256i popcnt_hi = _mm256_shuffle_epi8(lookup1, hi);\n\n        __m256i popcnt = _mm256_add_epi8(popcnt_lo, popcnt_hi);\n\n        // Horizontal sum of bytes into 32-bit integers\n        __m256i t0 = _mm256_sad_epu8(popcnt, _mm256_setzero_si256());\n        __m256i t1 = _mm256_permute4x64_epi64(t0, 0b10110001);\n        __m256i t2 = _mm256_add_epi32(t0, t1);\n        __m128i t3 = _mm256_castsi256_si128(t2);\n        __m128i t4 = _mm_add_epi32(t3, _mm_unpackhi_epi64(t3, t3));\n\n        // Store the results in the destination array\n        _mm_storeu_si128((__m128i*)(dst + i), t4);\n    }\n\n    // Process remaining elements (if any) with scalar code\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i)); // Load 8 uint32_t values\n        __m256i popcnt = _mm256_setzero_si256();                     // Initialize result vector to zero\n\n        // Perform population count for each 32-bit integer in the vector\n        for (int j = 0; j < 32; j++) {\n            __m256i mask = _mm256_set1_epi32(1 << j);                // Create a mask for the j-th bit\n            __m256i bits = _mm256_and_si256(vec, mask);              // Isolate the j-th bit\n            popcnt = _mm256_sub_epi32(popcnt, _mm256_srli_epi32(bits, j)); // Add the bit to the population count\n        }\n\n        // Store the results into the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), popcnt);\n    }\n\n    // Process remaining elements (if any) using scalar code\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i)); // Load 8 uint32_t values\n        __m256i popcnt = _mm256_setzero_si256();                     // Initialize result vector to zero\n\n        // Count bits in each 32-bit integer using AVX2 intrinsics\n        for (int k = 0; k < 32; ++k) {\n            __m256i mask = _mm256_set1_epi32(1 << k);                // Create a mask for the k-th bit\n            __m256i bits = _mm256_and_si256(vec, mask);              // Isolate the k-th bit\n            popcnt = _mm256_sub_epi32(popcnt, _mm256_srli_epi32(bits, k)); // Accumulate the count\n        }\n\n        // Store the results into the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), popcnt);\n    }\n\n    // Process remaining elements using scalar code\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i)); // Load 8 uint32_t values\n        __m256i count = _mm256_setzero_si256(); // Initialize count vector to zero\n\n        // Perform population count using AVX2 bitwise operations\n        for (int k = 0; k < 5; ++k) {\n            __m256i mask = _mm256_set1_epi32((1 << (1 << k)) - 1);\n            __m256i temp = _mm256_and_si256(data, mask);\n            count = _mm256_add_epi32(count, temp);\n            data = _mm256_srli_epi32(data, (1 << k));\n        }\n\n        // Horizontal sum of the counts within each lane\n        __m256i shuffle1 = _mm256_shuffle_epi32(count, 0xB1); // Swap high and low halves\n        __m256i sum1 = _mm256_add_epi32(count, shuffle1);\n\n        __m256i shuffle2 = _mm256_permute4x64_epi64(sum1, 0x4E); // Swap 128-bit halves\n        __m256i sum2 = _mm256_add_epi32(sum1, shuffle2);\n\n        __m256i shuffle3 = _mm256_shuffle_epi32(sum2, 0x0F); // Broadcast lowest element\n        __m256i final_sum = _mm256_add_epi32(sum2, shuffle3);\n\n        // Store results back to memory\n        _mm256_storeu_si256((__m256i*)(dst + i), final_sum);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(src + i)); // Load 8 uint32_t values\n        __m256i count = _mm256_setzero_si256(); // Initialize count vector to zero\n\n        // Perform population count using AVX2 bitwise operations\n        for (int k = 0; k < 5; ++k) { // Unroll the bit-counting loop\n            __m256i mask = _mm256_set1_epi32((1 << (1 << k)) - 1);\n            __m256i masked = _mm256_and_si256(vec, mask);\n            __m256i shifted = _mm256_srli_epi32(vec, (1 << k));\n            vec = _mm256_add_epi32(masked, shifted);\n        }\n\n        // Extract counts into destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), vec);\n    }\n\n    // Process remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i max_val = _mm256_set1_epi16(0xFFFF); // UINT16_MAX\n\n    // Process 16 elements at a time\n    for (; i + 15 < length; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&src1[i]); // Load 16 elements from src1\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&src2[i]); // Load 16 elements from src2\n        __m256i result = _mm256_adds_epu16(v1, v2);          // Saturating addition\n        _mm256_storeu_si256((__m256i*)&dst[i], result);      // Store 16 elements to dst\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] < src1[i]) ? 0xFFFF : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 16; // AVX2 processes 16 elements of uint16_t at a time\n\n    // Process in chunks of 16 elements using AVX2 intrinsics\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i const*)(src1 + i)); // Load 16 elements from src1\n        __m256i vec2 = _mm256_loadu_si256((__m256i const*)(src2 + i)); // Load 16 elements from src2\n        __m256i result = _mm256_adds_epu16(vec1, vec2);               // Perform saturating addition\n        _mm256_storeu_si256((__m256i*)(dst + i), result);             // Store the result in dst\n    }\n\n    // Handle remaining elements (if any) with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : (src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 16; // AVX2 processes 16 elements of uint16_t per __m256i\n\n    // Process in chunks of 16 elements using AVX2 intrinsics\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i v_result = _mm256_adds_epu16(v_src1, v_src2);\n        _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n    }\n\n    // Handle remaining elements (if any) with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : (src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t simd_width = 16; // AVX2 processes 16 elements of uint16_t per __m256i\n\n    // Process in chunks of 16 elements using AVX2 intrinsics\n    for (; i + simd_width <= length; i += simd_width) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i v_result = _mm256_adds_epu16(v_src1, v_src2);\n        _mm256_storeu_si256((__m256i*)(dst + i), v_result);\n    }\n\n    // Handle remaining elements (if any) with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : (src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length / 16 * 16;\n\n    for (; i < simd_length; i += 16) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i vec2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] > UINT16_MAX - src2[i]) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    const __m256d lower_bound = _mm256_set1_pd(-100.0);\n    const __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            __m256d c_sum = _mm256_setzero_pd();\n            size_t j = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; j + 3 < n; j += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + j]);\n                __m256d b_vec = _mm256_loadu_pd(&B[j * p + k]);\n\n                // Check if elements in A and B are within the range [-100, 100]\n                __m256d a_mask = _mm256_and_pd(_mm256_cmp_pd(a_vec, lower_bound, _CMP_GE_OS),\n                                               _mm256_cmp_pd(a_vec, upper_bound, _CMP_LE_OS));\n                __m256d b_mask = _mm256_and_pd(_mm256_cmp_pd(b_vec, lower_bound, _CMP_GE_OS),\n                                               _mm256_cmp_pd(b_vec, upper_bound, _CMP_LE_OS));\n                __m256d mask = _mm256_and_pd(a_mask, b_mask);\n\n                // Multiply and accumulate only valid elements\n                __m256d prod = _mm256_mul_pd(a_vec, b_vec);\n                prod = _mm256_and_pd(prod, mask);\n                c_sum = _mm256_add_pd(c_sum, prod);\n            }\n\n            // Horizontal sum of the accumulated vector\n            double temp[4];\n            _mm256_storeu_pd(temp, c_sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Process remaining elements\n            for (; j < n; ++j) {\n                double a_val = A[i * n + j];\n                double b_val = B[j * p + k];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    result += a_val * b_val;\n                }\n            }\n\n            // Store the result in C\n            C[i * p + k] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            double result = 0.0;\n            size_t j = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; j + 3 < n; j += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + j]);\n                __m256d b_vec = _mm256_loadu_pd(&B[j * p + k]);\n\n                // Check if elements are within the range [-100, 100]\n                __m256d mask_a = _mm256_and_pd(_mm256_cmp_pd(a_vec, lower_bound, _CMP_GE_OS),\n                                               _mm256_cmp_pd(a_vec, upper_bound, _CMP_LE_OS));\n                __m256d mask_b = _mm256_and_pd(_mm256_cmp_pd(b_vec, lower_bound, _CMP_GE_OS),\n                                               _mm256_cmp_pd(b_vec, upper_bound, _CMP_LE_OS));\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n\n                // Multiply and mask out-of-range elements\n                __m256d prod = _mm256_mul_pd(a_vec, b_vec);\n                prod = _mm256_and_pd(prod, mask);\n\n                // Horizontal sum of the valid products\n                __m128d low = _mm256_castpd256_pd128(prod);\n                __m128d high = _mm256_extractf128_pd(prod, 1);\n                __m128d sum = _mm_add_pd(low, high);\n                sum = _mm_hadd_pd(sum, sum);\n\n                result += _mm_cvtsd_f64(sum);\n            }\n\n            // Process remaining elements\n            for (; j < n; ++j) {\n                double a_val = A[i * n + j];\n                double b_val = B[j * p + k];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    result += a_val * b_val;\n                }\n            }\n\n            C[i * p + k] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            double result = 0.0;\n            size_t j = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; j + 3 < n; j += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + j]);\n                __m256d b_vec = _mm256_loadu_pd(&B[j * p + k]);\n\n                // Check if elements are within the range [-100, 100]\n                __m256d mask_a = _mm256_and_pd(_mm256_cmp_pd(a_vec, lower_bound, _CMP_GE_OQ),\n                                               _mm256_cmp_pd(a_vec, upper_bound, _CMP_LE_OQ));\n                __m256d mask_b = _mm256_and_pd(_mm256_cmp_pd(b_vec, lower_bound, _CMP_GE_OQ),\n                                               _mm256_cmp_pd(b_vec, upper_bound, _CMP_LE_OQ));\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n\n                // Multiply and mask out-of-range elements\n                __m256d prod = _mm256_mul_pd(a_vec, b_vec);\n                prod = _mm256_and_pd(prod, mask);\n\n                // Horizontal sum of the valid products\n                __m128d low = _mm256_castpd256_pd128(prod);\n                __m128d high = _mm256_extractf128_pd(prod, 1);\n                __m128d sum = _mm_add_pd(low, high);\n                sum = _mm_hadd_pd(sum, sum);\n\n                result += _mm_cvtsd_f64(sum);\n            }\n\n            // Process remaining elements\n            for (; j < n; ++j) {\n                double a_val = A[i * n + j];\n                double b_val = B[j * p + k];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    result += a_val * b_val;\n                }\n            }\n\n            C[i * p + k] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    // Constants for range checks\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            __m256d c_sum = _mm256_setzero_pd(); // Initialize accumulator to zero\n\n            for (size_t j = 0; j < n; j += 4) {\n                // Load 4 elements from A and B\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + j]);\n                __m256d b_vec = _mm256_loadu_pd(&B[j * p + k]);\n\n                // Check if elements in A are within range [-100, 100]\n                __m256d a_mask_low = _mm256_cmp_pd(a_vec, lower_bound, _CMP_GE_OQ);\n                __m256d a_mask_high = _mm256_cmp_pd(a_vec, upper_bound, _CMP_LE_OQ);\n                __m256d a_in_range = _mm256_and_pd(a_mask_low, a_mask_high);\n\n                // Check if elements in B are within range [-100, 100]\n                __m256d b_mask_low = _mm256_cmp_pd(b_vec, lower_bound, _CMP_GE_OQ);\n                __m256d b_mask_high = _mm256_cmp_pd(b_vec, upper_bound, _CMP_LE_OQ);\n                __m256d b_in_range = _mm256_and_pd(b_mask_low, b_mask_high);\n\n                // Combine masks for A and B\n                __m256d combined_mask = _mm256_and_pd(a_in_range, b_in_range);\n\n                // Perform multiplication and mask invalid results\n                __m256d mul_result = _mm256_mul_pd(a_vec, b_vec);\n                mul_result = _mm256_and_pd(mul_result, combined_mask);\n\n                // Accumulate valid results\n                c_sum = _mm256_add_pd(c_sum, mul_result);\n            }\n\n            // Horizontal sum of the accumulator\n            double temp[4];\n            _mm256_storeu_pd(temp, c_sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Store the result in C\n            C[i * p + k] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    // Constants for range check\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            __m256d c_sum = _mm256_setzero_pd(); // Initialize accumulator to zero\n\n            for (size_t j = 0; j < n; j += 4) { // Process 4 elements at a time\n                // Load 4 elements from A and B\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + j]);\n                __m256d b_vec = _mm256_loadu_pd(&B[j * p + k]);\n\n                // Check if elements of A are within range [-100, 100]\n                __m256d a_in_range = _mm256_and_pd(_mm256_cmp_pd(a_vec, lower_bound, _CMP_GE_OS),\n                                                    _mm256_cmp_pd(a_vec, upper_bound, _CMP_LE_OS));\n\n                // Check if elements of B are within range [-100, 100]\n                __m256d b_in_range = _mm256_and_pd(_mm256_cmp_pd(b_vec, lower_bound, _CMP_GE_OS),\n                                                    _mm256_cmp_pd(b_vec, upper_bound, _CMP_LE_OS));\n\n                // Combine range checks for A and B\n                __m256d in_range = _mm256_and_pd(a_in_range, b_in_range);\n\n                // Mask out elements that are out of range\n                __m256d masked_a = _mm256_and_pd(a_vec, in_range);\n                __m256d masked_b = _mm256_and_pd(b_vec, in_range);\n\n                // Multiply and accumulate only valid elements\n                c_sum = _mm256_add_pd(c_sum, _mm256_mul_pd(masked_a, masked_b));\n            }\n\n            // Horizontal sum of the accumulated vector\n            double temp[4];\n            _mm256_storeu_pd(temp, c_sum);\n            double result = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Store the result in C\n            C[i * p + k] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // AVX2 processes 4 int64_t elements per register (256 bits / 64 bits)\n    size_t simd_aligned_elements = total_elements - (total_elements % simd_width);\n\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX2 intrinsics\n    for (; i < simd_aligned_elements; i += simd_width) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vec_c = _mm256_add_epi64(vec_a, vec_b);\n        _mm256_storeu_si256((__m256i*)&C[i], vec_c);\n    }\n\n    // Process remaining elements that are not aligned to the SIMD width\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t simd_width = 4; // AVX2 processes 4 int64_t elements per register\n    size_t simd_aligned_elements = total_elements / simd_width * simd_width;\n\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX2 intrinsics\n    for (; i < simd_aligned_elements; i += simd_width) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vec_c = _mm256_add_epi64(vec_a, vec_b);\n        _mm256_storeu_si256((__m256i*)(C + i), vec_c);\n    }\n\n    // Process remaining elements (if any) with scalar operations\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < total_elements; i += 4) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 vround = _mm256_set1_ps(0.5f); // For rounding to nearest\n\n    // Process 8 elements at a time\n    for (; i + 7 < length; i += 8) {\n        __m256 vsrc1 = _mm256_loadu_ps(&src1[i]); // Load 8 floats from src1\n        __m256 vsrc2 = _mm256_loadu_ps(&src2[i]); // Load 8 floats from src2\n        __m256 vsum = _mm256_add_ps(vsrc1, vsrc2); // Add the two vectors element-wise\n\n        // Round to nearest integer by adding 0.5 and truncating\n        __m256 vrounded = _mm256_round_ps(_mm256_add_ps(vsum, vround), _MM_FROUND_TO_ZERO);\n\n        _mm256_storeu_ps(&dst[i], vrounded); // Store the result in dst\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (sum >= 0.0f) ? (float)((int)(sum + 0.5f)) : (float)((int)(sum - 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 vround = _mm256_set1_ps(0.5f); // For rounding to nearest\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vsrc1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vsrc2 = _mm256_loadu_ps(&src2[i]);\n\n        // Add the two vectors\n        __m256 vsum = _mm256_add_ps(vsrc1, vsrc2);\n\n        // Round to nearest integer by adding 0.5 and truncating\n        __m256 vrounded = _mm256_round_ps(_mm256_add_ps(vsum, vround), _MM_FROUND_TO_ZERO);\n\n        // Store the result in dst\n        _mm256_storeu_ps(&dst[i], vrounded);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = static_cast<float>(static_cast<int>(src1[i] + src2[i] + 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 rounding_const = _mm256_set1_ps(0.5f);\n\n    for (; i + 7 < length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]);\n\n        // Add the two vectors\n        __m256 result = _mm256_add_ps(vec1, vec2);\n\n        // Round to nearest integer by adding/subtracting 0.5 and truncating\n        __m256 rounded_result = _mm256_round_ps(result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Store the result in the destination array\n        _mm256_storeu_ps(&dst[i], rounded_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (float)((int)(sum + 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 rounding_const = _mm256_set1_ps(0.5f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&src2[i]);\n\n        // Add the two vectors\n        __m256 result = _mm256_add_ps(vec1, vec2);\n\n        // Round to nearest integer by adding/subtracting 0.5 and truncating\n        __m256 rounded_result = _mm256_round_ps(result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Store the result in the destination array\n        _mm256_storeu_ps(&dst[i], rounded_result);\n    }\n\n    // Handle any remaining elements\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 ones = _mm256_set1_ps(0.5f); // Used for rounding to nearest\n\n    // Process 8 elements at a time\n    for (; i + 7 < length; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(&src1[i]); // Load 8 floats from src1\n        __m256 v2 = _mm256_loadu_ps(&src2[i]); // Load 8 floats from src2\n        __m256 sum = _mm256_add_ps(v1, v2);    // Add the two vectors element-wise\n\n        // Round to nearest integer by adding 0.5 and truncating\n        __m256 rounded = _mm256_round_ps(_mm256_add_ps(sum, ones), _MM_FROUND_TO_ZERO);\n\n        _mm256_storeu_ps(&dst[i], rounded);    // Store the result in dst\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        float sum = src1[i] + src2[i];\n        dst[i] = (sum >= 0.0f) ? (float)((int)(sum + 0.5f)) : (float)((int)(sum - 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 two = _mm256_set1_ps(2.0f);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        // Load 8 floats from src1 and src2\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        // Compute absolute difference\n        __m256 diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        // Check if absolute difference is within 50\n        __m256 mask_diff = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n\n        // Check if index is a multiple of 2\n        __m256 indices = _mm256_add_ps(_mm256_set1_ps((float)i), _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f));\n        __m256 mod_indices = _mm256_fmod_ps(indices, two);\n        __m256 mask_index = _mm256_cmp_ps(mod_indices, zero, _CMP_EQ_OQ);\n\n        // Combine masks\n        __m256 mask = _mm256_and_ps(mask_diff, mask_index);\n\n        // Perform multiplication\n        __m256 mul_result = _mm256_mul_ps(v_src1, v_src2);\n\n        // Round up towards positive infinity\n        __m256 rounded_result = _mm256_ceil_ps(mul_result);\n\n        // Blend results based on mask\n        __m256 result = _mm256_blendv_ps(neg_one, rounded_result, mask);\n\n        // Store the result into dst\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - (length % 8); i < length; ++i) {\n        float diff = src1[i] - src2[i];\n        float abs_diff = (diff < 0) ? -diff : diff;\n        if (abs_diff <= 50.0f && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 two = _mm256_set1_ps(2.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 floats from src1 and src2\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        // Compute absolute difference\n        __m256 diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        // Check if absolute difference is within 50\n        __m256 mask_diff = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n\n        // Check if index is a multiple of 2\n        __m256 indices = _mm256_add_ps(_mm256_set1_ps((float)i), _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f));\n        __m256 mod_indices = _mm256_fmod_ps(indices, two);\n        __m256 mask_mod = _mm256_cmp_ps(mod_indices, zero, _CMP_EQ_OQ);\n\n        // Combine masks\n        __m256 combined_mask = _mm256_and_ps(mask_diff, mask_mod);\n\n        // Perform multiplication\n        __m256 mul_result = _mm256_mul_ps(v_src1, v_src2);\n\n        // Round up towards positive infinity\n        __m256 rounded_result = _mm256_ceil_ps(mul_result);\n\n        // Blend results based on the combined mask\n        __m256 final_result = _mm256_blendv_ps(neg_one, rounded_result, combined_mask);\n\n        // Store result in dst\n        _mm256_storeu_ps(&dst[i], final_result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = src1[i] - src2[i];\n        float abs_diff = (diff < 0) ? -diff : diff;\n\n        if (abs_diff <= 50.0f && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 two = _mm256_set1_ps(2.0f);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        // Load 8 elements from src1 and src2\n        __m256 vsrc1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vsrc2 = _mm256_loadu_ps(&src2[i]);\n\n        // Compute absolute difference\n        __m256 diff = _mm256_sub_ps(vsrc1, vsrc2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        // Check if absolute difference is within 50\n        __m256 mask_diff = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n\n        // Check if index is a multiple of 2\n        __m256 indices = _mm256_add_ps(_mm256_set1_ps((float)i), _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f));\n        __m256 mod_indices = _mm256_fmod_ps(indices, two);\n        __m256 mask_index = _mm256_cmp_ps(mod_indices, zero, _CMP_EQ_OQ);\n\n        // Combine masks\n        __m256 mask = _mm256_and_ps(mask_diff, mask_index);\n\n        // Perform multiplication\n        __m256 mul = _mm256_mul_ps(vsrc1, vsrc2);\n\n        // Round up towards positive infinity\n        __m256 rounded = _mm256_ceil_ps(mul);\n\n        // Blend results: -1 for elements not meeting conditions\n        __m256 result = _mm256_blendv_ps(neg_one, rounded, mask);\n\n        // Store results in dst\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - (length % 8); i < length; ++i) {\n        float diff = src1[i] - src2[i];\n        float abs_diff = diff < 0 ? -diff : diff;\n        if (abs_diff <= 50.0f && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 two = _mm256_set1_ps(2.0f);\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 floats from src1 and src2\n        __m256 v_src1 = _mm256_loadu_ps(&src1[i]);\n        __m256 v_src2 = _mm256_loadu_ps(&src2[i]);\n\n        // Compute absolute difference\n        __m256 diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // Clear sign bit\n\n        // Check if absolute difference is within 50\n        __m256 mask_diff = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OQ);\n\n        // Create index mask (i.e., indices that are multiples of 2)\n        __m256 indices = _mm256_set_ps(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256 mod_indices = _mm256_fmod_ps(indices, two);\n        __m256 mask_index = _mm256_cmp_ps(mod_indices, zero, _CMP_EQ_OQ);\n\n        // Combine masks\n        __m256 combined_mask = _mm256_and_ps(mask_diff, mask_index);\n\n        // Perform multiplication\n        __m256 mul_result = _mm256_mul_ps(v_src1, v_src2);\n\n        // Round up towards positive infinity\n        __m256 rounded_result = _mm256_ceil_ps(mul_result);\n\n        // Blend results: -1 for elements not meeting conditions\n        __m256 final_result = _mm256_blendv_ps(neg_one, rounded_result, combined_mask);\n\n        // Store the result in dst\n        _mm256_storeu_ps(&dst[i], final_result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    const __m256 threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 two = _mm256_set1_ps(2.0f);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        // Load 8 floats from src1 and src2\n        __m256 vsrc1 = _mm256_loadu_ps(&src1[i]);\n        __m256 vsrc2 = _mm256_loadu_ps(&src2[i]);\n\n        // Compute absolute difference\n        __m256 diff = _mm256_sub_ps(vsrc1, vsrc2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        // Check if absolute difference is within 50\n        __m256 mask_diff = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n\n        // Check if index is a multiple of 2\n        __m256 indices = _mm256_add_ps(_mm256_set1_ps((float)i), _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f));\n        __m256 mod_indices = _mm256_fmod_ps(indices, two);\n        __m256 mask_mod = _mm256_cmp_ps(mod_indices, zero, _CMP_EQ_OQ);\n\n        // Combine masks\n        __m256 mask = _mm256_and_ps(mask_diff, mask_mod);\n\n        // Compute element-wise multiplication\n        __m256 mul = _mm256_mul_ps(vsrc1, vsrc2);\n\n        // Round up towards positive infinity\n        __m256 rounded = _mm256_ceil_ps(mul);\n\n        // Blend results: -1 for elements not meeting conditions\n        __m256 result = _mm256_blendv_ps(neg_one, rounded, mask);\n\n        // Store the result in dst\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - (length % 8); i < length; ++i) {\n        float diff = src1[i] - src2[i];\n        float abs_diff = (diff < 0) ? -diff : diff;\n        if (abs_diff <= 50.0f && (i % 2 == 0)) {\n            dst[i] = ceilf(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n    size_t i, j;\n\n    for (i = 0; i < m; ++i) {\n        size_t k;\n        for (k = 0; k + 4 <= n; k += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + k]);\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(&dst[i * n + k], result);\n        }\n\n        for (j = k; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar); // Broadcast scalar to all elements of the SIMD register\n\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        size_t k = 0;\n        // Process 4 elements at a time using AVX\n        for (; k + 3 < n; k += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + k]); // Load 4 doubles from mat\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec); // Multiply by scalar\n            _mm256_storeu_pd(&dst[i * n + k], result); // Store result in dst\n        }\n        // Handle remaining elements that are not a multiple of 4\n        for (; k < n; ++k) {\n            dst[i * n + k] = mat[i * n + k] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar); // Broadcast scalar to all elements of the SIMD register\n\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        size_t k = 0;\n        // Process 4 elements at a time using AVX\n        for (; k + 3 < n; k += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + k]); // Load 4 doubles from mat\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec); // Multiply by scalar\n            _mm256_storeu_pd(&dst[i * n + k], result); // Store the result in dst\n        }\n        // Handle remaining elements that are not a multiple of 4\n        for (; k < n; ++k) {\n            dst[i * n + k] = mat[i * n + k] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar); // Broadcast scalar to all elements of the SIMD register\n\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        size_t k = 0;\n        // Process 4 elements at a time using AVX\n        for (; k + 3 < n; k += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + k]); // Load 4 doubles from mat\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec); // Multiply by scalar\n            _mm256_storeu_pd(&dst[i * n + k], result); // Store the result in dst\n        }\n        // Handle remaining elements that are not a multiple of 4\n        for (; k < n; ++k) {\n            dst[i * n + k] = mat[i * n + k] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm256_set1_pd(scalar); // Broadcast scalar to all elements of the SIMD register\n\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        size_t k = 0;\n        // Process 4 elements at a time using AVX\n        for (; k + 3 < n; k += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(&mat[i * n + k]); // Load 4 doubles from mat\n            __m256d result = _mm256_mul_pd(mat_vec, scalar_vec); // Multiply by scalar\n            _mm256_storeu_pd(&dst[i * n + k], result); // Store the result in dst\n        }\n        // Handle remaining elements that are not a multiple of 4\n        for (; k < n; ++k) {\n            dst[i * n + k] = mat[i * n + k] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_squares = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Compute the sum of squares using AVX\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n\n        // Horizontal sum of the partial sums in sum_squares\n        float norm = 0.0f;\n        alignas(32) float temp[8];\n        _mm256_store_ps(temp, sum_squares);\n        for (size_t k = 0; k < 8; ++k) {\n            norm += temp[k];\n        }\n\n        // Add remaining elements if n is not a multiple of 8\n        for (; j < n; ++j) {\n            norm += row[j] * row[j];\n        }\n\n        // Compute the L2 norm\n        norm = sqrtf(norm);\n\n        // Normalize the row\n        __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_mul_ps(vec, inv_norm);\n            _mm256_storeu_ps(row + j, vec);\n        }\n\n        // Normalize remaining elements if n is not a multiple of 8\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_squares = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Compute the sum of squares using AVX\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n\n        // Horizontal reduction of sum_squares\n        __m128 high = _mm256_extractf128_ps(sum_squares, 1);\n        __m128 low = _mm256_castps256_ps128(sum_squares);\n        low = _mm_add_ps(low, high);\n        low = _mm_hadd_ps(low, low);\n        low = _mm_hadd_ps(low, low);\n        float norm_sq = _mm_cvtss_f32(low);\n\n        // Add remaining elements if n is not a multiple of 8\n        for (; j < n; ++j) {\n            norm_sq += row[j] * row[j];\n        }\n\n        // Compute the L2 norm and its reciprocal\n        float norm = _mm_sqrt_ss(_mm_set_ss(norm_sq))[0];\n        float inv_norm = 1.0f / norm;\n\n        // Normalize the row using AVX\n        __m256 inv_norm_vec = _mm256_set1_ps(inv_norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_mul_ps(vec, inv_norm_vec);\n            _mm256_storeu_ps(row + j, vec);\n        }\n\n        // Normalize remaining elements if n is not a multiple of 8\n        for (; j < n; ++j) {\n            row[j] *= inv_norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_squares = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Compute the sum of squares using AVX\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n\n        // Horizontal sum of the partial sums in sum_squares\n        float norm_sq = 0.0f;\n        alignas(32) float temp[8];\n        _mm256_store_ps(temp, sum_squares);\n        for (size_t k = 0; k < 8; ++k) {\n            norm_sq += temp[k];\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            norm_sq += row[j] * row[j];\n        }\n\n        // Compute L2 norm\n        float norm = _mm_sqrt_ss(_mm_set_ss(norm_sq))[0];\n\n        // Normalize the row\n        __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_mul_ps(vec, inv_norm);\n            _mm256_storeu_ps(row + j, vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_squares = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Compute the sum of squares using AVX\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n\n        // Horizontal reduction of sum_squares\n        __m128 high = _mm256_extractf128_ps(sum_squares, 1);\n        __m128 low = _mm256_castps256_ps128(sum_squares);\n        __m128 sum = _mm_add_ps(low, high);\n\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n\n        float norm = _mm_cvtss_f32(sum);\n        // Add remaining elements if n is not a multiple of 8\n        for (; j < n; ++j) {\n            norm += row[j] * row[j];\n        }\n\n        norm = sqrtf(norm);\n\n        // Normalize the row\n        __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_mul_ps(vec, inv_norm);\n            _mm256_storeu_ps(row + j, vec);\n        }\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = mat + i * n;\n        __m256 sum_squares = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Compute the sum of squares using AVX\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            sum_squares = _mm256_fmadd_ps(vec, vec, sum_squares);\n        }\n\n        // Horizontal sum of the partial sums in sum_squares\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_squares);\n        float norm_sq = temp[0] + temp[1] + temp[2] + temp[3] +\n                        temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Add remaining elements if n is not a multiple of 8\n        for (; j < n; ++j) {\n            norm_sq += row[j] * row[j];\n        }\n\n        // Compute the L2 norm\n        float norm = _mm_sqrt_ss(_mm_set_ss(norm_sq))[0];\n\n        // Normalize the row\n        __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n        j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            vec = _mm256_mul_ps(vec, inv_norm);\n            _mm256_storeu_ps(row + j, vec);\n        }\n\n        // Normalize remaining elements if n is not a multiple of 8\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n\n            size_t k = 0;\n            for (; k + 3 < n; k += 4) {\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d mat2_vec = _mm256_loadu_pd(&mat2[k * p + j]);\n\n                sum_vec = _mm256_fmadd_pd(mat1_vec, mat2_vec, sum_vec);\n            }\n\n            double sum = 0.0;\n            if (k < n) {\n                for (; k < n; ++k) {\n                    sum += mat1[i * n + k] * mat2[k * p + j];\n                }\n            }\n\n            // Horizontal sum of the AVX register\n            sum += ((double*)&sum_vec)[0] + ((double*)&sum_vec)[1] +\n                   ((double*)&sum_vec)[2] + ((double*)&sum_vec)[3];\n\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = nearbyint(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            size_t k = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; k + 3 < n; k += 4) {\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d mat2_vec = _mm256_set_pd(mat2[(k + 3) * p + j],\n                                                 mat2[(k + 2) * p + j],\n                                                 mat2[(k + 1) * p + j],\n                                                 mat2[k * p + j]);\n                sum_vec = _mm256_fmadd_pd(mat1_vec, mat2_vec, sum_vec);\n            }\n\n            // Horizontal sum of the AVX vector\n            double sum = 0.0;\n            alignas(32) double temp[4];\n            _mm256_store_pd(temp, sum_vec);\n            sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Process remaining elements\n            for (; k < n; ++k) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = nearbyint(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            size_t k = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; k + 3 < n; k += 4) {\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d mat2_vec = _mm256_set_pd(mat2[(k + 3) * p + j],\n                                                 mat2[(k + 2) * p + j],\n                                                 mat2[(k + 1) * p + j],\n                                                 mat2[k * p + j]);\n                sum_vec = _mm256_fmadd_pd(mat1_vec, mat2_vec, sum_vec);\n            }\n\n            // Horizontal sum of the AVX vector\n            double sum_array[4];\n            _mm256_storeu_pd(sum_array, sum_vec);\n            double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n            // Process remaining elements\n            for (; k < n; ++k) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = nearbyint(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            size_t k = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; k + 3 < n; k += 4) {\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d mat2_vec = _mm256_loadu_pd(&mat2[k * p + j]);\n\n                // Multiply and accumulate\n                sum_vec = _mm256_fmadd_pd(mat1_vec, mat2_vec, sum_vec);\n            }\n\n            // Horizontal sum of the accumulated vector\n            double sum = 0.0;\n            alignas(32) double temp[4];\n            _mm256_store_pd(temp, sum_vec);\n            sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Process remaining elements\n            for (; k < n; ++k) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = nearbyint(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            size_t k = 0;\n\n            // Process 4 elements at a time using AVX\n            for (; k + 3 < n; k += 4) {\n                __m256d mat1_vec = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d mat2_vec = _mm256_loadu_pd(&mat2[k * p + j]);\n\n                // Multiply and accumulate\n                sum_vec = _mm256_fmadd_pd(mat1_vec, mat2_vec, sum_vec);\n            }\n\n            // Horizontal sum of the accumulated vector\n            double sum = 0.0;\n            alignas(32) double temp[4];\n            _mm256_store_pd(temp, sum_vec);\n            sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Process remaining elements\n            for (; k < n; ++k) {\n                sum += mat1[i * n + k] * mat2[k * p + j];\n            }\n\n            // Round to nearest integer and store in dst\n            dst[i * p + j] = nearbyint(sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    // Constants for rounding to nearest quarter\n    __m256 quarter = _mm256_set1_ps(0.25f);\n    __m256 zero = _mm256_setzero_ps();\n\n    // Temporary storage for transposed blocks\n    alignas(32) float temp[8][8];\n\n    // Process 8x8 blocks\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            // Load and transpose 8x8 block\n            for (size_t k = 0; k < 8; ++k) {\n                size_t row = i + k;\n                if (row < rows) {\n                    __m256 row_vec = _mm256_loadu_ps(&src[row * cols + j]);\n                    _mm256_store_ps(temp[k], row_vec);\n                } else {\n                    _mm256_store_ps(temp[k], zero);\n                }\n            }\n\n            // Transpose the 8x8 block\n            __m256 row0 = _mm256_load_ps(temp[0]);\n            __m256 row1 = _mm256_load_ps(temp[1]);\n            __m256 row2 = _mm256_load_ps(temp[2]);\n            __m256 row3 = _mm256_load_ps(temp[3]);\n            __m256 row4 = _mm256_load_ps(temp[4]);\n            __m256 row5 = _mm256_load_ps(temp[5]);\n            __m256 row6 = _mm256_load_ps(temp[6]);\n            __m256 row7 = _mm256_load_ps(temp[7]);\n\n            __m256 t0 = _mm256_unpacklo_ps(row0, row1);\n            __m256 t1 = _mm256_unpackhi_ps(row0, row1);\n            __m256 t2 = _mm256_unpacklo_ps(row2, row3);\n            __m256 t3 = _mm256_unpackhi_ps(row2, row3);\n            __m256 t4 = _mm256_unpacklo_ps(row4, row5);\n            __m256 t5 = _mm256_unpackhi_ps(row4, row5);\n            __m256 t6 = _mm256_unpacklo_ps(row6, row7);\n            __m256 t7 = _mm256_unpackhi_ps(row6, row7);\n\n            __m256 u0 = _mm256_shuffle_ps(t0, t2, 0x44);\n            __m256 u1 = _mm256_shuffle_ps(t0, t2, 0xEE);\n            __m256 u2 = _mm256_shuffle_ps(t1, t3, 0x44);\n            __m256 u3 = _mm256_shuffle_ps(t1, t3, 0xEE);\n            __m256 u4 = _mm256_shuffle_ps(t4, t6, 0x44);\n            __m256 u5 = _mm256_shuffle_ps(t4, t6, 0xEE);\n            __m256 u6 = _mm256_shuffle_ps(t5, t7, 0x44);\n            __m256 u7 = _mm256_shuffle_ps(t5, t7, 0xEE);\n\n            __m256 v0 = _mm256_permute2f128_ps(u0, u4, 0x20);\n            __m256 v1 = _mm256_permute2f128_ps(u1, u5, 0x20);\n            __m256 v2 = _mm256_permute2f128_ps(u2, u6, 0x20);\n            __m256 v3 = _mm256_permute2f128_ps(u3, u7, 0x20);\n            __m256 v4 = _mm256_permute2f128_ps(u0, u4, 0x31);\n            __m256 v5 = _mm256_permute2f128_ps(u1, u5, 0x31);\n            __m256 v6 = _mm256_permute2f128_ps(u2, u6, 0x31);\n            __m256 v7 = _mm256_permute2f128_ps(u3, u7, 0x31);\n\n            // Round each element to the nearest quarter\n            v0 = _mm256_mul_ps(v0, quarter);\n            v1 = _mm256_mul_ps(v1, quarter);\n            v2 = _mm256_mul_ps(v2, quarter);\n            v3 = _mm256_mul_ps(v3, quarter);\n            v4 = _mm256_mul_ps(v4, quarter);\n            v5 = _mm256_mul_ps(v5, quarter);\n            v6 = _mm256_mul_ps(v6, quarter);\n            v7 = _mm256_mul_ps(v7, quarter);\n\n            v0 = _mm256_round_ps(v0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v1 = _mm256_round_ps(v1, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v2 = _mm256_round_ps(v2, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v3 = _mm256_round_ps(v3, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v4 = _mm256_round_ps(v4, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v5 = _mm256_round_ps(v5, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v6 = _mm256_round_ps(v6, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            v7 = _mm256_round_ps(v7, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            v0 = _mm256_mul_ps(v0, quarter);\n            v1 = _mm256_mul_ps(v1, quarter);\n            v2 = _mm256_mul_ps(v2, quarter);\n            v3 = _mm256_mul_ps(v3, quarter);\n            v4 = _mm256_mul_ps(v4, quarter);\n            v5 = _mm256_mul_ps(v5, quarter);\n            v6 = _mm256_mul_ps(v6, quarter);\n            v7 = _mm256_mul_ps(v7, quarter);\n\n            // Store the transposed and rounded values into the destination matrix\n            for (size_t k = 0; k < 8; ++k) {\n                size_t col = j + k;\n                if (col < cols) {\n                    _mm256_storeu_ps(&dst[col * rows + i], _mm256_load_ps(&temp[k][0]));\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    // Constants for rounding to nearest quarter\n    __m256 multiplier = _mm256_set1_ps(4.0f);\n    __m256 divisor = _mm256_set1_ps(4.0f);\n\n    // Temporary buffer for transposed blocks\n    alignas(32) float temp[8][8];\n\n    // Process 8x8 blocks\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            // Load 8x8 block from src into registers\n            __m256 row0 = _mm256_loadu_ps(&src[(i + 0) * cols + j]);\n            __m256 row1 = _mm256_loadu_ps(&src[(i + 1) * cols + j]);\n            __m256 row2 = _mm256_loadu_ps(&src[(i + 2) * cols + j]);\n            __m256 row3 = _mm256_loadu_ps(&src[(i + 3) * cols + j]);\n            __m256 row4 = _mm256_loadu_ps(&src[(i + 4) * cols + j]);\n            __m256 row5 = _mm256_loadu_ps(&src[(i + 5) * cols + j]);\n            __m256 row6 = _mm256_loadu_ps(&src[(i + 6) * cols + j]);\n            __m256 row7 = _mm256_loadu_ps(&src[(i + 7) * cols + j]);\n\n            // Transpose the 8x8 block\n            __m256 t0 = _mm256_unpacklo_ps(row0, row1);\n            __m256 t1 = _mm256_unpackhi_ps(row0, row1);\n            __m256 t2 = _mm256_unpacklo_ps(row2, row3);\n            __m256 t3 = _mm256_unpackhi_ps(row2, row3);\n            __m256 t4 = _mm256_unpacklo_ps(row4, row5);\n            __m256 t5 = _mm256_unpackhi_ps(row4, row5);\n            __m256 t6 = _mm256_unpacklo_ps(row6, row7);\n            __m256 t7 = _mm256_unpackhi_ps(row6, row7);\n\n            __m256 col0 = _mm256_permute2f128_ps(t0, t2, 0x20);\n            __m256 col1 = _mm256_permute2f128_ps(t1, t3, 0x20);\n            __m256 col2 = _mm256_permute2f128_ps(t0, t2, 0x31);\n            __m256 col3 = _mm256_permute2f128_ps(t1, t3, 0x31);\n            __m256 col4 = _mm256_permute2f128_ps(t4, t6, 0x20);\n            __m256 col5 = _mm256_permute2f128_ps(t5, t7, 0x20);\n            __m256 col6 = _mm256_permute2f128_ps(t4, t6, 0x31);\n            __m256 col7 = _mm256_permute2f128_ps(t5, t7, 0x31);\n\n            // Round each element to the nearest quarter\n            col0 = _mm256_mul_ps(col0, multiplier);\n            col1 = _mm256_mul_ps(col1, multiplier);\n            col2 = _mm256_mul_ps(col2, multiplier);\n            col3 = _mm256_mul_ps(col3, multiplier);\n            col4 = _mm256_mul_ps(col4, multiplier);\n            col5 = _mm256_mul_ps(col5, multiplier);\n            col6 = _mm256_mul_ps(col6, multiplier);\n            col7 = _mm256_mul_ps(col7, multiplier);\n\n            col0 = _mm256_round_ps(col0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col1 = _mm256_round_ps(col1, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col2 = _mm256_round_ps(col2, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col3 = _mm256_round_ps(col3, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col4 = _mm256_round_ps(col4, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col5 = _mm256_round_ps(col5, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col6 = _mm256_round_ps(col6, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col7 = _mm256_round_ps(col7, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            col0 = _mm256_div_ps(col0, divisor);\n            col1 = _mm256_div_ps(col1, divisor);\n            col2 = _mm256_div_ps(col2, divisor);\n            col3 = _mm256_div_ps(col3, divisor);\n            col4 = _mm256_div_ps(col4, divisor);\n            col5 = _mm256_div_ps(col5, divisor);\n            col6 = _mm256_div_ps(col6, divisor);\n            col7 = _mm256_div_ps(col7, divisor);\n\n            // Store the transposed and rounded block into dst\n            _mm256_storeu_ps(&dst[(j + 0) * rows + i], col0);\n            _mm256_storeu_ps(&dst[(j + 1) * rows + i], col1);\n            _mm256_storeu_ps(&dst[(j + 2) * rows + i], col2);\n            _mm256_storeu_ps(&dst[(j + 3) * rows + i], col3);\n            _mm256_storeu_ps(&dst[(j + 4) * rows + i], col4);\n            _mm256_storeu_ps(&dst[(j + 5) * rows + i], col5);\n            _mm256_storeu_ps(&dst[(j + 6) * rows + i], col6);\n            _mm256_storeu_ps(&dst[(j + 7) * rows + i], col7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    // Constants for rounding to nearest quarter\n    const __m256 multiplier = _mm256_set1_ps(4.0f);\n    const __m256 divisor = _mm256_set1_ps(0.25f);\n\n    // Ensure rows and cols are multiples of 8 for AVX alignment\n    size_t aligned_cols = (cols / 8) * 8;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < aligned_cols; j += 8) {\n            // Load 8 elements from the source matrix row\n            __m256 src_vec = _mm256_loadu_ps(&src[i * cols + j]);\n\n            // Multiply by 4 to scale up for rounding\n            __m256 scaled = _mm256_mul_ps(src_vec, multiplier);\n\n            // Round to the nearest integer\n            __m256 rounded = _mm256_round_ps(scaled, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            // Divide by 4 to scale back down\n            __m256 result = _mm256_mul_ps(rounded, divisor);\n\n            // Store the transposed result in the destination matrix\n            for (int k = 0; k < 8; ++k) {\n                dst[(j + k) * rows + i] = result[k];\n            }\n        }\n\n        // Handle remaining columns (if any)\n        for (size_t j = aligned_cols; j < cols; ++j) {\n            float value = src[i * cols + j];\n            value = roundf(value * 4.0f) * 0.25f;\n            dst[j * rows + i] = value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    // Constants for rounding to nearest quarter\n    __m256 multiplier = _mm256_set1_ps(4.0f);\n    __m256 divisor = _mm256_set1_ps(4.0f);\n\n    // Temporary buffer for transposed blocks\n    alignas(32) float temp[8][8];\n\n    // Process 8x8 blocks\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            // Load 8x8 block from src into registers\n            __m256 row0 = _mm256_loadu_ps(&src[(i + 0) * cols + j]);\n            __m256 row1 = _mm256_loadu_ps(&src[(i + 1) * cols + j]);\n            __m256 row2 = _mm256_loadu_ps(&src[(i + 2) * cols + j]);\n            __m256 row3 = _mm256_loadu_ps(&src[(i + 3) * cols + j]);\n            __m256 row4 = _mm256_loadu_ps(&src[(i + 4) * cols + j]);\n            __m256 row5 = _mm256_loadu_ps(&src[(i + 5) * cols + j]);\n            __m256 row6 = _mm256_loadu_ps(&src[(i + 6) * cols + j]);\n            __m256 row7 = _mm256_loadu_ps(&src[(i + 7) * cols + j]);\n\n            // Transpose the 8x8 block\n            __m256 t0 = _mm256_unpacklo_ps(row0, row1);\n            __m256 t1 = _mm256_unpackhi_ps(row0, row1);\n            __m256 t2 = _mm256_unpacklo_ps(row2, row3);\n            __m256 t3 = _mm256_unpackhi_ps(row2, row3);\n            __m256 t4 = _mm256_unpacklo_ps(row4, row5);\n            __m256 t5 = _mm256_unpackhi_ps(row4, row5);\n            __m256 t6 = _mm256_unpacklo_ps(row6, row7);\n            __m256 t7 = _mm256_unpackhi_ps(row6, row7);\n\n            __m256 col0 = _mm256_permute2f128_ps(t0, t2, 0x20);\n            __m256 col1 = _mm256_permute2f128_ps(t1, t3, 0x20);\n            __m256 col2 = _mm256_permute2f128_ps(t0, t2, 0x31);\n            __m256 col3 = _mm256_permute2f128_ps(t1, t3, 0x31);\n            __m256 col4 = _mm256_permute2f128_ps(t4, t6, 0x20);\n            __m256 col5 = _mm256_permute2f128_ps(t5, t7, 0x20);\n            __m256 col6 = _mm256_permute2f128_ps(t4, t6, 0x31);\n            __m256 col7 = _mm256_permute2f128_ps(t5, t7, 0x31);\n\n            // Round each element to the nearest quarter\n            col0 = _mm256_mul_ps(col0, multiplier);\n            col1 = _mm256_mul_ps(col1, multiplier);\n            col2 = _mm256_mul_ps(col2, multiplier);\n            col3 = _mm256_mul_ps(col3, multiplier);\n            col4 = _mm256_mul_ps(col4, multiplier);\n            col5 = _mm256_mul_ps(col5, multiplier);\n            col6 = _mm256_mul_ps(col6, multiplier);\n            col7 = _mm256_mul_ps(col7, multiplier);\n\n            col0 = _mm256_round_ps(col0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col1 = _mm256_round_ps(col1, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col2 = _mm256_round_ps(col2, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col3 = _mm256_round_ps(col3, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col4 = _mm256_round_ps(col4, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col5 = _mm256_round_ps(col5, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col6 = _mm256_round_ps(col6, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col7 = _mm256_round_ps(col7, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            col0 = _mm256_div_ps(col0, divisor);\n            col1 = _mm256_div_ps(col1, divisor);\n            col2 = _mm256_div_ps(col2, divisor);\n            col3 = _mm256_div_ps(col3, divisor);\n            col4 = _mm256_div_ps(col4, divisor);\n            col5 = _mm256_div_ps(col5, divisor);\n            col6 = _mm256_div_ps(col6, divisor);\n            col7 = _mm256_div_ps(col7, divisor);\n\n            // Store the transposed and rounded block into dst\n            _mm256_storeu_ps(&dst[(j + 0) * rows + i], col0);\n            _mm256_storeu_ps(&dst[(j + 1) * rows + i], col1);\n            _mm256_storeu_ps(&dst[(j + 2) * rows + i], col2);\n            _mm256_storeu_ps(&dst[(j + 3) * rows + i], col3);\n            _mm256_storeu_ps(&dst[(j + 4) * rows + i], col4);\n            _mm256_storeu_ps(&dst[(j + 5) * rows + i], col5);\n            _mm256_storeu_ps(&dst[(j + 6) * rows + i], col6);\n            _mm256_storeu_ps(&dst[(j + 7) * rows + i], col7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    // Constants for rounding to nearest quarter\n    __m256 multiplier = _mm256_set1_ps(4.0f);\n    __m256 divisor = _mm256_set1_ps(4.0f);\n\n    // Temporary buffer for transposed blocks\n    alignas(32) float temp[8][8];\n\n    // Process 8x8 blocks\n    for (size_t i = 0; i < rows; i += 8) {\n        for (size_t j = 0; j < cols; j += 8) {\n            // Load 8x8 block from src into registers\n            __m256 row0 = _mm256_loadu_ps(&src[(i + 0) * cols + j]);\n            __m256 row1 = _mm256_loadu_ps(&src[(i + 1) * cols + j]);\n            __m256 row2 = _mm256_loadu_ps(&src[(i + 2) * cols + j]);\n            __m256 row3 = _mm256_loadu_ps(&src[(i + 3) * cols + j]);\n            __m256 row4 = _mm256_loadu_ps(&src[(i + 4) * cols + j]);\n            __m256 row5 = _mm256_loadu_ps(&src[(i + 5) * cols + j]);\n            __m256 row6 = _mm256_loadu_ps(&src[(i + 6) * cols + j]);\n            __m256 row7 = _mm256_loadu_ps(&src[(i + 7) * cols + j]);\n\n            // Transpose the 8x8 block\n            __m256 t0 = _mm256_unpacklo_ps(row0, row1);\n            __m256 t1 = _mm256_unpackhi_ps(row0, row1);\n            __m256 t2 = _mm256_unpacklo_ps(row2, row3);\n            __m256 t3 = _mm256_unpackhi_ps(row2, row3);\n            __m256 t4 = _mm256_unpacklo_ps(row4, row5);\n            __m256 t5 = _mm256_unpackhi_ps(row4, row5);\n            __m256 t6 = _mm256_unpacklo_ps(row6, row7);\n            __m256 t7 = _mm256_unpackhi_ps(row6, row7);\n\n            __m256 col0 = _mm256_permute2f128_ps(t0, t2, 0x20);\n            __m256 col1 = _mm256_permute2f128_ps(t1, t3, 0x20);\n            __m256 col2 = _mm256_permute2f128_ps(t0, t2, 0x31);\n            __m256 col3 = _mm256_permute2f128_ps(t1, t3, 0x31);\n            __m256 col4 = _mm256_permute2f128_ps(t4, t6, 0x20);\n            __m256 col5 = _mm256_permute2f128_ps(t5, t7, 0x20);\n            __m256 col6 = _mm256_permute2f128_ps(t4, t6, 0x31);\n            __m256 col7 = _mm256_permute2f128_ps(t5, t7, 0x31);\n\n            // Round each element to the nearest quarter\n            col0 = _mm256_mul_ps(col0, multiplier);\n            col1 = _mm256_mul_ps(col1, multiplier);\n            col2 = _mm256_mul_ps(col2, multiplier);\n            col3 = _mm256_mul_ps(col3, multiplier);\n            col4 = _mm256_mul_ps(col4, multiplier);\n            col5 = _mm256_mul_ps(col5, multiplier);\n            col6 = _mm256_mul_ps(col6, multiplier);\n            col7 = _mm256_mul_ps(col7, multiplier);\n\n            col0 = _mm256_round_ps(col0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col1 = _mm256_round_ps(col1, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col2 = _mm256_round_ps(col2, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col3 = _mm256_round_ps(col3, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col4 = _mm256_round_ps(col4, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col5 = _mm256_round_ps(col5, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col6 = _mm256_round_ps(col6, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            col7 = _mm256_round_ps(col7, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            col0 = _mm256_div_ps(col0, divisor);\n            col1 = _mm256_div_ps(col1, divisor);\n            col2 = _mm256_div_ps(col2, divisor);\n            col3 = _mm256_div_ps(col3, divisor);\n            col4 = _mm256_div_ps(col4, divisor);\n            col5 = _mm256_div_ps(col5, divisor);\n            col6 = _mm256_div_ps(col6, divisor);\n            col7 = _mm256_div_ps(col7, divisor);\n\n            // Store the transposed and rounded block into temp\n            _mm256_store_ps(temp[0], col0);\n            _mm256_store_ps(temp[1], col1);\n            _mm256_store_ps(temp[2], col2);\n            _mm256_store_ps(temp[3], col3);\n            _mm256_store_ps(temp[4], col4);\n            _mm256_store_ps(temp[5], col5);\n            _mm256_store_ps(temp[6], col6);\n            _mm256_store_ps(temp[7], col7);\n\n            // Write the transposed block to dst\n            for (size_t k = 0; k < 8; ++k) {\n                _mm256_storeu_ps(&dst[(j + k) * rows + i], _mm256_load_ps(temp[k]));\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < total_elements; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i); // Load 8 floats from tensor1\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i); // Load 8 floats from tensor2\n        __m256 sub = _mm256_sub_ps(vec1, vec2);     // Perform element-wise subtraction\n\n        // Round towards zero using truncation\n        __m256i truncated = _mm256_cvttps_epi32(sub); // Convert to integers with truncation\n        __m256 result = _mm256_cvtepi32_ps(truncated); // Convert back to floats\n\n        _mm256_storeu_ps(dst + i, result); // Store the result in dst\n    }\n\n    // Process remaining elements (if any) using scalar operations\n    for (; i < total_elements; ++i) {\n        float sub = tensor1[i] - tensor2[i];\n        dst[i] = (sub > 0.0f) ? floorf(sub) : ceilf(sub);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < total_elements; i += 8) {\n        // Load 8 floats from tensor1 and tensor2\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n\n        // Perform element-wise subtraction\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n\n        // Round towards zero (truncate)\n        __m256i truncated = _mm256_cvttps_epi32(result); // Convert to int32 with truncation\n        __m256 final_result = _mm256_cvtepi32_ps(truncated); // Convert back to float\n\n        // Store the result in dst\n        _mm256_storeu_ps(dst + i, final_result);\n    }\n\n    // Process remaining elements (if any) using scalar operations\n    for (; i < total_elements; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0.0f) ? floorf(result) : ceilf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < total_elements; i += 8) {\n        // Load 8 floats from tensor1 and tensor2\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n\n        // Perform element-wise subtraction\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n\n        // Round towards zero (truncate)\n        __m256i truncated = _mm256_cvttps_epi32(result); // Convert to integers with truncation\n        __m256 final_result = _mm256_cvtepi32_ps(truncated); // Convert back to floats\n\n        // Store the result in dst\n        _mm256_storeu_ps(dst + i, final_result);\n    }\n\n    // Process remaining elements (if any) using scalar operations\n    for (; i < total_elements; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0.0f) ? floorf(result) : ceilf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < total_elements; i += 8) {\n        // Load 8 floats from tensor1 and tensor2\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n\n        // Perform element-wise subtraction\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n\n        // Round towards zero (truncate)\n        __m256i truncated = _mm256_cvttps_epi32(result); // Convert to integers with truncation\n        __m256 final_result = _mm256_cvtepi32_ps(truncated); // Convert back to floats\n\n        // Store the result in dst\n        _mm256_storeu_ps(dst + i, final_result);\n    }\n\n    // Process remaining elements (if any) using scalar operations\n    for (; i < total_elements; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0.0f) ? floorf(result) : ceilf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < total_elements; i += 8) {\n        // Load 8 floats from tensor1 and tensor2\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n\n        // Perform element-wise subtraction\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n\n        // Round towards zero (truncate)\n        __m256i truncated = _mm256_cvttps_epi32(result); // Convert to int32 with truncation\n        __m256 final_result = _mm256_cvtepi32_ps(truncated); // Convert back to float\n\n        // Store the result in dst\n        _mm256_storeu_ps(dst + i, final_result);\n    }\n\n    // Process remaining elements (if any) using scalar operations\n    for (; i < total_elements; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0.0f) ? floorf(result) : ceilf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t n_aligned = (n / simd_width) * simd_width;\n\n    for (i = 0; i < m; ++i) {\n        size_t k = 0;\n\n        // Process elements in chunks of 4 using AVX\n        for (k = 0; k < n_aligned; k += simd_width) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[i * n + k]); // Load 4 doubles from mat1\n            __m256d vec2 = _mm256_loadu_pd(&mat2[i * n + k]); // Load 4 doubles from mat2\n            __m256d result = _mm256_mul_pd(vec1, vec2);       // Perform element-wise multiplication\n            _mm256_storeu_pd(&dst[i * n + k], result);        // Store the result in dst\n        }\n\n        // Process remaining elements that don't fit into a full SIMD register\n        for (; k < n; ++k) {\n            dst[i * n + k] = mat1[i * n + k] * mat2[i * n + k];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t n_aligned = (n / simd_width) * simd_width;\n\n    for (i = 0; i < m; ++i) {\n        size_t row_offset = i * n;\n\n        // Process elements in chunks of 4 using AVX\n        for (j = 0; j < n_aligned; j += simd_width) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[row_offset + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[row_offset + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[row_offset + j], result);\n        }\n\n        // Handle remaining elements that don't fit into a full SIMD register\n        for (; j < n; ++j) {\n            dst[row_offset + j] = mat1[row_offset + j] * mat2[row_offset + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t n_aligned = (n / simd_width) * simd_width;\n\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = 0;\n        // Process elements in chunks of 4 using AVX\n        for (; j < n_aligned; j += simd_width) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        // Process remaining elements that don't fit into a full SIMD vector\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t n_aligned = (n / simd_width) * simd_width;\n\n    for (i = 0; i < m; ++i) {\n        size_t row_offset = i * n;\n\n        // Process elements in chunks of 4 using AVX\n        for (j = 0; j < n_aligned; j += simd_width) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[row_offset + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[row_offset + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[row_offset + j], result);\n        }\n\n        // Process remaining elements that don't fit into SIMD width\n        for (; j < n; ++j) {\n            dst[row_offset + j] = mat1[row_offset + j] * mat2[row_offset + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    size_t vec_size = n / simd_width * simd_width;\n\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = 0;\n        // Process elements in chunks of 4 using AVX\n        for (; j < vec_size; j += simd_width) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        // Process remaining elements that don't fit into a vector\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);\n            sum_vec = _mm256_fmadd_ps(mat_row, vec_vals, sum_vec);\n        }\n\n        // Horizontal sum of the AVX register\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_vec);\n        float result = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            result += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t n_aligned = (n / 8) * 8; // Process in chunks of 8 floats for AVX\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_vec = _mm256_setzero_ps(); // Initialize sum vector to zero\n\n        size_t j = 0;\n        for (; j < n_aligned; j += 8) {\n            __m256 mat_row = _mm256_loadu_ps(&mat[i * n + j]); // Load 8 elements from the matrix row\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);         // Load 8 elements from the vector\n            sum_vec = _mm256_fmadd_ps(mat_row, vec_vals, sum_vec); // Multiply and accumulate\n        }\n\n        // Horizontal sum of the AVX register\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_vec);\n        float sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Handle remaining elements\n        for (; j < n; ++j) {\n            sum += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = sum; // Store the result in the destination vector\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);\n            sum_vec = _mm256_fmadd_ps(mat_row, vec_vals, sum_vec);\n        }\n\n        // Horizontal sum of the 8 partial sums in sum_vec\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_vec);\n        float result = temp[0] + temp[1] + temp[2] + temp[3] +\n                       temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            result += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);\n            sum_vec = _mm256_fmadd_ps(mat_row, vec_vals, sum_vec);\n        }\n\n        // Horizontal sum of the 8 partial sums in sum_vec\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_vec);\n        float result = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            result += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum_vec = _mm256_setzero_ps();\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX\n        for (; j + 7 < n; j += 8) {\n            __m256 mat_row = _mm256_loadu_ps(&mat[i * n + j]);\n            __m256 vec_vals = _mm256_loadu_ps(&vec[j]);\n            sum_vec = _mm256_fmadd_ps(mat_row, vec_vals, sum_vec);\n        }\n\n        // Horizontal sum of the AVX register\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_vec);\n        float result = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            result += mat[i * n + j] * vec[j];\n        }\n\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d vec_sum = _mm256_setzero_pd();\n    size_t i, j;\n    size_t simd_width = 4;\n    size_t n_aligned = (n / simd_width) * simd_width;\n    double total_sum = 0.0;\n\n    for (i = 0; i < m; ++i) {\n        const double* row = mat + i * n;\n        size_t j;\n\n        // Process elements in chunks of 4 using AVX\n        for (j = 0; j < n_aligned; j += simd_width) {\n            __m256d vec = _mm256_loadu_pd(row + j);\n            vec_sum = _mm256_add_pd(vec_sum, vec);\n        }\n\n        // Horizontal sum of the accumulated vector\n        double temp[4];\n        _mm256_storeu_pd(temp, vec_sum);\n        total_sum += temp[0] + temp[1] + temp[2] + temp[3];\n        vec_sum = _mm256_setzero_pd();\n\n        // Process remaining elements in the row\n        for (; j < n; ++j) {\n            total_sum += row[j];\n        }\n    }\n\n    return total_sum / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    size_t simd_width = 4;\n    size_t total_elements = m * n;\n    size_t simd_aligned_length = (n / simd_width) * simd_width;\n\n    for (i = 0; i < m; ++i) {\n        const double* row_ptr = mat + i * n;\n        j = 0;\n\n        // Process elements in chunks of 4 using AVX\n        for (; j < simd_aligned_length; j += simd_width) {\n            __m256d vec = _mm256_loadu_pd(row_ptr + j);\n            sum_vec = _mm256_add_pd(sum_vec, vec);\n        }\n\n        // Handle remaining elements in the row\n        for (; j < n; ++j) {\n            sum_vec = _mm256_add_pd(sum_vec, _mm256_set1_pd(row_ptr[j]));\n        }\n    }\n\n    // Horizontal sum of the AVX register\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    double total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d vec_sum = _mm256_setzero_pd();\n    size_t i, j;\n    size_t aligned_n = n - (n % 4);\n    double total_sum = 0.0;\n\n    for (i = 0; i < m; ++i) {\n        const double* row = mat + i * n;\n        j = 0;\n\n        // Process 4 elements at a time using AVX\n        for (; j < aligned_n; j += 4) {\n            __m256d vec = _mm256_loadu_pd(row + j);\n            vec_sum = _mm256_add_pd(vec_sum, vec);\n        }\n\n        // Horizontal sum of vec_sum every 4 rows to prevent overflow\n        if ((i + 1) % 4 == 0 || i == m - 1) {\n            double temp[4];\n            _mm256_storeu_pd(temp, vec_sum);\n            total_sum += temp[0] + temp[1] + temp[2] + temp[3];\n            vec_sum = _mm256_setzero_pd();\n        }\n\n        // Process remaining elements in the row\n        for (; j < n; ++j) {\n            total_sum += row[j];\n        }\n    }\n\n    return total_sum / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    size_t simd_width = 4;\n    size_t total_elements = m * n;\n\n    // Process elements in chunks of 4 using AVX\n    for (i = 0; i < total_elements / simd_width; ++i) {\n        __m256d vec = _mm256_loadu_pd(mat + i * simd_width);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the accumulated vector\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    double sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining elements that didn't fit into SIMD width\n    for (j = i * simd_width; j < total_elements; ++j) {\n        sum += mat[j];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    size_t simd_width = 4;\n    size_t total_elements = m * n;\n    size_t simd_aligned = (total_elements / simd_width) * simd_width;\n\n    // Process elements in chunks of 4 using AVX\n    for (i = 0; i < simd_aligned; i += simd_width) {\n        __m256d vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX vector\n    double sum = 0.0;\n    alignas(32) double temp[4];\n    _mm256_store_pd(temp, sum_vec);\n    for (j = 0; j < simd_width; ++j) {\n        sum += temp[j];\n    }\n\n    // Add remaining elements that didn't fit into AVX chunks\n    for (; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        \n        size_t j = 0;\n        \n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            \n            __m256i cmp;\n            if (ascending) {\n                cmp = _mm256_cmpgt_epi32(current, next); // Check if current > next\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current); // Check if next > current\n            }\n            \n            int mask = _mm256_movemask_ps((__m256)cmp);\n            if (mask != 0) {\n                return false; // Not sorted\n            }\n        }\n        \n        // Process remaining elements\n        for (; j + 1 < cols; ++j) {\n            if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                return false; // Not sorted\n            }\n        }\n    }\n    \n    return true; // All rows are sorted\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        \n        size_t j = 0;\n        if (cols >= 8) {\n            __m256i prev = _mm256_loadu_si256((__m256i const*)(row));\n            for (j = 8; j + 8 <= cols; j += 8) {\n                __m256i curr = _mm256_loadu_si256((__m256i const*)(row + j));\n                __m256i cmp;\n                if (ascending) {\n                    cmp = _mm256_cmpgt_epi32(_mm256_permutevar8x32_epi32(prev, _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 0)), curr);\n                } else {\n                    cmp = _mm256_cmpgt_epi32(curr, _mm256_permutevar8x32_epi32(prev, _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 0)));\n                }\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n                prev = curr;\n            }\n        }\n\n        // Process remaining elements\n        if (ascending) {\n            for (; j < cols - 1; ++j) {\n                if (row[j] > row[j + 1]) {\n                    return false;\n                }\n            }\n        } else {\n            for (; j < cols - 1; ++j) {\n                if (row[j] < row[j + 1]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp;\n            if (ascending) {\n                cmp = _mm256_cmpgt_epi32(current, next); // Check if current > next\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current); // Check if next > current\n            }\n\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n            if (mask != 0) {\n                return false; // Row is not sorted correctly\n            }\n        }\n\n        // Process remaining elements\n        for (; j + 1 < cols; ++j) {\n            if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        \n        size_t j = 0;\n        if (ascending) {\n            // Process 8 elements at a time for ascending order\n            for (; j + 8 <= cols; j += 8) {\n                __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n                __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n                \n                __m256i cmp = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        } else {\n            // Process 8 elements at a time for descending order\n            for (; j + 8 <= cols; j += 8) {\n                __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n                __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n                \n                __m256i cmp = _mm256_cmpgt_epi32(next, current);\n                if (!_mm256_testz_si256(cmp, cmp)) {\n                    return false;\n                }\n            }\n        }\n\n        // Process remaining elements\n        for (; j + 1 < cols; ++j) {\n            if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool ascending = directions[i];\n        \n        size_t j = 0;\n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp;\n            if (ascending) {\n                cmp = _mm256_cmpgt_epi32(current, next); // Check if current > next\n            } else {\n                cmp = _mm256_cmpgt_epi32(next, current); // Check if next > current\n            }\n\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n            if (mask != 0) {\n                return false; // Early exit if any element is out of order\n            }\n        }\n\n        // Process remaining elements\n        for (; j + 1 < cols; ++j) {\n            if ((ascending && row[j] > row[j + 1]) || (!ascending && row[j] < row[j + 1])) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n    \n    for (size_t i = 0; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n        _mm256_storeu_si256((__m256i*)(dst + i), vmul);\n    }\n\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        dst[i] = (int16_t)((src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n    \n    for (size_t i = 0; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n        _mm256_storeu_si256((__m256i*)(dst + i), vmul);\n    }\n\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        dst[i] = (int16_t)((src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n        _mm256_storeu_si256((__m256i*)&dst[i], vmul);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (int16_t)((src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 16 <= length; i += 16) {\n        // Load 16 elements from src\n        __m256i vsrc = _mm256_loadu_si256((const __m256i*)(src + i));\n\n        // Compute quotient: divide each element by base\n        __m256i vquotient = _mm256_div_epi16(vsrc, vbase);\n\n        // Multiply quotient by base to get the largest multiple of base <= src\n        __m256i vmultiple = _mm256_mullo_epi16(vquotient, vbase);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)(dst + i), vmultiple);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        dst[i] = (int16_t)((src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i vbase = _mm256_set1_epi16(base);\n    __m256i vzero = _mm256_setzero_si256();\n    \n    for (size_t i = 0; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i vdiv = _mm256_div_epi16(vsrc, vbase);\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n        _mm256_storeu_si256((__m256i*)(dst + i), vmul);\n    }\n\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        dst[i] = (int16_t)((src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    // Load the vector into an AVX register for reuse\n    __m256d vec_reg[4]; // Up to 4 AVX registers for a vector of up to 16 doubles\n    size_t vec_chunks = cols / 4;\n\n    for (size_t i = 0; i < vec_chunks; ++i) {\n        vec_reg[i] = _mm256_loadu_pd(vector + i * 4);\n    }\n\n    // Process each row of the matrix\n    for (size_t r = 0; r < rows; ++r) {\n        bool match = true;\n        const double* row_ptr = matrix + r * cols;\n\n        for (size_t i = 0; i < vec_chunks; ++i) {\n            __m256d row_reg = _mm256_loadu_pd(row_ptr + i * 4);\n            __m256d cmp = _mm256_cmp_pd(row_reg, vec_reg[i], _CMP_NEQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n\n            if (mask != 0) { // If any element doesn't match\n                match = false;\n                break;\n            }\n        }\n\n        // Handle remaining elements if cols is not a multiple of 4\n        if (match && cols % 4 != 0) {\n            size_t remainder_start = vec_chunks * 4;\n            for (size_t c = remainder_start; c < cols; ++c) {\n                if (row_ptr[c] != vector[c]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        if (match) {\n            return true; // Found a matching row\n        }\n    }\n\n    return false; // No matching row found\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    // Load the vector into an AVX register for reuse\n    __m256d vec_reg[4]; // Up to 4 registers for a maximum of 16 doubles (AVX supports 4 doubles per register)\n    size_t vec_loads = cols / 4;\n    for (size_t i = 0; i < vec_loads; ++i) {\n        vec_reg[i] = _mm256_loadu_pd(&vector[i * 4]);\n    }\n\n    // Iterate over each row of the matrix\n    for (size_t r = 0; r < rows; ++r) {\n        bool match = true;\n\n        // Compare each chunk of 4 doubles in the row with the corresponding chunk in the vector\n        for (size_t i = 0; i < vec_loads; ++i) {\n            __m256d mat_reg = _mm256_loadu_pd(&matrix[r * cols + i * 4]);\n            __m256d cmp = _mm256_cmp_pd(mat_reg, vec_reg[i], _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n\n            if (mask != 0xF) { // If any element doesn't match, break early\n                match = false;\n                break;\n            }\n        }\n\n        // Handle remaining elements if cols is not a multiple of 4\n        if (match && cols % 4 != 0) {\n            size_t remainder_start = vec_loads * 4;\n            for (size_t c = remainder_start; c < cols; ++c) {\n                if (matrix[r * cols + c] != vector[c]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        // If all elements matched, return true\n        if (match) {\n            return true;\n        }\n    }\n\n    // No matching row found\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    // Load the vector into AVX registers for reuse\n    __m256d vec_reg[4];\n    size_t i = 0;\n    for (; i + 3 < cols; i += 4) {\n        vec_reg[i / 4] = _mm256_loadu_pd(&vector[i]);\n    }\n\n    for (size_t r = 0; r < rows; ++r) {\n        bool match = true;\n        size_t c = 0;\n\n        // Compare chunks of 4 doubles using AVX\n        for (; c + 3 < cols; c += 4) {\n            __m256d mat_reg = _mm256_loadu_pd(&matrix[r * cols + c]);\n            __m256d cmp = _mm256_cmp_pd(mat_reg, vec_reg[c / 4], _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xF) { // If any element doesn't match\n                match = false;\n                break;\n            }\n        }\n\n        // Compare remaining elements (if cols is not a multiple of 4)\n        if (match && c < cols) {\n            for (; c < cols; ++c) {\n                if (matrix[r * cols + c] != vector[c]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        if (match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    // Load the vector into an AVX register for reuse\n    __m256d vec_reg[4]; // Up to 4 registers for a maximum of 16 doubles (AVX supports 4 doubles per register)\n    size_t vec_chunks = cols / 4;\n    size_t remainder = cols % 4;\n\n    for (size_t i = 0; i < vec_chunks; ++i) {\n        vec_reg[i] = _mm256_loadu_pd(vector + i * 4);\n    }\n\n    // Process each row of the matrix\n    for (size_t r = 0; r < rows; ++r) {\n        bool match = true;\n\n        // Compare chunks of 4 doubles at a time\n        for (size_t i = 0; i < vec_chunks; ++i) {\n            __m256d mat_reg = _mm256_loadu_pd(matrix + r * cols + i * 4);\n            __m256d cmp_result = _mm256_cmp_pd(mat_reg, vec_reg[i], _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp_result);\n\n            if (mask != 0xF) { // If any element doesn't match\n                match = false;\n                break;\n            }\n        }\n\n        // Handle remaining elements (if any)\n        if (match && remainder > 0) {\n            for (size_t i = vec_chunks * 4; i < cols; ++i) {\n                if (matrix[r * cols + i] != vector[i]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        // If a matching row is found, return true\n        if (match) {\n            return true;\n        }\n    }\n\n    // No matching row found\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    // Load the vector into AVX registers for reuse\n    __m256d vec_reg[4]; // Up to 4 AVX registers for a vector of up to 16 doubles\n    size_t vec_chunks = cols / 4;\n    size_t remainder = cols % 4;\n\n    for (size_t i = 0; i < vec_chunks; ++i) {\n        vec_reg[i] = _mm256_loadu_pd(&vector[i * 4]);\n    }\n\n    // Process each row of the matrix\n    for (size_t r = 0; r < rows; ++r) {\n        bool match = true;\n        size_t c = 0;\n\n        // Compare chunks of 4 doubles using AVX\n        for (size_t i = 0; i < vec_chunks; ++i) {\n            __m256d mat_reg = _mm256_loadu_pd(&matrix[r * cols + i * 4]);\n            __m256d cmp = _mm256_cmp_pd(mat_reg, vec_reg[i], _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n\n            if (mask != 0xF) { // If any element doesn't match\n                match = false;\n                break;\n            }\n            c += 4;\n        }\n\n        // Compare remaining elements (if any)\n        if (match && remainder > 0) {\n            for (size_t i = 0; i < remainder; ++i) {\n                if (matrix[r * cols + c + i] != vector[c + i]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        // If a matching row is found, return true\n        if (match) {\n            return true;\n        }\n    }\n\n    // No matching row found\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t equal_count = 0;\n\n    __m256i v_zero = _mm256_setzero_si256();\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i v_a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i v_b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compare for equality\n        __m256i v_cmp = _mm256_cmpeq_epi8(v_a, v_b);\n\n        // Convert comparison result to mask (0xFF for true, 0x00 for false)\n        __m256i v_mask = _mm256_and_si256(v_cmp, _mm256_set1_epi8(0xFF));\n\n        // Count the number of matching elements in this vector\n        int mask = _mm256_movemask_epi8(v_mask);\n        equal_count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++equal_count;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t equal_count = 0;\n\n    __m256i v_zero = _mm256_setzero_si256();\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i v_a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i v_b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compare for equality\n        __m256i v_eq = _mm256_cmpeq_epi8(v_a, v_b);\n\n        // Convert comparison result to mask and count the number of matching elements\n        int mask = _mm256_movemask_epi8(v_eq);\n        equal_count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++equal_count;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t equal_count = 0;\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compare for equality\n        __m256i cmp = _mm256_cmpeq_epi8(vecA, vecB);\n\n        // Create a mask from the comparison result\n        uint32_t mask = _mm256_movemask_epi8(cmp);\n\n        // Count the number of set bits in the mask\n        equal_count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++equal_count;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t equal_count = 0;\n\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i v_a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i v_b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        __m256i v_cmp = _mm256_cmpeq_epi8(v_a, v_b);\n        __m256i v_mask = _mm256_and_si256(v_cmp, _mm256_set1_epi8(1));\n\n        int mask = _mm256_movemask_epi8(v_mask);\n        equal_count += _mm_popcnt_u32(mask);\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++equal_count;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n    size_t equal_count = 0;\n\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    // Process 32 elements at a time using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compare for equality\n        __m256i cmp_result = _mm256_cmpeq_epi8(vec_a, vec_b);\n\n        // Convert comparison result to a bitmask\n        uint32_t mask = _mm256_movemask_epi8(cmp_result);\n\n        // Count the number of set bits in the mask\n        equal_count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            ++equal_count;\n        }\n    }\n\n    return equal_count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i va = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of a vector\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x into a SIMD register\n        __m256i vx = _mm256_loadu_si256((const __m256i*)(x + i));\n        \n        // Multiply each element of vx by the scalar 'a'\n        __m256i vmul = _mm256_mullo_epi64(vx, va);\n        \n        // Compute absolute value of y[i:i+3]\n        __m256i vy = _mm256_loadu_si256((const __m256i*)(y + i));\n        __m256i vabs = _mm256_abs_epi64(vy);\n        \n        // Subtract abs(y) from (a * x)\n        __m256i vsub = _mm256_sub_epi64(vmul, vabs);\n        \n        // Store the result back into y\n        _mm256_storeu_si256((__m256i*)(y + i), vsub);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - llabs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i va = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of the vector\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x into a SIMD register\n        __m256i vx = _mm256_loadu_si256((const __m256i*)(x + i));\n        \n        // Multiply each element of vx by the scalar 'a'\n        __m256i vmul = _mm256_mullo_epi64(vx, va);\n        \n        // Compute absolute value of y[i:i+3]\n        __m256i vy = _mm256_loadu_si256((const __m256i*)(y + i));\n        __m256i vabs = _mm256_abs_epi64(vy);\n        \n        // Subtract absolute values from the multiplied result\n        __m256i vsub = _mm256_sub_epi64(vmul, vabs);\n        \n        // Store the result back to y\n        _mm256_storeu_si256((__m256i*)(y + i), vsub);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        int64_t mul = a * x[i];\n        int64_t abs_val = (y[i] < 0) ? -y[i] : y[i];\n        y[i] = mul - abs_val;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i va = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of the vector\n\n    // Process 4 elements at a time (AVX2 processes 256 bits, and each int64_t is 64 bits)\n    for (; i + 3 < length; i += 4) {\n        // Load 4 elements from x and y\n        __m256i vx = _mm256_loadu_si256((__m256i const*)(x + i));\n        __m256i vy = _mm256_loadu_si256((__m256i const*)(y + i));\n\n        // Multiply 'a' with vector x\n        __m256i vmul = _mm256_mullo_epi64(va, vx);\n\n        // Compute absolute value of vector y\n        __m256i vabs = _mm256_abs_epi64(vy);\n\n        // Subtract absolute value of y from the result of multiplication\n        __m256i vsub = _mm256_sub_epi64(vmul, vabs);\n\n        // Store the result back to y\n        _mm256_storeu_si256((__m256i*)(y + i), vsub);\n    }\n\n    // Handle remaining elements (if any) using scalar operations\n    for (; i < length; ++i) {\n        int64_t mul = a * x[i];\n        int64_t abs_val = (y[i] < 0) ? -y[i] : y[i];\n        y[i] = mul - abs_val;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i va = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of a SIMD register\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x into a SIMD register\n        __m256i vx = _mm256_loadu_si256((__m256i const *)(x + i));\n        \n        // Multiply each element of vx by the scalar 'a'\n        __m256i vmul = _mm256_mullo_epi64(vx, va);\n        \n        // Compute absolute value of y[i:i+3]\n        __m256i vy = _mm256_loadu_si256((__m256i const *)(y + i));\n        __m256i vabs = _mm256_abs_epi64(vy);\n        \n        // Subtract absolute values from the multiplied result\n        __m256i vsub = _mm256_sub_epi64(vmul, vabs);\n        \n        // Store the result back to y\n        _mm256_storeu_si256((__m256i *)(y + i), vsub);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        int64_t mul = a * x[i];\n        int64_t abs_val = (y[i] < 0) ? -y[i] : y[i];\n        y[i] = mul - abs_val;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i va = _mm256_set1_epi64x(a); // Broadcast scalar 'a' to all elements of the vector\n\n    for (; i + 4 <= length; i += 4) {\n        // Load 4 elements from x into a SIMD register\n        __m256i vx = _mm256_loadu_si256((const __m256i*)(x + i));\n        \n        // Multiply each element of vx by the scalar 'a'\n        __m256i vmul = _mm256_mullo_epi64(vx, va);\n        \n        // Compute absolute value of y[i:i+3]\n        __m256i vy = _mm256_loadu_si256((const __m256i*)(y + i));\n        __m256i vabs = _mm256_abs_epi64(vy);\n        \n        // Subtract absolute values from the multiplied result\n        __m256i vsub = _mm256_sub_epi64(vmul, vabs);\n        \n        // Store the result back to y\n        _mm256_storeu_si256((__m256i*)(y + i), vsub);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        int64_t mul = a * x[i];\n        int64_t abs_val = (y[i] < 0) ? -y[i] : y[i];\n        y[i] = mul - abs_val;\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    __m256i vmin = _mm256_set1_epi16(0x7FFF); // Initialize to max int16_t\n    __m256i vmax = _mm256_set1_epi16(0x8000); // Initialize to min int16_t\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    // Horizontal reduction for min and max\n    __m128i vmin_low = _mm256_castsi256_si128(vmin);\n    __m128i vmin_high = _mm256_extracti128_si256(vmin, 1);\n    vmin_low = _mm_min_epi16(vmin_low, vmin_high);\n\n    __m128i vmax_low = _mm256_castsi256_si128(vmax);\n    __m128i vmax_high = _mm256_extracti128_si256(vmax, 1);\n    vmax_low = _mm_max_epi16(vmax_low, vmax_high);\n\n    // Further reduce within 128-bit lanes\n    vmin_low = _mm_min_epi16(vmin_low, _mm_shuffle_epi32(vmin_low, 0b10110001));\n    vmin_low = _mm_min_epi16(vmin_low, _mm_shuffle_epi32(vmin_low, 0b01001110));\n    result.min_num = (int16_t)_mm_extract_epi16(vmin_low, 0);\n\n    vmax_low = _mm_max_epi16(vmax_low, _mm_shuffle_epi32(vmax_low, 0b10110001));\n    vmax_low = _mm_max_epi16(vmax_low, _mm_shuffle_epi32(vmax_low, 0b01001110));\n    result.max_num = (int16_t)_mm_extract_epi16(vmax_low, 0);\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < result.min_num) result.min_num = vec[i];\n        if (vec[i] > result.max_num) result.max_num = vec[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i vmin = _mm256_set1_epi16(0x7FFF); // Initialize to max int16_t\n    __m256i vmax = _mm256_set1_epi16(0x8000); // Initialize to min int16_t\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    // Horizontal reduction for min and max\n    alignas(32) int16_t temp_min[16];\n    alignas(32) int16_t temp_max[16];\n    _mm256_store_si256((__m256i*)temp_min, vmin);\n    _mm256_store_si256((__m256i*)temp_max, vmax);\n\n    int16_t min_val = 0x7FFF;\n    int16_t max_val = 0x8000;\n    for (int j = 0; j < 16; ++j) {\n        if (temp_min[j] < min_val) min_val = temp_min[j];\n        if (temp_max[j] > max_val) max_val = temp_max[j];\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i vmin = _mm256_set1_epi16(0x7FFF); // Initialize to max int16_t\n    __m256i vmax = _mm256_set1_epi16(0x8000); // Initialize to min int16_t\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const*)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    // Horizontal reduction for min and max\n    int16_t min_vals[16], max_vals[16];\n    _mm256_storeu_si256((__m256i*)min_vals, vmin);\n    _mm256_storeu_si256((__m256i*)max_vals, vmax);\n\n    int16_t min_num = 0x7FFF, max_num = 0x8000;\n    for (int j = 0; j < 16; ++j) {\n        if (min_vals[j] < min_num) min_num = min_vals[j];\n        if (max_vals[j] > max_num) max_num = max_vals[j];\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    MinMaxPair result = {min_num, max_num};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result;\n    __m256i vmin = _mm256_set1_epi16(0x7FFF); // Initialize to max int16_t\n    __m256i vmax = _mm256_set1_epi16(0x8000); // Initialize to min int16_t\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((const __m256i*)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    // Horizontal reduction for min and max\n    alignas(32) int16_t temp_min[16];\n    alignas(32) int16_t temp_max[16];\n    _mm256_store_si256((__m256i*)temp_min, vmin);\n    _mm256_store_si256((__m256i*)temp_max, vmax);\n\n    int16_t min_val = 0x7FFF;\n    int16_t max_val = 0x8000;\n    for (int j = 0; j < 16; ++j) {\n        if (temp_min[j] < min_val) min_val = temp_min[j];\n        if (temp_max[j] > max_val) max_val = temp_max[j];\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i vmin = _mm256_set1_epi16(0x7FFF); // Initialize to max int16_t\n    __m256i vmax = _mm256_set1_epi16(0x8000); // Initialize to min int16_t\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    // Horizontal reduction for min and max\n    alignas(32) int16_t temp_min[16], temp_max[16];\n    _mm256_store_si256((__m256i*)temp_min, vmin);\n    _mm256_store_si256((__m256i*)temp_max, vmax);\n\n    int16_t min_val = 0x7FFF, max_val = 0x8000;\n    for (int j = 0; j < 16; ++j) {\n        if (temp_min[j] < min_val) min_val = temp_min[j];\n        if (temp_max[j] > max_val) max_val = temp_max[j];\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            // Compare current < next for all 8 elements\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            // Check if all comparisons are true (all elements strictly increasing)\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        // Process remaining elements in the row\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            // Compare current < next for all 8 elements\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            // Check if all comparisons are true (all elements strictly increasing)\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        // Process remaining elements in the row\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((const __m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((const __m256i*)(row + j + 1));\n\n            // Compare current < next for all 8 elements\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            // Check if all comparisons are true (all bits set)\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        // Process remaining elements\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            // Compare current < next for all 8 elements\n            __m256i cmp = _mm256_cmpgt_epi32(next, current);\n\n            // Check if all comparisons are true (all bits set in mask)\n            if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n                return false;\n            }\n        }\n\n        // Process remaining elements\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j + 8 <= cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i const*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i const*)(row + j + 1));\n\n            // Compare current with next to check strictly increasing\n            __m256i cmp = _mm256_cmpgt_epi32(current, next);\n\n            // If any element is not strictly increasing, return false\n            if (!_mm256_testz_si256(cmp, cmp)) {\n                return false;\n            }\n        }\n\n        // Process remaining elements\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Load the first block into AVX registers for comparison\n    __m256i reference_block[4]; // Assuming block_size <= 4 for simplicity\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            reference_block[i * (block_size / 8) + (j / 8)] = _mm256_loadu_si256(\n                reinterpret_cast<const __m256i*>(&matrix[i * cols + j]));\n        }\n    }\n\n    // Compare all other blocks with the reference block\n    for (size_t bi = 0; bi < num_blocks_row; ++bi) {\n        for (size_t bj = 0; bj < num_blocks_col; ++bj) {\n            if (bi == 0 && bj == 0) continue; // Skip the reference block\n\n            bool block_equal = true;\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    __m256i current_block = _mm256_loadu_si256(\n                        reinterpret_cast<const __m256i*>(&matrix[(bi * block_size + i) * cols + bj * block_size + j]));\n                    __m256i cmp_result = _mm256_cmpeq_epi32(current_block, reference_block[i * (block_size / 8) + (j / 8)]);\n                    int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_result));\n                    if (mask != 0xFF) { // If any element is not equal\n                        block_equal = false;\n                        break;\n                    }\n                }\n                if (!block_equal) break;\n            }\n            if (!block_equal) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_area = block_size * block_size;\n\n    // Load the first block into AVX registers for comparison\n    __m256i reference_block[4]; // Assuming block_size <= 4 for simplicity\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            reference_block[i * (block_size / 8) + j / 8] = _mm256_loadu_si256(\n                reinterpret_cast<const __m256i*>(&matrix[i * cols + j]));\n        }\n    }\n\n    // Compare all other blocks with the reference block\n    for (size_t bi = 0; bi < num_blocks_row; ++bi) {\n        for (size_t bj = 0; bj < num_blocks_col; ++bj) {\n            if (bi == 0 && bj == 0) continue; // Skip the reference block\n\n            size_t block_start_row = bi * block_size;\n            size_t block_start_col = bj * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    __m256i current_block = _mm256_loadu_si256(\n                        reinterpret_cast<const __m256i*>(&matrix[(block_start_row + i) * cols + block_start_col + j]));\n\n                    __m256i ref_block = reference_block[i * (block_size / 8) + j / 8];\n                    __m256i cmp_result = _mm256_cmpeq_epi32(current_block, ref_block);\n                    int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_result));\n\n                    if (mask != 0xFF) { // If any element is not equal\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Load the first block into AVX registers for comparison\n    __m256i reference_block[4]; // Assuming block_size <= 4 for simplicity\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            reference_block[i] = _mm256_loadu_si256((__m256i const*)(matrix + i * cols + j));\n        }\n    }\n\n    // Compare all other blocks with the reference block\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue; // Skip the reference block\n\n            size_t block_start_row = block_row * block_size;\n            size_t block_start_col = block_col * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    __m256i current_block = _mm256_loadu_si256((__m256i const*)(matrix + (block_start_row + i) * cols + (block_start_col + j)));\n                    __m256i cmp_result = _mm256_cmpeq_epi32(current_block, reference_block[i]);\n                    int mask = _mm256_movemask_ps((__m256)cmp_result);\n                    if (mask != 0xFF) { // If any element is not equal\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Load the first block into AVX registers for comparison\n    __m256i reference[4]; // Assuming block_size <= 4 for simplicity\n    for (size_t i = 0; i < block_size; ++i) {\n        reference[i] = _mm256_loadu_si256((__m256i const*)(matrix + i * cols));\n    }\n\n    // Compare all other blocks with the first block\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue; // Skip the first block\n\n            size_t block_start = (block_row * block_size * cols) + (block_col * block_size);\n\n            for (size_t i = 0; i < block_size; ++i) {\n                __m256i current = _mm256_loadu_si256((__m256i const*)(matrix + block_start + i * cols));\n                __m256i cmp = _mm256_cmpeq_epi32(current, reference[i]);\n                int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n                if (mask != 0xFFFFFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_elements = block_size * block_size;\n\n    // Load the first block into AVX registers for comparison\n    __m256i reference_block[4]; // Assuming block_size <= 4 for simplicity\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            reference_block[i * (block_size / 8) + (j / 8)] = _mm256_loadu_si256(\n                reinterpret_cast<const __m256i*>(&matrix[i * cols + j]));\n        }\n    }\n\n    // Compare all other blocks with the reference block\n    for (size_t block_row = 0; block_row < num_blocks_row; ++block_row) {\n        for (size_t block_col = 0; block_col < num_blocks_col; ++block_col) {\n            if (block_row == 0 && block_col == 0) continue; // Skip the reference block\n\n            size_t block_start_row = block_row * block_size;\n            size_t block_start_col = block_col * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    __m256i current_block = _mm256_loadu_si256(\n                        reinterpret_cast<const __m256i*>(&matrix[(block_start_row + i) * cols + block_start_col + j]));\n\n                    __m256i ref_block = reference_block[i * (block_size / 8) + (j / 8)];\n                    __m256i cmp_result = _mm256_cmpeq_epi32(current_block, ref_block);\n\n                    int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_result));\n                    if (mask != 0xFF) { // If any element is not equal\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block[4]; // Assuming AVX can handle up to 4 doubles per register\n\n    for (size_t i = 0; i < block_size; i += 4) {\n        first_block[i / 4] = _mm256_loadu_pd(vec + i);\n    }\n\n    for (size_t b = 1; b < num_blocks; ++b) {\n        const double* current_block = vec + b * block_size;\n\n        for (size_t i = 0; i < block_size; i += 4) {\n            __m256d current_vals = _mm256_loadu_pd(current_block + i);\n            __m256d diff = _mm256_sub_pd(current_vals, first_block[i / 4]);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tolerance_vec, _CMP_GT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || length < block_size || block_size % 4 != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return false;\n    }\n\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n    __m256d neg_tolerance_vec = _mm256_set1_pd(-tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d first_block = _mm256_loadu_pd(vec + j);\n            __m256d current_block = _mm256_loadu_pd(vec + i * block_size + j);\n\n            __m256d diff = _mm256_sub_pd(current_block, first_block);\n            __m256d cmp1 = _mm256_cmp_pd(diff, tolerance_vec, _CMP_LE_OS);\n            __m256d cmp2 = _mm256_cmp_pd(diff, neg_tolerance_vec, _CMP_GE_OS);\n            __m256d result = _mm256_and_pd(cmp1, cmp2);\n\n            if (_mm256_movemask_pd(result) != 0xF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d first_block = _mm256_loadu_pd(&vec[j]);\n            __m256d current_block = _mm256_loadu_pd(&vec[i * block_size + j]);\n\n            __m256d diff = _mm256_sub_pd(current_block, first_block);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tolerance_vec, _CMP_GT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            __m256d first_block = _mm256_loadu_pd(vec + j);\n            __m256d current_block = _mm256_loadu_pd(vec + i * block_size + j);\n\n            __m256d diff = _mm256_sub_pd(current_block, first_block);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tolerance_vec, _CMP_GT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    __m256d tolerance_vec = _mm256_set1_pd(tolerance);\n\n    // Load the first block into AVX registers for comparison\n    __m256d first_block[4]; // Assuming block_size is a multiple of 4 doubles\n    for (size_t i = 0; i < block_size / 4; ++i) {\n        first_block[i] = _mm256_loadu_pd(vec + i * 4);\n    }\n\n    // Compare each subsequent block with the first block\n    for (size_t b = 1; b < num_blocks; ++b) {\n        const double* current_block = vec + b * block_size;\n\n        for (size_t i = 0; i < block_size / 4; ++i) {\n            __m256d current = _mm256_loadu_pd(current_block + i * 4);\n            __m256d diff = _mm256_sub_pd(current, first_block[i]);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tolerance_vec, _CMP_GT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) { // Check if any element exceeds tolerance\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t col_align = cols - (cols % 4); // Align columns to process in chunks of 4\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd(); // Initialize sum vector to zero\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j < col_align; j += 4) {\n            __m256d row_vals = _mm256_loadu_pd(&matrix[i * cols + j]); // Load 4 doubles from the row\n            sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(row_vals, row_vals)); // Accumulate squared values\n        }\n\n        // Horizontal sum of the 4 partial sums in sum_vec\n        double sum = 0.0;\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_vec);\n        sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining columns that didn't fit into the AVX chunk\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n\n        // Compare the Euclidean norm with the threshold\n        double norm = _mm_sqrt_sd(_mm_set_sd(sum), _mm_set_sd(sum))[0];\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256d row_vals = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d squared = _mm256_mul_pd(row_vals, row_vals);\n            sum_vec = _mm256_add_pd(sum_vec, squared);\n        }\n\n        // Horizontal sum of the AVX vector\n        double sum = 0.0;\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_vec);\n        sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining elements that don't fit into SIMD width\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            sum += val * val;\n        }\n\n        // Compare the Euclidean norm with the threshold\n        double norm = sqrt(sum);\n        result[i] = (norm > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t vec_size = 4; // AVX processes 4 doubles at a time\n    __m256d threshold_vec;\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        const double* row_ptr = matrix + i * cols;\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j + vec_size <= cols; j += vec_size) {\n            __m256d data_vec = _mm256_loadu_pd(row_ptr + j);\n            sum_vec = _mm256_fmadd_pd(data_vec, data_vec, sum_vec);\n        }\n\n        // Horizontal sum of the AVX vector\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_vec);\n        double norm_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining elements that don't fit into AVX chunks\n        for (; j < cols; ++j) {\n            norm_sq += row_ptr[j] * row_ptr[j];\n        }\n\n        // Compare with threshold\n        double norm = _mm_sqrt_sd(_mm_set_sd(norm_sq), _mm_set_sd(norm_sq))[0];\n        threshold_vec = _mm256_set1_pd(thresholds[i]);\n        __m256d norm_vec = _mm256_set1_pd(norm);\n        __m256d cmp_vec = _mm256_cmp_pd(norm_vec, threshold_vec, _CMP_GT_OQ);\n\n        // Extract comparison result\n        int mask = _mm256_movemask_pd(cmp_vec);\n        result[i] = (mask & 0x1) != 0;\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    __m256d threshold_vec;\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        const double* row_ptr = matrix + i * cols;\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256d data_vec = _mm256_loadu_pd(row_ptr + j);\n            sum_vec = _mm256_fmadd_pd(data_vec, data_vec, sum_vec);\n        }\n\n        // Horizontal sum of the partial sums in sum_vec\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_vec);\n        double norm_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining elements that don't fit into SIMD width\n        for (; j < cols; ++j) {\n            double val = row_ptr[j];\n            norm_sq += val * val;\n        }\n\n        // Compare the Euclidean norm with the threshold\n        double norm = _mm_sqrt_sd(_mm_set_sd(norm_sq), _mm_set_sd(norm_sq))[0];\n        threshold_vec = _mm256_set1_pd(thresholds[i]);\n        __m256d norm_vec = _mm256_set1_pd(norm);\n        __m256d cmp_vec = _mm256_cmp_pd(norm_vec, threshold_vec, _CMP_GT_OQ);\n\n        // Extract comparison result\n        int mask = _mm256_movemask_pd(cmp_vec);\n        result[i] = (mask & 0x1) != 0;\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t simd_width = 4; // AVX processes 4 doubles at a time\n    __m256d threshold_vec;\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum_vec = _mm256_setzero_pd();\n        size_t j = 0;\n\n        // Process columns in chunks of 4 using AVX\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256d row_vec = _mm256_loadu_pd(&matrix[i * cols + j]);\n            sum_vec = _mm256_fmadd_pd(row_vec, row_vec, sum_vec);\n        }\n\n        // Horizontal sum of the partial sums in sum_vec\n        double temp[4];\n        _mm256_storeu_pd(temp, sum_vec);\n        double norm_sq = temp[0] + temp[1] + temp[2] + temp[3];\n\n        // Process remaining elements that don't fit into SIMD width\n        for (; j < cols; ++j) {\n            double val = matrix[i * cols + j];\n            norm_sq += val * val;\n        }\n\n        // Compare with threshold\n        threshold_vec = _mm256_set1_pd(thresholds[i]);\n        __m256d norm_sq_vec = _mm256_set1_pd(norm_sq);\n        __m256d cmp_vec = _mm256_cmp_pd(norm_sq_vec, threshold_vec, _CMP_GT_OQ);\n\n        // Extract comparison result\n        int mask = _mm256_movemask_pd(cmp_vec);\n        result[i] = (mask & 0x1) != 0; // Only the first element matters\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length / 8 * 8;\n\n    for (; i < simd_length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length - (length % 8);\n\n    for (; i < simd_length; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vec_c = _mm256_or_si256(vec_a, vec_b);\n        _mm256_storeu_si256((__m256i*)(C + i), vec_c);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length - (length % 8);\n\n    for (; i < simd_length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length - (length % 8);\n\n    for (; i < simd_length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length / 8 * 8;\n\n    for (i = 0; i < simd_length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i vsum_squares = _mm256_setzero_si256();\n    __m256i vsum_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i va = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compute squares of A and B\n        __m256i va_sq = _mm256_maddubs_epi16(va, va);\n        __m256i vb_sq = _mm256_maddubs_epi16(vb, vb);\n\n        // Add the squares\n        vsum_squares = _mm256_add_epi16(vsum_squares, va_sq);\n        vsum_squares = _mm256_add_epi16(vsum_squares, vb_sq);\n\n        // Compute products of A and B\n        __m256i vprod = _mm256_maddubs_epi16(va, vb);\n\n        // Add the products\n        vsum_products = _mm256_add_epi16(vsum_products, vsum_products);\n    }\n\n    // Horizontal sum of vsum_squares\n    __m128i sum_squares_lo = _mm256_castsi256_si128(vsum_squares);\n    __m128i sum_squares_hi = _mm256_extracti128_si256(vsum_squares, 1);\n    sum_squares_lo = _mm_add_epi16(sum_squares_lo, sum_squares_hi);\n    sum_squares_lo = _mm_hadd_epi16(sum_squares_lo, sum_squares_lo);\n    sum_squares_lo = _mm_hadd_epi16(sum_squares_lo, sum_squares_lo);\n    int64_t sum_squares = _mm_cvtsi128_si64(sum_squares_lo);\n\n    // Horizontal sum of vsum_products\n    __m128i sum_products_lo = _mm256_castsi256_si128(vsum_products);\n    __m128i sum_products_hi = _mm256_extracti128_si256(vsum_products, 1);\n    sum_products_lo = _mm_add_epi16(sum_products_lo, sum_products_hi);\n    sum_products_lo = _mm_hadd_epi16(sum_products_lo, sum_products_lo);\n    sum_products_lo = _mm_hadd_epi16(sum_products_lo, sum_products_lo);\n    int64_t sum_products = _mm_cvtsi128_si64(sum_products_lo);\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int32_t a = A[i];\n        int32_t b = B[i];\n        sum_squares += (a * a) + (b * b);\n        sum_products += (a * b);\n    }\n\n    return sum_squares - sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i vsum_squares = _mm256_setzero_si256();\n    __m256i vsum_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i va = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compute squares of A and B\n        __m256i va_sq = _mm256_maddubs_epi16(va, va);\n        __m256i vb_sq = _mm256_maddubs_epi16(vb, vb);\n\n        // Sum of squares\n        __m256i vsum_sq = _mm256_add_epi16(va_sq, vb_sq);\n        vsum_squares = _mm256_add_epi16(vsum_squares, vsum_sq);\n\n        // Compute products of A and B\n        __m256i vprod = _mm256_maddubs_epi16(va, vb);\n        vsum_products = _mm256_add_epi16(vsum_products, vprod);\n    }\n\n    // Horizontal sum of the accumulated results\n    int16_t sum_squares_arr[16];\n    int16_t sum_products_arr[16];\n    _mm256_storeu_si256((__m256i*)sum_squares_arr, vsum_squares);\n    _mm256_storeu_si256((__m256i*)sum_products_arr, vsum_products);\n\n    int64_t sum_squares = 0;\n    int64_t sum_products = 0;\n    for (int j = 0; j < 16; ++j) {\n        sum_squares += sum_squares_arr[j];\n        sum_products += sum_products_arr[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        sum_squares += (int16_t)(A[i] * A[i]) + (int16_t)(B[i] * B[i]);\n        sum_products += (int16_t)(A[i] * B[i]);\n    }\n\n    return sum_squares - sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i vsum_squares = _mm256_setzero_si256();\n    __m256i vsum_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i va = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compute squares of A and B\n        __m256i va_sq = _mm256_maddubs_epi16(va, va);\n        __m256i vb_sq = _mm256_maddubs_epi16(vb, vb);\n\n        // Sum of squares\n        __m256i vsum_sq = _mm256_add_epi16(va_sq, vb_sq);\n        vsum_squares = _mm256_add_epi16(vsum_squares, vsum_sq);\n\n        // Compute products of A and B\n        __m256i vprod = _mm256_maddubs_epi16(va, vb);\n\n        // Sum of products\n        vsum_products = _mm256_add_epi16(vsum_products, vprod);\n    }\n\n    // Horizontal sum for squares\n    int64_t sum_squares = 0;\n    int16_t temp_squares[16];\n    _mm256_storeu_si256((__m256i*)temp_squares, vsum_squares);\n    for (int j = 0; j < 16; ++j) {\n        sum_squares += temp_squares[j];\n    }\n\n    // Horizontal sum for products\n    int64_t sum_products = 0;\n    int16_t temp_products[16];\n    _mm256_storeu_si256((__m256i*)temp_products, vsum_products);\n    for (int j = 0; j < 16; ++j) {\n        sum_products += temp_products[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        sum_squares += (int64_t)(A[i] * A[i]) + (int64_t)(B[i] * B[i]);\n        sum_products += (int64_t)(A[i] * B[i]);\n    }\n\n    return sum_squares - sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i vsum_squares = _mm256_setzero_si256();\n    __m256i vsum_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i va = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compute squares of A and B\n        __m256i va_sq = _mm256_maddubs_epi16(va, va);\n        __m256i vb_sq = _mm256_maddubs_epi16(vb, vb);\n\n        // Sum of squares\n        __m256i vsum_sq = _mm256_add_epi16(va_sq, vb_sq);\n        vsum_squares = _mm256_add_epi16(vsum_squares, vsum_sq);\n\n        // Compute products of A and B\n        __m256i vprod = _mm256_maddubs_epi16(va, vb);\n\n        // Sum of products\n        vsum_products = _mm256_add_epi16(vsum_products, vprod);\n    }\n\n    // Horizontal sum of the accumulated results\n    int64_t sum_squares = 0;\n    int64_t sum_products = 0;\n\n    int16_t temp[16];\n    _mm256_storeu_si256((__m256i*)temp, vsum_squares);\n    for (int j = 0; j < 16; ++j) {\n        sum_squares += temp[j];\n    }\n\n    _mm256_storeu_si256((__m256i*)temp, vsum_products);\n    for (int j = 0; j < 16; ++j) {\n        sum_products += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        sum_squares += (int16_t)(A[i] * A[i]) + (int16_t)(B[i] * B[i]);\n        sum_products += (int16_t)(A[i] * B[i]);\n    }\n\n    return sum_squares - sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i vsum_squares = _mm256_setzero_si256();\n    __m256i vsum_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 32 <= length; i += 32) {\n        __m256i va = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Compute squares of A and B\n        __m256i va_sq = _mm256_maddubs_epi16(va, va);\n        __m256i vb_sq = _mm256_maddubs_epi16(vb, vb);\n\n        // Sum of squares\n        __m256i vsum_sq = _mm256_add_epi16(va_sq, vb_sq);\n        vsum_squares = _mm256_add_epi16(vsum_squares, vsum_sq);\n\n        // Compute products of A and B\n        __m256i vprod = _mm256_maddubs_epi16(va, vb);\n        vsum_products = _mm256_add_epi16(vsum_products, vprod);\n    }\n\n    // Horizontal sum of the accumulated results\n    int16_t sum_squares_arr[16];\n    int16_t sum_products_arr[16];\n    _mm256_storeu_si256((__m256i*)sum_squares_arr, vsum_squares);\n    _mm256_storeu_si256((__m256i*)sum_products_arr, vsum_products);\n\n    int64_t sum_squares = 0;\n    int64_t sum_products = 0;\n    for (int j = 0; j < 16; ++j) {\n        sum_squares += sum_squares_arr[j];\n        sum_products += sum_products_arr[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        sum_squares += (int16_t)(A[i] * A[i]) + (int16_t)(B[i] * B[i]);\n        sum_products += (int16_t)(A[i] * B[i]);\n    }\n\n    return sum_squares - sum_products;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi64x(-1); // All ones (64-bit)\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 4 <= length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);       // Load 4 elements from A\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);       // Load 4 elements from B\n        __m256i neg_b = _mm256_xor_si256(b, mask);             // Bitwise NOT of B\n        __m256i result = _mm256_and_si256(a, neg_b);           // A & ~B\n        _mm256_storeu_si256((__m256i*)&C[i], result);          // Store result in C\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t simd_length = length - (length % 4);\n\n    for (; i < simd_length; i += 4) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i neg_b = _mm256_andnot_si256(vec_b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(vec_a, neg_b);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi64x(-1); // All ones (64-bit)\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < length; i += 4) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]); // Load 4 elements from A\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]); // Load 4 elements from B\n\n        __m256i negB = _mm256_xor_si256(vecB, mask); // Bitwise NOT of B\n        __m256i result = _mm256_and_si256(vecA, negB); // A & ~B\n\n        _mm256_storeu_si256((__m256i*)&C[i], result); // Store the result in C\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi64x(-1); // All ones (64-bit)\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]); // Load 4 elements from A\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]); // Load 4 elements from B\n\n        // Compute bitwise NOT of B\n        __m256i not_b = _mm256_xor_si256(b, mask);\n\n        // Perform bitwise AND between A and NOT B\n        __m256i result = _mm256_and_si256(a, not_b);\n\n        // Store the result in C\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi64x(-1); // All bits set to 1 (for bitwise negation)\n\n    // Process 4 elements at a time (AVX2 processes 256 bits = 4 x 64-bit integers)\n    for (; i + 3 < length; i += 4) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]); // Load 4 elements from A\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]); // Load 4 elements from B\n\n        __m256i negB = _mm256_xor_si256(vecB, mask); // Bitwise NOT of vecB\n        __m256i result = _mm256_and_si256(vecA, negB); // Bitwise AND of vecA and negB\n\n        _mm256_storeu_si256((__m256i*)&C[i], result); // Store the result in C\n    }\n\n    // Process remaining elements (if any) using scalar operations\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Check if both A and B are positive\n        __m256i pos_mask = _mm256_and_si256(_mm256_cmpgt_epi8(a, zeros), _mm256_cmpgt_epi8(b, zeros));\n        // Check if both A and B are negative\n        __m256i neg_mask = _mm256_and_si256(_mm256_cmpgt_epi8(zeros, a), _mm256_cmpgt_epi8(zeros, b));\n\n        // Combine results: 1 for positive, -1 for negative, 0 otherwise\n        __m256i result = _mm256_blendv_epi8(zeros, ones, pos_mask);\n        result = _mm256_blendv_epi8(result, neg_ones, neg_mask);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Check if both A and B are positive\n        __m256i pos_mask = _mm256_and_si256(_mm256_cmpgt_epi8(a, zeros), _mm256_cmpgt_epi8(b, zeros));\n        // Check if both A and B are negative\n        __m256i neg_mask = _mm256_and_si256(_mm256_cmpgt_epi8(zeros, a), _mm256_cmpgt_epi8(zeros, b));\n\n        // Combine results: 1 for positive, -1 for negative, 0 otherwise\n        __m256i result = _mm256_blendv_epi8(zeros, ones, pos_mask);\n        result = _mm256_blendv_epi8(result, neg_ones, neg_mask);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Check if both A and B are positive\n        __m256i pos_mask = _mm256_and_si256(_mm256_cmpgt_epi8(a, zeros), _mm256_cmpgt_epi8(b, zeros));\n        // Check if both A and B are negative\n        __m256i neg_mask = _mm256_and_si256(_mm256_cmpgt_epi8(zeros, a), _mm256_cmpgt_epi8(zeros, b));\n\n        // Combine masks to determine output values\n        __m256i result = _mm256_blendv_epi8(zeros, ones, pos_mask);\n        result = _mm256_blendv_epi8(result, neg_ones, neg_mask);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        // Check if both A and B are positive\n        __m256i pos_mask = _mm256_and_si256(_mm256_cmpgt_epi8(a, zeros), _mm256_cmpgt_epi8(b, zeros));\n        // Check if both A and B are negative\n        __m256i neg_mask = _mm256_and_si256(_mm256_cmpgt_epi8(zeros, a), _mm256_cmpgt_epi8(zeros, b));\n\n        // Combine masks to determine output values\n        __m256i result = _mm256_blendv_epi8(zeros, ones, pos_mask);\n        result = _mm256_blendv_epi8(result, neg_ones, neg_mask);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n    __m256i zeros = _mm256_setzero_si256();\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((const __m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((const __m256i*)(B + i));\n\n        __m256i both_positive = _mm256_and_si256(_mm256_cmpgt_epi8(a, zeros), _mm256_cmpgt_epi8(b, zeros));\n        __m256i both_negative = _mm256_and_si256(_mm256_cmpgt_epi8(zeros, a), _mm256_cmpgt_epi8(zeros, b));\n\n        __m256i result = _mm256_blendv_epi8(zeros, ones, both_positive);\n        result = _mm256_blendv_epi8(result, neg_ones, both_negative);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Handle remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 == 0) {\n        C[midpoint] = A[midpoint];\n    } else {\n        C[midpoint] = A[midpoint];\n        ++midpoint;\n    }\n\n    // Process elements after the midpoint using AVX2\n    for (; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Handle remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2\n    for (; i + 16 <= mid; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 == 0) {\n        C[mid] = A[mid];\n    } else {\n        C[mid] = A[mid];\n        ++mid;\n    }\n\n    // Process elements after the midpoint using AVX2\n    for (; mid + 16 <= length; mid += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[mid]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[mid]);\n        __m256i result = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[mid], result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; mid < length; ++mid) {\n        C[mid] = A[mid] & B[mid];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 == 0) {\n        C[midpoint] = A[midpoint];\n    } else {\n        C[midpoint] = A[midpoint];\n        ++midpoint;\n    }\n\n    // Process elements after the midpoint using AVX2\n    for (i = midpoint; i + 16 <= length; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t vec_size = 16; // AVX2 processes 16 elements of uint16_t per __m256i register\n\n    // Process elements before the midpoint using XOR\n    size_t i = 0;\n    for (; i + vec_size <= midpoint; i += vec_size) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements before the midpoint using scalar XOR\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 == 0) {\n        C[midpoint] = A[midpoint];\n    } else {\n        C[midpoint] = A[midpoint];\n        ++midpoint;\n    }\n\n    // Process elements after the midpoint using AND\n    for (i = midpoint; i + vec_size <= length; i += vec_size) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n\n    // Process remaining elements after the midpoint using scalar AND\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint using AVX2\n    for (; i + 16 <= midpoint; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process remaining elements before the midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    if (length % 2 == 0) {\n        C[midpoint] = A[midpoint];\n    } else {\n        C[midpoint] = A[midpoint];\n        ++midpoint;\n    }\n\n    // Process elements after the midpoint using AVX2\n    for (i = midpoint; i + 16 <= length; i += 16) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i result = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Process remaining elements after the midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    for (size_t i = 0; i < total_elements; i += 8) {\n        // Load 8 floats from each input tensor\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n        __m256 vecC = _mm256_loadu_ps(&C[i]);\n        __m256 vecD = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean values from cond1 and cond2\n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean conditions to masks\n        __m256 mask_cond1 = _mm256_cvtepi32_ps(_mm256_and_si256(cond1_vec, _mm256_set1_epi32(0xFF)));\n        __m256 mask_cond2 = _mm256_cvtepi32_ps(_mm256_and_si256(cond2_vec, _mm256_set1_epi32(0xFF)));\n\n        // Compute the four possible cases using masks\n        __m256 case1 = _mm256_and_ps(mask_cond1, _mm256_and_ps(mask_cond2, vecA)); // cond1 && cond2 -> A\n        __m256 case2 = _mm256_and_ps(mask_cond1, _mm256_andnot_ps(mask_cond2, vecB)); // cond1 && !cond2 -> B\n        __m256 case3 = _mm256_and_ps(_mm256_andnot_ps(mask_cond1, mask_cond2), vecC); // !cond1 && cond2 -> C\n        __m256 case4 = _mm256_and_ps(_mm256_andnot_ps(mask_cond1, _mm256_andnot_ps(mask_cond2, vecD)), vecD); // !cond1 && !cond2 -> D\n\n        // Combine all cases into the final result\n        __m256 result = _mm256_or_ps(_mm256_or_ps(case1, case2), _mm256_or_ps(case3, case4));\n\n        // Store the result back to the output tensor\n        _mm256_storeu_ps(&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    for (size_t i = 0; i < total_elements; i += 8) {\n        // Load 8 elements from each input tensor\n        __m256 vec_A = _mm256_loadu_ps(&A[i]);\n        __m256 vec_B = _mm256_loadu_ps(&B[i]);\n        __m256 vec_C = _mm256_loadu_ps(&C[i]);\n        __m256 vec_D = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean conditions from cond1 and cond2\n        __m256i vec_cond1 = _mm256_cvtepi8_epi32(_mm_loadu_si128((__m128i*)&cond1[i])); // Expand to 32-bit integers\n        __m256i vec_cond2 = _mm256_cvtepi8_epi32(_mm_loadu_si128((__m128i*)&cond2[i]));\n\n        // Convert boolean conditions to masks\n        __m256 mask_cond1 = _mm256_castsi256_ps(_mm256_slli_epi32(vec_cond1, 31)); // Shift to create a mask\n        __m256 mask_cond2 = _mm256_castsi256_ps(_mm256_slli_epi32(vec_cond2, 31));\n\n        // Compute the four possible cases using masks\n        __m256 case1 = _mm256_and_ps(mask_cond1, _mm256_and_ps(mask_cond2, vec_A)); // cond1 && cond2 -> A\n        __m256 case2 = _mm256_and_ps(mask_cond1, _mm256_andnot_ps(mask_cond2, vec_B)); // cond1 && !cond2 -> B\n        __m256 case3 = _mm256_and_ps(_mm256_andnot_ps(mask_cond1, mask_cond2), vec_C); // !cond1 && cond2 -> C\n        __m256 case4 = _mm256_and_ps(_mm256_andnot_ps(mask_cond1, _mm256_andnot_ps(mask_cond2, vec_D)), vec_D); // !cond1 && !cond2 -> D\n\n        // Combine all cases into the final result\n        __m256 result = _mm256_or_ps(_mm256_or_ps(case1, case2), _mm256_or_ps(case3, case4));\n\n        // Store the result back to the output tensor\n        _mm256_storeu_ps(&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    for (size_t i = 0; i < total_elements; i += 8) {\n        // Load 8 elements from each input tensor\n        __m256 vec_A = _mm256_loadu_ps(&A[i]);\n        __m256 vec_B = _mm256_loadu_ps(&B[i]);\n        __m256 vec_C = _mm256_loadu_ps(&C[i]);\n        __m256 vec_D = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean conditions from cond1 and cond2\n        __m256i vec_cond1 = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i vec_cond2 = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean conditions to masks\n        __m256 mask_cond1 = _mm256_cvtepi32_ps(vec_cond1);\n        __m256 mask_cond2 = _mm256_cvtepi32_ps(vec_cond2);\n\n        // Compute the four cases using masks\n        __m256 case1 = _mm256_and_ps(mask_cond1, mask_cond2); // cond1 && cond2\n        __m256 case2 = _mm256_and_ps(mask_cond1, _mm256_xor_ps(mask_cond2, _mm256_set1_ps(1.0f))); // cond1 && !cond2\n        __m256 case3 = _mm256_and_ps(_mm256_xor_ps(mask_cond1, _mm256_set1_ps(1.0f)), mask_cond2); // !cond1 && cond2\n        __m256 case4 = _mm256_and_ps(_mm256_xor_ps(mask_cond1, _mm256_set1_ps(1.0f)), _mm256_xor_ps(mask_cond2, _mm256_set1_ps(1.0f))); // !cond1 && !cond2\n\n        // Select values based on the cases\n        __m256 result = _mm256_or_ps(\n            _mm256_or_ps(\n                _mm256_and_ps(case1, vec_A),\n                _mm256_and_ps(case2, vec_B)\n            ),\n            _mm256_or_ps(\n                _mm256_and_ps(case3, vec_C),\n                _mm256_and_ps(case4, vec_D)\n            )\n        );\n\n        // Store the result back to the output tensor\n        _mm256_storeu_ps(&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    for (size_t i = 0; i < total_elements; i += 8) {\n        // Load 8 elements from each input tensor\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n        __m256 vecC = _mm256_loadu_ps(&C[i]);\n        __m256 vecD = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean conditions from cond1 and cond2\n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean conditions to masks\n        __m256 mask_cond1 = _mm256_cvtepi32_ps(_mm256_and_si256(cond1_vec, _mm256_set1_epi32(0xFF)));\n        __m256 mask_cond2 = _mm256_cvtepi32_ps(_mm256_and_si256(cond2_vec, _mm256_set1_epi32(0xFF)));\n\n        // Compute combined masks for the four cases\n        __m256 mask_case1 = _mm256_and_ps(mask_cond1, mask_cond2); // cond1 && cond2\n        __m256 mask_case2 = _mm256_and_ps(mask_cond1, _mm256_xor_ps(mask_cond2, _mm256_set1_ps(1.0f))); // cond1 && !cond2\n        __m256 mask_case3 = _mm256_and_ps(_mm256_xor_ps(mask_cond1, _mm256_set1_ps(1.0f)), mask_cond2); // !cond1 && cond2\n        __m256 mask_case4 = _mm256_and_ps(_mm256_xor_ps(mask_cond1, _mm256_set1_ps(1.0f)), _mm256_xor_ps(mask_cond2, _mm256_set1_ps(1.0f))); // !cond1 && !cond2\n\n        // Select values based on masks\n        __m256 result = _mm256_or_ps(\n            _mm256_or_ps(\n                _mm256_and_ps(mask_case1, vecA),\n                _mm256_and_ps(mask_case2, vecB)\n            ),\n            _mm256_or_ps(\n                _mm256_and_ps(mask_case3, vecC),\n                _mm256_and_ps(mask_case4, vecD)\n            )\n        );\n\n        // Store the result back to the output tensor\n        _mm256_storeu_ps(&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    for (size_t i = 0; i < total_elements; i += 8) {\n        // Load 8 elements from each input tensor\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n        __m256 vecC = _mm256_loadu_ps(&C[i]);\n        __m256 vecD = _mm256_loadu_ps(&D[i]);\n\n        // Load 8 boolean conditions from cond1 and cond2\n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        // Convert boolean conditions to masks\n        __m256 mask_cond1 = _mm256_cvtepi32_ps(_mm256_and_si256(cond1_vec, _mm256_set1_epi32(0xFF)));\n        __m256 mask_cond2 = _mm256_cvtepi32_ps(_mm256_and_si256(cond2_vec, _mm256_set1_epi32(0xFF)));\n\n        // Compute combined masks for the four cases\n        __m256 mask_case1 = _mm256_and_ps(mask_cond1, mask_cond2); // cond1 && cond2\n        __m256 mask_case2 = _mm256_and_ps(mask_cond1, _mm256_xor_ps(mask_cond2, _mm256_set1_ps(1.0f))); // cond1 && !cond2\n        __m256 mask_case3 = _mm256_and_ps(_mm256_xor_ps(mask_cond1, _mm256_set1_ps(1.0f)), mask_cond2); // !cond1 && cond2\n        __m256 mask_case4 = _mm256_and_ps(_mm256_xor_ps(mask_cond1, _mm256_set1_ps(1.0f)), _mm256_xor_ps(mask_cond2, _mm256_set1_ps(1.0f))); // !cond1 && !cond2\n\n        // Select values based on the masks\n        __m256 result = _mm256_or_ps(\n            _mm256_or_ps(\n                _mm256_and_ps(mask_case1, vecA),\n                _mm256_and_ps(mask_case2, vecB)\n            ),\n            _mm256_or_ps(\n                _mm256_and_ps(mask_case3, vecC),\n                _mm256_and_ps(mask_case4, vecD)\n            )\n        );\n\n        // Store the result back to the output tensor\n        _mm256_storeu_ps(&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= total_elements; i += 8) {\n        // Load 8 uint32_t elements from A into an AVX2 register\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Compute the popcount (number of bits set to 1) for each element\n        // First step: Count bits in 2-bit chunks\n        __m256i count = _mm256_setzero_si256();\n        __m256i mask = _mm256_set1_epi8(0x55);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, mask));\n        vec = _mm256_srli_epi64(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, mask));\n\n        // Second step: Count bits in 4-bit chunks\n        mask = _mm256_set1_epi8(0x33);\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi64(count, 2), mask));\n        count = _mm256_add_epi8(count, _mm256_and_si256(count, mask));\n\n        // Third step: Count bits in 8-bit chunks\n        mask = _mm256_set1_epi8(0x0F);\n        count = _mm256_and_si256(_mm256_add_epi8(count, _mm256_srli_epi64(count, 4)), mask);\n\n        // Horizontal sum across lanes to get final counts\n        __m256i shuffle = _mm256_shuffle_epi32(count, _MM_SHUFFLE(1, 0, 3, 2));\n        count = _mm256_add_epi8(count, shuffle);\n        shuffle = _mm256_shuffle_epi32(count, _MM_SHUFFLE(2, 3, 0, 1));\n        count = _mm256_add_epi8(count, shuffle);\n\n        // Store results as uint8_t in out\n        __m128i result = _mm256_castsi256_si128(count);\n        _mm_storeu_si128((__m128i*)(out + i), result);\n    }\n\n    // Process remaining elements (if any) using scalar code\n    for (; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    // Process 8 elements at a time using AVX2\n    size_t i = 0;\n    for (; i + 8 <= total_elements; i += 8) {\n        // Load 8 uint32_t elements into an AVX register\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Compute the popcount for each 32-bit integer using AVX2 intrinsics\n        // First step: Count bits in 2-bit chunks\n        __m256i count = _mm256_setzero_si256();\n        __m256i mask = _mm256_set1_epi8(0x55);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, mask));\n        vec = _mm256_srli_epi64(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, mask));\n\n        // Second step: Count bits in 4-bit chunks\n        mask = _mm256_set1_epi8(0x33);\n        count = _mm256_add_epi8(count, _mm256_srli_epi64(count, 2));\n        count = _mm256_and_si256(count, mask);\n\n        // Third step: Count bits in 8-bit chunks\n        mask = _mm256_set1_epi8(0x0F);\n        count = _mm256_add_epi8(count, _mm256_srli_epi64(count, 4));\n        count = _mm256_and_si256(count, mask);\n\n        // Sum up the bytes to get the final popcount for each 32-bit integer\n        __m256i sum = _mm256_sad_epu8(count, _mm256_setzero_si256());\n        __m128i low = _mm256_castsi256_si128(sum);\n        __m128i high = _mm256_extracti128_si256(sum, 1);\n        __m128i result = _mm_add_epi32(low, high);\n\n        // Store the results as 8-bit integers in the output array\n        alignas(32) uint32_t temp[8];\n        _mm_storeu_si128((__m128i*)temp, result);\n        for (size_t j = 0; j < 8; ++j) {\n            out[i + j] = static_cast<uint8_t>(temp[j]);\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        out[i] = static_cast<uint8_t>(_mm_popcnt_u32(A[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 8 <= total_elements; i += 8) {\n        // Load 8 uint32_t elements from A into an AVX register\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Compute the popcount (number of bits set to 1) for each element\n        // First step: Count bits in 2-bit chunks\n        __m256i count = _mm256_setzero_si256();\n        __m256i mask = _mm256_set1_epi8(0x55);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, mask));\n        vec = _mm256_srli_epi64(vec, 1);\n        count = _mm256_add_epi8(count, _mm256_and_si256(vec, mask));\n\n        // Second step: Count bits in 4-bit chunks\n        mask = _mm256_set1_epi8(0x33);\n        count = _mm256_add_epi8(count, _mm256_and_si256(_mm256_srli_epi64(count, 2), mask));\n        count = _mm256_add_epi8(count, _mm256_and_si256(count, mask));\n\n        // Third step: Count bits in 8-bit chunks\n        mask = _mm256_set1_epi8(0x0F);\n        count = _mm256_and_si256(_mm256_add_epi8(count, _mm256_srli_epi64(count, 4)), mask);\n\n        // Horizontal sum across 8-bit chunks to get final counts\n        __m256i shuffle = _mm256_shuffle_epi32(count, _MM_SHUFFLE(1, 0, 3, 2));\n        count = _mm256_add_epi8(count, shuffle);\n        shuffle = _mm256_shuffle_epi32(count, _MM_SHUFFLE(2, 3, 0, 1));\n        count = _mm256_add_epi8(count, shuffle);\n\n        // Extract results and store in out array\n        alignas(32) uint8_t temp[32];\n        _mm256_store_si256((__m256i*)temp, count);\n        for (size_t j = 0; j < 8; ++j) {\n            out[i + j] = temp[j];\n        }\n    }\n\n    // Process remaining elements with scalar code\n    for (; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    for (size_t i = 0; i < total_elements; i += 8) {\n        // Load 8 elements from A into an AVX register\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Compute the bit count for each 32-bit integer using AVX2 intrinsics\n        __m256i count = _mm256_setzero_si256();\n\n        // Parallel bit counting using the population count technique\n        for (int shift = 0; shift < 5; ++shift) {\n            __m256i mask = _mm256_set1_epi32((1 << (1 << shift)) - 1);\n            __m256i temp = _mm256_and_si256(vec, mask);\n            count = _mm256_add_epi32(count, temp);\n            vec = _mm256_srli_epi32(vec, (1 << shift));\n        }\n\n        // Extract the results and store them in the output array\n        alignas(32) uint32_t temp[8];\n        _mm256_store_si256((__m256i*)temp, count);\n\n        for (int j = 0; j < 8 && (i + j) < total_elements; ++j) {\n            out[i + j] = static_cast<uint8_t>(temp[j]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n\n    for (size_t i = 0; i < total_elements; i += 8) {\n        // Load 8 elements from A into an AVX register\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Compute the bit count for each 32-bit integer using AVX2 intrinsics\n        __m256i count = _mm256_setzero_si256();\n\n        // Parallel bit count using the population count technique\n        for (int shift = 0; shift < 5; ++shift) {\n            __m256i mask = _mm256_set1_epi32((1 << (1 << shift)) - 1);\n            __m256i temp = _mm256_and_si256(vec, mask);\n            count = _mm256_add_epi32(count, temp);\n            vec = _mm256_srli_epi32(vec, (1 << shift));\n        }\n\n        // Extract the results and store them in the output array\n        alignas(32) uint32_t temp[8];\n        _mm256_store_si256((__m256i*)temp, count);\n\n        for (int j = 0; j < 8 && (i + j) < total_elements; ++j) {\n            out[i + j] = static_cast<uint8_t>(temp[j]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i = 0;\n\n    __m256i div_by_3 = _mm256_set1_epi32(3); // Vector with all elements set to 3\n    __m256i zero = _mm256_setzero_si256();   // Vector of zeros for absolute value calculation\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i)); // Load 8 integers from A\n        __m256i remainder = _mm256_rem_epi32(vecA, div_by_3);       // Compute A % 3\n        __m256i mask = _mm256_cmpeq_epi32(remainder, zero);         // Mask where A % 3 == 0\n        __m256i absA = _mm256_abs_epi32(vecA);                      // Compute absolute value of A\n        __m256i result = _mm256_blendv_epi8(vecA, absA, mask);      // Blend based on mask\n        _mm256_storeu_si256((__m256i*)(B + i), result);             // Store result into B\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = (A[i] < 0) ? -A[i] : A[i];\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from A into an AVX register\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Create a vector of 3s for the divisibility check\n        __m256i vec3 = _mm256_set1_epi32(3);\n\n        // Compute the remainder of each element when divided by 3\n        __m256i remainder = _mm256_rem_epi32(vecA, vec3);\n\n        // Check if the remainder is zero (divisible by 3)\n        __m256i mask = _mm256_cmpeq_epi32(remainder, _mm256_setzero_si256());\n\n        // Compute the absolute value of the elements in vecA\n        __m256i absA = _mm256_abs_epi32(vecA);\n\n        // Blend the original values and absolute values based on the mask\n        __m256i result = _mm256_blendv_epi8(vecA, absA, mask);\n\n        // Store the result into B\n        _mm256_storeu_si256((__m256i*)(B + i), result);\n    }\n\n    // Process remaining elements (if any) using scalar code\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    __m256i vec_3 = _mm256_set1_epi32(3); // Vector with all elements set to 3\n    __m256i vec_zero = _mm256_setzero_si256(); // Vector of zeros\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + simd_width <= cols; j += simd_width) {\n            // Load 8 integers from A into a vector\n            __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i * cols + j));\n\n            // Compute the remainder when dividing by 3\n            __m256i vec_mod = _mm256_rem_epi32(vec_a, vec_3);\n\n            // Check if the remainder is zero (divisible by 3)\n            __m256i vec_mask = _mm256_cmpeq_epi32(vec_mod, vec_zero);\n\n            // Compute the absolute value of vec_a\n            __m256i vec_neg_a = _mm256_sub_epi32(vec_zero, vec_a);\n            __m256i vec_abs_a = _mm256_blendv_epi8(vec_a, vec_neg_a, _mm256_cmpgt_epi32(vec_zero, vec_a));\n\n            // Blend: if divisible by 3, take abs value; otherwise, keep original\n            __m256i vec_result = _mm256_blendv_epi8(vec_a, vec_abs_a, vec_mask);\n\n            // Store the result into B\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), vec_result);\n        }\n\n        // Handle remaining elements that don't fit into a full SIMD vector\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            if (val % 3 == 0) {\n                B[i * cols + j] = (val < 0) ? -val : val;\n            } else {\n                B[i * cols + j] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i = 0;\n\n    __m256i div_by_3 = _mm256_set1_epi32(3); // Vector with all elements set to 3\n    __m256i zero = _mm256_setzero_si256();   // Vector of zeros\n\n    // Process 8 elements at a time\n    for (; i + 7 < size; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i)); // Load 8 integers from A\n\n        // Compute the remainder when dividing by 3 (vecA % 3)\n        __m256i rem = _mm256_rem_epi32(vecA, div_by_3);\n\n        // Check if the remainder is zero (i.e., divisible by 3)\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n\n        // Compute the absolute value of vecA\n        __m256i absA = _mm256_abs_epi32(vecA);\n\n        // Blend: if divisible by 3, take absA; otherwise, keep vecA\n        __m256i result = _mm256_blendv_epi8(vecA, absA, mask);\n\n        // Store the result into B\n        _mm256_storeu_si256((__m256i*)(B + i), result);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t simd_width = 8; // AVX2 processes 8 integers (32-bit) at a time\n    __m256i div_by_3 = _mm256_set1_epi32(3); // Vector with all elements set to 3\n    __m256i zero = _mm256_setzero_si256();   // Vector of zeros\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t j = 0;\n        // Process 8 elements at a time using AVX2\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i * cols + j)); // Load 8 elements from A\n            __m256i rem = _mm256_rem_epi32(vec_a, div_by_3); // Compute remainder when divided by 3\n            __m256i mask = _mm256_cmpeq_epi32(rem, zero);    // Create mask where remainder is zero\n            __m256i abs_a = _mm256_abs_epi32(vec_a);         // Compute absolute value of vec_a\n            __m256i result = _mm256_blendv_epi8(vec_a, abs_a, mask); // Blend based on mask\n            _mm256_storeu_si256((__m256i*)(B + i * cols + j), result); // Store result in B\n        }\n        // Process remaining elements (if any) using scalar code\n        for (; j < cols; ++j) {\n            int val = A[i * cols + j];\n            if (val % 3 == 0) {\n                B[i * cols + j] = (val < 0) ? -val : val;\n            } else {\n                B[i * cols + j] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    const __m256i low4_mask = _mm256_set1_epi32(0xF); // Mask for low 4 bits\n    const __m256i ones = _mm256_set1_epi32(0xF);      // All ones in low 4 bits\n    const __m256i rotate_mask = _mm256_set1_epi32(rotate_bits);\n    const __m256i neg_one = _mm256_set1_epi32(~0);    // Bitwise negation mask\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 elements from src into a SIMD register\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Extract the low 4 bits of each element\n        __m256i low4_bits = _mm256_and_si256(src_vec, low4_mask);\n\n        // Compare if the low 4 bits are all 1s\n        __m256i condition = _mm256_cmpeq_epi32(low4_bits, ones);\n\n        // Perform bitwise negation of src\n        __m256i negated = _mm256_xor_si256(src_vec, neg_one);\n\n        // Perform rotation by rotate_bits\n        __m256i rotated_left = _mm256_sllv_epi32(src_vec, rotate_mask);\n        __m256i rotated_right = _mm256_srlv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask));\n        __m256i rotated = _mm256_or_si256(rotated_left, rotated_right);\n\n        // Blend based on the condition: if true, use rotated; otherwise, use negated\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF); // Mask to extract low 4 bits\n    __m256i ones = _mm256_set1_epi32(0xF);      // All ones for comparison\n    __m256i rotate_mask = _mm256_set1_epi32((1 << rotate_bits) - 1);\n    __m256i rotate_shift = _mm256_set1_epi32(rotate_bits);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Extract low 4 bits and compare with 0xF\n        __m256i low4 = _mm256_and_si256(src_vec, mask_low4);\n        __m256i cmp = _mm256_cmpeq_epi32(low4, ones);\n\n        // Compute bitwise negation of src\n        __m256i neg_src = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        // Perform rotation\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srli_epi32(src_vec, rotate_bits),\n            _mm256_slli_epi32(src_vec, 32 - rotate_bits)\n        );\n\n        // Blend results based on condition\n        __m256i result = _mm256_blendv_epi8(neg_src, rotated, cmp);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF); // Mask to extract low 4 bits\n    __m256i ones = _mm256_set1_epi32(0xF);      // All 1s in low 4 bits\n    __m256i rotate_mask = _mm256_set1_epi32(rotate_bits);\n    __m256i neg_one = _mm256_set1_epi32(~0);    // All bits set to 1 for bitwise negation\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 elements from src\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n\n        // Extract low 4 bits of each element\n        __m256i low4_bits = _mm256_and_si256(src_vec, mask_low4);\n\n        // Compare if low 4 bits are all 1s\n        __m256i condition = _mm256_cmpeq_epi32(low4_bits, ones);\n\n        // Perform bitwise negation for elements not satisfying the condition\n        __m256i negated = _mm256_xor_si256(src_vec, neg_one);\n\n        // Perform rotation for elements satisfying the condition\n        __m256i rotated_left = _mm256_sllv_epi32(src_vec, rotate_mask);\n        __m256i rotated_right = _mm256_srlv_epi32(src_vec, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_mask));\n        __m256i rotated = _mm256_or_si256(rotated_left, rotated_right);\n\n        // Blend results based on the condition\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask_low4 = _mm256_set1_epi32(0xF); // Mask to extract low 4 bits\n    __m256i ones = _mm256_set1_epi32(0xF);      // All ones for comparison\n    __m256i rotate_mask = _mm256_set1_epi32((1 << rotate_bits) - 1);\n    __m256i rotate_shift = _mm256_set1_epi32(rotate_bits);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Extract low 4 bits and compare with 0xF\n        __m256i low4_bits = _mm256_and_si256(src_vec, mask_low4);\n        __m256i condition = _mm256_cmpeq_epi32(low4_bits, ones);\n\n        // Compute bitwise negation of src\n        __m256i negated = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n\n        // Perform rotation\n        __m256i rotated_right = _mm256_srli_epi32(src_vec, rotate_bits);\n        __m256i rotated_left = _mm256_slli_epi32(src_vec, 32 - rotate_bits);\n        __m256i rotated = _mm256_or_si256(rotated_right, rotated_left);\n\n        // Blend based on condition\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i rotate_mask = _mm256_set1_epi32(0xF); // Mask for low 4 bits\n    __m256i ones = _mm256_set1_epi32(0xF);       // All ones for comparison\n    __m256i rotate_bits_vec = _mm256_set1_epi32(rotate_bits);\n    \n    for (size_t i = 0; i < length; i += 8) {\n        // Load 8 elements from src\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Extract low 4 bits of each element\n        __m256i low_4_bits = _mm256_and_si256(src_vec, rotate_mask);\n        \n        // Compare if low 4 bits are all 1s\n        __m256i condition = _mm256_cmpeq_epi32(low_4_bits, ones);\n        \n        // Compute bitwise negation of src\n        __m256i negated_src = _mm256_xor_si256(src_vec, _mm256_set1_epi32(0xFFFFFFFF));\n        \n        // Perform rotation for elements where condition is true\n        __m256i rotated_src = _mm256_or_si256(\n            _mm256_srli_epi32(src_vec, rotate_bits),\n            _mm256_slli_epi32(src_vec, 32 - rotate_bits)\n        );\n        \n        // Blend results based on condition\n        __m256i result = _mm256_blendv_epi8(negated_src, rotated_src, condition);\n        \n        // Store the result into dst\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps_vec = _mm256_set1_ps(epsilon);\n    __m256 neg_eps_vec = _mm256_set1_ps(-epsilon);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff_vec, eps_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(diff_vec, neg_eps_vec, _CMP_LT_OS);\n\n        __m256i result_gt = _mm256_castps_si256(cmp_gt);\n        __m256i result_lt = _mm256_castps_si256(cmp_lt);\n\n        __m256i result = _mm256_setzero_si256();\n        result = _mm256_sub_epi32(result, result_lt); // -1 where diff < -epsilon\n        result = _mm256_add_epi32(result, result_gt); // +1 where diff > epsilon\n\n        __m256i packed_result = _mm256_packs_epi32(result, result);\n        packed_result = _mm256_packs_epi16(packed_result, packed_result);\n\n        _mm_storeu_si64(&out[i], _mm256_castsi256_si128(packed_result));\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_neg_epsilon = _mm256_set1_ps(-epsilon);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_neg_one = _mm256_set1_epi32(-1);\n    __m256i v_zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 v_a = _mm256_loadu_ps(&A[i]);\n        __m256 v_b = _mm256_loadu_ps(&B[i]);\n        __m256 v_diff = _mm256_sub_ps(v_a, v_b);\n\n        __m256 v_cmp_gt = _mm256_cmp_ps(v_diff, v_epsilon, _CMP_GT_OS);\n        __m256 v_cmp_lt = _mm256_cmp_ps(v_diff, v_neg_epsilon, _CMP_LT_OS);\n\n        __m256i v_mask_gt = _mm256_castps_si256(v_cmp_gt);\n        __m256i v_mask_lt = _mm256_castps_si256(v_cmp_lt);\n\n        __m256i v_result = _mm256_blendv_epi8(v_zero, v_one, v_mask_gt);\n        v_result = _mm256_blendv_epi8(v_result, v_neg_one, v_mask_lt);\n\n        // Pack results into int8_t\n        __m256i v_packed = _mm256_packs_epi32(v_result, v_zero);\n        v_packed = _mm256_packs_epi16(v_packed, v_zero);\n        _mm256_storeu_si256((__m256i*)&out[i], v_packed);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_neg_epsilon = _mm256_set1_ps(-epsilon);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_zero = _mm256_set1_epi32(0);\n    __m256i v_neg_one = _mm256_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 v_a = _mm256_loadu_ps(&A[i]);\n        __m256 v_b = _mm256_loadu_ps(&B[i]);\n        __m256 v_diff = _mm256_sub_ps(v_a, v_b);\n\n        __m256 v_cmp_gt = _mm256_cmp_ps(v_diff, v_epsilon, _CMP_GT_OS);\n        __m256 v_cmp_lt = _mm256_cmp_ps(v_diff, v_neg_epsilon, _CMP_LT_OS);\n\n        __m256i v_mask_gt = _mm256_castps_si256(v_cmp_gt);\n        __m256i v_mask_lt = _mm256_castps_si256(v_cmp_lt);\n\n        __m256i v_result_gt = _mm256_and_si256(v_mask_gt, v_one);\n        __m256i v_result_lt = _mm256_and_si256(v_mask_lt, v_neg_one);\n\n        __m256i v_result = _mm256_or_si256(v_result_gt, v_result_lt);\n        __m256i v_final = _mm256_blendv_epi8(v_zero, v_result, _mm256_or_si256(v_mask_gt, v_mask_lt));\n\n        // Convert to int8_t and store\n        __m256i v_packed = _mm256_packs_epi32(v_final, v_final);\n        v_packed = _mm256_permute4x64_epi64(v_packed, 0xD8); // Reorder lanes\n        __m128i v_out = _mm256_castsi256_si128(v_packed);\n        _mm_storeu_si128((__m128i*)&out[i], v_out);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 eps_vec = _mm256_set1_ps(epsilon);\n    __m256 neg_eps_vec = _mm256_set1_ps(-epsilon);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n\n        __m256 cmp_gt = _mm256_cmp_ps(diff_vec, eps_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(diff_vec, neg_eps_vec, _CMP_LT_OS);\n\n        __m256i result_gt = _mm256_castps_si256(cmp_gt);\n        __m256i result_lt = _mm256_castps_si256(cmp_lt);\n\n        __m256i result = _mm256_blendv_epi8(_mm256_set1_epi8(0), _mm256_set1_epi8(1), result_gt);\n        result = _mm256_blendv_epi8(result, _mm256_set1_epi8(-1), result_lt);\n\n        __m256i packed_result = _mm256_packs_epi32(result, result);\n        packed_result = _mm256_packs_epi16(packed_result, packed_result);\n\n        _mm_storeu_si64(&out[i], _mm256_castsi256_si128(packed_result));\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 v_epsilon = _mm256_set1_ps(epsilon);\n    __m256 v_neg_epsilon = _mm256_set1_ps(-epsilon);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_neg_one = _mm256_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 v_a = _mm256_loadu_ps(&A[i]);\n        __m256 v_b = _mm256_loadu_ps(&B[i]);\n        __m256 v_diff = _mm256_sub_ps(v_a, v_b);\n\n        __m256 v_cmp_gt = _mm256_cmp_ps(v_diff, v_epsilon, _CMP_GT_OS);\n        __m256 v_cmp_lt = _mm256_cmp_ps(v_diff, v_neg_epsilon, _CMP_LT_OS);\n\n        __m256i v_mask_gt = _mm256_castps_si256(v_cmp_gt);\n        __m256i v_mask_lt = _mm256_castps_si256(v_cmp_lt);\n\n        __m256i v_result_gt = _mm256_and_si256(v_mask_gt, v_one);\n        __m256i v_result_lt = _mm256_and_si256(v_mask_lt, v_neg_one);\n\n        __m256i v_result = _mm256_or_si256(v_result_gt, v_result_lt);\n        __m256i v_result_final = _mm256_blendv_epi8(v_zero, v_result, _mm256_or_si256(v_mask_gt, v_mask_lt));\n\n        // Pack results to int8_t\n        __m128i v_packed_low = _mm_packs_epi32(_mm256_castsi256_si128(v_result_final), _mm256_extracti128_si256(v_result_final, 1));\n        __m128i v_packed = _mm_packs_epi16(v_packed_low, v_packed_low);\n\n        _mm_storeu_si128((__m128i*)&out[i], v_packed);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > epsilon) {\n            out[i] = 1;\n        } else if (diff < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        // Clamp values between INT32_MIN and INT32_MAX\n        a = _mm256_max_ps(a, min_val);\n        a = _mm256_min_ps(a, max_val);\n\n        // Convert floats to integers with saturation\n        __m256i b = _mm256_cvtps_epi32(a);\n\n        // Store the result into B\n        _mm256_storeu_si256((__m256i*)&B[i], b);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_float = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A\n        __m256 float_vals = _mm256_loadu_ps(&A[i]);\n\n        // Clamp values between INT32_MIN and INT32_MAX\n        float_vals = _mm256_max_ps(float_vals, min_float);\n        float_vals = _mm256_min_ps(float_vals, max_float);\n\n        // Convert floats to integers with saturation\n        __m256i int_vals = _mm256_cvtps_epi32(float_vals);\n\n        // Store the result in B\n        _mm256_storeu_si256((__m256i*)&B[i], int_vals);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_float = _mm256_set1_ps((float)INT32_MIN);\n    const __m256i max_int = _mm256_set1_epi32(INT32_MAX);\n    const __m256i min_int = _mm256_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        // Clamp values between INT32_MIN and INT32_MAX\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(a, min_float), max_float);\n\n        // Convert floats to integers with truncation\n        __m256i converted = _mm256_cvtps_epi32(clamped);\n\n        // Store the result into B\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        // Clamp values between INT32_MIN and INT32_MAX\n        a = _mm256_max_ps(a, min_val);\n        a = _mm256_min_ps(a, max_val);\n\n        // Convert floats to integers with saturation\n        __m256i converted = _mm256_cvtps_epi32(a);\n\n        // Store the result into B\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_float = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 min_float = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A\n        __m256 a = _mm256_loadu_ps(&A[i]);\n\n        // Clamp values to [INT32_MIN, INT32_MAX]\n        a = _mm256_min_ps(a, max_float);\n        a = _mm256_max_ps(a, min_float);\n\n        // Convert floats to integers with saturation\n        __m256i converted = _mm256_cvtps_epi32(a);\n\n        // Store the result into B\n        _mm256_storeu_si256((__m256i*)&B[i], converted);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > (float)INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < (float)INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps(static_cast<float>(scale));\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(&B[i], float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * static_cast<float>(scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps(static_cast<float>(scale));\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(&B[i], float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * static_cast<float>(scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps(static_cast<float>(scale));\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(&B[i], float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * static_cast<float>(scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps(static_cast<float>(scale));\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(&B[i], float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * static_cast<float>(scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_cvtepi32_ps(int_vec);\n        float_vec = _mm256_mul_ps(float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)A[i] * (float)scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF); // Mask to clear the sign bit for absolute value comparison\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A and B\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m256 vdiv = _mm256_div_ps(va, vb);\n\n        // Round to nearest integer (convert PS to PI)\n        __m256i vround = _mm256_cvtps_epi32(vdiv);\n\n        // Store the result into C\n        _mm256_storeu_si256((__m256i*)&C[i], vround);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = (int32_t)lrintf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256 ones = _mm256_set1_ps(1.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A and B\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        // Perform division: va / vb\n        __m256 vdiv = _mm256_div_ps(va, vb);\n\n        // Add small bias to ensure correct rounding to nearest integer\n        __m256 biased = _mm256_add_ps(vdiv, _mm256_set1_ps(0.5f));\n\n        // Convert the result to integers using truncation\n        __m256i vrounded = _mm256_cvtps_epi32(biased);\n\n        // Store the resulting 8 integers into C\n        _mm256_storeu_si256((__m256i*)&C[i], vrounded);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256 ones = _mm256_set1_ps(1.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A and B\n        __m256 vecA = _mm256_loadu_ps(&A[i]);\n        __m256 vecB = _mm256_loadu_ps(&B[i]);\n\n        // Perform division A / B\n        __m256 divResult = _mm256_div_ps(vecA, vecB);\n\n        // Add 0.5 for rounding to nearest integer\n        __m256 rounded = _mm256_add_ps(divResult, ones);\n        rounded = _mm256_round_ps(rounded, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert the result to 32-bit integers\n        __m256i intResult = _mm256_cvtps_epi32(rounded);\n\n        // Store the result in the destination array C\n        _mm256_storeu_si256((__m256i*)&C[i], intResult);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF); // Mask to clear the sign bit for absolute value calculation\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A and B\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        // Perform element-wise division\n        __m256 vdiv = _mm256_div_ps(va, vb);\n\n        // Round to nearest integer (convert PS to PI with rounding)\n        __m256i vrounded = _mm256_cvtps_epi32(vdiv);\n\n        // Store the result into the destination array\n        _mm256_storeu_si256((__m256i*)&C[i], vrounded);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        C[i] = (int32_t)lrintf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256 ones = _mm256_set1_ps(1.0f);\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from A and B\n        __m256 va = _mm256_loadu_ps(&A[i]);\n        __m256 vb = _mm256_loadu_ps(&B[i]);\n\n        // Perform element-wise division: A / B\n        __m256 vdiv = _mm256_div_ps(va, vb);\n\n        // Add 0.5 for rounding to nearest integer\n        __m256 vrounded = _mm256_add_ps(vdiv, ones);\n        vrounded = _mm256_round_ps(vrounded, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        // Convert the result to 32-bit integers\n        __m256i vint = _mm256_cvtps_epi32(vrounded);\n\n        // Store the result in the destination array\n        _mm256_storeu_si256((__m256i*)&C[i], vint);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        C[i] = (int32_t)lrintf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d vmax = _mm256_set1_pd(FLT_MAX);\n    const __m256d vmin = _mm256_set1_pd(-FLT_MAX);\n    const __m256 vflt_max = _mm256_set1_ps(FLT_MAX);\n    const __m256 vflt_min = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 doubles from A\n        __m256d va = _mm256_loadu_pd(&A[i]);\n\n        // Clamp values between -FLT_MAX and FLT_MAX\n        __m256d clamped = _mm256_min_pd(_mm256_max_pd(va, vmin), vmax);\n\n        // Convert clamped doubles to floats\n        __m128 vf_lo = _mm256_cvtpd_ps(_mm256_castpd256_pd128(clamped));\n        __m128 vf_hi = _mm256_cvtpd_ps(_mm256_extractf128_pd(clamped, 1));\n\n        // Combine the two halves into a single 256-bit vector\n        __m256 vf = _mm256_insertf128_ps(_mm256_castps128_ps256(vf_lo), vf_hi, 1);\n\n        // Store 4 floats into B\n        _mm256_storeu_ps(&B[i], vf);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d vmax = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d vmin = _mm256_set1_pd((double)-FLT_MAX);\n    const __m256 vflt_max = _mm256_set1_ps(FLT_MAX);\n    const __m256 vflt_min = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 doubles from A\n        __m256d vd = _mm256_loadu_pd(&A[i]);\n\n        // Clamp values between -FLT_MAX and FLT_MAX\n        __m256d clamped = _mm256_min_pd(_mm256_max_pd(vd, vmin), vmax);\n\n        // Convert clamped doubles to floats\n        __m128 vf_lo = _mm256_cvtpd_ps(_mm256_castpd256_pd128(clamped));\n        __m128 vf_hi = _mm256_cvtpd_ps(_mm256_extractf128_pd(clamped, 1));\n\n        // Combine the two 128-bit results into one 256-bit register\n        __m256 vf = _mm256_insertf128_ps(_mm256_castps128_ps256(vf_lo), vf_hi, 1);\n\n        // Store 4 floats into B\n        _mm256_storeu_ps(&B[i], vf);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d vmax = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d vmin = _mm256_set1_pd((double)-FLT_MAX);\n    const __m256 vflt_max = _mm256_set1_ps(FLT_MAX);\n    const __m256 vflt_min = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 doubles from A\n        __m256d vd = _mm256_loadu_pd(&A[i]);\n\n        // Clamp values between -FLT_MAX and FLT_MAX\n        vd = _mm256_min_pd(vd, vmax);\n        vd = _mm256_max_pd(vd, vmin);\n\n        // Convert clamped doubles to floats\n        __m128 vf_lo = _mm256_cvtpd_ps(_mm256_castpd256_pd128(vd));\n        __m128 vf_hi = _mm256_cvtpd_ps(_mm256_extractf128_pd(vd, 1));\n\n        // Combine the two halves into a single 256-bit vector\n        __m256 vf = _mm256_insertf128_ps(_mm256_castps128_ps256(vf_lo), vf_hi, 1);\n\n        // Store 4 floats into B\n        _mm256_storeu_ps(&B[i], vf);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d vmax = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d vmin = _mm256_set1_pd((double)-FLT_MAX);\n    const __m256 vflt_max = _mm256_set1_ps(FLT_MAX);\n    const __m256 vflt_min = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 doubles from A\n        __m256d vd = _mm256_loadu_pd(&A[i]);\n\n        // Clamp values between -FLT_MAX and FLT_MAX\n        __m256d clamped = _mm256_min_pd(_mm256_max_pd(vd, vmin), vmax);\n\n        // Convert clamped doubles to floats\n        __m128 vf0 = _mm256_cvtpd_ps(_mm256_castpd256_pd128(clamped));\n        __m128 vf1 = _mm256_cvtpd_ps(_mm256_extractf128_pd(clamped, 1));\n\n        // Combine the two __m128 results into one __m256\n        __m256 vf = _mm256_insertf128_ps(_mm256_castps128_ps256(vf0), vf1, 1);\n\n        // Store 4 floats into B\n        _mm256_storeu_ps(&B[i], vf);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d vmax = _mm256_set1_pd(FLT_MAX);\n    const __m256d vmin = _mm256_set1_pd(-FLT_MAX);\n    const __m256 vflt_max = _mm256_set1_ps(FLT_MAX);\n    const __m256 vflt_min = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        // Load 4 doubles from A\n        __m256d vda = _mm256_loadu_pd(&A[i]);\n\n        // Clamp values between -FLT_MAX and FLT_MAX\n        __m256d clamped = _mm256_min_pd(_mm256_max_pd(vda, vmin), vmax);\n\n        // Convert clamped doubles to floats\n        __m128 vf0 = _mm256_cvtpd_ps(_mm256_castpd256_pd128(clamped));\n        __m128 vf1 = _mm256_cvtpd_ps(_mm256_extractf128_pd(clamped, 1));\n\n        // Combine the two __m128 results into one __m256\n        __m256 vresult = _mm256_insertf128_ps(_mm256_castps128_ps256(vf0), vf1, 1);\n\n        // Store 4 floats into B\n        _mm256_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= size; i += 16) {\n        // Load 16 int16_t values from A\n        __m256i vec16 = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Convert the first 8 int16_t to int32_t\n        __m256i vec32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec16));\n\n        // Convert the next 8 int16_t to int32_t\n        __m256i vec32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec16, 1));\n\n        // Store the 8 int32_t results to B\n        _mm256_storeu_si256((__m256i*)(B + i), vec32_low);\n\n        // Store the next 8 int32_t results to B\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec32_high);\n    }\n\n    // Process remaining elements (if any) using scalar code\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= size; i += 16) {\n        // Load 16 int16_t values from A\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Convert the 16 int16_t values to 32-bit integers (unpack and sign-extend)\n        __m256i low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec));\n        __m256i high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec, 1));\n\n        // Store the resulting 32-bit integers into B\n        _mm256_storeu_si256((__m256i*)(B + i), low);\n        _mm256_storeu_si256((__m256i*)(B + i + 8), high);\n    }\n\n    // Handle any remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= size; i += 16) {\n        // Load 16 int16_t values from A\n        __m256i vec16 = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Convert the first 8 int16_t to int32_t\n        __m256i vec32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec16));\n\n        // Convert the next 8 int16_t to int32_t\n        __m256i vec32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec16, 1));\n\n        // Store the 8 int32_t values from vec32_low into B\n        _mm256_storeu_si256((__m256i*)(B + i), vec32_low);\n\n        // Store the 8 int32_t values from vec32_high into B\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec32_high);\n    }\n\n    // Handle any remaining elements\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= size; i += 16) {\n        // Load 16 int16_t values from A\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Convert the first 8 int16_t values to int32_t\n        __m256i low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec));\n\n        // Convert the next 8 int16_t values to int32_t\n        __m256i high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec, 1));\n\n        // Store the 8 int32_t values from low into B\n        _mm256_storeu_si256((__m256i*)(B + i), low);\n\n        // Store the 8 int32_t values from high into B\n        _mm256_storeu_si256((__m256i*)(B + i + 8), high);\n    }\n\n    // Process remaining elements (if any) using scalar code\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    \n    // Process 16 elements at a time using AVX2\n    for (; i + 16 <= size; i += 16) {\n        // Load 16 int16_t values from A\n        __m256i vec16 = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        // Convert the first 8 int16_t to int32_t\n        __m256i vec32_low = _mm256_cvtepi16_epi32(_mm256_castsi256_si128(vec16));\n        \n        // Convert the next 8 int16_t to int32_t\n        __m256i vec32_high = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(vec16, 1));\n        \n        // Store the 8 int32_t results from the low part\n        _mm256_storeu_si256((__m256i*)(B + i), vec32_low);\n        \n        // Store the 8 int32_t results from the high part\n        _mm256_storeu_si256((__m256i*)(B + i + 8), vec32_high);\n    }\n    \n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        // Load 8 floats from A and control arrays\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Create mask for control > 0\n        __m256i control_gt_zero_mask = _mm256_cmpgt_epi32(control_vec, _mm256_setzero_si256());\n        __m256 control_gt_zero_mask_ps = _mm256_castsi256_ps(control_gt_zero_mask);\n\n        // Normalize where control > 0\n        __m256 normalized = _mm256_sub_ps(a_vec, min_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n\n        // Clamp normalized values to [0, 1]\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        // Blend normalized values with original A values based on control mask\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, control_gt_zero_mask_ps);\n\n        // Store the result into B\n        _mm256_storeu_ps(&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i + 8 <= size; i += 8) {\n        // Load 8 floats from A and control arrays\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Create mask for control > 0\n        __m256i control_gt_zero_mask = _mm256_cmpgt_epi32(control_vec, _mm256_setzero_si256());\n        __m256 control_gt_zero_mask_ps = _mm256_castsi256_ps(control_gt_zero_mask);\n\n        // Normalize where control > 0\n        __m256 normalized = _mm256_sub_ps(a_vec, min_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n\n        // Clamp normalized values to [0, 1]\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        // Blend: use normalized if control > 0, otherwise use original A values\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, control_gt_zero_mask_ps);\n\n        // Store the result into B\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size / 8) * 8; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            val = (val < 0.0f) ? 0.0f : ((val > 1.0f) ? 1.0f : val);\n            B[i] = val;\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        // Load 8 floats from A and control arrays\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i ctrl_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Create mask for control > 0\n        __m256 ctrl_gt_zero_mask = _mm256_cmp_ps(_mm256_cvtepi32_ps(ctrl_vec), zero_vec, _CMP_GT_OQ);\n\n        // Normalize: (A[i] - min_val) / (max_val - min_val)\n        __m256 norm_vec = _mm256_sub_ps(a_vec, min_vec);\n        norm_vec = _mm256_div_ps(norm_vec, range_vec);\n\n        // Clamp normalized values between 0 and 1\n        norm_vec = _mm256_max_ps(norm_vec, zero_vec);\n        norm_vec = _mm256_min_ps(norm_vec, one_vec);\n\n        // Blend based on control > 0 condition\n        __m256 result_vec = _mm256_blendv_ps(a_vec, norm_vec, ctrl_gt_zero_mask);\n\n        // Store the result into B\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        // Load 8 floats from A and control arrays\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i ctrl_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Create mask for control > 0\n        __m256 ctrl_gt_zero_mask = _mm256_castsi256_ps(_mm256_cmpgt_epi32(ctrl_vec, _mm256_setzero_si256()));\n\n        // Normalize: (A[i] - min_val) / (max_val - min_val)\n        __m256 norm_vec = _mm256_sub_ps(a_vec, min_vec);\n        norm_vec = _mm256_div_ps(norm_vec, range_vec);\n\n        // Clamp values to [0, 1]\n        norm_vec = _mm256_max_ps(norm_vec, zero_vec);\n        norm_vec = _mm256_min_ps(norm_vec, one_vec);\n\n        // Blend based on control > 0\n        __m256 result_vec = _mm256_blendv_ps(a_vec, norm_vec, ctrl_gt_zero_mask);\n\n        // Store the result into B\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_vec = _mm256_set1_ps(min_val);\n    __m256 max_vec = _mm256_set1_ps(max_val);\n    __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 8) {\n        // Load 8 floats from A and control\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i control_vec = _mm256_loadu_si256((__m256i*)&control[i]);\n\n        // Create mask for control > 0\n        __m256i zero_control = _mm256_setzero_si256();\n        __m256i cmp_mask = _mm256_cmpgt_epi32(control_vec, zero_control);\n        __m256 cmp_mask_f = _mm256_castsi256_ps(cmp_mask);\n\n        // Normalize where control > 0\n        __m256 norm_val = _mm256_sub_ps(a_vec, min_vec);\n        norm_val = _mm256_div_ps(norm_val, range_vec);\n\n        // Clamp normalized values to [0, 1]\n        norm_val = _mm256_max_ps(norm_val, zero_vec);\n        norm_val = _mm256_min_ps(norm_val, one_vec);\n\n        // Blend based on control > 0\n        __m256 result = _mm256_blendv_ps(a_vec, norm_val, cmp_mask_f);\n\n        // Store the result into B\n        _mm256_storeu_ps(&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xFFFF0000); // Mask to extract high 16 bits\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        // Extract the high 16 bits of each uint32_t\n        __m256i highBits = _mm256_and_si256(vecA, mask);\n        __m256i shifted = _mm256_srli_epi32(highBits, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVec = _mm256_cvtepi32_ps(shifted);\n\n        // Compute the square root of the float values\n        __m256 sqrtVec = _mm256_sqrt_ps(floatVec);\n\n        // Store the result in B\n        _mm256_storeu_ps(B + i, sqrtVec);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        uint32_t highBits = (A[i] >> 16) & 0xFFFF;\n        float floatValue = (float)highBits;\n        B[i] = sqrtf(floatValue);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xFFFF0000); // Mask to extract high 16 bits\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        // Extract the high 16 bits of each uint32_t\n        __m256i highBits = _mm256_and_si256(vecA, mask);\n        __m256i shifted = _mm256_srli_epi32(highBits, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVec = _mm256_cvtepi32_ps(shifted);\n\n        // Compute the square root of the float values\n        __m256 sqrtVec = _mm256_sqrt_ps(floatVec);\n\n        // Store the result in B\n        _mm256_storeu_ps(B + i, sqrtVec);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        uint32_t highBits = (A[i] >> 16) & 0xFFFF;\n        float floatValue = (float)highBits;\n        B[i] = sqrtf(floatValue);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xFFFF0000); // Mask to extract high 16 bits\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        // Extract the high 16 bits of each uint32_t\n        __m256i highBits = _mm256_and_si256(vecA, mask);\n        __m256i shifted = _mm256_srli_epi32(highBits, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVec = _mm256_cvtepi32_ps(shifted);\n\n        // Compute the square root of the float values\n        __m256 sqrtVec = _mm256_sqrt_ps(floatVec);\n\n        // Store the result in B\n        _mm256_storeu_ps(B + i, sqrtVec);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        uint32_t highBits = (A[i] >> 16) & 0xFFFF;\n        float floatValue = (float)highBits;\n        B[i] = sqrtf(floatValue);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xFFFF0000); // Mask to extract high 16 bits\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        // Extract the high 16 bits of each uint32_t\n        __m256i highBits = _mm256_and_si256(vecA, mask);\n        __m256i shifted = _mm256_srli_epi32(highBits, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVec = _mm256_cvtepi32_ps(shifted);\n\n        // Compute the square root of the float values\n        __m256 sqrtVec = _mm256_sqrt_ps(floatVec);\n\n        // Store the result in B\n        _mm256_storeu_ps(B + i, sqrtVec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        uint32_t highBits = (A[i] >> 16) & 0xFFFF;\n        float floatValue = (float)highBits;\n        B[i] = sqrtf(floatValue);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xFFFF0000); // Mask to extract high 16 bits\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i vecA = _mm256_loadu_si256((__m256i const*)(A + i));\n        \n        // Extract the high 16 bits of each uint32_t\n        __m256i highBits = _mm256_and_si256(vecA, mask);\n        __m256i shifted = _mm256_srli_epi32(highBits, 16);\n\n        // Convert the high 16 bits to float\n        __m256 floatVec = _mm256_cvtepi32_ps(shifted);\n\n        // Compute the square root of the float values\n        __m256 sqrtVec = _mm256_sqrt_ps(floatVec);\n\n        // Store the result in B\n        _mm256_storeu_ps(B + i, sqrtVec);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        uint32_t highBits = (A[i] >> 16) & 0xFFFF;\n        float floatValue = (float)highBits;\n        B[i] = sqrtf(floatValue);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Reinterpret the bit pattern as floats (type punning)\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n\n        // Store the resulting floats into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Reinterpret the bits as floats (no conversion)\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n\n        // Store the resulting floats into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Reinterpret the bits as floats (no conversion)\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n\n        // Store the resulting floats into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Reinterpret the bits as floats (no conversion)\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n\n        // Store the resulting floats into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 uint32_t values from A as integers\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n\n        // Reinterpret the bit pattern of the integers as floats\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n\n        // Store the resulting floats into B\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Process remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        B[i] = *reinterpret_cast<const float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    // Set the first element of diff to 0.0f\n    diff[0] = 0.0f;\n\n    // Process elements in chunks of 8 using AVX\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from A starting at index i and i-1\n        __m256i current = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i previous = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute the difference between consecutive elements\n        __m256i diff_i = _mm256_sub_epi32(current, previous);\n\n        // Convert the differences to float\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n\n        // Load 8 boolean conditions\n        __m256i cond_i = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Create a mask from the boolean conditions\n        __m256 mask = _mm256_castsi256_ps(_mm256_and_si256(cond_i, _mm256_set1_epi32(0xFFFFFFFF)));\n\n        // Blend the results: use diff_f where cond is true, otherwise 0.0f\n        __m256 result = _mm256_blendv_ps(_mm256_setzero_ps(), diff_f, mask);\n\n        // Store the result into the diff array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements (if any) using scalar code\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    // Set the first element of diff to 0.0f as there's no previous element\n    diff[0] = 0.0f;\n\n    // Process elements in chunks of 8 using AVX\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from A starting at index i and i-1\n        __m256i current = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i previous = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute the difference between consecutive elements\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n\n        // Convert the differences to floats\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        // Load 8 boolean conditions\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Create a mask for the condition (true -> 0xFFFFFFFF, false -> 0x0)\n        __m256 mask = _mm256_cmp_ps(_mm256_castsi256_ps(cond_mask), _mm256_setzero_ps(), _CMP_NEQ_UQ);\n\n        // Blend the results: keep diff_float where cond is true, otherwise set to 0.0f\n        __m256 result = _mm256_blendv_ps(_mm256_setzero_ps(), diff_float, mask);\n\n        // Store the result into the diff array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    // Set the first element of diff to 0.0f as there's no previous element\n    diff[0] = 0.0f;\n\n    // Process elements in chunks of 8 using AVX\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from A starting at index i and i-1\n        __m256i current = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i previous = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute the difference between consecutive elements\n        __m256i diff_int = _mm256_sub_epi32(current, previous);\n\n        // Convert the integer differences to floats\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_int);\n\n        // Load 8 boolean conditions\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Create a mask for the condition (true -> 0xFFFFFFFF, false -> 0x0)\n        __m256 mask = _mm256_cmp_ps(_mm256_castsi256_ps(cond_mask), _mm256_setzero_ps(), _CMP_NEQ_UQ);\n\n        // Blend the results: keep diff_float where cond is true, otherwise set to 0.0f\n        __m256 result = _mm256_blendv_ps(_mm256_setzero_ps(), diff_float, mask);\n\n        // Store the result into the diff array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    // Set the first element of diff to 0.0f\n    diff[0] = 0.0f;\n\n    // Process elements in chunks of 8 using AVX\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from A starting at index i and i-1\n        __m256i a_curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i a_prev = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute differences: A[i] - A[i-1]\n        __m256i diffs = _mm256_sub_epi32(a_curr, a_prev);\n\n        // Convert differences to float\n        __m256 diffs_float = _mm256_cvtepi32_ps(diffs);\n\n        // Load 8 boolean conditions\n        __m256i cond_mask = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Create a mask for the condition (true -> 0xFFFFFFFF, false -> 0x0)\n        __m256 mask = _mm256_castsi256_ps(_mm256_and_si256(cond_mask, _mm256_set1_epi32(0xFFFFFFFF)));\n\n        // Blend: Use mask to set diff[i] to 0.0f where cond[i] is false\n        __m256 result = _mm256_blendv_ps(_mm256_setzero_ps(), diffs_float, mask);\n\n        // Store the result into diff array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements (if any) with scalar code\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    // Set the first element of diff to 0.0f\n    diff[0] = 0.0f;\n\n    // Process elements in chunks of 8 using AVX\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from A starting at index i and i-1\n        __m256i vecA_curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecA_prev = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        // Compute the difference between consecutive elements\n        __m256i vecDiff = _mm256_sub_epi32(vecA_curr, vecA_prev);\n\n        // Convert the differences to float\n        __m256 vecDiff_float = _mm256_cvtepi32_ps(vecDiff);\n\n        // Load 8 boolean conditions\n        __m256i vecCond = _mm256_loadu_si256((__m256i*)&cond[i]);\n\n        // Create a mask from the boolean conditions\n        __m256 mask = _mm256_castsi256_ps(_mm256_and_si256(vecCond, _mm256_set1_epi32(0xFFFFFFFF)));\n\n        // Blend the results: use vecDiff_float where cond is true, otherwise 0.0f\n        __m256 result = _mm256_blendv_ps(_mm256_setzero_ps(), vecDiff_float, mask);\n\n        // Store the result into the diff array\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    // Handle remaining elements (if any) using scalar code\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 uint32_t values from src\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Zero-extend to 64-bit integers\n        __m256i low_part = _mm256_cvtepu32_epi64(_mm256_castsi256_si128(src_vec));\n        __m256i high_part = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 1));\n        \n        // Set high 32 bits to 1\n        low_part = _mm256_or_si256(low_part, high_bits);\n        high_part = _mm256_or_si256(high_part, high_bits);\n        \n        // Store results in dst\n        _mm256_storeu_si256((__m256i*)(dst + i), low_part);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high_part);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits_mask = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 packed 32-bit integers from the source array\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Zero extend 32-bit integers to 64-bit integers (unpack)\n        __m256i low_part = _mm256_cvtepu32_epi64(_mm256_castsi256_si128(src_vec));\n        __m256i high_part = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 1));\n\n        // Set high 32 bits of each 64-bit integer to 1\n        low_part = _mm256_or_si256(low_part, high_bits_mask);\n        high_part = _mm256_or_si256(high_part, high_bits_mask);\n\n        // Store results into the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), low_part);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high_part);\n    }\n\n    // Handle remaining elements using scalar code\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 uint32_t values from src\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Zero extend to 64-bit integers\n        __m256i low_part = _mm256_cvtepu32_epi64(_mm256_castsi256_si128(src_vec));\n        __m256i high_part = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 1));\n        \n        // Set high 32 bits to 1\n        low_part = _mm256_or_si256(low_part, high_bits);\n        high_part = _mm256_or_si256(high_part, high_bits);\n        \n        // Store results into dst\n        _mm256_storeu_si256((__m256i*)(dst + i), low_part);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high_part);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits_mask = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 packed 32-bit integers from the source array\n        __m256i src_vec = _mm256_loadu_si256((__m256i const*)(src + i));\n        \n        // Zero extend to 64 bits (convert 32-bit to 64-bit)\n        __m256i low_part = _mm256_cvtepu32_epi64(_mm256_castsi256_si128(src_vec));\n        __m256i high_part = _mm256_cvtepu32_epi64(_mm256_extracti128_si256(src_vec, 1));\n\n        // Set the high 32 bits of each 64-bit value to 1\n        low_part = _mm256_or_si256(low_part, high_bits_mask);\n        high_part = _mm256_or_si256(high_part, high_bits_mask);\n\n        // Store the results into the destination array\n        _mm256_storeu_si256((__m256i*)(dst + i), low_part);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), high_part);\n    }\n\n    // Handle any remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000); // Set high 32 bits to 1\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]); // Load 8 uint32_t values\n        __m256i low_part = _mm256_cvtepu32_epi64(src_vec);       // Zero-extend to 64-bit\n        __m256i result = _mm256_or_si256(low_part, high_bits);   // Set high 32 bits to 1\n        _mm256_storeu_si256((__m256i*)&dst[i], result);          // Store 8 uint64_t values\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double result[4] = {0.0, 0.0, 0.0, 0.0};\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        \n        __m256d val0 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 0)]);\n        __m256d val1 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 1)]);\n        __m256d val2 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 2)]);\n        __m256d val3 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 3)]);\n        \n        __m256d vals = _mm256_add_pd(_mm256_add_pd(val0, val1), _mm256_add_pd(val2, val3));\n        sum_vec = _mm256_add_pd(sum_vec, vals);\n    }\n    \n    _mm256_storeu_pd(result, sum_vec);\n    double total_sum = result[0] + result[1] + result[2] + result[3];\n    \n    for (; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n    \n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double result[4] = {0.0, 0.0, 0.0, 0.0};\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        \n        __m256d val0 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 0)]);\n        __m256d val1 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 1)]);\n        __m256d val2 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 2)]);\n        __m256d val3 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 3)]);\n        \n        __m256d vals = _mm256_add_pd(_mm256_add_pd(val0, val1), _mm256_add_pd(val2, val3));\n        sum_vec = _mm256_add_pd(sum_vec, vals);\n    }\n    \n    _mm256_storeu_pd(result, sum_vec);\n    double total_sum = result[0] + result[1] + result[2] + result[3];\n    \n    for (; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n    \n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double result[4] = {0.0, 0.0, 0.0, 0.0};\n    \n    size_t i = 0;\n    // Process indices in chunks of 4 for AVX\n    for (; i + 3 < length; i += 4) {\n        // Load 4 indices\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        \n        // Convert indices to 32-bit integers\n        __m128i idx_32 = _mm_cvtepi16_epi32(idx);\n        \n        // Gather 4 doubles from vec using the indices\n        __m256d values = _mm256_i32gather_pd(vec, idx_32, 8);\n        \n        // Accumulate into sum vector\n        sum_vec = _mm256_add_pd(sum_vec, values);\n    }\n    \n    // Horizontal sum of the AVX register\n    _mm256_storeu_pd(result, sum_vec);\n    double total_sum = result[0] + result[1] + result[2] + result[3];\n    \n    // Handle remaining elements\n    for (; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n    \n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double result[4] = {0.0, 0.0, 0.0, 0.0};\n    \n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        \n        __m256d val0 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 0)]);\n        __m256d val1 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 1)]);\n        __m256d val2 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 2)]);\n        __m256d val3 = _mm256_set1_pd(vec[_mm_extract_epi16(idx, 3)]);\n        \n        __m256d vals = _mm256_add_pd(_mm256_add_pd(val0, val1), _mm256_add_pd(val2, val3));\n        sum_vec = _mm256_add_pd(sum_vec, vals);\n    }\n    \n    _mm256_storeu_pd(result, sum_vec);\n    double total_sum = result[0] + result[1] + result[2] + result[3];\n    \n    for (; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n    \n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    double result = 0.0;\n\n    size_t i = 0;\n    // Process indices in chunks of 4 for AVX\n    for (; i + 3 < length; i += 4) {\n        // Load 4 indices\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        // Convert indices to 32-bit integers\n        __m256i idx_32 = _mm256_cvtepi16_epi32(idx);\n\n        // Gather 4 doubles from vec using the indices\n        __m256d vals = _mm256_i32gather_pd(vec, idx_32, 8);\n\n        // Accumulate into sum vector\n        sum_vec = _mm256_add_pd(sum_vec, vals);\n    }\n\n    // Horizontal sum of the AVX register\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    result += temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    __m256d mul_vec = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum_vec = _mm256_add_pd(sum_vec, mul_vec);\n                }\n\n                // Handle remaining elements if kernel_size is not a multiple of 4\n                for (size_t kj = (kernel_size / 4) * 4; kj < kernel_size; ++kj) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n                    sum_vec = _mm256_add_pd(sum_vec, _mm256_set1_pd(input[input_idx] * kernel[kernel_idx]));\n                }\n            }\n\n            // Horizontal sum of the vector\n            double sum_array[4];\n            _mm256_storeu_pd(sum_array, sum_vec);\n            double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = (sum > 0.0) ? sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    __m256d mul_vec = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum_vec = _mm256_add_pd(sum_vec, mul_vec);\n                }\n            }\n\n            // Horizontal sum of the vector\n            double temp[4];\n            _mm256_storeu_pd(temp, sum_vec);\n            double sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Apply ReLU activation\n            if (sum > 0.0) {\n                output[i * output_size + j] = sum;\n            } else {\n                output[i * output_size + j] = 0.0;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    __m256d mul_vec = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum_vec = _mm256_add_pd(sum_vec, mul_vec);\n                }\n            }\n\n            // Horizontal sum of the vector\n            double sum_array[4];\n            _mm256_storeu_pd(sum_array, sum_vec);\n            double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n            // Apply ReLU activation\n            if (sum > 0.0) {\n                output[i * output_size + j] = sum;\n            } else {\n                output[i * output_size + j] = 0.0;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    sum_vec = _mm256_fmadd_pd(input_vec, kernel_vec, sum_vec);\n                }\n            }\n\n            // Horizontal sum of the vector\n            double temp[4];\n            _mm256_storeu_pd(temp, sum_vec);\n            double sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n            // Apply ReLU activation\n            if (sum > 0.0) {\n                output[i * output_size + j] = sum;\n            } else {\n                output[i * output_size + j] = 0.0;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t ki = 0; ki < kernel_size; ++ki) {\n                for (size_t kj = 0; kj < kernel_size; kj += 4) {\n                    size_t input_idx = (i + ki) * input_size + (j + kj);\n                    size_t kernel_idx = ki * kernel_size + kj;\n\n                    __m256d input_vec = _mm256_loadu_pd(&input[input_idx]);\n                    __m256d kernel_vec = _mm256_loadu_pd(&kernel[kernel_idx]);\n\n                    __m256d mul_vec = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum_vec = _mm256_add_pd(sum_vec, mul_vec);\n                }\n            }\n\n            // Horizontal sum of the vector\n            double sum_array[4];\n            _mm256_storeu_pd(sum_array, sum_vec);\n            double sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n            // Apply ReLU activation\n            output[i * output_size + j] = (sum > 0.0) ? sum : 0.0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t size = numbers.size();\n    if (size < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < size; ++i) {\n        __m256d num_i = _mm256_set1_pd(numbers[i]);\n\n        size_t j = i + 1;\n        for (; j + 3 < size; j += 4) {\n            __m256d num_j = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(num_i, num_j);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return true;\n            }\n        }\n\n        for (; j < size; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t size = numbers.size();\n    if (size < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < size; ++i) {\n        __m256d num_i = _mm256_set1_pd(numbers[i]);\n\n        for (size_t j = i + 1; j + 3 < size; j += 4) {\n            __m256d num_j = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(num_i, num_j);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return true;\n            }\n        }\n\n        // Handle remaining elements\n        for (size_t j = size - (size % 4); j < size; ++j) {\n            if (fabs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t size = numbers.size();\n    if (size < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < size; ++i) {\n        __m256d num_i = _mm256_set1_pd(numbers[i]);\n\n        size_t j = i + 1;\n        for (; j + 3 < size; j += 4) {\n            __m256d num_j = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(num_i, num_j);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return true;\n            }\n        }\n\n        for (; j < size; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t size = numbers.size();\n    if (size < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < size; ++i) {\n        __m256d num_i = _mm256_set1_pd(numbers[i]);\n        size_t j = i + 1;\n\n        // Process 4 elements at a time using AVX\n        for (; j + 3 < size; j += 4) {\n            __m256d num_j = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(num_i, num_j);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return true;\n            }\n        }\n\n        // Process remaining elements\n        for (; j < size; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold) {\n    size_t size = numbers.size();\n    if (size < 2) return false;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n\n    for (size_t i = 0; i < size; ++i) {\n        __m256d num_i = _mm256_set1_pd(numbers[i]);\n\n        size_t j = i + 1;\n        for (; j + 3 < size; j += 4) {\n            __m256d num_j = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(num_i, num_j);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n            __m256d cmp = _mm256_cmp_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return true;\n            }\n        }\n\n        for (; j < size; ++j) {\n            if (std::abs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int balance = 0;\n\n    // Process in chunks of 8 using AVX2\n    int i = 0;\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_set1_epi32(0);\n\n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers from the operations array\n        __m256i ops_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n\n        // Add the loaded values to the current balance vector\n        balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n        // Compare if any value in the balance vector is less than zero\n        __m256i cmp_vec = _mm256_cmpgt_epi32(zero_vec, balance_vec);\n        int mask = _mm256_movemask_epi8(cmp_vec);\n\n        // If any element is less than zero, return true\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    // Horizontal sum of the balance vector to update scalar balance\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), balance_vec);\n    for (int j = 0; j < 8; ++j) {\n        balance += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int balance = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    __m256i zero_vec = _mm256_set1_epi32(0);\n    __m256i balance_vec = _mm256_set1_epi32(0);\n\n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers from the operations array\n        __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n\n        // Add the loaded values to the current balance vector\n        balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n        // Compare if any of the balances are less than zero\n        __m256i cmp_vec = _mm256_cmpgt_epi32(zero_vec, balance_vec);\n\n        // Check if any comparison result is true (i.e., any balance < 0)\n        int mask = _mm256_movemask_epi8(cmp_vec);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    // Accumulate the remaining balance from the vector into a scalar\n    int temp_balance[8];\n    _mm256_storeu_si256((__m256i*)temp_balance, balance_vec);\n    for (int j = 0; j < 8; ++j) {\n        balance += temp_balance[j];\n    }\n\n    // Process any remaining elements in the operations array\n    for (; i < n; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int balance = 0;\n\n    // Process in chunks of 8 using AVX2\n    int i = 0;\n    __m256i zero_vec = _mm256_set1_epi32(0);\n    __m256i balance_vec = _mm256_set1_epi32(0);\n\n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers into a vector\n        __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n\n        // Add the current balance to the vector\n        balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n        // Check if any value in the vector is less than zero\n        __m256i cmp_vec = _mm256_cmpgt_epi32(zero_vec, balance_vec);\n        int mask = _mm256_movemask_epi8(cmp_vec);\n\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    // Accumulate the remaining balance from the vector\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, balance_vec);\n    for (int j = 0; j < 8; ++j) {\n        balance += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int balance = 0;\n    int i = 0;\n\n    // Process in chunks of 8 using AVX2\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_set1_epi32(0);\n\n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers from the operations vector\n        __m256i ops_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n\n        // Add the loaded values to the current balance vector\n        balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n        // Compare if any balance is less than zero\n        __m256i cmp_vec = _mm256_cmpgt_epi32(zero_vec, balance_vec);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_vec));\n\n        // If any element is below zero, return true\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    // Horizontal sum of the balance vector to update scalar balance\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), balance_vec);\n    balance += temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_zero_simd(std::vector<int> operations) {\n    int n = operations.size();\n    int balance = 0;\n\n    // Process in chunks of 8 using AVX2\n    int i = 0;\n    __m256i zero_vec = _mm256_set1_epi32(0);\n    __m256i balance_vec = _mm256_set1_epi32(0);\n\n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers from the operations vector\n        __m256i ops_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n\n        // Add the loaded values to the balance vector\n        balance_vec = _mm256_add_epi32(balance_vec, ops_vec);\n\n        // Compare if any value in balance_vec is less than zero\n        __m256i cmp_vec = _mm256_cmpgt_epi32(zero_vec, balance_vec);\n\n        // Check if any comparison result is true (i.e., any balance < 0)\n        int mask = _mm256_movemask_epi8(cmp_vec);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    // Horizontal sum of the balance vector to update scalar balance\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, balance_vec);\n    for (int j = 0; j < 8; ++j) {\n        balance += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        balance += operations[i];\n        if (balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Ensure the size is a multiple of 4 for AVX processing\n    size_t simd_size = (n / 4) * 4;\n    __m256d sum_vec = _mm256_setzero_pd();\n\n    // Compute sum using AVX\n    size_t i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX register\n    double sum = 0.0;\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    // Compute mean\n    double mean = sum / n;\n\n    // Compute absolute deviations and their sum\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_sum_vec = _mm256_setzero_pd();\n\n    i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_and_pd(_mm256_abs_pd(diff), _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF)));\n        abs_sum_vec = _mm256_add_pd(abs_sum_vec, abs_diff);\n    }\n\n    // Horizontal sum of the absolute differences\n    _mm256_storeu_pd(temp, abs_sum_vec);\n    double abs_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining absolute differences\n    for (; i < n; ++i) {\n        abs_sum += std::abs(numbers[i] - mean);\n    }\n\n    // Compute MAD\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Ensure the size is a multiple of 4 for AVX processing\n    size_t simd_size = n / 4 * 4;\n    __m256d sum_vec = _mm256_setzero_pd();\n\n    // Compute the sum of all elements using AVX\n    size_t i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX register\n    double sum = 0.0;\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    // Compute the mean\n    double mean = sum / n;\n\n    // Compute the absolute deviations from the mean using AVX\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_dev_sum_vec = _mm256_setzero_pd();\n\n    i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_and_pd(_mm256_abs_pd(diff), _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF)));\n        abs_dev_sum_vec = _mm256_add_pd(abs_dev_sum_vec, abs_diff);\n    }\n\n    // Horizontal sum of the absolute deviations\n    _mm256_storeu_pd(temp, abs_dev_sum_vec);\n    double abs_dev_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining absolute deviations\n    for (; i < n; ++i) {\n        abs_dev_sum += std::abs(numbers[i] - mean);\n    }\n\n    // Compute the Mean Absolute Deviation\n    return abs_dev_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Ensure the size is a multiple of 4 for AVX processing\n    size_t simd_size = n / 4 * 4;\n    __m256d sum_vec = _mm256_setzero_pd();\n    \n    // Compute sum of all elements using AVX\n    size_t i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX register\n    double sum = 0.0;\n    alignas(32) double temp[4];\n    _mm256_store_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    // Compute mean\n    double mean = sum / n;\n\n    // Compute absolute deviations and their sum\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_sum_vec = _mm256_setzero_pd();\n\n    i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_and_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n        abs_sum_vec = _mm256_add_pd(abs_sum_vec, abs_diff);\n    }\n\n    // Horizontal sum of absolute differences\n    _mm256_store_pd(temp, abs_sum_vec);\n    double abs_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining absolute differences\n    for (; i < n; ++i) {\n        abs_sum += std::abs(numbers[i] - mean);\n    }\n\n    // Compute MAD\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Ensure the size is a multiple of 4 for AVX processing\n    size_t simd_size = (n / 4) * 4;\n    __m256d sum_vec = _mm256_setzero_pd();\n\n    // Compute sum using AVX\n    size_t i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the AVX register\n    double sum = 0.0;\n    double temp[4];\n    _mm256_storeu_pd(temp, sum_vec);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining elements\n    for (; i < n; ++i) {\n        sum += numbers[i];\n    }\n\n    // Compute mean\n    double mean = sum / n;\n\n    // Compute absolute deviations and their sum\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d abs_sum_vec = _mm256_setzero_pd();\n\n    i = 0;\n    for (; i < simd_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_and_pd(_mm256_abs_pd(diff), _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF)));\n        abs_sum_vec = _mm256_add_pd(abs_sum_vec, abs_diff);\n    }\n\n    // Horizontal sum of the AVX register\n    _mm256_storeu_pd(temp, abs_sum_vec);\n    double abs_sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    // Add remaining elements\n    for (; i < n; ++i) {\n        abs_sum += std::abs(numbers[i] - mean);\n    }\n\n    // Compute MAD\n    return abs_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers) {\n    int n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Ensure the size is a multiple of 4 for AVX processing\n    int simd_size = 4;\n    int padded_size = ((n + simd_size - 1) / simd_size) * simd_size;\n    std::vector<double> padded_numbers(padded_size, 0.0);\n    std::copy(numbers.begin(), numbers.end(), padded_numbers.begin());\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    for (int i = 0; i < padded_size; i += simd_size) {\n        __m256d vec = _mm256_loadu_pd(&padded_numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of sum_vec\n    double sum = 0.0;\n    alignas(32) double temp[4];\n    _mm256_store_pd(temp, sum_vec);\n    for (int i = 0; i < simd_size; ++i) {\n        sum += temp[i];\n    }\n\n    double mean = sum / n;\n\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n\n    for (int i = 0; i < padded_size; i += simd_size) {\n        __m256d vec = _mm256_loadu_pd(&padded_numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff); // Absolute value\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n\n    // Horizontal sum of mad_sum_vec\n    double mad_sum = 0.0;\n    _mm256_store_pd(temp, mad_sum_vec);\n    for (int i = 0; i < simd_size; ++i) {\n        mad_sum += temp[i];\n    }\n\n    return mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    size_t n = numbers.size();\n    \n    if (n == 0) return result;\n\n    // Reserve space for the result vector\n    result.reserve(n * 2 - 1);\n\n    // Load the delimiter into a SIMD register\n    __m256i delim = _mm256_set1_epi32(delimeter);\n\n    // Process 8 elements at a time using AVX2\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        // Load 8 elements from the input vector\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Prepare the output block: [vec[0], delim, vec[1], delim, ..., vec[7]]\n        alignas(32) int temp[15];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vec);\n        for (int j = 0; j < 7; ++j) {\n            result.push_back(temp[j]);\n            result.push_back(delimeter);\n        }\n        result.push_back(temp[7]);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result.push_back(numbers[i]);\n        if (i != n - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    size_t n = numbers.size();\n    \n    if (n == 0) return result;\n\n    // Reserve space for the result vector\n    result.reserve(n * 2 - 1);\n\n    // Load the delimiter into a SIMD register\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n\n    // Process 8 elements at a time using AVX2\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        // Load 8 elements from the input vector\n        __m256i nums_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Prepare the output with interleaved delimiters\n        __m256i res1 = _mm256_unpacklo_epi32(nums_vec, delim_vec);\n        __m256i res2 = _mm256_unpackhi_epi32(nums_vec, delim_vec);\n\n        // Store the first 8 results (7 numbers + 7 delimiters)\n        alignas(32) int temp[16];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), res1);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp + 8), res2);\n\n        // Append to the result vector, skipping the last delimiter\n        for (size_t j = 0; j < 14; j += 2) {\n            result.push_back(temp[j]);\n            result.push_back(temp[j + 1]);\n        }\n        result.push_back(temp[14]);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result.push_back(numbers[i]);\n        if (i != n - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    size_t n = numbers.size();\n    \n    if (n == 0) return result;\n\n    // Reserve space for the result vector\n    result.reserve(n * 2 - 1);\n\n    // Load the delimiter into a SIMD register\n    __m256i delim = _mm256_set1_epi32(delimeter);\n\n    // Process 8 elements at a time using AVX2\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        // Load 8 elements from the input vector\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Prepare the output with interleaved delimiter\n        alignas(32) int temp[15];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vec);\n        temp[8] = delimeter;\n        temp[9] = delimeter;\n        temp[10] = delimeter;\n        temp[11] = delimeter;\n        temp[12] = delimeter;\n        temp[13] = delimeter;\n        temp[14] = delimeter;\n\n        // Store the interleaved values into the result vector\n        for (size_t j = 0; j < 8; ++j) {\n            result.push_back(temp[j]);\n            result.push_back(delimeter);\n        }\n    }\n\n    // Handle the remaining elements\n    for (; i < n; ++i) {\n        result.push_back(numbers[i]);\n        if (i != n - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    size_t n = numbers.size();\n    \n    if (n == 0) return result;\n\n    // Reserve space for the result vector\n    result.reserve(n * 2 - 1);\n\n    // Load delimiter into a SIMD register\n    __m256i delim = _mm256_set1_epi32(delimeter);\n\n    // Process 8 elements at a time using AVX2\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        // Load 8 elements from the input vector\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Prepare the output with interleaved delimiter\n        alignas(32) int temp[15];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vec);\n        temp[8] = delimeter;\n        temp[9] = delimeter;\n        temp[10] = delimeter;\n        temp[11] = delimeter;\n        temp[12] = delimeter;\n        temp[13] = delimeter;\n        temp[14] = delimeter;\n\n        // Store the first 7 elements and their delimiters\n        for (size_t j = 0; j < 7; ++j) {\n            result.push_back(temp[j]);\n            result.push_back(delimeter);\n        }\n        // Store the last element without a delimiter\n        result.push_back(temp[7]);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result.push_back(numbers[i]);\n        if (i != n - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter) {\n    std::vector<int> result;\n    size_t n = numbers.size();\n    \n    if (n == 0) return result;\n\n    // Reserve enough space in the result vector\n    result.reserve(n * 2 - 1);\n\n    // Load delimiter into a SIMD register\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n\n    // Process 8 elements at a time using AVX2\n    size_t i = 0;\n    for (; i + 8 <= n; i += 8) {\n        // Load 8 elements from the input vector\n        __m256i nums_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Prepare the output with interleaved delimiters\n        __m256i res1 = _mm256_unpacklo_epi32(nums_vec, delim_vec);\n        __m256i res2 = _mm256_unpackhi_epi32(nums_vec, delim_vec);\n\n        // Store the first 8 results (7 numbers + delimiter)\n        alignas(32) int temp[16];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), res1);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp + 8), res2);\n\n        // Append to the result vector, skipping the last delimiter\n        for (size_t j = 0; j < 14; j += 2) {\n            result.push_back(temp[j]);\n        }\n        result.push_back(temp[14]);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result.push_back(numbers[i]);\n        if (i != n - 1) {\n            result.push_back(delimeter);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t total_sum = 0;\n    int64_t total_product = 1;\n\n    size_t i = 0;\n    size_t size = numbers.size();\n\n    if (size == 0) {\n        return {total_sum, total_product};\n    }\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vproduct = _mm256_set1_epi16(1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Horizontal sum and product are not directly supported in AVX2, so accumulate vertically\n        vsum = _mm256_add_epi16(vsum, vnums);\n\n        // Product is more complex due to lack of direct SIMD multiply-accumulate\n        // Split into two halves for partial products\n        __m256i vprod_lo = _mm256_mullo_epi16(vproduct, vnums);\n        vproduct = vprod_lo;\n    }\n\n    // Reduce the vector sum horizontally\n    int16_t temp[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for (int j = 0; j < 16; ++j) {\n        total_sum += temp[j];\n    }\n\n    // Reduce the vector product horizontally\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vproduct);\n\n    for (int j = 0; j < 16; ++j) {\n        total_product *= temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        total_sum += numbers[i];\n        total_product *= numbers[i];\n    }\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {sum, product};\n    }\n\n    // Ensure the size is a multiple of 16 for AVX2 processing\n    size_t i = 0;\n    size_t size = numbers.size();\n    size_t simd_size = (size / 16) * 16;\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vproduct = _mm256_set1_epi16(1);\n\n    for (; i < simd_size; i += 16) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Horizontal sum and product are not directly supported in AVX2, so accumulate vertically\n        vsum = _mm256_add_epi16(vsum, v);\n\n        // Compute product using iterative multiplication\n        vproduct = _mm256_mullo_epi16(vproduct, v);\n    }\n\n    // Reduce the vector sum horizontally\n    int16_t temp[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n    for (int j = 0; j < 16; ++j) {\n        sum += temp[j];\n    }\n\n    // Reduce the vector product horizontally\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vproduct);\n    for (int j = 0; j < 16; ++j) {\n        product *= temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    size_t i = 0;\n    size_t size = numbers.size();\n\n    if (size == 0) {\n        return {sum, product};\n    }\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vproduct = _mm256_set1_epi16(1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Horizontal sum and product are not directly supported in AVX2, so we accumulate vertically.\n        vsum = _mm256_add_epi16(vsum, v);\n\n        // For product, we need to handle it carefully since AVX2 doesn't have integer multiplication across lanes.\n        // We'll process the product sequentially after the loop.\n    }\n\n    // Extract partial sums from vsum\n    int16_t temp[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n    for (size_t j = 0; j < 16; ++j) {\n        sum += temp[j];\n    }\n\n    // Process remaining elements and calculate product\n    for (size_t j = 0; j < i; ++j) {\n        product *= numbers[j];\n    }\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if (numbers.empty()) {\n        return {sum, product};\n    }\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vproduct = _mm256_set1_epi16(1);\n\n    size_t i = 0;\n    size_t size = numbers.size();\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Horizontal sum and product are not directly supported in AVX2, so we process in chunks.\n        vsum = _mm256_add_epi16(vsum, vnums);\n        vproduct = _mm256_mullo_epi16(vproduct, vnums);\n    }\n\n    // Reduce the vector sum into a scalar sum\n    int16_t temp[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n    for (size_t j = 0; j < 16; ++j) {\n        sum += temp[j];\n    }\n\n    // Reduce the vector product into a scalar product\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vproduct);\n    for (size_t j = 0; j < 16; ++j) {\n        product *= temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers) {\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    size_t i = 0;\n    size_t size = numbers.size();\n\n    if (size == 0) {\n        return {sum, product};\n    }\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vproduct = _mm256_set1_epi16(1);\n\n    for (; i + 16 <= size; i += 16) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Horizontal sum and product are not directly supported in AVX2, so we accumulate vertically.\n        vsum = _mm256_add_epi16(vsum, vnums);\n\n        // Compute product using a loop since AVX2 does not have native multiply across lanes.\n        __m256i temp_product = vproduct;\n        for (int j = 0; j < 16; ++j) {\n            __m256i broadcast = _mm256_set1_epi16(((int16_t*)&vnums)[j]);\n            temp_product = _mm256_mullo_epi16(temp_product, broadcast);\n        }\n        vproduct = temp_product;\n    }\n\n    // Reduce the vector sum into a scalar sum.\n    int16_t temp_sum[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_sum), vsum);\n    for (int j = 0; j < 16; ++j) {\n        sum += temp_sum[j];\n    }\n\n    // Reduce the vector product into a scalar product.\n    int16_t temp_product[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_product), vproduct);\n    for (int j = 0; j < 16; ++j) {\n        product *= temp_product[j];\n    }\n\n    // Process remaining elements.\n    for (; i < size; ++i) {\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    int n = numbers.size();\n\n    if (n == 0) return result;\n\n    // Initialize the first element as it is the maximum at the start\n    result[0] = numbers[0];\n\n    // Process in chunks of 8 using AVX2 intrinsics\n    for (int i = 1; i < n; i += 8) {\n        // Load previous results into a SIMD register\n        __m256i prev_max = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&result[i - 1]));\n\n        // Load current numbers into a SIMD register\n        __m256i curr_vals;\n        if (i + 7 < n) {\n            curr_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        } else {\n            // Handle the last partial chunk\n            int temp[8] = {0};\n            for (int j = 0; j < 8 && i + j < n; ++j) {\n                temp[j] = numbers[i + j];\n            }\n            curr_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(temp));\n        }\n\n        // Compute the rolling max using AVX2 intrinsic\n        __m256i rolling_max = _mm256_max_epi32(prev_max, curr_vals);\n\n        // Store the result back to the result vector\n        if (i + 7 < n) {\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), rolling_max);\n        } else {\n            // Handle the last partial chunk\n            int temp_res[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_res), rolling_max);\n            for (int j = 0; j < 8 && i + j < n; ++j) {\n                result[i + j] = temp_res[j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    int n = numbers.size();\n\n    if (n == 0) return result;\n\n    // Initialize the first element as it is the max by default\n    result[0] = numbers[0];\n\n    // Process in chunks of 8 using AVX2\n    int i = 1;\n    __m256i vmax = _mm256_set1_epi32(numbers[0]);\n\n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers from the input vector\n        __m256i vcurrent = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Compute the running maximum\n        vmax = _mm256_max_epi32(vmax, vcurrent);\n\n        // Store the results back to the result vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vmax);\n    }\n\n    // Handle the remaining elements (if any)\n    int current_max = result[i - 1];\n    for (; i < n; ++i) {\n        current_max = std::max(current_max, numbers[i]);\n        result[i] = current_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    int n = numbers.size();\n\n    if (n == 0) return result;\n\n    // Initialize the first element as it is the maximum by default\n    result[0] = numbers[0];\n\n    // Process the input in chunks of 8 using AVX2 intrinsics\n    for (int i = 1; i < n; i += 8) {\n        // Load the current chunk of 8 integers from the input\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n\n        // Load the previous rolling maximum values\n        __m256i prev_max;\n        if (i >= 8) {\n            prev_max = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&result[i - 8]));\n        } else {\n            // Handle the case where there are fewer than 8 elements before the current chunk\n            prev_max = _mm256_set1_epi32(result[i - 1]);\n        }\n\n        // Compute the rolling maximum for the current chunk\n        __m256i rolling_max = _mm256_max_epi32(current, prev_max);\n\n        // Update the rolling maximum across the vector to propagate the maximum value\n        for (int j = 1; j < 8; ++j) {\n            __m256i shifted = _mm256_alignr_epi8(rolling_max, rolling_max, 4);\n            rolling_max = _mm256_max_epi32(rolling_max, shifted);\n        }\n\n        // Store the result back into the output vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), rolling_max);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    std::vector<int> result(numbers.size());\n    int n = numbers.size();\n\n    if (n == 0) return result;\n\n    __m256i vmax = _mm256_set1_epi32(numbers[0]);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        vmax = _mm256_max_epi32(vmax, v);\n        _mm256_storeu_si256((__m256i*)&result[i], vmax);\n\n        // Update the scalar maximum value for the next iteration\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, vmax);\n        int max_val = temp[0];\n        for (int j = 1; j < 8; ++j) {\n            if (temp[j] > max_val) max_val = temp[j];\n        }\n        vmax = _mm256_set1_epi32(max_val);\n    }\n\n    // Handle any remaining elements\n    int processed = (n / 8) * 8;\n    int current_max = result[processed - 1];\n    for (int i = processed; i < n; ++i) {\n        if (numbers[i] > current_max) current_max = numbers[i];\n        result[i] = current_max;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers) {\n    size_t n = numbers.size();\n    std::vector<int> result(n);\n    \n    if (n == 0) return result;\n\n    // Initialize the first element as it is the maximum at the start\n    result[0] = numbers[0];\n    \n    // Process the input in chunks of 8 using AVX2\n    size_t i = 1;\n    __m256i vmax = _mm256_set1_epi32(numbers[0]); // Initialize vector with the first element\n    \n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers from the input array\n        __m256i vcurrent = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        \n        // Compute the running maximum\n        vmax = _mm256_max_epi32(vmax, vcurrent);\n        \n        // Store the results back to the result array\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vmax);\n    }\n    \n    // Extract the last maximum value from the vector to continue scalar processing\n    int max_scalar = result[i - 1];\n    \n    // Process the remaining elements (if any) using scalar code\n    for (; i < n; ++i) {\n        max_scalar = std::max(max_scalar, numbers[i]);\n        result[i] = max_scalar;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    std::string result(len, '0');\n    \n    // Ensure both strings are of the same length\n    if (a.size() != b.size()) {\n        return result;\n    }\n\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= len; i += 32) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n\n        // XOR operation\n        __m256i vxor = _mm256_xor_si256(va, vb);\n\n        // Store the result back to the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vxor);\n    }\n\n    // Process remaining characters (less than 32)\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len_a = a.size();\n    size_t len_b = b.size();\n    size_t max_len = std::max(len_a, len_b);\n\n    // Pad strings with '0' to make them equal length\n    a.resize(max_len, '0');\n    b.resize(max_len, '0');\n\n    // Result string\n    std::string result(max_len, '0');\n\n    // Process in chunks of 32 characters (256 bits)\n    size_t i = 0;\n    for (; i + 32 <= max_len; i += 32) {\n        // Load 32 bytes from each string\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n\n        // XOR the two vectors\n        __m256i vxor = _mm256_xor_si256(va, vb);\n\n        // Store the result back into the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vxor);\n    }\n\n    // Handle remaining characters (less than 32)\n    for (; i < max_len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len_a = a.size();\n    size_t len_b = b.size();\n    size_t max_len = std::max(len_a, len_b);\n\n    // Pad the shorter string with '0's to match lengths\n    a.resize(max_len, '0');\n    b.resize(max_len, '0');\n\n    // Result string\n    std::string result(max_len, '0');\n\n    // Process 32 characters at a time using AVX2\n    size_t i = 0;\n    for (; i + 32 <= max_len; i += 32) {\n        // Load 32 bytes from each string\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n\n        // XOR the two vectors\n        __m256i vec_result = _mm256_xor_si256(vec_a, vec_b);\n\n        // Store the result back into the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vec_result);\n    }\n\n    // Process remaining characters (if any)\n    for (; i < max_len; ++i) {\n        result[i] = (a[i] ^ b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len_a = a.size();\n    size_t len_b = b.size();\n    size_t max_len = std::max(len_a, len_b);\n\n    // Pad the shorter string with '0's to match lengths\n    a.resize(max_len, '0');\n    b.resize(max_len, '0');\n\n    // Result string\n    std::string result(max_len, '0');\n\n    // Process 32 characters at a time using AVX2\n    size_t i = 0;\n    for (; i + 32 <= max_len; i += 32) {\n        // Load 32 bytes from each string into AVX registers\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n\n        // XOR the two vectors\n        __m256i vxor = _mm256_xor_si256(va, vb);\n\n        // Store the result back into the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vxor);\n    }\n\n    // Process remaining characters (if any)\n    for (; i < max_len; ++i) {\n        result[i] = (a[i] ^ b[i]) ? '1' : '0';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len_a = a.size();\n    size_t len_b = b.size();\n    size_t max_len = std::max(len_a, len_b);\n    \n    // Pad strings with '0' to make them equal length\n    a.resize(max_len, '0');\n    b.resize(max_len, '0');\n\n    // Result string\n    std::string result(max_len, '0');\n\n    // Process in chunks of 32 characters (256 bits)\n    size_t i = 0;\n    for (; i + 32 <= max_len; i += 32) {\n        // Load 32 bytes from each string\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n\n        // XOR the two vectors\n        __m256i vxor = _mm256_xor_si256(va, vb);\n\n        // Store the result back into the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vxor);\n    }\n\n    // Process remaining characters (if any)\n    for (; i < max_len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) return \"\";\n\n    const __m256i increment = _mm256_set1_epi32(1);\n    const __m256i zero = _mm256_setzero_si256();\n    int max_len = 0;\n    size_t max_idx = 0;\n\n    for (size_t i = 0; i < strings.size(); i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        int limit = std::min(i + 8, strings.size());\n\n        for (size_t j = i; j < limit; ++j) {\n            int len = strings[j].size();\n            lengths = _mm256_insert_epi32(lengths, len, j - i);\n        }\n\n        // Find the maximum length in this batch\n        __m256i max_vec = lengths;\n        max_vec = _mm256_max_epi32(max_vec, _mm256_permutevar8x32_epi32(max_vec, _mm256_setr_epi32(4, 5, 6, 7, 0, 1, 2, 3)));\n        max_vec = _mm256_max_epi32(max_vec, _mm256_permutevar8x32_epi32(max_vec, _mm256_setr_epi32(2, 3, 0, 1, 6, 7, 4, 5)));\n        max_vec = _mm256_max_epi32(max_vec, _mm256_permutevar8x32_epi32(max_vec, _mm256_setr_epi32(1, 0, 3, 2, 5, 4, 7, 6)));\n\n        int current_max = _mm256_extract_epi32(max_vec, 0);\n        if (current_max > max_len) {\n            // Find which string has this length\n            for (size_t j = i; j < limit; ++j) {\n                if (static_cast<int>(strings[j].size()) == current_max) {\n                    max_len = current_max;\n                    max_idx = j;\n                    break;\n                }\n            }\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) return \"\";\n\n    size_t max_len = 0;\n    size_t max_idx = 0;\n\n    for (size_t i = 0; i < strings.size(); i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        int valid_elements = (strings.size() - i) >= 8 ? 8 : (strings.size() - i);\n\n        // Load lengths of 8 strings into a vector\n        for (int j = 0; j < valid_elements; ++j) {\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(strings[i + j].size()), j);\n        }\n\n        // Find the maximum length in the vector\n        int max_in_vector = 0;\n        int idx_in_vector = 0;\n        for (int j = 0; j < valid_elements; ++j) {\n            int len = _mm256_extract_epi32(lengths, j);\n            if (len > max_in_vector) {\n                max_in_vector = len;\n                idx_in_vector = j;\n            }\n        }\n\n        // Compare with global maximum\n        if (max_in_vector > static_cast<int>(max_len)) {\n            max_len = max_in_vector;\n            max_idx = i + idx_in_vector;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) return \"\";\n\n    const __m256i increment = _mm256_set1_epi32(1);\n    int max_length = 0;\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < strings.size(); i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        for (int j = 0; j < 8 && (i + j) < strings.size(); ++j) {\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(strings[i + j].size()), j);\n        }\n\n        int max_in_batch = 0;\n        int index_in_batch = 0;\n        for (int j = 0; j < 8 && (i + j) < strings.size(); ++j) {\n            int current_length = _mm256_extract_epi32(lengths, j);\n            if (current_length > max_in_batch) {\n                max_in_batch = current_length;\n                index_in_batch = i + j;\n            }\n        }\n\n        if (max_in_batch > max_length) {\n            max_length = max_in_batch;\n            max_index = index_in_batch;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) return \"\";\n\n    const __m256i zero = _mm256_setzero_si256();\n    size_t max_len = 0;\n    size_t max_idx = 0;\n\n    for (size_t i = 0; i < strings.size(); ++i) {\n        const char *str = strings[i].c_str();\n        size_t len = strings[i].size();\n\n        // Process in chunks of 32 bytes using AVX2\n        size_t j = 0;\n        for (; j + 32 <= len; j += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + j));\n            __m256i cmp = _mm256_cmpeq_epi8(chunk, zero);\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                int pos = _tzcnt_u32(mask);\n                len = j + pos;\n                break;\n            }\n        }\n\n        // Check remaining characters\n        for (; j < len; ++j) {\n            if (str[j] == '\\0') {\n                len = j;\n                break;\n            }\n        }\n\n        if (len > max_len) {\n            max_len = len;\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> &strings) {\n    if (strings.empty()) return \"\";\n\n    const __m256i increment = _mm256_set1_epi32(1);\n    int max_len = 0;\n    size_t max_idx = 0;\n\n    for (size_t i = 0; i < strings.size(); i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        __m256i indices = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n\n        for (size_t j = 0; j < 8 && (i + j) < strings.size(); ++j) {\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(strings[i + j].size()), j);\n        }\n\n        int max_val = 0;\n        int max_pos = 0;\n        int temp_max[8];\n        _mm256_storeu_si256((__m256i*)temp_max, lengths);\n\n        for (int k = 0; k < 8 && (i + k) < strings.size(); ++k) {\n            if (temp_max[k] > max_val) {\n                max_val = temp_max[k];\n                max_pos = i + k;\n            }\n        }\n\n        if (max_val > max_len) {\n            max_len = max_val;\n            max_idx = max_pos;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i vcmp = _mm256_cmpeq_epi64(va, vb);\n        int mask = _mm256_movemask_epi8(vcmp);\n        if (mask == 0xFFFFFFFF) {\n            break;\n        }\n\n        __m256i vgreater = _mm256_cmpgt_epi64(va, vb);\n        __m256i vsub = _mm256_sub_epi64(va, vb);\n        __m256i vmod = _mm256_blendv_epi8(vb, vsub, vgreater);\n\n        vb = _mm256_blendv_epi8(vb, va, vgreater);\n        va = vmod;\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i vcmp = _mm256_cmpeq_epi64(va, vb);\n        int mask = _mm256_movemask_epi8(vcmp);\n        if (mask == 0xFFFFFFFF) {\n            break;\n        }\n\n        __m256i vgreater = _mm256_cmpgt_epi64(va, vb);\n        __m256i vless = _mm256_cmpgt_epi64(vb, va);\n\n        __m256i vsub = _mm256_sub_epi64(va, vb);\n        va = _mm256_blendv_epi8(va, vsub, vgreater);\n\n        vsub = _mm256_sub_epi64(vb, va);\n        vb = _mm256_blendv_epi8(vb, vsub, vless);\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i vcmp = _mm256_cmpeq_epi64(va, vb);\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(vcmp));\n        if (mask == 0xF) {\n            break;\n        }\n\n        __m256i vgreater = _mm256_cmpgt_epi64(va, vb);\n        __m256i vdiff = _mm256_sub_epi64(va, vb);\n        __m256i vremainder = _mm256_blendv_epi8(vb, vdiff, vgreater);\n\n        vb = _mm256_blendv_epi8(vb, va, vgreater);\n        va = vremainder;\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i vcmp = _mm256_cmpeq_epi64(va, vb);\n        int mask = _mm256_movemask_epi8(vcmp);\n        if (mask == 0xFFFFFFFF) {\n            break;\n        }\n\n        __m256i vgreater = _mm256_cmpgt_epi64(va, vb);\n        __m256i vdiff = _mm256_sub_epi64(va, vb);\n        __m256i vremainder = _mm256_blendv_epi8(vb, vdiff, vgreater);\n\n        vb = _mm256_blendv_epi8(vb, va, vgreater);\n        va = vremainder;\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i vcmp = _mm256_cmpeq_epi64(va, vb);\n        int mask = _mm256_movemask_epi8(vcmp);\n        if (mask == 0xFFFFFFFF) {\n            break;\n        }\n\n        __m256i vgreater = _mm256_cmpgt_epi64(va, vb);\n        __m256i vless = _mm256_cmpgt_epi64(vb, va);\n\n        __m256i vsub = _mm256_sub_epi64(va, vb);\n        va = _mm256_blendv_epi8(va, vsub, vgreater);\n\n        vsub = _mm256_sub_epi64(vb, va);\n        vb = _mm256_blendv_epi8(vb, vsub, vless);\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    // Ensure the size is a multiple of 8 for AVX processing\n    int simd_size = n - (n % 8);\n\n    for (int i = 0; i < simd_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n\n        for (int j = i + 1; j < simd_size; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n\n            // Compute absolute differences between all pairs in vec1 and vec2\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            diff = _mm256_and_ps(diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF))); // Absolute value\n\n            // Find the minimum difference in this vector\n            float diffs[8];\n            _mm256_storeu_ps(diffs, diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (diffs[k] < min_diff) {\n                    min_diff = diffs[k];\n                    closest_a = numbers[i + k];\n                    closest_b = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements that don't fit into SIMD blocks\n    for (int i = simd_size; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest_a = numbers[i];\n                closest_b = numbers[j];\n            }\n        }\n    }\n\n    return {std::min(closest_a, closest_b), std::max(closest_a, closest_b)};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    // Ensure the size is a multiple of 8 for AVX processing\n    int simd_size = n - (n % 8);\n\n    for (int i = 0; i < simd_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n\n        for (int j = i + 1; j < simd_size; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n\n            // Compute absolute differences between vec1 and vec2\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            diff = _mm256_and_ps(diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF))); // Absolute value\n\n            // Find the minimum difference in the vector\n            float diffs[8];\n            _mm256_storeu_ps(diffs, diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (diffs[k] < min_diff && diffs[k] > 0.0f) {\n                    min_diff = diffs[k];\n                    closest_a = numbers[i + k];\n                    closest_b = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements that don't fit into SIMD blocks\n    for (int i = simd_size; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff && diff > 0.0f) {\n                min_diff = diff;\n                closest_a = numbers[i];\n                closest_b = numbers[j];\n            }\n        }\n    }\n\n    return {std::min(closest_a, closest_b), std::max(closest_a, closest_b)};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float closest_pair[2] = {0.0f, 0.0f};\n    float min_diff = std::numeric_limits<float>::max();\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n        for (int j = i + 1; j < n; ++j) {\n            __m256 vec2 = _mm256_set1_ps(numbers[j]);\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n            __m128 low = _mm256_extractf128_ps(abs_diff, 0);\n            __m128 high = _mm256_extractf128_ps(abs_diff, 1);\n            __m128 min1 = _mm_min_ps(low, high);\n            float min_val = std::min(std::min(min1[0], min1[1]), std::min(min1[2], min1[3]));\n\n            if (min_val < min_diff) {\n                min_diff = min_val;\n                closest_pair[0] = numbers[i];\n                closest_pair[1] = numbers[j];\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = (n / 8) * 8; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff) {\n                min_diff = diff;\n                closest_pair[0] = numbers[i];\n                closest_pair[1] = numbers[j];\n            }\n        }\n    }\n\n    return std::vector<float>(closest_pair, closest_pair + 2);\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    // Ensure the size is a multiple of 8 for AVX processing\n    int simd_size = n - (n % 8);\n\n    for (int i = 0; i < simd_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&numbers[i]);\n\n        for (int j = i + 1; j < simd_size; j += 8) {\n            __m256 vec2 = _mm256_loadu_ps(&numbers[j]);\n\n            // Compute absolute differences between vec1 and vec2\n            __m256 diff = _mm256_sub_ps(vec1, vec2);\n            diff = _mm256_and_ps(diff, _mm256_castsi256_ps(_mm256_set1_epi32(0x7FFFFFFF))); // Absolute value\n\n            // Find the minimum difference in this vector\n            float diffs[8];\n            _mm256_storeu_ps(diffs, diff);\n\n            for (int k = 0; k < 8; ++k) {\n                if (diffs[k] < min_diff && diffs[k] > 0.0f) {\n                    min_diff = diffs[k];\n                    closest_a = numbers[i + k];\n                    closest_b = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements that don't fit into SIMD blocks\n    for (int i = simd_size; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            float diff = std::abs(numbers[i] - numbers[j]);\n            if (diff < min_diff && diff > 0.0f) {\n                min_diff = diff;\n                closest_a = numbers[i];\n                closest_b = numbers[j];\n            }\n        }\n    }\n\n    return {std::min(closest_a, closest_b), std::max(closest_a, closest_b)};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_a = 0.0f, closest_b = 0.0f;\n\n    // Ensure the vector is padded to a multiple of 8 for AVX processing\n    int padded_size = ((n + 7) / 8) * 8;\n    std::vector<float> padded_numbers(padded_size, std::numeric_limits<float>::max());\n    for (int i = 0; i < n; ++i) {\n        padded_numbers[i] = numbers[i];\n    }\n\n    for (int i = 0; i < n; ++i) {\n        __m256 vi = _mm256_set1_ps(numbers[i]); // Broadcast current number to all elements of the vector\n        int j = 0;\n\n        for (; j <= padded_size - 8; j += 8) {\n            __m256 vj = _mm256_loadu_ps(&padded_numbers[j]); // Load 8 numbers from the array\n            __m256 diff = _mm256_sub_ps(vi, vj);             // Compute differences\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff); // Absolute value of differences\n\n            // Find the minimum difference in this vector\n            float diffs[8];\n            _mm256_storeu_ps(diffs, abs_diff);\n            for (int k = 0; k < 8; ++k) {\n                if (j + k != i && diffs[k] < min_diff) { // Avoid comparing the same element\n                    min_diff = diffs[k];\n                    closest_a = std::min(numbers[i], padded_numbers[j + k]);\n                    closest_b = std::max(numbers[i], padded_numbers[j + k]);\n                }\n            }\n        }\n\n        // Process remaining elements (if any)\n        for (; j < n; ++j) {\n            if (i != j) {\n                float diff = std::abs(numbers[i] - numbers[j]);\n                if (diff < min_diff) {\n                    min_diff = diff;\n                    closest_a = std::min(numbers[i], numbers[j]);\n                    closest_b = std::max(numbers[i], numbers[j]);\n                }\n            }\n        }\n    }\n\n    return {closest_a, closest_b};\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    // Find min and max using standard library for simplicity\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    // Handle edge case to avoid division by zero\n    if (range == 0.0f) {\n        std::fill(result.begin(), result.end(), 0.0f);\n        return result;\n    }\n\n    // Load constants into AVX registers\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vrange = _mm256_set1_ps(range);\n\n    // Process 8 elements at a time\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        // Load 8 floats into an AVX register\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n\n        // Subtract min_val\n        v = _mm256_sub_ps(v, vmin);\n\n        // Divide by range\n        v = _mm256_div_ps(v, vrange);\n\n        // Store the result back to memory\n        _mm256_storeu_ps(&result[i], v);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    // Find min and max using standard library for simplicity\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    // Handle edge case where all elements are the same\n    if (range == 0.0f) {\n        std::fill(result.begin(), result.end(), 0.0f);\n        return result;\n    }\n\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vrange = _mm256_set1_ps(range);\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n        v = _mm256_sub_ps(v, vmin);\n        v = _mm256_div_ps(v, vrange);\n        _mm256_storeu_ps(&result[i], v);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    // Find min and max using standard library for simplicity\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    // Handle edge case to avoid division by zero\n    if (range == 0.0f) {\n        std::fill(result.begin(), result.end(), 0.0f);\n        return result;\n    }\n\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vrange = _mm256_set1_ps(range);\n\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n        v = _mm256_sub_ps(v, vmin);\n        v = _mm256_div_ps(v, vrange);\n        _mm256_storeu_ps(&result[i], v);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    // Find min and max using SIMD\n    __m256 vmin = _mm256_set1_ps(numbers[0]);\n    __m256 vmax = _mm256_set1_ps(numbers[0]);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n        vmin = _mm256_min_ps(vmin, v);\n        vmax = _mm256_max_ps(vmax, v);\n    }\n\n    // Horizontal reduction to find the true min and max\n    float min_val = numbers[0], max_val = numbers[0];\n    alignas(32) float temp[8];\n    _mm256_store_ps(temp, vmin);\n    for (int i = 0; i < 8; ++i) min_val = std::min(min_val, temp[i]);\n    _mm256_store_ps(temp, vmax);\n    for (int i = 0; i < 8; ++i) max_val = std::max(max_val, temp[i]);\n\n    float range = max_val - min_val;\n    __m256 vrange = _mm256_set1_ps(range);\n    __m256 vmin_vec = _mm256_set1_ps(min_val);\n\n    // Rescale using SIMD\n    for (int i = 0; i < n; i += 8) {\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n        __m256 rescaled = _mm256_div_ps(_mm256_sub_ps(v, vmin_vec), vrange);\n        _mm256_storeu_ps(&result[i], rescaled);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers) {\n    int n = numbers.size();\n    std::vector<float> result(n);\n\n    // Find min and max using standard library for simplicity\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n    float range = max_val - min_val;\n\n    // Handle edge case to avoid division by zero\n    if (range == 0.0f) {\n        std::fill(result.begin(), result.end(), 0.0f);\n        return result;\n    }\n\n    // Load constants into SIMD registers\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vrange = _mm256_set1_ps(range);\n\n    // Process 8 elements at a time\n    int i = 0;\n    for (; i + 7 < n; i += 8) {\n        // Load 8 floats into an AVX register\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n\n        // Subtract min_val\n        v = _mm256_sub_ps(v, vmin);\n\n        // Divide by range\n        v = _mm256_div_ps(v, vrange);\n\n        // Store the result back to memory\n        _mm256_storeu_ps(&result[i], v);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(n / 2 - 3, n / 2 - 2, n / 2 - 1, n / 2 - 0);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    uint64_t result = 1;\n\n    for (uint64_t i = n / 2; i >= 2; i -= 4) {\n        __m256i vec_mod = _mm256_setzero_si256();\n        __m256i vec_temp = vec_i;\n        \n        // Compute modulo using iterative subtraction approach\n        for (int j = 0; j < 64; ++j) {\n            __m256i cmp = _mm256_cmpgt_epi64(vec_temp, vec_n);\n            vec_temp = _mm256_blendv_epi8(_mm256_sub_epi64(vec_temp, vec_n), vec_temp, cmp);\n        }\n\n        // Check if any of the elements in vec_temp are zero (i.e., perfect divisors)\n        int mask = _mm256_movemask_pd((__m256)_mm256_cmpeq_epi64(vec_temp, vec_zero));\n        if (mask != 0) {\n            alignas(32) uint64_t indices[4];\n            _mm256_store_si256((__m256i*)indices, vec_i);\n\n            for (int k = 0; k < 4; ++k) {\n                if (n % indices[k] == 0) {\n                    return indices[k];\n                }\n            }\n        }\n\n        // Decrement the vector of indices\n        vec_i = _mm256_sub_epi64(vec_i, vec_one);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(n / 4, n / 3, n / 2, n - 1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_mod;\n    __m256i vec_res;\n    __m256i vec_max = _mm256_set1_epi64x(0);\n\n    for (uint64_t i = n / 2; i > 1; i -= 4) {\n        vec_i = _mm256_set_epi64x(i - 3, i - 2, i - 1, i);\n        vec_mod = _mm256_rem_epi64(vec_n, vec_i);\n        vec_res = _mm256_cmpeq_epi64(vec_mod, vec_zero);\n        __m256i mask = _mm256_and_si256(vec_res, vec_i);\n        vec_max = _mm256_max_epi64(vec_max, mask);\n    }\n\n    uint64_t results[4];\n    _mm256_storeu_si256((__m256i*)results, vec_max);\n\n    uint64_t max_divisor = 0;\n    for (int i = 0; i < 4; ++i) {\n        if (results[i] > max_divisor && results[i] < n) {\n            max_divisor = results[i];\n        }\n    }\n\n    return max_divisor;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(n / 4, n / 3, n / 2, n - 1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_mod, vec_cmp;\n    uint64_t result = 1;\n\n    for (int i = n - 1; i > 1; i -= 4) {\n        vec_mod = _mm256_rem_epu64(vec_n, vec_i);\n        vec_cmp = _mm256_cmpeq_epi64(vec_mod, vec_zero);\n\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(vec_cmp));\n        if (mask != 0) {\n            alignas(32) uint64_t indices[4];\n            _mm256_store_si256((__m256i*)indices, vec_i);\n\n            for (int j = 0; j < 4; ++j) {\n                if ((mask & (1 << j)) && indices[j] < n && n % indices[j] == 0) {\n                    return indices[j];\n                }\n            }\n        }\n\n        vec_i = _mm256_sub_epi64(vec_i, _mm256_set1_epi64x(4));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(n / 4, n / 3, n / 2, n - 1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_mod, vec_cmp;\n    uint64_t result = 1;\n\n    for (int i = n - 1; i > 1; i -= 4) {\n        vec_mod = _mm256_rem_epi64(vec_n, vec_i);\n        vec_cmp = _mm256_cmpeq_epi64(vec_mod, vec_zero);\n\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(vec_cmp));\n        if (mask != 0) {\n            alignas(32) uint64_t indices[4];\n            _mm256_store_si256((__m256i*)indices, vec_i);\n\n            for (int j = 0; j < 4; ++j) {\n                if ((mask & (1 << j)) && indices[j] < n && n % indices[j] == 0) {\n                    return indices[j];\n                }\n            }\n        }\n\n        vec_i = _mm256_sub_epi64(vec_i, _mm256_set1_epi64x(4));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_i = _mm256_set_epi64x(n / 4, n / 3, n / 2, n - 1);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_mod, vec_cmp;\n    uint64_t result = 1;\n\n    for (int i = n / 2; i > 1; i -= 4) {\n        vec_mod = _mm256_rem_epu64(vec_n, vec_i);\n        vec_cmp = _mm256_cmpeq_epi64(vec_mod, vec_zero);\n\n        int mask = _mm256_movemask_pd(_mm256_castsi256_pd(vec_cmp));\n        if (mask != 0) {\n            alignas(32) uint64_t indices[4];\n            _mm256_store_si256((__m256i*)indices, vec_i);\n\n            for (int j = 0; j < 4; ++j) {\n                if (indices[j] > 0 && n % indices[j] == 0) {\n                    return indices[j];\n                }\n            }\n        }\n\n        vec_i = _mm256_sub_epi64(vec_i, _mm256_set1_epi64x(4));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Handle factor of 2 separately\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    while (_mm256_movemask_epi8(_mm256_cmpeq_epi32(_mm256_and_si256(vec_n, vec_two), vec_zero)) == 0) {\n        factors.push_back(2);\n        n /= 2;\n        vec_n = _mm256_set1_epi32(n);\n    }\n\n    // Handle odd factors from 3 onwards\n    int factor = 3;\n    __m256i vec_factor = _mm256_set1_epi32(factor);\n    while (factor * factor <= n) {\n        __m256i vec_remainder = _mm256_set1_epi32(n % factor);\n        while (_mm256_movemask_epi8(_mm256_cmpeq_epi32(vec_remainder, vec_zero)) == 0) {\n            factors.push_back(factor);\n            n /= factor;\n            vec_n = _mm256_set1_epi32(n);\n            vec_remainder = _mm256_set1_epi32(n % factor);\n        }\n        factor += 2;\n        vec_factor = _mm256_set1_epi32(factor);\n    }\n\n    // If remaining n is a prime number greater than 2\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Handle factor of 2 separately\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    while (_mm256_extract_epi32(vec_n, 0) % 2 == 0) {\n        factors.push_back(2);\n        vec_n = _mm256_div_epi32(vec_n, vec_two);\n    }\n\n    // Handle odd factors\n    __m256i vec_i = _mm256_set1_epi32(3);\n    __m256i vec_limit = _mm256_set1_epi32(static_cast<int>(sqrt(n)));\n\n    while (_mm256_extract_epi32(vec_i, 0) <= _mm256_extract_epi32(vec_limit, 0)) {\n        __m256i vec_mod = _mm256_rem_epi32(vec_n, vec_i);\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(vec_mod, vec_zero)) != 0) {\n            factors.push_back(_mm256_extract_epi32(vec_i, 0));\n            vec_n = _mm256_div_epi32(vec_n, vec_i);\n        } else {\n            vec_i = _mm256_add_epi32(vec_i, _mm256_set1_epi32(2));\n        }\n    }\n\n    // If remaining n is greater than 1, it is a prime factor\n    if (_mm256_extract_epi32(vec_n, 0) > 1) {\n        factors.push_back(_mm256_extract_epi32(vec_n, 0));\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Handle factor of 2 separately\n    __m256i vec_two = _mm256_set1_epi32(2);\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Process odd factors using AVX2\n    int limit = static_cast<int>(sqrt(n)) + 1;\n    for (int i = 3; i <= limit; i += 2) {\n        __m256i vec_i = _mm256_set1_epi32(i);\n        while (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        }\n    }\n\n    // If n is still greater than 2, it must be prime\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_divisor = _mm256_set1_epi32(2);\n\n    while (true) {\n        __m256i vec_mod = _mm256_setzero_si256();\n        __m256i vec_temp = _mm256_setzero_si256();\n        int mask = 0;\n\n        for (int i = 0; i < 8; ++i) {\n            int divisor = _mm256_extract_epi32(vec_divisor, i);\n            if (divisor * divisor > n) break;\n\n            int mod = n % divisor;\n            vec_mod = _mm256_insert_epi32(vec_mod, mod, i);\n            vec_temp = _mm256_insert_epi32(vec_temp, divisor, i);\n        }\n\n        mask = _mm256_movemask_epi8(_mm256_cmpeq_epi32(vec_mod, _mm256_setzero_si256()));\n        if (mask == 0) break;\n\n        for (int i = 0; i < 8; ++i) {\n            if (mask & (1 << i)) {\n                int divisor = _mm256_extract_epi32(vec_temp, i);\n                while (n % divisor == 0) {\n                    factors.push_back(divisor);\n                    n /= divisor;\n                }\n            }\n        }\n\n        vec_divisor = _mm256_add_epi32(vec_divisor, _mm256_set1_epi32(1));\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    // Handle factor of 2 separately\n    __m256i vec_two = _mm256_set1_epi32(2);\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Handle odd factors using AVX2\n    int limit = static_cast<int>(sqrt(n)) + 1;\n    for (int i = 3; i <= limit; i += 2) {\n        __m256i vec_i = _mm256_set1_epi32(i);\n        __m256i vec_n = _mm256_set1_epi32(n);\n        __m256i vec_mod = _mm256_setzero_si256();\n\n        // Check divisibility in parallel\n        while (true) {\n            __m256i vec_rem = _mm256_rem_epi32(vec_n, vec_i);\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(vec_rem));\n            if (mask != 0) break; // No divisible elements\n\n            factors.push_back(i);\n            n /= i;\n            vec_n = _mm256_set1_epi32(n);\n        }\n    }\n\n    // If remaining n is a prime number greater than 2\n    if (n > 2) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n\n    // Process 8 floats at a time using AVX\n    int i = 0;\n    int size = l.size();\n    __m256 zero = _mm256_setzero_ps();\n\n    for (; i <= size - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);       // Load 8 floats\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ); // Compare > 0\n        __m256 positive = _mm256_and_ps(vec, mask);         // Zero out negatives\n\n        // Store positive values into result\n        alignas(32) float temp[8];\n        _mm256_store_ps(temp, positive);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] > 0.0f) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n\n    __m256 zero = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        // Store positive values into the result vector\n        float buffer[8];\n        _mm256_storeu_ps(buffer, vec);\n\n        int mask_bits = _mm256_movemask_ps(mask);\n        for (int j = 0; j < 8; ++j) {\n            if (mask_bits & (1 << j)) {\n                result.push_back(buffer[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < l.size(); ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n\n    __m256 zero = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]); // Load 8 floats\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ); // Compare > 0\n        int mask_bits = _mm256_movemask_ps(mask); // Get comparison results as bitmask\n\n        // Extract positive values based on mask\n        if (mask_bits & 0x01) result.push_back(l[i]);\n        if (mask_bits & 0x02) result.push_back(l[i + 1]);\n        if (mask_bits & 0x04) result.push_back(l[i + 2]);\n        if (mask_bits & 0x08) result.push_back(l[i + 3]);\n        if (mask_bits & 0x10) result.push_back(l[i + 4]);\n        if (mask_bits & 0x20) result.push_back(l[i + 5]);\n        if (mask_bits & 0x40) result.push_back(l[i + 6]);\n        if (mask_bits & 0x80) result.push_back(l[i + 7]);\n    }\n\n    // Process remaining elements\n    for (; i < l.size(); ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n\n    __m256 zero = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time\n    for (; i + 7 < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        // Blend positive values with zero\n        __m256 positives = _mm256_and_ps(vec, mask);\n\n        // Store results temporarily\n        alignas(32) float temp[8];\n        _mm256_store_ps(temp, positives);\n\n        // Add non-zero values to the result vector\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] > 0.0f) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < l.size(); ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l) {\n    std::vector<float> result;\n    result.reserve(l.size());\n\n    __m256 zero = _mm256_setzero_ps();\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX\n    for (; i + 7 < l.size(); i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        // Blend positive values with zero for masking\n        __m256 positives = _mm256_and_ps(mask, vec);\n\n        // Store results temporarily to extract valid values\n        alignas(32) float temp[8];\n        _mm256_store_ps(temp, positives);\n\n        // Add positive values to the result vector\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] > 0.0f) {\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < l.size(); ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = _mm_cvtsi128_si64(_mm_sqrt_epi64(_mm_set1_epi64x(n))) + 1;\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_mod1 = _mm256_rem_epi64(vec_n, vec_i);\n        __m256i vec_mod2 = _mm256_rem_epi64(vec_n, _mm256_add_epi64(vec_i, _mm256_set1_epi64x(2)));\n\n        __m256i cmp1 = _mm256_cmpeq_epi64(vec_mod1, vec_zero);\n        __m256i cmp2 = _mm256_cmpeq_epi64(vec_mod2, vec_zero);\n\n        if (!_mm256_testz_si256(cmp1, vec_one) || !_mm256_testz_si256(cmp2, vec_one)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = _mm_cvtsi128_si64(_mm_sqrt_epi64(_mm_set1_epi64x(n))) + 1;\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_i_plus_2 = _mm256_add_epi64(vec_i, _mm256_set1_epi64x(2));\n\n        // Check divisibility by i\n        __m256i mod_i = _mm256_rem_epi64(vec_n, vec_i);\n        __m256i cmp_i = _mm256_cmpeq_epi64(mod_i, vec_zero);\n        if (!_mm256_testz_si256(cmp_i, cmp_i)) return false;\n\n        // Check divisibility by i + 2\n        __m256i mod_i_plus_2 = _mm256_rem_epi64(vec_n, vec_i_plus_2);\n        __m256i cmp_i_plus_2 = _mm256_cmpeq_epi64(mod_i_plus_2, vec_zero);\n        if (!_mm256_testz_si256(cmp_i_plus_2, cmp_i_plus_2)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = _mm_cvtsi128_si64(_mm_sqrt_epi64(_mm_set1_epi64x(n))) + 1;\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_mod1 = _mm256_rem_epi64(vec_n, vec_i);\n        __m256i vec_mod2 = _mm256_rem_epi64(vec_n, _mm256_add_epi64(vec_i, _mm256_set1_epi64x(2)));\n\n        __m256i cmp1 = _mm256_cmpeq_epi64(vec_mod1, vec_zero);\n        __m256i cmp2 = _mm256_cmpeq_epi64(vec_mod2, vec_zero);\n\n        if (!_mm256_testz_si256(cmp1, vec_one) || !_mm256_testz_si256(cmp2, vec_one)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t limit = _mm_cvtsi128_si64(_mm_sqrt_epi64(_mm_set1_epi64x(n))) + 1;\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int64_t i = 5; i <= limit; i += 6) {\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_mod1 = _mm256_rem_epi64(vec_n, vec_i);\n        __m256i vec_mod2 = _mm256_rem_epi64(vec_n, _mm256_add_epi64(vec_i, vec_one));\n\n        __m256i cmp1 = _mm256_cmpeq_epi64(vec_mod1, vec_zero);\n        __m256i cmp2 = _mm256_cmpeq_epi64(vec_mod2, vec_zero);\n\n        if (!_mm256_testz_si256(cmp1, cmp1) || !_mm256_testz_si256(cmp2, cmp2)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    int64_t sqrt_n = _mm_cvtsi128_si64(_mm_sqrt_epi64(_mm_set_epi64x(n, 0)));\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    for (int64_t i = 5; i <= sqrt_n; i += 6) {\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_mod1 = _mm256_rem_epi64(vec_n, vec_i);\n        __m256i vec_mod2 = _mm256_rem_epi64(vec_n, _mm256_add_epi64(vec_i, _mm256_set1_epi64x(2)));\n\n        __m256i cmp1 = _mm256_cmpeq_epi64(vec_mod1, vec_zero);\n        __m256i cmp2 = _mm256_cmpeq_epi64(vec_mod2, vec_zero);\n\n        if (!_mm256_testz_si256(cmp1, vec_one) || !_mm256_testz_si256(cmp2, vec_one)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(s);\n    bool hasLetter = false;\n\n    // Check if the string contains any letters\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            hasLetter = true;\n            break;\n        }\n    }\n\n    if (!hasLetter) {\n        // Reverse the string if no letters are found\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    size_t i = 0;\n    __m256i v_lower = _mm256_set1_epi8('a');\n    __m256i v_upper = _mm256_set1_epi8('A');\n    __m256i v_alpha_mask = _mm256_set1_epi8(0x20); // Mask to toggle case\n\n    // Process the string in chunks of 32 bytes using AVX2\n    for (; i + 32 <= len; i += 32) {\n        __m256i v_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create masks for lowercase and uppercase letters\n        __m256i v_is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(v_chars, v_lower - 1), _mm256_cmplt_epi8(v_chars, _mm256_set1_epi8('z' + 1)));\n        __m256i v_is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(v_chars, v_upper - 1), _mm256_cmplt_epi8(v_chars, _mm256_set1_epi8('Z' + 1)));\n\n        // Combine masks to identify alphabetic characters\n        __m256i v_is_alpha = _mm256_or_si256(v_is_lower, v_is_upper);\n\n        // Toggle case for alphabetic characters\n        __m256i v_toggle = _mm256_blendv_epi8(_mm256_setzero_si256(), v_alpha_mask, v_is_alpha);\n        v_chars = _mm256_xor_si256(v_chars, v_toggle);\n\n        // Store the result back to the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), v_chars);\n    }\n\n    // Process remaining characters (if any)\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = c - ('a' - 'A');\n        } else if (c >= 'A' && c <= 'Z') {\n            result[i] = c + ('a' - 'A');\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    int n = s.size();\n    std::string result = s;\n    bool hasLetter = false;\n\n    // Check if the string contains any letters\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            hasLetter = true;\n            break;\n        }\n    }\n\n    if (!hasLetter) {\n        // Reverse the string if no letters are found\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 32 bytes using AVX2\n    int i = 0;\n    for (; i <= n - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create masks for lowercase and uppercase letters\n        __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n        __m256i upper_mask = _mm256_set1_epi8('z' + 1);\n        __m256i lower_check = _mm256_cmpgt_epi8(chunk, lower_mask);\n        __m256i upper_check = _mm256_cmplt_epi8(chunk, upper_mask);\n        __m256i is_lowercase = _mm256_and_si256(lower_check, upper_check);\n\n        lower_mask = _mm256_set1_epi8('A' - 1);\n        upper_mask = _mm256_set1_epi8('Z' + 1);\n        lower_check = _mm256_cmpgt_epi8(chunk, lower_mask);\n        upper_check = _mm256_cmplt_epi8(chunk, upper_mask);\n        __m256i is_uppercase = _mm256_and_si256(lower_check, upper_check);\n\n        // Toggle case for letters\n        __m256i toggle_mask = _mm256_set1_epi8(0x20);\n        __m256i toggled_chunk = _mm256_xor_si256(chunk, toggle_mask);\n\n        // Combine results based on letter detection\n        __m256i is_letter = _mm256_or_si256(is_lowercase, is_uppercase);\n        chunk = _mm256_blendv_epi8(chunk, toggled_chunk, is_letter);\n\n        // Store the processed chunk back into the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), chunk);\n    }\n\n    // Process remaining characters (if any)\n    for (; i < n; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = c - ('a' - 'A');\n        } else if (c >= 'A' && c <= 'Z') {\n            result[i] = c + ('a' - 'A');\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(s);\n    bool has_letter = false;\n\n    // Check if the string contains any letters\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        // Reverse the string if no letters are found\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    size_t i = 0;\n    __m256i mask_lower = _mm256_set1_epi8('a' - 1);\n    __m256i mask_upper = _mm256_set1_epi8('z' + 1);\n    __m256i flip_mask = _mm256_set1_epi8(0x20);\n\n    // Process the string in chunks of 32 bytes using AVX2\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create masks to identify lowercase and uppercase letters\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, mask_lower), _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('z' + 1)));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)), _mm256_cmplt_epi8(chunk, _mm256_set1_epi8('Z' + 1)));\n\n        // Combine masks to identify all alphabetic characters\n        __m256i is_alpha = _mm256_or_si256(is_lower, is_upper);\n\n        // Flip case for alphabetic characters\n        __m256i flipped = _mm256_xor_si256(chunk, _mm256_and_si256(is_alpha, flip_mask));\n\n        // Store the result back into the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), flipped);\n    }\n\n    // Process remaining characters that didn't fit into a full 32-byte chunk\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = c - 0x20; // Convert to uppercase\n        } else if (c >= 'A' && c <= 'Z') {\n            result[i] = c + 0x20; // Convert to lowercase\n        } else {\n            result[i] = c; // Keep non-alphabetic characters unchanged\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    int n = s.size();\n    std::string result = s;\n    bool hasLetter = false;\n\n    // Check if the string contains any letters\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            hasLetter = true;\n            break;\n        }\n    }\n\n    if (!hasLetter) {\n        // Reverse the string if no letters are found\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 32 bytes using AVX2\n    int i = 0;\n    for (; i + 32 <= n; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create masks for lowercase and uppercase letters\n        __m256i lowercaseMask = _mm256_set1_epi8(0x20);\n        __m256i letterMask = _mm256_set1_epi8(0xDF);\n        __m256i alphaMask = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n                                              _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk));\n\n        // Toggle case for alphabetic characters\n        __m256i toggledCase = _mm256_xor_si256(chunk, _mm256_and_si256(lowercaseMask, alphaMask));\n\n        // Blend original and toggled case based on alphabetic mask\n        __m256i finalChunk = _mm256_blendv_epi8(chunk, toggledCase, alphaMask);\n\n        // Store the result back to the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), finalChunk);\n    }\n\n    // Process remaining characters (if any)\n    for (; i < n; ++i) {\n        char c = s[i];\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            result[i] ^= 0x20;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s) {\n    int len = s.size();\n    std::string result(s);\n    bool hasLetter = false;\n\n    // Check if the string contains any letters\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            hasLetter = true;\n            break;\n        }\n    }\n\n    if (!hasLetter) {\n        // Reverse the string if no letters are found\n        std::reverse(result.begin(), result.end());\n        return result;\n    }\n\n    // Process the string in chunks of 32 bytes using AVX2\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create masks for lowercase and uppercase letters\n        __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n        __m256i upper_mask = _mm256_set1_epi8('z' + 1);\n        __m256i lower_check = _mm256_cmpgt_epi8(chunk, lower_mask);\n        __m256i upper_check = _mm256_cmplt_epi8(chunk, upper_mask);\n        __m256i is_lowercase = _mm256_and_si256(lower_check, upper_check);\n\n        lower_mask = _mm256_set1_epi8('A' - 1);\n        upper_mask = _mm256_set1_epi8('Z' + 1);\n        lower_check = _mm256_cmpgt_epi8(chunk, lower_mask);\n        upper_check = _mm256_cmplt_epi8(chunk, upper_mask);\n        __m256i is_uppercase = _mm256_and_si256(lower_check, upper_check);\n\n        __m256i is_letter = _mm256_or_si256(is_lowercase, is_uppercase);\n\n        // Toggle case for letters\n        __m256i toggle_mask = _mm256_set1_epi8(0x20);\n        __m256i toggled_chunk = _mm256_xor_si256(chunk, toggle_mask);\n        __m256i final_chunk = _mm256_blendv_epi8(chunk, toggled_chunk, is_letter);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final_chunk);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = c - ('a' - 'A');\n        } else if (c >= 'A' && c <= 'Z') {\n            result[i] = c + ('a' - 'A');\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> indices;\n    std::vector<int> values;\n\n    // Extract indices divisible by 3 and their corresponding values\n    for (int i = 0; i < n; i += 3) {\n        indices.push_back(i);\n        values.push_back(l[i]);\n    }\n\n    // Sort the extracted values\n    std::sort(values.begin(), values.end());\n\n    // Use AVX2 to parallelize the update of values at indices divisible by 3\n    int m = values.size();\n    int simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    int simd_aligned_size = (m + simd_width - 1) / simd_width * simd_width;\n\n    // Pad values and indices to make them SIMD-aligned\n    values.resize(simd_aligned_size, 0);\n    std::vector<int> padded_indices(indices.size(), 0);\n    for (int i = 0; i < indices.size(); ++i) {\n        padded_indices[i] = indices[i];\n    }\n    padded_indices.resize(simd_aligned_size, 0);\n\n    // Load values into SIMD registers and scatter them back to the original vector\n    for (int i = 0; i < simd_aligned_size; i += simd_width) {\n        __m256i simd_values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&values[i]));\n        __m256i simd_indices = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_indices[i]));\n\n        // Scatter values back to the original vector using scalar fallback for simplicity\n        for (int j = 0; j < simd_width; ++j) {\n            int idx = padded_indices[i + j];\n            if (idx < n) {\n                l[idx] = values[i + j];\n            }\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result = l;\n\n    // Extract elements at indices divisible by 3\n    std::vector<int> divisible_by_three;\n    for (int i = 0; i < n; i += 3) {\n        divisible_by_three.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(divisible_by_three.begin(), divisible_by_three.end());\n\n    // Replace the original indices divisible by 3 with sorted values\n    int idx = 0;\n    for (int i = 0; i < n; i += 3) {\n        result[i] = divisible_by_three[idx++];\n    }\n\n    // Use AVX2 to process the vector in parallel\n    for (int i = 0; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i res = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&result[i]));\n\n        // Create a mask for indices divisible by 3\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i mod3 = _mm256_set1_epi32(3);\n        __m256i remainder = _mm256_rem_epi32(indices, mod3);\n        __m256i mask = _mm256_cmpeq_epi32(remainder, _mm256_setzero_si256());\n\n        // Blend the original and sorted values based on the mask\n        res = _mm256_blendv_epi8(vec, res, mask);\n\n        // Store the result back\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), res);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> indices;\n    std::vector<int> values;\n\n    // Collect indices and values divisible by 3\n    for (int i = 0; i < n; i += 3) {\n        indices.push_back(i);\n        values.push_back(l[i]);\n    }\n\n    // Sort the collected values\n    std::sort(values.begin(), values.end());\n\n    // Use AVX2 to efficiently update the values at indices divisible by 3\n    int m = values.size();\n    for (int i = 0; i + 8 <= m; i += 8) {\n        __m256i sorted_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&values[i]));\n        for (int j = 0; j < 8; ++j) {\n            l[indices[i + j]] = sorted_vals[j];\n        }\n    }\n\n    // Handle remaining elements that are not a multiple of 8\n    for (int i = m - (m % 8); i < m; ++i) {\n        l[indices[i]] = values[i];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> result = l;\n\n    // Extract elements at indices divisible by 3\n    std::vector<int> third_elements;\n    for (int i = 0; i < n; i += 3) {\n        third_elements.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(third_elements.begin(), third_elements.end());\n\n    // Replace the original indices divisible by 3 with sorted values\n    int k = 0;\n    for (int i = 0; i < n; i += 3) {\n        result[i] = third_elements[k++];\n    }\n\n    // Use AVX2 to process the vector in parallel\n    __m256i mask = _mm256_set_epi32(0, 1, 1, 1, 0, 1, 1, 1); // Mask for indices divisible by 3\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i res = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&result[i]));\n        __m256i blend = _mm256_blendv_epi8(vec, res, mask);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), blend);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l) {\n    int n = l.size();\n    std::vector<int> indices;\n    std::vector<int> values;\n\n    // Extract elements at indices divisible by 3\n    for (int i = 0; i < n; i += 3) {\n        indices.push_back(i);\n        values.push_back(l[i]);\n    }\n\n    // Sort the extracted values\n    std::sort(values.begin(), values.end());\n\n    // Use AVX2 to parallelize the update of the original vector\n    int m = values.size();\n    int simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    int simd_aligned_size = (m + simd_width - 1) / simd_width * simd_width;\n\n    // Create aligned buffers for SIMD operations\n    __m256i* simd_values = (__m256i*)_mm_malloc(simd_aligned_size * sizeof(int), 32);\n    __m256i* simd_indices = (__m256i*)_mm_malloc(simd_aligned_size * sizeof(int), 32);\n\n    // Fill the SIMD buffers with values and indices, padding with zeros if necessary\n    for (int i = 0; i < m; ++i) {\n        ((int*)simd_values)[i] = values[i];\n        ((int*)simd_indices)[i] = indices[i];\n    }\n    for (int i = m; i < simd_aligned_size; ++i) {\n        ((int*)simd_values)[i] = 0;\n        ((int*)simd_indices)[i] = 0;\n    }\n\n    // Perform SIMD-based updates\n    for (int i = 0; i < simd_aligned_size; i += simd_width) {\n        __m256i idx_vec = _mm256_load_si256(&simd_indices[i / simd_width]);\n        __m256i val_vec = _mm256_load_si256(&simd_values[i / simd_width]);\n\n        // Scatter values back into the original vector using indices\n        for (int j = 0; j < simd_width; ++j) {\n            int idx = ((int*)&idx_vec)[j];\n            if (idx < n) { // Ensure index is within bounds\n                l[idx] = ((int*)&val_vec)[j];\n            }\n        }\n    }\n\n    // Free allocated memory\n    _mm_free(simd_values);\n    _mm_free(simd_indices);\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    int size = l.size();\n    float result = l[0];\n    __m256 max_vec = _mm256_set1_ps(result);\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > result) {\n            result = temp[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    int size = l.size();\n    float result = l[0];\n    __m256 max_vec = _mm256_set1_ps(result);\n\n    int i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > result) {\n            result = temp[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    int size = l.size();\n    float result = l[0];\n    __m256 max_vec = _mm256_set1_ps(result);\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > result) {\n            result = temp[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    int size = l.size();\n    float result = l[0];\n    __m256 max_vec = _mm256_set1_ps(result);\n\n    int i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > result) {\n            result = temp[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > result) {\n            result = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l) {\n    __m256 max_vec = _mm256_set1_ps(l[0]);\n    int i = 0;\n    int size = l.size();\n\n    for (; i + 7 < size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_vec = _mm256_max_ps(max_vec, vec);\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, max_vec);\n\n    float max_val = temp[0];\n    for (int j = 1; j < 8; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    const __m256i sevens = _mm256_set1_epi8('7');\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i eleven = _mm256_set1_epi32(11);\n    const __m256i thirteen = _mm256_set1_epi32(13);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i nums = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i div11 = _mm256_cmpeq_epi32(_mm256_rem_epi32(nums, eleven), zero);\n        __m256i div13 = _mm256_cmpeq_epi32(_mm256_rem_epi32(nums, thirteen), zero);\n        __m256i mask = _mm256_or_si256(div11, div13);\n\n        for (int j = 0; j < 8; ++j) {\n            if (i + j >= n) break;\n            if (mask.m256i_i32[j]) {\n                char buffer[16];\n                sprintf(buffer, \"%d\", i + j);\n                int len = strlen(buffer);\n                for (int k = 0; k < len; ++k) {\n                    if (buffer[k] == '7') {\n                        count++;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i sevens = _mm256_set1_epi8('7');\n    __m256i zeros = _mm256_set1_epi8(0);\n    char buffer[32];\n\n    for (int i = 0; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int num = i;\n            int digits = 0;\n\n            // Convert number to string\n            do {\n                buffer[digits++] = '0' + (num % 10);\n                num /= 10;\n            } while (num > 0);\n\n            // Pad the buffer with null characters to ensure it's a multiple of 32 bytes\n            for (int j = digits; j < 32; j++) {\n                buffer[j] = 0;\n            }\n\n            // Load 32 characters into AVX register\n            __m256i chars = _mm256_loadu_si256((__m256i*)buffer);\n\n            // Compare each character with '7'\n            __m256i cmp = _mm256_cmpeq_epi8(chars, sevens);\n\n            // Count the number of matches\n            int mask = _mm256_movemask_epi8(cmp);\n            count += _mm_popcnt_u32(mask);\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i sevens = _mm256_set1_epi8('7');\n    __m256i zeros = _mm256_set1_epi8(0);\n    char buffer[32];\n\n    for (int i = 0; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int len = sprintf(buffer, \"%d\", i);\n            int j = 0;\n\n            while (j + 32 <= len) {\n                __m256i strVec = _mm256_loadu_si256((__m256i*)&buffer[j]);\n                __m256i cmp = _mm256_cmpeq_epi8(strVec, sevens);\n                int mask = _mm256_movemask_epi8(cmp);\n                count += _mm_popcnt_u32(mask);\n                j += 32;\n            }\n\n            while (j < len) {\n                if (buffer[j] == '7') {\n                    count++;\n                }\n                j++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i sevens = _mm256_set1_epi8('7');\n    __m256i zeros = _mm256_set1_epi8(0);\n    char buffer[32];\n\n    for (int i = 0; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            int len = sprintf(buffer, \"%d\", i);\n            int j = 0;\n\n            while (j + 32 <= len) {\n                __m256i strVec = _mm256_loadu_si256((__m256i*)&buffer[j]);\n                __m256i cmp = _mm256_cmpeq_epi8(strVec, sevens);\n                int mask = _mm256_movemask_epi8(cmp);\n                count += _mm_popcnt_u32(mask);\n                j += 32;\n            }\n\n            while (j < len) {\n                if (buffer[j] == '7') {\n                    count++;\n                }\n                j++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i sevens = _mm256_set1_epi8('7'); // Vector filled with '7'\n    __m256i zeros = _mm256_set1_epi8(0);    // Vector filled with zeros\n\n    for (int i = 0; i < n; i++) {\n        if (i % 11 == 0 || i % 13 == 0) {\n            char buffer[32];\n            int len = sprintf(buffer, \"%d\", i);\n\n            // Process the number in chunks of 32 bytes using AVX2\n            int j = 0;\n            for (; j + 32 <= len; j += 32) {\n                __m256i chars = _mm256_loadu_si256((__m256i*)(buffer + j));\n                __m256i cmp = _mm256_cmpeq_epi8(chars, sevens);\n                int mask = _mm256_movemask_epi8(cmp);\n                count += _mm_popcnt_u32(mask);\n            }\n\n            // Handle remaining characters (less than 32 bytes)\n            for (; j < len; ++j) {\n                if (buffer[j] == '7') {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n < 2) return l;\n\n    // Ensure the size is a multiple of 8 for AVX processing\n    int simd_size = 8;\n    int padded_size = ((n + simd_size - 1) / simd_size) * simd_size;\n    std::vector<float> padded_l(padded_size, 0.0f);\n    std::copy(l.begin(), l.end(), padded_l.begin());\n\n    // Extract even indices into a separate array\n    std::vector<float> even_values((padded_size + 1) / 2);\n    for (int i = 0; i < n; i += 2) {\n        even_values[i / 2] = l[i];\n    }\n\n    // Sort the even indices\n    std::sort(even_values.begin(), even_values.end());\n\n    // Use AVX to merge sorted even values back into the original vector\n    __m256 even_vec, result_vec;\n    for (int i = 0; i < padded_size; i += simd_size) {\n        // Load 8 elements from the padded vector\n        __m256 original_vec = _mm256_loadu_ps(&padded_l[i]);\n\n        // Create a mask for even indices: 11111111 for even, 00000000 for odd\n        __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n\n        // Load sorted even values into an AVX register\n        float even_temp[8];\n        for (int j = 0; j < 8; ++j) {\n            even_temp[j] = (i + j < n && (i + j) % 2 == 0) ? even_values[(i + j) / 2] : 0.0f;\n        }\n        even_vec = _mm256_loadu_ps(even_temp);\n\n        // Blend the original vector with the sorted even values\n        result_vec = _mm256_blendv_ps(original_vec, even_vec, _mm256_castsi256_ps(mask));\n\n        // Store the result back into the padded vector\n        _mm256_storeu_ps(&padded_l[i], result_vec);\n    }\n\n    // Copy the result back to the original vector\n    std::copy(padded_l.begin(), padded_l.begin() + n, l.begin());\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    int even_count = (n + 1) / 2;\n    std::vector<float> even_elements(even_count);\n\n    // Extract even-indexed elements\n    for (int i = 0, j = 0; i < n; i += 2, ++j) {\n        even_elements[j] = l[i];\n    }\n\n    // Sort the even-indexed elements\n    std::sort(even_elements.begin(), even_elements.end());\n\n    // Reinsert sorted even-indexed elements using AVX/AVX2\n    int avx_iters = even_count / 8;\n    int remainder = even_count % 8;\n\n    __m256 vec_sorted;\n    for (int i = 0, j = 0; i < avx_iters; ++i, j += 8) {\n        vec_sorted = _mm256_loadu_ps(&even_elements[j]);\n        _mm256_storeu_ps(&l[j * 2], vec_sorted);\n    }\n\n    // Handle remaining elements\n    for (int i = avx_iters * 8, j = i * 2; i < even_count; ++i, j += 2) {\n        l[j] = even_elements[i];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    int even_count = (n + 1) / 2;\n    std::vector<float> even_elements(even_count);\n\n    // Extract even-indexed elements\n    for (int i = 0, j = 0; i < n; i += 2, ++j) {\n        even_elements[j] = l[i];\n    }\n\n    // Sort the even-indexed elements\n    std::sort(even_elements.begin(), even_elements.end());\n\n    // Use AVX to merge sorted even elements back into the original vector\n    __m256 vec_sorted, vec_original;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        // Load 8 elements from the original vector\n        vec_original = _mm256_loadu_ps(&l[i]);\n\n        // Create a mask to select even indices: 1101101101101101 (0xD5D5)\n        __m256i mask = _mm256_setr_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        __m256 vec_mask = _mm256_castsi256_ps(mask);\n\n        // Load sorted even elements into a vector\n        vec_sorted = _mm256_setzero_ps();\n        for (int j = 0; j < 4; ++j) {\n            vec_sorted = _mm256_insert_ps(vec_sorted, _mm_set_ss(even_elements[(i / 2) + j]), j * 0x20);\n        }\n\n        // Blend sorted even elements with original odd elements\n        vec_original = _mm256_blendv_ps(vec_original, vec_sorted, vec_mask);\n\n        // Store the result back into the original vector\n        _mm256_storeu_ps(&l[i], vec_original);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < n; i += 2) {\n        l[i] = even_elements[i / 2];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    if (n <= 1) return l;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_elements;\n    for (int i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    // Sort the even elements\n    std::sort(even_elements.begin(), even_elements.end());\n\n    // Use AVX to merge sorted even elements back into the original vector\n    int even_idx = 0;\n    for (int i = 0; i < n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]); // Load 8 floats from l\n\n        // Create a mask for even indices: 11111111 00000000 11111111 00000000 ...\n        __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n\n        // Load sorted even values into a vector\n        __m256 even_vals = _mm256_setzero_ps();\n        for (int j = 0; j < 8; j += 2) {\n            if (i + j < n && (i + j) % 2 == 0 && even_idx < even_elements.size()) {\n                even_vals = _mm256_insert_ps(even_vals, _mm_set_ss(even_elements[even_idx++]), j * 0x20);\n            }\n        }\n\n        // Blend the original vector with the sorted even values\n        vec = _mm256_blendv_ps(vec, even_vals, _mm256_castsi256_ps(mask));\n\n        // Store the result back to the original vector\n        _mm256_storeu_ps(&l[i], vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    int n = l.size();\n    int even_count = (n + 1) / 2;\n    std::vector<float> even_elements(even_count);\n\n    // Extract even-indexed elements\n    for (int i = 0, j = 0; i < n; i += 2, ++j) {\n        even_elements[j] = l[i];\n    }\n\n    // Sort the even-indexed elements\n    std::sort(even_elements.begin(), even_elements.end());\n\n    // Use AVX to merge sorted even elements back into the original vector\n    __m256 vec_even, vec_original;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        // Load 8 elements from the original vector\n        vec_original = _mm256_loadu_ps(&l[i]);\n\n        // Prepare a vector of sorted even elements\n        float even_vals[8];\n        for (int j = 0; j < 8; ++j) {\n            even_vals[j] = (j % 2 == 0) ? even_elements[j / 2 + i / 2] : 0.0f;\n        }\n        vec_even = _mm256_loadu_ps(even_vals);\n\n        // Blend: Keep odd indices from vec_original and even indices from vec_even\n        __m256 blend_mask = _mm256_setr_ps(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256 blended = _mm256_blendv_ps(vec_original, vec_even, blend_mask);\n\n        // Store the result back\n        _mm256_storeu_ps(&l[i], blended);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < n; i += 2) {\n        l[i] = even_elements[i / 2];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    // Align the vector to 32 bytes for AVX operations\n    __m256i* data = (__m256i*)__builtin_assume_aligned(l.data(), 32);\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            int target = -(l[i] + l[j]);\n            int k = j + 1;\n\n            // Process 8 elements at a time using AVX2\n            while (k <= n - 8) {\n                __m256i vec = _mm256_loadu_si256(&data[k / 8]);\n                __m256i cmp = _mm256_set1_epi32(target);\n                __m256i mask = _mm256_cmpeq_epi32(vec, cmp);\n\n                int match = _mm256_movemask_epi8(mask);\n                if (match != 0) {\n                    return true;\n                }\n                k += 8;\n            }\n\n            // Check remaining elements\n            while (k < n) {\n                if (l[k] == target) {\n                    return true;\n                }\n                ++k;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    // Align the vector to 32 bytes for AVX operations\n    __m256i* data = (__m256i*)__builtin_assume_aligned(l.data(), 32);\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            int target = -(l[i] + l[j]);\n            int k = j + 1;\n\n            // Process 8 elements at a time using AVX2\n            while (k <= n - 8) {\n                __m256i vec = _mm256_loadu_si256(&data[k / 8]);\n                __m256i cmp = _mm256_set1_epi32(target);\n                __m256i mask = _mm256_cmpeq_epi32(vec, cmp);\n\n                int match = _mm256_movemask_epi8(mask);\n                if (match != 0) {\n                    return true;\n                }\n                k += 8;\n            }\n\n            // Handle remaining elements\n            while (k < n) {\n                if (l[k] == target) {\n                    return true;\n                }\n                ++k;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    // Align the vector to 32 bytes for AVX operations\n    __m256i* data = (__m256i*)__builtin_assume_aligned(l.data(), 32);\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            __m256i vi = _mm256_set1_epi32(l[i]);\n            __m256i vj = _mm256_set1_epi32(l[j]);\n            __m256i target = _mm256_set1_epi32(-(l[i] + l[j]));\n\n            for (int k = j + 1; k <= n - 8; k += 8) {\n                __m256i vk = _mm256_loadu_si256(&data[k / 8]);\n                __m256i cmp = _mm256_cmpeq_epi32(vk, target);\n                int mask = _mm256_movemask_epi8(cmp);\n                if (mask != 0) {\n                    return true;\n                }\n            }\n\n            // Handle remaining elements\n            for (int k = (n / 8) * 8; k < n; ++k) {\n                if (l[k] == -(l[i] + l[j])) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    // Align the vector to 32 bytes for AVX operations\n    __m256i* data = (__m256i*)__builtin_assume_aligned(l.data(), 32);\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            int target = -(l[i] + l[j]);\n            int k = j + 1;\n\n            // Process 8 elements at a time using AVX2\n            while (k <= n - 8) {\n                __m256i vec = _mm256_loadu_si256(&data[k / 8]);\n                __m256i cmp = _mm256_set1_epi32(target);\n                __m256i mask = _mm256_cmpeq_epi32(vec, cmp);\n\n                int mask_result = _mm256_movemask_epi8(mask);\n                if (mask_result != 0) {\n                    return true;\n                }\n\n                k += 8;\n            }\n\n            // Process remaining elements\n            for (; k < n; ++k) {\n                if (l[k] == target) {\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 3) return false;\n\n    // Align the vector to 32 bytes for AVX operations\n    __m256i* data = (__m256i*)__builtin_assume_aligned(l.data(), 32);\n\n    for (int i = 0; i < n - 2; ++i) {\n        for (int j = i + 1; j < n - 1; ++j) {\n            int target = -(l[i] + l[j]);\n            int k = j + 1;\n\n            // Process 8 elements at a time using AVX2\n            while (k <= n - 8) {\n                __m256i vec = _mm256_loadu_si256(&data[k / 8]);\n                __m256i cmp = _mm256_set1_epi32(target);\n                __m256i mask = _mm256_cmpeq_epi32(vec, cmp);\n\n                int bitmask = _mm256_movemask_ps((__m256)mask);\n                if (bitmask != 0) {\n                    return true;\n                }\n\n                k += 8;\n            }\n\n            // Check remaining elements\n            while (k < n) {\n                if (l[k] == target) {\n                    return true;\n                }\n                ++k;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t avx_size = 8; // AVX2 processes 8 integers (256 bits / 32 bits per int)\n    __m256i increment = _mm256_set1_epi32(1); // Create a vector with all elements set to 1\n\n    // Process the input vector in chunks of 8 integers\n    size_t i = 0;\n    for (; i + avx_size <= n; i += avx_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i])); // Load 8 integers\n        vec = _mm256_add_epi32(vec, increment); // Add 1 to each integer\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&l[i]), vec); // Store the result back\n    }\n\n    // Process any remaining elements that didn't fit into a full AVX vector\n    for (; i < n; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t size = l.size();\n    size_t simd_size = size - (size % 8);\n    __m256i increment = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < simd_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        vec = _mm256_add_epi32(vec, increment);\n        _mm256_storeu_si256((__m256i*)&l[i], vec);\n    }\n\n    for (size_t i = simd_size; i < size; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t avx_size = 8; // AVX2 processes 8 integers (32-bit) at a time\n    __m256i one = _mm256_set1_epi32(1); // Create a vector of eight 1's\n\n    // Ensure the vector is aligned to 32 bytes for optimal performance\n    std::vector<int> result(n);\n    size_t i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + avx_size <= n; i += avx_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i])); // Load 8 integers\n        vec = _mm256_add_epi32(vec, one); // Add 1 to each element\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vec); // Store the result\n    }\n\n    // Handle remaining elements that are not a multiple of 8\n    for (; i < n; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t avx_size = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    __m256i increment = _mm256_set1_epi32(1); // Create a vector with all elements set to 1\n\n    // Process the input vector in chunks of 8 integers\n    size_t i = 0;\n    for (; i + avx_size <= n; i += avx_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i])); // Load 8 integers\n        vec = _mm256_add_epi32(vec, increment); // Add 1 to each element\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&l[i]), vec); // Store the result back\n    }\n\n    // Process any remaining elements that didn't fit into a full AVX vector\n    for (; i < n; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    size_t n = l.size();\n    size_t avx_size = 8; // AVX2 processes 8 integers (256 bits / 32 bits per int)\n    __m256i increment = _mm256_set1_epi32(1); // Create a vector with all elements set to 1\n\n    // Process the input vector in chunks of 8 integers\n    size_t i = 0;\n    for (; i + avx_size <= n; i += avx_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i])); // Load 8 integers\n        vec = _mm256_add_epi32(vec, increment); // Add 1 to each integer\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&l[i]), vec); // Store the result back\n    }\n\n    // Handle any remaining elements that didn't fit into a full AVX vector\n    for (; i < n; ++i) {\n        l[i] += 1;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_l(padded_size, 0);\n    std::copy(l.begin(), l.end(), padded_l.begin());\n\n    for (int i = 0; i < n; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]); // Broadcast l[i] to all elements of the vector\n\n        for (int j = 0; j < padded_size; j += 8) {\n            __m256i vj = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_l[j])); // Load 8 elements\n            __m256i sum = _mm256_add_epi32(vi, vj); // Add vectors element-wise\n\n            // Check if any element in the sum is zero\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero); // Compare for equality with zero\n            int mask = _mm256_movemask_epi8(cmp); // Create a mask of comparison results\n\n            if (mask != 0) {\n                // Ensure the match is not with the same element\n                for (int k = 0; k < 8; ++k) {\n                    if (j + k != i && l[i] + padded_l[j + k] == 0) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int simd_size = 8;\n    int padded_size = ((n + simd_size - 1) / simd_size) * simd_size;\n    std::vector<int> padded_l(padded_size, 0);\n    std::copy(l.begin(), l.end(), padded_l.begin());\n\n    for (int i = 0; i < n; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]); // Broadcast l[i] to all elements of the vector\n\n        for (int j = 0; j < padded_size; j += simd_size) {\n            __m256i vj = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_l[j])); // Load 8 integers\n            __m256i sum = _mm256_add_epi32(vi, vj); // Add vectors element-wise\n\n            // Check if any element in the sum is zero\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero); // Compare for equality with zero\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp)); // Extract comparison results into a bitmask\n\n            if (mask != 0) {\n                // Ensure the match is not with the same index\n                for (int k = 0; k < simd_size; ++k) {\n                    if ((mask & (1 << k)) && (i != j + k)) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_l(padded_size, 0);\n    std::copy(l.begin(), l.end(), padded_l.begin());\n\n    for (int i = 0; i < n; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]); // Broadcast l[i] to all elements of the vector\n        int j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j <= padded_size - 8; j += 8) {\n            __m256i vj = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_l[j])); // Load 8 integers\n            __m256i sum = _mm256_add_epi32(vi, vj); // Add vectors element-wise\n\n            // Check if any element in the result is zero\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero); // Compare for equality with zero\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp)); // Extract comparison mask\n\n            if (mask != 0) {\n                // Verify that the match is not due to comparing an element with itself\n                for (int k = 0; k < 8; ++k) {\n                    if ((mask & (1 << k)) && (i != j + k)) {\n                        return true;\n                    }\n                }\n            }\n        }\n\n        // Process remaining elements (if any)\n        for (; j < padded_size; ++j) {\n            if (l[i] + padded_l[j] == 0 && i != j) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_l(padded_size, 0);\n    std::copy(l.begin(), l.end(), padded_l.begin());\n\n    for (int i = 0; i < n; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]); // Broadcast l[i] to all elements of the vector\n        int j = 0;\n\n        // Process 8 elements at a time\n        for (; j <= padded_size - 8; j += 8) {\n            __m256i vj = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_l[j])); // Load 8 elements\n            __m256i sum = _mm256_add_epi32(vi, vj); // Add vectors element-wise\n\n            // Check if any element in the sum is zero\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero); // Compare for equality with zero\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp)); // Extract comparison mask\n\n            if (mask != 0) {\n                // Verify that the match is not due to comparing an element with itself\n                for (int k = 0; k < 8; ++k) {\n                    if ((mask & (1 << k)) && (i != j + k)) {\n                        return true;\n                    }\n                }\n            }\n        }\n\n        // Process remaining elements (if any)\n        for (; j < padded_size; ++j) {\n            if (l[i] + padded_l[j] == 0 && i != j) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n < 2) return false;\n\n    // Align the vector size to a multiple of 8 for AVX2 processing\n    int aligned_size = (n + 7) / 8 * 8;\n    std::vector<int> aligned_l(aligned_size, 0);\n    std::copy(l.begin(), l.end(), aligned_l.begin());\n\n    for (int i = 0; i < n; ++i) {\n        __m256i vi = _mm256_set1_epi32(l[i]); // Broadcast current element\n\n        for (int j = 0; j < aligned_size; j += 8) {\n            __m256i vj = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&aligned_l[j])); // Load 8 elements\n            __m256i sum = _mm256_add_epi32(vi, vj); // Add vectors\n\n            // Check if any element in sum is zero\n            __m256i zero = _mm256_setzero_si256();\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero); // Compare for equality with zero\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp)); // Get comparison mask\n\n            if (mask != 0) {\n                // Ensure the match is not with the same element\n                for (int k = 0; k < 8; ++k) {\n                    if ((mask & (1 << k)) && (i != j + k)) {\n                        return true;\n                    }\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    char buffer[65];\n    int index = 64;\n    buffer[index] = '\\0';\n\n    __m256i vbase = _mm256_set1_epi8(base);\n    __m256i zeros = _mm256_setzero_si256();\n    __m256i tens = _mm256_set1_epi8(10);\n\n    while (x > 0) {\n        __m256i vx = _mm256_set1_epi64x(x);\n        __m256i remainder = _mm256_setzero_si256();\n\n        // Compute remainder using AVX2\n        remainder = _mm256_rem_epi64(vx, vbase);\n\n        // Extract the low 8 bytes of the remainder\n        int64_t rem = _mm256_cvtsi256_si64(remainder);\n\n        // Convert remainder to ASCII and store in buffer\n        buffer[--index] = '0' + static_cast<char>(rem);\n\n        // Update x for next iteration\n        x /= base;\n    }\n\n    return std::string(buffer + index);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    alignas(32) char buffer[64];\n    int index = 63;\n    buffer[index--] = '\\0';\n\n    __m256i zero = _mm256_set1_epi8('0');\n    __m256i base_vec = _mm256_set1_epi8(base);\n\n    while (x > 0) {\n        // Compute 8 remainders in parallel using SIMD\n        __m256i values = _mm256_setr_epi64x(x, x / base, x / (base * base), x / (base * base * base),\n                                            x / (base * base * base * base), x / (base * base * base * base * base),\n                                            x / (base * base * base * base * base * base),\n                                            x / (base * base * base * base * base * base * base));\n        __m256i remainders = _mm256_rem_epi64(values, base_vec);\n        __m256i digits = _mm256_add_epi8(remainders, zero);\n\n        // Store the results into the buffer\n        char temp[32];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), digits);\n\n        for (int i = 0; i < 32 && x > 0; ++i) {\n            buffer[index--] = temp[i];\n            x /= base;\n        }\n    }\n\n    return std::string(buffer + index + 1);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    char buffer[65]; // Maximum length for base 2 of a 64-bit number + null terminator\n    char* ptr = buffer + sizeof(buffer) - 1;\n    *ptr = '\\0';\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i base_vec = _mm256_set1_epi8(base);\n    __m256i ten = _mm256_set1_epi8(10);\n    __m256i digits = _mm256_setr_epi8('0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    while (x > 0) {\n        int64_t remainder = x % base;\n        x /= base;\n\n        __m256i rem_vec = _mm256_set1_epi8(remainder);\n        __m256i cmp = _mm256_cmpeq_epi8(rem_vec, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        __m256i result = _mm256_shuffle_epi8(digits, rem_vec);\n        char digit = _mm256_extract_epi8(result, 0);\n\n        *(--ptr) = digit;\n    }\n\n    return std::string(ptr);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    char buffer[65];\n    int index = 64;\n    buffer[index] = '\\0';\n\n    __m256i vbase = _mm256_set1_epi8(base);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vdigits = _mm256_setr_epi8('0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    while (x > 0) {\n        int64_t remainder = x % base;\n        x /= base;\n\n        __m256i vremainder = _mm256_set1_epi8(remainder);\n        __m256i mask = _mm256_cmpeq_epi8(vremainder, _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0));\n        int bit_mask = _mm256_movemask_epi8(mask);\n\n        int digit_pos = __builtin_ctz(bit_mask);\n        char digit = _mm256_extract_epi8(vdigits, digit_pos);\n\n        buffer[--index] = digit;\n    }\n\n    return std::string(buffer + index);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base < 2 || base >= 10) return \"\";\n    if (x == 0) return \"0\";\n\n    char buffer[65];\n    int index = 64;\n    buffer[index] = '\\0';\n\n    __m256i vbase = _mm256_set1_epi8(base);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vdigits = _mm256_setr_epi8('0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    while (x > 0) {\n        int64_t remainder = x % base;\n        x /= base;\n\n        __m256i vremainder = _mm256_set1_epi8(remainder);\n        __m256i mask = _mm256_cmpeq_epi8(vremainder, _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0));\n        __m256i result = _mm256_blendv_epi8(vzero, vdigits, mask);\n\n        char digit = _mm256_extract_epi8(result, 0);\n        buffer[--index] = digit;\n    }\n\n    return std::string(buffer + index);\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    // Align the string to process in chunks of 32 bytes\n    int simd_len = len & ~31; // Round down to nearest multiple of 32\n    __m256i mask = _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (int i = 0; i < simd_len; i += 32) {\n        // Load 32 characters from the start and end of the current window\n        __m256i front = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[i]));\n        __m256i back = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[len - i - 32]));\n\n        // Reverse the back vector using a shuffle mask\n        back = _mm256_shuffle_epi8(back, mask);\n\n        // Compare the two vectors\n        __m256i cmp = _mm256_cmpeq_epi8(front, back);\n        int mask_result = _mm256_movemask_epi8(cmp);\n\n        // If any byte doesn't match, it's not a palindrome\n        if (mask_result != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    // Handle remaining characters that didn't fit into 32-byte chunks\n    for (int i = simd_len; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    // Align the string to process in chunks of 32 bytes\n    int simd_len = len & ~0x1F; // Round down to nearest multiple of 32\n    __m256i mask = _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (int i = 0; i < simd_len; i += 32) {\n        // Load 32 characters from the start and end of the current window\n        __m256i front = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[i]));\n        __m256i back = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[len - i - 32]));\n\n        // Reverse the back vector using a shuffle mask\n        __m256i reversed_back = _mm256_shuffle_epi8(back, mask);\n\n        // Compare the front and reversed back vectors\n        __m256i cmp = _mm256_cmpeq_epi8(front, reversed_back);\n\n        // Check if all bytes matched\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    // Process remaining characters that didn't fit into 32-byte chunks\n    for (int i = simd_len; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    // Align the string to process in chunks of 32 bytes\n    int simd_len = len & ~31; // Process up to the largest multiple of 32 less than or equal to len\n    __m256i mask = _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n\n    for (int i = 0; i < simd_len; i += 32) {\n        // Load 32 characters from the start and end of the current window\n        __m256i front = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[i]));\n        __m256i back = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[len - i - 32]));\n\n        // Reverse the back vector using a shuffle\n        __m256i reversed_back = _mm256_shuffle_epi8(back, mask);\n\n        // Compare the front and reversed back vectors\n        __m256i cmp = _mm256_cmpeq_epi8(front, reversed_back);\n        int mask_result = _mm256_movemask_epi8(cmp);\n\n        // If any byte doesn't match, it's not a palindrome\n        if (mask_result != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    // Check remaining characters that didn't fit into a 32-byte chunk\n    for (int i = simd_len; i < len / 2; ++i) {\n        if (text[i] != text[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    int mid = len / 2;\n    int vec_len = 32; // AVX2 processes 32 bytes at a time\n\n    // Align the string to process in chunks of 32 bytes\n    int i = 0;\n    for (; i <= mid - vec_len; i += vec_len) {\n        __m256i front = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[i]));\n        __m256i back = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[len - i - vec_len]));\n\n        // Reverse the back vector\n        __m256i mask = _mm256_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n                                       16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);\n        back = _mm256_shuffle_epi8(back, mask);\n\n        // Compare front and reversed back\n        __m256i cmp = _mm256_cmpeq_epi8(front, back);\n        int mask_result = _mm256_movemask_epi8(cmp);\n        if (mask_result != 0xFFFFFFFF) return false;\n    }\n\n    // Process remaining characters\n    for (; i < mid; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text) {\n    int len = text.length();\n    if (len <= 1) return true;\n\n    int mid = len / 2;\n    int vec_size = 32; // AVX2 processes 32 bytes at a time\n    int i = 0;\n\n    // Process in chunks of 32 bytes using AVX2\n    for (; i <= mid - vec_size; i += vec_size) {\n        __m256i front = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[i]));\n        __m256i back = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&text[len - i - vec_size]));\n\n        // Reverse the 'back' vector byte-wise\n        __m256i shuffle_mask = _mm256_setr_epi8(31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16,\n                                                 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n        back = _mm256_shuffle_epi8(back, shuffle_mask);\n\n        // Compare front and reversed back\n        __m256i cmp = _mm256_cmpeq_epi8(front, back);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) return false;\n    }\n\n    // Process remaining bytes (less than 32)\n    for (; i < mid; ++i) {\n        if (text[i] != text[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    int len = s.length();\n    std::string out(len, 0);\n\n    // Process 32 characters at a time using AVX2\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i base = _mm256_set1_epi8('a');\n        __m256i shift = _mm256_set1_epi8(5);\n\n        // Subtract 'a' to normalize to 0-25 range\n        __m256i normalized = _mm256_sub_epi8(input, base);\n\n        // Subtract 5 (mod 26)\n        __m256i shifted = _mm256_sub_epi8(normalized, shift);\n        __m256i mod_mask = _mm256_set1_epi8(26);\n        shifted = _mm256_add_epi8(shifted, mod_mask);\n        shifted = _mm256_rem_epu8(shifted, mod_mask);\n\n        // Add 'a' back to get the final character\n        __m256i result = _mm256_add_epi8(shifted, base);\n\n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    int len = s.length();\n    std::string out(len, 0);\n\n    // Process 32 characters at a time using AVX2\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shift = _mm256_set1_epi8(-5); // Shift by -5 to decode\n        __m256i base = _mm256_set1_epi8('a');\n        \n        // Subtract 'a' to normalize to 0-25 range\n        __m256i normalized = _mm256_sub_epi8(input, base);\n        // Add the shift (-5)\n        __m256i shifted = _mm256_add_epi8(normalized, shift);\n        // Ensure wrapping using modulo 26 (using SSE/AVX tricks for modulo)\n        __m256i mod_base = _mm256_set1_epi8(26);\n        shifted = _mm256_add_epi8(shifted, mod_base); // Avoid negative values\n        shifted = _mm256_rem_epu8(shifted, mod_base); // Modulo operation\n        // Add 'a' back to get the final character\n        __m256i result = _mm256_add_epi8(shifted, base);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Process remaining characters (if any) using scalar code\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    int len = s.length();\n    std::string out(len, 0);\n\n    // Process 32 characters at a time using AVX2\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i base = _mm256_set1_epi8('a');\n        __m256i shift = _mm256_set1_epi8(5);\n\n        // Subtract 'a' to normalize to 0-25 range\n        __m256i normalized = _mm256_sub_epi8(input, base);\n\n        // Subtract 5 to reverse the shift\n        __m256i shifted = _mm256_sub_epi8(normalized, shift);\n\n        // Add 26 to ensure positive values before modulo\n        __m256i adjusted = _mm256_add_epi8(shifted, _mm256_set1_epi8(26));\n\n        // Modulo 26 using bitwise AND (valid because 26 is a power of 2 minus 2)\n        __m256i mod = _mm256_and_si256(adjusted, _mm256_set1_epi8(25));\n\n        // Add 'a' back to get the final character\n        __m256i result = _mm256_add_epi8(mod, base);\n\n        // Store the result back into the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Process remaining characters (if any) using scalar code\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    int len = s.length();\n    std::string out(len, 0);\n\n    // Process 32 characters at a time using AVX2\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i shift = _mm256_set1_epi8(-5);\n        __m256i base = _mm256_set1_epi8('a');\n        \n        // Subtract 'a' to normalize to 0-25 range\n        __m256i normalized = _mm256_sub_epi8(input, base);\n        // Add 26 to ensure positive values before modulo\n        __m256i adjusted = _mm256_add_epi8(normalized, _mm256_set1_epi8(26));\n        // Apply shift\n        __m256i shifted = _mm256_add_epi8(adjusted, shift);\n        // Modulo 26\n        __m256i mod = _mm256_rem_epu8(shifted, _mm256_set1_epi8(26));\n        // Add 'a' back to get the final character\n        __m256i result = _mm256_add_epi8(mod, base);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s) {\n    int len = s.length();\n    std::string out(len, 0);\n\n    // Process 32 characters at a time using AVX2\n    int i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i base = _mm256_set1_epi8('a');\n        __m256i shift = _mm256_set1_epi8(5);\n\n        // Subtract 'a' to normalize to 0-25 range\n        __m256i normalized = _mm256_sub_epi8(input, base);\n\n        // Subtract 5 to reverse the shift\n        __m256i shifted = _mm256_sub_epi8(normalized, shift);\n\n        // Add 26 to ensure positive values before modulo\n        __m256i adjusted = _mm256_add_epi8(shifted, _mm256_set1_epi8(26));\n\n        // Modulo 26 using bitwise AND (valid because 26 is a power of 2 minus 2)\n        __m256i mod = _mm256_and_si256(adjusted, _mm256_set1_epi8(25));\n\n        // Add 'a' back to get the final character\n        __m256i result = _mm256_add_epi8(mod, base);\n\n        // Store the result into the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Process remaining characters (if any) using scalar code\n    for (; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i all_ones = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i result = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < l.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        result = _mm256_and_si256(result, cmp);\n    }\n\n    int mask = _mm256_movemask_epi8(result);\n    return mask == 0;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i all_ones = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i result = _mm256_set1_epi32(0xFFFFFFFF);\n\n    size_t i = 0;\n    size_t size = l.size();\n\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(threshold, vec);\n        result = _mm256_and_si256(result, cmp);\n    }\n\n    int mask = _mm256_movemask_ps(_mm256_castsi256_ps(result));\n    if (mask != 0xFF) {\n        return false;\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    int n = l.size();\n    int threshold = t;\n    __m256i vthreshold = _mm256_set1_epi32(threshold);\n    bool result = true;\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(v, vthreshold);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i all_ones = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i result = _mm256_set1_epi32(0);\n\n    size_t i = 0;\n    size_t size = l.size();\n\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        result = _mm256_or_si256(result, cmp);\n    }\n\n    int mask = _mm256_movemask_ps((__m256)result);\n    if (mask != 0) return false;\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(std::vector<int> l, int t) {\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i all_ones = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i result = _mm256_set1_epi32(0);\n\n    size_t i = 0;\n    size_t size = l.size();\n\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        result = _mm256_or_si256(result, cmp);\n    }\n\n    int mask = _mm256_movemask_ps((__m256)result);\n    if (mask != 0) return false;\n\n    for (; i < size; ++i) {\n        if (l[i] >= t) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 2 != 0) return false;\n\n    int balance = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n    __m256i balance_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i less_than = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('<'));\n        __m256i greater_than = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('>'));\n\n        __m256i add_vec = _mm256_blendv_epi8(zero, ones, less_than);\n        __m256i sub_vec = _mm256_blendv_epi8(zero, neg_ones, greater_than);\n\n        balance_vec = _mm256_add_epi8(balance_vec, add_vec);\n        balance_vec = _mm256_add_epi8(balance_vec, sub_vec);\n\n        // Horizontal sum to check for negative values\n        int8_t temp[32];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), balance_vec);\n        for (int j = 0; j < 32; ++j) {\n            balance += temp[j];\n            if (balance < 0) return false;\n        }\n        balance_vec = _mm256_setzero_si256();\n    }\n\n    // Process remaining characters\n    for (int i = length - (length % 32); i < length; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else if (brackets[i] == '>') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 32 != 0) {\n        // Pad the string to make its length a multiple of 32\n        int padded_length = ((length + 31) / 32) * 32;\n        brackets.resize(padded_length, ' ');\n    }\n\n    __m256i less_than = _mm256_set1_epi8('<');\n    __m256i greater_than = _mm256_set1_epi8('>');\n    __m256i zero = _mm256_setzero_si256();\n\n    int balance = 0;\n    for (int i = 0; i < brackets.length(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i is_less = _mm256_cmpeq_epi8(_mm256_and_si256(chunk, less_than), less_than);\n        __m256i is_greater = _mm256_cmpeq_epi8(_mm256_and_si256(chunk, greater_than), greater_than);\n\n        int mask_less = _mm256_movemask_epi8(is_less);\n        int mask_greater = _mm256_movemask_epi8(is_greater);\n\n        int count_less = _mm_popcnt_u32(mask_less);\n        int count_greater = _mm_popcnt_u32(mask_greater);\n\n        balance += (count_less - count_greater);\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 2 != 0) return false;\n\n    int balance = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_setzero_si256();\n\n    for (int i = 0; i <= length - 32; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i less_than = _mm256_set1_epi8('<');\n        __m256i greater_than = _mm256_set1_epi8('>');\n\n        __m256i is_less = _mm256_cmpeq_epi8(_mm256_and_si256(input, less_than), less_than);\n        __m256i is_greater = _mm256_cmpeq_epi8(_mm256_and_si256(input, greater_than), greater_than);\n\n        __m256i add_sub_mask = _mm256_blendv_epi8(_mm256_set1_epi8(-1), _mm256_set1_epi8(1), is_less);\n        balance_vec = _mm256_add_epi8(balance_vec, add_sub_mask);\n    }\n\n    alignas(32) char temp[32];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), balance_vec);\n\n    for (int i = 0; i < 32; ++i) {\n        balance += temp[i];\n    }\n\n    for (int i = length - (length % 32); i < length; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else if (brackets[i] == '>') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 2 != 0) return false;\n\n    int balance = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n\n    for (int i = 0; i <= length - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i less_than = _mm256_cmpeq_epi8(_mm256_and_si256(chunk, _mm256_set1_epi8(0x7F)), _mm256_set1_epi8('<'));\n        __m256i greater_than = _mm256_cmpeq_epi8(_mm256_and_si256(chunk, _mm256_set1_epi8(0x7F)), _mm256_set1_epi8('>'));\n\n        __m256i add_vec = _mm256_blendv_epi8(zero, ones, less_than);\n        __m256i sub_vec = _mm256_blendv_epi8(zero, neg_ones, greater_than);\n\n        __m256i result = _mm256_add_epi8(add_vec, sub_vec);\n\n        int mask = _mm256_movemask_epi8(result);\n        int count = _mm_popcnt_u32(mask);\n        balance += count;\n    }\n\n    for (int i = (length / 32) * 32; i < length; ++i) {\n        if (brackets[i] == '<') {\n            balance++;\n        } else if (brackets[i] == '>') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 2 != 0) return false;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i minus_one = _mm256_set1_epi32(-1);\n\n    for (int i = 0; i <= length - 8; i += 8) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i less_than = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('<'));\n        __m256i greater_than = _mm256_cmpeq_epi8(chars, _mm256_set1_epi8('>'));\n\n        __m256i add_mask = _mm256_and_si256(less_than, one);\n        __m256i sub_mask = _mm256_and_si256(greater_than, minus_one);\n\n        balance = _mm256_add_epi32(balance, _mm256_cvtepi8_epi32(add_mask));\n        balance = _mm256_add_epi32(balance, _mm256_cvtepi8_epi32(sub_mask));\n\n        // Horizontal sum to check for negative values\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), balance);\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] < 0) return false;\n        }\n    }\n\n    // Handle remaining characters\n    int remaining = length % 8;\n    for (int i = length - remaining; i < length; ++i) {\n        if (brackets[i] == '<') {\n            balance = _mm256_add_epi32(balance, one);\n        } else if (brackets[i] == '>') {\n            balance = _mm256_sub_epi32(balance, one);\n        }\n\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), balance);\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] < 0) return false;\n        }\n    }\n\n    // Final horizontal sum\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), balance);\n    int final_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        final_sum += temp[i];\n    }\n\n    return final_sum == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 32 != 0) {\n        // Pad the string to make its length a multiple of 32\n        brackets.resize((length + 31) / 32 * 32, ' ');\n    }\n\n    __m256i open_bracket = _mm256_set1_epi8('(');\n    __m256i close_bracket = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n\n    int balance = 0;\n    for (int i = 0; i < brackets.length(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        // Compare for '('\n        __m256i is_open = _mm256_cmpeq_epi8(chunk, open_bracket);\n        // Compare for ')'\n        __m256i is_close = _mm256_cmpeq_epi8(chunk, close_bracket);\n\n        // Count the number of '(' and ')'\n        __m256i open_mask = _mm256_and_si256(is_open, _mm256_set1_epi8(1));\n        __m256i close_mask = _mm256_and_si256(is_close, _mm256_set1_epi8(1));\n\n        // Horizontal sum of the masks\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        // Update balance\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        // Early exit if balance goes negative\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 32 != 0) {\n        // Pad the string to make its length a multiple of 32\n        brackets.resize((length + 31) / 32 * 32, ' ');\n    }\n\n    __m256i open_bracket = _mm256_set1_epi8('(');\n    __m256i close_bracket = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n\n    int balance = 0;\n    for (int i = 0; i < brackets.length(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        // Compare for '('\n        __m256i is_open = _mm256_cmpeq_epi8(chunk, open_bracket);\n        // Compare for ')'\n        __m256i is_close = _mm256_cmpeq_epi8(chunk, close_bracket);\n\n        // Count the number of '(' and ')'\n        __m256i open_mask = _mm256_and_si256(is_open, _mm256_set1_epi8(1));\n        __m256i close_mask = _mm256_and_si256(is_close, _mm256_set1_epi8(1));\n\n        // Horizontal sum of the masks\n        int open_count = _mm256_movemask_epi8(open_mask);\n        int close_count = _mm256_movemask_epi8(close_mask);\n\n        // Update balance\n        balance += __builtin_popcount(open_count) - __builtin_popcount(close_count);\n\n        // Early exit if balance goes negative\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 32 != 0) {\n        // Pad the string to make its length a multiple of 32\n        int new_length = ((length + 31) / 32) * 32;\n        brackets.resize(new_length, ' ');\n    }\n\n    __m256i open_bracket = _mm256_set1_epi8('(');\n    __m256i close_bracket = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n\n    int balance = 0;\n    for (int i = 0; i < brackets.length(); i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i is_open = _mm256_cmpeq_epi8(chunk, open_bracket);\n        __m256i is_close = _mm256_cmpeq_epi8(chunk, close_bracket);\n\n        int mask_open = _mm256_movemask_epi8(is_open);\n        int mask_close = _mm256_movemask_epi8(is_close);\n\n        int count_open = _mm_popcnt_u32(mask_open);\n        int count_close = _mm_popcnt_u32(mask_close);\n\n        balance += count_open - count_close;\n\n        if (balance < 0) {\n            return false;\n        }\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 2 != 0) return false;\n\n    int balance = 0;\n    __m256i v_open = _mm256_set1_epi8('(');\n    __m256i v_close = _mm256_set1_epi8(')');\n    __m256i v_zero = _mm256_setzero_si256();\n\n    for (int i = 0; i <= length - 32; i += 32) {\n        __m256i v_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i v_is_open = _mm256_cmpeq_epi8(v_chars, v_open);\n        __m256i v_is_close = _mm256_cmpeq_epi8(v_chars, v_close);\n\n        int mask_open = _mm256_movemask_epi8(v_is_open);\n        int mask_close = _mm256_movemask_epi8(v_is_close);\n\n        int count_open = _mm_popcnt_u32(mask_open);\n        int count_close = _mm_popcnt_u32(mask_close);\n\n        balance += count_open - count_close;\n\n        if (balance < 0) return false;\n    }\n\n    for (int i = length - (length % 32); i < length; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets) {\n    int length = brackets.length();\n    if (length % 2 != 0) return false;\n\n    __m256i open_bracket = _mm256_set1_epi8('(');\n    __m256i close_bracket = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n\n    int balance = 0;\n    int i = 0;\n\n    for (; i + 32 <= length; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i is_open = _mm256_cmpeq_epi8(chunk, open_bracket);\n        __m256i is_close = _mm256_cmpeq_epi8(chunk, close_bracket);\n\n        int mask_open = _mm256_movemask_epi8(is_open);\n        int mask_close = _mm256_movemask_epi8(is_close);\n\n        int count_open = _mm_popcnt_u32(mask_open);\n        int count_close = _mm_popcnt_u32(mask_close);\n\n        balance += count_open - count_close;\n\n        if (balance < 0) return false;\n    }\n\n    for (; i < length; ++i) {\n        if (brackets[i] == '(') {\n            balance++;\n        } else if (brackets[i] == ')') {\n            balance--;\n        }\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    if (n <= 1) return {}; // Derivative of constant is zero.\n\n    std::vector<float> result(n - 1);\n    int i = 0;\n\n    // Process 8 elements at a time using AVX.\n    for (; i + 8 <= n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]); // Load coefficients starting from xs[1].\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 mul = _mm256_mul_ps(coeffs, indices); // Multiply coefficients by their indices.\n        _mm256_storeu_ps(&result[i], mul);          // Store the result.\n    }\n\n    // Process remaining elements sequentially.\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    if (n <= 1) return {}; // The derivative of a constant is zero.\n\n    std::vector<float> result(n - 1);\n    int i = 0;\n\n    // Process 8 elements at a time using AVX.\n    for (; i + 8 <= n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]); // Load coefficients starting from xs[1].\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 products = _mm256_mul_ps(coeffs, indices); // Multiply coefficients by their indices.\n        _mm256_storeu_ps(&result[i], products); // Store the results.\n    }\n\n    // Process remaining elements sequentially.\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    if (n <= 1) return {}; // The derivative of a constant is zero.\n\n    std::vector<float> result(n - 1);\n    int i = 0;\n\n    // Process 8 elements at a time using AVX.\n    for (; i + 8 <= n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]); // Load coefficients starting from xs[1].\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 mul = _mm256_mul_ps(coeffs, indices); // Multiply coefficients by their respective indices.\n        _mm256_storeu_ps(&result[i], mul);           // Store the result back into the output vector.\n    }\n\n    // Process remaining elements sequentially.\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    if (n <= 1) return {}; // The derivative of a constant is zero.\n\n    std::vector<float> result(n - 1);\n    int i = 0;\n\n    // Process 8 elements at a time using AVX.\n    for (; i + 8 <= n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]); // Load coefficients starting from xs[1].\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 mul = _mm256_mul_ps(coeffs, indices); // Multiply coefficients by their respective indices.\n        _mm256_storeu_ps(&result[i], mul);           // Store the result back to the output vector.\n    }\n\n    // Process remaining elements sequentially.\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs) {\n    int n = xs.size();\n    if (n <= 1) return {}; // Derivative of constant is zero.\n\n    std::vector<float> result(n - 1);\n    int i = 0;\n\n    // Process 8 elements at a time using AVX.\n    for (; i + 8 <= n - 1; i += 8) {\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]); // Load coefficients starting from xs[1].\n        __m256 indices = _mm256_set_ps(i + 8, i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1);\n        __m256 mul = _mm256_mul_ps(coeffs, indices); // Multiply coefficients by their respective indices.\n        _mm256_storeu_ps(&result[i], mul);           // Store the result back into the output vector.\n    }\n\n    // Process remaining elements sequentially.\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0,\n                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    int len = s.length();\n    int i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(0)));\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(1)));\n        __m256i cmp3 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(2)));\n        __m256i cmp4 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(3)));\n        __m256i cmp5 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(4)));\n        __m256i cmp6 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(5)));\n        __m256i cmp7 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(6)));\n        __m256i cmp8 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(7)));\n        __m256i cmp9 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(8)));\n        __m256i cmp10 = _mm256_cmpeq_epi8(chunk, _mm256_shuffle_epi8(vowels, _mm256_set1_epi8(9)));\n\n        __m256i cmp = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4)),\n                                      _mm256_or_si256(_mm256_or_si256(cmp5, cmp6), _mm256_or_si256(cmp7, cmp8)));\n        cmp = _mm256_or_si256(cmp, cmp9);\n        cmp = _mm256_or_si256(cmp, cmp10);\n\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            ++count;\n        }\n    }\n\n    // Check if the last character is 'y' or 'Y'\n    if (len > 0 && (s[len - 1] == 'y' || s[len - 1] == 'Y')) {\n        ++count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0,\n                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    int len = s.length();\n    int i = 0;\n\n    // Process 32 characters at a time\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_setzero_si256()));\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(1)));\n        __m256i cmp3 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(2)));\n        __m256i cmp4 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(3)));\n        __m256i cmp5 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(4)));\n        __m256i cmp6 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(5)));\n        __m256i cmp7 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(6)));\n        __m256i cmp8 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(7)));\n        __m256i cmp9 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(8)));\n        __m256i cmp10 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(9)));\n\n        __m256i cmp = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4)),\n                                      _mm256_or_si256(_mm256_or_si256(cmp5, cmp6), _mm256_or_si256(cmp7, cmp8)));\n        cmp = _mm256_or_si256(cmp, _mm256_or_si256(cmp9, cmp10));\n\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    // Check if the last character is 'y' or 'Y'\n    if (len > 0 && (s[len - 1] == 'y' || s[len - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0,\n                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    int len = s.length();\n    int i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 31 < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_setzero_si256()));\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(1)));\n        __m256i cmp3 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(2)));\n        __m256i cmp4 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(3)));\n        __m256i cmp5 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(4)));\n        __m256i cmp6 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(5)));\n        __m256i cmp7 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(6)));\n        __m256i cmp8 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(7)));\n        __m256i cmp9 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(8)));\n        __m256i cmp10 = _mm256_cmpeq_epi8(chunk, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(9)));\n\n        __m256i cmp_result = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4)),\n                                             _mm256_or_si256(_mm256_or_si256(cmp5, cmp6), _mm256_or_si256(cmp7, cmp8)));\n        cmp_result = _mm256_or_si256(cmp_result, _mm256_or_si256(cmp9, cmp10));\n\n        int mask = _mm256_movemask_epi8(cmp_result);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    // Check if the last character is 'y' or 'Y'\n    if (len > 0 && (s[len - 1] == 'y' || s[len - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0,\n                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    int len = s.length();\n    int i = 0;\n\n    // Process 32 characters at a time\n    for (; i + 31 < len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i cmp1 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('a'));\n        __m256i cmp2 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('e'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('i'));\n        __m256i cmp4 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('o'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('u'));\n        __m256i cmp6 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('A'));\n        __m256i cmp7 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('E'));\n        __m256i cmp8 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('I'));\n        __m256i cmp9 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('O'));\n        __m256i cmp10 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('U'));\n\n        __m256i cmp = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4)),\n                                      _mm256_or_si256(_mm256_or_si256(cmp5, cmp6), _mm256_or_si256(cmp7, cmp8)));\n        cmp = _mm256_or_si256(cmp, _mm256_or_si256(cmp9, cmp10));\n\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Handle remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    // Check if the last character is 'y' or 'Y'\n    if (len > 0 && (s[len - 1] == 'y' || s[len - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s) {\n    __m256i vowels = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0,\n                                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    int len = s.length();\n    int i = 0;\n\n    // Process 32 characters at a time\n    for (; i + 31 < len; i += 32) {\n        __m256i str = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i cmp1 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_setzero_si256()));\n        __m256i cmp2 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(1)));\n        __m256i cmp3 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(2)));\n        __m256i cmp4 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(3)));\n        __m256i cmp5 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(4)));\n        __m256i cmp6 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(5)));\n        __m256i cmp7 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(6)));\n        __m256i cmp8 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(7)));\n        __m256i cmp9 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(8)));\n        __m256i cmp10 = _mm256_cmpeq_epi8(str, _mm256_permutevar8x32_epi32(vowels, _mm256_set1_epi32(9)));\n\n        __m256i cmp = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4)),\n                                      _mm256_or_si256(_mm256_or_si256(cmp5, cmp6), _mm256_or_si256(cmp7, cmp8)));\n        cmp = _mm256_or_si256(cmp, _mm256_or_si256(cmp9, cmp10));\n\n        int mask = _mm256_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n            c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            count++;\n        }\n    }\n\n    // Check if the last character is 'y' or 'Y'\n    if (len > 0 && (s[len - 1] == 'y' || s[len - 1] == 'Y')) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.size();\n    int i = 0;\n\n    // Process 32 characters at a time using AVX2\n    __m256i upper_mask = _mm256_set1_epi8(0x40); // Mask to check if character is uppercase (ASCII 'A' = 65, 'Z' = 90)\n    __m256i lower_bound = _mm256_set1_epi8('A' - 1);\n    __m256i upper_bound = _mm256_set1_epi8('Z' + 1);\n\n    for (; i + 31 < len; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Check if characters are in the range 'A' to 'Z'\n        __m256i cmp_lower = _mm256_cmpgt_epi8(chars, lower_bound);\n        __m256i cmp_upper = _mm256_cmplt_epi8(chars, upper_bound);\n        __m256i mask = _mm256_and_si256(cmp_lower, cmp_upper);\n\n        // Zero out non-uppercase characters\n        __m256i filtered_chars = _mm256_and_si256(chars, mask);\n\n        // Extract ASCII values and sum them horizontally\n        alignas(32) char temp[32];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp), filtered_chars);\n        for (int j = 0; j < 32; ++j) {\n            sum += static_cast<unsigned char>(temp[j]);\n        }\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.size();\n    int i = 0;\n\n    // Process 32 characters at a time using AVX2\n    __m256i upper_mask = _mm256_set1_epi8(0x40); // Mask to check if character is uppercase ('A' = 65, 'Z' = 90)\n    __m256i lower_bound = _mm256_set1_epi8('A' - 1);\n    __m256i upper_bound = _mm256_set1_epi8('Z' + 1);\n\n    for (; i + 31 < len; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Check if characters are in the range 'A' to 'Z'\n        __m256i cmp_lower = _mm256_cmpgt_epi8(chars, lower_bound);\n        __m256i cmp_upper = _mm256_cmplt_epi8(chars, upper_bound);\n        __m256i mask = _mm256_and_si256(cmp_lower, cmp_upper);\n\n        // Zero out non-uppercase characters\n        __m256i filtered_chars = _mm256_and_si256(chars, mask);\n\n        // Extract ASCII values and sum them horizontally\n        int temp[32];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), filtered_chars);\n        for (int j = 0; j < 32; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.size();\n    int i = 0;\n\n    // Process 32 characters at a time using AVX2\n    __m256i upper_mask = _mm256_set1_epi8(0x40); // Mask for ASCII values <= '@' (64)\n    __m256i lower_mask = _mm256_set1_epi8(0x5B); // Mask for ASCII values >= '[' (91)\n\n    for (; i + 31 < len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create a mask for uppercase letters: (data > '@') && (data < '[')\n        __m256i cmp1 = _mm256_cmpgt_epi8(data, upper_mask);\n        __m256i cmp2 = _mm256_cmplt_epi8(data, lower_mask);\n        __m256i mask = _mm256_and_si256(cmp1, cmp2);\n\n        // Zero out non-uppercase characters\n        __m256i filtered = _mm256_blendv_epi8(_mm256_setzero_si256(), data, mask);\n\n        // Extract the sum of the remaining values\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), filtered);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int sum = 0;\n    int len = s.size();\n    int i = 0;\n\n    // Load constants for comparison\n    __m256i upper_mask = _mm256_set1_epi8('A' - 1);\n    __m256i lower_mask = _mm256_set1_epi8('Z');\n\n    // Process 32 characters at a time\n    for (; i + 31 < len; i += 32) {\n        // Load 32 bytes from the string\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Compare if characters are greater than 'A' - 1 and less than or equal to 'Z'\n        __m256i cmp1 = _mm256_cmpgt_epi8(data, upper_mask);\n        __m256i cmp2 = _mm256_cmple_epi8(data, lower_mask);\n\n        // Combine the comparisons\n        __m256i mask = _mm256_and_si256(cmp1, cmp2);\n\n        // Zero out non-uppercase characters\n        __m256i filtered = _mm256_and_si256(data, mask);\n\n        // Extract sums from each lane\n        int temp[32];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), filtered);\n\n        for (int j = 0; j < 32; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s) {\n    int result = 0;\n    int length = s.size();\n    int i = 0;\n\n    // Process 32 characters at a time using AVX2\n    __m256i upper_threshold = _mm256_set1_epi8('A' - 1);\n    __m256i lower_threshold = _mm256_set1_epi8('Z' + 1);\n    __m256i zero_vector = _mm256_setzero_si256();\n\n    for (; i <= length - 32; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create mask for uppercase letters\n        __m256i cmp1 = _mm256_cmpgt_epi8(data, upper_threshold); // data > 'A' - 1\n        __m256i cmp2 = _mm256_cmplt_epi8(data, lower_threshold); // data < 'Z' + 1\n        __m256i mask = _mm256_and_si256(cmp1, cmp2);\n\n        // Zero out non-uppercase characters\n        __m256i masked_data = _mm256_blendv_epi8(zero_vector, data, mask);\n\n        // Sum the ASCII values horizontally\n        __m128i sum128 = _mm_add_epi32(\n            _mm256_castsi256_si128(masked_data),\n            _mm256_extracti128_si256(masked_data, 1)\n        );\n\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        result += _mm_cvtsi128_si32(sum128);\n    }\n\n    // Process remaining characters\n    for (; i < length; ++i) {\n        if (s[i] >= 'A' && s[i] <= 'Z') {\n            result += s[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    size_t i = 0;\n    size_t size = arr.size();\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i ones = _mm256_set1_epi32(1);\n\n        // Check even numbers: (vec & 1) == 0\n        __m256i is_even = _mm256_and_si256(vec, ones);\n        __m256i mask = _mm256_cmpeq_epi32(is_even, zeros);\n\n        // Extract the mask to check which elements are even\n        int even_mask = _mm256_movemask_epi8(mask);\n\n        if (even_mask != 0) {\n            // Compare against current minimum even value\n            for (int j = 0; j < 8; ++j) {\n                if (even_mask & (1 << (j * 4))) { // Check each element in the mask\n                    int value = arr[i + j];\n                    if (value < min_even) {\n                        min_even = value;\n                        min_index = static_cast<int>(i + j);\n                    }\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = static_cast<int>(i);\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    size_t i = 0;\n    size_t size = arr.size();\n\n    // Process 8 integers at a time using AVX2\n    __m256i even_mask = _mm256_set1_epi32(1); // Mask to check even numbers (x & 1 == 0)\n    __m256i max_val = _mm256_set1_epi32(std::numeric_limits<int>::max()); // Max value for comparison\n\n    for (; i + 7 < size; i += 8) {\n        __m256i values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i is_even = _mm256_and_si256(values, even_mask); // Check if values are even\n        __m256i cmp_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256()); // Compare with zero\n\n        // Extract even values and compare with current minimum\n        __m256i valid_values = _mm256_blendv_epi8(max_val, values, cmp_even);\n        __m256i min_val_vec = _mm256_min_epi32(valid_values, _mm256_set1_epi32(min_even));\n\n        // Find the smallest even value in this vector\n        int temp_min[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_min), min_val_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp_min[j] < min_even && (arr[i + j] & 1) == 0) {\n                min_even = temp_min[j];\n                min_index = static_cast<int>(i + j);\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if ((arr[i] & 1) == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = static_cast<int>(i);\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    size_t i = 0;\n    size_t size = arr.size();\n\n    // Process 8 integers at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i evens_mask = _mm256_andnot_si256(_mm256_and_si256(vec, ones), _mm256_set1_epi32(-1));\n\n        // Extract even numbers\n        __m256i even_values = _mm256_blendv_epi8(_mm256_set1_epi32(std::numeric_limits<int>::max()), vec, evens_mask);\n\n        // Find the minimum value in the vector\n        __m256i min_val_vec = _mm256_min_epi32(even_values, _mm256_permutevar8x32_epi32(even_values, _mm256_setr_epi32(4, 5, 6, 7, 0, 1, 2, 3)));\n        min_val_vec = _mm256_min_epi32(min_val_vec, _mm256_permutevar8x32_epi32(min_val_vec, _mm256_setr_epi32(2, 3, 0, 1, 6, 7, 4, 5)));\n        min_val_vec = _mm256_min_epi32(min_val_vec, _mm256_permutevar8x32_epi32(min_val_vec, _mm256_setr_epi32(1, 0, 3, 2, 5, 4, 7, 6)));\n\n        int min_val = _mm256_extract_epi32(min_val_vec, 0);\n\n        // If a new minimum is found, update the result\n        if (min_val < min_even) {\n            for (int j = 0; j < 8; ++j) {\n                int val = arr[i + j];\n                if ((val & 1) == 0 && val == min_val) {\n                    min_even = val;\n                    min_index = i + j;\n                    break;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int val = arr[i];\n        if ((val & 1) == 0 && val < min_even) {\n            min_even = val;\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    size_t i = 0;\n    size_t size = arr.size();\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i evens_mask = _mm256_andnot_si256(_mm256_and_si256(vec, ones), _mm256_cmpeq_epi32(vec, vec));\n\n        // Extract even numbers and compare with current minimum\n        int mask = _mm256_movemask_epi8(evens_mask);\n        if (mask != 0) {\n            alignas(32) int temp[8];\n            _mm256_store_si256(reinterpret_cast<__m256i*>(temp), vec);\n\n            for (int j = 0; j < 8; ++j) {\n                if ((temp[j] & 1) == 0 && temp[j] < min_even) {\n                    min_even = temp[j];\n                    min_index = i + j;\n                }\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if ((arr[i] & 1) == 0 && arr[i] < min_even) {\n            min_even = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    int min_even = -1;\n    int min_index = -1;\n\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i increment = _mm256_set1_epi32(8);\n\n    size_t i = 0;\n    size_t size = arr.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i values = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i even_mask = _mm256_and_si256(values, _mm256_set1_epi32(1));\n        even_mask = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n\n        if (!_mm256_testz_si256(even_mask, even_mask)) {\n            int mask = _mm256_movemask_epi8(even_mask);\n            for (int j = 0; j < 8; ++j) {\n                if (mask & (0xFF << (j * 4))) {\n                    int value = arr[i + j];\n                    if (min_even == -1 || value < min_even || (value == min_even && (i + j) < min_index)) {\n                        min_even = value;\n                        min_index = i + j;\n                    }\n                }\n            }\n        }\n\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i] % 2 == 0) {\n            if (min_even == -1 || arr[i] < min_even || (arr[i] == min_even && i < min_index)) {\n                min_even = arr[i];\n                min_index = i;\n            }\n        }\n    }\n\n    if (min_even == -1) return {};\n    return {min_even, min_index};\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> freq(max_val + 1, 0);\n\n    // Count frequencies using AVX2\n    size_t i = 0;\n    size_t size = lst.size();\n    __m256i v_one = _mm256_set1_epi32(1);\n    for (; i + 7 < size; i += 8) {\n        __m256i v_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(v_vals, j);\n            if (val > 0 && val <= static_cast<int>(freq.size())) {\n                freq[val]++;\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int val = lst[i];\n        if (val > 0 && val <= static_cast<int>(freq.size())) {\n            freq[val]++;\n        }\n    }\n\n    // Find the maximum value satisfying the condition\n    int result = -1;\n    for (int val = 1; val <= max_val; ++val) {\n        if (freq[val] >= val) {\n            result = std::max(result, val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> freq(max_val + 1, 0);\n\n    // Count frequencies using AVX2\n    size_t i = 0;\n    size_t n = lst.size();\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    for (; i + 7 < n; i += 8) {\n        __m256i v_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(v_vals, j);\n            if (val > 0 && val <= static_cast<int>(freq.size())) {\n                freq[val]++;\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val > 0 && val <= static_cast<int>(freq.size())) {\n            freq[val]++;\n        }\n    }\n\n    // Find the maximum value satisfying the condition\n    int result = -1;\n    for (int val = 1; val <= max_val; ++val) {\n        if (freq[val] >= val) {\n            result = std::max(result, val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> freq(max_val + 1, 0);\n\n    // Count frequencies using AVX2\n    size_t i = 0;\n    size_t n = lst.size();\n    __m256i v_one = _mm256_set1_epi32(1);\n    for (; i + 7 < n; i += 8) {\n        __m256i v_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(v_vals, j);\n            if (val > 0 && val <= static_cast<int>(freq.size())) {\n                freq[val]++;\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        int val = lst[i];\n        if (val > 0 && val <= static_cast<int>(freq.size())) {\n            freq[val]++;\n        }\n    }\n\n    // Find the largest integer with frequency >= value\n    int result = -1;\n    for (int val = 1; val <= max_val; ++val) {\n        if (freq[val] >= val) {\n            result = std::max(result, val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    const int size = lst.size();\n    const int simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    int max_value = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> freq(max_value + 1, 0);\n\n    // Count frequencies using scalar code for simplicity\n    for (int num : lst) {\n        if (num > 0 && num <= max_value) {\n            freq[num]++;\n        }\n    }\n\n    // Use AVX2 to find the largest integer satisfying the condition\n    int result = -1;\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i increment = _mm256_set1_epi32(simd_width);\n\n    for (int i = 1; i <= max_value; i += simd_width) {\n        __m256i values = _mm256_i32gather_epi32(freq.data() + i, indices, sizeof(int));\n        __m256i thresholds = _mm256_set1_epi32(i);\n        __m256i mask = _mm256_cmpgt_epi32(values, thresholds);\n\n        int mask_bits = _mm256_movemask_epi8(mask);\n        if (mask_bits != 0) {\n            for (int j = 0; j < simd_width; ++j) {\n                if (freq[i + j] > i + j) {\n                    result = std::max(result, i + j);\n                }\n            }\n        }\n\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst) {\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    std::vector<int> freq(max_val + 1, 0);\n\n    // Count frequencies using AVX2\n    size_t i = 0;\n    size_t size = lst.size();\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        for (int j = 0; j < 8; ++j) {\n            int val = _mm256_extract_epi32(v_vals, j);\n            if (val > 0 && val <= static_cast<int>(freq.size())) {\n                freq[val]++;\n            }\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int val = lst[i];\n        if (val > 0 && val <= static_cast<int>(freq.size())) {\n            freq[val]++;\n        }\n    }\n\n    // Find the largest integer with frequency >= value\n    int result = -1;\n    for (int val = 1; val <= max_val; ++val) {\n        if (freq[val] >= val) {\n            result = std::max(result, val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_q(padded_size, 0);\n    for (int i = 0; i < n; ++i) {\n        padded_q[i] = q[i];\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    bool is_palindromic = true;\n\n    for (int i = 0; i < padded_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_q[i]));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        // Check palindromic property\n        __m256i reverse_vec = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        __m256i cmp = _mm256_cmpeq_epi32(vec, reverse_vec);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            is_palindromic = false;\n        }\n    }\n\n    // Horizontal sum of sum_vec\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    return is_palindromic && (total_sum <= w);\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int sum = 0;\n    bool balanced = true;\n\n    // Ensure the size is a multiple of 8 for AVX2 processing\n    int simd_size = 8;\n    int padded_size = ((n + simd_size - 1) / simd_size) * simd_size;\n    std::vector<int> padded_q(padded_size, 0);\n    for (int i = 0; i < n; ++i) {\n        padded_q[i] = q[i];\n    }\n\n    __m256i v_sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < padded_size; i += simd_size) {\n        __m256i v_q = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_q[i]));\n        v_sum = _mm256_add_epi32(v_sum, v_q);\n\n        // Check balance using mirrored indices\n        int mirror_index = n - 1 - i;\n        if (mirror_index >= 0 && mirror_index < n && i < n) {\n            if (q[i] != q[mirror_index]) {\n                balanced = false;\n            }\n        }\n    }\n\n    // Horizontal sum of v_sum\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), v_sum);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    return balanced && (sum <= w);\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    int sum = 0;\n    bool is_palindromic = true;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int simd_size = 8;\n    int aligned_size = (n / simd_size) * simd_size;\n\n    __m256i v_sum = _mm256_setzero_si256();\n\n    // Process elements in chunks of 8 using AVX2\n    for (int i = 0; i < aligned_size; i += simd_size) {\n        // Load 8 integers from q into a SIMD register\n        __m256i v_q = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n\n        // Accumulate the sum horizontally\n        v_sum = _mm256_add_epi32(v_sum, v_q);\n\n        // Check palindromic property\n        __m256i v_reverse = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[n - i - simd_size]));\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_q, _mm256_permutevar8x32_epi32(v_reverse, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n        if (_mm256_movemask_epi8(v_cmp) != 0xFFFFFFFF) {\n            is_palindromic = false;\n        }\n    }\n\n    // Horizontally add the accumulated sums in the SIMD register\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), v_sum);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    // Process remaining elements that didn't fit into the SIMD loop\n    for (int i = aligned_size; i < n; ++i) {\n        sum += q[i];\n        if (q[i] != q[n - i - 1]) {\n            is_palindromic = false;\n        }\n    }\n\n    // Final check: balanced and weight condition\n    return is_palindromic && (sum <= w);\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_q(padded_size, 0);\n    for (int i = 0; i < n; ++i) {\n        padded_q[i] = q[i];\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    bool is_palindromic = true;\n\n    for (int i = 0; i < padded_size; i += 8) {\n        // Load 8 elements from the vector\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_q[i]));\n\n        // Accumulate the sum of the elements\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        // Check palindromic property\n        __m256i reverse_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_q[padded_size - i - 8]));\n        __m256i cmp = _mm256_cmpeq_epi32(vec, reverse_vec);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        if (mask != 0xFF) {\n            is_palindromic = false;\n        }\n    }\n\n    // Horizontal sum of the accumulated sums\n    int sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    // Check conditions\n    return is_palindromic && (sum <= w);\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    int n = q.size();\n    if (n == 0) return false;\n\n    // Ensure the vector size is a multiple of 8 for AVX2 processing\n    int padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_q(padded_size, 0);\n    for (int i = 0; i < n; ++i) {\n        padded_q[i] = q[i];\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    bool is_palindromic = true;\n\n    for (int i = 0; i < padded_size; i += 8) {\n        // Load 8 elements from the vector\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_q[i]));\n\n        // Add to the running sum\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n\n        // Check palindromic property\n        __m256i reverse_vec = _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        __m256i cmp = _mm256_cmpeq_epi32(vec, reverse_vec);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFFFFFF) {\n            is_palindromic = false;\n        }\n    }\n\n    // Horizontal sum of the accumulated sums in sum_vec\n    int sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n    for (int i = 0; i < 8; ++i) {\n        sum += temp[i];\n    }\n\n    // Check if the sum is within the weight limit and if the vector is palindromic\n    return (sum <= w) && is_palindromic;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int result = 0;\n\n    // Process 8 elements at a time using AVX2\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n\n        // Reverse the order of the right vector\n        right = _mm256_permutevar8x32_epi32(right, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare left and right vectors\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n\n        // Create a mask to count mismatches\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        result += _mm_popcnt_u32(~mask & 0xFF);\n    }\n\n    // Process remaining elements\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int result = 0;\n\n    // Process 8 elements at a time using AVX2\n    int simd_size = 8;\n    int simd_aligned_size = (n / simd_size) * simd_size;\n\n    for (int i = 0; i < simd_aligned_size / 2; i += simd_size) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - simd_size]);\n\n        // Reverse the order of elements in 'right' vector\n        right = _mm256_permutevar8x32_epi32(right, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare left and reversed right vectors\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n\n        // Count non-equal elements\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        result += _mm_popcnt_u32(~mask & 0xFF);\n    }\n\n    // Process remaining elements that didn't fit into SIMD processing\n    for (int i = simd_aligned_size / 2; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int result = 0;\n\n    // Process 8 elements at a time using AVX2\n    int simd_size = 8;\n    int simd_aligned_size = (n / simd_size) * simd_size;\n\n    for (int i = 0; i < simd_aligned_size / 2; i += simd_size) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - simd_size]);\n\n        // Reverse the order of elements in 'right'\n        right = _mm256_permutevar8x32_epi32(right, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare left and reversed right\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n\n        // Count mismatches\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        result += _mm_popcnt_u32(~mask & 0xFF);\n    }\n\n    // Process remaining elements that are not a multiple of 8\n    for (int i = simd_aligned_size / 2; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int result = 0;\n\n    // Process 8 elements at a time using AVX2\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        // Load 8 integers from the start and end of the current window\n        __m256i vec_start = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vec_end = _mm256_loadu_si256((__m256i*)&arr[n - i - 8]);\n\n        // Reverse the order of the end vector\n        __m256i vec_end_reversed = _mm256_permutevar8x32_epi32(vec_end, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare the two vectors for equality\n        __m256i cmp = _mm256_cmpeq_epi32(vec_start, vec_end_reversed);\n\n        // Create a mask to count mismatches\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n        // Count the number of mismatches (0 bits in the mask)\n        result += _mm_popcnt_u32(~mask & 0xFF);\n    }\n\n    // Process remaining elements sequentially\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    int result = 0;\n\n    // Process 8 elements at a time using AVX2\n    int simd_size = 8;\n    int simd_aligned_size = (n / simd_size) * simd_size;\n\n    for (int i = 0; i < simd_aligned_size; i += simd_size) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - simd_size]);\n\n        // Reverse the order of elements in 'right'\n        right = _mm256_permutevar8x32_epi32(right, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n\n        // Compare left and reversed right\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n\n        // Count mismatches\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        result += _mm_popcnt_u32(~mask & 0xFF);\n    }\n\n    // Process remaining elements\n    for (int i = simd_aligned_size; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++result;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t total1 = 0, total2 = 0;\n\n    // Process lst1\n    for (const auto& str : lst1) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            total1 += 32 - __builtin_popcount(mask);\n        }\n        // Process remaining characters\n        total1 += len - i;\n    }\n\n    // Process lst2\n    for (const auto& str : lst2) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            total2 += 32 - __builtin_popcount(mask);\n        }\n        // Process remaining characters\n        total2 += len - i;\n    }\n\n    // Compare totals and return the appropriate vector\n    if (total1 < total2) {\n        return lst1;\n    } else if (total2 < total1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t total1 = 0, total2 = 0;\n\n    // Process lst1\n    for (const auto& str : lst1) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            total1 += 32 - __builtin_popcount(mask);\n        }\n        // Process remaining characters\n        total1 += len - i;\n    }\n\n    // Process lst2\n    for (const auto& str : lst2) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            total2 += 32 - __builtin_popcount(mask);\n        }\n        // Process remaining characters\n        total2 += len - i;\n    }\n\n    // Compare totals and return the appropriate vector\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t total1 = 0, total2 = 0;\n\n    // Process lst1\n    for (const auto& str : lst1) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            total1 += 32 - __builtin_popcount(mask);\n        }\n        // Process remaining characters\n        for (; i < len; ++i) {\n            total1 += (str[i] != '\\0');\n        }\n    }\n\n    // Process lst2\n    for (const auto& str : lst2) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            total2 += 32 - __builtin_popcount(mask);\n        }\n        // Process remaining characters\n        for (; i < len; ++i) {\n            total2 += (str[i] != '\\0');\n        }\n    }\n\n    // Compare totals and return the appropriate vector\n    if (total1 <= total2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t total1 = 0, total2 = 0;\n\n    // Process lst1\n    for (const auto& str : lst1) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                // Count non-zero characters up to the first zero\n                int pos = __builtin_ctz(mask);\n                total1 += pos;\n                break;\n            }\n            total1 += 32;\n        }\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if (str[i] == '\\0') break;\n            ++total1;\n        }\n    }\n\n    // Process lst2\n    for (const auto& str : lst2) {\n        size_t len = str.size();\n        size_t i = 0;\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                // Count non-zero characters up to the first zero\n                int pos = __builtin_ctz(mask);\n                total2 += pos;\n                break;\n            }\n            total2 += 32;\n        }\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if (str[i] == '\\0') break;\n            ++total2;\n        }\n    }\n\n    // Return the vector with fewer total characters, or lst1 if equal\n    return (total1 <= total2) ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2) {\n    size_t total_chars_lst1 = 0;\n    size_t total_chars_lst2 = 0;\n\n    // Process lst1\n    for (const auto& str : lst1) {\n        size_t len = str.size();\n        size_t i = 0;\n\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                // Count non-zero characters up to the first zero\n                int first_zero = __builtin_ctz(mask);\n                total_chars_lst1 += first_zero;\n                break;\n            }\n            total_chars_lst1 += 32;\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if (str[i] == '\\0') break;\n            ++total_chars_lst1;\n        }\n    }\n\n    // Process lst2\n    for (const auto& str : lst2) {\n        size_t len = str.size();\n        size_t i = 0;\n\n        // Process in chunks of 32 bytes using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_setzero_si256());\n            int mask = _mm256_movemask_epi8(cmp);\n            if (mask != 0) {\n                // Count non-zero characters up to the first zero\n                int first_zero = __builtin_ctz(mask);\n                total_chars_lst2 += first_zero;\n                break;\n            }\n            total_chars_lst2 += 32;\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if (str[i] == '\\0') break;\n            ++total_chars_lst2;\n        }\n    }\n\n    // Compare total character counts and return the appropriate vector\n    if (total_chars_lst1 <= total_chars_lst2) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n <= 1 || x == 0) return false;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vresult = _mm256_setzero_si256();\n    __m256i vone = _mm256_set1_epi64x(1);\n    __m256i vpower = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 64; ++i) {\n        vpower = _mm256_mul_epu32(vpower, vn);\n        __m256i cmp = _mm256_cmpeq_epi64(vpower, vx);\n        vresult = _mm256_or_si256(vresult, cmp);\n\n        // Check if any element in vresult is non-zero\n        int mask = _mm256_movemask_epi8(vresult);\n        if (mask != 0) {\n            return true;\n        }\n\n        // Break early if power exceeds x\n        __m256i cmp_gt = _mm256_cmpgt_epi64(vpower, vx);\n        int gt_mask = _mm256_movemask_epi8(cmp_gt);\n        if (gt_mask != 0) {\n            break;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n <= 1 || x == 0) return false;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vpow = _mm256_set1_epi64x(1);\n    __m256i vresult = _mm256_setzero_si256();\n    __m256i vcmp = _mm256_setzero_si256();\n\n    for (int i = 0; i < 64; ++i) {\n        vpow = _mm256_mullo_epi64(vpow, vn);\n        vcmp = _mm256_cmpeq_epi64(vpow, vx);\n        vresult = _mm256_or_si256(vresult, vcmp);\n\n        __m256i mask = _mm256_cmpgt_epi64(vpow, vx);\n        if (_mm256_movemask_epi8(mask) == 0xFFFF) break;\n    }\n\n    int mask_result = _mm256_movemask_epi8(vresult);\n    return mask_result != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n <= 1 || x == 0) return false;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vresult = _mm256_setzero_si256();\n    __m256i vone = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i vpow = _mm256_set1_epi64x(1);\n        __m256i vtemp = _mm256_set1_epi64x(1);\n        for (int j = 0; j < 64; ++j) {\n            vtemp = _mm256_mullo_epi64(vtemp, vn);\n            __m256i mask = _mm256_cmpeq_epi64(vtemp, vx);\n            vresult = _mm256_or_si256(vresult, mask);\n            __m256i overflow_mask = _mm256_cmpgt_epi64(vtemp, vx);\n            if (_mm256_movemask_epi8(overflow_mask)) break;\n        }\n    }\n\n    int mask = _mm256_movemask_epi8(vresult);\n    return mask != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n <= 1 || x == 0) return false;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vresult = _mm256_setzero_si256();\n    __m256i vone = _mm256_set1_epi64x(1);\n\n    for (int i = 0; i < 4; ++i) {\n        __m256i vpower = _mm256_set1_epi64x(1);\n        __m256i vtemp = _mm256_set1_epi64x(n);\n        __m256i vcmp = _mm256_setzero_si256();\n\n        for (int j = 0; j < 64; ++j) {\n            vcmp = _mm256_cmpeq_epi64(vpower, vx);\n            vresult = _mm256_or_si256(vresult, vcmp);\n            vpower = _mm256_mullo_epi64(vpower, vtemp);\n        }\n    }\n\n    int mask = _mm256_movemask_epi8(vresult);\n    return mask != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    if (n <= 1 || x == 0) return false;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vpow = _mm256_set1_epi64x(1);\n    __m256i vcmp = _mm256_setzero_si256();\n    __m256i vresult = _mm256_setzero_si256();\n\n    for (int i = 0; i < 64; ++i) {\n        vpow = _mm256_mullo_epi64(vpow, vn);\n        vcmp = _mm256_cmpeq_epi64(vpow, vx);\n        vresult = _mm256_or_si256(vresult, vcmp);\n\n        __m256i mask = _mm256_cmpgt_epi64(vpow, vx);\n        if (_mm256_movemask_epi8(mask) == 0xFFFFFFFF) break;\n    }\n\n    int mask_result = _mm256_movemask_epi8(vresult);\n    return mask_result != 0;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    // Prime digits in hexadecimal: '2', '3', '5', '7', 'B', 'D'\n    const __m256i prime_chars = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, \n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= len; i += 32) {\n        // Load 32 characters from the string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n\n        // Compare each character with the prime characters\n        __m256i cmp2 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('2'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('3'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('5'));\n        __m256i cmp7 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('7'));\n        __m256i cmpB = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('B'));\n        __m256i cmpD = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('D'));\n\n        // Combine all comparisons using bitwise OR\n        __m256i is_prime = _mm256_or_si256(_mm256_or_si256(cmp2, cmp3), \n                                           _mm256_or_si256(cmp5, cmp7));\n        is_prime = _mm256_or_si256(is_prime, _mm256_or_si256(cmpB, cmpD));\n\n        // Create a mask of matching characters\n        uint32_t mask = _mm256_movemask_epi8(is_prime);\n\n        // Count the number of set bits in the mask\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining characters (less than 32)\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    // Prime digits in hexadecimal: '2', '3', '5', '7', 'B', 'D'\n    const __m256i prime_chars = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, \n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= len; i += 32) {\n        // Load 32 characters from the string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n\n        // Compare each character with the prime characters\n        __m256i cmp1 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('2'));\n        __m256i cmp2 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('3'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('5'));\n        __m256i cmp4 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('7'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('B'));\n        __m256i cmp6 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('D'));\n\n        // Combine all comparisons using bitwise OR\n        __m256i cmp_result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), \n                                             _mm256_or_si256(cmp3, cmp4));\n        cmp_result = _mm256_or_si256(cmp_result, _mm256_or_si256(cmp5, cmp6));\n\n        // Create a mask to count the number of matches\n        int mask = _mm256_movemask_epi8(cmp_result);\n\n        // Count the number of set bits in the mask\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining characters (less than 32)\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    // Prime digits in hexadecimal: '2', '3', '5', '7', 'B', 'D'\n    const __m256i prime_chars = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, \n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= len; i += 32) {\n        // Load 32 characters from the input string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n\n        // Compare each character with the prime characters\n        __m256i cmp1 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('2'));\n        __m256i cmp2 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('3'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('5'));\n        __m256i cmp4 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('7'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('B'));\n        __m256i cmp6 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('D'));\n\n        // Combine all comparisons using bitwise OR\n        __m256i result = _mm256_or_si256(cmp1, cmp2);\n        result = _mm256_or_si256(result, cmp3);\n        result = _mm256_or_si256(result, cmp4);\n        result = _mm256_or_si256(result, cmp5);\n        result = _mm256_or_si256(result, cmp6);\n\n        // Convert the result to a bitmask and count the number of set bits\n        int mask = _mm256_movemask_epi8(result);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining characters (less than 32)\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    // Prime digits in hexadecimal: '2', '3', '5', '7', 'B', 'D'\n    const __m256i prime_chars = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, \n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= len; i += 32) {\n        // Load 32 characters from the string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n\n        // Compare each character with the prime characters\n        __m256i cmp1 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('2'));\n        __m256i cmp2 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('3'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('5'));\n        __m256i cmp4 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('7'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('B'));\n        __m256i cmp6 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('D'));\n\n        // Combine all comparisons using bitwise OR\n        __m256i is_prime = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), \n                                           _mm256_or_si256(cmp3, cmp4));\n        is_prime = _mm256_or_si256(is_prime, _mm256_or_si256(cmp5, cmp6));\n\n        // Count the number of primes in this block\n        count += _mm_popcnt_u32(_mm256_movemask_epi8(is_prime));\n    }\n\n    // Process remaining characters (less than 32)\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint hex_key_simd(const std::string & num) {\n    // Prime digits in hexadecimal: '2', '3', '5', '7', 'B', 'D'\n    const __m256i prime_chars = _mm256_setr_epi8('2', '3', '5', '7', 'B', 'D', 0, 0, \n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0,\n                                                 0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n    size_t len = num.size();\n    size_t i = 0;\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= len; i += 32) {\n        // Load 32 characters from the input string\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n\n        // Compare each character with the prime characters\n        __m256i cmp1 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('2'));\n        __m256i cmp2 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('3'));\n        __m256i cmp3 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('5'));\n        __m256i cmp4 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('7'));\n        __m256i cmp5 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('B'));\n        __m256i cmp6 = _mm256_cmpeq_epi8(input, _mm256_set1_epi8('D'));\n\n        // Combine all comparisons using bitwise OR\n        __m256i is_prime = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), \n                                           _mm256_or_si256(cmp3, cmp4));\n        is_prime = _mm256_or_si256(is_prime, _mm256_or_si256(cmp5, cmp6));\n\n        // Create a mask to count the number of primes in this block\n        int mask = _mm256_movemask_epi8(is_prime);\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining characters (less than 32)\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    __m256i prev1 = _mm256_set1_epi8(0);\n    __m256i prev2 = _mm256_set1_epi8(0);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Compare current with previous two vectors\n        __m256i cmp1 = _mm256_cmpeq_epi8(curr, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(curr, prev2);\n\n        // Check for any matches in the current block\n        int mask1 = _mm256_movemask_epi8(cmp1);\n        int mask2 = _mm256_movemask_epi8(cmp2);\n\n        if (mask1 || mask2) return false;\n\n        // Shift the previous vectors\n        prev2 = prev1;\n        prev1 = curr;\n    }\n\n    // Handle the remaining characters (less than 32)\n    for (size_t i = len - (len % 32); i < len - 2; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    __m256i prev1 = _mm256_set1_epi8(0);\n    __m256i prev2 = _mm256_set1_epi8(0);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Compare current with previous two vectors\n        __m256i cmp1 = _mm256_cmpeq_epi8(current, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(current, prev2);\n\n        // Check for any matches in the current block\n        int mask1 = _mm256_movemask_epi8(cmp1);\n        int mask2 = _mm256_movemask_epi8(cmp2);\n\n        if (mask1 || mask2) return false;\n\n        // Shift the previous vectors\n        prev2 = prev1;\n        prev1 = current;\n    }\n\n    // Handle the remaining characters (less than 32)\n    for (size_t i = len - (len % 32); i < len; ++i) {\n        if ((i >= 1 && s[i] == s[i - 1]) || (i >= 2 && s[i] == s[i - 2])) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    __m256i prev1 = _mm256_set1_epi8(0);\n    __m256i prev2 = _mm256_set1_epi8(0);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Compare current with previous two vectors\n        __m256i cmp1 = _mm256_cmpeq_epi8(curr, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(curr, prev2);\n\n        // Check for any matches within the current vector\n        __m256i shifted1 = _mm256_alignr_epi8(curr, prev1, 15);\n        __m256i shifted2 = _mm256_alignr_epi8(curr, prev1, 14);\n        __m256i cmp3 = _mm256_cmpeq_epi8(curr, shifted1);\n        __m256i cmp4 = _mm256_cmpeq_epi8(curr, shifted2);\n\n        // Combine all comparisons\n        __m256i or_result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp3, cmp4));\n        int mask = _mm256_movemask_epi8(or_result);\n\n        if (mask != 0) return false;\n\n        prev2 = prev1;\n        prev1 = curr;\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len - 2; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    __m256i prev1 = _mm256_set1_epi8(0);\n    __m256i prev2 = _mm256_set1_epi8(0);\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Compare current with previous two vectors\n        __m256i cmp1 = _mm256_cmpeq_epi8(current, prev1);\n        __m256i cmp2 = _mm256_cmpeq_epi8(current, prev2);\n\n        // Check for any matches within the current vector\n        __m256i shifted1 = _mm256_alignr_epi8(current, prev1, 15);\n        __m256i shifted2 = _mm256_alignr_epi8(current, prev2, 15);\n\n        __m256i cmp_shift1 = _mm256_cmpeq_epi8(current, shifted1);\n        __m256i cmp_shift2 = _mm256_cmpeq_epi8(current, shifted2);\n\n        __m256i or_result = _mm256_or_si256(_mm256_or_si256(cmp1, cmp2), _mm256_or_si256(cmp_shift1, cmp_shift2));\n        int mask = _mm256_movemask_epi8(or_result);\n\n        if (mask != 0) return false;\n\n        prev2 = prev1;\n        prev1 = current;\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - len % 32; i < len - 2; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s) {\n    size_t len = s.size();\n    if (len < 3) return false;\n\n    __m256i mask = _mm256_set1_epi8(0xFF);\n    __m256i prev_chunk = _mm256_setzero_si256();\n\n    for (size_t i = 0; i <= len - 32; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Compare each byte with the next one and the one after that\n        __m256i cmp1 = _mm256_cmpeq_epi8(chunk, _mm256_alignr_epi8(chunk, prev_chunk, 15));\n        __m256i cmp2 = _mm256_cmpeq_epi8(chunk, _mm256_alignr_epi8(chunk, prev_chunk, 14));\n\n        // Combine comparisons\n        __m256i cmp_or = _mm256_or_si256(cmp1, cmp2);\n\n        // Check if any of the bytes are equal\n        int mask_result = _mm256_movemask_epi8(cmp_or);\n        if (mask_result != 0) return false;\n\n        prev_chunk = chunk;\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - (len % 32); i <= len - 3; ++i) {\n        if (s[i] == s[i + 1] || s[i] == s[i + 2] || s[i + 1] == s[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len < 2) return false;\n\n    __m256i vec_len = _mm256_set1_epi32(len);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_prime = _mm256_set1_epi32(1);\n\n    for (int i = 2; i * i <= len; i += 8) {\n        __m256i vec_i = _mm256_setr_epi32(i, i + 1, i + 2, i + 3, i + 4, i + 5, i + 6, i + 7);\n        __m256i vec_mod = _mm256_rem_epi32(vec_len, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero);\n        vec_prime = _mm256_andnot_si256(vec_cmp, vec_prime);\n\n        if (_mm256_testz_si256(vec_prime, vec_prime) == 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len < 2) return false;\n\n    // Load the length into a vector register for parallel processing\n    __m256i vec_len = _mm256_set1_epi32(len);\n\n    // Prepare a vector of potential divisors (2, 3, 4, 5, 6, 7, 8, 9)\n    __m256i divisors = _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9);\n\n    // Compute modulo operations in parallel\n    __m256i mod_results = _mm256_setzero_si256();\n    mod_results = _mm256_rem_epi32(vec_len, divisors);\n\n    // Check if any modulo result is zero (indicating divisibility)\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i cmp_results = _mm256_cmpeq_epi32(mod_results, zero_vec);\n\n    // Extract mask to check results\n    int mask = _mm256_movemask_epi8(cmp_results);\n\n    // If any divisor divides the length evenly, it's not prime\n    if (mask != 0) return false;\n\n    // Handle edge cases and larger divisors sequentially\n    for (size_t i = 10; i * i <= len; ++i) {\n        if (len % i == 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len < 2) return false;\n\n    // Load the length into a vector register for parallel processing\n    __m256i vec_len = _mm256_set1_epi32(len);\n\n    // Prepare a vector of potential divisors (2, 3, 4, 5, 6, 7, 8, 9)\n    __m256i divisors = _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9);\n\n    // Compute modulo operations in parallel\n    __m256i mod_results = _mm256_setzero_si256();\n    mod_results = _mm256_rem_epi32(vec_len, divisors);\n\n    // Check if any modulo result is zero\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i cmp_results = _mm256_cmpeq_epi32(mod_results, zero_vec);\n\n    // Extract mask to check results\n    int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_results));\n\n    // If any divisor divides the length evenly, it's not prime\n    if (mask != 0) return false;\n\n    // Handle divisors greater than 9 sequentially\n    for (int i = 10; i * i <= len; ++i) {\n        if (len % i == 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len < 2) return false;\n\n    // Load the length into a vector register for parallel processing\n    __m256i len_vec = _mm256_set1_epi32(len);\n\n    // Prepare a vector of potential divisors (2, 3, 4, 5, 6, 7, 8, 9)\n    __m256i divisors = _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9);\n\n    // Perform division checks in parallel\n    __m256i mod_results = _mm256_rem_epi32(len_vec, divisors);\n\n    // Check if any modulus result is zero\n    int mask = _mm256_movemask_ps((__m256)_mm256_cmpeq_epi32(mod_results, _mm256_setzero_si256()));\n\n    // If any modulus result is zero, it's not a prime\n    if (mask != 0) return false;\n\n    // Handle divisors beyond 9 with scalar code\n    for (int i = 10; i * i <= len; ++i) {\n        if (len % i == 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str) {\n    size_t len = str.length();\n    if (len < 2) return false;\n\n    // Load the length into a vector register for parallel processing\n    __m256i vec_len = _mm256_set1_epi32(len);\n\n    // Prepare a vector of potential divisors (2, 3, 4, 5, 6, 7, 8, 9)\n    __m256i divisors = _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9);\n\n    // Perform division checks in parallel\n    __m256i mod_results = _mm256_rem_epi32(vec_len, divisors);\n\n    // Check if any modulus result is zero\n    int mask = _mm256_movemask_ps((__m256)_mm256_cmpeq_epi32(mod_results, _mm256_setzero_si256()));\n\n    // If any modulus result is zero, it's not a prime\n    if (mask != 0) return false;\n\n    // Handle divisors beyond 9 sequentially\n    for (size_t i = 10; i * i <= len; ++i) {\n        if (len % i == 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vdigits = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    uint64_t temp = N;\n    int total_sum = 0;\n\n    while (temp > 0) {\n        uint64_t block = temp % 100000000; // Process 8 digits at a time\n        temp /= 100000000;\n\n        __m256i vblock = _mm256_set1_epi64x(block);\n        __m256i vdigit_mask = _mm256_set1_epi64x(10);\n\n        for (int i = 0; i < 8; ++i) {\n            __m256i vdigit = _mm256_and_si256(vblock, _mm256_set1_epi64x(0xF));\n            vsum = _mm256_add_epi64(vsum, vdigit);\n            vblock = _mm256_srli_epi64(vblock, 4);\n        }\n    }\n\n    alignas(32) int sums[8];\n    _mm256_store_si256((__m256i*)sums, vsum);\n\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sums[i];\n    }\n\n    std::string binary = std::bitset<32>(total_sum).to_string();\n    auto first_one = binary.find('1');\n    return first_one != std::string::npos ? binary.substr(first_one) : \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vdigits = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    uint64_t temp = N;\n    int total_sum = 0;\n\n    while (temp > 0) {\n        uint64_t block = temp % 100000000; // Process 8 digits at a time\n        temp /= 100000000;\n\n        __m256i vblock = _mm256_set1_epi64x(block);\n        __m256i vmod = _mm256_set1_epi64x(10);\n        __m256i vdigit_sum = _mm256_setzero_si256();\n\n        for (int i = 0; i < 8; ++i) {\n            __m256i vremainder = _mm256_rem_epu64(vblock, vmod);\n            vdigit_sum = _mm256_add_epi64(vdigit_sum, vremainder);\n            vblock = _mm256_div_epu64(vblock, vmod);\n        }\n\n        alignas(32) uint64_t sums[4];\n        _mm256_store_si256((__m256i*)sums, vdigit_sum);\n        for (int i = 0; i < 4; ++i) {\n            total_sum += sums[i];\n        }\n    }\n\n    std::string binary = std::bitset<32>(total_sum).to_string();\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    return first_one != std::string::npos ? binary.substr(first_one) : \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i tens = _mm256_set1_epi8(10);\n    __m256i ones = _mm256_set1_epi8(1);\n\n    while (N > 0) {\n        uint64_t chunk = N % 100000000;\n        N /= 100000000;\n\n        __m256i digits = _mm256_setr_epi64x(\n            (chunk / 1) % 10,\n            (chunk / 10) % 10,\n            (chunk / 100) % 10,\n            (chunk / 1000) % 10\n        );\n\n        vsum = _mm256_add_epi8(vsum, digits);\n    }\n\n    alignas(32) uint8_t temp[32];\n    _mm256_store_si256((__m256i*)temp, vsum);\n\n    int total_sum = 0;\n    for (int i = 0; i < 32; ++i) {\n        total_sum += temp[i];\n    }\n\n    std::string binary = std::bitset<32>(total_sum).to_string();\n    auto first_one = binary.find('1');\n    return first_one != std::string::npos ? binary.substr(first_one) : \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vsum = _mm256_setzero_si256();\n    uint64_t temp = N;\n    int digit_count = 0;\n\n    // Extract digits and load into AVX registers\n    while (temp > 0) {\n        uint64_t chunk = temp % 1000000000; // Process 9 digits at a time\n        temp /= 1000000000;\n\n        // Load digits into AVX register\n        int digits[8] = {0};\n        for (int i = 0; i < 8 && chunk > 0; ++i) {\n            digits[i] = chunk % 10;\n            chunk /= 10;\n        }\n        __m256i vdigits = _mm256_loadu_si256((__m256i*)digits);\n\n        // Add to the running sum\n        vsum = _mm256_add_epi32(vsum, vdigits);\n        digit_count += 8;\n    }\n\n    // Horizontal sum of the AVX register\n    int sums[8];\n    _mm256_storeu_si256((__m256i*)sums, vsum);\n    int total_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        total_sum += sums[i];\n    }\n\n    // Convert the sum to binary string\n    std::string binary = std::bitset<32>(total_sum).to_string();\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    return first_one != std::string::npos ? binary.substr(first_one) : \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string solve_simd(uint64_t N) {\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vdigits = _mm256_setr_epi8('0','1','2','3','4','5','6','7','8','9', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i vmask = _mm256_set1_epi8(0xF);\n    \n    while (N > 0) {\n        uint64_t chunk = N % 1000000000; // Process 9 digits at a time\n        N /= 1000000000;\n\n        __m256i vchunk = _mm256_set1_epi64x(chunk);\n        __m256i vdigit_vals = _mm256_setzero_si256();\n\n        for (int i = 0; i < 9; ++i) {\n            __m256i vdigit = _mm256_and_si256(vchunk, vmask);\n            vdigit_vals = _mm256_add_epi8(vdigit_vals, _mm256_shuffle_epi8(vdigits, vdigit));\n            vchunk = _mm256_srli_epi64(vchunk, 4);\n        }\n\n        vsum = _mm256_add_epi8(vsum, vdigit_vals);\n    }\n\n    alignas(32) uint8_t temp[32];\n    _mm256_store_si256((__m256i*)temp, vsum);\n\n    int total_sum = 0;\n    for (int i = 0; i < 32; ++i) {\n        total_sum += temp[i];\n    }\n\n    return std::bitset<32>(total_sum).to_string();\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int result = 0;\n    int size = lst.size();\n    int aligned_size = size - (size % 8);\n    __m256i indices = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n    __m256i increment = _mm256_set1_epi32(8);\n\n    for (int i = 1; i < aligned_size; i += 8) {\n        __m256i values = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask_even = _mm256_set1_epi32(1);\n        __m256i is_even = _mm256_and_si256(values, mask_even);\n        __m256i mask_odd_indices = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_even), mask_even);\n        __m256i mask = _mm256_andnot_si256(is_even, mask_odd_indices);\n        __m256i selected = _mm256_blendv_epi8(_mm256_setzero_si256(), values, mask);\n        \n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, selected);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    for (int i = aligned_size; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int result = 0;\n    int size = lst.size();\n    int i = 1; // Start from the first odd index\n\n    // Ensure there are at least 8 elements to process with AVX\n    while (i + 7 < size) {\n        // Load 8 integers from the list into a SIMD register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Create a mask for even numbers (0xFFFFFFFF for even, 0x0 for odd)\n        __m256i even_mask = _mm256_set1_epi32(1);\n        __m256i is_even = _mm256_and_si256(vec, even_mask);\n        __m256i even_check = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n        // Create a mask for odd indices (0xFFFFFFFF for odd indices, 0x0 for even indices)\n        __m256i index_mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n        // Combine masks: only keep values that are even and at odd indices\n        __m256i combined_mask = _mm256_and_si256(even_check, index_mask);\n\n        // Blend the vector: keep only the values that pass the combined mask\n        __m256i blended = _mm256_blendv_epi8(_mm256_setzero_si256(), vec, combined_mask);\n\n        // Horizontally add the results in the SIMD register\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), blended);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n\n        // Move to the next block of 8 elements\n        i += 8;\n    }\n\n    // Process remaining elements\n    for (; i < size; i += 2) {\n        if (lst[i] % 2 == 0) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int result = 0;\n    int size = lst.size();\n    int i = 1; // Start from the first odd index\n\n    // Process 8 integers at a time using AVX2\n    __m256i indices = _mm256_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15);\n    __m256i increment = _mm256_set1_epi32(16);\n\n    while (i + 7 < size) {\n        // Load 8 integers from the vector starting at index i\n        __m256i values = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        // Create a mask for even numbers (value & 1 == 0)\n        __m256i is_even = _mm256_and_si256(values, _mm256_set1_epi32(1));\n        __m256i even_mask = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n        // Blend values based on the even mask\n        __m256i selected_values = _mm256_blendv_epi8(_mm256_setzero_si256(), values, even_mask);\n\n        // Horizontally add the selected values\n        __m128i low = _mm256_castsi256_si128(selected_values);\n        __m128i high = _mm256_extracti128_si256(selected_values, 1);\n        __m128i sum128 = _mm_add_epi32(low, high);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n        sum128 = _mm_hadd_epi32(sum128, sum128);\n\n        // Extract the sum and add to the result\n        result += _mm_cvtsi128_si32(sum128);\n\n        // Increment indices and move to the next block\n        indices = _mm256_add_epi32(indices, increment);\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < size; i += 2) {\n        if ((lst[i] & 1) == 0) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int result = 0;\n    int size = lst.size();\n    int aligned_size = size - (size % 8);\n    __m256i indices = _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 8);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 1; i < aligned_size; i += 8) {\n        __m256i values = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask_even = _mm256_and_si256(values, two);\n        __m256i is_even = _mm256_cmpeq_epi32(mask_even, zero);\n        __m256i mask_odd_indices = _mm256_setr_epi32(\n            i % 2 == 1 ? -1 : 0,\n            (i + 1) % 2 == 1 ? -1 : 0,\n            (i + 2) % 2 == 1 ? -1 : 0,\n            (i + 3) % 2 == 1 ? -1 : 0,\n            (i + 4) % 2 == 1 ? -1 : 0,\n            (i + 5) % 2 == 1 ? -1 : 0,\n            (i + 6) % 2 == 1 ? -1 : 0,\n            (i + 7) % 2 == 1 ? -1 : 0\n        );\n        __m256i combined_mask = _mm256_and_si256(is_even, mask_odd_indices);\n        __m256i masked_values = _mm256_blendv_epi8(zero, values, combined_mask);\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, masked_values);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n    }\n\n    for (int i = aligned_size; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_simd(std::vector<int> lst) {\n    int result = 0;\n    int size = lst.size();\n    int i = 1; // Start from the first odd index\n\n    // Process 8 integers at a time using AVX2\n    __m256i mask_even = _mm256_set1_epi32(1); // Mask to check even numbers (n & 1 == 0)\n    __m256i sum_vec = _mm256_setzero_si256(); // Accumulator for SIMD results\n\n    while (i + 7 < size) {\n        // Load 8 integers starting from the current odd index\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Check which elements are even\n        __m256i is_even = _mm256_and_si256(vec, mask_even);\n        __m256i even_mask = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n        // Blend the even elements into the accumulator\n        __m256i even_elements = _mm256_blendv_epi8(_mm256_setzero_si256(), vec, even_mask);\n        sum_vec = _mm256_add_epi32(sum_vec, even_elements);\n\n        i += 8; // Move to the next block of 8 integers\n    }\n\n    // Horizontal sum of the SIMD accumulator\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n\n    // Process remaining elements\n    while (i < size) {\n        if (lst[i] % 2 == 0) {\n            result += lst[i];\n        }\n        i += 2; // Skip to the next odd index\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n    \n    // Process 32 bytes at a time using AVX2\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Create a mask to identify alphabetic characters\n        __m256i lower_a = _mm256_set1_epi8('a');\n        __m256i upper_a = _mm256_set1_epi8('A');\n        __m256i lower_z = _mm256_set1_epi8('z');\n        __m256i upper_z = _mm256_set1_epi8('Z');\n        \n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(input, lower_a), _mm256_cmpgt_epi8(lower_z, input));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(input, upper_a), _mm256_cmpgt_epi8(upper_z, input));\n        __m256i is_alpha = _mm256_or_si256(is_lower, is_upper);\n        \n        // Compute the rotation (shift by 4 positions)\n        __m256i shift = _mm256_set1_epi8(4);\n        __m256i rotated = _mm256_add_epi8(input, shift);\n        \n        // Wrap around for lowercase letters\n        __m256i wrap_lower = _mm256_and_si256(is_lower, _mm256_cmpgt_epi8(rotated, lower_z));\n        __m256i adjust_lower = _mm256_sub_epi8(rotated, _mm256_set1_epi8(26));\n        rotated = _mm256_blendv_epi8(rotated, adjust_lower, wrap_lower);\n        \n        // Wrap around for uppercase letters\n        __m256i wrap_upper = _mm256_and_si256(is_upper, _mm256_cmpgt_epi8(rotated, upper_z));\n        __m256i adjust_upper = _mm256_sub_epi8(rotated, _mm256_set1_epi8(26));\n        rotated = _mm256_blendv_epi8(rotated, adjust_upper, wrap_upper);\n        \n        // Blend original and rotated based on alphabetic mask\n        __m256i output = _mm256_blendv_epi8(input, rotated, is_alpha);\n        \n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), output);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if ('a' <= c && c <= 'z') {\n            c = (c - 'a' + 4) % 26 + 'a';\n        } else if ('A' <= c && c <= 'Z') {\n            c = (c - 'A' + 4) % 26 + 'A';\n        }\n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n    \n    // Process 32 bytes at a time using AVX2\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Create a mask to identify alphabetic characters\n        __m256i lower_a = _mm256_set1_epi8('a');\n        __m256i upper_a = _mm256_set1_epi8('A');\n        __m256i lower_z = _mm256_set1_epi8('z');\n        __m256i upper_z = _mm256_set1_epi8('Z');\n        \n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(input, lower_a), _mm256_cmpgt_epi8(lower_z, input));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(input, upper_a), _mm256_cmpgt_epi8(upper_z, input));\n        __m256i is_alpha = _mm256_or_si256(is_lower, is_upper);\n        \n        // Compute the shifted values\n        __m256i shift = _mm256_set1_epi8(4); // Shift by 4 positions (2 * 2)\n        __m256i shifted = _mm256_add_epi8(input, shift);\n        \n        // Wrap around for lowercase letters\n        __m256i wrap_lower = _mm256_cmpgt_epi8(shifted, lower_z);\n        __m256i wrapped_lower = _mm256_sub_epi8(shifted, _mm256_set1_epi8(26));\n        shifted = _mm256_blendv_epi8(shifted, wrapped_lower, wrap_lower);\n        \n        // Wrap around for uppercase letters\n        __m256i wrap_upper = _mm256_cmpgt_epi8(shifted, upper_z);\n        __m256i wrapped_upper = _mm256_sub_epi8(shifted, _mm256_set1_epi8(26));\n        shifted = _mm256_blendv_epi8(shifted, wrapped_upper, wrap_upper);\n        \n        // Blend original and shifted values based on alpha mask\n        __m256i output = _mm256_blendv_epi8(input, shifted, is_alpha);\n        \n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), output);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if ('a' <= c && c <= 'z') {\n            c = (c - 'a' + 4) % 26 + 'a';\n        } else if ('A' <= c && c <= 'Z') {\n            c = (c - 'A' + 4) % 26 + 'A';\n        }\n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n    \n    // Process 32 bytes at a time using AVX2\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Create a mask to identify alphabetic characters (a-z and A-Z)\n        __m256i lower_a = _mm256_set1_epi8('a');\n        __m256i upper_z = _mm256_set1_epi8('z');\n        __m256i lower_A = _mm256_set1_epi8('A');\n        __m256i upper_Z = _mm256_set1_epi8('Z');\n        \n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(input, lower_a - 1), _mm256_cmpgt_epi8(upper_z, input));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(input, lower_A - 1), _mm256_cmpgt_epi8(upper_Z, input));\n        __m256i is_alpha = _mm256_or_si256(is_lower, is_upper);\n        \n        // Calculate the shift (2 * 2 = 4 places)\n        __m256i shift = _mm256_set1_epi8(4);\n        \n        // Apply the shift only to alphabetic characters\n        __m256i shifted = _mm256_add_epi8(input, _mm256_and_si256(shift, is_alpha));\n        \n        // Wrap around for lowercase letters\n        __m256i wrap_lower = _mm256_and_si256(_mm256_cmpgt_epi8(shifted, upper_z), is_lower);\n        shifted = _mm256_sub_epi8(shifted, _mm256_and_si256(_mm256_set1_epi8(26), wrap_lower));\n        \n        // Wrap around for uppercase letters\n        __m256i wrap_upper = _mm256_and_si256(_mm256_cmpgt_epi8(shifted, upper_Z), is_upper);\n        shifted = _mm256_sub_epi8(shifted, _mm256_and_si256(_mm256_set1_epi8(26), wrap_upper));\n        \n        // Store the result back into the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), shifted);\n    }\n    \n    // Process remaining characters (if any) using scalar code\n    for (; i < len; ++i) {\n        char c = s[i];\n        if ('a' <= c && c <= 'z') {\n            c = 'a' + (c - 'a' + 4) % 26;\n        } else if ('A' <= c && c <= 'Z') {\n            c = 'A' + (c - 'A' + 4) % 26;\n        }\n        result[i] = c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n    \n    // Process 32 characters at a time using AVX2\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Create a vector of 'a' and 'A' for lower and upper case adjustment\n        __m256i lower_a = _mm256_set1_epi8('a');\n        __m256i upper_a = _mm256_set1_epi8('A');\n        \n        // Create masks to identify lower and upper case letters\n        __m256i is_lower = _mm256_cmpgt_epi8(input, _mm256_set1_epi8('z')); // Inverted mask\n        __m256i is_upper = _mm256_cmpgt_epi8(input, _mm256_set1_epi8('Z')); // Inverted mask\n        \n        // Adjust character range to 0-25 for both lower and upper case\n        __m256i lower_adjusted = _mm256_sub_epi8(input, lower_a);\n        __m256i upper_adjusted = _mm256_sub_epi8(input, upper_a);\n        \n        // Combine adjusted values based on case\n        __m256i adjusted = _mm256_blendv_epi8(upper_adjusted, lower_adjusted, is_lower);\n        \n        // Apply rotation (shift by 4 positions)\n        __m256i rotated = _mm256_add_epi8(adjusted, _mm256_set1_epi8(4));\n        rotated = _mm256_and_si256(rotated, _mm256_set1_epi8(0x1F)); // Modulo 26\n        \n        // Restore original range\n        __m256i lower_restored = _mm256_add_epi8(rotated, lower_a);\n        __m256i upper_restored = _mm256_add_epi8(rotated, upper_a);\n        \n        // Combine restored values based on case\n        __m256i output = _mm256_blendv_epi8(upper_restored, lower_restored, is_lower);\n        \n        // Store the result back to the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), output);\n    }\n    \n    // Process remaining characters (less than 32)\n    for (; i < len; ++i) {\n        char c = s[i];\n        if (c >= 'a' && c <= 'z') {\n            result[i] = 'a' + ((c - 'a' + 4) % 26);\n        } else if (c >= 'A' && c <= 'Z') {\n            result[i] = 'A' + ((c - 'A' + 4) % 26);\n        } else {\n            result[i] = c;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s) {\n    size_t len = s.size();\n    std::string result(len, '\\0');\n    \n    // Process 32 bytes at a time using AVX2\n    size_t i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Create a vector of 'a' and 'A' for lower and upper case adjustment\n        __m256i lower_a = _mm256_set1_epi8('a');\n        __m256i upper_a = _mm256_set1_epi8('A');\n        \n        // Create masks for lowercase and uppercase letters\n        __m256i mask_lower = _mm256_cmpgt_epi8(input, _mm256_set1_epi8('z'));\n        __m256i mask_upper = _mm256_cmpgt_epi8(input, _mm256_set1_epi8('Z'));\n        __m256i is_alpha = _mm256_andnot_si256(mask_lower, _mm256_cmpgt_epi8(input, _mm256_set1_epi8('`')));\n        is_alpha = _mm256_or_si256(is_alpha, _mm256_andnot_si256(mask_upper, _mm256_cmpgt_epi8(input, _mm256_set1_epi8('@'))));\n        \n        // Adjust to 0-25 range for both lowercase and uppercase\n        __m256i offset = _mm256_blendv_epi8(upper_a, lower_a, _mm256_cmpgt_epi8(input, _mm256_set1_epi8('Z')));\n        __m256i normalized = _mm256_sub_epi8(input, offset);\n        \n        // Apply the rotation (shift by 4 positions)\n        __m256i rotated = _mm256_add_epi8(normalized, _mm256_set1_epi8(4));\n        rotated = _mm256_and_si256(rotated, _mm256_set1_epi8(0x1F)); // Modulo 26\n        \n        // Convert back to ASCII\n        __m256i encrypted = _mm256_add_epi8(rotated, offset);\n        \n        // Blend original and encrypted characters based on alphabetic mask\n        __m256i final = _mm256_blendv_epi8(input, encrypted, is_alpha);\n        \n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final);\n    }\n    \n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = s[i];\n        if ('a' <= c && c <= 'z') {\n            result[i] = 'a' + ((c - 'a' + 4) % 26);\n        } else if ('A' <= c && c <= 'Z') {\n            result[i] = 'A' + ((c - 'A' + 4) % 26);\n        } else {\n            result[i] = c;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    // Result string to store the encoded message\n    std::string result(message.size(), 0);\n\n    // Constants for SIMD operations\n    const __m256i mask_case_swap = _mm256_set1_epi8(0x20); // Mask to swap case (bit 5)\n    const __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_lower = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                   'c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_upper = _mm256_setr_epi8('C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                    'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    size_t length = message.size();\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= length; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Swap case by flipping the 5th bit\n        __m256i swapped_case = _mm256_xor_si256(input, mask_case_swap);\n\n        // Check for lowercase vowels and replace them\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(input, vowels_lower);\n        __m256i replaced_lower = _mm256_blendv_epi8(swapped_case, replace_lower, is_vowel_lower);\n\n        // Check for uppercase vowels and replace them\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(input, vowels_upper);\n        __m256i replaced_upper = _mm256_blendv_epi8(replaced_lower, replace_upper, is_vowel_upper);\n\n        // Store the result back into the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), replaced_upper);\n    }\n\n    // Process remaining characters (if any) using scalar code\n    for (; i < length; ++i) {\n        char c = message[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            result[i] = c + 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            result[i] = c + 2;\n        } else {\n            result[i] = c ^ 0x20; // Swap case\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string &message) {\n    size_t len = message.size();\n    size_t simd_len = (len / 32) * 32;\n    std::string result(len, '\\0');\n\n    // Constants for SIMD operations\n    __m256i mask_case_swap = _mm256_set1_epi8(0x20); // Mask to swap case\n    __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                            'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                            'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i replace_lower = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                             'c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i replace_upper = _mm256_setr_epi8('C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                             'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Process in chunks of 32 bytes\n    for (size_t i = 0; i < simd_len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Swap case\n        __m256i is_alpha = _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1));\n        __m256i swapped_case = _mm256_xor_si256(chunk, mask_case_swap);\n        chunk = _mm256_blendv_epi8(chunk, swapped_case, is_alpha);\n\n        // Replace vowels\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(chunk, vowels_lower);\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(chunk, vowels_upper);\n        __m256i is_vowel = _mm256_or_si256(is_vowel_lower, is_vowel_upper);\n        __m256i replacement = _mm256_blendv_epi8(replace_lower, replace_upper, is_vowel_upper);\n        chunk = _mm256_blendv_epi8(chunk, replacement, is_vowel);\n\n        // Store result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), chunk);\n    }\n\n    // Handle remaining characters\n    for (size_t i = simd_len; i < len; ++i) {\n        char c = message[i];\n        if ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {\n            c ^= 0x20; // Swap case\n            if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n                c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n                c = (c >= 'a' && c <= 'z') ? c + 2 : c + 2;\n            }\n        }\n        result[i] = c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    // Result string to store the encoded message\n    std::string result(message.size(), '\\0');\n    \n    // Constants for SIMD operations\n    const __m256i mask_case_swap = _mm256_set1_epi8(0x20); // Mask to swap case (bit 5)\n    const __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_lower = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                   'c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_upper = _mm256_setr_epi8('C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                   'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    size_t length = message.size();\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= length; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Swap case by flipping the 5th bit\n        __m256i swapped = _mm256_xor_si256(input, mask_case_swap);\n\n        // Check for lowercase vowels and replace them\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(input, vowels_lower);\n        __m256i replaced_lower = _mm256_blendv_epi8(swapped, replace_lower, is_vowel_lower);\n\n        // Check for uppercase vowels and replace them\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(input, vowels_upper);\n        __m256i replaced_upper = _mm256_blendv_epi8(replaced_lower, replace_upper, is_vowel_upper);\n\n        // Store the result back into the output string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), replaced_upper);\n    }\n\n    // Process remaining characters (if any) using scalar code\n    for (; i < length; ++i) {\n        char c = message[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            result[i] = c + 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            result[i] = c + 2;\n        } else {\n            result[i] = c ^ 0x20; // Swap case\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message) {\n    // Result string to store the encoded message\n    std::string result(message.size(), 0);\n\n    // Constants for SIMD operations\n    const __m256i mask_case_swap = _mm256_set1_epi8(0x20); // Mask to swap case (bit 5)\n    const __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                  'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_lower = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                   'c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_upper = _mm256_setr_epi8('C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n                                                   'C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    size_t length = message.size();\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= length; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Swap case by flipping the 5th bit\n        __m256i swapped_case = _mm256_xor_si256(input, mask_case_swap);\n\n        // Check for lowercase vowels and replace them\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(input, vowels_lower);\n        __m256i replaced_lower = _mm256_blendv_epi8(swapped_case, replace_lower, is_vowel_lower);\n\n        // Check for uppercase vowels and replace them\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(input, vowels_upper);\n        __m256i replaced_upper = _mm256_blendv_epi8(replaced_lower, replace_upper, is_vowel_upper);\n\n        // Store the result back to memory\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), replaced_upper);\n    }\n\n    // Handle remaining characters (less than 32)\n    for (; i < length; ++i) {\n        char c = message[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            result[i] = c + 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            result[i] = c + 2;\n        } else {\n            result[i] = c ^ 0x20; // Swap case\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string &message) {\n    // Result string to store the encoded message\n    std::string result(message.size(), 0);\n\n    // Constants for SIMD operations\n    const __m256i mask_case_swap = _mm256_set1_epi8(0x20); // Mask to swap case (bit 5)\n    const __m256i vowels_lower = _mm256_setr_epi8('a', 'e', 'i', 'o', 'u', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i vowels_upper = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_lower = _mm256_setr_epi8('c', 'g', 'k', 'q', 'w', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m256i replace_upper = _mm256_setr_epi8('C', 'G', 'K', 'Q', 'W', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    size_t i = 0;\n    size_t length = message.size();\n\n    // Process 32 characters at a time using AVX2\n    for (; i + 32 <= length; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Swap case by flipping the 5th bit\n        __m256i swapped_case = _mm256_xor_si256(chunk, mask_case_swap);\n\n        // Check for lowercase vowels and replace them\n        __m256i is_vowel_lower = _mm256_cmpeq_epi8(chunk, vowels_lower);\n        __m256i replaced_lower = _mm256_blendv_epi8(swapped_case, replace_lower, is_vowel_lower);\n\n        // Check for uppercase vowels and replace them\n        __m256i is_vowel_upper = _mm256_cmpeq_epi8(chunk, vowels_upper);\n        __m256i replaced_upper = _mm256_blendv_epi8(replaced_lower, replace_upper, is_vowel_upper);\n\n        // Store the result back into the result string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), replaced_upper);\n    }\n\n    // Process remaining characters (less than 32) using scalar code\n    for (; i < length; ++i) {\n        char c = message[i];\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            result[i] = c + 2;\n        } else if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U') {\n            result[i] = c + 2;\n        } else {\n            result[i] = c ^ 0x20; // Swap case\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20); // Mask to identify lowercase letters\n    __m256i upper_mask = _mm256_set1_epi8(0x00); // Mask to identify uppercase letters\n    __m256i num_mask = _mm256_set1_epi8(0x30);   // Mask to identify numeric characters\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        for (size_t i = 0; i < len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.c_str() + i));\n            __m256i is_alpha = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n                                                _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk));\n            __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('a' - 1)),\n                                                _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk));\n            __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n                                                _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chunk));\n\n            __m256i not_alpha = _mm256_xor_si256(is_alpha, _mm256_set1_epi8(0xFF));\n            __m256i not_lower = _mm256_xor_si256(is_lower, _mm256_set1_epi8(0xFF));\n            __m256i not_upper = _mm256_xor_si256(is_upper, _mm256_set1_epi8(0xFF));\n\n            if (!_mm256_testz_si256(not_alpha, not_alpha)) {\n                return false; // Non-alphabetic character found\n            }\n\n            if (!_mm256_testz_si256(not_lower, not_lower)) {\n                all_lower = false;\n            }\n\n            if (!_mm256_testz_si256(not_upper, not_upper)) {\n                all_upper = false;\n            }\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20); // Mask for checking lowercase\n    __m256i upper_mask = _mm256_set1_epi8(0x00); // Mask for checking uppercase\n    __m256i all_lower = _mm256_set1_epi8(0xFF);\n    __m256i all_upper = _mm256_set1_epi8(0x00);\n\n    bool first_key = true;\n    bool is_lower = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        if (len == 0) return false;\n\n        size_t i = 0;\n        __m256i key_lower = _mm256_set1_epi8(0x00);\n        __m256i key_upper = _mm256_set1_epi8(0xFF);\n\n        for (; i + 32 <= len; i += 32) {\n            __m256i key_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&key[i]));\n            __m256i lower_check = _mm256_and_si256(_mm256_cmplt_epi8(key_vec, _mm256_set1_epi8('a')), lower_mask);\n            __m256i upper_check = _mm256_and_si256(_mm256_cmpgt_epi8(key_vec, _mm256_set1_epi8('Z')), upper_mask);\n\n            key_lower = _mm256_or_si256(key_lower, lower_check);\n            key_upper = _mm256_and_si256(key_upper, upper_check);\n        }\n\n        int lower_mask_result = _mm256_movemask_epi8(key_lower);\n        int upper_mask_result = _mm256_movemask_epi8(key_upper);\n\n        bool current_is_lower = (lower_mask_result == 0);\n        bool current_is_upper = (upper_mask_result == 0xFFFF);\n\n        if (first_key) {\n            if (!current_is_lower && !current_is_upper) return false;\n            is_lower = current_is_lower;\n            first_key = false;\n        } else {\n            if ((is_lower && !current_is_lower) || (!is_lower && !current_is_upper)) return false;\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (is_lower) {\n                if (!(c >= 'a' && c <= 'z')) return false;\n            } else {\n                if (!(c >= 'A' && c <= 'Z')) return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20); // Mask for lowercase bit\n    __m256i upper_mask = _mm256_set1_epi8(0xDF); // Mask to convert to uppercase\n    __m256i zero = _mm256_setzero_si256();\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n        size_t i = 0;\n\n        while (i + 32 <= len) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&key[i]));\n\n            // Check if all characters are lowercase\n            __m256i is_lower = _mm256_and_si256(chunk, lower_mask);\n            __m256i cmp_lower = _mm256_cmpeq_epi8(is_lower, lower_mask);\n            all_lower &= !_mm256_testz_si256(cmp_lower, cmp_lower);\n\n            // Check if all characters are uppercase\n            __m256i upper_chunk = _mm256_and_si256(chunk, upper_mask);\n            __m256i cmp_upper = _mm256_cmpeq_epi8(upper_chunk, chunk);\n            all_upper &= !_mm256_testz_si256(cmp_upper, cmp_upper);\n\n            i += 32;\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            char c = key[i];\n            all_lower &= (c >= 'a' && c <= 'z');\n            all_upper &= (c >= 'A' && c <= 'Z');\n        }\n\n        if (!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20); // Mask for checking lowercase\n    __m256i upper_mask = _mm256_set1_epi8(0x00); // Mask for checking uppercase\n    __m256i all_lower = _mm256_set1_epi8(0xFF);\n    __m256i all_upper = _mm256_set1_epi8(0xFF);\n\n    bool first_key = true;\n    bool is_lower = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.size();\n\n        if (len == 0) return false;\n\n        size_t i = 0;\n        for (; i + 32 <= len; i += 32) {\n            __m256i key_chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&key[i]));\n            __m256i lower_check = _mm256_and_si256(_mm256_cmpgt_epi8(key_chunk, _mm256_set1_epi8('z')), lower_mask);\n            __m256i upper_check = _mm256_and_si256(_mm256_cmpgt_epi8(_mm256_set1_epi8('Z'), key_chunk), upper_mask);\n\n            if (first_key) {\n                all_lower = lower_check;\n                all_upper = upper_check;\n                is_lower = _mm256_movemask_epi8(lower_check) != 0;\n                first_key = false;\n            } else {\n                if (is_lower) {\n                    all_lower = _mm256_and_si256(all_lower, lower_check);\n                } else {\n                    all_upper = _mm256_and_si256(all_upper, upper_check);\n                }\n            }\n\n            if (_mm256_testz_si256(all_lower, all_lower) || _mm256_testz_si256(all_upper, all_upper)) {\n                return false;\n            }\n        }\n\n        for (; i < len; ++i) {\n            char c = key[i];\n            if (first_key) {\n                is_lower = (c >= 'a' && c <= 'z');\n                first_key = false;\n            } else {\n                if ((is_lower && !(c >= 'a' && c <= 'z')) || (!is_lower && !(c >= 'A' && c <= 'Z'))) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) return false;\n\n    __m256i lower_mask = _mm256_set1_epi8(0x20); // Mask to identify lowercase letters\n    __m256i upper_mask = _mm256_set1_epi8(0x00); // Mask to identify uppercase letters\n    __m256i num_mask = _mm256_set1_epi8(0x30);   // Mask to identify numeric characters\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        size_t len = key.length();\n\n        for (size_t i = 0; i < len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.c_str() + i));\n            __m256i is_alpha = _mm256_and_si256(_mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('A' - 1)),\n                                                _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), chunk));\n\n            __m256i is_lower = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(chunk, _mm256_set1_epi8('Z')));\n            __m256i is_upper = _mm256_and_si256(is_alpha, _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), chunk));\n\n            int lower_mask_result = _mm256_movemask_epi8(is_lower);\n            int upper_mask_result = _mm256_movemask_epi8(is_upper);\n\n            if (lower_mask_result != 0) all_upper = false;\n            if (upper_mask_result != 0) all_lower = false;\n\n            if (!all_lower && !all_upper) return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    // Allocate a boolean array to mark non-prime numbers\n    size_t size = (n + 7) / 8; // Each byte holds 8 bits\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(0xFF);\n    std::vector<uint8_t> sieve((size + 31) / 32 * 32, 0xFF); // Align to 32 bytes for AVX\n\n    // Mark 0 and 1 as non-prime\n    sieve[0] &= ~(1 << 0);\n    sieve[0] &= ~(1 << 1);\n\n    // Process using AVX2 intrinsics\n    for (int i = 2; i * i < n; ++i) {\n        if (sieve[i / 8] & (1 << (i % 8))) { // Check if i is prime\n            __m256i index = _mm256_set1_epi32(i);\n            for (int j = i * i; j < n; j += i) {\n                int byte_idx = j / 8;\n                int bit_idx = j % 8;\n                uint8_t mask = ~(1 << bit_idx);\n                sieve[byte_idx] &= mask;\n            }\n        }\n    }\n\n    // Collect primes\n    for (int i = 2; i < n; ++i) {\n        if (sieve[i / 8] & (1 << (i % 8))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    // Allocate a boolean array to mark non-prime numbers\n    size_t size = (n + 7) / 8; // Each byte holds 8 bits for 8 numbers\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(0xFF);\n    std::vector<uint8_t> sieve((size + 31) / 32 * 32, 0xFF); // Align to 32 bytes\n\n    // Mark 0 and 1 as non-prime\n    sieve[0] &= ~(1 << 0); // Clear bit 0\n    sieve[0] &= ~(1 << 1); // Clear bit 1\n\n    int limit = static_cast<int>(sqrt(n)) + 1;\n    for (int p = 2; p < limit; ++p) {\n        if (sieve[p / 8] & (1 << (p % 8))) { // Check if p is prime\n            // Use AVX2 to mark multiples of p as non-prime\n            for (int multiple = p * p; multiple < n; multiple += p) {\n                int byte_idx = multiple / 8;\n                int bit_idx = multiple % 8;\n                uint8_t mask = ~(1 << bit_idx);\n\n                // Load 32 bytes into AVX register\n                __m256i* ptr = reinterpret_cast<__m256i*>(&sieve[byte_idx & ~31]);\n                __m256i vec = _mm256_load_si256(ptr);\n\n                // Create a mask to clear the specific bit\n                __m256i bit_mask = _mm256_set1_epi8(mask);\n                vec = _mm256_and_si256(vec, bit_mask);\n\n                // Store the result back\n                _mm256_store_si256(ptr, vec);\n            }\n        }\n    }\n\n    // Collect all primes\n    for (int i = 2; i < n; ++i) {\n        if (sieve[i / 8] & (1 << (i % 8))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    // Allocate a boolean array to mark non-prime numbers\n    size_t size = (n + 7) / 8; // Number of 8-bit chunks needed\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(0xFF);\n    std::vector<__m256i> sieve(size, ones);\n\n    // Mark 0 and 1 as non-prime\n    _mm256_maskstore_epi32((int*)&sieve[0], _mm256_setr_epi32(-1, -1, 0, 0, 0, 0, 0, 0), zero);\n\n    for (int i = 2; i * i < n; ++i) {\n        if (_mm256_movemask_epi8(_mm256_loadu_si256(&sieve[i / 32])) & (1 << (i % 32))) {\n            for (int j = i * i; j < n; j += i) {\n                __m256i mask = _mm256_set1_epi32(1 << (j % 32));\n                __m256i current = _mm256_loadu_si256(&sieve[j / 32]);\n                current = _mm256_andnot_si256(mask, current);\n                _mm256_storeu_si256(&sieve[j / 32], current);\n            }\n        }\n    }\n\n    // Collect primes\n    for (int i = 2; i < n; ++i) {\n        if (_mm256_movemask_epi8(_mm256_loadu_si256(&sieve[i / 32])) & (1 << (i % 32))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    // Allocate a boolean array to mark non-prime numbers\n    size_t size = (n + 7) / 8; // Number of bytes needed for bit-packed array\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(0xFF);\n    std::vector<uint8_t> is_prime((size + 31) / 32 * 32, 0xFF); // Align to 32 bytes\n\n    // Mark 0 and 1 as non-prime\n    is_prime[0] &= ~0x03;\n\n    // Process in chunks of 32 bytes (256 bits)\n    for (int i = 2; i * i < n; ++i) {\n        if (is_prime[i / 8] & (1 << (i % 8))) {\n            __m256i mask = _mm256_set1_epi8(1 << (i % 8));\n            for (int j = i * i; j < n; j += i) {\n                int byte_idx = j / 8;\n                int bit_idx = j % 8;\n                if (byte_idx % 32 == 0 && byte_idx + 32 <= n / 8) {\n                    // Vectorized update for aligned 32-byte blocks\n                    __m256i block = _mm256_loadu_si256(reinterpret_cast<__m256i*>(&is_prime[byte_idx]));\n                    __m256i clear_mask = _mm256_set1_epi8(~(1 << bit_idx));\n                    block = _mm256_and_si256(block, clear_mask);\n                    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&is_prime[byte_idx]), block);\n                    j += 32 * 8 - i; // Skip processed block\n                } else {\n                    // Scalar update for remaining bytes\n                    is_prime[byte_idx] &= ~(1 << bit_idx);\n                }\n            }\n        }\n    }\n\n    // Collect prime numbers\n    for (int i = 2; i < n; ++i) {\n        if (is_prime[i / 8] & (1 << (i % 8))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    // Allocate a boolean array to mark non-prime numbers\n    size_t size = (n + 7) / 8; // Number of bytes needed for bitset\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(0xFF);\n    std::vector<uint8_t> sieve((size + 31) & ~31, 0xFF); // Align to 32 bytes\n\n    // Mark 0 and 1 as non-prime\n    sieve[0] &= ~(1 | (1 << 1));\n\n    // Process in chunks of 32 bytes (256 bits)\n    for (int i = 2; i * i < n; ++i) {\n        if (sieve[i / 8] & (1 << (i % 8))) {\n            __m256i mask = _mm256_set1_epi8(1 << (i % 8));\n            for (int j = i * i; j < n; j += i) {\n                int byte_idx = j / 8;\n                int bit_idx = j % 8;\n                if ((byte_idx % 32) == 0 && (j + 32 * 8 < n)) {\n                    // Vectorized update for aligned 32-byte chunks\n                    __m256i sieve_chunk = _mm256_loadu_si256(reinterpret_cast<__m256i*>(&sieve[byte_idx]));\n                    __m256i clear_mask = _mm256_set1_epi8(~(1 << bit_idx));\n                    sieve_chunk = _mm256_and_si256(sieve_chunk, clear_mask);\n                    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&sieve[byte_idx]), sieve_chunk);\n                    j += 32 * 8 - i; // Skip processed chunk\n                } else {\n                    // Scalar update for remaining elements\n                    sieve[byte_idx] &= ~(1 << bit_idx);\n                }\n            }\n        }\n    }\n\n    // Collect prime numbers\n    for (int i = 2; i < n; ++i) {\n        if (sieve[i / 8] & (1 << (i % 8))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int len = s.size();\n    int result = 0;\n    const char *data = s.data();\n\n    // Constants for comparison\n    __m256i upper_A = _mm256_set1_epi8('A');\n    __m256i upper_U = _mm256_set1_epi8('U');\n    __m256i vowels = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                       'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < len; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n\n        // Create a mask for even indices\n        __m256i index_mask = _mm256_setr_epi8(0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n                                              0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n                                              0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n                                              0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00);\n        __m256i even_chars = _mm256_blendv_epi8(_mm256_setzero_si256(), chunk, index_mask);\n\n        // Check if the character is between 'A' and 'U'\n        __m256i ge_A = _mm256_cmpgt_epi8(even_chars, upper_A);\n        __m256i le_U = _mm256_cmpgt_epi8(upper_U, even_chars);\n        __m256i in_range = _mm256_and_si256(ge_A, le_U);\n\n        // Check if the character is one of the uppercase vowels\n        __m256i vowel_check = _mm256_cmpeq_epi8(even_chars, vowels);\n        __m256i vowel_mask = _mm256_movemask_epi8(vowel_check);\n\n        // Count the number of matches\n        result += _mm_popcnt_u32(vowel_mask);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int len = s.size();\n    int result = 0;\n    \n    // Constants for comparison\n    __m256i upper_A = _mm256_set1_epi8('A');\n    __m256i upper_Z = _mm256_set1_epi8('Z');\n    __m256i vowels = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                       'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    \n    for (int i = 0; i < len; i += 32) {\n        int chunk_size = std::min(32, len - i);\n        \n        // Load 32 characters into a SIMD register\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Check if characters are uppercase\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(data, upper_A),\n            _mm256_cmpgt_epi8(upper_Z, data)\n        );\n        \n        // Check if characters are vowels\n        __m256i is_vowel = _mm256_cmpeq_epi8(data, vowels);\n        \n        // Combine conditions: must be uppercase AND vowel\n        __m256i mask = _mm256_and_si256(is_upper, is_vowel);\n        \n        // Extract the mask and process only even indices\n        uint32_t bit_mask = _mm256_movemask_epi8(mask);\n        for (int j = 0; j < chunk_size; ++j) {\n            if ((j % 2 == 0) && (bit_mask & (1 << j))) {\n                ++result;\n            }\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int len = s.size();\n    int result = 0;\n\n    // Constants for SIMD operations\n    __m256i zero = _mm256_setzero_si256();\n    __m256i upper_a = _mm256_set1_epi8('A');\n    __m256i upper_u = _mm256_set1_epi8('U');\n    __m256i vowels = _mm256_setr_epi8(\n        0, 'A', 0, 'E', 0, 'I', 0, 'O', 0, 'U', 0, 0, 0, 0, 0, 0,\n        0, 'A', 0, 'E', 0, 'I', 0, 'O', 0, 'U', 0, 0, 0, 0, 0, 0\n    );\n\n    for (int i = 0; i + 32 <= len; i += 32) {\n        // Load 32 characters into a SIMD register\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create a mask for even indices (0, 2, 4, ..., 30)\n        __m256i even_mask = _mm256_setr_epi8(\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00,\n            0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00, 0xFF, 0x00\n        );\n        __m256i even_chars = _mm256_and_si256(data, even_mask);\n\n        // Check if characters are uppercase vowels\n        __m256i is_upper = _mm256_and_si256(\n            _mm256_cmpgt_epi8(even_chars, upper_a),\n            _mm256_cmpgt_epi8(upper_u, even_chars)\n        );\n        __m256i is_vowel = _mm256_cmpeq_epi8(even_chars, vowels);\n        __m256i match = _mm256_and_si256(is_upper, is_vowel);\n\n        // Count matches\n        int mask = _mm256_movemask_epi8(match);\n        result += __builtin_popcount(mask);\n    }\n\n    // Handle remaining characters\n    for (int i = (len / 32) * 32; i < len; i += 2) {\n        char c = s[i];\n        if ((c >= 'A' && c <= 'Z') && (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U')) {\n            result++;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int len = s.size();\n    int result = 0;\n\n    // Constants for comparison\n    __m256i upper_A = _mm256_set1_epi8('A');\n    __m256i upper_Z = _mm256_set1_epi8('Z');\n    __m256i vowels = _mm256_setr_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                       'A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < len; i += 32) {\n        int chunk_size = std::min(32, len - i);\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n\n        // Check if characters are uppercase\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(data, upper_A),\n                                            _mm256_cmpgt_epi8(upper_Z, data));\n\n        // Check if characters are vowels\n        __m256i is_vowel = _mm256_cmpeq_epi8(data, vowels);\n\n        // Combine conditions\n        __m256i mask = _mm256_and_si256(is_upper, is_vowel);\n\n        // Extract even indices\n        alignas(32) uint8_t mask_array[32];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(mask_array), mask);\n\n        for (int j = 0; j < chunk_size; j += 2) {\n            if (mask_array[j]) {\n                result++;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_upper_simd(const std::string & s) {\n    int len = s.size();\n    int result = 0;\n\n    // Constants for comparison\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i upper_A = _mm256_set1_epi8('A');\n    const __m256i upper_U = _mm256_set1_epi8('U');\n    const __m256i mask_vowels = _mm256_setr_epi8(\n        0, 0, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0,\n        0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    // Process the string in chunks of 32 bytes\n    int i = 0;\n    for (; i + 32 <= len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create a mask for even indices (0, 2, 4, ...)\n        __m256i even_mask = _mm256_setr_epi8(\n            -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0,\n            -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0\n        );\n        __m256i even_chars = _mm256_and_si256(chunk, even_mask);\n\n        // Check if characters are uppercase vowels\n        __m256i ge_A = _mm256_cmpgt_epi8(even_chars, upper_A);\n        __m256i le_U = _mm256_cmpgt_epi8(upper_U, even_chars);\n        __m256i is_upper = _mm256_and_si256(ge_A, le_U);\n        __m256i is_vowel = _mm256_and_si256(is_upper, mask_vowels);\n\n        // Count the number of matches\n        int mask = _mm256_movemask_epi8(is_vowel);\n        result += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining characters\n    for (; i < len; i += 2) {\n        char c = s[i];\n        if ((c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U')) {\n            result++;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    int current = n;\n    int index = 0;\n\n    // Process 8 elements at a time using AVX2\n    __m256i increment = _mm256_set1_epi32(2); // Increment by 2 for odd/even sequence\n    while (index + 8 <= n) {\n        // Load current values into a SIMD register\n        __m256i current_vec = _mm256_setr_epi32(\n            current, current + 2, current + 4, current + 6,\n            current + 8, current + 10, current + 12, current + 14\n        );\n\n        // Store the results back to the vector\n        _mm256_storeu_si256((__m256i*)&result[index], current_vec);\n\n        // Update current and index for the next iteration\n        current += 16;\n        index += 8;\n    }\n\n    // Process remaining elements\n    while (index < n) {\n        result[index] = current;\n        current += 2;\n        index++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    int current = n;\n    int index = 0;\n\n    // Process 8 elements at a time using AVX2\n    __m256i increment = _mm256_set1_epi32(2); // Increment by 2 for odd/even sequence\n\n    while (index + 8 <= n) {\n        // Load current values into a SIMD register\n        __m256i current_vals = _mm256_setr_epi32(\n            current, current + 2, current + 4, current + 6,\n            current + 8, current + 10, current + 12, current + 14\n        );\n\n        // Store the results back to the vector\n        _mm256_storeu_si256((__m256i*)&result[index], current_vals);\n\n        // Update current and index for the next iteration\n        current += 16;\n        index += 8;\n    }\n\n    // Process remaining elements\n    while (index < n) {\n        result[index++] = current;\n        current += 2;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    int current = n;\n\n    // Process 8 elements at a time using AVX2 intrinsics\n    __m256i increment = _mm256_set1_epi32(2); // Increment by 2 for odd/even sequence\n    __m256i current_vec = _mm256_set1_epi32(current);\n\n    for (int i = 0; i < n; i += 8) {\n        int limit = std::min(i + 8, n);\n\n        // Load the current values into a vector\n        __m256i values = current_vec;\n\n        // Store the results back to the output array\n        for (int j = i; j < limit; ++j) {\n            result[j] = _mm256_extract_epi32(values, j - i);\n        }\n\n        // Update the current vector for the next iteration\n        current_vec = _mm256_add_epi32(current_vec, increment);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    // Initialize the first level\n    int current = n;\n    result[0] = current;\n\n    // Process in chunks of 8 using AVX2\n    __m256i increment = _mm256_set1_epi32(2); // Increment by 2 for odd/even sequence\n    __m256i current_vec = _mm256_set1_epi32(current);\n\n    for (int i = 1; i < n; i += 8) {\n        // Load the current values into a vector\n        current_vec = _mm256_add_epi32(current_vec, increment);\n\n        // Store the results back to the result array\n        _mm256_storeu_si256((__m256i*)&result[i], current_vec);\n\n        // Update the scalar current value for correctness beyond AVX boundary\n        current += 2 * 8;\n    }\n\n    // Handle any remaining elements that didn't fit into the last full AVX vector\n    for (int i = 1; i < n; ++i) {\n        if (result[i] == 0) { // Only compute if not already set by SIMD\n            current += 2;\n            result[i] = current;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n) {\n    std::vector<int> result(n);\n    int current = n;\n    \n    // Align the loop to process 8 elements at a time using AVX2\n    int i = 0;\n    for (; i <= n - 8; i += 8) {\n        __m256i vec_current = _mm256_setr_epi32(\n            current, current + (n % 2 == 0 ? 2 : 1),\n            current + 2 * (n % 2 == 0 ? 2 : 1),\n            current + 3 * (n % 2 == 0 ? 2 : 1),\n            current + 4 * (n % 2 == 0 ? 2 : 1),\n            current + 5 * (n % 2 == 0 ? 2 : 1),\n            current + 6 * (n % 2 == 0 ? 2 : 1),\n            current + 7 * (n % 2 == 0 ? 2 : 1)\n        );\n\n        // Store the results into the vector\n        _mm256_storeu_si256((__m256i*)&result[i], vec_current);\n\n        // Update the current value for the next iteration\n        current += 8 * (n % 2 == 0 ? 2 : 1);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = current;\n        current += (n % 2 == 0 ? 2 : 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    // Calculate the sum of integers from n to m using AVX2 intrinsics\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_setzero_si256();\n\n    int64_t current = n;\n    while (current <= m) {\n        __m256i values = _mm256_set_epi64x(\n            (current + 3 <= m) ? (current + 3) : 0,\n            (current + 2 <= m) ? (current + 2) : 0,\n            (current + 1 <= m) ? (current + 1) : 0,\n            current\n        );\n\n        __m256i mask = _mm256_cmpgt_epi64(_mm256_set1_epi64x(m), values);\n        sum_vec = _mm256_add_epi64(sum_vec, _mm256_and_si256(values, mask));\n        count_vec = _mm256_sub_epi64(count_vec, _mm256_and_si256(_mm256_set1_epi64x(-1), mask));\n\n        current += 4;\n    }\n\n    // Horizontal sum of the AVX registers\n    int64_t sum = 0;\n    int64_t count = 0;\n\n    alignas(32) int64_t temp_sum[4];\n    alignas(32) int64_t temp_count[4];\n\n    _mm256_store_si256((__m256i*)temp_sum, sum_vec);\n    _mm256_store_si256((__m256i*)temp_count, count_vec);\n\n    for (int i = 0; i < 4; ++i) {\n        sum += temp_sum[i];\n        count += temp_count[i];\n    }\n\n    // Compute average and round down\n    int64_t avg = sum / count;\n\n    // Convert to binary\n    std::stringstream ss;\n    ss << std::bitset<64>(avg);\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    // Calculate the sum of integers from n to m using AVX2 intrinsics\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_m = _mm256_set1_epi64x(m);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n\n    // Compute the number of terms (m - n + 1)\n    __m256i num_terms = _mm256_sub_epi64(vec_m, vec_n);\n    num_terms = _mm256_add_epi64(num_terms, vec_one);\n\n    // Compute the sum using the formula: sum = (n + m) * (m - n + 1) / 2\n    __m256i sum_vec = _mm256_add_epi64(vec_n, vec_m);\n    sum_vec = _mm256_mullo_epi64(sum_vec, num_terms);\n    __m256i vec_two = _mm256_set1_epi64x(2);\n    sum_vec = _mm256_div_epi64(sum_vec, vec_two); // Division is not directly supported in AVX2, fallback to scalar\n\n    // Extract the result from the vector\n    int64_t sum = 0;\n    int64_t count = m - n + 1;\n    sum = (n + m) * count / 2;\n\n    // Compute the average\n    int64_t avg = sum / count;\n\n    // Convert the average to binary\n    std::stringstream ss;\n    ss << std::bitset<64>(avg).to_string();\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    // Calculate the sum of integers from n to m using AVX2 intrinsics\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_setzero_si256();\n\n    int64_t current = n;\n    while (current <= m) {\n        __m256i value = _mm256_set1_epi64x(current);\n        sum_vec = _mm256_add_epi64(sum_vec, value);\n        count_vec = _mm256_add_epi64(count_vec, _mm256_set1_epi64x(1));\n        current += 4; // Process 4 elements at a time\n    }\n\n    // Horizontal sum of the AVX registers\n    int64_t sum = 0, count = 0;\n    for (int i = 0; i < 4; ++i) {\n        sum += ((int64_t*)&sum_vec)[i];\n        count += ((int64_t*)&count_vec)[i];\n    }\n\n    // Add remaining elements if any\n    while (current <= m) {\n        sum += current;\n        count++;\n        current++;\n    }\n\n    // Compute average and round down\n    int64_t avg = sum / count;\n\n    // Convert to binary representation\n    std::stringstream ss;\n    ss << std::bitset<64>(avg).to_string();\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    // Calculate the sum of integers from n to m using AVX2 intrinsics\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_setzero_si256();\n\n    int64_t current = n;\n    while (current <= m) {\n        __m256i val = _mm256_set1_epi64x(current);\n        sum_vec = _mm256_add_epi64(sum_vec, val);\n        count_vec = _mm256_add_epi64(count_vec, _mm256_set1_epi64x(1));\n        current += 4; // Process 4 elements at a time\n    }\n\n    // Horizontal sum for sum_vec and count_vec\n    int64_t sum = 0, count = 0;\n    for (int i = 0; i < 4; ++i) {\n        sum += ((int64_t*)&sum_vec)[i];\n        count += ((int64_t*)&count_vec)[i];\n    }\n\n    // Add remaining elements if any\n    while (current <= m) {\n        sum += current;\n        count += 1;\n        current++;\n    }\n\n    // Compute average\n    int64_t avg = sum / count;\n\n    // Convert to binary\n    std::stringstream ss;\n    ss << std::bitset<64>(avg).to_string();\n    std::string binary = ss.str();\n\n    // Remove leading zeros\n    size_t first_one = binary.find('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m) {\n    if (n > m) return \"-1\";\n\n    // Calculate the sum of integers from n to m using AVX2 intrinsics\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i count_vec = _mm256_setzero_si256();\n\n    int64_t current = n;\n    while (current <= m) {\n        __m256i values = _mm256_set_epi64x(\n            (current + 3 <= m) ? (current + 3) : 0,\n            (current + 2 <= m) ? (current + 2) : 0,\n            (current + 1 <= m) ? (current + 1) : 0,\n            current\n        );\n\n        sum_vec = _mm256_add_epi64(sum_vec, values);\n        count_vec = _mm256_add_epi64(count_vec, _mm256_set1_epi64x(1));\n\n        current += 4;\n    }\n\n    // Horizontal sum of the AVX2 registers\n    int64_t sum[4];\n    _mm256_storeu_si256((__m256i*)sum, sum_vec);\n\n    int64_t total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    int64_t count[4];\n    _mm256_storeu_si256((__m256i*)count, count_vec);\n\n    int64_t total_count = count[0] + count[1] + count[2] + count[3];\n\n    // Compute average and round down\n    int64_t avg = total_sum / total_count;\n\n    // Convert to binary\n    std::stringstream ss;\n    ss << std::hex << std::uppercase << avg;\n    std::string binary_str = ss.str();\n\n    return binary_str;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 1;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + 7 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i factorials = _mm256_set1_epi32(1);\n        __m256i sums = _mm256_set1_epi32(0);\n        __m256i mod_val = _mm256_set1_epi32(10000);\n\n        // Compute factorial for even indices and sum for odd indices\n        for (int j = 1; j <= i + 7; ++j) {\n            __m256i mask_even = _mm256_setr_epi32(\n                (i % 2 == 0), ((i + 1) % 2 == 0), ((i + 2) % 2 == 0), ((i + 3) % 2 == 0),\n                ((i + 4) % 2 == 0), ((i + 5) % 2 == 0), ((i + 6) % 2 == 0), ((i + 7) % 2 == 0)\n            );\n            __m256i mask_odd = _mm256_xor_si256(mask_even, ones);\n\n            __m256i multiplier = _mm256_set1_epi32(j);\n            factorials = _mm256_mullo_epi32(factorials, multiplier);\n            factorials = _mm256_and_si256(factorials, mask_even);\n\n            sums = _mm256_add_epi32(sums, _mm256_and_si256(multiplier, mask_odd));\n        }\n\n        factorials = _mm256_mod_epi32(factorials, mod_val); // Custom modulo implementation needed\n        __m256i final_values = _mm256_or_si256(factorials, sums);\n\n        // Store results back to the vector\n        _mm256_storeu_si256((__m256i*)&result[i - 1], final_values);\n    }\n\n    // Handle remaining elements\n    for (; i <= n; ++i) {\n        if (i % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= i; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i - 1] = fact;\n        } else {\n            int sum = 0;\n            for (int j = 1; j <= i; ++j) {\n                sum += j;\n            }\n            result[i - 1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 1;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + 7 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i factorials = _mm256_set1_epi32(1);\n        __m256i sums = _mm256_set1_epi32(0);\n        __m256i mod_val = _mm256_set1_epi32(10000);\n\n        // Compute factorial for even indices and sum for odd indices\n        for (int j = 2; j <= i + 7; ++j) {\n            __m256i is_even = _mm256_and_si256(_mm256_set1_epi32(j), _mm256_set1_epi32(1));\n            is_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n            __m256i mul_factor = _mm256_set1_epi32(j);\n            factorials = _mm256_mullo_epi32(factorials, mul_factor);\n            factorials = _mm256_mullo_epi32(factorials, _mm256_and_si256(is_even, ones));\n\n            __m256i add_factor = _mm256_set1_epi32(j);\n            sums = _mm256_add_epi32(sums, add_factor);\n            sums = _mm256_add_epi32(sums, _mm256_andnot_si256(is_even, ones));\n        }\n\n        // Modulo operation for factorials\n        factorials = _mm256_mod_epu32(factorials, mod_val);\n\n        // Blend results based on even/odd indices\n        __m256i blended = _mm256_blendv_epi8(sums, factorials, _mm256_set1_epi32(0xFF));\n\n        // Store results into the output vector\n        _mm256_storeu_si256((__m256i*)&result[i - 1], blended);\n    }\n\n    // Handle remaining elements\n    for (; i <= n; ++i) {\n        if (i % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= i; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i - 1] = fact;\n        } else {\n            int sum = 0;\n            for (int j = 1; j <= i; ++j) {\n                sum += j;\n            }\n            result[i - 1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 1;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + 7 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i factorials = _mm256_set1_epi32(1);\n        __m256i sums = _mm256_set1_epi32(0);\n        __m256i mod_val = _mm256_set1_epi32(10000);\n\n        // Compute factorial for even indices and sum for odd indices\n        for (int j = 2; j <= i + 7; ++j) {\n            __m256i is_even = _mm256_and_si256(_mm256_set1_epi32(j), _mm256_set1_epi32(1));\n            is_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n            __m256i multiplier = _mm256_set1_epi32(j);\n            factorials = _mm256_mullo_epi32(factorials, multiplier);\n            factorials = _mm256_mullo_epi32(factorials, _mm256_and_si256(is_even, _mm256_set1_epi32(1)));\n\n            sums = _mm256_add_epi32(sums, _mm256_andnot_si256(is_even, multiplier));\n        }\n\n        factorials = _mm256_mod_epu32(factorials, mod_val); // Custom modulo implementation needed\n        __m256i results = _mm256_blendv_epi8(sums, factorials, is_even);\n\n        // Store results into the vector\n        _mm256_storeu_si256((__m256i*)&result[i - 1], results);\n    }\n\n    // Handle remaining elements\n    for (; i <= n; ++i) {\n        if (i % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= i; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i - 1] = fact;\n        } else {\n            int sum = 0;\n            for (int j = 1; j <= i; ++j) {\n                sum += j;\n            }\n            result[i - 1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 1;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + 7 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i factorials = _mm256_set1_epi32(1);\n        __m256i sums = _mm256_set1_epi32(0);\n        __m256i mod_val = _mm256_set1_epi32(10000);\n\n        // Compute factorial for even indices and sum for odd indices\n        for (int j = 2; j <= i + 7; ++j) {\n            __m256i is_even = _mm256_and_si256(_mm256_set1_epi32(j), _mm256_set1_epi32(1));\n            is_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n            __m256i mul_factor = _mm256_set1_epi32(j);\n            factorials = _mm256_mullo_epi32(factorials, mul_factor);\n            factorials = _mm256_and_si256(factorials, is_even);\n\n            __m256i add_factor = _mm256_set1_epi32(j);\n            sums = _mm256_add_epi32(sums, add_factor);\n            sums = _mm256_andnot_si256(is_even, sums);\n\n            factorials = _mm256_or_si256(factorials, sums);\n            factorials = _mm256_mod_epu32(factorials, mod_val); // Custom modulo implementation needed\n        }\n\n        // Store results\n        _mm256_storeu_si256((__m256i*)&result[i - 1], factorials);\n    }\n\n    // Handle remaining elements\n    for (; i <= n; ++i) {\n        if (i % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= i; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i - 1] = fact;\n        } else {\n            int sum = (i * (i + 1)) / 2;\n            result[i - 1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    int i = 1;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + 7 <= n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i factorials = _mm256_set1_epi32(1);\n        __m256i sums = _mm256_set1_epi32(0);\n        __m256i mod_val = _mm256_set1_epi32(10000);\n\n        // Compute factorial for even indices and sum for odd indices\n        for (int j = 2; j <= i + 7; ++j) {\n            __m256i is_even = _mm256_and_si256(_mm256_set1_epi32(j), _mm256_set1_epi32(1));\n            is_even = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n\n            __m256i multiplier = _mm256_set1_epi32(j);\n            factorials = _mm256_mullo_epi32(factorials, multiplier);\n            factorials = _mm256_and_si256(factorials, is_even);\n\n            __m256i adder = _mm256_set1_epi32(j);\n            sums = _mm256_add_epi32(sums, adder);\n            sums = _mm256_andnot_si256(is_even, sums);\n\n            factorials = _mm256_or_si256(factorials, sums);\n            factorials = _mm256_mod_epu32(factorials, mod_val); // Custom modulo implementation needed\n        }\n\n        // Store results back to the result vector\n        _mm256_storeu_si256((__m256i*)&result[i - 1], factorials);\n    }\n\n    // Handle remaining elements\n    for (; i <= n; ++i) {\n        if (i % 2 == 0) {\n            int fact = 1;\n            for (int j = 1; j <= i; ++j) {\n                fact = (fact * j) % 10000;\n            }\n            result[i - 1] = fact;\n        } else {\n            int sum = (i * (i + 1)) / 2;\n            result[i - 1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n    __m256i v_even_count = _mm256_setzero_si256();\n    __m256i v_odd_count = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        int rev = 0;\n        int temp = num;\n\n        // Reverse the number to check if it's a palindrome\n        while (temp > 0) {\n            rev = rev * 10 + (temp % 10);\n            temp /= 10;\n        }\n\n        if (rev == num) {\n            __m256i v_num = _mm256_set1_epi32(num);\n            __m256i v_two = _mm256_set1_epi32(2);\n            __m256i v_mod = _mm256_and_si256(v_num, v_two); // Equivalent to num & 1\n\n            __m256i v_is_even = _mm256_cmpeq_epi32(v_mod, _mm256_setzero_si256());\n            __m256i v_is_odd = _mm256_xor_si256(v_is_even, _mm256_set1_epi32(-1));\n\n            v_even_count = _mm256_sub_epi32(v_even_count, v_is_even);\n            v_odd_count = _mm256_sub_epi32(v_odd_count, v_is_odd);\n        }\n    }\n\n    // Horizontal sum of counts\n    int counts[8];\n    _mm256_storeu_si256((__m256i*)counts, v_even_count);\n    for (int i = 0; i < 8; ++i) {\n        even_count += counts[i];\n    }\n\n    _mm256_storeu_si256((__m256i*)counts, v_odd_count);\n    for (int i = 0; i < 8; ++i) {\n        odd_count += counts[i];\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n    __m256i v_even_count = _mm256_setzero_si256();\n    __m256i v_odd_count = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        int rev = 0;\n        int temp = num;\n\n        // Reverse the number\n        while (temp > 0) {\n            rev = rev * 10 + (temp % 10);\n            temp /= 10;\n        }\n\n        // Check if palindrome\n        if (rev == num) {\n            // Load current counts into SIMD registers\n            __m256i v_num = _mm256_set1_epi32(num);\n            __m256i v_two = _mm256_set1_epi32(2);\n            __m256i v_mod = _mm256_and_si256(v_num, v_two); // num & 2 (even check)\n\n            // Compare and update counts\n            __m256i v_is_even = _mm256_cmpeq_epi32(v_mod, _mm256_setzero_si256());\n            __m256i v_is_odd = _mm256_xor_si256(v_is_even, _mm256_set1_epi32(-1));\n\n            v_even_count = _mm256_add_epi32(v_even_count, v_is_even);\n            v_odd_count = _mm256_add_epi32(v_odd_count, v_is_odd);\n        }\n    }\n\n    // Horizontal sum of the SIMD registers\n    int counts[8];\n    _mm256_storeu_si256((__m256i*)counts, v_even_count);\n    even_count = counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    _mm256_storeu_si256((__m256i*)counts, v_odd_count);\n    odd_count = counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n    __m256i v_even_count = _mm256_setzero_si256();\n    __m256i v_odd_count = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; i++) {\n        int num = i;\n        int rev = 0;\n        int temp = num;\n\n        // Reverse the number to check if it's a palindrome\n        while (temp > 0) {\n            rev = rev * 10 + (temp % 10);\n            temp /= 10;\n        }\n\n        if (rev == num) {\n            // Load current counts into SIMD registers\n            __m256i v_num = _mm256_set1_epi32(num);\n            __m256i v_two = _mm256_set1_epi32(2);\n            __m256i v_mod = _mm256_and_si256(v_num, v_two); // Check if even (num & 2 == 0)\n\n            // Compare and update counts in parallel\n            __m256i v_is_even = _mm256_cmpeq_epi32(v_mod, _mm256_setzero_si256());\n            __m256i v_is_odd = _mm256_xor_si256(v_is_even, _mm256_set1_epi32(-1));\n\n            v_even_count = _mm256_add_epi32(v_even_count, _mm256_and_si256(v_is_even, _mm256_set1_epi32(1)));\n            v_odd_count = _mm256_add_epi32(v_odd_count, _mm256_and_si256(v_is_odd, _mm256_set1_epi32(1)));\n        }\n    }\n\n    // Horizontal sum of the SIMD registers\n    int counts[8];\n    _mm256_storeu_si256((__m256i*)counts, v_even_count);\n    for (int i = 0; i < 8; ++i) {\n        even_count += counts[i];\n    }\n\n    _mm256_storeu_si256((__m256i*)counts, v_odd_count);\n    for (int i = 0; i < 8; ++i) {\n        odd_count += counts[i];\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n    __m256i v_even = _mm256_setzero_si256();\n    __m256i v_odd = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        int rev = 0;\n        int temp = num;\n\n        // Reverse the number\n        while (temp > 0) {\n            rev = rev * 10 + (temp % 10);\n            temp /= 10;\n        }\n\n        // Check if palindrome\n        if (rev == num) {\n            __m256i v_num = _mm256_set1_epi32(num);\n            __m256i v_two = _mm256_set1_epi32(2);\n            __m256i v_mod = _mm256_and_si256(v_num, v_two); // Equivalent to num & 1\n\n            // If mod is zero, it's even, otherwise odd\n            __m256i v_is_even = _mm256_cmpeq_epi32(v_mod, _mm256_setzero_si256());\n            __m256i v_is_odd = _mm256_xor_si256(v_is_even, _mm256_set1_epi32(-1));\n\n            // Increment counters\n            v_even = _mm256_sub_epi32(v_even, v_is_even); // -1 for true, 0 for false\n            v_odd = _mm256_sub_epi32(v_odd, v_is_odd);     // -1 for true, 0 for false\n        }\n    }\n\n    // Horizontal sum of the vector elements\n    int counts[8];\n    _mm256_storeu_si256((__m256i*)counts, v_even);\n    for (int i = 0; i < 8; ++i) {\n        even_count += counts[i];\n    }\n\n    _mm256_storeu_si256((__m256i*)counts, v_odd);\n    for (int i = 0; i < 8; ++i) {\n        odd_count += counts[i];\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n) {\n    int even_count = 0, odd_count = 0;\n    __m256i v_even_count = _mm256_setzero_si256();\n    __m256i v_odd_count = _mm256_setzero_si256();\n\n    for (int i = 1; i <= n; ++i) {\n        int num = i;\n        int rev = 0;\n        int temp = num;\n\n        // Reverse the number\n        while (temp > 0) {\n            rev = rev * 10 + (temp % 10);\n            temp /= 10;\n        }\n\n        // Check if palindrome\n        if (rev == num) {\n            __m256i v_num = _mm256_set1_epi32(num);\n            __m256i v_two = _mm256_set1_epi32(2);\n            __m256i v_mod = _mm256_and_si256(v_num, v_two); // num & 2 (check even/odd)\n\n            __m256i v_is_even = _mm256_cmpeq_epi32(v_mod, _mm256_setzero_si256());\n            __m256i v_is_odd = _mm256_xor_si256(v_is_even, _mm256_set1_epi32(-1));\n\n            v_even_count = _mm256_sub_epi32(v_even_count, v_is_even);\n            v_odd_count = _mm256_sub_epi32(v_odd_count, v_is_odd);\n        }\n    }\n\n    // Horizontal sum of counts\n    alignas(32) int even_arr[8], odd_arr[8];\n    _mm256_store_si256((__m256i*)even_arr, v_even_count);\n    _mm256_store_si256((__m256i*)odd_arr, v_odd_count);\n\n    for (int i = 0; i < 8; ++i) {\n        even_count += even_arr[i];\n        odd_count += odd_arr[i];\n    }\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = n.size();\n    __m256i zero = _mm256_setzero_si256();\n\n    // Process 8 integers at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i nums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i sums = _mm256_setzero_si256();\n\n        // Extract digits and compute their sum for each number in the vector\n        for (int j = 0; j < 8; ++j) {\n            int num = n[i + j];\n            int sum = 0;\n\n            if (num != 0) {\n                int sign = (num < 0) ? -1 : 1;\n                num = std::abs(num);\n\n                while (num > 0) {\n                    sum += num % 10;\n                    num /= 10;\n                }\n\n                if (sign == -1) {\n                    int first_digit = n[i + j] / static_cast<int>(std::pow(10, static_cast<int>(std::log10(std::abs(n[i + j])))));\n                    sum -= 2 * first_digit;\n                }\n            }\n\n            sums[j] = (sum > 0) ? 1 : 0;\n        }\n\n        // Accumulate results\n        int mask = _mm256_movemask_epi8(_mm256_cmpgt_epi32(sums, zero));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n\n        if (num != 0) {\n            int sign = (num < 0) ? -1 : 1;\n            num = std::abs(num);\n\n            while (num > 0) {\n                sum += num % 10;\n                num /= 10;\n            }\n\n            if (sign == -1) {\n                int first_digit = n[i] / static_cast<int>(std::pow(10, static_cast<int>(std::log10(std::abs(n[i])))));\n                sum -= 2 * first_digit;\n            }\n        }\n\n        if (sum > 0) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = n.size();\n    __m256i zero = _mm256_setzero_si256();\n\n    // Process 8 integers at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i nums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i sums = _mm256_setzero_si256();\n\n        // Extract digits and compute their sum for each number in the vector\n        for (int j = 0; j < 8; ++j) {\n            int num = n[i + j];\n            int sum = 0;\n            int temp = std::abs(num);\n            while (temp > 0) {\n                sum += temp % 10;\n                temp /= 10;\n            }\n            if (num < 0) sum = -sum; // Adjust for negative numbers\n            sums[j] = sum;\n        }\n\n        // Compare sums > 0\n        __m256i cmp = _mm256_cmpgt_epi32(sums, zero);\n\n        // Count the number of elements where sum > 0\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = std::abs(num);\n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n        if (num < 0) sum = -sum; // Adjust for negative numbers\n        if (sum > 0) ++count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = n.size();\n    \n    // Process in chunks of 8 integers using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        // Extract absolute values and handle negatives\n        __m256i sign_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        // Compute digit sums for each integer in the vector\n        for (int digit = 0; digit < 10; ++digit) {\n            __m256i div = _mm256_set1_epi32(10);\n            __m256i mod = _mm256_rem_epu32(abs_vec, div);\n            sum_vec = _mm256_add_epi32(sum_vec, mod);\n            abs_vec = _mm256_div_epu32(abs_vec, div);\n        }\n\n        // Adjust the first digit for negative numbers\n        __m256i first_digit = _mm256_rem_epu32(vec, _mm256_set1_epi32(10));\n        __m256i adjusted_sum = _mm256_sub_epi32(sum_vec, _mm256_and_si256(sign_mask, first_digit));\n\n        // Compare if the adjusted sum is greater than zero\n        __m256i cmp = _mm256_cmpgt_epi32(adjusted_sum, _mm256_setzero_si256());\n\n        // Count the number of elements satisfying the condition\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = std::abs(num);\n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n        if (num < 0) sum -= 2 * (std::abs(num) % 10);\n        if (sum > 0) ++count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t size = n.size();\n    size_t i = 0;\n\n    // Process 8 integers at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        // Extract digits and compute their signed sum for each integer\n        for (int j = 0; j < 8; ++j) {\n            int num = n[i + j];\n            int digit_sum = 0;\n            int sign = (num < 0) ? -1 : 1;\n            num = std::abs(num);\n\n            if (num == 0) {\n                digit_sum = 0;\n            } else {\n                while (num > 0) {\n                    digit_sum += num % 10;\n                    num /= 10;\n                }\n            }\n            digit_sum *= sign;\n\n            // Store the result back into the vector\n            sum_vec = _mm256_insert_epi32(sum_vec, digit_sum, j);\n        }\n\n        // Compare each sum with 0 and create a mask\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i cmp_mask = _mm256_cmpgt_epi32(sum_vec, zero_vec);\n\n        // Count the number of elements where sum > 0\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_mask));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int digit_sum = 0;\n        int sign = (num < 0) ? -1 : 1;\n        num = std::abs(num);\n\n        if (num == 0) {\n            digit_sum = 0;\n        } else {\n            while (num > 0) {\n                digit_sum += num % 10;\n                num /= 10;\n            }\n        }\n        digit_sum *= sign;\n\n        if (digit_sum > 0) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint count_nums_simd(std::vector<int> n) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = n.size();\n    \n    // Process 8 integers at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i sum_vec = _mm256_setzero_si256();\n\n        // Extract absolute values and process digits\n        __m256i sign_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        // Compute digit sums for each integer in the vector\n        for (int digit = 0; digit < 10; ++digit) {\n            __m256i div = _mm256_set1_epi32(10);\n            __m256i mod = _mm256_rem_epu32(abs_vec, div);\n            sum_vec = _mm256_add_epi32(sum_vec, mod);\n            abs_vec = _mm256_div_epu32(abs_vec, div);\n        }\n\n        // Adjust for negative numbers by subtracting twice the first digit\n        __m256i first_digit = _mm256_rem_epu32(_mm256_abs_epi32(vec), _mm256_set1_epi32(10));\n        __m256i adjust = _mm256_and_si256(sign_mask, _mm256_slli_epi32(first_digit, 1));\n        sum_vec = _mm256_sub_epi32(sum_vec, adjust);\n\n        // Compare sum > 0 and create a mask\n        __m256i zero = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpgt_epi32(sum_vec, zero);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n        // Count the number of elements with sum > 0\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = n[i];\n        int sum = 0;\n        int temp = std::abs(num);\n        while (temp > 0) {\n            sum += temp % 10;\n            temp /= 10;\n        }\n        if (num < 0) sum -= 2 * (std::abs(num) % 10);\n        if (sum > 0) ++count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Ensure the array is padded to a multiple of 8 for AVX2 processing\n    int padded_size = ((N + 7) / 8) * 8;\n    std::vector<int> padded_arr(padded_size, 0);\n    std::copy(arr.begin(), arr.end(), padded_arr.begin());\n\n    __m256i count_vec = _mm256_setzero_si256(); // To count drops in parallel\n\n    for (int i = 0; i < padded_size; i += 8) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_arr[i]));\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_arr[(i + 1) % padded_size]));\n\n        // Compare current > next to detect drops\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n\n        // Accumulate the number of drops\n        count_vec = _mm256_sub_epi32(count_vec, cmp);\n    }\n\n    // Horizontal sum of the count_vec to get total drops\n    int drop_count = 0;\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), count_vec);\n    for (int i = 0; i < 8; ++i) {\n        drop_count += temp[i];\n    }\n\n    // A valid rotation exists if there's at most one drop\n    return drop_count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N == 0) return true;\n\n    int drop_count = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i increment = _mm256_set1_epi32(8);\n\n    for (; i + 8 <= N; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        // Compare current > next to find drops\n        __m256i cmp = _mm256_cmpgt_epi32(current, next);\n\n        // Count the number of drops in this block\n        int mask = _mm256_movemask_ps((__m256)cmp);\n        drop_count += __builtin_popcount(mask);\n\n        // Early exit if more than one drop is found\n        if (drop_count > 1) return false;\n\n        // Increment indices for next iteration\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    // Process remaining elements\n    for (; i < N - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            drop_count++;\n            if (drop_count > 1) return false;\n        }\n    }\n\n    // Check wrap-around case\n    if (arr[N - 1] > arr[0]) {\n        drop_count++;\n    }\n\n    return drop_count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Count the number of drops in the array\n    int drop_count = 0;\n    int last_index = N - 1;\n\n    // Load data into AVX registers for parallel comparison\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    for (int i = 1; i < N; i += 8) {\n        // Load 8 integers into an AVX register\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Compare current with previous to find drops\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n\n        // Extract mask of comparisons\n        int mask = _mm256_movemask_ps((__m256)cmp);\n\n        // Count the number of drops using the mask\n        while (mask) {\n            drop_count += mask & 1;\n            mask >>= 1;\n        }\n\n        // Update previous to current for next iteration\n        prev = curr;\n    }\n\n    // Handle the wrap-around case (last element vs first element)\n    if (arr[last_index] > arr[0]) {\n        drop_count++;\n    }\n\n    // If there is at most one drop, it can be sorted by right shifts\n    return drop_count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Ensure the array size is a multiple of 8 for AVX2 processing\n    int padded_size = ((N + 7) / 8) * 8;\n    std::vector<int> padded_arr(padded_size, 0);\n    std::copy(arr.begin(), arr.end(), padded_arr.begin());\n\n    __m256i v_prev = _mm256_set1_epi32(0); // Initialize to zero\n    int drop_count = 0;\n\n    for (int i = 0; i < padded_size; i += 8) {\n        __m256i v_curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_arr[i]));\n\n        // Compare current block with previous block\n        __m256i v_cmp = _mm256_cmpgt_epi32(v_prev, v_curr);\n\n        // Count the number of drops in this block\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(v_cmp));\n        drop_count += __builtin_popcount(mask);\n\n        // Update previous block\n        v_prev = v_curr;\n    }\n\n    // Handle the circular case: compare last element with first element\n    if (arr[N - 1] > arr[0]) {\n        drop_count++;\n    }\n\n    return drop_count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Ensure the array size is a multiple of 8 for AVX2 processing\n    int padded_size = ((N + 7) / 8) * 8;\n    std::vector<int> padded_arr(padded_size, 0);\n    std::copy(arr.begin(), arr.end(), padded_arr.begin());\n\n    __m256i v_prev = _mm256_set1_epi32(0);\n    int drop_count = 0;\n\n    for (int i = 0; i < padded_size; i += 8) {\n        __m256i v_curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_arr[i]));\n\n        // Compare current vector with previous vector to detect drops\n        __m256i v_cmp = _mm256_cmpgt_epi32(v_prev, v_curr);\n\n        // Extract mask of comparison results\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(v_cmp));\n\n        // Count the number of drops in this iteration\n        while (mask) {\n            drop_count += mask & 1;\n            mask >>= 1;\n        }\n\n        // Update previous vector for next iteration\n        v_prev = v_curr;\n    }\n\n    // Handle wrap-around case: compare last element with first element\n    if (arr[N - 1] > arr[0]) {\n        drop_count++;\n    }\n\n    // If there is at most one drop, it's possible to sort by right shifts\n    return drop_count <= 1;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int n1 = lst1.size();\n    int n2 = lst2.size();\n    int i = 0;\n\n    // Count the number of odd elements in lst1\n    int odd_count_lst1 = 0;\n    for (i = 0; i <= n1 - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i odd_mask = _mm256_and_si256(vec, ones);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(odd_mask));\n        odd_count_lst1 += __builtin_popcount(mask);\n    }\n    for (; i < n1; ++i) {\n        if (lst1[i] & 1) {\n            ++odd_count_lst1;\n        }\n    }\n\n    // Count the number of even elements in lst2\n    int even_count_lst2 = 0;\n    for (i = 0; i <= n2 - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i odd_mask = _mm256_and_si256(vec, ones);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(odd_mask));\n        even_count_lst2 += (8 - __builtin_popcount(mask));\n    }\n    for (; i < n2; ++i) {\n        if ((lst2[i] & 1) == 0) {\n            ++even_count_lst2;\n        }\n    }\n\n    // If the number of odd elements in lst1 is less than or equal to\n    // the number of even elements in lst2, return \"YES\", else \"NO\"\n    return (odd_count_lst1 <= even_count_lst2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int n1 = lst1.size();\n    int n2 = lst2.size();\n    int max_size = (n1 > n2 ? n1 : n2);\n    int i = 0;\n\n    // Counters for odd numbers in lst1 and even numbers in lst2\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    // Process 8 integers at a time using AVX2\n    for (i = 0; i + 7 < max_size; i += 8) {\n        __m256i vec1, vec2;\n        if (i < n1) {\n            vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        } else {\n            vec1 = _mm256_setzero_si256();\n        }\n\n        if (i < n2) {\n            vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        } else {\n            vec2 = _mm256_setzero_si256();\n        }\n\n        // Check odd numbers in lst1\n        __m256i odd_mask1 = _mm256_and_si256(vec1, _mm256_set1_epi32(1));\n        int odd_mask_result1 = _mm256_movemask_epi8(_mm256_cmpgt_epi32(odd_mask1, _mm256_setzero_si256()));\n        if (odd_mask_result1 != 0) {\n            odd_count_lst1 += __builtin_popcount(odd_mask_result1) / 4;\n        }\n\n        // Check even numbers in lst2\n        __m256i even_mask2 = _mm256_and_si256(vec2, _mm256_set1_epi32(1));\n        int even_mask_result2 = _mm256_movemask_epi8(_mm256_cmpeq_epi32(even_mask2, _mm256_setzero_si256()));\n        if (even_mask_result2 != 0) {\n            even_count_lst2 += __builtin_popcount(even_mask_result2) / 4;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n1; ++i) {\n        if (lst1[i] & 1) {\n            ++odd_count_lst1;\n        }\n    }\n\n    for (; i < n2; ++i) {\n        if ((lst2[i] & 1) == 0) {\n            ++even_count_lst2;\n        }\n    }\n\n    // Determine if exchange is possible\n    return (odd_count_lst1 <= even_count_lst2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int n1 = lst1.size();\n    int n2 = lst2.size();\n    int i = 0;\n\n    // Counters for odd numbers in lst1 and even numbers in lst2\n    int odd_in_lst1 = 0;\n    int even_in_lst2 = 0;\n\n    // Process lst1 to count odd numbers using AVX2\n    for (i = 0; i <= n1 - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_and_si256(vec, ones); // Mask to check if the number is odd\n        int move_mask = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n        odd_in_lst1 += __builtin_popcount(move_mask);\n    }\n\n    // Process remaining elements of lst1\n    for (; i < n1; ++i) {\n        if (lst1[i] & 1) {\n            ++odd_in_lst1;\n        }\n    }\n\n    // Process lst2 to count even numbers using AVX2\n    for (i = 0; i <= n2 - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_and_si256(vec, ones); // Mask to check if the number is odd\n        int move_mask = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n        even_in_lst2 += 8 - __builtin_popcount(move_mask);\n    }\n\n    // Process remaining elements of lst2\n    for (; i < n2; ++i) {\n        if ((lst2[i] & 1) == 0) {\n            ++even_in_lst2;\n        }\n    }\n\n    // Check if we can exchange enough elements\n    return (odd_in_lst1 <= even_in_lst2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int n1 = lst1.size();\n    int n2 = lst2.size();\n    int i = 0;\n\n    // Counters for odd numbers in lst1 and even numbers in lst2\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    // Process lst1 in chunks of 8 using AVX2\n    for (i = 0; i + 8 <= n1; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_and_si256(vec, ones); // Mask to check odd numbers\n        int odd_mask = _mm256_movemask_ps(_mm256_castsi256_ps(mask)); // Extract odd mask\n        odd_count_lst1 += __builtin_popcount(odd_mask); // Count odd numbers\n    }\n\n    // Process remaining elements in lst1\n    for (; i < n1; ++i) {\n        if (lst1[i] & 1) {\n            ++odd_count_lst1;\n        }\n    }\n\n    // Process lst2 in chunks of 8 using AVX2\n    for (i = 0; i + 8 <= n2; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_and_si256(vec, ones); // Mask to check odd numbers\n        int even_mask = ~_mm256_movemask_ps(_mm256_castsi256_ps(mask)); // Invert to get even mask\n        even_count_lst2 += __builtin_popcount(even_mask); // Count even numbers\n    }\n\n    // Process remaining elements in lst2\n    for (; i < n2; ++i) {\n        if ((lst2[i] & 1) == 0) {\n            ++even_count_lst2;\n        }\n    }\n\n    // Check if the number of even numbers in lst2 is sufficient to replace odd numbers in lst1\n    return (even_count_lst2 >= odd_count_lst1) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int n1 = lst1.size();\n    int n2 = lst2.size();\n    int i = 0;\n\n    // Counters for odd numbers in lst1 and even numbers in lst2\n    int odd_in_lst1 = 0;\n    int even_in_lst2 = 0;\n\n    // Process lst1 in chunks of 8 using AVX2\n    for (i = 0; i + 8 <= n1; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_and_si256(vec, ones); // Mask to check odd numbers\n        int odd_mask = _mm256_movemask_ps(_mm256_castsi256_ps(mask)); // Extract mask\n        odd_in_lst1 += __builtin_popcount(odd_mask); // Count odd numbers\n    }\n\n    // Process remaining elements in lst1\n    for (; i < n1; ++i) {\n        if (lst1[i] & 1) {\n            ++odd_in_lst1;\n        }\n    }\n\n    // Process lst2 in chunks of 8 using AVX2\n    for (i = 0; i + 8 <= n2; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mask = _mm256_and_si256(vec, ones); // Mask to check odd numbers\n        int even_mask = ~_mm256_movemask_ps(_mm256_castsi256_ps(mask)); // Invert to count even numbers\n        even_in_lst2 += __builtin_popcount(even_mask); // Count even numbers\n    }\n\n    // Process remaining elements in lst2\n    for (; i < n2; ++i) {\n        if ((lst2[i] & 1) == 0) {\n            ++even_in_lst2;\n        }\n    }\n\n    // Check if the number of odd numbers in lst1 can be replaced by even numbers in lst2\n    return (odd_in_lst1 <= even_in_lst2) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    const char *template_str = \"the number of odd elements in the string X of the input.\";\n    __m256i odd_mask = _mm256_set1_epi8(0x01); // Mask to check odd digits (1, 3, 5, 7, 9)\n\n    for (const auto &s : lst) {\n        int count = 0;\n        size_t len = s.size();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0')); // Convert char to digit\n            __m256i is_odd = _mm256_and_si256(digits, odd_mask); // Check if odd\n            __m256i cmp = _mm256_cmpgt_epi8(is_odd, _mm256_setzero_si256()); // Compare with zero\n            int mask = _mm256_movemask_epi8(cmp); // Get bitmask of odd digits\n            count += __builtin_popcount(mask); // Count set bits\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') & 1) {\n                ++count;\n            }\n        }\n\n        // Replace 'X' in the template string with the count\n        std::string output = template_str;\n        size_t pos = output.find('X');\n        if (pos != std::string::npos) {\n            output.replace(pos, 1, std::to_string(count));\n        }\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    const char *template_str = \"the number of odd elements in the string X of the input.\";\n    __m256i odd_mask = _mm256_set1_epi8(0x01); // Mask to check for odd digits (lowest bit set)\n\n    for (const auto &str : lst) {\n        int count = 0;\n        size_t len = str.size();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0')); // Convert chars to digit values\n            __m256i is_odd = _mm256_and_si256(digits, odd_mask); // Check if lowest bit is set\n            __m256i cmp = _mm256_cmpeq_epi8(is_odd, odd_mask); // Compare with mask\n            count += _mm256_movemask_epi8(cmp); // Count odd digits\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') & 1) {\n                ++count;\n            }\n        }\n\n        // Replace 'X' in the template string with the count\n        std::string output = template_str;\n        size_t pos = output.find('X');\n        if (pos != std::string::npos) {\n            output.replace(pos, 1, std::to_string(count));\n        }\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    const char *template_str = \"the number of odd elements in the string X of the input.\";\n    __m256i odd_mask = _mm256_set1_epi8(0x01); // Mask to check for odd digits (lowest bit set)\n\n    for (const auto &str : lst) {\n        int count = 0;\n        size_t len = str.size();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0')); // Convert chars to digit values\n            __m256i is_odd = _mm256_and_si256(digits, odd_mask); // Check if lowest bit is set\n            __m256i cmp = _mm256_cmpeq_epi8(is_odd, odd_mask); // Compare with mask\n            count += _mm256_movemask_epi8(cmp); // Extract comparison results\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') & 1) {\n                ++count;\n            }\n        }\n\n        // Replace 'X' in the template string with the count\n        std::string output = template_str;\n        size_t pos = output.find('X');\n        if (pos != std::string::npos) {\n            output.replace(pos, 1, std::to_string(count));\n        }\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    const char *template_str = \"the number of odd elements in the string X of the input.\";\n    __m256i odd_mask = _mm256_set1_epi8(0x01); // Mask to check if a digit is odd\n\n    for (const auto &str : lst) {\n        int count = 0;\n        size_t len = str.size();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0')); // Convert chars to digits\n            __m256i is_odd = _mm256_and_si256(digits, odd_mask); // Check if odd\n            __m256i cmp = _mm256_cmpeq_epi8(is_odd, odd_mask); // Compare with mask\n            int mask = _mm256_movemask_epi8(cmp); // Get bitmask of results\n            count += __builtin_popcount(mask); // Count set bits\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') & 1) {\n                ++count;\n            }\n        }\n\n        // Replace 'X' in the template string with the count\n        std::string output = template_str;\n        size_t pos = output.find('X');\n        if (pos != std::string::npos) {\n            output.replace(pos, 1, std::to_string(count));\n        }\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    const char *template_str = \"the number of odd elements in the string X of the input.\";\n    __m256i odd_mask = _mm256_set1_epi8(0x01); // Mask to check if a digit is odd\n\n    for (const auto &str : lst) {\n        int count = 0;\n        size_t len = str.size();\n        size_t i = 0;\n\n        // Process 32 characters at a time using AVX2\n        for (; i + 32 <= len; i += 32) {\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i digits = _mm256_sub_epi8(chars, _mm256_set1_epi8('0')); // Convert chars to integers\n            __m256i is_odd = _mm256_and_si256(digits, odd_mask); // Check oddness\n            __m256i cmp = _mm256_cmpeq_epi8(is_odd, odd_mask); // Compare with mask\n            count += _mm256_movemask_epi8(cmp); // Count odd digits\n        }\n\n        // Process remaining characters\n        for (; i < len; ++i) {\n            if ((str[i] - '0') & 1) {\n                ++count;\n            }\n        }\n\n        // Replace 'X' in the template string with the count\n        std::string output = template_str;\n        size_t pos = output.find('X');\n        if (pos != std::string::npos) {\n            output.replace(pos, 1, std::to_string(count));\n        }\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    __m256i vmin = _mm256_set1_epi64x(INT64_MAX);\n    __m256i vcurrent = _mm256_set1_epi64x(0);\n\n    size_t i = 0;\n    size_t simd_width = 4; // AVX2 processes 4 int64_t elements at a time\n\n    for (; i + simd_width <= nums.size(); i += simd_width) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        vcurrent = _mm256_add_epi64(vcurrent, vnums);\n        vmin = _mm256_min_epi64(vmin, vcurrent);\n\n        // Reset current sum to zero if it becomes positive\n        __m256i vzero = _mm256_set1_epi64x(0);\n        __m256i vcmp = _mm256_cmpgt_epi64(vzero, vcurrent);\n        vcurrent = _mm256_blendv_epi8(vcurrent, vzero, vcmp);\n    }\n\n    // Horizontal reduction of vmin\n    int64_t result[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), vmin);\n\n    int64_t min_sum = result[0];\n    for (size_t j = 1; j < simd_width; ++j) {\n        if (result[j] < min_sum) {\n            min_sum = result[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < nums.size(); ++i) {\n        int64_t current = 0;\n        current += nums[i];\n        if (current < min_sum) {\n            min_sum = current;\n        }\n        if (current > 0) {\n            current = 0;\n        }\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    __m256i vmin = _mm256_set1_epi64x(nums[0]);\n    __m256i vcurrent = _mm256_set1_epi64x(nums[0]);\n\n    for (size_t i = 1; i < nums.size(); i += 4) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        // Compute current sum\n        vcurrent = _mm256_add_epi64(vcurrent, vnums);\n\n        // Compute minimum of current and nums[i]\n        __m256i vtemp = _mm256_min_epi64(vcurrent, vnums);\n\n        // Update global minimum\n        vmin = _mm256_min_epi64(vmin, vtemp);\n\n        // Reset current sum if it's greater than nums[i]\n        vcurrent = _mm256_blendv_epi8(vcurrent, vnums, _mm256_cmpgt_epi64(vcurrent, vnums));\n    }\n\n    // Horizontal reduction to find the minimum across all lanes\n    int64_t result[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), vmin);\n\n    int64_t final_min = result[0];\n    for (int i = 1; i < 4; ++i) {\n        if (result[i] < final_min) {\n            final_min = result[i];\n        }\n    }\n\n    return final_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    __m256i vmin = _mm256_set1_epi64x(nums[0]);\n    __m256i vcurrent = _mm256_set1_epi64x(nums[0]);\n\n    for (size_t i = 1; i < nums.size(); i += 4) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        // Compute current sum using Kadane's algorithm in SIMD\n        __m256i vsum = _mm256_add_epi64(vcurrent, vnums);\n        __m256i vzero = _mm256_setzero_si256();\n        __m256i vcmp = _mm256_cmpgt_epi64(vsum, vzero);\n        vcurrent = _mm256_blendv_epi8(vsum, vzero, vcmp);\n\n        // Update minimum value\n        vmin = _mm256_min_epi64(vmin, vcurrent);\n    }\n\n    // Horizontal reduction to find the minimum across all lanes\n    int64_t result[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), vmin);\n\n    int64_t final_min = result[0];\n    for (int i = 1; i < 4; ++i) {\n        if (result[i] < final_min) {\n            final_min = result[i];\n        }\n    }\n\n    return final_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    __m256i vmin = _mm256_set1_epi64x(INT64_MAX);\n    __m256i vcurrent = _mm256_set1_epi64x(0);\n\n    int64_t result = INT64_MAX;\n    size_t i = 0;\n\n    // Process 4 elements at a time using AVX2\n    for (; i + 3 < nums.size(); i += 4) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        // Update current sum\n        vcurrent = _mm256_add_epi64(vcurrent, vnums);\n\n        // Update minimum\n        vmin = _mm256_min_epi64(vmin, vcurrent);\n\n        // Reset current sum if it becomes positive\n        __m256i vzero = _mm256_set1_epi64x(0);\n        __m256i vcmp = _mm256_cmpgt_epi64(vcurrent, vzero);\n        vcurrent = _mm256_blendv_epi8(vcurrent, vzero, vcmp);\n    }\n\n    // Extract minimum from vector\n    int64_t temp[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vmin);\n    for (int j = 0; j < 4; ++j) {\n        result = std::min(result, temp[j]);\n    }\n\n    // Process remaining elements\n    int64_t current = 0;\n    for (; i < nums.size(); ++i) {\n        current += nums[i];\n        result = std::min(result, current);\n        if (current > 0) current = 0;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    if (nums.empty()) return 0;\n\n    __m256i vmin = _mm256_set1_epi64x(nums[0]);\n    __m256i vcurrent = _mm256_set1_epi64x(nums[0]);\n\n    for (size_t i = 1; i < nums.size(); i += 4) {\n        __m256i vnums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        __m256i vtemp = _mm256_add_epi64(vcurrent, vnums);\n        vcurrent = _mm256_min_epi64(vtemp, vnums);\n\n        vmin = _mm256_min_epi64(vmin, vcurrent);\n    }\n\n    int64_t result[4];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), vmin);\n\n    int64_t final_min = result[0];\n    for (int i = 1; i < 4; ++i) {\n        if (result[i] < final_min) {\n            final_min = result[i];\n        }\n    }\n\n    // Handle any remaining elements\n    for (size_t i = (nums.size() / 4) * 4; i < nums.size(); ++i) {\n        final_min = std::min(final_min, nums[i]);\n    }\n\n    return final_min;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_water = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int i = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i cmp = _mm256_cmpeq_epi32(vec, ones);\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n            total_water += __builtin_popcount(mask);\n        }\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            total_water += row[i];\n        }\n    }\n\n    // Calculate the number of bucket operations\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto& row : grid) {\n        int row_size = row.size();\n        int i = 0;\n\n        // Process 8 integers at a time using AVX2\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i cmp = _mm256_cmpeq_epi32(vec, ones);\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n            total_buckets += __builtin_popcount(mask);\n        }\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            total_buckets += (row[i] == 1);\n        }\n    }\n\n    // Calculate the number of bucket operations needed\n    return (total_buckets + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int i = 0;\n\n        // Process 8 integers at a time using AVX2\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i mask = _mm256_cmpeq_epi32(vec, ones);\n            int mask_bits = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n            int count = _mm_popcnt_u32(mask_bits);\n            total_buckets += (count + capacity - 1) / capacity;\n        }\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            if (row[i] == 1) {\n                total_buckets += (1 + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int i = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i mask = _mm256_cmpeq_epi32(vec, ones);\n            int mask_bits = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n            int count = _mm_popcnt_u32(mask_bits);\n            total_buckets += (count + capacity - 1) / capacity;\n        }\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            if (row[i] == 1) {\n                total_buckets += (1 + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>> &grid, int capacity) {\n    int total_buckets = 0;\n\n    for (const auto &row : grid) {\n        int row_size = row.size();\n        int i = 0;\n\n        // Process 8 integers at a time using AVX2\n        for (; i + 8 <= row_size; i += 8) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&row[i]));\n            __m256i ones = _mm256_set1_epi32(1);\n            __m256i cmp = _mm256_cmpeq_epi32(vec, ones);\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n            // Count the number of 1s in the current 8 elements\n            int count = _mm_popcnt_u32(mask);\n            total_buckets += (count + capacity - 1) / capacity;\n        }\n\n        // Process remaining elements\n        for (; i < row_size; ++i) {\n            if (row[i] == 1) {\n                total_buckets += (1 + capacity - 1) / capacity;\n            }\n        }\n    }\n\n    return total_buckets;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t result = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 15 < n; i += 16) {\n        // Load 16 elements into an AVX register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Create a mask for even positions (0, 2, 4, ..., 14)\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n        // Mask out elements in odd positions\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n\n        // Check which elements are odd using bitwise AND with 1\n        __m256i odd_check = _mm256_and_si256(even_elements, _mm256_set1_epi16(1));\n\n        // Compare to create a mask of odd elements\n        __m256i odd_mask = _mm256_cmpeq_epi16(odd_check, _mm256_set1_epi16(1));\n\n        // Blend the original values with zeros based on the odd mask\n        __m256i filtered = _mm256_blendv_epi8(_mm256_setzero_si256(), even_elements, odd_mask);\n\n        // Horizontally sum the 16 elements in the AVX register\n        int16_t temp[16];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), filtered);\n        for (size_t j = 0; j < 16; ++j) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if ((i % 2 == 0) && (lst[i] % 2 != 0)) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t result = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 15 < n; i += 16) {\n        // Load 16 elements into an AVX register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Create a mask to select even positions (0, 2, 4, ..., 14)\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n\n        // Check which elements are odd using bitwise AND with 1\n        __m256i odd_check = _mm256_and_si256(even_elements, _mm256_set1_epi16(1));\n        __m256i odd_mask = _mm256_cmpeq_epi16(odd_check, _mm256_set1_epi16(1));\n\n        // Blend the odd elements from even positions\n        __m256i odd_elements = _mm256_blendv_epi8(_mm256_setzero_si256(), even_elements, odd_mask);\n\n        // Horizontally sum the elements in the AVX register\n        __m128i low = _mm256_castsi256_si128(odd_elements);\n        __m128i high = _mm256_extracti128_si256(odd_elements, 1);\n        __m128i sum128 = _mm_add_epi16(low, high);\n\n        // Further reduce to 64-bit integer\n        int16_t temp[8];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), sum128);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if ((i % 2 == 0) && (lst[i] % 2 != 0)) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t result = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 15 < n; i += 16) {\n        // Load 16 integers into a vector\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Create a mask to select even positions (0, 2, 4, ..., 14)\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n\n        // Check which elements are odd\n        __m256i odd_mask = _mm256_set1_epi16(1);\n        __m256i is_odd = _mm256_and_si256(even_elements, odd_mask);\n\n        // Compare to create a mask of odd values\n        __m256i cmp_mask = _mm256_cmpeq_epi16(is_odd, odd_mask);\n\n        // Blend the original vector with zeros based on the odd mask\n        __m256i filtered = _mm256_blendv_epi8(_mm256_setzero_si256(), even_elements, cmp_mask);\n\n        // Horizontally sum the results\n        int16_t temp[16];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), filtered);\n        for (size_t j = 0; j < 16; ++j) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if ((i % 2 == 0) && (lst[i] % 2 != 0)) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t result = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 15 < n; i += 16) {\n        // Load 16 elements into a 256-bit register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Create a mask to select even positions (0, 2, 4, ..., 14)\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n\n        // Check for odd numbers (x & 1 != 0)\n        __m256i odd_mask = _mm256_set1_epi16(1);\n        __m256i is_odd = _mm256_and_si256(even_elements, odd_mask);\n\n        // Compare to create a mask of odd numbers\n        __m256i cmp_mask = _mm256_cmpeq_epi16(is_odd, odd_mask);\n\n        // Blend the original values with zeros based on the mask\n        __m256i filtered = _mm256_blendv_epi8(_mm256_setzero_si256(), even_elements, cmp_mask);\n\n        // Horizontally sum the 16 elements in the vector\n        __m256i hi = _mm256_unpackhi_epi16(filtered, _mm256_setzero_si256());\n        __m256i lo = _mm256_unpacklo_epi16(filtered, _mm256_setzero_si256());\n        __m256i sum_vec = _mm256_add_epi32(lo, hi);\n\n        hi = _mm256_unpackhi_epi32(sum_vec, _mm256_setzero_si256());\n        lo = _mm256_unpacklo_epi32(sum_vec, _mm256_setzero_si256());\n        sum_vec = _mm256_add_epi64(lo, hi);\n\n        // Extract the sum from the vector\n        int64_t temp[4];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), sum_vec);\n        result += temp[0] + temp[1] + temp[2] + temp[3];\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if ((i % 2 == 0) && (lst[i] % 2 != 0)) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t result = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n\n    // Process 16 elements at a time using AVX2\n    for (; i + 15 < n; i += 16) {\n        // Load 16 elements into an AVX2 register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Create a mask to select even positions (0, 2, 4, ..., 14)\n        __m256i even_mask = _mm256_set_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n\n        // Check for odd values in the even positions\n        __m256i odd_mask = _mm256_set1_epi16(1);\n        __m256i is_odd = _mm256_and_si256(even_elements, odd_mask);\n\n        // Compare to filter only odd values\n        __m256i cmp_mask = _mm256_cmpeq_epi16(is_odd, odd_mask);\n        __m256i filtered = _mm256_and_si256(even_elements, cmp_mask);\n\n        // Horizontal sum of the filtered values\n        int16_t temp[16];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), filtered);\n        for (size_t j = 0; j < 16; j += 2) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; i += 2) {\n        if (i % 2 == 0 && (lst[i] & 1)) {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= k - 8; i += 8) {\n        // Load 8 integers from the array into an AVX register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n\n        // Create a mask to identify elements with absolute value less than 100\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(99), abs_vec);\n\n        // Use the mask to zero out elements that don't meet the condition\n        __m256i masked_vec = _mm256_and_si256(vec, cmp_mask);\n\n        // Horizontally add the valid elements in the vector\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), masked_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements (less than 8)\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= k - 8; i += 8) {\n        // Load 8 integers from the array into an AVX register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n\n        // Create a mask to identify numbers with absolute value less than 100\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(99), abs_vec);\n\n        // Use the mask to zero out elements that don't meet the condition\n        __m256i masked_vec = _mm256_and_si256(vec, cmp_mask);\n\n        // Horizontally add the valid elements in the vector\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), masked_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements (less than 8)\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= k - 8; i += 8) {\n        // Load 8 integers from the array into a SIMD register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n\n        // Create a mask to identify elements with absolute value less than 100\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(99), abs_vec);\n\n        // Blend the original vector with zeros based on the mask\n        __m256i filtered = _mm256_blendv_epi8(_mm256_setzero_si256(), vec, cmp_mask);\n\n        // Horizontally add the elements in the filtered vector\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), filtered);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements (less than 8)\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int result = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= k - 8; i += 8) {\n        // Load 8 integers from the array into a SIMD register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n\n        // Create a mask to identify elements with absolute value less than 100\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(99), abs_vec);\n\n        // Use the mask to zero out elements that don't meet the condition\n        __m256i masked_vec = _mm256_and_si256(vec, cmp_mask);\n\n        // Horizontally sum the valid elements in the vector\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), masked_vec);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements (less than 8)\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            result += arr[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    int sum = 0;\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= k - 8; i += 8) {\n        // Load 8 integers into a SIMD register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n\n        // Create a mask to identify numbers with absolute value less than 100\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i cmp_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(99), abs_vec);\n\n        // Use the mask to zero out elements that don't meet the condition\n        __m256i masked_vec = _mm256_and_si256(vec, cmp_mask);\n\n        // Horizontally add the valid elements in the vector\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), masked_vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    // Ensure the vector is padded to a multiple of 8 for AVX2 processing\n    size_t padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_lst(padded_size, 0);\n    std::copy(lst.begin(), lst.end(), padded_lst.begin());\n\n    __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(padded_lst.data()));\n    bool has_duplicates = false;\n\n    for (size_t i = 8; i < padded_size; i += 8) {\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_lst[i]));\n\n        // Check if sorted: prev <= curr\n        __m256i cmp_sorted = _mm256_cmpgt_epi32(prev, curr);\n        int sorted_mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_sorted));\n        if (sorted_mask != 0) return false;\n\n        // Check for duplicates: prev == curr\n        __m256i cmp_eq = _mm256_cmpeq_epi32(prev, curr);\n        int eq_mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_eq));\n        if (eq_mask != 0) {\n            // Count duplicates in the current block\n            int count[8];\n            _mm256_storeu_si256(reinterpret_cast<__m256i*>(count), cmp_eq);\n            for (int j = 0; j < 8; ++j) {\n                if (count[j] && (i + j >= n || padded_lst[i + j - 1] == padded_lst[i + j])) {\n                    has_duplicates = true;\n                    break;\n                }\n            }\n        }\n\n        prev = curr;\n    }\n\n    // Handle edge case: more than one duplicate in the original list\n    for (size_t i = 1; i < n; ++i) {\n        if (padded_lst[i] == padded_lst[i - 1]) {\n            if (i + 1 < n && padded_lst[i] == padded_lst[i + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return !has_duplicates;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    // Ensure the vector is padded to a multiple of 8 for AVX2 processing\n    size_t padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_lst(padded_size, 0);\n    std::copy(lst.begin(), lst.end(), padded_lst.begin());\n\n    __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(padded_lst.data()));\n    bool sorted = true;\n    bool duplicates = false;\n\n    for (size_t i = 8; i < padded_size; i += 8) {\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_lst[i]));\n\n        // Check if current elements are greater than or equal to previous elements\n        __m256i cmp_ge = _mm256_cmpgt_epi32(curr, prev);\n        int mask_ge = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_ge));\n        if (mask_ge != 0xFF) {\n            sorted = false;\n            break;\n        }\n\n        // Check for duplicates by comparing adjacent elements\n        __m256i cmp_eq = _mm256_cmpeq_epi32(curr, prev);\n        int mask_eq = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_eq));\n        if (mask_eq != 0) {\n            duplicates = true;\n        }\n\n        prev = curr;\n    }\n\n    // Handle duplicates check for the last group\n    if (duplicates) {\n        for (size_t i = 1; i < n; ++i) {\n            if (padded_lst[i] == padded_lst[i - 1]) {\n                int count = 1;\n                while (i < n && padded_lst[i] == padded_lst[i - 1]) {\n                    ++count;\n                    ++i;\n                }\n                if (count > 2) {\n                    return false;\n                }\n            }\n        }\n    }\n\n    return sorted;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    size_t simd_width = 8; // AVX2 processes 8 integers at a time (256 bits / 32 bits per int)\n    size_t simd_aligned_size = (n / simd_width) * simd_width;\n\n    __m256i prev = _mm256_set1_epi32(0); // Initialize with zeros\n    bool first_iteration = true;\n\n    for (size_t i = 0; i < simd_aligned_size; i += simd_width) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        if (!first_iteration) {\n            // Check if current >= prev for all elements\n            __m256i cmp_ge = _mm256_cmpgt_epi32(current, prev);\n            __m256i cmp_eq = _mm256_cmpeq_epi32(current, prev);\n\n            int mask_ge = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_ge));\n            int mask_eq = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_eq));\n\n            if ((mask_ge | mask_eq) != 0xFF) {\n                return false;\n            }\n        }\n\n        // Check for duplicates within the current vector\n        for (int j = 0; j < simd_width; ++j) {\n            __m256i broadcast = _mm256_set1_epi32(lst[i + j]);\n            __m256i cmp_eq = _mm256_cmpeq_epi32(broadcast, current);\n            int mask_eq = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_eq));\n\n            // Count number of matches (including self-match)\n            int count = __builtin_popcount(mask_eq);\n            if (count > 2) {\n                return false;\n            }\n        }\n\n        prev = current;\n        first_iteration = false;\n    }\n\n    // Handle remaining elements that don't fit into SIMD width\n    for (size_t i = simd_aligned_size; i < n; ++i) {\n        if (i > 0 && lst[i] < lst[i - 1]) {\n            return false;\n        }\n        if (i > 1 && lst[i] == lst[i - 1] && lst[i] == lst[i - 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    // Ensure the vector is padded to a multiple of 8 for AVX2 processing\n    size_t padded_size = ((n + 7) / 8) * 8;\n    std::vector<int> padded_lst(padded_size, 0);\n    std::copy(lst.begin(), lst.end(), padded_lst.begin());\n\n    __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(padded_lst.data()));\n\n    for (size_t i = 8; i < padded_size; i += 8) {\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_lst[i]));\n\n        // Check if current >= previous (ascending order check)\n        __m256i cmp_ge = _mm256_cmpgt_epi32(curr, prev);\n        int mask_ge = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_ge));\n        if (mask_ge != 0xFF) return false;\n\n        // Check for duplicates by comparing adjacent elements in curr\n        __m256i shifted_curr = _mm256_alignr_epi8(curr, curr, 4); // Shift right by one element\n        __m256i cmp_eq = _mm256_cmpeq_epi32(curr, shifted_curr);\n        int mask_eq = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_eq));\n        if (mask_eq & 0xFE) return false; // Ignore the highest bit as it's padding\n\n        prev = curr;\n    }\n\n    // Handle the original vector size (ignore padding)\n    for (size_t i = 1; i < n; ++i) {\n        if (lst[i] < lst[i - 1]) return false;\n        if (i > 1 && lst[i] == lst[i - 1] && lst[i] == lst[i - 2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst) {\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    // Ensure the vector is padded to a multiple of 8 for AVX2 processing\n    size_t simd_size = 8;\n    size_t padded_size = ((n + simd_size - 1) / simd_size) * simd_size;\n    std::vector<int> padded_lst(padded_size, 0);\n    std::copy(lst.begin(), lst.end(), padded_lst.begin());\n\n    __m256i prev = _mm256_set1_epi32(0); // Initialize with zeros\n    bool has_duplicates = false;\n\n    for (size_t i = 0; i < padded_size; i += simd_size) {\n        __m256i curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_lst[i]));\n\n        // Check if current elements are sorted compared to previous elements\n        __m256i cmp_sorted = _mm256_cmpgt_epi32(curr, prev);\n        int mask_sorted = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_sorted));\n\n        // Check for duplicates by comparing adjacent elements\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&padded_lst[i + 1]));\n        __m256i cmp_duplicates = _mm256_cmpeq_epi32(curr, next);\n        int mask_duplicates = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_duplicates));\n\n        // Update flags\n        if (mask_sorted != 0xFF || mask_duplicates != 0) {\n            return false;\n        }\n\n        prev = curr;\n    }\n\n    // Final check for duplicates in the original list\n    for (size_t i = 1; i < n; ++i) {\n        if (lst[i] == lst[i - 1]) {\n            if (i + 1 < n && lst[i] == lst[i + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_ones = _mm256_set1_epi32(-1);\n    __m256i signs = _mm256_set1_epi32(1);\n    __m256i abs_sum_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    int size = arr.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i is_zero = _mm256_cmpeq_epi32(vec, zero);\n        __m256i is_neg = _mm256_cmpgt_epi32(zero, vec);\n        __m256i sign = _mm256_blendv_epi8(ones, neg_ones, is_neg);\n        sign = _mm256_blendv_epi8(sign, zero, is_zero);\n\n        signs = _mm256_mullo_epi32(signs, sign);\n\n        __m256i abs_val = _mm256_abs_epi32(vec);\n        abs_sum_vec = _mm256_add_epi32(abs_sum_vec, abs_val);\n    }\n\n    // Horizontal sum of absolute values in abs_sum_vec\n    int abs_sum = 0;\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, abs_sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        abs_sum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int val = arr[i];\n        if (val == 0) {\n            return 0;\n        }\n        signs = _mm256_mullo_epi32(signs, _mm256_set1_epi32((val > 0) ? 1 : -1));\n        abs_sum += std::abs(val);\n    }\n\n    // Horizontal product of signs\n    alignas(32) int sign_temp[8];\n    _mm256_store_si256((__m256i*)sign_temp, signs);\n    int sign_product = 1;\n    for (int j = 0; j < 8; ++j) {\n        sign_product *= sign_temp[j];\n    }\n\n    return abs_sum * sign_product;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_ones = _mm256_set1_epi32(-1);\n\n    int total_sum = 0;\n    int sign_product = 1;\n\n    for (size_t i = 0; i < arr.size(); i += 8) {\n        size_t limit = std::min(arr.size() - i, (size_t)8);\n        int temp_arr[8] = {0};\n        for (size_t j = 0; j < limit; ++j) {\n            temp_arr[j] = arr[i + j];\n        }\n\n        __m256i vec = _mm256_loadu_si256((__m256i*)temp_arr);\n        __m256i is_zero = _mm256_cmpeq_epi32(vec, zero);\n        __m256i is_neg = _mm256_cmpgt_epi32(zero, vec);\n\n        int mask_zero = _mm256_movemask_ps((__m256)is_zero);\n        int mask_neg = _mm256_movemask_ps((__m256)is_neg);\n\n        if (mask_zero != 0) {\n            sign_product = 0;\n        } else {\n            int neg_count = _mm_popcnt_u32(mask_neg);\n            if (neg_count % 2 != 0) {\n                sign_product *= -1;\n            }\n        }\n\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n        int sum[8];\n        _mm256_storeu_si256((__m256i*)sum, abs_vals);\n\n        for (size_t j = 0; j < limit; ++j) {\n            total_sum += sum[j];\n        }\n    }\n\n    return sign_product == 0 ? 0 : total_sum * sign_product;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_ones = _mm256_set1_epi32(-1);\n    __m256i signs = _mm256_set1_epi32(1);\n    __m256i abs_sum_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    int size = arr.size();\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 integers into a SIMD register\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Compute the absolute values\n        __m256i mask = _mm256_cmpgt_epi32(vec, zero);\n        __m256i abs_vals = _mm256_blendv_epi8(_mm256_sub_epi32(zero, vec), vec, mask);\n\n        // Accumulate the absolute values\n        abs_sum_vec = _mm256_add_epi32(abs_sum_vec, abs_vals);\n\n        // Compute the signs\n        __m256i sign_mask = _mm256_cmpeq_epi32(vec, zero);\n        __m256i current_signs = _mm256_blendv_epi8(_mm256_blendv_epi8(neg_ones, ones, mask), zero, sign_mask);\n\n        // Multiply the product of signs\n        signs = _mm256_mullo_epi32(signs, current_signs);\n    }\n\n    // Horizontal sum of the accumulated absolute values\n    int abs_sum = 0;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, abs_sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        abs_sum += temp[j];\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        int val = arr[i];\n        abs_sum += (val > 0) ? val : -val;\n        if (val == 0) {\n            return 0;\n        }\n        signs = _mm256_mullo_epi32(signs, _mm256_set1_epi32((val > 0) ? 1 : -1));\n    }\n\n    // Horizontal product of signs\n    int sign_product = 1;\n    _mm256_storeu_si256((__m256i*)temp, signs);\n    for (int j = 0; j < 8; ++j) {\n        sign_product *= temp[j];\n    }\n\n    return abs_sum * sign_product;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_ones = _mm256_set1_epi32(-1);\n    __m256i signs = _mm256_set1_epi32(1);\n    __m256i sum = _mm256_setzero_si256();\n\n    int i = 0;\n    int size = arr.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i is_zero = _mm256_cmpeq_epi32(vec, zero);\n        __m256i is_neg = _mm256_cmpgt_epi32(zero, vec);\n\n        __m256i current_signs = _mm256_blendv_epi8(ones, neg_ones, is_neg);\n        current_signs = _mm256_blendv_epi8(current_signs, zero, is_zero);\n\n        signs = _mm256_mullo_epi32(signs, current_signs);\n\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n        sum = _mm256_add_epi32(sum, abs_vals);\n    }\n\n    // Horizontal reduction of signs and sum\n    int sign_reduction[8];\n    _mm256_storeu_si256((__m256i*)sign_reduction, signs);\n    int final_sign = 1;\n    for (int j = 0; j < 8; ++j) {\n        final_sign *= sign_reduction[j];\n    }\n\n    int sum_reduction[8];\n    _mm256_storeu_si256((__m256i*)sum_reduction, sum);\n    int final_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        final_sum += sum_reduction[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (arr[i] == 0) return 0;\n        final_sign *= (arr[i] > 0) ? 1 : -1;\n        final_sum += std::abs(arr[i]);\n    }\n\n    return final_sign * final_sum;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint prod_signs_simd(const std::vector<int> & arr) {\n    if (arr.empty()) return -32768;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i neg_ones = _mm256_set1_epi32(-1);\n    __m256i signs = _mm256_set1_epi32(1);\n    __m256i abs_sum_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    int size = arr.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i is_zero = _mm256_cmpeq_epi32(vec, zero);\n        __m256i is_neg = _mm256_cmpgt_epi32(zero, vec);\n\n        __m256i sign = _mm256_blendv_epi8(ones, neg_ones, is_neg);\n        sign = _mm256_blendv_epi8(sign, zero, is_zero);\n\n        signs = _mm256_mullo_epi32(signs, sign);\n\n        __m256i abs_val = _mm256_abs_epi32(vec);\n        abs_sum_vec = _mm256_add_epi32(abs_sum_vec, abs_val);\n    }\n\n    int abs_sum = 0;\n    int hsum[8];\n    _mm256_storeu_si256((__m256i*)hsum, abs_sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        abs_sum += hsum[j];\n    }\n\n    int sign_product = 1;\n    int hprod[8];\n    _mm256_storeu_si256((__m256i*)hprod, signs);\n    for (int j = 0; j < 8; ++j) {\n        sign_product *= hprod[j];\n    }\n\n    for (; i < size; ++i) {\n        if (arr[i] == 0) {\n            sign_product = 0;\n        } else {\n            sign_product *= (arr[i] > 0) ? 1 : -1;\n        }\n        abs_sum += std::abs(arr[i]);\n    }\n\n    return abs_sum * sign_product;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k, INT32_MAX);\n\n    // Directions: up, down, left, right\n    const int dx[4] = {-1, 1, 0, 0};\n    const int dy[4] = {0, 0, -1, 1};\n\n    // AVX register for minimum path comparison\n    __m256i minPathVec = _mm256_set1_epi32(INT32_MAX);\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            std::vector<int> currentPath;\n            currentPath.push_back(grid[i][j]);\n\n            // Initialize AVX register with the starting cell value\n            __m256i currentPathVec = _mm256_set1_epi32(grid[i][j]);\n\n            // Perform BFS-like exploration using AVX for parallelism\n            for (int step = 1; step < k; ++step) {\n                __m256i nextMin = _mm256_set1_epi32(INT32_MAX);\n                for (int dir = 0; dir < 4; ++dir) {\n                    int ni = i + dx[dir];\n                    int nj = j + dy[dir];\n\n                    if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                        // Load the neighbor value into an AVX register\n                        __m256i neighborVal = _mm256_set1_epi32(grid[ni][nj]);\n\n                        // Compare and update the minimum value for the next step\n                        nextMin = _mm256_min_epi32(nextMin, neighborVal);\n                    }\n                }\n\n                // Update the current path vector with the minimum value found\n                currentPathVec = _mm256_alignr_epi8(currentPathVec, nextMin, 4);\n                int minValue = _mm256_extract_epi32(nextMin, 0);\n                currentPath.push_back(minValue);\n            }\n\n            // Compare the current path with the global minimum path\n            minPathVec = _mm256_min_epi32(minPathVec, currentPathVec);\n        }\n    }\n\n    // Extract the result from the AVX register\n    int tempResult[8];\n    _mm256_storeu_si256((__m256i*)tempResult, minPathVec);\n    for (int i = 0; i < k; ++i) {\n        result[i] = tempResult[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k, INT32_MAX);\n    alignas(32) int current_path[8] = {0};\n    alignas(32) int best_path[8] = {0};\n\n    // Initialize best_path with maximum values\n    for (int i = 0; i < 8; ++i) {\n        best_path[i] = INT32_MAX;\n    }\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            // Start DFS from each cell (i, j)\n            std::vector<std::pair<int, int>> stack;\n            stack.emplace_back(i, j);\n\n            while (!stack.empty()) {\n                auto [x, y] = stack.back();\n                stack.pop_back();\n\n                // Load current path into SIMD register\n                __m256i current_path_simd = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(current_path));\n\n                // Add current cell value to the path\n                current_path[0] = grid[x][y];\n                current_path_simd = _mm256_insert_epi32(current_path_simd, grid[x][y], 0);\n\n                // Compare current path with best path lexicographically\n                __m256i best_path_simd = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(best_path));\n                __m256i cmp = _mm256_cmpgt_epi32(best_path_simd, current_path_simd);\n\n                // If current path is better, update best path\n                if (_mm256_movemask_epi8(cmp)) {\n                    _mm256_storeu_si256(reinterpret_cast<__m256i*>(best_path), current_path_simd);\n                }\n\n                // Push neighbors onto the stack\n                if (x > 0) stack.emplace_back(x - 1, y);\n                if (x < N - 1) stack.emplace_back(x + 1, y);\n                if (y > 0) stack.emplace_back(x, y - 1);\n                if (y < N - 1) stack.emplace_back(x, y + 1);\n            }\n        }\n    }\n\n    // Copy the best path to result\n    for (int i = 0; i < k; ++i) {\n        result[i] = best_path[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k, 0);\n    std::vector<std::vector<int>> dp(N, std::vector<int>(N, 0));\n    std::vector<std::vector<int>> next_dp(N, std::vector<int>(N, 0));\n\n    // Initialize dp with the grid values as starting points\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp[i][j] = grid[i][j];\n        }\n    }\n\n    for (int step = 1; step < k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j < N; ++j) {\n                __m256i min_val = _mm256_set1_epi32(INT32_MAX);\n\n                // Check all 8 neighbors\n                for (int di = -1; di <= 1; ++di) {\n                    for (int dj = -1; dj <= 1; ++dj) {\n                        if (di == 0 && dj == 0) continue;\n                        int ni = i + di, nj = j + dj;\n                        if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                            __m256i val = _mm256_set1_epi32(dp[ni][nj]);\n                            min_val = _mm256_min_epi32(min_val, val);\n                        }\n                    }\n                }\n\n                // Extract minimum value from SIMD register\n                int min_neighbor = INT32_MAX;\n                int temp[8];\n                _mm256_storeu_si256((__m256i*)temp, min_val);\n                for (int t = 0; t < 8; ++t) {\n                    min_neighbor = std::min(min_neighbor, temp[t]);\n                }\n\n                next_dp[i][j] = grid[i][j] + min_neighbor;\n            }\n        }\n        dp.swap(next_dp);\n    }\n\n    // Find the minimum path\n    int min_val = INT32_MAX;\n    int start_i = 0, start_j = 0;\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (dp[i][j] < min_val) {\n                min_val = dp[i][j];\n                start_i = i;\n                start_j = j;\n            }\n        }\n    }\n\n    // Reconstruct path\n    int i = start_i, j = start_j;\n    for (int step = 0; step < k; ++step) {\n        result[step] = grid[i][j];\n        int min_neighbor = INT32_MAX;\n        int next_i = i, next_j = j;\n\n        for (int di = -1; di <= 1; ++di) {\n            for (int dj = -1; dj <= 1; ++dj) {\n                if (di == 0 && dj == 0) continue;\n                int ni = i + di, nj = j + dj;\n                if (ni >= 0 && ni < N && nj >= 0 && nj < N) {\n                    if (dp[ni][nj] < min_neighbor) {\n                        min_neighbor = dp[ni][nj];\n                        next_i = ni;\n                        next_j = nj;\n                    }\n                }\n            }\n        }\n\n        i = next_i;\n        j = next_j;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result(k, 0);\n    std::vector<std::vector<int>> dp_prev(N, std::vector<int>(N, 0));\n    std::vector<std::vector<int>> dp_curr(N, std::vector<int>(N, 0));\n\n    // Initialize DP for the first step\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            dp_prev[i][j] = grid[i][j];\n        }\n    }\n\n    for (int step = 1; step < k; ++step) {\n        for (int i = 0; i < N; ++i) {\n            for (int j = 0; j <= N - 8; j += 8) {\n                __m256i min_vals = _mm256_set1_epi32(INT32_MAX);\n\n                // Check all neighbors and compute minimum using AVX\n                if (i > 0) { // Up\n                    __m256i up_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&dp_prev[i - 1][j]));\n                    min_vals = _mm256_min_epi32(min_vals, up_vals);\n                }\n                if (i < N - 1) { // Down\n                    __m256i down_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&dp_prev[i + 1][j]));\n                    min_vals = _mm256_min_epi32(min_vals, down_vals);\n                }\n                if (j > 0) { // Left\n                    __m256i left_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&dp_prev[i][j - 1]));\n                    min_vals = _mm256_min_epi32(min_vals, left_vals);\n                }\n                if (j < N - 1) { // Right\n                    __m256i right_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&dp_prev[i][j + 1]));\n                    min_vals = _mm256_min_epi32(min_vals, right_vals);\n                }\n\n                // Add current cell value to the minimum path value\n                __m256i curr_vals = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[i][j]));\n                min_vals = _mm256_add_epi32(min_vals, curr_vals);\n\n                // Store the result back into dp_curr\n                _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dp_curr[i][j]), min_vals);\n            }\n\n            // Handle remaining columns (if N % 8 != 0)\n            for (int j = N - (N % 8); j < N; ++j) {\n                int min_val = INT32_MAX;\n                if (i > 0) min_val = std::min(min_val, dp_prev[i - 1][j]);\n                if (i < N - 1) min_val = std::min(min_val, dp_prev[i + 1][j]);\n                if (j > 0) min_val = std::min(min_val, dp_prev[i][j - 1]);\n                if (j < N - 1) min_val = std::min(min_val, dp_prev[i][j + 1]);\n                dp_curr[i][j] = grid[i][j] + min_val;\n            }\n        }\n\n        // Swap dp_prev and dp_curr for the next iteration\n        dp_prev.swap(dp_curr);\n    }\n\n    // Find the lexicographically smallest path\n    int min_val = INT32_MAX;\n    int start_i = 0, start_j = 0;\n\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            if (dp_prev[i][j] < min_val) {\n                min_val = dp_prev[i][j];\n                start_i = i;\n                start_j = j;\n            }\n        }\n    }\n\n    // Reconstruct the path\n    int i = start_i, j = start_j;\n    result[0] = grid[i][j];\n\n    for (int step = 1; step < k; ++step) {\n        int next_val = INT32_MAX;\n        int next_i = i, next_j = j;\n\n        if (i > 0 && dp_prev[i - 1][j] < next_val) {\n            next_val = dp_prev[i - 1][j];\n            next_i = i - 1;\n            next_j = j;\n        }\n        if (i < N - 1 && dp_prev[i + 1][j] < next_val) {\n            next_val = dp_prev[i + 1][j];\n            next_i = i + 1;\n            next_j = j;\n        }\n        if (j > 0 && dp_prev[i][j - 1] < next_val) {\n            next_val = dp_prev[i][j - 1];\n            next_i = i;\n            next_j = j - 1;\n        }\n        if (j < N - 1 && dp_prev[i][j + 1] < next_val) {\n            next_val = dp_prev[i][j + 1];\n            next_i = i;\n            next_j = j + 1;\n        }\n\n        result[step] = grid[next_i][next_j];\n        i = next_i;\n        j = next_j;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    const int INF = 1e9;\n    std::vector<int> result(k, INF);\n\n    // Directions: up, down, left, right\n    int dirX[4] = {-1, 1, 0, 0};\n    int dirY[4] = {0, 0, -1, 1};\n\n    // Flatten the grid into a 1D array for SIMD processing\n    std::vector<int> flatGrid(N * N);\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; ++j) {\n            flatGrid[i * N + j] = grid[i][j];\n        }\n    }\n\n    // Initialize AVX registers for minimum path tracking\n    __m256i minPath = _mm256_set1_epi32(INF);\n\n    // Iterate over all starting positions in the grid\n    for (int start = 0; start < N * N; ++start) {\n        int startX = start / N;\n        int startY = start % N;\n\n        // Initialize current path with the starting cell value\n        std::vector<int> currentPath(k, INF);\n        currentPath[0] = grid[startX][startY];\n\n        // Use BFS-like approach to explore paths of length k\n        std::vector<std::pair<int, int>> queue;\n        queue.emplace_back(startX, startY);\n\n        int step = 1;\n        while (!queue.empty() && step < k) {\n            std::vector<std::pair<int, int>> nextQueue;\n            for (const auto &[x, y] : queue) {\n                for (int d = 0; d < 4; ++d) {\n                    int nx = x + dirX[d];\n                    int ny = y + dirY[d];\n\n                    if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n                        currentPath[step] = std::min(currentPath[step], grid[nx][ny]);\n                        nextQueue.emplace_back(nx, ny);\n                    }\n                }\n            }\n            queue = std::move(nextQueue);\n            ++step;\n        }\n\n        // Compare currentPath with the global minimum path using SIMD\n        for (int i = 0; i < k; i += 8) {\n            __m256i currentChunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&currentPath[i]));\n            minPath = _mm256_min_epi32(minPath, currentChunk);\n        }\n    }\n\n    // Extract the minimum path from AVX registers\n    alignas(32) int tempResult[8];\n    _mm256_store_si256(reinterpret_cast<__m256i*>(tempResult), minPath);\n\n    for (int i = 0; i < k; ++i) {\n        result[i] = std::min(result[i], tempResult[i % 8]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i tens = _mm256_set1_epi8(10);\n    __m256i product = _mm256_set1_epi8(1);\n    __m256i all_even_mask = _mm256_set1_epi8(0xFF);\n\n    while (n > 0) {\n        uint64_t chunk = n % 100000000;\n        n /= 100000000;\n\n        __m256i digits = _mm256_set_epi64x(\n            (chunk / 1000000) % 10,\n            (chunk / 100000) % 10,\n            (chunk / 10000) % 10,\n            (chunk / 1000) % 10,\n            (chunk / 100) % 10,\n            (chunk / 10) % 10,\n            chunk % 10,\n            0\n        );\n\n        __m256i is_odd = _mm256_and_si256(digits, ones);\n        __m256i odd_mask = _mm256_cmpgt_epi8(is_odd, zero);\n\n        all_even_mask = _mm256_and_si256(all_even_mask, odd_mask);\n\n        __m256i masked_digits = _mm256_blendv_epi8(_mm256_set1_epi8(1), digits, odd_mask);\n        product = _mm256_mullo_epi16(product, masked_digits);\n    }\n\n    alignas(32) uint8_t temp[32];\n    _mm256_store_si256((__m256i*)temp, product);\n\n    uint64_t result = 1;\n    for (int i = 0; i < 32; ++i) {\n        result *= temp[i];\n    }\n\n    alignas(32) uint8_t even_check[32];\n    _mm256_store_si256((__m256i*)even_check, all_even_mask);\n\n    for (int i = 0; i < 32; ++i) {\n        if (even_check[i] == 0) {\n            return 0;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i tens = _mm256_set1_epi8(10);\n    __m256i product = _mm256_set1_epi8(1);\n    __m256i all_even = _mm256_set1_epi8(1);\n\n    while (n > 0) {\n        uint64_t chunk = n % 100000000;\n        n /= 100000000;\n\n        __m256i digits = _mm256_set_epi64x(\n            (chunk / 1000000) % 10,\n            (chunk / 100000) % 10,\n            (chunk / 10000) % 10,\n            (chunk / 1000) % 10,\n            (chunk / 100) % 10,\n            (chunk / 10) % 10,\n            chunk % 10,\n            0\n        );\n\n        __m256i is_odd = _mm256_and_si256(digits, ones);\n        __m256i mask = _mm256_cmpgt_epi8(is_odd, zero);\n        all_even = _mm256_and_si256(all_even, _mm256_cmpeq_epi8(mask, zero));\n\n        __m256i odd_digits = _mm256_blendv_epi8(ones, digits, mask);\n        product = _mm256_mullo_epi16(product, odd_digits);\n    }\n\n    int result = 1;\n    int16_t temp[16];\n    _mm256_storeu_si256((__m256i*)temp, product);\n\n    for (int i = 0; i < 16; ++i) {\n        result *= temp[i];\n    }\n\n    int16_t even_check[16];\n    _mm256_storeu_si256((__m256i*)even_check, all_even);\n\n    for (int i = 0; i < 16; ++i) {\n        if (even_check[i] != 0) {\n            return 0;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i v_odd_mask = _mm256_set1_epi8(0x01); // Mask to check odd digits (1, 3, 5, 7, 9)\n    __m256i v_zero = _mm256_setzero_si256();     // Zero vector for comparison\n    __m256i v_product = _mm256_set1_epi64x(1);   // Initialize product vector to 1\n\n    uint64_t result = 1;\n    int has_odd = 0;\n\n    while (n > 0) {\n        // Load 4 digits into a vector (assuming n is split into bytes)\n        uint8_t digits[32];\n        for (int i = 0; i < 32 && n > 0; ++i) {\n            digits[i] = n % 10;\n            n /= 10;\n        }\n\n        __m256i v_digits = _mm256_loadu_si256((__m256i*)digits);\n\n        // Check if digits are odd\n        __m256i v_is_odd = _mm256_and_si256(v_digits, v_odd_mask);\n        __m256i v_mask = _mm256_cmpgt_epi8(v_is_odd, v_zero);\n\n        // Extract odd digits using mask\n        __m256i v_odd_digits = _mm256_blendv_epi8(_mm256_set1_epi8(1), v_digits, v_mask);\n\n        // Multiply odd digits together\n        v_product = _mm256_mullo_epi64(v_product, v_odd_digits);\n\n        // Check if any odd digit was found\n        int mask = _mm256_movemask_epi8(v_mask);\n        if (mask != 0) {\n            has_odd = 1;\n        }\n    }\n\n    // Horizontal reduction of the product vector\n    uint64_t temp[4];\n    _mm256_storeu_si256((__m256i*)temp, v_product);\n    result = temp[0] * temp[1] * temp[2] * temp[3];\n\n    return has_odd ? result : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i tens = _mm256_set1_epi8(10);\n    __m256i product = _mm256_set1_epi8(1);\n    __m256i all_even_mask = _mm256_set1_epi8(0xFF);\n\n    while (n > 0) {\n        uint64_t chunk = n % 100000000;\n        n /= 100000000;\n\n        __m256i digits = _mm256_set_epi64x(\n            (chunk / 1000000) % 10,\n            (chunk / 100000) % 10,\n            (chunk / 10000) % 10,\n            (chunk / 1000) % 10,\n            (chunk / 100) % 10,\n            (chunk / 10) % 10,\n            chunk % 10,\n            0\n        );\n\n        __m256i is_odd = _mm256_and_si256(digits, ones);\n        __m256i mask = _mm256_cmpgt_epi8(is_odd, zero);\n\n        product = _mm256_mullo_epi8(product, _mm256_blendv_epi8(ones, digits, mask));\n        all_even_mask = _mm256_and_si256(all_even_mask, mask);\n    }\n\n    int result = 1;\n    int mask_result = 0xFF;\n    for (int i = 0; i < 32; ++i) {\n        result *= ((uint8_t*)&product)[i];\n        mask_result &= ((uint8_t*)&all_even_mask)[i];\n    }\n\n    return mask_result ? result : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i tens = _mm256_set1_epi8(10);\n    __m256i product = _mm256_set1_epi8(1);\n    __m256i all_even_mask = _mm256_set1_epi8(0xFF);\n\n    while (n > 0) {\n        uint64_t chunk = n % 100000000; // Process 8 digits at a time\n        n /= 100000000;\n\n        __m256i digits = _mm256_set_epi64x(\n            (chunk / 1000000) % 10,\n            (chunk / 100000) % 10,\n            (chunk / 10000) % 10,\n            (chunk / 1000) % 10,\n            (chunk / 100) % 10,\n            (chunk / 10) % 10,\n            chunk % 10,\n            0\n        );\n\n        __m256i is_odd = _mm256_and_si256(digits, ones);\n        __m256i odd_mask = _mm256_cmpgt_epi8(is_odd, zero);\n\n        product = _mm256_mullo_epi8(product, _mm256_blendv_epi8(ones, digits, odd_mask));\n        all_even_mask = _mm256_and_si256(all_even_mask, odd_mask);\n    }\n\n    // Horizontal reduction of the product vector\n    uint8_t result = 1;\n    uint8_t buffer[32];\n    _mm256_storeu_si256((__m256i*)buffer, product);\n\n    for (int i = 0; i < 32; ++i) {\n        if (buffer[i] != 1) {\n            result *= buffer[i];\n        }\n    }\n\n    // Check if all digits were even\n    uint8_t even_check[32];\n    _mm256_storeu_si256((__m256i*)even_check, all_even_mask);\n\n    for (int i = 0; i < 32; ++i) {\n        if (even_check[i] == 0xFF) {\n            return 0;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t i = 0;\n    size_t size = lst.size();\n    uint64_t result = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from the vector\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Compute ceiling of each float\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n\n        // Convert ceiling results to integers\n        __m256i int_vec = _mm256_cvtps_epi32(ceil_vec);\n\n        // Square the integers\n        __m256i square_vec = _mm256_mullo_epi32(int_vec, int_vec);\n\n        // Accumulate the squared values into sum_vec\n        sum_vec = _mm256_add_epi32(sum_vec, square_vec);\n    }\n\n    // Horizontal sum of the accumulated SIMD register\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int ceil_val = static_cast<int>(std::ceil(lst[i]));\n        result += ceil_val * ceil_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t i = 0;\n    size_t size = lst.size();\n    uint64_t result = 0;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from the vector\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Compute ceiling for each float\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n\n        // Convert to integers (32-bit)\n        __m256i int_vec = _mm256_cvtps_epi32(ceil_vec);\n\n        // Compute squares of integers\n        __m256i square_vec = _mm256_mullo_epi32(int_vec, int_vec);\n\n        // Accumulate horizontally within the SIMD register\n        sum_vec = _mm256_add_epi32(sum_vec, square_vec);\n    }\n\n    // Horizontal sum of the accumulated SIMD register\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for (int j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int ceil_val = static_cast<int>(std::ceil(lst[i]));\n        result += ceil_val * ceil_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t i = 0;\n    size_t size = lst.size();\n    __m256i v_ones = _mm256_set1_epi32(1);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_sum = _mm256_setzero_si256();\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from the vector\n        __m256 v_floats = _mm256_loadu_ps(&lst[i]);\n\n        // Convert floats to integers with ceiling\n        __m256 v_rounded = _mm256_ceil_ps(v_floats);\n        __m256i v_ints = _mm256_cvtps_epi32(v_rounded);\n\n        // Compute squares of integers\n        __m256i v_squared_lo = _mm256_mullo_epi32(v_ints, v_ints);\n\n        // Accumulate horizontally within the SIMD register\n        __m256i v_temp = _mm256_hadd_epi32(v_squared_lo, v_squared_lo);\n        v_temp = _mm256_hadd_epi32(v_temp, v_temp);\n\n        // Extract low and high parts and add them\n        __m128i v_low = _mm256_castsi256_si128(v_temp);\n        __m128i v_high = _mm256_extracti128_si256(v_temp, 1);\n        __m128i v_total = _mm_add_epi32(v_low, v_high);\n\n        // Add to the running sum\n        v_sum = _mm256_add_epi32(v_sum, _mm256_cvtepi32_epi64(v_total));\n    }\n\n    // Horizontal sum of the accumulated results in v_sum\n    uint64_t result = 0;\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, v_sum);\n    for (int j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int rounded = static_cast<int>(std::ceil(lst[i]));\n        result += static_cast<uint64_t>(rounded * rounded);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t i = 0;\n    size_t size = lst.size();\n    __m256 acc = _mm256_setzero_ps();\n\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from the vector\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Compute ceiling of each element\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n\n        // Square each element\n        __m256 squared = _mm256_mul_ps(ceil_vec, ceil_vec);\n\n        // Accumulate the results\n        acc = _mm256_add_ps(acc, squared);\n    }\n\n    // Horizontal sum of the accumulated AVX register\n    float result[8];\n    _mm256_storeu_ps(result, acc);\n    uint64_t sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        sum += static_cast<uint64_t>(result[j]);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = std::ceil(lst[i]);\n        sum += static_cast<uint64_t>(val * val);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    size_t i = 0;\n    size_t size = lst.size();\n    __m256i vsum = _mm256_setzero_si256();\n    \n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats from the vector\n        __m256 v = _mm256_loadu_ps(&lst[i]);\n        \n        // Compute ceiling of each float\n        __m256 vceil = _mm256_ceil_ps(v);\n        \n        // Convert to integers (32-bit)\n        __m256i vi = _mm256_cvtps_epi32(vceil);\n        \n        // Square the integers\n        __m256i vsq = _mm256_mullo_epi32(vi, vi);\n        \n        // Accumulate the squared values\n        vsum = _mm256_add_epi32(vsum, vsq);\n    }\n    \n    // Horizontal sum of the accumulated 8 integers\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    uint64_t result = 0;\n    for (int j = 0; j < 8; ++j) {\n        result += temp[j];\n    }\n    \n    // Process remaining elements\n    for (; i < size; ++i) {\n        int ceiled = static_cast<int>(std::ceil(lst[i]));\n        result += ceiled * ceiled;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    int i = 1;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i prev    = _mm256_loadu_si256((__m256i*)&arr[i - 1]);\n\n        // Compare current >= prev\n        __m256i cmp = _mm256_cmpgt_epi32(current, prev);\n\n        // Create a mask from the comparison result\n        int mask = _mm256_movemask_epi8(cmp);\n\n        // If any element is not greater than or equal to the previous one\n        if (mask != 0xFFFFFFFF) {\n            // Find the first index where the condition fails\n            for (int j = 0; j < 8; ++j) {\n                if (!(mask & (1 << (j * 4)))) {\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i increment = _mm256_set1_epi32(8);\n\n    int result = -1;\n    for (int i = 0; i + 8 <= n; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i next = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n\n        __m256i cmp = _mm256_cmpgt_epi32(_mm256_permutevar8x32_epi32(current, _mm256_setr_epi32(1, 2, 3, 4, 5, 6, 7, 0)), current);\n        int mask = _mm256_movemask_ps((__m256)cmp);\n\n        if (mask != 0) {\n            int pos = __builtin_ctz(mask);\n            result = i + pos;\n            break;\n        }\n\n        indices = _mm256_add_epi32(indices, increment);\n    }\n\n    for (int i = n - (n % 8); i < n - 1; ++i) {\n        if (arr[i] >= arr[i + 1]) {\n            result = i + 1;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    int i = 1;\n\n    // Process elements in chunks of 8 using AVX2\n    for (; i <= n - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i - 1]));\n\n        // Compare current >= prev\n        __m256i cmp = _mm256_cmpgt_epi32(current, prev);\n\n        // Create a mask from the comparison result\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n        // If any element is not greater than or equal to the previous one\n        if (mask != 0xFF) {\n            // Find the first index where the condition fails\n            for (int j = 0; j < 8; ++j) {\n                if (!(mask & (1 << j))) {\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    int result = -1;\n    int i = 1;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= n - 8; i += 8) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i prev    = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i - 1]));\n\n        // Compare current >= prev\n        __m256i cmp = _mm256_cmpgt_epi32(current, prev);\n\n        // Create a mask of the comparison results\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n\n        // If any element is not greater than or equal to the previous one\n        if (mask != 0xFF) {\n            // Find the first index where the condition fails\n            for (int j = 0; j < 8; ++j) {\n                if (!(mask & (1 << j))) {\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (arr[i] < arr[i - 1]) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint can_arrange_simd(const std::vector<int> & arr) {\n    int n = arr.size();\n    if (n < 2) return -1;\n\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    int result = -1;\n\n    for (int i = 1; i < n; i += 8) {\n        __m256i curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(prev, curr);\n\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            for (int j = 0; j < 8; ++j) {\n                if (i + j < n && arr[i + j] < arr[i + j - 1]) {\n                    result = i + j;\n                    break;\n                }\n            }\n            if (result != -1) break;\n        }\n\n        prev = curr;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_neg = 0;\n    int smallest_pos = 0;\n\n    if (lst.empty()) {\n        return {largest_neg, smallest_pos};\n    }\n\n    __m256i neg_inf = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i pos_inf = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_set1_epi32(0);\n\n    __m256i largest_neg_vec = neg_inf;\n    __m256i smallest_pos_vec = pos_inf;\n\n    size_t i = 0;\n    size_t simd_size = lst.size() - (lst.size() % 8);\n\n    for (; i < simd_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n\n        // Extract negatives and update largest_neg_vec\n        __m256i neg_vals = _mm256_blendv_epi8(neg_inf, vec, neg_mask);\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, neg_vals);\n\n        // Extract positives and update smallest_pos_vec\n        __m256i pos_vals = _mm256_blendv_epi8(pos_inf, vec, pos_mask);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, pos_vals);\n    }\n\n    // Horizontal reduction for largest_neg_vec\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), largest_neg_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] < 0 && temp[j] > largest_neg) {\n            largest_neg = temp[j];\n        }\n    }\n\n    // Horizontal reduction for smallest_pos_vec\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), smallest_pos_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > 0 && temp[j] < smallest_pos) {\n            smallest_pos = temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_neg) {\n            largest_neg = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_pos) {\n            smallest_pos = lst[i];\n        }\n    }\n\n    // Set to 0 if no valid values found\n    if (largest_neg == std::numeric_limits<int>::min()) largest_neg = 0;\n    if (smallest_pos == std::numeric_limits<int>::max()) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_neg = 0;\n    int smallest_pos = 0;\n\n    if (lst.empty()) {\n        return {largest_neg, smallest_pos};\n    }\n\n    __m256i neg_inf = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i pos_inf = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_set1_epi32(0);\n\n    __m256i largest_neg_vec = neg_inf;\n    __m256i smallest_pos_vec = pos_inf;\n\n    size_t i = 0;\n    size_t size = lst.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n\n        // Extract negatives and update largest_neg_vec\n        __m256i neg_vals = _mm256_blendv_epi8(neg_inf, vec, neg_mask);\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, neg_vals);\n\n        // Extract positives and update smallest_pos_vec\n        __m256i pos_vals = _mm256_blendv_epi8(pos_inf, vec, pos_mask);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, pos_vals);\n    }\n\n    // Horizontal reduction for largest_neg_vec\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), largest_neg_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] < 0 && temp[j] > largest_neg) {\n            largest_neg = temp[j];\n        }\n    }\n\n    // Horizontal reduction for smallest_pos_vec\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), smallest_pos_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > 0 && temp[j] < smallest_pos) {\n            smallest_pos = temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (lst[i] < 0 && lst[i] > largest_neg) {\n            largest_neg = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_pos) {\n            smallest_pos = lst[i];\n        }\n    }\n\n    // If no valid values were found, set to 0\n    if (largest_neg == std::numeric_limits<int>::min()) largest_neg = 0;\n    if (smallest_pos == std::numeric_limits<int>::max()) smallest_pos = 0;\n\n    return {largest_neg, smallest_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {largest_negative, smallest_positive};\n    }\n\n    __m256i neg_inf = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i pos_inf = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_set1_epi32(0);\n\n    __m256i largest_neg_vec = neg_inf;\n    __m256i smallest_pos_vec = pos_inf;\n\n    size_t i = 0;\n    size_t simd_size = lst.size() - (lst.size() % 8);\n\n    for (; i < simd_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n\n        // Extract negatives and update largest negative\n        __m256i negatives = _mm256_blendv_epi8(neg_inf, vec, neg_mask);\n        largest_neg_vec = _mm256_max_epi32(largest_neg_vec, negatives);\n\n        // Extract positives and update smallest positive\n        __m256i positives = _mm256_blendv_epi8(pos_inf, vec, pos_mask);\n        smallest_pos_vec = _mm256_min_epi32(smallest_pos_vec, positives);\n    }\n\n    // Horizontal reduction for largest negative\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), largest_neg_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] < 0 && temp[j] > largest_negative) {\n            largest_negative = temp[j];\n        }\n    }\n\n    // Horizontal reduction for smallest positive\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), smallest_pos_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] > 0 && temp[j] < smallest_positive) {\n            smallest_positive = temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_positive) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    // Set to 0 if no valid values found\n    if (largest_negative == std::numeric_limits<int>::min()) {\n        largest_negative = 0;\n    }\n    if (smallest_positive == std::numeric_limits<int>::max()) {\n        smallest_positive = 0;\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {largest_negative, smallest_positive};\n    }\n\n    __m256i neg_inf = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i pos_inf = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_set1_epi32(0);\n\n    __m256i max_neg = neg_inf;\n    __m256i min_pos = pos_inf;\n\n    size_t i = 0;\n    size_t size = lst.size();\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n\n        // Extract negatives and update max_neg\n        __m256i negatives = _mm256_blendv_epi8(neg_inf, vec, neg_mask);\n        max_neg = _mm256_max_epi32(max_neg, negatives);\n\n        // Extract positives and update min_pos\n        __m256i positives = _mm256_blendv_epi8(pos_inf, vec, pos_mask);\n        min_pos = _mm256_min_epi32(min_pos, positives);\n    }\n\n    // Horizontal reduction for max_neg\n    int temp_max_neg[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_max_neg), max_neg);\n    for (int j = 0; j < 8; ++j) {\n        if (temp_max_neg[j] < 0 && temp_max_neg[j] > largest_negative) {\n            largest_negative = temp_max_neg[j];\n        }\n    }\n\n    // Horizontal reduction for min_pos\n    int temp_min_pos[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_min_pos), min_pos);\n    for (int j = 0; j < 8; ++j) {\n        if (temp_min_pos[j] > 0 && temp_min_pos[j] < smallest_positive) {\n            smallest_positive = temp_min_pos[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_positive) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    int largest_negative = 0;\n    int smallest_positive = 0;\n\n    if (lst.empty()) {\n        return {largest_negative, smallest_positive};\n    }\n\n    __m256i neg_inf = _mm256_set1_epi32(std::numeric_limits<int>::min());\n    __m256i pos_inf = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i zero = _mm256_set1_epi32(0);\n\n    __m256i max_neg = neg_inf;\n    __m256i min_pos = pos_inf;\n\n    size_t i = 0;\n    size_t simd_size = lst.size() / 8 * 8;\n\n    for (; i < simd_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n        // Mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, zero);\n\n        // Extract negatives and update max_neg\n        __m256i negatives = _mm256_blendv_epi8(neg_inf, vec, neg_mask);\n        max_neg = _mm256_max_epi32(max_neg, negatives);\n\n        // Extract positives and update min_pos\n        __m256i positives = _mm256_blendv_epi8(pos_inf, vec, pos_mask);\n        min_pos = _mm256_min_epi32(min_pos, positives);\n    }\n\n    // Horizontal reduction for max_neg\n    int max_neg_vals[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(max_neg_vals), max_neg);\n    for (int j = 0; j < 8; ++j) {\n        if (max_neg_vals[j] < 0 && max_neg_vals[j] > largest_negative) {\n            largest_negative = max_neg_vals[j];\n        }\n    }\n\n    // Horizontal reduction for min_pos\n    int min_pos_vals[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(min_pos_vals), min_pos);\n    for (int j = 0; j < 8; ++j) {\n        if (min_pos_vals[j] > 0 && min_pos_vals[j] < smallest_positive) {\n            smallest_positive = min_pos_vals[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < lst.size(); ++i) {\n        if (lst[i] < 0 && lst[i] > largest_negative) {\n            largest_negative = lst[i];\n        }\n        if (lst[i] > 0 && lst[i] < smallest_positive) {\n            smallest_positive = lst[i];\n        }\n    }\n\n    return {largest_negative, smallest_positive};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    __m256i vec_ones = _mm256_set1_epi64x(1);\n    __m256i vec_result = _mm256_set1_epi64x(1);\n    __m256i vec_factorial = _mm256_set1_epi64x(1);\n    __m256i vec_i = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        __m256i vec_current = _mm256_set1_epi64x(i);\n        vec_factorial = _mm256_set1_epi64x(1);\n\n        for (uint64_t j = 1; j <= i; ++j) {\n            vec_factorial = _mm256_mullo_epi64(vec_factorial, _mm256_set1_epi64x(j));\n        }\n\n        vec_result = _mm256_mullo_epi64(vec_result, vec_factorial);\n    }\n\n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, vec_result);\n\n    return result[0] * result[1] * result[2] * result[3];\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    // Initialize result as 1 (using SIMD)\n    __m256i result = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        // Compute factorial(i) using scalar computation\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n\n        // Load the computed factorial into a SIMD register\n        __m256i fact_vec = _mm256_set1_epi64x(fact);\n\n        // Multiply the current result by the factorial\n        result = _mm256_mullo_epi64(result, fact_vec);\n    }\n\n    // Horizontal reduction to compute the final product\n    uint64_t final_result = 1;\n    for (int i = 0; i < 4; ++i) {\n        final_result *= ((uint64_t*)&result)[i];\n    }\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    // Initialize result to 1 (as a vector of 1s)\n    __m256i ones = _mm256_set1_epi64x(1);\n    __m256i result = ones;\n\n    // Compute factorials in chunks of 4 using AVX2\n    for (uint64_t i = 1; i <= n; ++i) {\n        __m256i factorial = ones;\n        uint64_t current = i;\n\n        // Compute the factorial of `current` iteratively\n        for (uint64_t j = 2; j <= current; ++j) {\n            __m256i multiplier = _mm256_set1_epi64x(j);\n            factorial = _mm256_mullo_epi64(factorial, multiplier);\n        }\n\n        // Multiply the result by the computed factorial\n        result = _mm256_mullo_epi64(result, factorial);\n    }\n\n    // Horizontal reduction to combine the 4 accumulated values\n    uint64_t final_result = 1;\n    alignas(32) uint64_t temp[4];\n    _mm256_store_si256((__m256i*)temp, result);\n\n    for (int i = 0; i < 4; ++i) {\n        final_result *= temp[i];\n    }\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    // Initialize result as 1 (using SIMD)\n    __m256i result = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        // Compute factorial(i) using scalar computation\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n\n        // Load the computed factorial into a SIMD register\n        __m256i fact_vec = _mm256_set1_epi64x(fact);\n\n        // Multiply the current result with the new factorial\n        result = _mm256_mullo_epi64(result, fact_vec);\n    }\n\n    // Horizontal reduction to compute the final product\n    uint64_t final_result = 1;\n    for (int i = 0; i < 4; ++i) {\n        final_result *= ((uint64_t*)&result)[i];\n    }\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0) return 0;\n\n    // Initialize result to 1 (as a vector)\n    __m256i result = _mm256_set1_epi64x(1);\n\n    for (uint64_t i = 1; i <= n; ++i) {\n        // Compute factorial(i) iteratively\n        uint64_t fact = 1;\n        for (uint64_t j = 1; j <= i; ++j) {\n            fact *= j;\n        }\n\n        // Load the computed factorial into a vector\n        __m256i fact_vec = _mm256_set1_epi64x(fact);\n\n        // Multiply the result vector by the current factorial vector\n        result = _mm256_mullo_epi64(result, fact_vec);\n    }\n\n    // Horizontal multiplication of the vector elements\n    uint64_t final_result = 1;\n    alignas(32) uint64_t temp[4];\n    _mm256_store_si256((__m256i*)temp, result);\n\n    for (int i = 0; i < 4; ++i) {\n        final_result *= temp[i];\n    }\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int sum = 0;\n    int n = lst.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Check if index is a multiple of 3\n        __m256i mod3 = _mm256_set1_epi32(3);\n        __m256i rem3 = _mm256_rem_epi32(indices, mod3);\n        __m256i mask3 = _mm256_cmpeq_epi32(rem3, _mm256_setzero_si256());\n\n        // Square the elements where index is a multiple of 3\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        vec = _mm256_blendv_epi8(vec, squared, mask3);\n\n        // Check if index is a multiple of 4 and not a multiple of 3\n        __m256i mod4 = _mm256_set1_epi32(4);\n        __m256i rem4 = _mm256_rem_epi32(indices, mod4);\n        __m256i mask4 = _mm256_andnot_si256(mask3, _mm256_cmpeq_epi32(rem4, _mm256_setzero_si256()));\n\n        // Cube the elements where index is a multiple of 4 and not a multiple of 3\n        __m256i cubed = _mm256_mullo_epi32(vec, _mm256_mullo_epi32(vec, vec));\n        vec = _mm256_blendv_epi8(vec, cubed, mask4);\n\n        // Accumulate the results horizontally\n        alignas(32) int temp[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp), vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int result = 0;\n    int n = lst.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= n - 8; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Check if index is a multiple of 3\n        __m256i mod3 = _mm256_set1_epi32(3);\n        __m256i mask3 = _mm256_cmpeq_epi32(_mm256_rem_epi32(indices, mod3), _mm256_setzero_si256());\n\n        // Check if index is a multiple of 4 and not a multiple of 3\n        __m256i mod4 = _mm256_set1_epi32(4);\n        __m256i mask4 = _mm256_andnot_si256(mask3, _mm256_cmpeq_epi32(_mm256_rem_epi32(indices, mod4), _mm256_setzero_si256()));\n\n        // Square the values where index is a multiple of 3\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        values = _mm256_blendv_epi8(values, squared, mask3);\n\n        // Cube the values where index is a multiple of 4 and not a multiple of 3\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(values, values), values);\n        values = _mm256_blendv_epi8(values, cubed, mask4);\n\n        // Accumulate results horizontally\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), values);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            result += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            result += lst[i] * lst[i] * lst[i];\n        } else {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int sum = 0;\n    int n = lst.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Check if index is multiple of 3\n        __m256i mod3 = _mm256_set1_epi32(3);\n        __m256i idx_mod3 = _mm256_rem_epi32(indices, mod3);\n        __m256i mask_mod3 = _mm256_cmpeq_epi32(idx_mod3, _mm256_setzero_si256());\n\n        // Square the values where index is multiple of 3\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        vec = _mm256_blendv_epi8(vec, squared, mask_mod3);\n\n        // Check if index is multiple of 4 and not multiple of 3\n        __m256i mod4 = _mm256_set1_epi32(4);\n        __m256i idx_mod4 = _mm256_rem_epi32(indices, mod4);\n        __m256i mask_mod4 = _mm256_cmpeq_epi32(idx_mod4, _mm256_setzero_si256());\n        __m256i mask_not_mod3 = _mm256_cmpgt_epi32(_mm256_setzero_si256(), idx_mod3);\n        __m256i mask_mod4_not_mod3 = _mm256_and_si256(mask_mod4, mask_not_mod3);\n\n        // Cube the values where index is multiple of 4 and not multiple of 3\n        __m256i cubed = _mm256_mullo_epi32(vec, vec);\n        cubed = _mm256_mullo_epi32(cubed, vec);\n        vec = _mm256_blendv_epi8(vec, cubed, mask_mod4_not_mod3);\n\n        // Accumulate the results horizontally\n        alignas(32) int temp[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp), vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int result = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i values = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Check if index is a multiple of 3\n        __m256i mod3 = _mm256_set1_epi32(3);\n        __m256i idx_mod3 = _mm256_rem_epi32(indices, mod3);\n        __m256i mask_mod3 = _mm256_cmpeq_epi32(idx_mod3, _mm256_setzero_si256());\n\n        // Square the values where index is a multiple of 3\n        __m256i squared = _mm256_mullo_epi32(values, values);\n        values = _mm256_blendv_epi8(values, squared, mask_mod3);\n\n        // Check if index is a multiple of 4 and not a multiple of 3\n        __m256i mod4 = _mm256_set1_epi32(4);\n        __m256i idx_mod4 = _mm256_rem_epi32(indices, mod4);\n        __m256i mask_mod4 = _mm256_cmpeq_epi32(idx_mod4, _mm256_setzero_si256());\n        __m256i mask_not_mod3 = _mm256_andnot_si256(mask_mod3, mask_mod4);\n\n        // Cube the values where index is a multiple of 4 and not a multiple of 3\n        __m256i cubed = _mm256_mullo_epi32(values, values);\n        cubed = _mm256_mullo_epi32(cubed, values);\n        values = _mm256_blendv_epi8(values, cubed, mask_not_mod3);\n\n        // Accumulate the results horizontally\n        int temp[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), values);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        if (i % 3 == 0) {\n            result += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            result += lst[i] * lst[i] * lst[i];\n        } else {\n            result += lst[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst) {\n    int sum = 0;\n    int n = lst.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= n - 8; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n\n        // Check if index is a multiple of 3\n        __m256i mod3 = _mm256_set1_epi32(3);\n        __m256i rem3 = _mm256_rem_epi32(indices, mod3);\n        __m256i mask3 = _mm256_cmpeq_epi32(rem3, _mm256_setzero_si256());\n\n        // Square the elements where index is a multiple of 3\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        vec = _mm256_blendv_epi8(vec, squared, mask3);\n\n        // Check if index is a multiple of 4 and not a multiple of 3\n        __m256i mod4 = _mm256_set1_epi32(4);\n        __m256i rem4 = _mm256_rem_epi32(indices, mod4);\n        __m256i mask4 = _mm256_andnot_si256(mask3, _mm256_cmpeq_epi32(rem4, _mm256_setzero_si256()));\n\n        // Cube the elements where index is a multiple of 4 and not a multiple of 3\n        __m256i cubed = _mm256_mullo_epi32(vec, vec);\n        cubed = _mm256_mullo_epi32(cubed, vec);\n        vec = _mm256_blendv_epi8(vec, cubed, mask4);\n\n        // Accumulate the results horizontally\n        alignas(32) int temp[8];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp), vec);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            sum += lst[i] * lst[i] * lst[i];\n        } else {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    size_t i = 0;\n    size_t size = nums.size();\n    __m256i v_ten = _mm256_set1_epi32(10);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_odd_mask = _mm256_set1_epi32(0x1); // Mask to check if a number is odd\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_nums = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i v_abs_nums = _mm256_abs_epi32(v_nums);\n\n        // Check if numbers are greater than 10\n        __m256i v_gt_ten = _mm256_cmpgt_epi32(v_abs_nums, v_ten);\n\n        // Extract first digit\n        __m256i v_first_digit = v_abs_nums;\n        for (int j = 0; j < 9; ++j) {\n            v_first_digit = _mm256_srli_epi32(v_first_digit, 1);\n        }\n\n        // Extract last digit\n        __m256i v_last_digit = _mm256_and_si256(v_abs_nums, _mm256_set1_epi32(0xF));\n\n        // Check if first and last digits are odd\n        __m256i v_first_odd = _mm256_and_si256(v_first_digit, v_odd_mask);\n        __m256i v_last_odd = _mm256_and_si256(v_last_digit, v_odd_mask);\n\n        // Combine conditions: greater than 10, first digit odd, last digit odd\n        __m256i v_valid = _mm256_and_si256(v_gt_ten, _mm256_and_si256(v_first_odd, v_last_odd));\n\n        // Count valid elements\n        int mask = _mm256_movemask_ps((__m256)v_valid);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = std::abs(nums[i]);\n        if (num > 10) {\n            int first_digit = num;\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            int last_digit = num % 10;\n            if ((first_digit & 1) && (last_digit & 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= size - 8; i += 8) {\n        // Load 8 integers into an AVX register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        // Compare each element with 10 to check if it's greater than 10\n        __m256i cmp_gt_10 = _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(10));\n\n        // Extract absolute values to handle negative numbers\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n\n        // Get the last digit by modulo 10\n        __m256i last_digit = _mm256_rem_epu32(abs_vals, _mm256_set1_epi32(10));\n\n        // Get the first digit by repeatedly dividing by 10\n        __m256i first_digit = abs_vals;\n        for (int j = 0; j < 9; ++j) { // Max 10 digits in a 32-bit integer\n            __m256i div_by_10 = _mm256_srli_epi32(first_digit, 1); // Approximate division by 10\n            first_digit = _mm256_div_epu32(first_digit, _mm256_set1_epi32(10));\n            __m256i mask = _mm256_cmpgt_epi32(div_by_10, _mm256_setzero_si256());\n            first_digit = _mm256_blendv_epi8(first_digit, div_by_10, mask);\n        }\n\n        // Check if both first and last digits are odd\n        __m256i first_odd = _mm256_and_si256(first_digit, _mm256_set1_epi32(1));\n        __m256i last_odd = _mm256_and_si256(last_digit, _mm256_set1_epi32(1));\n        __m256i both_odd = _mm256_and_si256(first_odd, last_odd);\n\n        // Combine conditions: greater than 10 AND both digits odd\n        __m256i valid = _mm256_and_si256(cmp_gt_10, both_odd);\n\n        // Count the number of valid elements\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(valid));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            int first_digit = abs_num;\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            if ((first_digit % 2 == 1) && (last_digit % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= size - 8; i += 8) {\n        // Load 8 integers into a SIMD register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n\n        // Compare each element with 10, create mask for > 10\n        __m256i cmp_gt_10 = _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(10));\n\n        // Extract absolute values to handle negatives\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n\n        // Compute last digit using modulo 10\n        __m256i last_digit = _mm256_rem_epu32(abs_vals, _mm256_set1_epi32(10));\n\n        // Compute first digit by repeatedly dividing by 10\n        __m256i first_digit = abs_vals;\n        for (int j = 0; j < 9; ++j) { // Max 10 digits in a 32-bit integer\n            first_digit = _mm256_srli_epi32(first_digit, 1); // Divide by 10 approximation\n        }\n\n        // Check if first and last digits are odd\n        __m256i first_odd = _mm256_and_si256(first_digit, _mm256_set1_epi32(1));\n        __m256i last_odd = _mm256_and_si256(last_digit, _mm256_set1_epi32(1));\n\n        // Combine conditions: > 10, first digit odd, last digit odd\n        __m256i cond = _mm256_and_si256(cmp_gt_10, _mm256_and_si256(first_odd, last_odd));\n\n        // Create a mask to count the number of true conditions\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cond));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            int first_digit = abs_num;\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            if ((first_digit % 2 == 1) && (last_digit % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= size - 8; i += 8) {\n        // Load 8 integers into a SIMD register\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n\n        // Compare each element with 10 to check if it's greater than 10\n        __m256i cmp_gt_10 = _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(10));\n\n        // Extract absolute values to handle negative numbers\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n\n        // Get the last digit by modulo 10\n        __m256i last_digit = _mm256_rem_epu32(abs_vals, _mm256_set1_epi32(10));\n\n        // Get the first digit\n        __m256i first_digit = abs_vals;\n        for (int j = 0; j < 9; ++j) { // Iteratively divide by 10 to get the first digit\n            first_digit = _mm256_srli_epi32(first_digit, 1);\n        }\n\n        // Check if both first and last digits are odd\n        __m256i is_last_odd = _mm256_and_si256(last_digit, _mm256_set1_epi32(1));\n        __m256i is_first_odd = _mm256_and_si256(first_digit, _mm256_set1_epi32(1));\n        __m256i both_odd = _mm256_and_si256(is_last_odd, is_first_odd);\n\n        // Combine conditions: greater than 10 AND both digits odd\n        __m256i mask = _mm256_and_si256(cmp_gt_10, both_odd);\n\n        // Count the number of true values in the mask\n        int mask_bits = _mm256_movemask_ps((__m256)mask);\n        count += _mm_popcnt_u32(mask_bits);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            while (abs_num >= 10) {\n                abs_num /= 10;\n            }\n            int first_digit = abs_num;\n            if ((first_digit % 2 == 1) && (last_digit % 2 == 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint specialFilter_simd(const std::vector<int> & nums) {\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    // Process 8 elements at a time using AVX2\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i ten = _mm256_set1_epi32(10);\n        \n        // Check if numbers are greater than 10\n        __m256i greater_than_ten = _mm256_cmpgt_epi32(vec, ten);\n\n        // Extract absolute values to handle negatives\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n\n        // Extract last digit\n        __m256i last_digit = _mm256_and_si256(abs_vals, _mm256_set1_epi32(0xF));\n\n        // Compute first digit\n        __m256i temp = abs_vals;\n        for (int j = 0; j < 9; ++j) { // Reduce to first digit\n            temp = _mm256_srli_epi32(temp, 1);\n        }\n        __m256i first_digit = _mm256_and_si256(temp, _mm256_set1_epi32(0xF));\n\n        // Check if both first and last digits are odd\n        __m256i odd_mask = _mm256_set1_epi32(1);\n        __m256i first_odd = _mm256_and_si256(first_digit, odd_mask);\n        __m256i last_odd = _mm256_and_si256(last_digit, odd_mask);\n        __m256i both_odd = _mm256_and_si256(first_odd, last_odd);\n\n        // Combine conditions: greater than 10 AND both digits odd\n        __m256i valid = _mm256_and_si256(greater_than_ten, both_odd);\n\n        // Count the number of valid elements\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(valid));\n        count += __builtin_popcount(mask);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            while (abs_num >= 10) {\n                abs_num /= 10;\n            }\n            int first_digit = abs_num;\n            if ((first_digit % 2 != 0) && (last_digit % 2 != 0)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i mod3 = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i twos = _mm256_set1_epi32(2);\n\n    // Allocate aligned memory for the array a\n    uint64_t *a = (uint64_t*)_mm_malloc(n * sizeof(uint64_t), 32);\n    int *mod_counts = (int*)_mm_malloc(3 * sizeof(int), 32);\n    mod_counts[0] = mod_counts[1] = mod_counts[2] = 0;\n\n    // Compute a[i] = i * i - i + 1 and store modulo 3 values\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i - 1] = i * i - i + 1;\n        int mod_val = a[i - 1] % 3;\n        mod_counts[mod_val]++;\n    }\n\n    // Process in chunks of 8 using AVX2\n    for (uint64_t i = 0; i < n; i += 8) {\n        __m256i indices = _mm256_set_epi32(i + 7, i + 6, i + 5, i + 4, i + 3, i + 2, i + 1, i);\n        __m256i values = _mm256_i64gather_epi64((const long long*)a, indices, 8);\n        __m256i mods = _mm256_rem_epi32(values, mod3);\n\n        // Count occurrences of each modulo value\n        __m256i cmp0 = _mm256_cmpeq_epi32(mods, zero);\n        __m256i cmp1 = _mm256_cmpeq_epi32(mods, ones);\n        __m256i cmp2 = _mm256_cmpeq_epi32(mods, twos);\n\n        int mask0 = _mm256_movemask_ps((__m256)cmp0);\n        int mask1 = _mm256_movemask_ps((__m256)cmp1);\n        int mask2 = _mm256_movemask_ps((__m256)cmp2);\n\n        mod_counts[0] += _mm_popcnt_u32(mask0);\n        mod_counts[1] += _mm_popcnt_u32(mask1);\n        mod_counts[2] += _mm_popcnt_u32(mask2);\n    }\n\n    // Calculate the number of valid triples\n    count += (uint64_t)mod_counts[0] * (mod_counts[0] - 1) * (mod_counts[0] - 2) / 6;\n    count += (uint64_t)mod_counts[1] * (mod_counts[1] - 1) * (mod_counts[1] - 2) / 6;\n    count += (uint64_t)mod_counts[2] * (mod_counts[2] - 1) * (mod_counts[2] - 2) / 6;\n    count += (uint64_t)mod_counts[0] * mod_counts[1] * mod_counts[2];\n\n    // Free allocated memory\n    _mm_free(a);\n    _mm_free(mod_counts);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i v_mod3 = _mm256_set1_epi32(3);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    // Allocate aligned memory for the array\n    uint64_t *a = (uint64_t*)_mm_malloc(n * sizeof(uint64_t), 32);\n    int *mod_counts = (int*)_mm_malloc(3 * sizeof(int), 32);\n    mod_counts[0] = mod_counts[1] = mod_counts[2] = 0;\n\n    // Compute a[i] = i * i - i + 1 and store mod 3 values\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i - 1] = i * i - i + 1;\n        mod_counts[a[i - 1] % 3]++;\n    }\n\n    // Process in chunks of 8 using AVX2\n    for (uint64_t i = 0; i < n; i += 8) {\n        __m256i v_a = _mm256_loadu_si256((__m256i*)&a[i]);\n        __m256i v_mod = _mm256_rem_epu32(v_a, v_mod3);\n\n        for (uint64_t j = i + 8; j < n; j += 8) {\n            __m256i v_b = _mm256_loadu_si256((__m256i*)&a[j]);\n            __m256i v_sum_ab = _mm256_add_epi32(v_a, v_b);\n\n            for (uint64_t k = j + 8; k < n; k += 8) {\n                __m256i v_c = _mm256_loadu_si256((__m256i*)&a[k]);\n                __m256i v_sum_abc = _mm256_add_epi32(v_sum_ab, v_c);\n                __m256i v_mod_abc = _mm256_rem_epu32(v_sum_abc, v_mod3);\n\n                __m256i v_cmp = _mm256_cmpeq_epi32(v_mod_abc, v_zero);\n                int mask = _mm256_movemask_epi8(v_cmp);\n                count += _mm_popcnt_u32(mask) / 4; // Count valid triples\n            }\n        }\n    }\n\n    // Handle remaining elements if n is not a multiple of 8\n    for (uint64_t i = (n / 8) * 8; i < n; ++i) {\n        for (uint64_t j = i + 1; j < n; ++j) {\n            for (uint64_t k = j + 1; k < n; ++k) {\n                uint64_t sum = a[i] + a[j] + a[k];\n                if (sum % 3 == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    _mm_free(a);\n    _mm_free(mod_counts);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i mod3 = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i twos = _mm256_set1_epi32(2);\n\n    // Allocate space for the array a and compute values using SIMD\n    uint64_t simd_size = (n + 7) / 8 * 8; // Round up to multiple of 8\n    int* a = (int*)_mm_malloc(simd_size * sizeof(int), 32);\n    for (uint64_t i = 0; i < simd_size; ++i) {\n        a[i] = (i < n) ? (int)(i * i - i + 1) : 0;\n    }\n\n    // Compute modulo 3 for all elements in parallel\n    for (uint64_t i = 0; i < simd_size; i += 8) {\n        __m256i values = _mm256_load_si256((__m256i*)&a[i]);\n        __m256i mod_values = _mm256_rem_epi32(values, mod3); // Modulo operation\n        _mm256_store_si256((__m256i*)&a[i], mod_values);\n    }\n\n    // Count triples where sum is divisible by 3\n    for (uint64_t i = 0; i < n; ++i) {\n        for (uint64_t j = i + 1; j < n; ++j) {\n            for (uint64_t k = j + 1; k < n; k += 8) {\n                __m256i ai = _mm256_set1_epi32(a[i]);\n                __m256i aj = _mm256_set1_epi32(a[j]);\n                __m256i ak = _mm256_loadu_si256((__m256i*)&a[k]);\n\n                __m256i sum = _mm256_add_epi32(ai, aj);\n                sum = _mm256_add_epi32(sum, ak);\n\n                __m256i mask = _mm256_cmpeq_epi32(_mm256_rem_epi32(sum, mod3), zero);\n                int valid_count = _mm256_movemask_epi8(mask);\n                count += __builtin_popcount(valid_count) / 4; // Each int contributes 4 bytes\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i mod3 = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i twos = _mm256_set1_epi32(2);\n\n    // Allocate aligned memory for the array\n    uint64_t *a = (uint64_t*)_mm_malloc(n * sizeof(uint64_t), 32);\n    int *mod_counts = (int*)_mm_malloc(3 * sizeof(int), 32);\n    mod_counts[0] = mod_counts[1] = mod_counts[2] = 0;\n\n    // Compute a[i] = i * i - i + 1 and store modulo 3 values\n    for (uint64_t i = 1; i <= n; ++i) {\n        a[i - 1] = i * i - i + 1;\n        int mod_val = a[i - 1] % 3;\n        mod_counts[mod_val]++;\n    }\n\n    // Process in chunks of 8 using AVX2\n    uint64_t num_chunks = n / 8;\n    for (uint64_t chunk = 0; chunk < num_chunks; ++chunk) {\n        __m256i indices = _mm256_set_epi32(\n            chunk * 8 + 7, chunk * 8 + 6, chunk * 8 + 5, chunk * 8 + 4,\n            chunk * 8 + 3, chunk * 8 + 2, chunk * 8 + 1, chunk * 8\n        );\n        __m256i values = _mm256_i64gather_epi64((const long long*)a, indices, 8);\n        __m256i mods = _mm256_rem_epi32(values, mod3);\n\n        // Count occurrences of each modulo value\n        __m256i cmp0 = _mm256_cmpeq_epi32(mods, zero);\n        __m256i cmp1 = _mm256_cmpeq_epi32(mods, ones);\n        __m256i cmp2 = _mm256_cmpeq_epi32(mods, twos);\n\n        int mask0 = _mm256_movemask_ps((__m256)cmp0);\n        int mask1 = _mm256_movemask_ps((__m256)cmp1);\n        int mask2 = _mm256_movemask_ps((__m256)cmp2);\n\n        mod_counts[0] += __builtin_popcount(mask0);\n        mod_counts[1] += __builtin_popcount(mask1);\n        mod_counts[2] += __builtin_popcount(mask2);\n    }\n\n    // Handle remaining elements\n    for (uint64_t i = num_chunks * 8; i < n; ++i) {\n        int mod_val = a[i] % 3;\n        mod_counts[mod_val]++;\n    }\n\n    // Calculate the number of valid triples\n    count += (uint64_t)mod_counts[0] * (mod_counts[0] - 1) * (mod_counts[0] - 2) / 6;\n    count += (uint64_t)mod_counts[1] * (mod_counts[1] - 1) * (mod_counts[1] - 2) / 6;\n    count += (uint64_t)mod_counts[2] * (mod_counts[2] - 1) * (mod_counts[2] - 2) / 6;\n    count += (uint64_t)mod_counts[0] * mod_counts[1] * mod_counts[2];\n\n    _mm_free(a);\n    _mm_free(mod_counts);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n) {\n    uint64_t count = 0;\n    __m256i v_mod3 = _mm256_set1_epi32(3);\n    __m256i v_zero = _mm256_setzero_si256();\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    // Allocate aligned memory for the array a\n    int *a = (int*)_mm_malloc(n * sizeof(int), 32);\n    if (!a) return 0;\n\n    // Compute a[i] = i * i - i + 1 for all i\n    for (uint64_t i = 0; i < n; ++i) {\n        a[i] = i * i - i + 1;\n    }\n\n    // Count the number of valid triples\n    for (uint64_t i = 0; i < n - 2; ++i) {\n        for (uint64_t j = i + 1; j < n - 1; j += 8) {\n            __m256i v_ai = _mm256_set1_epi32(a[i]);\n            __m256i v_aj = _mm256_loadu_si256((__m256i*)&a[j]);\n\n            for (uint64_t k = j + 1; k < n; k += 8) {\n                __m256i v_ak = _mm256_loadu_si256((__m256i*)&a[k]);\n\n                // Compute (a[i] + a[j] + a[k]) % 3\n                __m256i v_sum = _mm256_add_epi32(v_ai, _mm256_add_epi32(v_aj, v_ak));\n                __m256i v_remainder = _mm256_rem_epi32(v_sum, v_mod3);\n\n                // Check if remainder is zero\n                __m256i v_mask = _mm256_cmpeq_epi32(v_remainder, v_zero);\n                int mask = _mm256_movemask_epi8(v_mask);\n\n                // Count the number of valid triples in this iteration\n                count += __builtin_popcount(mask) / 4;\n            }\n        }\n    }\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_sqrt_n = _mm256_set1_epi32((int)sqrtf((float)n));\n    __m256i vec_i = _mm256_set1_epi32(2);\n\n    int is_prime = 1;\n\n    for (int i = 2; i <= sqrtf((float)n); i += 8) {\n        __m256i vec_divisor = _mm256_add_epi32(vec_i, _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i vec_mod = _mm256_setzero_si256();\n\n        // Compute n % i for all 8 values of i in parallel\n        for (int j = 0; j < 8; ++j) {\n            int divisor = i + j;\n            if (divisor > sqrtf((float)n)) break;\n            int mod = n % divisor;\n            vec_mod = _mm256_insert_epi32(vec_mod, mod, j);\n        }\n\n        // Check if any modulus result is zero\n        __m256i cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            is_prime = 0;\n            break;\n        }\n\n        vec_i = _mm256_add_epi32(vec_i, _mm256_set1_epi32(8));\n    }\n\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    // Broadcast n into a vector for parallel comparison\n    __m256i vn = _mm256_set1_epi32(n);\n\n    // Prepare constants for prime checking (2, 3, 5, 7, 11, 13, 17, 19)\n    __m256i primes = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n\n    // Compute modulus of n with each prime in parallel\n    __m256i mod_results = _mm256_setzero_si256();\n    for (int i = 0; i < 8; ++i) {\n        int prime = _mm256_extract_epi32(primes, i);\n        if (prime > n) break;\n        int mod = n % prime;\n        mod_results = _mm256_insert_epi32(mod_results, mod, i);\n    }\n\n    // Check if any modulus result is zero (not prime)\n    __m256i zero = _mm256_setzero_si256();\n    __m256i cmp = _mm256_cmpeq_epi32(mod_results, zero);\n    int mask = _mm256_movemask_epi8(cmp);\n\n    // If any modulus result is zero, n is not prime\n    if (mask != 0) {\n        return y;\n    }\n\n    // Handle edge cases for n <= 1\n    if (n <= 1) return y;\n\n    // Otherwise, n is prime\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    // Load constants into AVX registers\n    __m256i vn = _mm256_set1_epi32(n);\n    __m256i vx = _mm256_set1_epi32(x);\n    __m256i vy = _mm256_set1_epi32(y);\n    __m256i vprime = _mm256_set1_epi32(1); // Assume prime initially\n\n    // Check divisors from 2 to sqrt(n)\n    for (int i = 2; i * i <= n; ++i) {\n        __m256i vi = _mm256_set1_epi32(i);\n        __m256i mod = _mm256_set1_epi32(n % i);\n\n        // If n % i == 0, n is not prime\n        __m256i cmp = _mm256_cmpeq_epi32(mod, _mm256_setzero_si256());\n        vprime = _mm256_andnot_si256(cmp, vprime);\n    }\n\n    // Check if any lane still considers n as prime\n    int mask = _mm256_movemask_epi8(vprime);\n    if (mask != 0) {\n        return x;\n    } else {\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    // Broadcast n into a vector for parallel comparison\n    __m256i vn = _mm256_set1_epi32(n);\n\n    // Prepare vectors for divisors (2 to 9) to test primality\n    __m256i divisors = _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9);\n\n    // Perform modulo operation in parallel\n    __m256i mod_results = _mm256_rem_epi32(vn, divisors);\n\n    // Compare if any modulo result is zero (non-prime indicator)\n    __m256i zero = _mm256_setzero_si256();\n    __m256i is_zero = _mm256_cmpeq_epi32(mod_results, zero);\n\n    // Check if any lane has a zero (non-prime)\n    int mask = _mm256_movemask_epi8(is_zero);\n    int is_non_prime = (mask != 0);\n\n    // Select x or y based on primality\n    return is_non_prime ? y : x;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y) {\n    if (n <= 1) return y;\n\n    // Broadcast n into a vector for parallel comparison\n    __m256i vn = _mm256_set1_epi32(n);\n    __m256i vprime = _mm256_set1_epi32(1); // Assume prime initially\n    __m256i vnot_prime = _mm256_set1_epi32(0);\n\n    // Check divisors from 2 to sqrt(n)\n    for (int i = 2; i * i <= n; ++i) {\n        __m256i vi = _mm256_set1_epi32(i);\n        __m256i vmod = _mm256_set1_epi32(n % i);\n        __m256i vis_zero = _mm256_cmpeq_epi32(vmod, _mm256_setzero_si256());\n\n        // If any modulus is zero, n is not prime\n        vprime = _mm256_andnot_si256(vis_zero, vprime);\n    }\n\n    // Reduce the result to check if n is prime\n    int mask = _mm256_movemask_epi8(vprime);\n    int is_prime = (mask == 0xFFFFFFFF) ? 1 : 0;\n\n    // Select x or y based on primality\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats into an AVX register\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Create a mask for positive numbers (vec > 0)\n        __m256 zero = _mm256_setzero_ps();\n        __m256 positive_mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        // Convert floats to integers (truncate towards zero)\n        __m256i int_vec = _mm256_cvttps_epi32(vec);\n\n        // Check if the integers are odd\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i odd_mask = _mm256_and_si256(int_vec, one);\n\n        // Combine masks: positive and odd\n        __m256i combined_mask = _mm256_and_si256(_mm256_castps_si256(positive_mask), odd_mask);\n\n        // Square the original floats\n        __m256 squared = _mm256_mul_ps(vec, vec);\n\n        // Blend squared values based on the combined mask\n        __m256 masked_squared = _mm256_blendv_ps(_mm256_setzero_ps(), squared, _mm256_castsi256_ps(combined_mask));\n\n        // Convert the squared floats to doubles for horizontal sum\n        __m256d low = _mm256_cvtps_pd(_mm256_extractf128_ps(masked_squared, 0));\n        __m256d high = _mm256_cvtps_pd(_mm256_extractf128_ps(masked_squared, 1));\n\n        // Horizontal sum of low and high parts\n        __m128d sum_low = _mm_add_pd(_mm256_castpd256_pd128(low), _mm256_extractf128_pd(low, 1));\n        __m128d sum_high = _mm_add_pd(_mm256_castpd256_pd128(high), _mm256_extractf128_pd(high, 1));\n\n        // Add the two sums together\n        __m128d total_sum = _mm_add_pd(sum_low, sum_high);\n\n        // Extract the final sum as a scalar\n        result += _mm_cvtsd_f64(total_sum);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        int intval = static_cast<int>(val);\n        if (val > 0 && intval % 2 != 0) {\n            result += val * val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats into an AVX register\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Create a mask for positive numbers (greater than 0)\n        __m256 zero = _mm256_setzero_ps();\n        __m256 positive_mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        // Convert floats to integers (truncate towards zero)\n        __m256i int_vec = _mm256_cvttps_epi32(vec);\n\n        // Check if the integers are odd\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i odd_mask = _mm256_and_si256(int_vec, one);\n\n        // Combine masks: positive and odd\n        __m256i combined_mask = _mm256_and_si256(_mm256_castps_si256(positive_mask), odd_mask);\n\n        // Square the original float values\n        __m256 squared = _mm256_mul_ps(vec, vec);\n\n        // Blend squared values based on the combined mask\n        __m256 masked_squared = _mm256_blendv_ps(_mm256_setzero_ps(), squared, _mm256_castsi256_ps(combined_mask));\n\n        // Extract the squared values and accumulate\n        alignas(32) float temp[8];\n        _mm256_store_ps(temp, masked_squared);\n        for (int j = 0; j < 8; ++j) {\n            result += static_cast<uint64_t>(temp[j]);\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        if (val > 0 && static_cast<int>(val) % 2 != 0) {\n            result += static_cast<uint64_t>(val * val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats into an AVX register\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Create a mask for positive numbers (vec > 0)\n        __m256 zero = _mm256_setzero_ps();\n        __m256 pos_mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        // Convert floats to integers (truncate towards zero)\n        __m256i int_vec = _mm256_cvttps_epi32(vec);\n\n        // Check if the integers are odd\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i odd_mask = _mm256_and_si256(int_vec, one);\n\n        // Combine masks: positive and odd\n        __m256 combined_mask = _mm256_and_ps(pos_mask, _mm256_castsi256_ps(_mm256_cmpeq_epi32(odd_mask, one)));\n\n        // Square the original values\n        __m256 squared = _mm256_mul_ps(vec, vec);\n\n        // Apply the combined mask to filter out unwanted values\n        __m256 masked_squared = _mm256_and_ps(combined_mask, squared);\n\n        // Convert the valid squared values to integers\n        __m256i squared_int = _mm256_cvtps_epi32(masked_squared);\n\n        // Extract the 8 integers from the AVX register and sum them\n        alignas(32) int temp[8];\n        _mm256_store_si256((__m256i*)temp, squared_int);\n        for (int j = 0; j < 8; ++j) {\n            result += temp[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        if (val > 0 && static_cast<int>(val) % 2 != 0) {\n            result += static_cast<int>(val) * static_cast<int>(val);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats into an AVX register\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Create a mask for positive numbers (greater than or equal to 0)\n        __m256 zero = _mm256_setzero_ps();\n        __m256 pos_mask = _mm256_cmp_ps(vec, zero, _CMP_GE_OQ);\n\n        // Truncate floats to integers and back to floats to check if they are whole numbers\n        __m256 truncated = _mm256_round_ps(vec, _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC);\n        __m256 is_integer_mask = _mm256_cmp_ps(vec, truncated, _CMP_EQ_OQ);\n\n        // Combine masks: positive and integer\n        __m256 valid_mask = _mm256_and_ps(pos_mask, is_integer_mask);\n\n        // Convert valid floats to integers\n        __m256i int_vec = _mm256_cvtps_epi32(vec);\n\n        // Check for odd integers\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i odd_mask = _mm256_and_si256(int_vec, one);\n        __m256i is_odd_mask = _mm256_cmpeq_epi32(odd_mask, one);\n\n        // Convert the odd mask back to float for blending\n        __m256 blend_mask = _mm256_castsi256_ps(is_odd_mask);\n\n        // Blend valid odd integers with zero\n        __m256 odd_values = _mm256_blendv_ps(_mm256_setzero_ps(), vec, _mm256_and_ps(valid_mask, blend_mask));\n\n        // Square the odd values\n        __m256 squared = _mm256_mul_ps(odd_values, odd_values);\n\n        // Sum the squared values horizontally\n        __m128 low = _mm256_extractf128_ps(squared, 0);\n        __m128 high = _mm256_extractf128_ps(squared, 1);\n        __m128 sum = _mm_add_ps(low, high);\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n\n        // Extract the scalar result and add to the total\n        result += _mm_cvtss_f32(sum);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        int intval = static_cast<int>(val);\n        if (val >= 0 && val == intval && (intval & 1)) {\n            result += intval * intval;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    uint64_t result = 0;\n    size_t i = 0;\n    size_t size = lst.size();\n\n    // Process 8 elements at a time using AVX\n    for (; i + 8 <= size; i += 8) {\n        // Load 8 floats into an AVX register\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n\n        // Create a mask for positive numbers (vec > 0)\n        __m256 zero = _mm256_setzero_ps();\n        __m256 positive_mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n\n        // Convert floats to integers (truncate towards zero)\n        __m256i int_vec = _mm256_cvttps_epi32(vec);\n\n        // Check if the integers are odd\n        __m256i one = _mm256_set1_epi32(1);\n        __m256i odd_mask = _mm256_and_si256(int_vec, one);\n\n        // Combine masks: positive and odd\n        __m256i combined_mask = _mm256_and_si256(_mm256_castps_si256(positive_mask), odd_mask);\n\n        // Square the original floats\n        __m256 squared = _mm256_mul_ps(vec, vec);\n\n        // Blend squared values based on the combined mask\n        __m256 masked_squared = _mm256_blendv_ps(_mm256_setzero_ps(), squared, _mm256_castsi256_ps(combined_mask));\n\n        // Horizontal sum of the squared values\n        __m128 high = _mm256_extractf128_ps(masked_squared, 1);\n        __m128 low = _mm256_castps256_ps128(masked_squared);\n        __m128 sum = _mm_add_ps(low, high);\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n\n        // Extract the result and add to the total\n        result += _mm_cvtss_f32(sum);\n    }\n\n    // Process remaining elements\n    for (; i < size; ++i) {\n        float val = lst[i];\n        if (val > 0 && static_cast<int>(val) % 2 != 0) {\n            result += val * val;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n, 0);\n\n    size_t i = 0;\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < n; i += 8) {\n        // Load 8 integers from game and guess into AVX registers\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        // Compute the difference between game and guess\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n\n        // Compute the absolute value of the difference\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), diff);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        // Store the result back to the result vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Process remaining elements (if any)\n    for (; i < n; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t n = game.size();\n    std::vector<int> result(n, 0);\n    \n    size_t i = 0;\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < n; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        // Compute the difference between game and guess\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n\n        // Compute the absolute value of the difference\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), diff);\n        __m256i abs_diff = _mm256_blendv_epi8(diff, _mm256_sub_epi32(_mm256_setzero_si256(), diff), mask);\n\n        // Store the result back to the result vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Process remaining elements\n    for (; i < n; ++i) {\n        result[i] = (game[i] == guess[i]) ? 0 : std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n\n    size_t i = 0;\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from game and guess into AVX registers\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        // Compute the difference between game and guess\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n\n        // Compute the absolute value of the difference\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), diff);\n        __m256i abs_diff = _mm256_blendv_epi8(diff, _mm256_sub_epi32(_mm256_setzero_si256(), diff), mask);\n\n        // Store the result back to the result vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Process remaining elements (if any)\n    for (; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n\n    size_t i = 0;\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from game and guess into AVX registers\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        // Compute the difference between game and guess\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n\n        // Compute the absolute value of the difference\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), diff);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        // Store the result back into the result vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Process remaining elements (if any)\n    for (; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n\n    size_t i = 0;\n    // Process 8 elements at a time using AVX2\n    for (; i + 7 < size; i += 8) {\n        // Load 8 integers from game and guess into AVX registers\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        // Compute the difference between game and guess\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n\n        // Compute the absolute value of the difference\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), diff);\n        __m256i abs_diff = _mm256_blendv_epi8(diff, _mm256_sub_epi32(_mm256_setzero_si256(), diff), mask);\n\n        // Store the result back to the result vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Process remaining elements (if any)\n    for (; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}

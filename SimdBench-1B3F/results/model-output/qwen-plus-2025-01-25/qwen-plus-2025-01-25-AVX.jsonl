{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    const size_t vec_len = 4; // Number of 64-bit integers in a YMM register\n    const size_t vec_bytes = vec_len * sizeof(int64_t);\n\n    size_t i = 0;\n\n    // Handle vectors\n    for (; i + vec_len <= length; i += vec_len) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const *)(dst + i));\n\n        // Load mask and convert to 8-bit integers\n        __m256i mask_vec = _mm256_loadu_si256((__m256i const *)(mask + i));\n        __m256i cmp_mask = _mm256_cmpeq_epi8(mask_vec, _mm256_set1_epi8(1));\n\n        // Blend based on the mask\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, cmp_mask);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle leftovers\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    const size_t vec_len = 4; // Number of 64-bit integers in a YMM register\n    const size_t vec_bytes = vec_len * sizeof(int64_t);\n\n    size_t i = 0;\n\n    // Handle vectors\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const *)(dst + i));\n\n        // Load mask and convert to 64-bit mask\n        __m256i mask_vec = _mm256_castps_si256(_mm256_loadu_ps((float const *)(mask + i)));\n        mask_vec = _mm256_andnot_si256(mask_vec, _mm256_set1_epi64x(-1));\n\n        __m256i cond_vec = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)(dst + i), cond_vec);\n    }\n\n    // Handle leftovers\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    const size_t vec_len = 4; // Number of 64-bit integers in a YMM register\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i *)(dst + i));\n\n        __m256i mask_vec = _mm256_set_epi64x(\n            mask[i + 3] ? -1 : 0,\n            mask[i + 2] ? -1 : 0,\n            mask[i + 1] ? -1 : 0,\n            mask[i + 0] ? -1 : 0\n        );\n\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    const size_t vec_size = 4; // Number of 64-bit integers in a YMM register\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= length - vec_size; i += vec_size) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const *)(dst + i));\n\n        // Load mask values and convert to appropriate format\n        __m256i mask_vec = _mm256_set_epi64x(\n            mask[i + 3] ? -1 : 0,\n            mask[i + 2] ? -1 : 0,\n            mask[i + 1] ? -1 : 0,\n            mask[i + 0] ? -1 : 0\n        );\n\n        // Perform blend operation\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    const size_t vec_size = 4; // Number of 64-bit integers in a YMM register\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= length - vec_size; i += vec_size) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i dst_vec = _mm256_loadu_si256((__m256i const *)(dst + i));\n\n        // Load mask and convert to appropriate format\n        __m256i mask_vec = _mm256_set_epi64x(\n            mask[i + 3] ? -1 : 0,\n            mask[i + 2] ? -1 : 0,\n            mask[i + 1] ? -1 : 0,\n            mask[i + 0] ? -1 : 0\n        );\n\n        // Perform conditional move\n        __m256i result = _mm256_blendv_epi8(dst_vec, src_vec, mask_vec);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i v_scale = _mm256_set1_epi32(scale);\n    const __m256i v_seven = _mm256_set1_epi32(7);\n    const __m256i v_shift_right = _mm256_set1_epi32(8 >> 3);\n    const __m256i v_shift_left = _mm256_set1_epi32(8 << 3);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_mul = _mm256_mullo_epi32(v_src, v_scale);\n\n        __m256i v_tmp1 = _mm256_srai_epi32(v_mul, 3);\n        __m256i v_tmp2 = _mm256_slli_epi32(v_mul, 3);\n\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_sum = _mm256_add_epi32(v_mul, v_product);\n\n        __m256i v_result = _mm256_div_epi32(v_sum, v_seven); // Assuming a custom div function\n        _mm256_storeu_si256((__m256i*)&dst[i], v_result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i v_scale = _mm256_set1_epi32(scale);\n    const __m256i v_shift_right = _mm256_set1_epi32(3);\n    const __m256i v_shift_left = _mm256_set1_epi32(-3);\n    const __m256i v_divisor = _mm256_set1_epi32(7);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i v_mul = _mm256_mullo_epi32(v_src, v_scale);\n\n        __m256i v_tmp1 = _mm256_sra_epi32(v_mul, v_shift_right);\n        __m256i v_tmp2 = _mm256_sll_epi32(v_mul, v_shift_left);\n\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_sum = _mm256_add_epi32(v_mul, v_product);\n\n        __m256i v_result = _mm256_div_epi32(v_sum, v_divisor); // Assuming a custom div_epi32 function\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i v_scale = _mm256_set1_epi32(scale);\n    const __m256i v_seven = _mm256_set1_epi32(7);\n    const __m256i v_shift_right = _mm256_set1_epi32(8); // 3 bits right shift is div by 8\n    const __m256i v_shift_left = _mm256_set1_epi32(8);  // 3 bits left shift is mul by 8\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const *)(src + i));\n        \n        __m256i v_mul = _mm256_mullo_epi32(v_src, v_scale);\n        \n        __m256i v_tmp1 = _mm256_srai_epi32(v_mul, 3);\n        __m256i v_tmp2 = _mm256_slli_epi32(v_mul, 3);\n        \n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_sum = _mm256_add_epi32(v_mul, v_product);\n        \n        __m256i v_result = _mm256_div_epi32(v_sum, v_seven); // Assuming a custom AVX2 div function\n        \n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i v_scale = _mm256_set1_epi32(scale);\n    const __m256i v_seven = _mm256_set1_epi32(7);\n    const __m256i v_shift_right = _mm256_set1_epi32(8); // 2^3\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i v_mul = _mm256_mullo_epi32(v_src, v_scale);\n\n        __m256i v_tmp1 = _mm256_sra_epi32(v_mul, v_shift_right);\n        __m256i v_tmp2 = _mm256_sll_epi32(v_mul, _mm256_set1_epi32(3));\n\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_sum = _mm256_add_epi32(v_mul, v_product);\n\n        __m256i v_result = _mm256_div_epi32(v_sum, v_seven); // Assuming a custom div function\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m256i v_scale = _mm256_set1_epi32(scale);\n    const __m256i v_seven = _mm256_set1_epi32(7);\n    const __m256i v_shift_3 = _mm256_set1_epi32(3);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((const __m256i *)(src + i));\n        __m256i v_mul = _mm256_mullo_epi32(v_src, v_scale);\n\n        __m256i v_tmp1 = _mm256_sra_epi32(v_mul, v_shift_3);\n        __m256i v_tmp2 = _mm256_sll_epi32(v_mul, v_shift_3);\n\n        __m256i v_product = _mm256_mullo_epi32(v_tmp1, v_tmp2);\n        __m256i v_sum = _mm256_add_epi32(v_mul, v_product);\n\n        __m256i v_result = _mm256_div_epi32(v_sum, v_seven); // Assuming custom div function\n        _mm256_storeu_si256((__m256i *)(dst + i), v_result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 3 < rows; i += 4) {\n        __m256d vec = _mm256_set_pd(\n            src[i + 3 * cols + stride], \n            src[i + 2 * cols + stride], \n            src[i + 1 * cols + stride], \n            src[i + 0 * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m256d vtmp;\n\n    // Handle cases where rows is a multiple of 4 using AVX2\n    for (; i <= rows - 4; i += 4) {\n        size_t src_index = i * cols + stride;\n        vtmp = _mm256_loadu_pd(&src[src_index]);\n        _mm256_storeu_pd(&dst[i], vtmp);\n    }\n\n    // Handle remaining rows with scalar code\n    for (; i < rows; ++i) {\n        size_t src_index = i * cols + stride;\n        dst[i] = src[src_index];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    const size_t elements_per_vector = 4; // AVX/AVX2 can handle 4 doubles per 256-bit register\n    const size_t aligned_rows = rows / elements_per_vector;\n\n    size_t i = 0;\n    for (; i <= aligned_rows - elements_per_vector; i += elements_per_vector) {\n        __m256d vec = _mm256_set_pd(\n            src[i * cols + stride + 3],\n            src[i * cols + stride + 2],\n            src[i * cols + stride + 1],\n            src[i * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    const size_t vec_size = 4; // Number of elements in a YMM register for double precision\n    size_t r;\n\n    for (r = 0; r + vec_size <= rows; r += vec_size) {\n        __m256d vec = _mm256_set_pd(\n            src[r + vec_size - 1 + stride + r * cols],\n            src[r + vec_size - 2 + stride + r * cols],\n            src[r + vec_size - 3 + stride + r * cols],\n            src[r + vec_size - 4 + stride + r * cols]\n        );\n        _mm256_storeu_pd(&dst[r], vec);\n    }\n\n    if (r < rows) {\n        __m256d vec = _mm256_setzero_pd();\n        for (size_t i = 0; i < rows - r; ++i) {\n            vec = _mm256_insertf64x1(vec, _mm_load_sd(&src[r + i + stride + (r + i) * cols]), i);\n        }\n        _mm256_storeu_pd(&dst[r], vec);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 4 <= rows; i += 4) {\n        __m256d vec = _mm256_set_pd(\n            src[i + 3 * cols + stride],\n            src[i + 2 * cols + stride],\n            src[i + 1 * cols + stride],\n            src[i + 0 * cols + stride]\n        );\n        _mm256_storeu_pd(&dst[i], vec);\n    }\n    for (; i < rows; ++i) {\n        dst[i] = src[i * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    const size_t vec_len = 8; // Number of elements in a YMM register for floats\n    size_t i = 0;\n\n    // Gather operation\n    for (i = 0; i + vec_len <= length; i += vec_len) {\n        __m256 idx = _mm256_loadu_ps(reinterpret_cast<const float*>(&indices[i]));\n        __m256 vals = _mm256_i32gather_ps(src, _mm256_cvtps_epi32(idx), 4);\n        _mm256_storeu_ps(&dst[i], vals);\n    }\n    // Handle leftover elements for gather\n    if (i < length) {\n        __m256i mask = _mm256_set_epi32(-1, -1, -1, -1, -1, -1, -1, -1);\n        for (size_t j = i; j < length; ++j) {\n            mask = _mm256_set_epi32(-1, -1, -1, -1, -1, -1, -1, indices[j]);\n            __m256 vals = _mm256_mask_i32gather_ps(_mm256_undefined_ps(), mask, src, _mm256_set1_ps(0.0f), 4);\n            dst[j] = _mm256_cvtss_f32(vals);\n            i++;\n        }\n    }\n\n    i = 0;\n    // Scatter operation\n    for (i = 0; i + vec_len <= length; i += vec_len) {\n        __m256 vals = _mm256_loadu_ps(&src[i]);\n        __m256i idx = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&indices[i]));\n        _mm256_i32scatter_ps(dst, idx, vals, 4);\n    }\n    // Handle leftover elements for scatter\n    if (i < length) {\n        for (size_t j = i; j < length; ++j) {\n            dst[indices[j]] = src[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m256 indices_vec;\n    __m256 src_vec;\n\n    for (; i + 8 <= length; i += 8) {\n        // Load 8 indices\n        indices_vec = _mm256_loadu_si256((__m256i const *)(indices + i));\n\n        // Gather 8 floats from src using the indices\n        src_vec = _mm256_i32gather_ps(src, indices_vec, 4);\n\n        // Store gathered values into dst (scatter operation)\n        _mm256_storeu_ps(dst + i, src_vec);\n\n        // Scatter the same values back to dst using indices as write locations\n        _mm256_i32scatter_ps(dst, indices_vec, src_vec, 4);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    const size_t vec_len = 8; // Number of elements in a YMM register for floats\n    size_t i = 0;\n\n    // Gather operation\n    for (i = 0; i + vec_len <= length; i += vec_len) {\n        __m256 idx = _mm256_loadu_ps((const float *)(indices + i));\n        __m256 vals = _mm256_i32gather_ps(src, _mm256_cvtps_epi32(idx), sizeof(float));\n        _mm256_storeu_ps(dst + i, vals);\n    }\n    // Handle leftover elements for gather\n    if (i < length) {\n        __m256 idx = _mm256_setzero_ps();\n        __m256 vals = _mm256_setzero_ps();\n        for (size_t j = 0; j < length - i; ++j) {\n            ((int *)&idx)[j] = indices[i + j];\n            vals = _mm256_insert_ps(vals, src[indices[i + j]], j);\n        }\n        _mm256_storeu_ps(dst + i, vals);\n    }\n\n    i = 0;\n    // Scatter operation\n    for (i = 0; i + vec_len <= length; i += vec_len) {\n        __m256 vals = _mm256_loadu_ps(src + i);\n        __m256i idx = _mm256_loadu_si256((const __m256i *)(indices + i));\n        _mm256_i32scatter_ps(dst, idx, vals, sizeof(float));\n    }\n    // Handle leftover elements for scatter\n    if (i < length) {\n        for (size_t j = 0; j < length - i; ++j) {\n            dst[indices[i + j]] = src[i + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    const size_t vec_len = 8; // Number of elements in a YMM register for floats\n    size_t i = 0;\n\n    // Gather operation\n    for (i = 0; i + vec_len <= length; i += vec_len) {\n        __m256 idx_vec = _mm256_loadu_si256((const __m256i *)(indices + i));\n        __m256 gather_vec = _mm256_i32gather_ps(src, idx_vec, 4);\n        _mm256_storeu_ps(dst + i, gather_vec);\n    }\n    // Handle leftover elements for gather\n    if (i < length) {\n        __m256i idx_vec = _mm256_set_epi32(\n            (i + 7 < length) ? indices[i + 7] : 0,\n            (i + 6 < length) ? indices[i + 6] : 0,\n            (i + 5 < length) ? indices[i + 5] : 0,\n            (i + 4 < length) ? indices[i + 4] : 0,\n            (i + 3 < length) ? indices[i + 3] : 0,\n            (i + 2 < length) ? indices[i + 2] : 0,\n            (i + 1 < length) ? indices[i + 1] : 0,\n            indices[i]\n        );\n        __m256 gather_vec = _mm256_i32gather_ps(src, idx_vec, 4);\n        _mm256_storeu_ps(dst + i, gather_vec);\n    }\n\n    i = 0;\n    // Scatter operation\n    for (i = 0; i + vec_len <= length; i += vec_len) {\n        __m256 val_vec = _mm256_loadu_ps(src + i);\n        __m256i idx_vec = _mm256_loadu_si256((const __m256i *)(indices + i));\n        _mm256_i32scatter_ps(dst, idx_vec, val_vec, 4);\n    }\n    // Handle leftover elements for scatter\n    if (i < length) {\n        __m256 val_vec = _mm256_set_epi32(\n            (i + 7 < length) ? src[i + 7] : 0,\n            (i + 6 < length) ? src[i + 6] : 0,\n            (i + 5 < length) ? src[i + 5] : 0,\n            (i + 4 < length) ? src[i + 4] : 0,\n            (i + 3 < length) ? src[i + 3] : 0,\n            (i + 2 < length) ? src[i + 2] : 0,\n            (i + 1 < length) ? src[i + 1] : 0,\n            src[i]\n        );\n        __m256i idx_vec = _mm256_set_epi32(\n            (i + 7 < length) ? indices[i + 7] : 0,\n            (i + 6 < length) ? indices[i + 6] : 0,\n            (i + 5 < length) ? indices[i + 5] : 0,\n            (i + 4 < length) ? indices[i + 4] : 0,\n            (i + 3 < length) ? indices[i + 3] : 0,\n            (i + 2 < length) ? indices[i + 2] : 0,\n            (i + 1 < length) ? indices[i + 1] : 0,\n            indices[i]\n        );\n        _mm256_i32scatter_ps(dst, idx_vec, val_vec, 4);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```cpp\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m256 indices_vec_f;\n    __m256 src_vec;\n    \n    for (i = 0; i + 8 <= length; i += 8) {\n        // Load 8 indices\n        __m256i indices_vec_i = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&indices[i]));\n        \n        // Gather operation: load values from src using indices\n        indices_vec_f = _mm256_i32gather_ps(src, indices_vec_i, 4);\n        \n        // Store gathered values into dst (scatter operation is not directly supported in AVX2 for floats)\n        _mm256_storeu_ps(&dst[i], indices_vec_f);\n        \n        // Direct copy from src to dst using indices as write locations\n        src_vec = _mm256_loadu_ps(&src[i]);\n        _mm256_storeu_ps(&dst[indices[i]], src_vec);\n    }\n    \n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]]; // gather\n        dst[indices[i]] = src[i]; // scatter\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || end - start + 1 < 16) {\n        for (uint64_t i = 0; i <= (end - start) / 2; ++i) {\n            int16_t tmp = arr[start + i];\n            arr[start + i] = arr[end - i];\n            arr[end - i] = tmp;\n        }\n        return;\n    }\n\n    uint64_t len = end - start + 1;\n    uint64_t vec_len = len / 16;\n    uint64_t rem = len % 16;\n\n    uint64_t i = 0;\n    for (; i < vec_len / 2; ++i) {\n        __m256i v1 = _mm256_loadu_si256((__m256i const *)(arr + start + i * 16));\n        __m256i v2 = _mm256_loadu_si256((__m256i const *)(arr + end - i * 16 - 15));\n\n        __m256i reversed_v2 = _mm256_permute4x64_epi64(v2, _MM_SHUFFLE(0, 1, 2, 3));\n        __m256i reversed_v1 = _mm256_permute4x64_epi64(v1, _MM_SHUFFLE(0, 1, 2, 3));\n\n        reversed_v2 = _mm256_shufflelo_epi16(reversed_v2, _MM_SHUFFLE(0, 1, 2, 3));\n        reversed_v2 = _mm256_shufflehi_epi16(reversed_v2, _MM_SHUFFLE(0, 1, 2, 3));\n        reversed_v1 = _mm256_shufflelo_epi16(reversed_v1, _MM_SHUFFLE(0, 1, 2, 3));\n        reversed_v1 = _mm256_shufflehi_epi16(reversed_v1, _MM_SHUFFLE(0, 1, 2, 3));\n\n        _mm256_storeu_si256((__m256i *)(arr + start + i * 16), reversed_v2);\n        _mm256_storeu_si256((__m256i *)(arr + end - i * 16 - 15), reversed_v1);\n    }\n\n    if (vec_len % 2 == 1) {\n        __m256i v = _mm256_loadu_si256((__m256i const *)(arr + start + i * 16));\n        __m256i reversed_v = _mm256_permute4x64_epi64(v, _MM_SHUFFLE(0, 1, 2, 3));\n        reversed_v = _mm256_shufflelo_epi16(reversed_v, _MM_SHUFFLE(0, 1, 2, 3));\n        reversed_v = _mm256_shufflehi_epi16(reversed_v, _MM_SHUFFLE(0, 1, 2, 3));\n        _mm256_storeu_si256((__m256i *)(arr + start + i * 16), reversed_v);\n        i++;\n    }\n\n    for (uint64_t j = 0; j < rem / 2; ++j) {\n        int16_t tmp = arr[start + i * 16 + j];\n        arr[start + i * 16 + j] = arr[end - i * 16 - j];\n        arr[end - i * 16 - j] = tmp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || end - start + 1 < 8) {\n        for (uint64_t i = start; i <= (start + end) / 2; ++i) {\n            int16_t temp = arr[i];\n            arr[i] = arr[start + end - i];\n            arr[start + end - i] = temp;\n        }\n        return;\n    }\n\n    uint64_t len = end - start + 1;\n    uint64_t aligned_start = (start + 7) & ~7ULL;\n    uint64_t aligned_end = end & ~7ULL;\n\n    // Reverse elements before aligned section\n    for (uint64_t i = start; i < aligned_start && i <= (start + end) / 2; ++i) {\n        int16_t temp = arr[i];\n        arr[i] = arr[start + end - i];\n        arr[start + end - i] = temp;\n    }\n\n    // Reverse elements after aligned section\n    for (uint64_t i = aligned_end + 1; i <= end && i <= (start + end) / 2; ++i) {\n        int16_t temp = arr[i];\n        arr[i] = arr[start + end - i];\n        arr[start + end - i] = temp;\n    }\n\n    // Reverse aligned section using AVX2\n    if (aligned_start <= aligned_end) {\n        __m256i shuffle_mask = _mm256_set_epi32(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n        for (uint64_t i = aligned_start; i <= aligned_end; i += 16) {\n            __m256i vec = _mm256_loadu_si256((__m256i const *)(arr + i));\n            __m256i reversed_vec = _mm256_permutevar8x32_epi32(vec, shuffle_mask);\n            _mm256_storeu_si256((__m256i *)(arr + i), reversed_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || end - start + 1 < 8) {\n        for (uint64_t i = start, j = end; i < j; ++i, --j) {\n            int16_t tmp = arr[i];\n            arr[i] = arr[j];\n            arr[j] = tmp;\n        }\n        return;\n    }\n\n    uint64_t range = end - start + 1;\n    uint64_t aligned_start = (start + 7) & ~7ULL;\n    uint64_t aligned_end = end & ~7ULL;\n\n    // Reverse elements before alignment\n    for (uint64_t i = start, j = aligned_start - 1; i < j; ++i, --j) {\n        int16_t tmp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = tmp;\n    }\n\n    // Reverse elements after alignment\n    for (uint64_t i = aligned_end + 1, j = end; i < j; ++i, --j) {\n        int16_t tmp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = tmp;\n    }\n\n    // AVX2 reverse for aligned section\n    uint64_t i = aligned_start;\n    uint64_t j = aligned_end;\n    while (i < j) {\n        __m256i vec_i = _mm256_loadu_si256((__m256i const *)(arr + i));\n        __m256i vec_j = _mm256_loadu_si256((__m256i const *)(arr + j));\n\n        __m256i reversed_vec = _mm256_permute4x64_epi64(vec_j, 0xFF);\n        __m256i reversed_vec_j = _mm256_permute4x64_epi64(vec_i, 0xFF);\n\n        _mm256_storeu_si256((__m256i *)(arr + i), reversed_vec);\n        _mm256_storeu_si256((__m256i *)(arr + j), reversed_vec_j);\n\n        i += 8;\n        j -= 8;\n    }\n\n    if (i == j) {\n        __m256i vec = _mm256_loadu_si256((__m256i const *)(arr + i));\n        __m256i reversed_vec = _mm256_permute4x64_epi64(vec, 0xFF);\n        _mm256_storeu_si256((__m256i *)(arr + i), reversed_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    const uint64_t length = end - start + 1;\n    const uint64_t half_length = length / 2;\n    int16_t* left = arr + start;\n    int16_t* right = arr + end;\n\n    for (uint64_t i = 0; i < half_length / 8; ++i) {\n        __m256i vleft = _mm256_loadu_si256((__m256i*)left);\n        __m256i vright = _mm256_loadu_si256((__m256i*)right);\n\n        __m256i swapped = _mm256_permute4x64_epi64(vleft, 0xFF);\n        __m256i swapped_right = _mm256_permute4x64_epi64(vright, 0xFF);\n\n        _mm256_storeu_si256((__m256i*)left, swapped_right);\n        _mm256_storeu_si256((__m256i*)right, swapped);\n\n        left += 16;\n        right -= 16;\n    }\n\n    // Handle remaining elements with scalar code\n    for (uint64_t i = half_length / 8 * 16; i < half_length; ++i) {\n        int16_t temp = left[i];\n        left[i] = right[-i - 1];\n        right[-i - 1] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    const uint64_t length = end - start + 1;\n    int16_t* left = arr + start;\n    int16_t* right = arr + end;\n\n    if (length < 8) {\n        for (uint64_t i = 0; i < (length + 1) / 2; ++i) {\n            int16_t temp = left[i];\n            left[i] = right[length - 1 - i];\n            right[length - 1 - i] = temp;\n        }\n        return;\n    }\n\n    __m256i shuffle_mask = _mm256_set_epi32(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n\n    uint64_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i left_vec = _mm256_loadu_si256((__m256i*)(left + i));\n        __m256i right_vec = _mm256_loadu_si256((__m256i*)(right - i));\n\n        __m256i reversed_left = _mm256_permutevar8x32_epi32(left_vec, shuffle_mask);\n        __m256i reversed_right = _mm256_permutevar8x32_epi32(right_vec, shuffle_mask);\n\n        _mm256_storeu_si256((__m256i*)(left + i), reversed_right);\n        _mm256_storeu_si256((__m256i*)(right - i), reversed_left);\n    }\n\n    for (; i < (length + 1) / 2; ++i) {\n        int16_t temp = left[i];\n        left[i] = right[length - 1 - i];\n        right[length - 1 - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i, j;\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j + 31 < dim2; j += 32) {\n            __m256i vec = _mm256_loadu_si256((const __m256i *)(src + i * dim2 + j));\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j), vec);\n        }\n        for (; j < dim2; ++j) {\n            slice[i * dim2 + j] = src[i * dim2 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i, j;\n    __m256i vzero = _mm256_setzero_si256();\n    \n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j + 32 <= dim2; j += 32) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i *)(src + i * dim2 + j));\n            __m256i vec2 = _mm256_loadu_si256((__m256i *)(src + i * dim2 + j + 32 - 32));\n            \n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j), vec1);\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j + 32 - 32), vec2);\n        }\n        \n        for (; j < dim2; ++j) {\n            slice[i * dim2 + j] = src[i * dim2 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i, j;\n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j + 31 < dim2; j += 32) {\n            __m256i v0 = _mm256_loadu_si256((const __m256i *)(src + i * dim2 * dim3 + j * dim3));\n            __m256i v1 = _mm256_loadu_si256((const __m256i *)(src + i * dim2 * dim3 + (j + 8) * dim3));\n            __m256i v2 = _mm256_loadu_si256((const __m256i *)(src + i * dim2 * dim3 + (j + 16) * dim3));\n            __m256i v3 = _mm256_loadu_si256((const __m256i *)(src + i * dim2 * dim3 + (j + 24) * dim3));\n\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j), v0);\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j + 8), v1);\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j + 16), v2);\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j + 24), v3);\n        }\n        for (; j < dim2; ++j) {\n            slice[i * dim2 + j] = src[i * dim2 * dim3 + j * dim3 + slice_idx];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i, j;\n    size_t vec_width = 32; // AVX2 operates on 256-bit registers (32 bytes)\n    size_t aligned_dim2 = (dim2 / vec_width) * vec_width;\n\n    for (i = 0; i < dim1; i++) {\n        for (j = 0; j < aligned_dim2; j += vec_width) {\n            __m256i v = _mm256_loadu_si256((__m256i const *)(src + i * dim2 + j));\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j), v);\n        }\n        // Handle remaining elements that don't fit into a full AVX2 vector\n        for (; j < dim2; j++) {\n            slice[i * dim2 + j] = src[i * dim2 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i, j;\n    __m256i vzero = _mm256_setzero_si256();\n    \n    for (i = 0; i < dim1; ++i) {\n        for (j = 0; j + 32 <= dim2; j += 32) {\n            __m256i vec1 = _mm256_loadu_si256((__m256i *)(src + i * dim2 + j));\n            __m256i vec2 = _mm256_loadu_si256((__m256i *)(src + i * dim2 + j + 16));\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j), vec1);\n            _mm256_storeu_si256((__m256i *)(slice + i * dim2 + j + 16), vec2);\n        }\n        for (; j < dim2; ++j) {\n            slice[i * dim2 + j] = src[i * dim2 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ++ii) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const *)(src + ii * cols + jj));\n                    _mm256_storeu_si256((__m256i *)(dst + jj * rows + ii), vec);\n                }\n            }\n            // Perform in-register transpose if block_size >= 8\n            if (block_size >= 8) {\n                for (size_t ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                    for (size_t jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                        __m256i v0 = _mm256_loadu_si256((__m256i const *)(src + (ii + 0) * cols + jj));\n                        __m256i v1 = _mm256_loadu_si256((__m256i const *)(src + (ii + 1) * cols + jj));\n                        __m256i v2 = _mm256_loadu_si256((__m256i const *)(src + (ii + 2) * cols + jj));\n                        __m256i v3 = _mm256_loadu_si256((__m256i const *)(src + (ii + 3) * cols + jj));\n                        __m256i v4 = _mm256_loadu_si256((__m256i const *)(src + (ii + 4) * cols + jj));\n                        __m256i v5 = _mm256_loadu_si256((__m256i const *)(src + (ii + 5) * cols + jj));\n                        __m256i v6 = _mm256_loadu_si256((__m256i const *)(src + (ii + 6) * cols + jj));\n                        __m256i v7 = _mm256_loadu_si256((__m256i const *)(src + (ii + 7) * cols + jj));\n\n                        __m256i t0 = _mm256_unpacklo_epi16(v0, v1);\n                        __m256i t1 = _mm256_unpackhi_epi16(v0, v1);\n                        __m256i t2 = _mm256_unpacklo_epi16(v2, v3);\n                        __m256i t3 = _mm256_unpackhi_epi16(v2, v3);\n                        __m256i t4 = _mm256_unpacklo_epi16(v4, v5);\n                        __m256i t5 = _mm256_unpackhi_epi16(v4, v5);\n                        __m256i t6 = _mm256_unpacklo_epi16(v6, v7);\n                        __m256i t7 = _mm256_unpackhi_epi16(v6, v7);\n\n                        __m256i u0 = _mm256_permute2x128_si256(t0, t2, 0x20);\n                        __m256i u1 = _mm256_permute2x128_si256(t0, t2, 0x31);\n                        __m256i u2 = _mm256_permute2x128_si256(t1, t3, 0x20);\n                        __m256i u3 = _mm256_permute2x128_si256(t1, t3, 0x31);\n                        __m256i u4 = _mm256_permute2x128_si256(t4, t6, 0x20);\n                        __m256i u5 = _mm256_permute2x128_si256(t4, t6, 0x31);\n                        __m256i u6 = _mm256_permute2x128_si256(t5, t7, 0x20);\n                        __m256i u7 = _mm256_permute2x128_si256(t5, t7, 0x31);\n\n                        __m256i r0 = _mm256_shuffle_epi8(u0, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n                        __m256i r1 = _mm256_shuffle_epi8(u1, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n                        __m256i r2 = _mm256_shuffle_epi8(u2, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n                        __m256i r3 = _mm256_shuffle_epi8(u3, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n                        __m256i r4 = _mm256_shuffle_epi8(u4, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n                        __m256i r5 = _mm256_shuffle_epi8(u5, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n                        __m256i r6 = _mm256_shuffle_epi8(u6, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n                        __m256i r7 = _mm256_shuffle_epi8(u7, _mm256_set_epi8(0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0, 0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0));\n\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 0) * rows + ii), r0);\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 1) * rows + ii), r1);\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 2) * rows + ii), r2);\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 3) * rows + ii), r3);\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 4) * rows + ii), r4);\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 5) * rows + ii), r5);\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 6) * rows + ii), r6);\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + 7) * rows + ii), r7);\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k, l;\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = i; k < i + block_size && k < rows; k++) {\n                __m256i row_vec = _mm256_loadu_si256((__m256i const *)(src + k * cols + j));\n                for (l = j; l < j + block_size && l < cols; l += 16) {\n                    __m256i shuffled = _mm256_shuffle_epi8(row_vec, _mm256_set_epi8(\n                        15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n                        31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16));\n                    _mm256_storeu_si256((__m256i *)(dst + l * rows + k), shuffled);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ++ii) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    __m256i vec = _mm256_loadu_si256((const __m256i *)(src + ii * cols + jj));\n                    size_t kk;\n                    for (kk = 0; kk < 8 && jj + kk < cols && ii < rows; ++kk) {\n                        _mm256_storeu_si256((__m256i *)(dst + (jj + kk) * rows + ii), _mm256_permute4x64_epi64(vec, _MM_SHUFFLE(kk+1,kk+0,kk+3,kk+2)));\n                    }\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ++ii) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const *)(src + ii * cols + jj));\n                    _mm256_storeu_si256((__m256i *)(dst + jj * rows + ii), vec);\n                }\n            }\n            for (size_t jj = j; jj < j + block_size && jj < cols; ++jj) {\n                for (size_t ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                    __m256i vec = _mm256_loadu_si256((__m256i const *)(src + ii * cols + jj));\n                    _mm256_storeu_si256((__m256i *)(dst + jj * rows + ii), vec);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k, l;\n    for (i = 0; i < rows; i += block_size) {\n        for (j = 0; j < cols; j += block_size) {\n            for (k = i; k < i + block_size && k < rows; k++) {\n                __m256i row = _mm256_loadu_si256((__m256i const *)(src + k * cols + j));\n                for (l = j; l < j + block_size && l < cols; l += 8) {\n                    __m256i v = _mm256_permute4x64_epi64(row, 0xD8);\n                    _mm256_storeu_si256((__m256i *)(dst + l * rows + k), v);\n                    row = _mm256_alignr_epi8(row, _mm256_loadu_si256((__m256i const *)(src + k * cols + l + 8)), 8);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i;\n    \n    for (i = 0; i <= (dim - 8); i += 8) {\n        size_t idx = i * (dim * dim + dim + 1);\n        __m256 vec = _mm256_loadu_ps(&array[idx]);\n        sum_vec = _mm256_add_ps(sum_vec, vec);\n    }\n    \n    float sum[8] __attribute__((aligned(32)));\n    _mm256_store_ps(sum, sum_vec);\n    \n    float total_sum = sum[0] + sum[1] + sum[2] + sum[3] + sum[4] + sum[5] + sum[6] + sum[7];\n    \n    for (; i < dim; ++i) {\n        size_t idx = i * (dim * dim + dim + 1);\n        total_sum += array[idx];\n    }\n    \n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i;\n\n    for (i = 0; i < dim && ((uintptr_t)(array + i*dim*dim + i*dim + i) % 32 != 0); i++) {\n        sum_vec = _mm256_add_ps(sum_vec, _mm256_set_ps(array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i]));\n    }\n\n    for (; i + 7 < dim; i += 8) {\n        __m256 vals = _mm256_loadu_ps(array + i*dim*dim + i*dim + i);\n        sum_vec = _mm256_add_ps(sum_vec, vals);\n    }\n\n    for (; i < dim; i++) {\n        sum_vec = _mm256_add_ps(sum_vec, _mm256_set_ps(array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i], \n                                                       array[i*dim*dim + i*dim + i]));\n    }\n\n    __m128 lo = _mm256_extractf128_ps(sum_vec, 0);\n    __m128 hi = _mm256_extractf128_ps(sum_vec, 1);\n    lo = _mm_add_ps(lo, hi);\n    lo = _mm_hadd_ps(lo, lo);\n    lo = _mm_hadd_ps(lo, lo);\n\n    float result;\n    _mm_store_ss(&result, lo);\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    __m256 vsum = _mm256_setzero_ps();\n    size_t i;\n\n    for (i = 0; i <= (dim - 8); i += 8) {\n        __m256 v = _mm256_loadu_ps(&array[i * (dim * dim + dim + 1)]);\n        vsum = _mm256_add_ps(vsum, v);\n    }\n\n    for (; i < dim; ++i) {\n        sum += array[i * (dim * dim + dim + 1)];\n    }\n\n    float temp[8];\n    _mm256_storeu_ps(temp, vsum);\n\n    for (size_t j = 0; j < 8; ++j) {\n        sum += temp[j];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    __m256 vsum = _mm256_setzero_ps();\n    size_t i;\n\n    for (i = 0; i <= (dim - 8); i += 8) {\n        __m256 v = _mm256_loadu_ps(&array[i * (dim * dim + dim + 1)]);\n        vsum = _mm256_add_ps(vsum, v);\n    }\n\n    if (i < dim) {\n        __m256 v = _mm256_setzero_ps();\n        for (size_t j = i; j < dim; ++j) {\n            float val = array[j * (dim * dim + dim + 1)];\n            sum += val;\n        }\n    }\n\n    vsum = _mm256_hadd_ps(vsum, vsum);\n    vsum = _mm256_hadd_ps(vsum, vsum);\n    float tmp[8];\n    _mm256_storeu_ps(tmp, vsum);\n    for (int k = 0; k < 8; ++k) {\n        sum += tmp[k];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m256 sum_vec = _mm256_setzero_ps();\n    size_t i;\n\n    for (i = 0; i < dim - 7; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&array[i * (dim * dim + dim + 1)]);\n        sum_vec = _mm256_add_ps(sum_vec, vec);\n    }\n\n    float sum = 0.0f;\n    float temp[8];\n    _mm256_storeu_ps(temp, sum_vec);\n    for (size_t j = 0; j < 8; j++) {\n        sum += temp[j];\n    }\n\n    for (; i < dim; i++) {\n        sum += array[i * (dim * dim + dim + 1)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    size_t aligned_total = total / 4 * 4;\n\n    __m256d vthreshold = _mm256_set1_pd(threshold);\n    __m256d vscale = _mm256_set1_pd(scale);\n    __m256d vone = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < aligned_total; i += 4) {\n        __m256d vsrc = _mm256_loadu_pd(src + i);\n        __m256d cmp = _mm256_cmp_pd(vsrc, vthreshold, _CMP_GT_OS);\n        __m256d mask = _mm256_and_pd(cmp, _mm256_sub_pd(vscale, vone));\n        mask = _mm256_add_pd(mask, vone);\n        __m256d vdst = _mm256_mul_pd(vsrc, mask);\n        _mm256_storeu_pd(dst + i, vdst);\n    }\n\n    for (size_t i = aligned_total; i < total; ++i) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    size_t aligned_count = total / 4 * 4;\n\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    __m256d one_vec = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < aligned_count; i += 4) {\n        __m256d src_vec = _mm256_loadu_pd(src + i);\n        __m256d cmp_result = _mm256_cmpgt_pd(src_vec, threshold_vec);\n        __m256d mask = _mm256_and_pd(cmp_result, _mm256_sub_pd(scale_vec, one_vec));\n        __m256d result = _mm256_fmadd_pd(mask, src_vec, src_vec);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (size_t i = aligned_count; i < total; ++i) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    const size_t vector_size = 4; // Number of doubles in a YMM register\n    __m256d threshold_v = _mm256_set1_pd(threshold);\n    __m256d scale_v = _mm256_set1_pd(scale);\n    __m256d one_v = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; j += vector_size) {\n            if (j + vector_size <= cols) {\n                __m256d src_v = _mm256_loadu_pd(&src[i * cols + j]);\n                __m256d cmp_result = _mm256_cmpgt_pd(src_v, threshold_v); // Compare src > threshold\n                __m256d mask = _mm256_and_pd(cmp_result, _mm256_sub_pd(scale_v, one_v)); // Create mask (scale-1 or 0)\n                mask = _mm256_add_pd(mask, one_v); // Add 1 to get scale or 1\n                __m256d result = _mm256_mul_pd(src_v, mask); // Scale if greater than threshold\n                _mm256_storeu_pd(&dst[i * cols + j], result);\n            } else {\n                // Handle remaining elements with scalar code\n                for (size_t k = j; k < cols; ++k) {\n                    dst[i * cols + k] = (src[i * cols + k] > threshold) ? (src[i * cols + k] * scale) : src[i * cols + k];\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t total = rows * cols;\n    size_t aligned_count = total / 4 * 4;\n\n    __m256d vthreshold = _mm256_set1_pd(threshold);\n    __m256d vscale = _mm256_set1_pd(scale);\n    __m256d vone = _mm256_set1_pd(1.0);\n\n    for (size_t i = 0; i < aligned_count; i += 4) {\n        __m256d vsrc = _mm256_loadu_pd(src + i);\n        __m256d cmp = _mm256_cmpgt_pd(vsrc, vthreshold);\n        __m256d mask = _mm256_and_pd(cmp, _mm256_sub_pd(vscale, vone));\n        __m256d scaled = _mm256_fmadd_pd(vsrc, mask, vsrc);\n        _mm256_storeu_pd(dst + i, scaled);\n    }\n\n    for (size_t i = aligned_count; i < total; ++i) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    const size_t length = rows * cols;\n    const size_t vec_length = 4; // Number of doubles in a YMM register\n    const __m256d threshold_vec = _mm256_set1_pd(threshold);\n    const __m256d scale_vec = _mm256_set1_pd(scale);\n    const __m256d one_vec = _mm256_set1_pd(1.0);\n\n    size_t i = 0;\n    for (; i <= length - vec_length; i += vec_length) {\n        __m256d src_vec = _mm256_loadu_pd(&src[i]);\n        __m256d cmp_result = _mm256_cmpgt_pd(src_vec, threshold_vec); // Compare src > threshold\n        __m256d mask = _mm256_and_pd(cmp_result, scale_vec);          // Mask where src > threshold\n        __m256d inv_mask = _mm256_andnot_pd(cmp_result, one_vec);    // Inverse mask\n        __m256d scaled = _mm256_mul_pd(src_vec, mask);               // Scale where src > threshold\n        __m256d result = _mm256_add_pd(scaled, _mm256_mul_pd(src_vec, inv_mask)); // Combine results\n        _mm256_storeu_pd(&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = (src[i] > threshold) ? (src[i] * scale) : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    const size_t vec_size = 4; // Number of elements in a YMM register for doubles\n    size_t i, j;\n\n    for (i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        const double* src_row = src + idx * cols;\n        double* dst_row = dst + i * cols;\n\n        for (j = 0; j + vec_size <= cols; j += vec_size) {\n            __m256d vec = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, vec);\n        }\n\n        // Handle the remaining elements that are not aligned to the vector size\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    const size_t vector_size = 4; // Number of elements in a YMM register for doubles\n    const size_t aligned_cols = (cols / vector_size) * vector_size;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i];\n        const double* src_row = src + src_row_idx * cols;\n        double* dst_row = dst + i * cols;\n\n        // Process full vectors\n        for (size_t j = 0; j < aligned_cols; j += vector_size) {\n            __m256d vec = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, vec);\n        }\n\n        // Handle remaining scalar elements\n        for (size_t j = aligned_cols; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    const size_t vec_size = 4; // Number of elements in a YMM register for doubles\n    size_t i, j;\n\n    for (i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        const double* src_row = src + idx * cols;\n        double* dst_row = dst + i * cols;\n\n        for (j = 0; j + vec_size <= cols; j += vec_size) {\n            __m256d vec = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, vec);\n        }\n\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    const size_t vec_size = 4; // Number of elements in a YMM register for doubles\n    const size_t aligned_cols = (cols / vec_size) * vec_size;\n\n    for (size_t i = 0; i < rows; ++i) {\n        size_t src_row_idx = indices[i];\n        const double *src_row = src + src_row_idx * cols;\n        double *dst_row = dst + i * cols;\n\n        // Process full vectors\n        for (size_t j = 0; j < aligned_cols; j += vec_size) {\n            __m256d vec = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, vec);\n        }\n\n        // Handle remaining elements\n        for (size_t j = aligned_cols; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    size_t vector_width = 4; // Number of doubles in a YMM register using AVX\n    size_t aligned_cols = (cols / vector_width) * vector_width;\n\n    for (i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        const double* src_row = src + idx * cols;\n        double* dst_row = dst + i * cols;\n\n        // Process elements in chunks of 4 (AVX width)\n        for (j = 0; j < aligned_cols; j += vector_width) {\n            __m256d vec = _mm256_loadu_pd(src_row + j);\n            _mm256_storeu_pd(dst_row + j, vec);\n        }\n\n        // Handle any remaining elements that don't fit into the vector width\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n\n    for (; i + 3 < length; i += 4) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src1[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src2[i]));\n        __m256i vsum = _mm256_add_epi64(va, vb);\n        __m256i vsum_scalar = _mm256_add_epi64(vsum, vscalar);\n        __m256i vresult = _mm256_srai_epi64(vsum_scalar, 1);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), vresult);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n\n    for (; i <= length - 4; i += 4) {\n        __m256i va = _mm256_loadu_si256((const __m256i *)(src1 + i));\n        __m256i vb = _mm256_loadu_si256((const __m256i *)(src2 + i));\n        __m256i vsum = _mm256_add_epi64(va, vb);\n        __m256i vsum_scalar = _mm256_add_epi64(vsum, vscalar);\n        __m256i vresult = _mm256_sra_epi64(vsum_scalar, _mm256_set1_epi64x(1));\n        _mm256_storeu_si256((__m256i *)(dst + i), vresult);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n\n    for (; i + 3 < length; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i const *)&src1[i]);\n        __m256i vb = _mm256_loadu_si256((__m256i const *)&src2[i]);\n        __m256i vsum = _mm256_add_epi64(va, vb);\n        __m256i vsum_scalar = _mm256_add_epi64(vsum, vscalar);\n        __m256i vresult = _mm256_sra_epi64(vsum_scalar, _mm256_set1_epi64x(1));\n        _mm256_storeu_si256((__m256i *)&dst[i], vresult);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i vscalar = _mm256_set1_epi64x(scalar);\n\n    for (; i + 3 < length; i += 4) {\n        __m256i va = _mm256_loadu_si256((__m256i const *)(src1 + i));\n        __m256i vb = _mm256_loadu_si256((__m256i const *)(src2 + i));\n        __m256i vsum = _mm256_add_epi64(va, vb);\n        vsum = _mm256_add_epi64(vsum, vscalar);\n        vsum = _mm256_sra_epi64(vsum, _mm256_set1_epi64x(1));\n        _mm256_storeu_si256((__m256i *)(dst + i), vsum);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    const size_t vec_len = 4; // Number of elements in a YMM register for int64_t\n    size_t i = 0;\n\n    __m256i v_scalar = _mm256_set1_epi64x(scalar);\n\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i v_src1 = _mm256_loadu_si256((__m256i const *)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i const *)&src2[i]);\n        __m256i v_sum = _mm256_add_epi64(v_src1, v_src2);\n        __m256i v_sum_scalar = _mm256_add_epi64(v_sum, v_scalar);\n        __m256i v_result = _mm256_srai_epi64(v_sum_scalar, 1);\n        _mm256_storeu_si256((__m256i *)&dst[i], v_result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    const __m256i v_max = _mm256_set1_epi32(0x7FFFFFFF);\n    __m256i v_min = v_max;\n    size_t i = 0;\n\n    // Process 8 elements per iteration\n    for (; i <= length - 8; i += 8) {\n        __m256i v_load = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i v_mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i v_even = _mm256_andnot_si256(v_mask, v_load);\n\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_even, v_load);\n        __m256i v_select = _mm256_blendv_epi8(v_max, v_load, v_cmp);\n\n        v_min = _mm256_min_epi32(v_min, v_select);\n    }\n\n    // Horizontal min for the remaining vector\n    int min_val = 0x7FFFFFFF;\n    int arr[8];\n    _mm256_storeu_si256((__m256i *)arr, v_min);\n    for (size_t j = 0; j < 8; j++) {\n        if (arr[j] < min_val) min_val = arr[j];\n    }\n\n    // Process leftover elements\n    for (; i < length; i++) {\n        if ((i % 2 == 0) && (src[i] < min_val)) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val == 0x7FFFFFFF ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    const __m256i v_mask = _mm256_set1_epi32(0xAAAAAAAA); // Mask for even indices\n    __m256i v_min = _mm256_set1_epi32(0x7FFFFFFF); // Initialize with max int value\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i v_load = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_even = _mm256_andnot_si256(v_mask, v_load); // Zero out odd elements\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_even, _mm256_setzero_si256()); // Check for zero\n        v_even = _mm256_blendv_epi8(v_even, v_min, v_cmp); // Replace zero with current min\n        v_min = _mm256_min_epi32(v_min, v_even); // Update min\n    }\n\n    int min_val = 0x7FFFFFFF;\n    __m256i v_min_epi32 = _mm256_permutevar8x32_epi32(v_min, _mm256_set_epi32(0,1,2,3,4,5,6,7));\n    min_val = _mm256_extract_epi32(v_min_epi32, 0);\n    min_val = (min_val < _mm256_extract_epi32(v_min_epi32, 1)) ? min_val : _mm256_extract_epi32(v_min_epi32, 1);\n    min_val = (min_val < _mm256_extract_epi32(v_min_epi32, 2)) ? min_val : _mm256_extract_epi32(v_min_epi32, 2);\n    min_val = (min_val < _mm256_extract_epi32(v_min_epi32, 3)) ? min_val : _mm256_extract_epi32(v_min_epi32, 3);\n    min_val = (min_val < _mm256_extract_epi32(v_min_epi32, 4)) ? min_val : _mm256_extract_epi32(v_min_epi32, 4);\n    min_val = (min_val < _mm256_extract_epi32(v_min_epi32, 5)) ? min_val : _mm256_extract_epi32(v_min_epi32, 5);\n    min_val = (min_val < _mm256_extract_epi32(v_min_epi32, 6)) ? min_val : _mm256_extract_epi32(v_min_epi32, 6);\n    min_val = (min_val < _mm256_extract_epi32(v_min_epi32, 7)) ? min_val : _mm256_extract_epi32(v_min_epi32, 7);\n\n    for (; i < length; i++) {\n        if ((i % 2 == 0) && (src[i] < min_val)) {\n            min_val = src[i];\n        }\n    }\n\n    return (min_val == 0x7FFFFFFF) ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n#include <stdint.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    const __m256i v_max = _mm256_set1_epi32(INT32_MAX);\n    __m256i v_min = v_max;\n\n    size_t i = 0;\n    size_t vec_len = length / 8 * 8;\n\n    for (; i <= vec_len - 8; i += 8) {\n        __m256i v_data = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_idx = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n        __m256i v_mask = _mm256_andnot_si256(_mm256_srli_epi32(v_idx, 1), _mm256_set1_epi32(1));\n        __m256i v_even = _mm256_and_si256(v_data, v_mask);\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_even, v_data);\n        __m256i v_sel = _mm256_blendv_epi8(v_max, v_data, v_cmp);\n        v_min = _mm256_min_epi32(v_min, v_sel);\n    }\n\n    int min_val = INT32_MAX;\n    int min_vals[8];\n    _mm256_storeu_si256((__m256i*)min_vals, v_min);\n\n    for (size_t j = 0; j < 8; ++j) {\n        if (min_vals[j] < min_val) min_val = min_vals[j];\n    }\n\n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (src[i] < min_val)) min_val = src[i];\n    }\n\n    return (min_val == INT32_MAX) ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    const __m256i v_max = _mm256_set1_epi32(0x7FFFFFFF);\n    __m256i v_min = v_max;\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i v_load = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_idx = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n        __m256i v_mask = _mm256_andnot_si256(_mm256_srli_epi32(v_idx, 1), _mm256_set1_epi32(-1));\n        __m256i v_even = _mm256_and_si256(v_load, v_mask);\n        v_min = _mm256_min_epi32(v_min, v_even);\n    }\n\n    int min_val = 0x7FFFFFFF;\n    __m256i v_res = _mm256_permutevar8x32_epi32(v_min, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n    min_val = _mm256_extract_epi32(v_res, 0);\n    for(int j=1; j<8; ++j){\n        min_val = (min_val < _mm256_extract_epi32(v_res, j)) ? min_val : _mm256_extract_epi32(v_res, j);\n    }\n\n    for (; i < length; ++i) {\n        if ((i % 2) == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return (min_val == 0x7FFFFFFF) ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n#include <stdint.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    const __m256i v_mask = _mm256_set1_epi32(0xAAAAAAAA); // Mask for even indices\n    __m256i v_min = _mm256_set1_epi32(INT_MAX);\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i v_data = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i v_even = _mm256_andnot_si256(v_mask, v_data); // Keep only even-indexed elements\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_even, v_even); // Generate mask for valid even elements\n        v_min = _mm256_min_epi32(v_min, _mm256_blendv_epi8(_mm256_set1_epi32(INT_MAX), v_data, v_cmp));\n    }\n\n    int min_val = INT_MAX;\n    int tmp[8];\n    _mm256_storeu_si256((__m256i *)tmp, v_min);\n    for (int j = 0; j < 8; ++j) {\n        if ((i - 8 + j) % 2 == 0 && tmp[j] < min_val) {\n            min_val = tmp[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (i % 2 == 0 && src[i] < min_val) {\n            min_val = src[i];\n        }\n    }\n\n    return min_val == INT_MAX ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t vec_len = 8;\n    const __m256i sh = _mm256_set1_epi32(shift);\n    for (size_t i = 0; i + vec_len <= length; i += vec_len) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const *)(src + i));\n        \n        // Arithmetic right shift\n        __m256i v_ashr = _mm256_sra_epi32(v_src, sh);\n        \n        // Logical right shift: convert to unsigned, shift, then convert back to signed\n        __m256i v_lshr = _mm256_srlv_epi32(_mm256_castsi256_epi32(v_src), _mm256_castsi256_epi32(sh));\n        \n        // Add arithmetic and logical shifts\n        __m256i v_res = _mm256_add_epi32(v_ashr, v_lshr);\n        \n        // Store the result\n        _mm256_storeu_si256((__m256i *)(dst + i), v_res);\n    }\n    // Handle remaining elements\n    for (size_t i = (length / vec_len) * vec_len; i < length; ++i) {\n        int ashift = src[i] >> shift;\n        unsigned int usrc = static_cast<unsigned int>(src[i]);\n        int lshift = static_cast<int>(usrc >> shift);\n        dst[i] = ashift + lshift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t vec_len = 8;\n    const size_t aligned_length = length / vec_len * vec_len;\n\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (size_t i = 0; i < aligned_length; i += vec_len) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const *)(src + i));\n\n        // Arithmetic right shift\n        __m256i varith = _mm256_sra_epi32(vsrc, vshift);\n\n        // Logical right shift\n        __m256i vlogical = _mm256_srlv_epi32(_mm256_castsi256_epi32(_mm256_and_si256(vsrc, _mm256_set1_epi32(0xFFFFFFFF))), vshift);\n\n        // Add results\n        __m256i vresult = _mm256_add_epi32(varith, vlogical);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_length; i < length; ++i) {\n        int val = src[i];\n        int arith_shift = val >> shift;\n        int logic_shift = ((unsigned int)val) >> shift;\n        dst[i] = arith_shift + logic_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t vec_len = 8;\n    size_t i = 0;\n\n    __m256i shft = _mm256_set1_epi32(shift);\n\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i v_src = _mm256_loadu_si256((__m256i const *)(src + i));\n\n        // Arithmetic right shift\n        __m256i arith_shift = _mm256_sra_epi32(v_src, shft);\n\n        // Logical right shift\n        __m256i v_src_uint = _mm256_castsi256_epi32(_mm256_cvtepi32_epi64(v_src));\n        __m256i logic_shift_uint = _mm256_srl_epi32(v_src_uint, shft);\n        __m256i logic_shift = _mm256_castsi256_epi32(logic_shift_uint);\n\n        // Add results\n        __m256i result = _mm256_add_epi32(arith_shift, logic_shift);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int val = src[i];\n        int arith = val >> shift;\n        int logic = static_cast<int>(static_cast<unsigned int>(val) >> shift);\n        dst[i] = arith + logic;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t vec_size = sizeof(__m256i) / sizeof(int);\n    size_t i = 0;\n\n    for (; i + vec_size <= length; i += vec_size) {\n        __m256i v_src = _mm256_loadu_si256((__m256i *)(src + i));\n\n        // Arithmetic right shift\n        __m256i v_ars = _mm256_sra_epi32(v_src, _mm256_set1_epi32(shift));\n\n        // Logical right shift: convert to unsigned, shift, then convert back to signed\n        __m256i v_lrs = _mm256_srl_epi32(v_src, _mm256_set1_epi32(shift));\n\n        // Add arithmetic and logical shifts\n        __m256i v_res = _mm256_add_epi32(v_ars, v_lrs);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), v_res);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        int val = src[i];\n        int ars = val >> shift;\n        int lrs = static_cast<int>(static_cast<unsigned int>(val) >> shift);\n        dst[i] = ars + lrs;\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t vec_len = 8;\n    const size_t aligned_length = length / vec_len * vec_len;\n\n    __m256i vshift = _mm256_set1_epi32(shift);\n\n    for (size_t i = 0; i < aligned_length; i += vec_len) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const *)(src + i));\n\n        // Arithmetic right shift\n        __m256i varith = _mm256_sra_epi32(vsrc, vshift);\n\n        // Logical right shift: convert to unsigned, shift, then convert back to signed\n        __m256i vlogical = _mm256_srlv_epi32(_mm256_castsi256_epi32(_mm256_and_si256(vsrc, _mm256_set1_epi32(0xFFFFFFFF))), vshift);\n\n        // Add arithmetic and logical shifts\n        __m256i vresult = _mm256_add_epi32(varith, vlogical);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_length; i < length; ++i) {\n        int s = src[i];\n        int arith = s >> shift;\n        int logic = ((unsigned int)s) >> shift;\n        dst[i] = arith + logic;\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    __m256i sum_vec = _mm256_setzero_si256();\n    const size_t aligned_size = total / 8 * 8;\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t sum_arr[8] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i *)sum_arr, sum_vec);\n\n    int64_t sum = 0;\n    for (size_t i = 0; i < 8; ++i) {\n        sum += sum_arr[i];\n    }\n\n    for (size_t i = aligned_size; i < total; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const size_t length = rows * cols;\n    const size_t aligned_length = length / 8 * 8;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i v;\n\n    for (size_t i = 0; i < aligned_length; i += 8) {\n        v = _mm256_loadu_si256((__m256i *)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, v);\n    }\n\n    int temp[8] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i *)temp, sum_vec);\n\n    int scalar_sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        scalar_sum += temp[i];\n    }\n\n    for (size_t i = aligned_length; i < length; ++i) {\n        scalar_sum += matrix[i];\n    }\n\n    return scalar_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const size_t length = rows * cols;\n    const size_t aligned_length = length / 8 * 8;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i v;\n\n    for (size_t i = 0; i < aligned_length; i += 8) {\n        v = _mm256_loadu_si256((__m256i *)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, v);\n    }\n\n    int sum[8];\n    _mm256_storeu_si256((__m256i *)sum, sum_vec);\n\n    int total_sum = 0;\n    for (size_t i = 0; i < 8; ++i) {\n        total_sum += sum[i];\n    }\n\n    for (size_t i = aligned_length; i < length; ++i) {\n        total_sum += matrix[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    __m256i sum_vec = _mm256_setzero_si256();\n    const size_t aligned_size = total / 8 * 8;\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(matrix + i));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    int32_t sum_array[8];\n    _mm256_storeu_si256((__m256i *)sum_array, sum_vec);\n\n    int64_t sum = 0;\n    for (size_t i = 0; i < 8; ++i) {\n        sum += sum_array[i];\n    }\n\n    for (size_t i = aligned_size; i < total; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total_elements = rows * cols;\n    __m256i sum_vec = _mm256_setzero_si256();\n    const size_t vec_width = 8; // Number of 32-bit integers in a 256-bit register\n\n    for (size_t i = 0; i <= total_elements - vec_width; i += vec_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum of the vector\n    __m128i low = _mm256_castsi256_si128(sum_vec);\n    __m128i high = _mm256_extracti128_si256(sum_vec, 1);\n    low = _mm_add_epi32(low, high);\n    int intermediate_sum = _mm_cvtsi128_si32(_mm_hadd_epi32(low, low));\n\n    // Handle remaining elements that don't fit into full vectors\n    for (size_t i = (total_elements / vec_width) * vec_width; i < total_elements; ++i) {\n        intermediate_sum += matrix[i];\n    }\n\n    return intermediate_sum;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m256i v_broadcast = _mm256_set1_epi8(-128);\n    __m256i v_max = _mm256_set1_epi8(-128);\n    __m256i v_index = _mm256_setr_epi32(0,1,2,3,4,5,6,7);\n    __m256i v_result_index = _mm256_setzero_si256();\n    size_t max_idx = 0;\n    int8_t max_val = -128;\n\n    for(size_t i = 0; i + 31 < length; i += 32, src += 32){\n        __m256i v_src = _mm256_loadu_si256((__m256i*)src);\n        __m256i v_cmp = _mm256_cmpgt_epi8(v_src, v_max);\n        v_result_index = _mm256_blendv_epi8(v_result_index, _mm256_add_epi32(v_index, _mm256_set1_epi32(i/4)), v_cmp);\n        v_max = _mm256_max_epi8(v_max, v_src);\n    }\n\n    int cmp_res[8];\n    _mm256_storeu_si256((__m256i*)cmp_res, _mm256_cmpgt_epi8(v_max, v_broadcast));\n    for(int i = 0; i < 8; ++i){\n        if(cmp_res[i]){\n            max_val = ((int8_t*)_mm256_extracti128_si256(v_max, 0))[4*i];\n            max_idx = ((int*)_mm256_extracti128_si256(v_result_index, 0))[i];\n        }\n    }\n\n    for(size_t i = (length / 32) * 32; i < length; ++i){\n        if(src[i] > max_val){\n            max_val = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m256i cmp_mask = _mm256_set1_epi8(0xFF);\n    __m256i max_val = _mm256_set1_epi8(-128);\n    __m256i max_idx = _mm256_setzero_si256();\n    size_t ret_idx = 0;\n    size_t i = 0;\n\n    for (; i <= length - 32; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        __m256i gt = _mm256_cmpgt_epi8(vec, max_val);\n        max_val = _mm256_max_epi8(max_val, vec);\n        max_idx = _mm256_blendv_epi8(max_idx, _mm256_set1_epi64x(i), gt);\n    }\n\n    __m256i max_val_lo = _mm256_permute4x64_epi64(max_val, 0xD8);\n    __m256i max_val_hi = _mm256_permute4x64_epi64(max_val, 0xB1);\n    __m256i max_idx_lo = _mm256_permute4x64_epi64(max_idx, 0xD8);\n    __m256i max_idx_hi = _mm256_permute4x64_epi64(max_idx, 0xB1);\n\n    max_val = _mm256_max_epi8(max_val_lo, max_val_hi);\n    max_idx = _mm256_min_epi64(max_idx_lo, max_idx_hi);\n\n    __m128i max_val_128 = _mm256_castsi256_si128(max_val);\n    __m128i max_idx_128 = _mm256_castsi256_si128(max_idx);\n\n    __m128i max_val_128_hi = _mm256_extracti128_si256(max_val, 1);\n    __m128i max_idx_128_hi = _mm256_extracti128_si256(max_idx, 1);\n\n    max_val_128 = _mm_max_epi8(max_val_128, max_val_128_hi);\n    max_idx_128 = _mm_min_epi64(max_idx_128, max_idx_128_hi);\n\n    int max_val_int = _mm_cvtsi128_si32(_mm_max_epi8(max_val_128, _mm_shuffle_epi32(max_val_128, 0x55)));\n    __m128i max_idx_final = _mm_min_epi64(max_idx_128, _mm_shuffle_epi32(max_idx_128, 0x55));\n\n    ret_idx = _mm_cvtsi128_si32(max_idx_final);\n\n    for (; i < length; ++i) {\n        if (src[i] > src[ret_idx]) {\n            ret_idx = i;\n        }\n    }\n\n    return ret_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m256i cmp_mask = _mm256_set1_epi8(0xFF);\n    __m256i max_val = _mm256_set1_epi8(-128);\n    __m256i max_idx = _mm256_set1_epi32(0);\n\n    size_t idx = 0;\n    size_t aligned_length = length - (length % 32);\n\n    for (; idx < aligned_length; idx += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i *)(src + idx));\n        __m256i curr_max = _mm256_max_epi8(data, max_val);\n        __m256i mask = _mm256_cmpgt_epi8(curr_max, max_val);\n        max_val = _mm256_blendv_epi8(max_val, curr_max, mask);\n        __m256i indices = _mm256_set_epi32(idx + 31, idx + 30, idx + 29, idx + 28,\n                                          idx + 27, idx + 26, idx + 25, idx + 24,\n                                          idx + 23, idx + 22, idx + 21, idx + 20,\n                                          idx + 19, idx + 18, idx + 17, idx + 16);\n        max_idx = _mm256_blendv_epi8(max_idx, indices, mask);\n    }\n\n    int8_t max_value = -128;\n    size_t result_idx = 0;\n    __m256i res = _mm256_set1_epi8(0);\n    res = _mm256_max_epi8(res, max_val);\n    if (_mm256_movemask_epi8(_mm256_cmpeq_epi8(res, max_val))) {\n        max_value = _mm256_extract_epi8(res, 0);\n        result_idx = _mm256_extract_epi32(max_idx, 0);\n    }\n\n    for (; idx < length; ++idx) {\n        if (src[idx] > max_value) {\n            max_value = src[idx];\n            result_idx = idx;\n        }\n    }\n\n    return result_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m256i *vec_src = (const __m256i *)src;\n    size_t max_idx = 0;\n    int8_t max_val = src[0];\n\n    __m256i current_max = _mm256_set1_epi8(src[0]);\n    __m256i current_idx = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i idx_step = _mm256_set1_epi32(32);\n\n    size_t i = 0;\n    for (; i <= length - 32; vec_src++, i += 32) {\n        __m256i vec = _mm256_loadu_si256(vec_src);\n        __m256i gt_mask = _mm256_cmpgt_epi8(vec, current_max);\n\n        __m256i new_max = _mm256_blendv_epi8(current_max, vec, gt_mask);\n        __m256i new_idx = _mm256_blendv_epi8(current_idx, _mm256_add_epi32(current_idx, idx_step), gt_mask);\n\n        current_max = new_max;\n        current_idx = new_idx;\n\n        int any_gt = _mm256_movemask_epi8(gt_mask);\n        if (any_gt) {\n            int8_t temp_max = _mm256_extract_epi8(current_max, 0);\n            if (temp_max > max_val) {\n                max_val = temp_max;\n                max_idx = _mm256_extract_epi32(current_idx, 0);\n            }\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m256i cmp_mask = _mm256_set1_epi8(0xFF);\n    __m256i max_val = _mm256_set1_epi8(-128);\n    __m256i max_idx = _mm256_set1_epi32(0);\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m256i data_idx = _mm256_set_epi32(i+31, i+30, i+29, i+28, i+27, i+26, i+25, i+24,\n                                           i+23, i+22, i+21, i+20, i+19, i+18, i+17, i+16);\n\n        __m256i ge_mask = _mm256_cmpgt_epi8(data, max_val);\n        __m256i eq_mask = _mm256_cmpeq_epi8(data, max_val);\n\n        max_val = _mm256_max_epi8(max_val, data);\n\n        __m256i update_idx = _mm256_blendv_epi8(max_idx, data_idx, _mm256_andnot_si256(eq_mask, ge_mask));\n        max_idx = _mm256_blendv_epi8(update_idx, max_idx, eq_mask);\n    }\n\n    int8_t max_value = -128;\n    size_t result_idx = 0;\n\n    __m256i res_val = _mm256_permutevar8x32_epi32(_mm256_cvtepi8_epi32(max_val), _mm256_set_epi32(7,6,5,4,3,2,1,0));\n    __m256i res_idx = _mm256_permutevar8x32_epi32(max_idx, _mm256_set_epi32(7,6,5,4,3,2,1,0));\n\n    for (int j = 0; j < 8; ++j) {\n        int tmp_val = _mm256_extract_epi32(res_val, j);\n        if (tmp_val > max_value) {\n            max_value = tmp_val;\n            result_idx = _mm256_extract_epi32(res_idx, j);\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            result_idx = i;\n        }\n    }\n\n    return result_idx;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    for (; i <= length - 16; i += 16) {\n        __m256i indices = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i and_mask = _mm256_set1_epi32(mask);\n        __m256i cmp_res = _mm256_cmpgt_epi32(_mm256_and_si256(indices, and_mask), zero);\n        __m256i blend_mask = _mm256_packus_epi32(cmp_res, cmp_res);\n        \n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i v_dst = _mm256_blendv_epi8(v_src2, v_src1, blend_mask);\n        \n        _mm256_storeu_si256((__m256i*)&dst[i], v_dst);\n    }\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    for (; i <= length - 16; i += 16) {\n        __m256i indices = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i and_mask = _mm256_set1_epi32(mask);\n        __m256i cmp_res = _mm256_cmpgt_epi32(_mm256_and_si256(indices, and_mask), zero);\n        __m256i blend_mask = _mm256_slli_epi32(cmp_res, 15);\n\n        __m256i v_src1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v_src2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i v_dst = _mm256_blendv_epi8(v_src2, v_src1, blend_mask);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), v_dst);\n    }\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi16(1);\n\n    for (; i <= length - 16; i += 16) {\n        __m256i indices = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i m = _mm256_set1_epi32(mask);\n        __m256i and_res = _mm256_and_si256(indices, m);\n        __m256i cmp_res = _mm256_cmpeq_epi32(and_res, zero);\n        __m256i selector = _mm256_sub_epi32(ones, cmp_res);\n\n        __m256i v_src1 = _mm256_loadu_si256((const __m256i*)&src1[i]);\n        __m256i v_src2 = _mm256_loadu_si256((const __m256i*)&src2[i]);\n\n        __m256i blended = _mm256_blendv_epi8(v_src2, v_src1, _mm256_cvtepi32_epi8(selector));\n\n        _mm256_storeu_si256((__m256i*)&dst[i], blended);\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    for (; i + 16 <= length; i += 16) {\n        __m256i indices = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i and_mask = _mm256_set1_epi32(mask);\n        __m256i cond = _mm256_and_si256(indices, and_mask);\n        __m256i cmp_result = _mm256_cmpgt_epi32(cond, zero); // Non-zero check\n\n        __m256i vec_src1 = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i vec_src2 = _mm256_loadu_si256((__m256i const*)(src2 + i));\n\n        __m256i blended = _mm256_blendv_epi8(vec_src2, vec_src1, _mm256_castsi256_si128(cmp_result));\n\n        _mm256_storeu_si256((__m256i *)(dst + i), blended);\n    }\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi16(1);\n\n    for (; i <= length - 16; i += 16) {\n        __m256i indices = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i and_mask = _mm256_set1_epi32(mask);\n        __m256i cond = _mm256_and_si256(indices, and_mask);\n        __m256i cmp_res = _mm256_cmpgt_epi32(cond, zero);\n\n        __m256i vec_src1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec_src2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n\n        __m256i blend_mask = _mm256_cvtepi32_epi16(cmp_res);\n        __m256i select_src1 = _mm256_and_si256(blend_mask, vec_src1);\n        __m256i select_src2 = _mm256_andnot_si256(blend_mask, vec_src2);\n\n        __m256i result = _mm256_or_si256(select_src1, select_src2);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t vec_len = 8;\n    size_t i = 0;\n\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i popcnt = _mm256_popcnt_epi32(data);\n        _mm256_storeu_si256((__m256i *)(dst + i), popcnt);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t vec_len = 8;\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src[i]));\n        __m256i popcnt = _mm256_popcnt_epi64(data);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), popcnt);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t vec_size = 8;\n    size_t i = 0;\n\n    for (; i <= length - vec_size; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i popcnt = _mm256_popcnt_epi32(data);\n        _mm256_storeu_si256((__m256i *)(dst + i), popcnt);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t vec_len = 8;\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src[i]));\n        __m256i popcnt = _mm256_popcnt_epi32(data);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), popcnt);\n    }\n\n    // Process remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const size_t vec_len = 8;\n    size_t i = 0;\n\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src[i]));\n        __m256i popcnt = _mm256_popcnt_epi32(data);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), popcnt);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <stdint.h>\n#include <string.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i vzero = _mm256_setzero_si256();\n    const size_t len = length - (length % 16);\n    \n    for (; i <= len; i += 16) {\n        __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src1[i]));\n        __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src2[i]));\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), result);\n    }\n\n    // Handle remaining elements with scalar operations\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] > UINT16_MAX) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <string.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i max_val = _mm256_set1_epi16(0xFFFF);\n\n    for (; i + 16 <= length; i += 16) {\n        __m256i vec1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint16_t)((src1[i] + src2[i]) > 0xFFFF ? 0xFFFF : (src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <stdint.h>\n#include <string.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    const size_t vec_len = 16; // Number of elements in a YMM register for uint16_t\n\n    // Process vectors\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src1[i]));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src2[i]));\n        __m256i result = _mm256_adds_epu16(v1, v2);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), result);\n    }\n\n    // Process remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (uint16_t)((src1[i] + src2[i]) > UINT16_MAX ? UINT16_MAX : (src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <string.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t vec_size = 16; // Number of uint16_t elements in a YMM register\n    size_t i = 0;\n\n    for (; i <= length - vec_size; i += vec_size) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(src1 + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(src2 + i));\n        __m256i res = _mm256_adds_epu16(a, b);\n        _mm256_storeu_si256((__m256i *)(dst + i), res);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (uint16_t)((src1[i] + src2[i]) > UINT16_MAX ? UINT16_MAX : (src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <stdint.h>\n#include <string.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i max_val = _mm256_set1_epi16(0xFFFF);\n\n    for (; i + 15 < length; i += 16) {\n        __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src1[i]));\n        __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src2[i]));\n        __m256i result = _mm256_adds_epu16(vec1, vec2);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] > 0xFFFF) ? 0xFFFF : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m256d a_val = _mm256_broadcast_sd(A + i * n + k);\n            __m256d cmp_a_lower = _mm256_cmpgt_pd(a_val, lower_bound);\n            __m256d cmp_a_upper = _mm256_cmple_pd(a_val, upper_bound);\n            __m256d mask_a = _mm256_and_pd(cmp_a_lower, cmp_a_upper);\n\n            if (_mm256_testz_pd(mask_a, mask_a)) continue;\n\n            for (size_t j = 0; j < p; j += 4) {\n                __m256d b_val = _mm256_loadu_pd(B + k * p + j);\n                __m256d cmp_b_lower = _mm256_cmpgt_pd(b_val, lower_bound);\n                __m256d cmp_b_upper = _mm256_cmple_pd(b_val, upper_bound);\n                __m256d mask_b = _mm256_and_pd(cmp_b_lower, cmp_b_upper);\n\n                __m256d mask = _mm256_and_pd(mask_a, mask_b);\n                __m256d valid_b = _mm256_and_pd(mask, b_val);\n                __m256d product = _mm256_mul_pd(a_val, valid_b);\n\n                __m256d c_val = _mm256_loadu_pd(C + i * p + j);\n                __m256d result = _mm256_add_pd(c_val, product);\n                _mm256_storeu_pd(C + i * p + j, result);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m256d a_val = _mm256_broadcast_sd(A + i * n + k);\n            __m256d in_range_a = _mm256_and_ps(_mm256_cmp_pd(a_val, lower_bound, _CMP_GE_OS),\n                                              _mm256_cmp_pd(a_val, upper_bound, _CMP_LE_OS));\n            in_range_a = _mm256_andnot_pd(in_range_a, zero);\n\n            for (size_t j = 0; j < p; j += 4) {\n                __m256d b_val = _mm256_loadu_pd(B + k * p + j);\n                __m256d in_range_b = _mm256_and_ps(_mm256_cmp_pd(b_val, lower_bound, _CMP_GE_OS),\n                                                 _mm256_cmp_pd(b_val, upper_bound, _CMP_LE_OS));\n                in_range_b = _mm256_andnot_pd(in_range_b, zero);\n\n                __m256d mask = _mm256_or_pd(in_range_a, in_range_b);\n                b_val = _mm256_andnot_pd(mask, b_val);\n\n                __m256d product = _mm256_mul_pd(a_val, b_val);\n\n                __m256d c_val = _mm256_loadu_pd(C + i * p + j);\n                c_val = _mm256_add_pd(c_val, product);\n                _mm256_storeu_pd(C + i * p + j, c_val);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m256d a_val = _mm256_broadcast_sd(A + i * n + k);\n\n            // Check if a_val is within the range [-100, 100]\n            __m256d cmp_ge_lower = _mm256_cmp_pd(a_val, lower_bound, _CMP_GE_OS);\n            __m256d cmp_le_upper = _mm256_cmp_pd(a_val, upper_bound, _CMP_LE_OS);\n            __m256d in_range = _mm256_and_pd(cmp_ge_lower, cmp_le_upper);\n\n            for (size_t j = 0; j < p; j += 4) {\n                __m256d b_val = _mm256_loadu_pd(B + k * p + j);\n                __m256d c_val = _mm256_loadu_pd(C + i * p + j);\n\n                // Check if b_val is within the range [-100, 100]\n                __m256d cmp_b_ge_lower = _mm256_cmp_pd(b_val, lower_bound, _CMP_GE_OS);\n                __m256d cmp_b_le_upper = _mm256_cmp_pd(b_val, upper_bound, _CMP_LE_OS);\n                __m256d b_in_range = _mm256_and_pd(cmp_b_ge_lower, cmp_b_le_upper);\n\n                // Combine ranges\n                __m256d combined_range = _mm256_and_pd(in_range, b_in_range);\n\n                // Mask out values outside the range\n                __m256d masked_a = _mm256_and_pd(a_val, combined_range);\n                __m256d masked_b = _mm256_and_pd(b_val, combined_range);\n\n                // Multiply and add to C\n                __m256d product = _mm256_mul_pd(masked_a, masked_b);\n                __m256d sum = _mm256_add_pd(c_val, product);\n\n                _mm256_storeu_pd(C + i * p + j, sum);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            size_t k = 0;\n\n            // AVX2 vectorized loop\n            for (; k + 4 <= n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n\n                __m256d mask_a = _mm256_and_ps(\n                    _mm256_cmp_pd(a_vec, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(a_vec, upper_bound, _CMP_LE_OQ)\n                );\n\n                __m256d mask_b = _mm256_and_ps(\n                    _mm256_cmp_pd(b_vec, lower_bound, _CMP_GE_OQ),\n                    _mm256_cmp_pd(b_vec, upper_bound, _CMP_LE_OQ)\n                );\n\n                __m256d masked_a = _mm256_and_pd(a_vec, mask_a);\n                __m256d masked_b = _mm256_and_pd(b_vec, mask_b);\n\n                __m256d product = _mm256_mul_pd(masked_a, masked_b);\n                __m256d sums = _mm256_set1_pd(sum);\n                sums = _mm256_add_pd(sums, product);\n\n                double temp_sums[4];\n                _mm256_storeu_pd(temp_sums, sums);\n                sum += temp_sums[0] + temp_sums[1] + temp_sums[2] + temp_sums[3];\n            }\n\n            // Scalar loop for remainder\n            for (; k < n; ++k) {\n                double a_val = A[i * n + k];\n                double b_val = B[k * p + j];\n                if (a_val >= -100.0 && a_val <= 100.0 && b_val >= -100.0 && b_val <= 100.0) {\n                    sum += a_val * b_val;\n                }\n            }\n\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256d lower_bound = _mm256_set1_pd(-100.0);\n    __m256d upper_bound = _mm256_set1_pd(100.0);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m256d a_val = _mm256_set1_pd(A[i * n + k]);\n            __m256d in_range_a = _mm256_and_pd(_mm256_cmpge_pd(a_val, lower_bound), _mm256_cmple_pd(a_val, upper_bound));\n            in_range_a = _mm256_and_pd(in_range_a, _mm256_castsi256_pd(_mm256_cmpeq_epi32(_mm256_castpd_si256(a_val), _mm256_castpd_si256(a_val))));\n\n            for (size_t j = 0; j < p; j += 4) {\n                __m256d b_val = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d in_range_b = _mm256_and_pd(_mm256_cmpge_pd(b_val, lower_bound), _mm256_cmple_pd(b_val, upper_bound));\n                in_range_b = _mm256_and_pd(in_range_b, _mm256_castsi256_pd(_mm256_cmpeq_epi32(_mm256_castpd_si256(b_val), _mm256_castpd_si256(b_val))));\n\n                __m256d mask = _mm256_and_pd(in_range_a, in_range_b);\n                __m256d product = _mm256_mul_pd(a_val, b_val);\n                product = _mm256_and_pd(product, mask);\n\n                __m256d c_val = _mm256_loadu_pd(&C[i * p + j]);\n                c_val = _mm256_add_pd(c_val, product);\n                _mm256_storeu_pd(&C[i * p + j], c_val);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    // Process elements in chunks of 4 using AVX2\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)&C[i], c_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i va, vb, vc;\n\n    for (; i + 3 < total_elements; i += 4) {\n        va = _mm256_loadu_si256((__m256i const*)(A + i));\n        vb = _mm256_loadu_si256((__m256i const*)(B + i));\n        vc = _mm256_add_epi64(va, vb);\n        _mm256_storeu_si256((__m256i *)(C + i), vc);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i va, vb, vc;\n\n    for (; i + 3 <= total_elements; i += 4) {\n        va = _mm256_loadu_si256((__m256i const *)(A + i));\n        vb = _mm256_loadu_si256((__m256i const *)(B + i));\n        vc = _mm256_add_epi64(va, vb);\n        _mm256_storeu_si256((__m256i *)(C + i), vc);\n    }\n\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i;\n\n    // Process elements in chunks of 4 using AVX2\n    for (i = 0; i + 3 < total_elements; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i c_vec = _mm256_add_epi64(a_vec, b_vec);\n        _mm256_storeu_si256((__m256i*)&C[i], c_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t aligned_elements = total_elements / 4 * 4;\n\n    for (size_t i = 0; i < aligned_elements; i += 4) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n        __m256i vec_c = _mm256_add_epi64(vec_a, vec_b);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), vec_c);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 round_const = _mm256_set1_ps(0.5f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(_mm256_add_ps(sum, round_const), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 round_const = _mm256_set1_ps(0.5f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(_mm256_add_ps(sum, round_const), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```cpp\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 round_const = _mm256_set1_ps(0.5f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = static_cast<int>(src1[i] + src2[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 round_control = _mm256_set1_ps(0.5f); // For rounding to nearest integer\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m256 round_const = _mm256_set1_ps(0.5f);\n\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n        __m256 sum = _mm256_add_ps(vec1, vec2);\n        __m256 rounded = _mm256_round_ps(_mm256_add_ps(sum, round_const), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const __m256 abs_diff_threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const int mask = 0x55555555; // Mask for indices that are multiples of 2\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 vec_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec_src2 = _mm256_loadu_ps(src2 + i);\n\n        __m256 diff = _mm256_sub_ps(vec_src1, vec_src2);\n        __m256 abs_diff = _mm256_max_ps(diff, _mm256_sub_ps(zero, diff));\n\n        __m256 cond1 = _mm256_cmp_ps(abs_diff, abs_diff_threshold, _CMP_LE_OS);\n\n        __m256 mul_res = _mm256_mul_ps(vec_src1, vec_src2);\n        __m256 ceil_res = _mm256_ceil_ps(mul_res);\n\n        // Create index mask for multiples of 2\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i even_mask = _mm256_and_si256(indices, _mm256_set1_epi32(mask));\n        __m256 cond2 = _mm256_castsi256_ps(_mm256_cmpeq_epi32(even_mask, indices));\n\n        __m256 final_cond = _mm256_and_ps(cond1, cond2);\n\n        __m256 result = _mm256_blendv_ps(neg_one, ceil_res, final_cond);\n\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    for (; i < length; ++i) {\n        if (i % 2 == 0 && fabs(src1[i] - src2[i]) <= 50.0f) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const __m256 cond = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const size_t aligned_length = length / 8 * 8;\n\n    for(size_t i = 0; i < aligned_length; i += 8){\n        if(i % 2 == 0){\n            __m256 v1 = _mm256_loadu_ps(src1 + i);\n            __m256 v2 = _mm256_loadu_ps(src2 + i);\n\n            __m256 diff = _mm256_sub_ps(v1, v2);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_castsi256_ps(_mm256_set1_epi32(0x80000000)), diff);\n            __m256 mask = _mm256_cmp_ps(abs_diff, cond, _CMP_LE_OS);\n\n            __m256 mul = _mm256_mul_ps(v1, v2);\n            __m256 ceil_val = _mm256_ceil_ps(mul);\n\n            __m256 result = _mm256_blendv_ps(neg_one, ceil_val, mask);\n            _mm256_storeu_ps(dst + i, result);\n        }\n        else{\n            __m256 neg_one_vec = _mm256_set1_ps(-1.0f);\n            _mm256_storeu_ps(dst + i, neg_one_vec);\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = aligned_length; i < length; ++i){\n        if(i % 2 == 0){\n            float diff = src1[i] - src2[i];\n            if(diff >= -50.0f && diff <= 50.0f){\n                dst[i] = std::ceil(src1[i] * src2[i]);\n            }\n            else{\n                dst[i] = -1.0f;\n            }\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const __m256 abs_diff_threshold = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const size_t aligned_length = length / 8 * 8;\n\n    for(size_t i = 0; i < aligned_length; i += 8){\n        __m256 vec_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec_src2 = _mm256_loadu_ps(src2 + i);\n\n        // Calculate absolute difference\n        __m256 diff = _mm256_sub_ps(vec_src1, vec_src2);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_castsi256_ps(_mm256_set1_epi32(0x80000000)), diff);\n\n        // Check if absolute difference is within threshold\n        __m256 mask_within_range = _mm256_cmp_ps(abs_diff, abs_diff_threshold, _CMP_LE_OS);\n\n        // Create index mask (index is multiple of 2)\n        __m256i indices = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n        indices = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n        __m256i even_mask = _mm256_and_si256(_mm256_srli_epi32(indices, 1), _mm256_set1_epi32(1));\n        __m256 index_mask = _mm256_castsi256_ps(_mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256()));\n\n        // Combine masks\n        __m256 combined_mask = _mm256_and_ps(mask_within_range, index_mask);\n\n        // Perform multiplication\n        __m256 mul_result = _mm256_mul_ps(vec_src1, vec_src2);\n\n        // Round up toward positive infinity\n        __m256 rounded_result = _mm256_ceil_ps(mul_result);\n\n        // Select results based on combined mask\n        __m256 result = _mm256_blendv_ps(neg_one, rounded_result, combined_mask);\n\n        // Store the result\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for(size_t i = aligned_length; i < length; ++i){\n        if( ((i % 2) == 0) && (fabs(src1[i] - src2[i]) <= 50.0f) ){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const __m256 diff_limit = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const __m256 zero = _mm256_setzero_ps();\n    const int mask = 0xAAAA; // Binary: 1010101010101010 -> to check if index is multiple of 2\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n\n        // Calculate absolute difference\n        __m256 abs_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 abs_diff_cmp = _mm256_andnot_ps(_mm256_cmp_ps(abs_diff, zero, _CMP_LT_OS), abs_diff);\n        \n        // Check if abs_diff is within 50\n        __m256 cond1 = _mm256_cmp_ps(abs_diff_cmp, diff_limit, _CMP_LE_OS);\n\n        // Create a mask for indices that are multiples of 2\n        __m256i idx = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(idx, _mm256_set1_epi32(mask)), _mm256_setzero_si256());\n        __m256 cond2 = _mm256_castsi256_ps(even_mask);\n\n        // Combine conditions\n        __m256 combined_cond = _mm256_and_ps(cond1, _mm256_castsi256_ps(cond2));\n\n        // Perform multiplication\n        __m256 mul_result = _mm256_mul_ps(v_src1, v_src2);\n\n        // Round up toward positive infinity\n        __m256 ceil_result = _mm256_ceil_ps(mul_result);\n\n        // Select -1 or result based on condition\n        __m256 result = _mm256_blendv_ps(neg_one, ceil_result, combined_cond);\n\n        // Store the result\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        if ((i % 2 == 0) && (fabs(src1[i] - src2[i]) <= 50.0f)) {\n            dst[i] = std::ceil(src1[i] * src2[i]);\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    const __m256 cmp_val = _mm256_set1_ps(50.0f);\n    const __m256 neg_one = _mm256_set1_ps(-1.0f);\n    const size_t aligned_length = length / 8 * 8;\n\n    for(size_t i = 0; i < aligned_length; i += 8){\n        __m256 vec1 = _mm256_loadu_ps(src1 + i);\n        __m256 vec2 = _mm256_loadu_ps(src2 + i);\n\n        __m256 diff = _mm256_sub_ps(vec1, vec2);\n        __m256 abs_diff = _mm256_max_ps(diff, _mm256_sub_ps(_mm256_setzero_ps(), diff));\n        __m256 mask_cmp = _mm256_cmp_ps(abs_diff, cmp_val, _CMP_LE_OS);\n\n        __m256 indices = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n        __m256 even_mask = _mm256_cmp_epi32(indices, _mm256_setzero_si256(), _CMP_EQ_OQ);\n\n        __m256 mask = _mm256_and_ps(mask_cmp, _mm256_castsi256_ps(even_mask));\n\n        __m256 mul_res = _mm256_mul_ps(vec1, vec2);\n        __m256 ceil_res = _mm256_ceil_ps(mul_res);\n\n        __m256 res = _mm256_blendv_ps(neg_one, ceil_res, mask);\n\n        _mm256_storeu_ps(dst + i, res);\n    }\n\n    for(size_t i = aligned_length; i < length; ++i){\n        if((i % 2 == 0) && (fabs(src1[i] - src2[i]) <= 50)){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i;\n\n    __m256d vscalar = _mm256_set1_pd(scalar);\n\n    for (i = 0; i + 4 <= total; i += 4) {\n        __m256d vmat = _mm256_loadu_pd(mat + i);\n        __m256d vresult = _mm256_mul_pd(vmat, vscalar);\n        _mm256_storeu_pd(dst + i, vresult);\n    }\n\n    for (; i < total; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m256d s = _mm256_set1_pd(scalar);\n\n    size_t i = 0;\n    for (; i + 3 < total; i += 4) {\n        __m256d vec = _mm256_loadu_pd(mat + i);\n        __m256d res = _mm256_mul_pd(vec, s);\n        _mm256_storeu_pd(dst + i, res);\n    }\n\n    for (; i < total; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t aligned_size = total / 4 * 4;\n    __m256d vscalar = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m256d vmat = _mm256_loadu_pd(mat + i);\n        __m256d vresult = _mm256_mul_pd(vmat, vscalar);\n        _mm256_storeu_pd(dst + i, vresult);\n    }\n\n    for (size_t i = aligned_size; i < total; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m256d vscalar = _mm256_set1_pd(scalar);\n\n    size_t i = 0;\n    for (; i <= total - 4; i += 4) {\n        __m256d vmat = _mm256_loadu_pd(mat + i);\n        __m256d vresult = _mm256_mul_pd(vmat, vscalar);\n        _mm256_storeu_pd(dst + i, vresult);\n    }\n\n    for (; i < total; i++) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m256d vscalar = _mm256_set1_pd(scalar);\n\n    size_t i;\n    for (i = 0; i + 3 < total; i += 4) {\n        __m256d vmat = _mm256_loadu_pd(&mat[i]);\n        __m256d vresult = _mm256_mul_pd(vmat, vscalar);\n        _mm256_storeu_pd(&dst[i], vresult);\n    }\n\n    for (; i < total; ++i) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const size_t vector_size = 8; // Number of floats in a YMM register\n    __m256 broadcast_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m256 sum_squares = _mm256_setzero_ps();\n\n        // Process full vectors\n        size_t j = 0;\n        for (; j + vector_size <= n; j += vector_size) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            __m256 squared = _mm256_mul_ps(vec, vec);\n            sum_squares = _mm256_add_ps(sum_squares, squared);\n        }\n\n        // Horizontal sum of squares\n        float reduction[8] __attribute__((aligned(32)));\n        _mm256_store_ps(reduction, sum_squares);\n        float hsum = reduction[0] + reduction[1] + reduction[2] + reduction[3] +\n                    reduction[4] + reduction[5] + reduction[6] + reduction[7];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            hsum += row[j] * row[j];\n        }\n\n        // Compute L2 norm\n        float norm = sqrtf(hsum);\n        if (norm == 0.0f) norm = 1.0f; // Avoid division by zero\n\n        __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n\n        // Normalize the row\n        j = 0;\n        for (; j + vector_size <= n; j += vector_size) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            __m256 normalized = _mm256_mul_ps(vec, inv_norm);\n            _mm256_storeu_ps(&row[j], normalized);\n        }\n\n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const size_t vector_size = 8; // Number of floats in a YMM register (AVX)\n    __m256 broadcast_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        size_t row_start = i * n;\n        float norm_sq = 0.0f;\n\n        // Calculate L2-norm squared using AVX\n        for (size_t j = 0; j < n; j += vector_size) {\n            __m256 vec = _mm256_loadu_ps(mat + row_start + j);\n            __m256 vec_sq = _mm256_mul_ps(vec, vec);\n            norm_sq += horizontal_add_ps(vec_sq);\n        }\n\n        // Handle remaining elements if n is not a multiple of vector_size\n        size_t remainder = n % vector_size;\n        if (remainder > 0) {\n            alignas(32) float temp[8] = {0};\n            for (size_t j = 0; j < remainder; ++j) {\n                temp[j] = mat[row_start + n - remainder + j];\n            }\n            __m256 vec = _mm256_load_ps(temp);\n            __m256 vec_sq = _mm256_mul_ps(vec, vec);\n            norm_sq += horizontal_add_ps(vec_sq);\n        }\n\n        // Compute the L2-norm and avoid division by zero\n        float norm = sqrtf(norm_sq);\n        if (norm == 0.0f) continue; // Skip normalization if norm is zero\n\n        __m256 broadcast_norm = _mm256_set1_ps(norm);\n\n        // Normalize the row elements\n        for (size_t j = 0; j < n; j += vector_size) {\n            __m256 vec = _mm256_loadu_ps(mat + row_start + j);\n            __m256 normalized_vec = _mm256_div_ps(vec, broadcast_norm);\n            _mm256_storeu_ps(mat + row_start + j, normalized_vec);\n        }\n\n        // Normalize the remaining elements\n        size_t remainder_j = n % vector_size;\n        if (remainder_j > 0) {\n            alignas(32) float temp[8];\n            for (size_t j = 0; j < remainder_j; ++j) {\n                temp[j] = mat[row_start + n - remainder_j + j];\n            }\n            __m256 vec = _mm256_load_ps(temp);\n            __m256 normalized_vec = _mm256_div_ps(vec, broadcast_norm);\n            _mm256_store_ps(temp, normalized_vec);\n            for (size_t j = 0; j < remainder_j; ++j) {\n                mat[row_start + n - remainder_j + j] = temp[j];\n            }\n        }\n    }\n}\n\n// Helper function to compute horizontal sum of __m256\nfloat horizontal_add_ps(__m256 v) {\n    __m128 vlow  = _mm256_castps256_ps128(v);\n    __m128 vhigh = _mm256_extractf128_ps(v, 1);\n    vlow = _mm_add_ps(vlow, vhigh);\n    vlow = _mm_hadd_ps(vlow, vlow);\n    vlow = _mm_hadd_ps(vlow, vlow);\n    return _mm_cvtss_f32(vlow);\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const size_t vec_width = 8; // Number of floats processed per AVX vector (256 bits)\n    __m256 broadcast_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m256 sum_vec = _mm256_setzero_ps();\n\n        // Compute sum of squares using SIMD\n        for (size_t j = 0; j < n; j += vec_width) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            __m256 squared = _mm256_mul_ps(vec, vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n\n        // Horizontal sum of the partial sums in sum_vec\n        float sum_arr[8];\n        _mm256_storeu_ps(sum_arr, sum_vec);\n        float sum_of_squares = 0.0f;\n        for (size_t j = 0; j < vec_width; ++j) {\n            sum_of_squares += sum_arr[j];\n        }\n\n        // Handle remaining elements if n is not a multiple of vec_width\n        for (size_t j = (n / vec_width) * vec_width; j < n; ++j) {\n            sum_of_squares += row[j] * row[j];\n        }\n\n        // Compute L2 norm\n        float norm = sqrtf(sum_of_squares);\n\n        // Avoid division by zero\n        if (norm == 0.0f) {\n            continue;\n        }\n\n        // Normalize the row\n        __m256 norm_vec = _mm256_set1_ps(norm);\n\n        for (size_t j = 0; j < n; j += vec_width) {\n            __m256 vec = _mm256_loadu_ps(row + j);\n            __m256 normalized = _mm256_div_ps(vec, norm_vec);\n            _mm256_storeu_ps(row + j, normalized);\n        }\n\n        // Handle remaining elements if n is not a multiple of vec_width\n        for (size_t j = (n / vec_width) * vec_width; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const size_t simd_width = 8; // AVX processes 8 floats at a time (256 bits)\n    __m256 broadcast_one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; ++i) {\n        size_t row_offset = i * n;\n        float* row = &mat[row_offset];\n        \n        // Compute L2 norm of the row\n        __m256 sum_of_squares = _mm256_setzero_ps();\n        for (size_t j = 0; j < n; j += simd_width) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            __m256 squared = _mm256_mul_ps(vec, vec);\n            sum_of_squares = _mm256_add_ps(sum_of_squares, squared);\n        }\n\n        // Horizontal sum of squares\n        float norm_squared[8] = {0};\n        _mm256_storeu_ps(norm_squared, sum_of_squares);\n        float total_norm_squared = 0.0f;\n        for (size_t k = 0; k < simd_width; ++k) {\n            total_norm_squared += norm_squared[k];\n        }\n\n        // Handle remaining elements if n is not a multiple of simd_width\n        for (size_t j = (n / simd_width) * simd_width; j < n; ++j) {\n            total_norm_squared += row[j] * row[j];\n        }\n\n        float norm = sqrtf(total_norm_squared);\n        if (norm == 0.0f) continue; // Avoid division by zero\n\n        __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n\n        // Normalize the row\n        for (size_t j = 0; j < n; j += simd_width) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            __m256 normalized = _mm256_mul_ps(vec, inv_norm);\n            _mm256_storeu_ps(&row[j], normalized);\n        }\n\n        // Handle remaining elements if n is not a multiple of simd_width\n        for (size_t j = (n / simd_width) * simd_width; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    const size_t vector_size = 8; // Number of floats in a __m256 register\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m256 sum_vec = _mm256_setzero_ps();\n        \n        // Process full vectors\n        size_t j = 0;\n        for (; j + vector_size <= n; j += vector_size) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            __m256 squared = _mm256_mul_ps(vec, vec);\n            sum_vec = _mm256_add_ps(sum_vec, squared);\n        }\n\n        // Horizontal sum of the partial sums\n        float partial_sums[8];\n        _mm256_storeu_ps(partial_sums, sum_vec);\n        float sum = partial_sums[0] + partial_sums[1] + partial_sums[2] + partial_sums[3] +\n                   partial_sums[4] + partial_sums[5] + partial_sums[6] + partial_sums[7];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            sum += row[j] * row[j];\n        }\n\n        // Compute L2 norm\n        float norm = sqrtf(sum);\n        if (norm == 0.0f) continue; // Avoid division by zero\n\n        // Normalize the row\n        sum_vec = _mm256_set1_ps(norm);\n        j = 0;\n        for (; j + vector_size <= n; j += vector_size) {\n            __m256 vec = _mm256_loadu_ps(&row[j]);\n            vec = _mm256_div_ps(vec, sum_vec);\n            _mm256_storeu_ps(&row[j], vec);\n        }\n\n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t j = 0; j < n; j += 4) {\n                __m256d vec_mat1 = _mm256_loadu_pd(&mat1[i * n + j]);\n                __m256d vec_mat2 = _mm256_loadu_pd(&mat2[j * p + k]);\n                __m256d prod = _mm256_mul_pd(vec_mat1, vec_mat2);\n                sum_vec = _mm256_add_pd(sum_vec, prod);\n            }\n            double sum_arr[4];\n            _mm256_storeu_pd(sum_arr, sum_vec);\n            double total_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n            for (size_t j = (n / 4) * 4; j < n; ++j) {\n                total_sum += mat1[i * n + j] * mat2[j * p + k];\n            }\n            dst[i * p + k] = std::round(total_sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t j = 0; j < n; j += 4) {\n                __m256d vmat1 = _mm256_loadu_pd(&mat1[i * n + j]);\n                __m256d vmat2 = _mm256_set_pd(mat2[j + 3 * n + k], mat2[j + 2 * n + k], mat2[j + 1 * n + k], mat2[j + 0 * n + k]);\n                __m256d prod = _mm256_mul_pd(vmat1, vmat2);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            // Horizontal add and round\n            __m128d sum_low = _mm256_extractf128_pd(sum, 0);\n            __m128d sum_high = _mm256_extractf128_pd(sum, 1);\n            __m128d sum_all = _mm_add_pd(sum_low, sum_high);\n            __m128d sum_hadd = _mm_hadd_pd(sum_all, sum_all);\n            double result;\n            _mm_store_sd(&result, sum_hadd);\n            dst[i * p + k] = nearbyint(result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m256d vec_a = _mm256_broadcast_sd(mat1 + i * n + k);\n            for (size_t j = 0; j < p; j += 4) {\n                __m256d vec_b = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d prod = _mm256_mul_pd(vec_a, vec_b);\n                __m256d* dst_ptr = (__m256d*)(dst + i * p + j);\n                if (j == 0) {\n                    _mm256_storeu_pd(dst_ptr, prod);\n                } else {\n                    __m256d prev = _mm256_loadu_pd(dst_ptr);\n                    __m256d sum = _mm256_add_pd(prev, prod);\n                    _mm256_storeu_pd(dst_ptr, sum);\n                }\n            }\n        }\n    }\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 4) {\n            __m256d val = _mm256_loadu_pd(dst + i * p + j);\n            __m256d rounded = _mm256_round_pd(val, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d vmat1 = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d vmat2 = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d prod = _mm256_mul_pd(vmat1, vmat2);\n                sum = _mm256_add_pd(sum, prod);\n            }\n            __m128d low = _mm256_castpd256_pd128(sum);\n            __m128d high = _mm256_extractf128_pd(sum, 1);\n            __m128d total = _mm_add_pd(low, high);\n            double temp[2] = {0};\n            _mm_storeu_pd(temp, total);\n            dst[i * p + j] = nearbyint(temp[0] + temp[1]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d vec_a = _mm256_loadu_pd(&mat1[i * n + k]);\n                __m256d vec_b = _mm256_loadu_pd(&mat2[k * p + j]);\n                __m256d prod = _mm256_mul_pd(vec_a, vec_b);\n                sum_vec = _mm256_add_pd(sum_vec, prod);\n            }\n            double sum[4];\n            _mm256_storeu_pd(sum, sum_vec);\n            double total = sum[0] + sum[1] + sum[2] + sum[3];\n            for (size_t k = (n / 4) * 4; k < n; ++k) {\n                total += mat1[i * n + k] * mat2[k * p + j];\n            }\n            dst[i * p + j] = nearbyint(total);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 8) == 0) {\n        for (size_t i = 0; i < rows; i += 8) {\n            for (size_t j = 0; j < cols; j += 8) {\n                __m256 row0 = _mm256_loadu_ps(src + i + 0 * rows + j);\n                __m256 row1 = _mm256_loadu_ps(src + i + 1 * rows + j);\n                __m256 row2 = _mm256_loadu_ps(src + i + 2 * rows + j);\n                __m256 row3 = _mm256_loadu_ps(src + i + 3 * rows + j);\n                __m256 row4 = _mm256_loadu_ps(src + i + 4 * rows + j);\n                __m256 row5 = _mm256_loadu_ps(src + i + 5 * rows + j);\n                __m256 row6 = _mm256_loadu_ps(src + i + 6 * rows + j);\n                __m256 row7 = _mm256_loadu_ps(src + i + 7 * rows + j);\n\n                __m256 col0 = _mm256_shuffle_ps(row0, row4, _MM_SHUFFLE(1, 0, 1, 0));\n                __m256 col1 = _mm256_shuffle_ps(row0, row4, _MM_SHUFFLE(3, 2, 3, 2));\n                __m256 col2 = _mm256_shuffle_ps(row1, row5, _MM_SHUFFLE(1, 0, 1, 0));\n                __m256 col3 = _mm256_shuffle_ps(row1, row5, _MM_SHUFFLE(3, 2, 3, 2));\n                __m256 col4 = _mm256_shuffle_ps(row2, row6, _MM_SHUFFLE(1, 0, 1, 0));\n                __m256 col5 = _mm256_shuffle_ps(row2, row6, _MM_SHUFFLE(3, 2, 3, 2));\n                __m256 col6 = _mm256_shuffle_ps(row3, row7, _MM_SHUFFLE(1, 0, 1, 0));\n                __m256 col7 = _mm256_shuffle_ps(row3, row7, _MM_SHUFFLE(3, 2, 3, 2));\n\n                __m256 tmp0 = _mm256_permute2f128_ps(col0, col2, 0x20);\n                __m256 tmp1 = _mm256_permute2f128_ps(col0, col2, 0x31);\n                __m256 tmp2 = _mm256_permute2f128_ps(col1, col3, 0x20);\n                __m256 tmp3 = _mm256_permute2f128_ps(col1, col3, 0x31);\n                __m256 tmp4 = _mm256_permute2f128_ps(col4, col6, 0x20);\n                __m256 tmp5 = _mm256_permute2f128_ps(col4, col6, 0x31);\n                __m256 tmp6 = _mm256_permute2f128_ps(col5, col7, 0x20);\n                __m256 tmp7 = _mm256_permute2f128_ps(col5, col7, 0x31);\n\n                __m256 res0 = _mm256_blend_ps(tmp0, tmp4, 0xcc);\n                __m256 res1 = _mm256_blend_ps(tmp1, tmp5, 0xcc);\n                __m256 res2 = _mm256_blend_ps(tmp2, tmp6, 0xcc);\n                __m256 res3 = _mm256_blend_ps(tmp3, tmp7, 0xcc);\n                __m256 res4 = _mm256_permute2f128_ps(tmp0, tmp4, 0x21);\n                __m256 res5 = _mm256_permute2f128_ps(tmp1, tmp5, 0x21);\n                __m256 res6 = _mm256_permute2f128_ps(tmp2, tmp6, 0x21);\n                __m256 res7 = _mm256_permute2f128_ps(tmp3, tmp7, 0x21);\n\n                __m256 factor = _mm256_set1_ps(4.0f);\n                res0 = _mm256_round_ps(_mm256_mul_ps(res0, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                res1 = _mm256_round_ps(_mm256_mul_ps(res1, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                res2 = _mm256_round_ps(_mm256_mul_ps(res2, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                res3 = _mm256_round_ps(_mm256_mul_ps(res3, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                res4 = _mm256_round_ps(_mm256_mul_ps(res4, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                res5 = _mm256_round_ps(_mm256_mul_ps(res5, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                res6 = _mm256_round_ps(_mm256_mul_ps(res6, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                res7 = _mm256_round_ps(_mm256_mul_ps(res7, factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n                res0 = _mm256_div_ps(res0, factor);\n                res1 = _mm256_div_ps(res1, factor);\n                res2 = _mm256_div_ps(res2, factor);\n                res3 = _mm256_div_ps(res3, factor);\n                res4 = _mm256_div_ps(res4, factor);\n                res5 = _mm256_div_ps(res5, factor);\n                res6 = _mm256_div_ps(res6, factor);\n                res7 = _mm256_div_ps(res7, factor);\n\n                _mm256_storeu_ps(dst + j + 0 * cols + i, res0);\n                _mm256_storeu_ps(dst + j + 1 * cols + i, res1);\n                _mm256_storeu_ps(dst + j + 2 * cols + i, res2);\n                _mm256_storeu_ps(dst + j + 3 * cols + i, res3);\n                _mm256_storeu_ps(dst + j + 4 * cols + i, res4);\n                _mm256_storeu_ps(dst + j + 5 * cols + i, res5);\n                _mm256_storeu_ps(dst + j + 6 * cols + i, res6);\n                _mm256_storeu_ps(dst + j + 7 * cols + i, res7);\n            }\n        }\n    } else {\n        // Fallback to scalar implementation or smaller block sizes\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 8 == 0)) {\n        size_t i, j;\n        for(i = 0; i < rows; i += 8){\n            __m256 row0 = _mm256_loadu_ps(src + i * cols);\n            __m256 row1 = _mm256_loadu_ps(src + (i+1) * cols);\n            __m256 row2 = _mm256_loadu_ps(src + (i+2) * cols);\n            __m256 row3 = _mm256_loadu_ps(src + (i+3) * cols);\n            __m256 row4 = _mm256_loadu_ps(src + (i+4) * cols);\n            __m256 row5 = _mm256_loadu_ps(src + (i+5) * cols);\n            __m256 row6 = _mm256_loadu_ps(src + (i+6) * cols);\n            __m256 row7 = _mm256_loadu_ps(src + (i+7) * cols);\n\n            __m256 transposed[8];\n            transposed[0] = _mm256_shuffle_ps(row0, row4, _MM_SHUFFLE(1, 0, 1, 0));\n            transposed[1] = _mm256_shuffle_ps(row0, row4, _MM_SHUFFLE(3, 2, 3, 2));\n            transposed[2] = _mm256_shuffle_ps(row1, row5, _MM_SHUFFLE(1, 0, 1, 0));\n            transposed[3] = _mm256_shuffle_ps(row1, row5, _MM_SHUFFLE(3, 2, 3, 2));\n            transposed[4] = _mm256_shuffle_ps(row2, row6, _MM_SHUFFLE(1, 0, 1, 0));\n            transposed[5] = _mm256_shuffle_ps(row2, row6, _MM_SHUFFLE(3, 2, 3, 2));\n            transposed[6] = _mm256_shuffle_ps(row3, row7, _MM_SHUFFLE(1, 0, 1, 0));\n            transposed[7] = _mm256_shuffle_ps(row3, row7, _MM_SHUFFLE(3, 2, 3, 2));\n\n            transposed[0] = _mm256_permute2f128_ps(transposed[0], transposed[2], 0x20);\n            transposed[1] = _mm256_permute2f128_ps(transposed[1], transposed[3], 0x20);\n            transposed[2] = _mm256_permute2f128_ps(transposed[4], transposed[6], 0x20);\n            transposed[3] = _mm256_permute2f128_ps(transposed[5], transposed[7], 0x20);\n\n            transposed[0] = _mm256_blend_ps(transposed[0], transposed[2], 0xCC);\n            transposed[1] = _mm256_blend_ps(transposed[1], transposed[3], 0xCC);\n\n            __m256 round_const = _mm256_set1_ps(0.25f);\n            transposed[0] = _mm256_round_ps(_mm256_mul_ps(transposed[0], round_const), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            transposed[1] = _mm256_round_ps(_mm256_mul_ps(transposed[1], round_const), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            _mm256_storeu_ps(dst + i + 0 * rows, transposed[0]);\n            _mm256_storeu_ps(dst + i + 1 * rows, transposed[1]);\n        }\n    } else {\n        // Fallback to scalar implementation\n        for(size_t r = 0; r < rows; ++r){\n            for(size_t c = 0; c < cols; ++c){\n                float val = src[r * cols + c];\n                val = roundf(val * 4.0f) / 4.0f;\n                dst[c * rows + r] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 8) == 0) {\n        size_t i, j;\n        const size_t stride = rows / 8;\n\n        for (i = 0; i < rows; i += 8) {\n            __m256 row0 = _mm256_loadu_ps(src + i * cols);\n            __m256 row1 = _mm256_loadu_ps(src + (i + 1) * cols);\n            __m256 row2 = _mm256_loadu_ps(src + (i + 2) * cols);\n            __m256 row3 = _mm256_loadu_ps(src + (i + 3) * cols);\n            __m256 row4 = _mm256_loadu_ps(src + (i + 4) * cols);\n            __m256 row5 = _mm256_loadu_ps(src + (i + 5) * cols);\n            __m256 row6 = _mm256_loadu_ps(src + (i + 6) * cols);\n            __m256 row7 = _mm256_loadu_ps(src + (i + 7) * cols);\n\n            __m256 transposed[8];\n            transposed[0] = _mm256_unpacklo_ps(row0, row1);\n            transposed[1] = _mm256_unpackhi_ps(row0, row1);\n            transposed[2] = _mm256_unpacklo_ps(row2, row3);\n            transposed[3] = _mm256_unpackhi_ps(row2, row3);\n            transposed[4] = _mm256_unpacklo_ps(row4, row5);\n            transposed[5] = _mm256_unpackhi_ps(row4, row5);\n            transposed[6] = _mm256_unpacklo_ps(row6, row7);\n            transposed[7] = _mm256_unpackhi_ps(row6, row7);\n\n            __m256 temp[8];\n            temp[0] = _mm256_shuffle_ps(transposed[0], transposed[2], _MM_SHUFFLE(1, 0, 1, 0));\n            temp[1] = _mm256_shuffle_ps(transposed[0], transposed[2], _MM_SHUFFLE(3, 2, 3, 2));\n            temp[2] = _mm256_shuffle_ps(transposed[1], transposed[3], _MM_SHUFFLE(1, 0, 1, 0));\n            temp[3] = _mm256_shuffle_ps(transposed[1], transposed[3], _MM_SHUFFLE(3, 2, 3, 2));\n            temp[4] = _mm256_shuffle_ps(transposed[4], transposed[6], _MM_SHUFFLE(1, 0, 1, 0));\n            temp[5] = _mm256_shuffle_ps(transposed[4], transposed[6], _MM_SHUFFLE(3, 2, 3, 2));\n            temp[6] = _mm256_shuffle_ps(transposed[5], transposed[7], _MM_SHUFFLE(1, 0, 1, 0));\n            temp[7] = _mm256_shuffle_ps(transposed[5], transposed[7], _MM_SHUFFLE(3, 2, 3, 2));\n\n            __m256 result[8];\n            result[0] = _mm256_permute2f128_ps(temp[0], temp[4], 0x20);\n            result[1] = _mm256_permute2f128_ps(temp[0], temp[4], 0x31);\n            result[2] = _mm256_permute2f128_ps(temp[1], temp[5], 0x20);\n            result[3] = _mm256_permute2f128_ps(temp[1], temp[5], 0x31);\n            result[4] = _mm256_permute2f128_ps(temp[2], temp[6], 0x20);\n            result[5] = _mm256_permute2f128_ps(temp[2], temp[6], 0x31);\n            result[6] = _mm256_permute2f128_ps(temp[3], temp[7], 0x20);\n            result[7] = _mm256_permute2f128_ps(temp[3], temp[7], 0x31);\n\n            __m256 round_factor = _mm256_set1_ps(0.25f);\n            for (j = 0; j < 8; ++j) {\n                __m256 rounded = _mm256_round_ps(_mm256_mul_ps(result[j], round_factor), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                _mm256_storeu_ps(dst + j * rows + i, rounded);\n            }\n        }\n    } else {\n        // Fallback to scalar implementation\n        for (size_t r = 0; r < rows; ++r) {\n            for (size_t c = 0; c < cols; ++c) {\n                float val = src[r * cols + c];\n                float rounded = 0.25f * (float)(round(val / 0.25f));\n                dst[c * rows + r] = rounded;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && rows % 8 == 0) {\n        size_t i, j;\n        for(i = 0; i < rows; i += 8){\n            __m256 row0 = _mm256_loadu_ps(src + i * cols + 0);\n            __m256 row1 = _mm256_loadu_ps(src + i * cols + 1);\n            __m256 row2 = _mm256_loadu_ps(src + i * cols + 2);\n            __m256 row3 = _mm256_loadu_ps(src + i * cols + 3);\n            __m256 row4 = _mm256_loadu_ps(src + i * cols + 4);\n            __m256 row5 = _mm256_loadu_ps(src + i * cols + 5);\n            __m256 row6 = _mm256_loadu_ps(src + i * cols + 6);\n            __m256 row7 = _mm256_loadu_ps(src + i * cols + 7);\n\n            __m256 tmp[8];\n            tmp[0] = _mm256_shuffle_ps(row0, row1, _MM_SHUFFLE(3,2,3,2));\n            tmp[1] = _mm256_shuffle_ps(row0, row1, _MM_SHUFFLE(1,0,1,0));\n            tmp[2] = _mm256_shuffle_ps(row2, row3, _MM_SHUFFLE(3,2,3,2));\n            tmp[3] = _mm256_shuffle_ps(row2, row3, _MM_SHUFFLE(1,0,1,0));\n            tmp[4] = _mm256_shuffle_ps(row4, row5, _MM_SHUFFLE(3,2,3,2));\n            tmp[5] = _mm256_shuffle_ps(row4, row5, _MM_SHUFFLE(1,0,1,0));\n            tmp[6] = _mm256_shuffle_ps(row6, row7, _MM_SHUFFLE(3,2,3,2));\n            tmp[7] = _mm256_shuffle_ps(row6, row7, _MM_SHUFFLE(1,0,1,0));\n\n            __m256 res0 = _mm256_permute2f128_ps(tmp[0], tmp[2], 0x20);\n            __m256 res1 = _mm256_permute2f128_ps(tmp[1], tmp[3], 0x20);\n            __m256 res2 = _mm256_permute2f128_ps(tmp[0], tmp[2], 0x31);\n            __m256 res3 = _mm256_permute2f128_ps(tmp[1], tmp[3], 0x31);\n            __m256 res4 = _mm256_permute2f128_ps(tmp[4], tmp[6], 0x20);\n            __m256 res5 = _mm256_permute2f128_ps(tmp[5], tmp[7], 0x20);\n            __m256 res6 = _mm256_permute2f128_ps(tmp[4], tmp[6], 0x31);\n            __m256 res7 = _mm256_permute2f128_ps(tmp[5], tmp[7], 0x31);\n\n            __m256 mult = _mm256_set1_ps(4.0f);\n            __m256 add = _mm256_set1_ps(0.25f);\n            __m256 round = _mm256_set1_ps(0.5f);\n\n            res0 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res0, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n            res1 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res1, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n            res2 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res2, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n            res3 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res3, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n            res4 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res4, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n            res5 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res5, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n            res6 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res6, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n            res7 = _mm256_round_ps(_mm256_mul_ps(_mm256_add_ps(_mm256_mul_ps(_mm256_sub_ps(res7, add), mult), round), _MM_FROUND_TO_NEAREST_INT);\n\n            _mm256_storeu_ps(dst + j * rows + i + 0, res0);\n            _mm256_storeu_ps(dst + j * rows + i + 1, res1);\n            _mm256_storeu_ps(dst + j * rows + i + 2, res2);\n            _mm256_storeu_ps(dst + j * rows + i + 3, res3);\n            _mm256_storeu_ps(dst + j * rows + i + 4, res4);\n            _mm256_storeu_ps(dst + j * rows + i + 5, res5);\n            _mm256_storeu_ps(dst + j * rows + i + 6, res6);\n            _mm256_storeu_ps(dst + j * rows + i + 7, res7);\n        }\n    } else {\n        // Fallback to scalar implementation\n        for(size_t i = 0; i < rows; ++i){\n            for(size_t j = 0; j < cols; ++j){\n                float val = src[i * cols + j];\n                float rounded = roundf(val * 4.0f) / 4.0f;\n                dst[j * rows + i] = rounded;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```cpp\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 8) == 0) {\n        for (size_t i = 0; i < rows; i += 8) {\n            __m256 row0 = _mm256_loadu_ps(src + i * cols + 0);\n            __m256 row1 = _mm256_loadu_ps(src + i * cols + 1);\n            __m256 row2 = _mm256_loadu_ps(src + i * cols + 2);\n            __m256 row3 = _mm256_loadu_ps(src + i * cols + 3);\n            __m256 row4 = _mm256_loadu_ps(src + i * cols + 4);\n            __m256 row5 = _mm256_loadu_ps(src + i * cols + 5);\n            __m256 row6 = _mm256_loadu_ps(src + i * cols + 6);\n            __m256 row7 = _mm256_loadu_ps(src + i * cols + 7);\n\n            __m256 transposed[8];\n            transposed[0] = _mm256_unpacklo_ps(row0, row1);\n            transposed[1] = _mm256_unpackhi_ps(row0, row1);\n            transposed[2] = _mm256_unpacklo_ps(row2, row3);\n            transposed[3] = _mm256_unpackhi_ps(row2, row3);\n            transposed[4] = _mm256_unpacklo_ps(row4, row5);\n            transposed[5] = _mm256_unpackhi_ps(row4, row5);\n            transposed[6] = _mm256_unpacklo_ps(row6, row7);\n            transposed[7] = _mm256_unpackhi_ps(row6, row7);\n\n            __m256 result[8];\n            result[0] = _mm256_permute2f128_ps(transposed[0], transposed[2], 0x20);\n            result[1] = _mm256_permute2f128_ps(transposed[0], transposed[2], 0x31);\n            result[2] = _mm256_permute2f128_ps(transposed[1], transposed[3], 0x20);\n            result[3] = _mm256_permute2f128_ps(transposed[1], transposed[3], 0x31);\n            result[4] = _mm256_permute2f128_ps(transposed[4], transposed[6], 0x20);\n            result[5] = _mm256_permute2f128_ps(transposed[4], transposed[6], 0x31);\n            result[6] = _mm256_permute2f128_ps(transposed[5], transposed[7], 0x20);\n            result[7] = _mm256_permute2f128_ps(transposed[5], transposed[7], 0x31);\n\n            __m256 factor = _mm256_set1_ps(4.0f);\n            for (size_t j = 0; j < 8; ++j) {\n                result[j] = _mm256_mul_ps(result[j], factor);\n                result[j] = _mm256_round_ps(result[j], _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                result[j] = _mm256_div_ps(result[j], factor);\n            }\n\n            _mm256_storeu_ps(dst + 0 * cols + i, result[0]);\n            _mm256_storeu_ps(dst + 1 * cols + i, result[1]);\n            _mm256_storeu_ps(dst + 2 * cols + i, result[2]);\n            _mm256_storeu_ps(dst + 3 * cols + i, result[3]);\n            _mm256_storeu_ps(dst + 4 * cols + i, result[4]);\n            _mm256_storeu_ps(dst + 5 * cols + i, result[5]);\n            _mm256_storeu_ps(dst + 6 * cols + i, result[6]);\n            _mm256_storeu_ps(dst + 7 * cols + i, result[7]);\n        }\n    } else {\n        for (size_t r = 0; r < rows; ++r) {\n            for (size_t c = 0; c < cols; c += 8) {\n                __m256 vec = _mm256_loadu_ps(src + r * cols + c);\n                __m256 factor = _mm256_set1_ps(4.0f);\n                vec = _mm256_mul_ps(vec, factor);\n                vec = _mm256_round_ps(vec, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n                vec = _mm256_div_ps(vec, factor);\n                _mm256_storeu_ps(dst + c * rows + r, vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256 zero = _mm256_setzero_ps();\n    \n    for (; i + 8 <= total_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        \n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        __m256 rounded_result = _mm256_round_ps(result, _MM_FROUND_TO_ZERO);\n        \n        _mm256_storeu_ps(dst + i, rounded_result);\n    }\n\n    for (; i < total_size; ++i) {\n        dst[i] = (float)((int)(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t aligned_size = total_size / 8 * 8;\n\n    __m256 zero = _mm256_setzero_ps();\n    \n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        __m256 truncated = _mm256_floor_ps(result);\n        __m256 rounded = _mm256_add_ps(_mm256_sign_epi32(truncated, result), _mm256_andnot_ps(_mm256_cmp_ps(result, zero, _CMP_LT_OS), zero));\n        _mm256_storeu_ps(dst + i, rounded);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = (result > 0) ? floorf(result) : ceilf(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t aligned_size = total_size / 8 * 8;\n\n    __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n\n        __m256 result = _mm256_sub_ps(vec1, vec2);\n        __m256 trunc_result = _mm256_floor_ps(result);\n\n        __m256 mask = _mm256_cmpgt_epi32(_mm256_castps_si256(trunc_result), _mm256_castps_si256(result));\n        __m256 adjust = _mm256_and_ps(mask, _mm256_set1_ps(-0.0f));\n        trunc_result = _mm256_add_ps(trunc_result, adjust);\n\n        _mm256_storeu_ps(dst + i, trunc_result);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        float result = tensor1[i] - tensor2[i];\n        dst[i] = result > 0 ? (float)(int)result : -(float)(int)(-result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t aligned_size = total_size / 8 * 8;\n\n    __m256 zero = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(tensor1 + i);\n        __m256 vec2 = _mm256_loadu_ps(tensor2 + i);\n\n        __m256 diff = _mm256_sub_ps(vec1, vec2);\n\n        __m256 floored = _mm256_floor_ps(diff);\n        __m256 ceiled = _mm256_ceil_ps(diff);\n\n        __m256 mask = _mm256_cmp_ps(diff, zero, _CMP_LT_OS);\n\n        __m256 result = _mm256_blendv_ps(ceiled, floored, mask);\n\n        _mm256_storeu_ps(dst + i, result);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        float diff = tensor1[i] - tensor2[i];\n        dst[i] = diff > 0 ? (float)(int)diff : (float)(int)diff;\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t aligned_size = total_size / 8 * 8;\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 a = _mm256_loadu_ps(tensor1 + i);\n        __m256 b = _mm256_loadu_ps(tensor2 + i);\n        __m256 res = _mm256_sub_ps(a, b);\n        __m256 trunc_res = _mm256_round_ps(res, (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC));\n        _mm256_storeu_ps(dst + i, trunc_res);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        dst[i] = (float)((double)(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = total_elements - (total_elements % 4);\n\n    for (size_t i = 0; i < aligned_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = total_elements - (total_elements % 4);\n\n    for (size_t i = 0; i < aligned_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = total_elements - (total_elements % 4);\n\n    for (size_t i = 0; i < aligned_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = total_elements - (total_elements % 4);\n\n    for (size_t i = 0; i < aligned_elements; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(mat1 + i);\n        __m256d vec2 = _mm256_loadu_pd(mat2 + i);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i, result);\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t aligned_total = total - (total % 4);\n    \n    for (size_t i = 0; i < aligned_total; i += 4) {\n        __m256d vec1 = _mm256_loadu_pd(&mat1[i]);\n        __m256d vec2 = _mm256_loadu_pd(&mat2[i]);\n        __m256d result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(&dst[i], result);\n    }\n    \n    for (size_t i = aligned_total; i < total; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m256 zero = _mm256_setzero_ps();\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = zero;\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 v = _mm256_loadu_ps(vec + j);\n            __m256 m_val = _mm256_loadu_ps(mat + i * n + j);\n            __m256 prod = _mm256_mul_ps(m_val, v);\n            sum = _mm256_add_ps(sum, prod);\n        }\n        float temp_sum[8] __attribute__((aligned(32)));\n        _mm256_store_ps(temp_sum, sum);\n        float result = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3] \n                    + temp_sum[4] + temp_sum[5] + temp_sum[6] + temp_sum[7];\n        for (size_t j = (n / 8) * 8; j < n; ++j) {\n            result += mat[i * n + j] * vec[j];\n        }\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vvec = _mm256_loadu_ps(vec + j);\n            __m256 vm = _mm256_loadu_ps(mat + i * n + j);\n            __m256 prod = _mm256_mul_ps(vvec, vm);\n            sum = _mm256_add_ps(sum, prod);\n        }\n        float temp_sum[8] __attribute__((aligned(32))) = {0};\n        _mm256_store_ps(temp_sum, sum);\n        float s = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3] + temp_sum[4] + temp_sum[5] + temp_sum[6] + temp_sum[7];\n        for (; j < n; ++j) {\n            s += mat[i * n + j] * vec[j];\n        }\n        dst[i] = s;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 accumulators[8] = { _mm256_setzero_ps() };\n    \n    for (i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t k;\n        for (k = 0; k < n; k += 8) {\n            __m256 v = _mm256_loadu_ps(vec + k);\n            __m256 m_vec = _mm256_loadu_ps(mat + i * n + k);\n            __m256 product = _mm256_mul_ps(m_vec, v);\n            sum = _mm256_add_ps(sum, product);\n        }\n        // Horizontal add\n        __m256 tmp1 = _mm256_hadd_ps(sum, sum);\n        __m256 tmp2 = _mm256_hadd_ps(tmp1, tmp1);\n        float result[8];\n        _mm256_storeu_ps(result, tmp2);\n        dst[i] = result[0] + result[4];\n        \n        // Handle remaining elements if n is not a multiple of 8\n        for (k = (n / 8) * 8; k < n; ++k) {\n            dst[i] += mat[i * n + k] * vec[k];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm256_setzero_ps();\n        size_t j = 0;\n        for (; j + 8 <= n; j += 8) {\n            __m256 vmat = _mm256_loadu_ps(mat + i * n + j);\n            __m256 vvec = _mm256_loadu_ps(vec + j);\n            __m256 prod = _mm256_mul_ps(vmat, vvec);\n            sum = _mm256_add_ps(sum, prod);\n        }\n        float temp_sum[8] __attribute__((aligned(32)));\n        _mm256_store_ps(temp_sum, sum);\n        float scalar_sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3] \n                         + temp_sum[4] + temp_sum[5] + temp_sum[6] + temp_sum[7];\n        \n        for (; j < n; ++j) {\n            scalar_sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = scalar_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 sum = _mm256_setzero_ps();\n    __m256 v;\n\n    for (i = 0; i < m; ++i) {\n        sum = _mm256_setzero_ps();\n        for (j = 0; j < n; j += 8) {\n            __m256 m_vec = _mm256_loadu_ps(mat + i * n + j);\n            v = _mm256_loadu_ps(vec + j);\n            sum = _mm256_add_ps(sum, _mm256_mul_ps(m_vec, v));\n        }\n        __m128 sum_low = _mm_add_ps(_mm256_castps256_ps128(sum), _mm256_extractf128_ps(sum, 1));\n        __m128 sum_final = _mm_hadd_ps(sum_low, sum_low);\n        dst[i] = _mm_cvtss_f32(sum_final);\n\n        // Handle the remaining elements if n is not a multiple of 8\n        if (n % 8 != 0) {\n            float temp_sum = 0.0f;\n            for (size_t k = n - (n % 8); k < n; ++k) {\n                temp_sum += mat[i * n + k] * vec[k];\n            }\n            dst[i] += temp_sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    const size_t vector_size = 4;\n    size_t total_elements = m * n;\n    size_t i, j;\n\n    for (i = 0; i < total_elements / vector_size; ++i) {\n        __m256d vec = _mm256_loadu_pd(&mat[i * vector_size]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double scalar_sum = 0.0;\n    __m256d temp = _mm256_hadd_pd(sum_vec, sum_vec);\n    temp = _mm256_hadd_pd(temp, temp);\n    double intermediate_sums[4];\n    _mm256_storeu_pd(intermediate_sums, temp);\n    for (j = 0; j < vector_size; ++j) {\n        scalar_sum += intermediate_sums[j];\n    }\n\n    for (i = (total_elements / vector_size) * vector_size; i < total_elements; ++i) {\n        scalar_sum += mat[i];\n    }\n\n    return scalar_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    const size_t vector_size = 4;\n    size_t total_elements = m * n;\n    size_t i, j;\n\n    for (i = 0; i < total_elements / vector_size; ++i) {\n        __m256d vec = _mm256_loadu_pd(mat + i * vector_size);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double partial_sums[4];\n    _mm256_storeu_pd(partial_sums, sum_vec);\n\n    double total_sum = partial_sums[0] + partial_sums[1] + partial_sums[2] + partial_sums[3];\n\n    for (j = (total_elements / vector_size) * vector_size; j < total_elements; ++j) {\n        total_sum += mat[j];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = total_elements / 4 * 4;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d vec, temp;\n\n    const double* end_aligned = mat + aligned_elements;\n    for (const double* p = mat; p < end_aligned; p += 4) {\n        vec = _mm256_loadu_pd(p);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    // Horizontal sum of the accumulated vector\n    temp = _mm256_permute2f128_pd(sum_vec, sum_vec, 0x01);\n    sum_vec = _mm256_add_pd(sum_vec, temp);\n    temp = _mm256_shuffle_pd(sum_vec, sum_vec, 0x01);\n    sum_vec = _mm256_add_pd(sum_vec, temp);\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n\n    double total_sum = sum[0];\n\n    // Process remaining elements\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = total_elements / 4 * 4;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d temp_vec;\n\n    for (size_t i = 0; i < aligned_elements; i += 4) {\n        temp_vec = _mm256_loadu_pd(&mat[i]);\n        sum_vec = _mm256_add_pd(sum_vec, temp_vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    const size_t vector_size = 4;\n    size_t total_elements = m * n;\n    size_t i, j;\n\n    for (i = 0; i < total_elements / vector_size; ++i) {\n        __m256d vec = _mm256_loadu_pd(&mat[i * vector_size]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double partial_sum[4] __attribute__((aligned(32)));\n    _mm256_store_pd(partial_sum, sum_vec);\n\n    double total_sum = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3];\n\n    for (j = (total_elements / vector_size) * vector_size; j < total_elements; ++j) {\n        total_sum += mat[j];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (cols <= 1) return true;\n\n    size_t aligned_cols = cols / 8 * 8;\n    __m256i cmp_mask = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j < aligned_cols; j += 8) {\n            __m256i vec_curr = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i vec_next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            __m256i cmp_result;\n\n            if (ascending) {\n                cmp_result = _mm256_cmpgt_epi32(vec_curr, vec_next);\n            } else {\n                cmp_result = _mm256_cmpgt_epi32(vec_next, vec_curr);\n            }\n\n            __m256i mask_check = _mm256_and_si256(cmp_result, cmp_mask);\n            if (!_mm256_testz_si256(mask_check, mask_check)) {\n                return false;\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) return false;\n            } else {\n                if (row[j] < row[j + 1]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (cols <= 1) return true;\n\n    const size_t vec_size = 8;\n    const size_t aligned_cols = cols - (cols % vec_size);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const bool is_ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j <= aligned_cols - vec_size; j += vec_size) {\n            __m256i v_curr = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i v_next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp_result;\n            if (is_ascending) {\n                cmp_result = _mm256_cmpgt_epi32(v_curr, v_next);\n            } else {\n                cmp_result = _mm256_cmpgt_epi32(v_next, v_curr);\n            }\n\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_result));\n            if (mask != 0) return false;\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (is_ascending) {\n                if (row[j] > row[j + 1]) return false;\n            } else {\n                if (row[j] < row[j + 1]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    const size_t vector_size = 8; // Number of 32-bit integers in a YMM register\n    const __m256i compare_mask = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n\n        size_t j = 0;\n        for (; j <= cols - vector_size; j += vector_size) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            if (ascending) {\n                __m256i cmp_result = _mm256_cmpgt_epi32(next, current);\n                __m256i masked_cmp = _mm256_and_si256(cmp_result, compare_mask);\n                if (!_mm256_testz_si256(masked_cmp, compare_mask)) {\n                    return false;\n                }\n            } else {\n                __m256i cmp_result = _mm256_cmpgt_epi32(current, next);\n                __m256i masked_cmp = _mm256_and_si256(cmp_result, compare_mask);\n                if (!_mm256_testz_si256(masked_cmp, compare_mask)) {\n                    return false;\n                }\n            }\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] > row[j + 1]) return false;\n            } else {\n                if (row[j] < row[j + 1]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (cols <= 1) return true;\n\n    size_t aligned_cols = cols / 8 * 8;\n    __m256i cmp_mask_asc = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i cmp_mask_desc = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool* direction = (bool*)(directions + i);\n        bool is_ascending = *direction;\n\n        bool sorted = true;\n        for (size_t j = 0; j < aligned_cols; j += 8) {\n            __m256i current = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp_result;\n            if (is_ascending) {\n                cmp_result = _mm256_cmpgt_epi32(next, current);\n                if (!_mm256_testc_si256(cmp_result, cmp_mask_asc)) {\n                    sorted = false;\n                    break;\n                }\n            } else {\n                cmp_result = _mm256_cmpgt_epi32(current, next);\n                if (!_mm256_testc_si256(cmp_result, cmp_mask_asc)) {\n                    sorted = false;\n                    break;\n                }\n            }\n        }\n\n        for (size_t j = aligned_cols; j < cols - 1; ++j) {\n            if ((is_ascending && row[j] > row[j + 1]) || (!is_ascending && row[j] < row[j + 1])) {\n                sorted = false;\n                break;\n            }\n        }\n\n        if (!sorted) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```cpp\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    bool result = true;\n    if (cols <= 1) return result;\n\n    size_t aligned_cols = cols / 8 * 8;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool is_ascending = directions[i];\n\n        __m256i prev_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row));\n        result = true;\n\n        for (size_t j = 0; j < aligned_cols; j += 8) {\n            __m256i curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j + 8));\n\n            __m256i cmp_result;\n            if (is_ascending) {\n                cmp_result = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n            } else {\n                cmp_result = _mm256_cmplt_epi32(prev_vec, curr_vec);\n            }\n\n            int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_result));\n            if (mask != 0) {\n                result = false;\n                break;\n            }\n\n            prev_vec = curr_vec;\n        }\n\n        if (result && aligned_cols != cols - 8) {\n            for (size_t j = aligned_cols; j < cols - 1; ++j) {\n                if ((is_ascending && row[j] > row[j + 1]) || (!is_ascending && row[j] < row[j + 1])) {\n                    result = false;\n                    break;\n                }\n            }\n        }\n\n        if (!result) {\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i vbase = _mm256_set1_epi16(base);\n    const __m256i vbase_m1 = _mm256_set1_epi16(base - 1);\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src[i]));\n        __m256i vabs = _mm256_abs_epi16(vsrc);\n\n        __m256i vdiv = _mm256_div_epi16(vabs, vbase); // Custom div function needed\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n\n        __m256i vsign = _mm256_sign_epi16(vmul, vsrc);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), vsign);\n    }\n\n    for (; i < length; ++i) {\n        int16_t val = src[i];\n        int16_t res = (val / base) * base;\n        dst[i] = res;\n    }\n}\n``` \n\n(Note: `_mm256_div_epi16` is a placeholder and not an actual intrinsic. A custom implementation using shifts or other methods may be required.)"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i vbase = _mm256_set1_epi16(base);\n    const __m256i vbase_m1 = _mm256_set1_epi16(base - 1);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256((const __m256i*)&src[i]);\n        __m256i vdiv = _mm256_cvtepi16_epi32(vsrc); // Extend to 32-bit for division\n        __m256i vquot = _mm256_div_epi32(vdiv, _mm256_cvtepi8_epi32(vbase)); // Integer division\n        __m256i vmul = _mm256_mullo_epi32(vquot, _mm256_cvtepi8_epi32(vbase)); // Multiply back\n        __m256i vresult = _mm256_packs_epi32(vmul, vmul); // Pack back to 16-bit\n        _mm256_storeu_si256((__m256i*)&dst[i], vresult);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```\n\n**Note**: The `_mm256_div_epi32` intrinsic used above is hypothetical because AVX2 does not directly support integer division in SIMD instructions. A practical implementation would require additional steps to emulate division or use scalar operations for the division part."}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i vbase = _mm256_set1_epi16(base);\n    const __m256i vbase_m1 = _mm256_set1_epi16(base - 1);\n\n    size_t i = 0;\n    for (; i + 16 <= length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src[i]));\n        __m256i vabs = _mm256_abs_epi16(vsrc);\n\n        __m256i vdiv = _mm256_cvtepi16_epi32(vabs);\n        __m256i vbase32 = _mm256_cvtepi16_epi32(vbase);\n        __m256i vquotient = _mm256_div_epi32(vdiv, vbase32); // Custom div function\n\n        __m256i vmul = _mm256_mullo_epi32(vquotient, vbase32);\n        __m256i vresult = _mm256_cvtepi32_epi16(vmul);\n\n        __m256i vneg_mask = _mm256_cmpgt_epi16(_mm256_setzero_si256(), vsrc);\n        vresult = _mm256_sign_epi16(vresult, vneg_mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), vresult);\n    }\n\n    for (; i < length; ++i) {\n        int16_t val = src[i];\n        int16_t abs_val = (val >= 0) ? val : -val;\n        int16_t res = (abs_val / base) * base;\n        dst[i] = (val >= 0) ? res : -res;\n    }\n}\n``` \n\n// Note: `_mm256_div_epi32` is not a native AVX2 instruction and would need to be implemented using other methods or approximations."}
{"task_id": "SimdBench_31_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i vbase = _mm256_set1_epi16(base);\n    const __m256i vbase_m1 = _mm256_set1_epi16(base - 1);\n    size_t i = 0;\n\n    for (; i + 15 < length; i += 16) {\n        __m256i vsrc = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src[i]));\n        __m256i vabs = _mm256_abs_epi16(vsrc);\n        __m256i vdiv = _mm256_div_epi16(vabs, vbase); // Custom div function needed\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase);\n        __m256i vcmp = _mm256_cmpgt_epi16(vmul, vabs);\n        __m256i vsub = _mm256_sub_epi16(vmul, vbase);\n        vmul = _mm256_blendv_epi8(vsub, vmul, vcmp);\n        __m256i vsign = _mm256_sign_epi16(vmul, vsrc);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), vsign);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m256i vbase = _mm256_set1_epi16(base);\n    const size_t vec_len = 16;\n    size_t i = 0;\n\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i vsrc = _mm256_loadu_si256((__m256i const *)&src[i]);\n        __m256i vzero = _mm256_setzero_si256();\n        __m256i vabs = _mm256_subs_epu16(_mm256_xor_si256(vsrc, vzero), vzero); // abs(src[i])\n        __m256i vdiv = _mm256_cvtepi32_epi16(_mm256_div_epi16(vabs, vbase)); // div\n        __m256i vmul = _mm256_mullo_epi16(vdiv, vbase); // mul\n        __m256i vsign = _mm256_sign_epi16(vmul, vsrc); // sign\n        _mm256_storeu_si256((__m256i *)&dst[i], vsign);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    __m256d vec_cmp, mat_cmp, mask;\n    size_t aligned_cols = cols / 4 * 4;\n\n    for (i = 0; i < rows; ++i) {\n        const double* row_ptr = matrix + i * cols;\n        bool match = true;\n\n        for (j = 0; j < aligned_cols; j += 4) {\n            vec_cmp = _mm256_loadu_pd(vector + j);\n            mat_cmp = _mm256_loadu_pd(row_ptr + j);\n            mask = _mm256_cmp_pd(mat_cmp, vec_cmp, _CMP_EQ_OQ);\n\n            if (!_mm256_movemask_pd(mask) == 0xf) {\n                match = false;\n                break;\n            }\n        }\n\n        if (match && (cols % 4 != 0)) {\n            for (j = aligned_cols; j < cols; ++j) {\n                if (*(row_ptr + j) != *(vector + j)) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t aligned_cols = cols / 4 * 4;\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(vector + j);\n            __m256d mat = _mm256_loadu_pd(matrix + i * cols + j);\n            __m256d cmp = _mm256_cmp_pd(mat, vec, _CMP_EQ_OQ);\n            int mask = _mm256_movemask_pd(cmp);\n            if (mask != 0xf) {\n                match = false;\n                break;\n            }\n        }\n        if (match && (aligned_cols != cols)) {\n            for (size_t j = aligned_cols; j < cols; ++j) {\n                if (matrix[i * cols + j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    __m256d vec_cmp, mat_cmp, result;\n\n    for (i = 0; i < rows; ++i) {\n        bool match = true;\n        for (j = 0; j + 4 <= cols; j += 4) {\n            vec_cmp = _mm256_loadu_pd(vector + j);\n            mat_cmp = _mm256_loadu_pd(matrix + i * cols + j);\n            result = _mm256_cmp_pd(vec_cmp, mat_cmp, _CMP_EQ_OQ);\n            if (!_mm256_testz_pd(result, result)) {\n                continue;\n            } else {\n                match = false;\n                break;\n            }\n        }\n        if (match && (j < cols)) {\n            for (; j < cols; ++j) {\n                if (vector[j] != matrix[i * cols + j]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t aligned_cols = cols / 4 * 4;\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m256d vec_cmp = _mm256_loadu_pd(&vector[j]);\n            __m256d mat_cmp = _mm256_loadu_pd(&matrix[i * cols + j]);\n            __m256d cmp_res = _mm256_cmp_pd(mat_cmp, vec_cmp, _CMP_EQ_OQ);\n            if (!_mm256_movemask_pd(cmp_res) == 0xf) {\n                match = false;\n                break;\n            }\n        }\n        if (aligned_cols != cols) {\n            bool scalar_match = true;\n            for (size_t j = aligned_cols; j < cols; ++j) {\n                if (matrix[i * cols + j] != vector[j]) {\n                    scalar_match = false;\n                    break;\n                }\n            }\n            if (!scalar_match) {\n                match = false;\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    const size_t vec_size = 4; // Number of elements processed per AVX operation (double)\n    __m256d vec_reg, mat_reg;\n    __mmask8 cmp_result;\n\n    for (i = 0; i < rows; ++i) {\n        const double* mat_row = matrix + i * cols;\n        bool row_match = true;\n\n        for (j = 0; j + vec_size <= cols; j += vec_size) {\n            vec_reg = _mm256_loadu_pd(vector + j);\n            mat_reg = _mm256_loadu_pd(mat_row + j);\n            cmp_result = _mm256_cmp_pd_mask(mat_reg, vec_reg, _CMP_EQ_OQ);\n\n            if (_mm_popcnt_u32(cmp_result) != vec_size) {\n                row_match = false;\n                break;\n            }\n        }\n\n        if (row_match && (j < cols)) {\n            for (; j < cols; ++j) {\n                if (mat_row[j] != vector[j]) {\n                    row_match = false;\n                    break;\n                }\n            }\n        }\n\n        if (row_match) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sum = _mm256_setzero_si256();\n\n    for (; i <= total_size - 32; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i eq = _mm256_cmpeq_epi8(vec_a, vec_b);\n        __m256i masked_ones = _mm256_and_si256(eq, ones);\n        sum = _mm256_add_epi8(sum, masked_ones);\n    }\n\n    int intermediate_counts[32];\n    _mm256_storeu_si256((__m256i*)intermediate_counts, sum);\n\n    for (int j = 0; j < 32; ++j) {\n        count += intermediate_counts[j];\n    }\n\n    for (; i < total_size; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n\n    size_t i = 0;\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sum = _mm256_setzero_si256();\n\n    for (; i <= total_size - 32; i += 32) {\n        __m256i a1 = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b1 = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i equal1 = _mm256_cmpeq_epi8(a1, b1);\n        __m256i mask1 = _mm256_and_si256(equal1, ones);\n\n        __m256i a2 = _mm256_loadu_si256((__m256i*)(A + i + 16));\n        __m256i b2 = _mm256_loadu_si256((__m256i*)(B + i + 16));\n        __m256i equal2 = _mm256_cmpeq_epi8(a2, b2);\n        __m256i mask2 = _mm256_and_si256(equal2, ones);\n\n        sum = _mm256_add_epi8(sum, mask1);\n        sum = _mm256_add_epi8(sum, mask2);\n    }\n\n    // Horizontal add\n    __m128i sum_low = _mm256_castsi256_si128(sum);\n    __m128i sum_high = _mm256_extracti128_si256(sum, 1);\n    sum_low = _mm_add_epi8(sum_low, sum_high);\n\n    uint8_t result[16];\n    _mm_storeu_si128((__m128i*)result, sum_low);\n\n    for (int j = 0; j < 16; j++) {\n        count += result[j];\n    }\n\n    // Handle remaining elements\n    for (; i < total_size; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t vec_size = 32; // 32 bytes per AVX2 vector for uint8_t\n    size_t i;\n\n    for (i = 0; i <= total_size - vec_size; i += vec_size) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i cmp_result = _mm256_cmpeq_epi8(vec_a, vec_b);\n        __m256i mask = _mm256_set1_epi8(0x01);\n        __m256i ones = _mm256_and_si256(cmp_result, mask);\n        int popcount = _mm256_popcnt_epi64(_mm256_movemask_epi8(ones));\n        count += popcount;\n    }\n\n    for (; i < total_size; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t count = 0;\n    size_t i = 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sum = _mm256_setzero_si256();\n\n    for (; i <= total_elements - 32; i += 32) {\n        __m256i vec_a1 = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_b1 = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i cmp1 = _mm256_cmpeq_epi8(vec_a1, vec_b1);\n        __m256i mask1 = _mm256_and_si256(cmp1, ones);\n\n        __m256i vec_a2 = _mm256_loadu_si256((__m256i*)(A + i + 16));\n        __m256i vec_b2 = _mm256_loadu_si256((__m256i*)(B + i + 16));\n        __m256i cmp2 = _mm256_cmpeq_epi8(vec_a2, vec_b2);\n        __m256i mask2 = _mm256_and_si256(cmp2, ones);\n\n        sum = _mm256_add_epi8(sum, mask1);\n        sum = _mm256_add_epi8(sum, mask2);\n    }\n\n    int32_t temp[32];\n    _mm256_storeu_si256((__m256i*)temp, sum);\n    for (int j = 0; j < 32; ++j) {\n        count += temp[j];\n    }\n\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    __m256i vcount = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n\n    for (i = 0; i <= total_size - 32; i += 32) {\n        __m256i va = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vb = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i eq = _mm256_cmpeq_epi8(va, vb);\n        __m256i inc = _mm256_and_si256(eq, ones);\n        vcount = _mm256_add_epi8(vcount, inc);\n    }\n\n    // Horizontal sum of vcount\n    __m128i vlow = _mm256_castsi256_si128(vcount);\n    __m128i vhigh = _mm256_extracti128_si256(vcount, 1);\n    vlow = _mm_add_epi8(vlow, vhigh);\n    vlow = _mm_hadd_epi16(vlow, vlow);\n    vlow = _mm_hadd_epi16(vlow, vlow);\n    int temp_count = _mm_extract_epi16(vlow, 0);\n    count += temp_count;\n\n    // Handle remaining elements\n    for (; i < total_size; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    const __m256i scalar = _mm256_set1_epi64x(a);\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i *)(x + i));\n        __m256i vec_y = _mm256_loadu_si256((__m256i *)(y + i));\n\n        __m256i mul_result = _mm256_mullo_epi64(vec_x, scalar);\n        __m256i abs_result = _mm256_abs_epi64(vec_y);\n        __m256i sub_result = _mm256_sub_epi64(mul_result, abs_result);\n\n        _mm256_storeu_si256((__m256i *)(y + i), sub_result);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - labs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    const size_t vec_len = 4;\n    const size_t aligned_length = length / vec_len * vec_len;\n\n    __m256i va = _mm256_set1_epi64x(a);\n\n    for (size_t i = 0; i < aligned_length; i += vec_len) {\n        __m256i vx = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&x[i]));\n        __m256i vy = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&y[i]));\n\n        __m256i mul = _mm256_mullo_epi64(va, vx);\n        __m256i abs_y = _mm256_abs_epi64(vy);\n        __m256i sub = _mm256_sub_epi64(mul, abs_y);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&y[i]), sub);\n    }\n\n    for (size_t i = aligned_length; i < length; ++i) {\n        y[i] = a * x[i] - std::abs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    const size_t vec_len = 4;\n    const size_t num_vecs = length / vec_len;\n    __m256i broadcast_a = _mm256_set1_epi64x(a);\n\n    for (size_t i = 0; i < num_vecs; ++i) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i const *)(x + i * vec_len));\n        __m256i vec_y = _mm256_loadu_si256((__m256i const *)(y + i * vec_len));\n\n        __m256i mul_result = _mm256_mullo_epi64(broadcast_a, vec_x);\n        __m256i abs_vec_y = _mm256_abs_epi64(vec_y);\n        __m256i sub_result = _mm256_sub_epi64(mul_result, abs_vec_y);\n\n        _mm256_storeu_si256((__m256i *)(y + i * vec_len), sub_result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_vecs * vec_len; i < length; ++i) {\n        y[i] = a * x[i] - labs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i va = _mm256_set1_epi64x(a);\n    for (; i + 3 < length; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i *)(x + i));\n        __m256i vy = _mm256_loadu_si256((__m256i *)(y + i));\n\n        __m256i mul = _mm256_mullo_epi64(va, vx);\n        __m256i abs_y = _mm256_abs_epi64(vy);\n\n        __m256i res = _mm256_sub_epi64(mul, abs_y);\n\n        _mm256_storeu_si256((__m256i *)(y + i), res);\n    }\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - labs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m256i va = _mm256_set1_epi64x(a);\n\n    for (; i + 3 < length; i += 4) {\n        __m256i vx = _mm256_loadu_si256((__m256i *)(x + i));\n        __m256i vy = _mm256_loadu_si256((__m256i *)(y + i));\n\n        __m256i mul = _mm256_mullo_epi64(va, vx);\n        __m256i abs_y = _mm256_abs_epi64(vy);\n        __m256i sub = _mm256_sub_epi64(mul, abs_y);\n\n        _mm256_storeu_si256((__m256i *)(y + i), sub);\n    }\n\n    for (; i < length; i++) {\n        y[i] = a * x[i] - labs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i + 16 <= length; i += 16) {\n        __m256i vdata = _mm256_loadu_si256((__m256i const *)(vec + i));\n        vmin = _mm256_min_epi16(vmin, vdata);\n        vmax = _mm256_max_epi16(vmax, vdata);\n    }\n\n    // Horizontal min and max within the SIMD registers\n    __m128i vmin_low = _mm256_castsi256_si128(vmin);\n    __m128i vmin_high = _mm256_extracti128_si256(vmin, 1);\n    vmin_low = _mm_min_epi16(vmin_low, vmin_high);\n\n    __m128i vmax_low = _mm256_castsi256_si128(vmax);\n    __m128i vmax_high = _mm256_extracti128_si256(vmax, 1);\n    vmax_low = _mm_max_epi16(vmax_low, vmax_high);\n\n    // Further reduce to scalar values\n    int16_t min_val[8];\n    int16_t max_val[8];\n    _mm_storeu_si128((__m128i *)min_val, vmin_low);\n    _mm_storeu_si128((__m128i *)max_val, vmax_low);\n\n    int16_t min_result = min_val[0];\n    int16_t max_result = max_val[0];\n\n    for (int i = 1; i < 8; ++i) {\n        if (min_val[i] < min_result) min_result = min_val[i];\n        if (max_val[i] > max_result) max_result = max_val[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        if (vec[i] < min_result) min_result = vec[i];\n        if (vec[i] > max_result) max_result = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_result;\n    result.max_num = max_result;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_val = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_val = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length / 16 * 16; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const *)(vec + i));\n        min_val = _mm256_min_epi16(min_val, data);\n        max_val = _mm256_max_epi16(max_val, data);\n    }\n\n    // Horizontal min and max within the SIMD register\n    __m128i low_min = _mm256_extracti128_si256(min_val, 0);\n    __m128i high_min = _mm256_extracti128_si256(min_val, 1);\n    low_min = _mm_min_epi16(low_min, high_min);\n\n    __m128i low_max = _mm256_extracti128_si256(max_val, 0);\n    __m128i high_max = _mm256_extracti128_si256(max_val, 1);\n    low_max = _mm_max_epi16(low_max, high_max);\n\n    // Further reduce to get single min and max\n    int16_t min_arr[8];\n    int16_t max_arr[8];\n    _mm_storeu_si128((__m128i *)min_arr, low_min);\n    _mm_storeu_si128((__m128i *)max_arr, low_max);\n\n    int16_t min_result = min_arr[0];\n    int16_t max_result = max_arr[0];\n\n    for (size_t i = 1; i < 8; ++i) {\n        if (min_arr[i] < min_result) min_result = min_arr[i];\n        if (max_arr[i] > max_result) max_result = max_arr[i];\n    }\n\n    // Process remaining elements\n    for (size_t i = length / 16 * 16; i < length; ++i) {\n        if (vec[i] < min_result) min_result = vec[i];\n        if (vec[i] > max_result) max_result = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_result;\n    result.max_num = max_result;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i + 16 <= length; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i const *)(vec + i));\n        vmin = _mm256_min_epi16(vmin, v);\n        vmax = _mm256_max_epi16(vmax, v);\n    }\n\n    // Horizontal min and max\n    __m128i vmin_low = _mm256_castsi256_si128(vmin);\n    __m128i vmin_high = _mm256_extracti128_si256(vmin, 1);\n    vmin_low = _mm_min_epi16(vmin_low, vmin_high);\n\n    __m128i vmax_low = _mm256_castsi256_si128(vmax);\n    __m128i vmax_high = _mm256_extracti128_si256(vmax, 1);\n    vmax_low = _mm_max_epi16(vmax_low, vmax_high);\n\n    int16_t min_val = _mm_extract_epi16(vmin_low, 0);\n    int16_t max_val = _mm_extract_epi16(vmax_low, 0);\n\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i min_val = _mm256_set1_epi16(INT16_MAX);\n    __m256i max_val = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i + 16 <= length; i += 16) {\n        __m256i data = _mm256_loadu_si256((__m256i const *)(vec + i));\n        min_val = _mm256_min_epi16(min_val, data);\n        max_val = _mm256_max_epi16(max_val, data);\n    }\n\n    // Horizontal min and max within the SIMD registers\n    __m128i min_low = _mm256_castsi256_si128(min_val);\n    __m128i min_high = _mm256_extracti128_si256(min_val, 1);\n    min_low = _mm_min_epi16(min_low, min_high);\n\n    __m128i max_low = _mm256_castsi256_si128(max_val);\n    __m128i max_high = _mm256_extracti128_si256(max_val, 1);\n    max_low = _mm_max_epi16(max_low, max_high);\n\n    // Further reduce to scalar values\n    int16_t min_result = INT16_MAX;\n    int16_t max_result = INT16_MIN;\n\n    min_result = _mm_extract_epi16(min_low, 0) < min_result ? _mm_extract_epi16(min_low, 0) : min_result;\n    min_result = _mm_extract_epi16(min_low, 1) < min_result ? _mm_extract_epi16(min_low, 1) : min_result;\n    min_result = _mm_extract_epi16(min_low, 2) < min_result ? _mm_extract_epi16(min_low, 2) : min_result;\n    min_result = _mm_extract_epi16(min_low, 3) < min_result ? _mm_extract_epi16(min_low, 3) : min_result;\n    min_result = _mm_extract_epi16(min_low, 4) < min_result ? _mm_extract_epi16(min_low, 4) : min_result;\n    min_result = _mm_extract_epi16(min_low, 5) < min_result ? _mm_extract_epi16(min_low, 5) : min_result;\n    min_result = _mm_extract_epi16(min_low, 6) < min_result ? _mm_extract_epi16(min_low, 6) : min_result;\n    min_result = _mm_extract_epi16(min_low, 7) < min_result ? _mm_extract_epi16(min_low, 7) : min_result;\n\n    max_result = _mm_extract_epi16(max_low, 0) > max_result ? _mm_extract_epi16(max_low, 0) : max_result;\n    max_result = _mm_extract_epi16(max_low, 1) > max_result ? _mm_extract_epi16(max_low, 1) : max_result;\n    max_result = _mm_extract_epi16(max_low, 2) > max_result ? _mm_extract_epi16(max_low, 2) : max_result;\n    max_result = _mm_extract_epi16(max_low, 3) > max_result ? _mm_extract_epi16(max_low, 3) : max_result;\n    max_result = _mm_extract_epi16(max_low, 4) > max_result ? _mm_extract_epi16(max_low, 4) : max_result;\n    max_result = _mm_extract_epi16(max_low, 5) > max_result ? _mm_extract_epi16(max_low, 5) : max_result;\n    max_result = _mm_extract_epi16(max_low, 6) > max_result ? _mm_extract_epi16(max_low, 6) : max_result;\n    max_result = _mm_extract_epi16(max_low, 7) > max_result ? _mm_extract_epi16(max_low, 7) : max_result;\n\n    // Process remaining elements\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        if (vec[i] < min_result) min_result = vec[i];\n        if (vec[i] > max_result) max_result = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_result;\n    result.max_num = max_result;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i vmin = _mm256_set1_epi16(INT16_MAX);\n    __m256i vmax = _mm256_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i < length / 16 * 16; i += 16) {\n        __m256i vdata = _mm256_loadu_si256((__m256i const *)(vec + i));\n        vmin = _mm256_min_epi16(vmin, vdata);\n        vmax = _mm256_max_epi16(vmax, vdata);\n    }\n\n    // Horizontal min and max within the registers\n    __m128i vmin_low = _mm256_castsi256_si128(vmin);\n    __m128i vmin_high = _mm256_extracti128_si256(vmin, 1);\n    vmin_low = _mm_min_epi16(vmin_low, vmin_high);\n\n    __m128i vmax_low = _mm256_castsi256_si128(vmax);\n    __m128i vmax_high = _mm256_extracti128_si256(vmax, 1);\n    vmax_low = _mm_max_epi16(vmax_low, vmax_high);\n\n    // Further reduce to scalar values\n    int16_t min_val = _mm_extract_epi16(vmin_low, 0);\n    int16_t max_val = _mm_extract_epi16(vmax_low, 0);\n\n    for (int i = 1; i < 8; ++i) {\n        int16_t val = _mm_extract_epi16(vmin_low, i);\n        if (val < min_val) min_val = val;\n        val = _mm_extract_epi16(vmax_low, i);\n        if (val > max_val) max_val = val;\n    }\n\n    // Handle remaining elements\n    for (size_t i = length / 16 * 16; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t vec_size = 8; // Number of integers in a __m256i vector\n    const size_t aligned_cols = cols / vec_size * vec_size;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        for (; j <= aligned_cols - vec_size; j += vec_size) {\n            __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j));\n            __m256i v_shifted = _mm256_permutevar8x32_epi32(v, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n            __m256i v_cmp = _mm256_cmpgt_epi32(v_shifted, v);\n\n            // Check if all comparisons are true\n            int mask = _mm256_movemask_epi8(v_cmp);\n            if (mask != 0xFFFF) return false;\n        }\n\n        for (; j < cols - 1; ++j) {\n            if (!(row[j] < row[j + 1])) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t vec_size = 8; // Number of integers in a __m256i register\n    const int mask_all_ones = _mm256_movemask_epi8(_mm256_cmpeq_epi32(_mm256_set1_epi32(1), _mm256_set1_epi32(1)));\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process elements in batches of 8 using AVX2\n        for (; j + vec_size <= cols; j += vec_size) {\n            __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j));\n            __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j + 1));\n\n            __m256i cmp_result = _mm256_cmpgt_epi32(current, next);\n            int cmp_mask = _mm256_movemask_epi8(cmp_result);\n\n            if (cmp_mask != 0) {\n                return false;\n            }\n        }\n\n        // Handle remaining elements with scalar code\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t vec_size = 8; // Number of integers in a __m256i vector\n    const int mask_all_ones = _mm256_movemask_epi8(_mm256_cmpeq_epi32(_mm256_set1_epi32(1), _mm256_set1_epi32(1)));\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process 8 elements at a time using AVX2\n        for (; j + vec_size <= cols; j += vec_size) {\n            __m256i current_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j));\n            __m256i next_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row + j + 1));\n\n            __m256i cmp_result = _mm256_cmpgt_epi32(current_vec, next_vec);\n            int cmp_mask = _mm256_movemask_epi8(cmp_result);\n\n            if (cmp_mask != mask_all_ones) {\n                return false;\n            }\n        }\n\n        // Process remaining elements sequentially\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t vec_size = 8; // Number of integers processed per AVX2 vector (256 bits / 32 bits)\n    __m256i cmp_mask = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process elements in batches of 8 using AVX2\n        for (; j + vec_size <= cols; j += vec_size) {\n            __m256i vec_curr = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i vec_next = _mm256_loadu_si256((__m256i*)(row + j + 1));\n            __m256i vec_cmp = _mm256_cmpgt_epi32(vec_next, vec_curr);\n            __m256i vec_and = _mm256_and_si256(vec_cmp, cmp_mask);\n            if (!_mm256_testc_si256(vec_and, cmp_mask)) return false;\n        }\n\n        // Process remaining elements sequentially\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t vec_size = 8; // Number of integers in a __m256i vector\n    const int mask_all_ones = 0xFF;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n\n        // Process elements in batches of 8 using AVX2\n        for (; j + vec_size <= cols; j += vec_size) {\n            __m256i current_vec = _mm256_loadu_si256((__m256i*)(row + j));\n            __m256i next_vec = _mm256_loadu_si256((__m256i*)(row + j + 1));\n\n            __m256i cmp_result = _mm256_cmpgt_epi32(current_vec, next_vec);\n            int cmp_mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_result));\n\n            if (cmp_mask != 0) {\n                return false;\n            }\n        }\n\n        // Process any remaining elements sequentially\n        for (; j + 1 < cols; ++j) {\n            if (row[j] >= row[j + 1]) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    const int* first_block = matrix;\n    __m256i vfirst;\n\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            if (j + 8 > block_size) {\n                continue;\n            }\n            vfirst = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(first_block + i * cols + j));\n            for (size_t rb = 1; rb < num_row_blocks; ++rb) {\n                for (size_t cb = 0; cb < num_col_blocks; ++cb) {\n                    const int* current_block = matrix + rb * block_size * cols + cb * block_size;\n                    __m256i vcurrent = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(current_block + i * cols + j));\n                    if (_mm256_cmpneq_epi32_mask(vfirst, vcurrent) != 0) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t rb = 0; rb < num_row_blocks; ++rb) {\n        for (size_t cb = 0; cb < num_col_blocks; ++cb) {\n            const int* current_block = matrix + rb * block_size * cols + cb * block_size;\n            if (rb == 0 && cb == 0) {\n                continue; // Skip comparison of the first block with itself\n            }\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    __m256i v_first = _mm256_loadu_si256((__m256i const*)(first_block + i * cols + j));\n                    __m256i v_curr = _mm256_loadu_si256((__m256i const*)(current_block + i * cols + j));\n                    if (_mm256_cmpneq_epi32_mask(v_first, v_curr) != 0xff) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    const int* first_block = matrix;\n\n    for (size_t rb = 0; rb < num_row_blocks; ++rb) {\n        for (size_t cb = 0; cb < num_col_blocks; ++cb) {\n            const int* current_block = matrix + rb * block_size * cols + cb * block_size;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    __m256i vec_first = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(first_block + i * cols + j));\n                    __m256i vec_curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(current_block + i * cols + j));\n\n                    if (_mm256_cmpneq_epi32_mask(vec_first, vec_curr) != 0) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = cols / block_size;\n    size_t num_blocks_col = rows / block_size;\n\n    const int* first_block = matrix;\n    __m256i vfirst;\n\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            vfirst = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(first_block + i * cols + j));\n            for (size_t b_row = 1; b_row < num_blocks_col; ++b_row) {\n                for (size_t b_col = 0; b_col < num_blocks_row; ++b_col) {\n                    const int* current_block = matrix + b_row * block_size * cols + b_col * block_size * cols + i * cols + j;\n                    __m256i vcurrent = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(current_block));\n                    if (_mm256_cmpneq_epi32_mask(vfirst, vcurrent) != 0) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    const int* first_block = matrix;\n    __m256i vfirst;\n\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            if (j + 8 > block_size) {\n                continue;\n            }\n            vfirst = _mm256_loadu_si256((__m256i*)(first_block + i * cols + j));\n            for (size_t rb = 1; rb < num_row_blocks; ++rb) {\n                for (size_t cb = 0; cb < num_col_blocks; ++cb) {\n                    const int* current_block = matrix + rb * block_size * cols + cb * block_size;\n                    __m256i vcurrent = _mm256_loadu_si256((__m256i*)(current_block + i * cols + j));\n                    if (_mm256_cmpneq_epi32_mask(vfirst, vcurrent) != 0) {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || length < block_size || length % block_size != 0 || vec == nullptr) {\n        return false;\n    }\n\n    const size_t num_blocks = length / block_size;\n    const size_t num_full_vecs_per_block = block_size / 4;\n\n    if (num_full_vecs_per_block == 0) {\n        return false;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block_vec[num_full_vecs_per_block];\n\n    for (size_t i = 0; i < num_full_vecs_per_block; ++i) {\n        first_block_vec[i] = _mm256_loadu_pd(vec + 4 * i);\n    }\n\n    for (size_t b = 1; b < num_blocks; ++b) {\n        for (size_t i = 0; i < num_full_vecs_per_block; ++i) {\n            __m256d current_block_vec = _mm256_loadu_pd(vec + b * block_size + 4 * i);\n            __m256d diff = _mm256_sub_pd(current_block_vec, first_block_vec[i]);\n            __m256d abs_diff = _mm256_max_pd(_mm256_abs_pd(diff), _mm256_setzero_pd());\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n            if (!_mm256_movemask_pd(cmp) == 0xf) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || length < block_size || length % block_size != 0 || vec == NULL) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            if (j + 4 > block_size) {\n                continue;\n            }\n            __m256d first_block = _mm256_loadu_pd(&vec[j]);\n            __m256d curr_block = _mm256_loadu_pd(&vec[i * block_size + j]);\n\n            __m256d diff = _mm256_sub_pd(first_block, curr_block);\n            __m256d abs_diff = _mm256_max_pd(_mm256_sub_pd(_mm256_setzero_pd(), diff), diff);\n\n            __m256d mask = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n            if (!_mm256_testz_si256((__m256i)mask, (__m256i)_mm256_set1_pd(-1.0))) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || length < block_size || length % block_size != 0 || vec == nullptr) {\n        return false;\n    }\n\n    const size_t num_blocks = length / block_size;\n    const size_t vec_elements = 4; // Number of elements in a __m256d register\n    const size_t aligned_block_size = block_size - (block_size % vec_elements);\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block_vec[vec_elements];\n    for (size_t i = 0; i < aligned_block_size; i += vec_elements) {\n        first_block_vec[i / vec_elements] = _mm256_loadu_pd(&vec[i]);\n    }\n\n    for (size_t b = 1; b < num_blocks; ++b) {\n        size_t offset = b * block_size;\n        for (size_t i = 0; i < aligned_block_size; i += vec_elements) {\n            __m256d current_block = _mm256_loadu_pd(&vec[offset + i]);\n            __m256d diff = _mm256_sub_pd(current_block, first_block_vec[i / vec_elements]);\n            __m256d abs_diff = _mm256_and_pd(diff, _mm256_set1_pd(0x7FFFFFFFFFFFFFFF));\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n            if (!_mm256_movemask_pd(cmp) == 0xF) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || length == 0 || length < block_size || vec == NULL) return false;\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks <= 1) return true;\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block[4];\n    size_t i, j;\n\n    // Load the first block into reference vectors\n    for (i = 0; i < block_size; i += 4) {\n        first_block[i / 4] = _mm256_loadu_pd(&vec[i]);\n    }\n\n    // Compare each subsequent block to the first block\n    for (size_t block_idx = 1; block_idx < num_blocks; ++block_idx) {\n        const double* current_block = &vec[block_idx * block_size];\n        for (i = 0; i + 4 <= block_size; i += 4) {\n            __m256d current = _mm256_loadu_pd(&current_block[i]);\n            __m256d diff = _mm256_sub_pd(current, first_block[i / 4]);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n            if (!_mm256_testz_si256((__m256i)_mm256_castpd_si256(cmp), (__m256i)_mm256_castpd_si256(cmp))) {\n                return false;\n            }\n        }\n        // Handle remaining elements in the block that don't fit into a full AVX register\n        for (; i < block_size; ++i) {\n            if (fabs(current_block[i] - vec[i]) > tolerance) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || length < block_size || length % block_size != 0 || vec == NULL || tolerance < 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    size_t num_double_per_vec = 4; // AVX can handle 4 doubles per 256-bit register\n    size_t aligned_block_size = (block_size / num_double_per_vec) * num_double_per_vec;\n\n    if (aligned_block_size == 0) {\n        return true;\n    }\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    __m256d first_block_vec[num_double_per_vec];\n    for (size_t i = 0; i < aligned_block_size; i += num_double_per_vec) {\n        first_block_vec[i / num_double_per_vec] = _mm256_loadu_pd(&vec[i]);\n    }\n\n    for (size_t b = 1; b < num_blocks; ++b) {\n        size_t offset = b * block_size;\n        for (size_t i = 0; i < aligned_block_size; i += num_double_per_vec) {\n            __m256d current_block_vec = _mm256_loadu_pd(&vec[offset + i]);\n            __m256d diff = _mm256_sub_pd(current_block_vec, first_block_vec[i / num_double_per_vec]);\n            __m256d abs_diff = _mm256_and_pd(diff, _mm256_set1_pd(-0.0));\n            __m256d cmp = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OQ);\n            if (!_mm256_testz_pd(cmp, cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t aligned_cols = cols / 4 * 4;\n    __m256d zero = _mm256_setzero_pd();\n    \n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        const double* row = &matrix[i * cols];\n        \n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&row[j]);\n            __m256d squared = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, squared);\n        }\n        \n        double temp_sum[4] __attribute__((aligned(32)));\n        _mm256_store_pd(temp_sum, sum);\n        double scalar_sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n        \n        for (size_t j = aligned_cols; j < cols; ++j) {\n            scalar_sum += row[j] * row[j];\n        }\n        \n        double threshold = thresholds[i];\n        result[i] = (scalar_sum > threshold * threshold);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d threshold_vec;\n    __m256d row_data;\n\n    for (i = 0; i < rows; ++i) {\n        sum_vec = _mm256_setzero_pd();\n        const double* row_start = &matrix[i * cols];\n        size_t col_aligned = cols / 4 * 4;\n\n        for (j = 0; j < col_aligned; j += 4) {\n            row_data = _mm256_loadu_pd(&row_start[j]);\n            row_data = _mm256_mul_pd(row_data, row_data);\n            sum_vec = _mm256_add_pd(sum_vec, row_data);\n        }\n\n        double sums[4];\n        _mm256_storeu_pd(sums, sum_vec);\n\n        double total_sum = sums[0] + sums[1] + sums[2] + sums[3];\n\n        for (; j < cols; ++j) {\n            total_sum += row_start[j] * row_start[j];\n        }\n\n        double threshold = thresholds[i];\n        result[i] = (total_sum > threshold * threshold);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t aligned_cols = cols / 4 * 4;\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        const double* row = matrix + i * cols;\n\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(row + j);\n            __m256d squared = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, squared);\n        }\n\n        // Horizontal add for the remaining part of the vector\n        __m128d high = _mm256_extractf128_pd(sum, 1);\n        __m128d low = _mm256_castpd256_pd128(sum);\n        low = _mm_add_pd(low, high);\n        double norm_sq = _mm_cvtsd_f64(low) + _mm_extract_epi64(_mm_castpd_si128(low), 1);\n\n        // Add the remaining elements if cols is not a multiple of 4\n        for (size_t j = aligned_cols; j < cols; ++j) {\n            norm_sq += row[j] * row[j];\n        }\n\n        double threshold = thresholds[i];\n        result[i] = (norm_sq > threshold * threshold);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t aligned_cols = (cols / 4) * 4;\n    __m256d zero = _mm256_setzero_pd();\n    \n    for (size_t i = 0; i < rows; ++i) {\n        double sum = 0.0;\n        __m256d row_sum = _mm256_setzero_pd();\n        \n        const double* row = &matrix[i * cols];\n        size_t j = 0;\n        \n        for (; j + 3 < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(&row[j]);\n            __m256d squared = _mm256_mul_pd(vec, vec);\n            row_sum = _mm256_add_pd(row_sum, squared);\n        }\n        \n        double temp[4] = {0};\n        _mm256_storeu_pd(temp, row_sum);\n        sum += temp[0] + temp[1] + temp[2] + temp[3];\n        \n        for (; j < cols; ++j) {\n            sum += row[j] * row[j];\n        }\n        \n        double norm = sqrt(sum);\n        result[i] = norm > thresholds[i];\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t aligned_cols = cols / 4 * 4;\n    __m256d zero = _mm256_setzero_pd();\n    \n    for (size_t i = 0; i < rows; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        const double* row = matrix + i * cols;\n        \n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(row + j);\n            __m256d squared = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, squared);\n        }\n        \n        double temp_sum[4];\n        _mm256_storeu_pd(temp_sum, sum);\n        double scalar_sum = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n        \n        for (size_t j = aligned_cols; j < cols; ++j) {\n            scalar_sum += row[j] * row[j];\n        }\n        \n        double threshold = thresholds[i];\n        result[i] = (scalar_sum > threshold * threshold);\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m256i va, vb, vc;\n    for (; i + 8 <= length; i += 8) {\n        va = _mm256_loadu_si256((__m256i const *)(A + i));\n        vb = _mm256_loadu_si256((__m256i const *)(B + i));\n        vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i *)(C + i), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m256i va, vb, vc;\n    for (; i + 8 <= length; i += 8) {\n        va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n        vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m256i va, vb, vc;\n    for (; i + 7 < length; i += 8) {\n        va = _mm256_loadu_si256((__m256i const*)(A + i));\n        vb = _mm256_loadu_si256((__m256i const*)(B + i));\n        vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i *)(C + i), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m256i va, vb, vc;\n    for (; i + 7 < length; i += 8) {\n        va = _mm256_loadu_si256((__m256i const*)(A + i));\n        vb = _mm256_loadu_si256((__m256i const*)(B + i));\n        vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i *)(C + i), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m256i va, vb, vc;\n    for (; i + 7 < length; i += 8) {\n        va = _mm256_loadu_si256((__m256i const*)(A + i));\n        vb = _mm256_loadu_si256((__m256i const*)(B + i));\n        vc = _mm256_or_si256(va, vb);\n        _mm256_storeu_si256((__m256i *)(C + i), vc);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    const size_t vector_length = 32;\n    size_t i = 0;\n\n    for (; i <= length - vector_length; i += vector_length) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        // Compute squares of A and B\n        __m256i a_squared = _mm256_mullo_epi16(vec_a, vec_a);\n        __m256i b_squared = _mm256_mullo_epi16(vec_b, vec_b);\n\n        // Sum of squares\n        __m256i ab_sum_squares = _mm256_add_epi32(a_squared, b_squared);\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_sad_epu8(ab_sum_squares, _mm256_setzero_si256()));\n\n        // Compute products of A and B\n        __m256i ab_products = _mm256_maddubs_epi16(vec_a, vec_b);\n        sum_products = _mm256_add_epi32(sum_products, _mm256_sad_epu8(ab_products, _mm256_setzero_si256()));\n    }\n\n    int64_t total_sum_squares = 0;\n    int64_t total_sum_products = 0;\n\n    // Horizontal add for sum_squares\n    __m128i temp_squares_low = _mm_add_epi32(_mm256_extracti128_si256(sum_squares, 0), _mm256_extracti128_si256(sum_squares, 1));\n    __m128i temp_squares_high = _mm_add_epi32(_mm256_extracti128_si256(sum_squares, 2), _mm256_extracti128_si256(sum_squares, 3));\n    __m128i total_squares = _mm_add_epi32(temp_squares_low, temp_squares_high);\n    total_sum_squares += _mm_cvtsi128_si32(total_squares);\n    total_sum_squares += _mm_extract_epi32(total_squares, 1);\n    total_sum_squares += _mm_extract_epi32(total_squares, 2);\n    total_sum_squares += _mm_extract_epi32(total_squares, 3);\n\n    // Horizontal add for sum_products\n    __m128i temp_products_low = _mm_add_epi32(_mm256_extracti128_si256(sum_products, 0), _mm256_extracti128_si256(sum_products, 1));\n    __m128i temp_products_high = _mm_add_epi32(_mm256_extracti128_si256(sum_products, 2), _mm256_extracti128_si256(sum_products, 3));\n    __m128i total_products = _mm_add_epi32(temp_products_low, temp_products_high);\n    total_sum_products += _mm_cvtsi128_si32(total_products);\n    total_sum_products += _mm_extract_epi32(total_products, 1);\n    total_sum_products += _mm_extract_epi32(total_products, 2);\n    total_sum_products += _mm_extract_epi32(total_products, 3);\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        total_sum_squares += (int64_t)(A[i] * A[i]) + (int64_t)(B[i] * B[i]);\n        total_sum_products += (int64_t)(A[i] * B[i]);\n    }\n\n    return total_sum_squares - total_sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i const *)(B + i));\n\n        // Compute squares of A and B\n        __m256i a_sq = _mm256_maddubs_epi16(a_vec, a_vec);\n        __m256i b_sq = _mm256_maddubs_epi16(b_vec, b_vec);\n\n        // Compute products of A and B\n        __m256i ab = _mm256_maddubs_epi16(a_vec, b_vec);\n\n        // Horizontal add for squares\n        __m256i a_sq_hsum = _mm256_hadd_epi32(a_sq, a_sq);\n        __m256i b_sq_hsum = _mm256_hadd_epi32(b_sq, b_sq);\n\n        // Horizontal add for products\n        __m256i ab_hsum = _mm256_hadd_epi32(ab, ab);\n\n        // Accumulate sums\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_add_epi32(a_sq_hsum, b_sq_hsum));\n        sum_products = _mm256_add_epi32(sum_products, ab_hsum);\n    }\n\n    // Extract the accumulated sums\n    int32_t sum_squares_arr[8];\n    _mm256_storeu_si256((__m256i *)sum_squares_arr, sum_squares);\n    int64_t total_sum_squares = 0;\n    for (size_t j = 0; j < 8; ++j) {\n        total_sum_squares += sum_squares_arr[j];\n    }\n\n    int32_t sum_products_arr[8];\n    _mm256_storeu_si256((__m256i *)sum_products_arr, sum_products);\n    int64_t total_sum_products = 0;\n    for (size_t j = 0; j < 8; ++j) {\n        total_sum_products += sum_products_arr[j];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        total_sum_squares += A[i] * A[i] + B[i] * B[i];\n        total_sum_products += A[i] * B[i];\n    }\n\n    return total_sum_squares - total_sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    const size_t vector_length = 32; // 256 bits / 8 bits per element\n    size_t i = 0;\n\n    for (; i <= length - vector_length; i += vector_length) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const *)(B + i));\n\n        // Compute squares of A and B\n        __m256i vec_a_squared = _mm256_sign_epi8(vec_a, vec_a);\n        __m256i vec_b_squared = _mm256_sign_epi8(vec_b, vec_b);\n\n        // Compute products of A and B\n        __m256i vec_product = _mm256_mullo_epi16(_mm256_cvtepi8_epi16(vec_a), _mm256_cvtepi8_epi16(vec_b));\n\n        // Horizontal add for squares\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_sad_epu8(vec_a_squared, _mm256_setzero_si256()));\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_sad_epu8(vec_b_squared, _mm256_setzero_si256()));\n\n        // Horizontal add for products\n        sum_products = _mm256_add_epi32(sum_products, _mm256_sad_epu8(_mm256_packus_epi16(vec_product, vec_product), _mm256_setzero_si256()));\n    }\n\n    // Extract results from SIMD registers\n    int32_t sum_squares_arr[8];\n    _mm256_storeu_si256((__m256i *)sum_squares_arr, sum_squares);\n    int64_t total_sum_squares = 0;\n    for (size_t j = 0; j < 8; ++j) {\n        total_sum_squares += sum_squares_arr[j];\n    }\n\n    int32_t sum_products_arr[8];\n    _mm256_storeu_si256((__m256i *)sum_products_arr, sum_products);\n    int64_t total_sum_products = 0;\n    for (size_t j = 0; j < 8; ++j) {\n        total_sum_products += sum_products_arr[j];\n    }\n\n    // Handle leftover elements\n    for (; i < length; ++i) {\n        total_sum_squares += A[i] * A[i] + B[i] * B[i];\n        total_sum_products += A[i] * B[i];\n    }\n\n    return total_sum_squares - total_sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    const size_t vector_length = 32;\n    size_t i = 0;\n\n    for (; i <= length - vector_length; i += vector_length) {\n        __m256i a = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const *)(B + i));\n\n        __m256i a_squared = _mm256_mullo_epi16(a, a);\n        __m256i b_squared = _mm256_mullo_epi16(b, b);\n\n        __m256i ab = _mm256_maddubs_epi16(a, b);\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_cvtepi16_epi32(_mm256_extracti128_si256(a_squared, 0)));\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_cvtepi16_epi32(_mm256_extracti128_si256(a_squared, 1)));\n\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_cvtepi16_epi32(_mm256_extracti128_si256(b_squared, 0)));\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_cvtepi16_epi32(_mm256_extracti128_si256(b_squared, 1)));\n\n        sum_products = _mm256_add_epi32(sum_products, _mm256_cvtepi16_epi32(_mm256_extracti128_si256(ab, 0)));\n        sum_products = _mm256_add_epi32(sum_products, _mm256_cvtepi16_epi32(_mm256_extracti128_si256(ab, 1)));\n    }\n\n    int64_t total_sum_squares = 0;\n    int64_t total_sum_products = 0;\n\n    __m256i sums_squares = _mm256_hadd_epi32(sum_squares, sum_squares);\n    sums_squares = _mm256_hadd_epi32(sums_squares, sums_squares);\n    int32_t *sums_squares_ptr = (int32_t *)&sums_squares;\n    total_sum_squares += sums_squares_ptr[0] + sums_squares_ptr[4];\n\n    __m256i sums_products = _mm256_hadd_epi32(sum_products, sum_products);\n    sums_products = _mm256_hadd_epi32(sums_products, sums_products);\n    int32_t *sums_products_ptr = (int32_t *)&sums_products;\n    total_sum_products += sums_products_ptr[0] + sums_products_ptr[4];\n\n    for (; i < length; ++i) {\n        total_sum_squares += A[i] * A[i] + B[i] * B[i];\n        total_sum_products += A[i] * B[i];\n    }\n\n    return total_sum_squares - total_sum_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    size_t i = 0;\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (; i + 31 < length; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const *)(B + i));\n\n        // Compute squares of A and B\n        __m256i vec_a_squared = _mm256_maddubs_epi16(vec_a, vec_a);\n        __m256i vec_b_squared = _mm256_maddubs_epi16(vec_b, vec_b);\n\n        // Compute products of A and B\n        __m256i vec_ab = _mm256_maddubs_epi16(vec_a, vec_b);\n\n        // Horizontal add for squares\n        vec_a_squared = _mm256_hadd_epi16(vec_a_squared, vec_a_squared);\n        vec_b_squared = _mm256_hadd_epi16(vec_b_squared, vec_b_squared);\n\n        // Horizontal add for products\n        vec_ab = _mm256_hadd_epi16(vec_ab, vec_ab);\n\n        // Sum up the results\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_unpacklo_epi16(vec_a_squared, vec_b_squared));\n        sum_squares = _mm256_add_epi32(sum_squares, _mm256_unpackhi_epi16(vec_a_squared, vec_b_squared));\n\n        sum_products = _mm256_add_epi32(sum_products, _mm256_unpacklo_epi16(vec_ab, vec_ab));\n        sum_products = _mm256_add_epi32(sum_products, _mm256_unpackhi_epi16(vec_ab, vec_ab));\n    }\n\n    // Horizontal sum for squares\n    sum_squares = _mm256_hadd_epi32(sum_squares, sum_squares);\n    sum_squares = _mm256_hadd_epi32(sum_squares, sum_squares);\n    int64_t total_squares = _mm256_extract_epi32(sum_squares, 0) + _mm256_extract_epi32(sum_squares, 4);\n\n    // Horizontal sum for products\n    sum_products = _mm256_hadd_epi32(sum_products, sum_products);\n    sum_products = _mm256_hadd_epi32(sum_products, sum_products);\n    int64_t total_products = _mm256_extract_epi32(sum_products, 0) + _mm256_extract_epi32(sum_products, 4);\n\n    // Handle remaining elements\n    int64_t scalar_squares = 0, scalar_products = 0;\n    for (; i < length; ++i) {\n        scalar_squares += A[i] * A[i] + B[i] * B[i];\n        scalar_products += A[i] * B[i];\n    }\n\n    return (total_squares + scalar_squares) - (total_products + scalar_products);\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    const size_t vec_len = 32 / sizeof(uint64_t); // Number of elements processed per AVX vector\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= length - vec_len; i += vec_len) {\n        __m256i a = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const *)(B + i));\n        __m256i not_b = _mm256_andnot_si256(b, a);\n        _mm256_storeu_si256((__m256i *)(C + i), not_b);\n    }\n\n    // Process remaining elements with scalar operations\n    for (; i < length; ++i) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    for (; i + 3 <= length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i not_b = _mm256_xor_si256(b, _mm256_cmpeq_epi64(zero, zero));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i *)(C + i), result);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_set1_epi64x(0);\n    for (; i + 3 <= length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i not_b = _mm256_xor_si256(b, _mm256_set1_epi64x(-1));\n        __m256i result = _mm256_and_si256(a, not_b);\n        _mm256_storeu_si256((__m256i *)(C + i), result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i va, vb, vc;\n    for (; i + 3 <= length; i += 4) {\n        va = _mm256_loadu_si256 ((__m256i const *) (A + i));\n        vb = _mm256_loadu_si256 ((__m256i const *) (B + i));\n        vb = _mm256_andnot_si256(vb, va);\n        _mm256_storeu_si256 ((__m256i *) (C + i), vb);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m256i va, vb, vc;\n    const size_t len = length - (length % 4);\n    for (; i < len; i += 4) {\n        va = _mm256_loadu_si256((__m256i const *)(A + i));\n        vb = _mm256_loadu_si256((__m256i const *)(B + i));\n        vc = _mm256_andnot_si256(vb, va);\n        _mm256_storeu_si256((__m256i *)(C + i), vc);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    const __m256i zero = _mm256_set1_epi8(0);\n    const __m256i one = _mm256_set1_epi8(1);\n    const __m256i neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i b1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n\n        __m256i pos_a1 = _mm256_cmpgt_epi8(a1, zero);\n        __m256i pos_b1 = _mm256_cmpgt_epi8(b1, zero);\n        __m256i neg_a1 = _mm256_cmplt_epi8(a1, zero);\n        __m256i neg_b1 = _mm256_cmplt_epi8(b1, zero);\n\n        __m256i both_pos = _mm256_and_si256(pos_a1, pos_b1);\n        __m256i both_neg = _mm256_and_si256(neg_a1, neg_b1);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(both_pos, one),\n            _mm256_and_si256(both_neg, neg_one)\n        );\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    const __m256i zero = _mm256_set1_epi8(0);\n    const __m256i one = _mm256_set1_epi8(1);\n    const __m256i neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i b1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n\n        __m256i pos_a1 = _mm256_cmpgt_epi8(a1, zero);\n        __m256i pos_b1 = _mm256_cmpgt_epi8(b1, zero);\n        __m256i neg_a1 = _mm256_cmpgt_epi8(zero, a1);\n        __m256i neg_b1 = _mm256_cmpgt_epi8(zero, b1);\n\n        __m256i both_pos = _mm256_and_si256(pos_a1, pos_b1);\n        __m256i both_neg = _mm256_and_si256(neg_a1, neg_b1);\n\n        __m256i result_pos = _mm256_and_si256(both_pos, one);\n        __m256i result_neg = _mm256_and_si256(both_neg, neg_one);\n\n        __m256i result = _mm256_or_si256(result_pos, result_neg);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_set1_epi8(0);\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i neg_one = _mm256_set1_epi8(-1);\n\n    for (; i + 31 < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i pos_a = _mm256_cmpgt_epi8(a, zero);\n        __m256i pos_b = _mm256_cmpgt_epi8(b, zero);\n        __m256i both_pos = _mm256_and_si256(pos_a, pos_b);\n\n        __m256i neg_a = _mm256_cmpgt_epi8(zero, a);\n        __m256i neg_b = _mm256_cmpgt_epi8(zero, b);\n        __m256i both_neg = _mm256_and_si256(neg_a, neg_b);\n\n        __m256i result = _mm256_or_si256(\n            _mm256_and_si256(both_pos, one),\n            _mm256_and_si256(both_neg, neg_one)\n        );\n\n        _mm256_storeu_si256((__m256i *)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    const __m256i v_zero = _mm256_set1_epi8(0);\n    const __m256i v_one = _mm256_set1_epi8(1);\n    const __m256i v_neg_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i a1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i b1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n\n        __m256i sign_a1 = _mm256_sign_epi8(a1, a1);\n        __m256i sign_b1 = _mm256_sign_epi8(b1, b1);\n\n        __m256i cmp_pos = _mm256_cmpeq_epi8(sign_a1, sign_b1);\n        __m256i cmp_neg = _mm256_cmpeq_epi8(sign_a1, v_neg_one);\n        __m256i cmp_and = _mm256_and_si256(cmp_pos, cmp_neg);\n\n        __m256i result = _mm256_blendv_epi8(v_zero, v_one, cmp_pos);\n        result = _mm256_blendv_epi8(result, v_neg_one, cmp_and);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    const __m256i ones = _mm256_set1_epi8(1);\n    const __m256i neg_ones = _mm256_set1_epi8(-1);\n    const __m256i zeros = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length / 32 * 32; i += 32) {\n        __m256i a_lo = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i b_lo = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n\n        __m256i sign_a = _mm256_sign_epi8(a_lo, a_lo);\n        __m256i sign_b = _mm256_sign_epi8(b_lo, b_lo);\n\n        __m256i both_positive = _mm256_and_si256(_mm256_cmpgt_epi8(a_lo, zeros), _mm256_cmpgt_epi8(b_lo, zeros));\n        __m256i both_negative = _mm256_and_si256(_mm256_cmplt_epi8(a_lo, zeros), _mm256_cmplt_epi8(b_lo, zeros));\n\n        __m256i result = _mm256_blendv_epi8(\n            _mm256_blendv_epi8(zeros, neg_ones, both_negative),\n            ones, both_positive);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length / 32 * 32; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask_mid = _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1);\n\n    for (; i <= length - 16; i += 16) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]));\n\n        __m256i cmp_res = _mm256_cmpgt_epi16(_mm256_set1_epi16(mid - i), _mm256_set1_epi16(8));\n        __m256i mask = _mm256_blendv_epi8(mask_mid, zero, cmp_res);\n\n        __m256i and_res = _mm256_and_si256(vec_a, vec_b);\n        __m256i xor_res = _mm256_xor_si256(vec_a, vec_b);\n\n        __m256i res = _mm256_blendv_epi8(and_res, xor_res, mask);\n        res = _mm256_blendv_epi8(res, vec_a, _mm256_cmpeq_epi16(_mm256_set1_epi16(mid - i), _mm256_set1_epi16(8)));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&C[i]), res);\n    }\n\n    for (; i < length; ++i) {\n        if (i < mid) {\n            C[i] = A[i] ^ B[i];\n        } else if (i == mid) {\n            C[i] = A[i];\n        } else {\n            C[i] = A[i] & B[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask_midpoint = _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1);\n\n    for (; i <= length - 16; i += 16) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i vec_b = _mm256_loadu_si256((__m256i const*)(B + i));\n\n        __m256i cmp_res = _mm256_cmpgt_epi16(_mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1, i), _mm256_set1_epi16(midpoint));\n        __m256i mask = _mm256_cmpeq_epi16(cmp_res, zero);\n\n        __m256i and_res = _mm256_and_si256(vec_a, vec_b);\n        __m256i xor_res = _mm256_xor_si256(vec_a, vec_b);\n\n        __m256i select_and = _mm256_blendv_epi8(xor_res, and_res, mask);\n        __m256i result = _mm256_blendv_epi8(select_and, vec_a, mask_midpoint);\n\n        _mm256_storeu_si256((__m256i *)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (i < midpoint) {\n            C[i] = A[i] ^ B[i];\n        } else if (i > midpoint) {\n            C[i] = A[i] & B[i];\n        } else {\n            C[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t mid = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint with XOR\n    for (; i + 8 <= mid; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i res = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i *)(C + i), res);\n    }\n\n    // Handle remaining elements before the midpoint with scalar operations\n    for (; i < mid; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Store the midpoint element from A\n    C[mid] = A[mid];\n\n    // Process elements after the midpoint with AND\n    i = mid + 1;\n    for (; i + 8 <= length; i += 8) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i res = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i *)(C + i), res);\n    }\n\n    // Handle remaining elements after the midpoint with scalar operations\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Handle elements before the midpoint with XOR\n    for (; i <= (midpoint - 1) - 16; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i *)(C + i), result);\n    }\n\n    // Handle remaining elements before the midpoint with scalar operations\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Store the midpoint element from A\n    C[midpoint] = A[midpoint];\n\n    // Handle elements after the midpoint with AND\n    for (i = midpoint + 1; i <= (length - 1) - 16; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i *)(C + i), result);\n    }\n\n    // Handle remaining elements after the midpoint with scalar operations\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t half = length / 2;\n    size_t i = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set_epi16(0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, 0xFFFF, \n                                    0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000);\n\n    for (; i + 16 <= half; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i and_result = _mm256_and_si256(a, b);\n        _mm256_storeu_si256((__m256i *)(C + i), and_result);\n    }\n\n    for (; i < half; ++i) {\n        C[i] = A[i] & B[i];\n    }\n\n    C[half] = A[half];\n\n    for (i = half + 1; i + 16 <= length; i += 16) {\n        __m256i a = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i const*)(B + i));\n        __m256i xor_result = _mm256_xor_si256(a, b);\n        _mm256_storeu_si256((__m256i *)(C + i), xor_result);\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i true_mask = _mm256_set1_epi32(-1);\n    __m256i false_mask = _mm256_setzero_si256();\n\n    for (; i <= total_size - 8; i += 8) {\n        // Load conditions\n        __m256i c1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond1[i]));\n        __m256i c2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond2[i]));\n\n        // Convert bools to masks\n        __m256i mask_c1 = _mm256_cmpeq_epi32(c1, true_mask);\n        __m256i mask_c2 = _mm256_cmpeq_epi32(c2, true_mask);\n\n        // Invert masks where needed\n        __m256i not_mask_c1 = _mm256_xor_si256(mask_c1, true_mask);\n        __m256i not_mask_c2 = _mm256_xor_si256(mask_c2, true_mask);\n\n        // Combine conditions\n        __m256i mask_A = _mm256_and_si256(mask_c1, mask_c2);\n        __m256i mask_B = _mm256_and_si256(mask_c1, not_mask_c2);\n        __m256i mask_C = _mm256_and_si256(not_mask_c1, mask_c2);\n        __m256i mask_D = _mm256_and_si256(not_mask_c1, not_mask_c2);\n\n        // Load data\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        // Select based on masks\n        __m256 selected_A = _mm256_blendv_ps(_mm256_setzero_ps(), a, _mm256_castsi256_ps(mask_A));\n        __m256 selected_B = _mm256_blendv_ps(_mm256_setzero_ps(), b, _mm256_castsi256_ps(mask_B));\n        __m256 selected_C = _mm256_blendv_ps(_mm256_setzero_ps(), c, _mm256_castsi256_ps(mask_C));\n        __m256 selected_D = _mm256_blendv_ps(_mm256_setzero_ps(), d, _mm256_castsi256_ps(mask_D));\n\n        // Combine results\n        __m256 result = _mm256_or_ps(selected_A, selected_B);\n        result = _mm256_or_ps(result, selected_C);\n        result = _mm256_or_ps(result, selected_D);\n\n        // Store result\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i true_mask = _mm256_set1_epi32(-1);\n    __m256i false_mask = _mm256_setzero_si256();\n\n    for (; i + 8 <= total_size; i += 8) {\n        // Load conditions\n        __m256i c1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond1[i]));\n        __m256i c2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond2[i]));\n\n        // Convert boolean to mask (true -> -1, false -> 0)\n        __m256i cond1_mask = _mm256_cmpeq_epi32(c1, true_mask);\n        __m256i cond2_mask = _mm256_cmpeq_epi32(c2, true_mask);\n\n        // Create combined masks\n        __m256i both_true_mask = _mm256_and_si256(cond1_mask, cond2_mask);\n        __m256i c1_true_c2_false_mask = _mm256_andnot_si256(cond2_mask, cond1_mask);\n        __m256i c1_false_c2_true_mask = _mm256_andnot_si256(cond1_mask, cond2_mask);\n        __m256i both_false_mask = _mm256_andnot_si256(_mm256_or_si256(cond1_mask, cond2_mask), true_mask);\n\n        // Load data\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        // Select based on conditions\n        __m256 result = _mm256_blendv_ps(d, c, _mm256_castsi256_ps(c1_false_c2_true_mask));\n        result = _mm256_blendv_ps(result, b, _mm256_castsi256_ps(c1_true_c2_false_mask));\n        result = _mm256_blendv_ps(result, a, _mm256_castsi256_ps(both_true_mask));\n\n        // Store result\n        _mm256_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i true_mask = _mm256_set1_epi32(-1);\n    __m256i false_mask = _mm256_setzero_si256();\n\n    for (; i <= total_size - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 c = _mm256_loadu_ps(C + i);\n        __m256 d = _mm256_loadu_ps(D + i);\n\n        __m256i cond1_vec = _mm256_loadu_si256((__m256i*)(cond1 + i));\n        __m256i cond2_vec = _mm256_loadu_si256((__m256i*)(cond2 + i));\n\n        __m256i mask_cond1_true = _mm256_cmpeq_epi32(cond1_vec, true_mask);\n        __m256i mask_cond2_true = _mm256_cmpeq_epi32(cond2_vec, true_mask);\n\n        __m256i mask_both_true = _mm256_and_si256(mask_cond1_true, mask_cond2_true);\n        __m256i mask_cond1_true_cond2_false = _mm256_andnot_si256(mask_cond2_true, mask_cond1_true);\n        __m256i mask_cond1_false_cond2_true = _mm256_andnot_si256(mask_cond1_true, mask_cond2_true);\n        __m256i mask_both_false = _mm256_andnot_si256(_mm256_or_si256(mask_cond1_true, mask_cond2_true), true_mask);\n\n        __m256 result_a = _mm256_blendv_ps(d, a, _mm256_castsi256_ps(mask_both_true));\n        __m256 result_b = _mm256_blendv_ps(result_a, b, _mm256_castsi256_ps(mask_cond1_true_cond2_false));\n        __m256 result_c = _mm256_blendv_ps(result_b, c, _mm256_castsi256_ps(mask_cond1_false_cond2_true));\n        __m256 result_d = _mm256_blendv_ps(result_c, d, _mm256_castsi256_ps(mask_both_false));\n\n        _mm256_storeu_ps(out + i, result_d);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i true_mask = _mm256_set1_epi32(-1);\n    __m256i false_mask = _mm256_setzero_si256();\n\n    for (; i + 8 <= total_size; i += 8) {\n        // Load conditions\n        __m256i c1 = _mm256_loadu_si256((const __m256i*)&cond1[i]);\n        __m256i c2 = _mm256_loadu_si256((const __m256i*)&cond2[i]);\n\n        // Convert boolean to mask (true -> -1, false -> 0)\n        __m256i cond1_mask = _mm256_cmpeq_epi32(c1, true_mask);\n        __m256i cond2_mask = _mm256_cmpeq_epi32(c2, true_mask);\n\n        // Combine conditions\n        __m256i both_true_mask = _mm256_and_si256(cond1_mask, cond2_mask);\n        __m256i c1_true_c2_false_mask = _mm256_andnot_si256(cond2_mask, cond1_mask);\n        __m256i c1_false_c2_true_mask = _mm256_andnot_si256(cond1_mask, cond2_mask);\n        __m256i both_false_mask = _mm256_xor_si256(_mm256_or_si256(cond1_mask, cond2_mask), true_mask);\n\n        // Load data\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        // Select based on conditions\n        __m256 result_both_true = _mm256_blendv_ps(d, a, _mm256_castsi256_ps(both_true_mask));\n        __m256 result_c1_true_c2_false = _mm256_blendv_ps(result_both_true, b, _mm256_castsi256_ps(c1_true_c2_false_mask));\n        __m256 result_c1_false_c2_true = _mm256_blendv_ps(result_c1_true_c2_false, c, _mm256_castsi256_ps(c1_false_c2_true_mask));\n        __m256 result_both_false = _mm256_blendv_ps(result_c1_false_c2_true, d, _mm256_castsi256_ps(both_false_mask));\n\n        // Store result\n        _mm256_storeu_ps(&out[i], result_both_false);\n    }\n\n    // Handle remaining elements\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m256i true_mask = _mm256_set1_epi32(-1);\n    __m256i false_mask = _mm256_setzero_si256();\n\n    for (; i + 7 < total_size; i += 8) {\n        // Load conditions\n        __m256i c1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond1[i]));\n        __m256i c2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond2[i]));\n\n        // Create masks based on conditions\n        __m256i mask_c1 = _mm256_cmpeq_epi32(c1, true_mask);\n        __m256i mask_c2 = _mm256_cmpeq_epi32(c2, true_mask);\n\n        // Combine masks\n        __m256i mask_c1_and_c2 = _mm256_and_si256(mask_c1, mask_c2);\n        __m256i mask_c1_not_c2 = _mm256_andnot_si256(mask_c2, mask_c1);\n        __m256i mask_not_c1_c2 = _mm256_andnot_si256(mask_c1, mask_c2);\n        __m256i mask_not_c1_not_c2 = _mm256_andnot_si256(_mm256_or_si256(mask_c1, mask_c2), true_mask);\n\n        // Load data\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 c = _mm256_loadu_ps(&C[i]);\n        __m256 d = _mm256_loadu_ps(&D[i]);\n\n        // Select based on conditions\n        __m256 result_c1_and_c2 = _mm256_blendv_ps(_mm256_setzero_ps(), a, _mm256_castsi256_ps(mask_c1_and_c2));\n        __m256 result_c1_not_c2 = _mm256_blendv_ps(_mm256_setzero_ps(), b, _mm256_castsi256_ps(mask_c1_not_c2));\n        __m256 result_not_c1_c2 = _mm256_blendv_ps(_mm256_setzero_ps(), c, _mm256_castsi256_ps(mask_not_c1_c2));\n        __m256 result_not_c1_not_c2 = _mm256_blendv_ps(_mm256_setzero_ps(), d, _mm256_castsi256_ps(mask_not_c1_not_c2));\n\n        // Combine results\n        __m256 combined1 = _mm256_or_ps(result_c1_and_c2, result_c1_not_c2);\n        __m256 combined2 = _mm256_or_ps(result_not_c1_c2, result_not_c1_not_c2);\n        __m256 final_result = _mm256_or_ps(combined1, combined2);\n\n        // Store result\n        _mm256_storeu_ps(&out[i], final_result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_size = dim1 * dim2 * dim3;\n    const size_t vec_size = 8; // Number of 32-bit integers in a 256-bit register\n    const size_t aligned_size = total_size / vec_size * vec_size;\n\n    for (size_t i = 0; i < aligned_size; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i bit_count = _mm256_popcnt_epi32(vec);\n        __m256i packed = _mm256_cvtepi32_epi8(bit_count);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), _mm256_castsi256_si128(packed));\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    const __m256i ones = _mm256_set1_epi8(0x01);\n    const __m256i zeros = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < total_size; i += 8) {\n        __m256i input = _mm256_loadu_si256((const __m256i*)&A[i]);\n        __m256i count = _mm256_popcnt_epi32(input);\n        __m256i packed = _mm256_cvtepi32_epi8(count);\n        _mm_storeu_si128((__m128i*)&out[i], _mm256_castsi256_si128(packed));\n    }\n\n    // Handle remaining elements if total_size is not a multiple of 8\n    size_t remainder = total_size % 8;\n    if (remainder > 0) {\n        for (size_t i = total_size - remainder; i < total_size; ++i) {\n            out[i] = __builtin_popcount(A[i]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_size = dim1 * dim2 * dim3;\n    const size_t aligned_size = total_size - (total_size % 8);\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((const __m256i*)&A[i]);\n        __m256i ones = _mm256_sad_epu8(vec, _mm256_setzero_si256());\n        __m128i low = _mm256_extracti128_si256(ones, 0);\n        __m128i high = _mm256_extracti128_si256(ones, 1);\n        __m128i sum = _mm_add_epi16(low, high);\n        uint16_t counts[8];\n        _mm_storeu_si128((__m128i*)counts, sum);\n        for (size_t j = 0; j < 8; ++j) {\n            out[i + j] = counts[j] >> 4;\n        }\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_size = dim1 * dim2 * dim3;\n    const size_t vec_width = 8; // Number of 32-bit integers in a 256-bit register\n    const size_t aligned_size = total_size / vec_width * vec_width;\n\n    __m256i ones = _mm256_set1_epi8(0x01);\n\n    for (size_t i = 0; i < aligned_size; i += vec_width) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i count = _mm256_popcnt_epi32(vec);\n        __m256i packed = _mm256_packus_epi32(count, count);\n        packed = _mm256_packus_epi16(packed, packed);\n        __m128i result = _mm_packus_epi16(_mm256_castsi256_si128(packed), _mm256_extracti128_si256(packed, 1));\n        _mm_storeu_si128((__m128i*)(out + i), result);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_size = dim1 * dim2 * dim3;\n    const size_t vec_size = 8;\n    const size_t aligned_size = total_size / vec_size * vec_size;\n\n    for (size_t i = 0; i < aligned_size; i += vec_size) {\n        __m256i data = _mm256_loadu_si256((const __m256i*)&A[i]);\n        __m256i bit_counts = _mm256_popcnt_epi32(data);\n        __m128i low_bits = _mm256_extracti128_si256(bit_counts, 0);\n        __m128i high_bits = _mm256_extracti128_si256(bit_counts, 1);\n        low_bits = _mm_packus_epi32(low_bits, high_bits);\n        low_bits = _mm_packus_epi16(low_bits, low_bits);\n        _mm_storeu_si128((__m128i*)&out[i], low_bits);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t aligned_size = size / 8 * 8;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i three = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i abs_vec = _mm256_max_epi32(vec, zero); // Absolute value\n\n        __m256i rem = _mm256_rem_epi32(vec, three); // Remainder after division by 3\n        __m256i is_divisible = _mm256_cmpeq_epi32(rem, zero); // Check if remainder is 0\n\n        __m256i result = _mm256_blendv_epi8(vec, abs_vec, is_divisible); // Conditional blend\n        _mm256_storeu_si256((__m256i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t aligned_len = total / 8 * 8;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i three = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i < aligned_len; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i rem = _mm256_abs_epi32(_mm256_rem_epi32(vec, three));\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i result = _mm256_blendv_epi8(vec, abs_vec, mask);\n        _mm256_storeu_si256((__m256i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_len; i < total; ++i) {\n        if ((A[i] % 3) == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    size_t i = 0;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i three = _mm256_set1_epi32(3);\n\n    for (; i + 7 < total; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i abs_vec = _mm256_sign_epi32(vec, vec); // Absolute value\n\n        __m256i rem = _mm256_rem_epi32(vec, three); // Remainder after division by 3\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero); // Check if remainder is zero\n\n        __m256i result = _mm256_blendv_epi8(vec, abs_vec, mask); // Select based on mask\n        _mm256_storeu_si256((__m256i *)(B + i), result);\n    }\n\n    for (; i < total; ++i) {\n        B[i] = (A[i] % 3 == 0) ? abs(A[i]) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t i = 0;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i three = _mm256_set1_epi32(3);\n\n    for (; i + 7 < size; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i abs_vec_a = _mm256_max_epi32(vec_a, _mm256_sub_epi32(zero, vec_a));\n\n        __m256i mod_result = _mm256_rem_epi32(vec_a, three);\n        __m256i is_divisible = _mm256_cmpeq_epi32(mod_result, zero);\n\n        __m256i result = _mm256_blendv_epi8(vec_a, abs_vec_a, is_divisible);\n\n        _mm256_storeu_si256((__m256i *)(B + i), result);\n    }\n\n    for (; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t size = rows * cols;\n    size_t aligned_size = size / 8 * 8;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i three = _mm256_set1_epi32(3);\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i abs_vec = _mm256_sign_epi32(vec, vec); // Absolute value\n\n        __m256i rem = _mm256_rem_epi32(vec, three); // Remainder after division by 3\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero); // Compare remainder with 0\n\n        __m256i result = _mm256_blendv_epi8(vec, abs_vec, mask); // Select based on mask\n        _mm256_storeu_si256((__m256i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const size_t vec_len = 8;\n    const size_t num_vecs = length / vec_len;\n    const __m256i all_ones_mask = _mm256_set1_epi32(0xF);\n    const __m256i rotate_mask = _mm256_set1_epi32((rotate_bits == 0) ? 0 : ((1 << rotate_bits) - 1));\n    \n    for(size_t i = 0; i < num_vecs; ++i){\n        __m256i vec_src = _mm256_loadu_si256((__m256i const*)(src + i * vec_len));\n        __m256i low_nibble = _mm256_and_si256(vec_src, all_ones_mask);\n        __m256i condition = _mm256_cmpeq_epi32(low_nibble, all_ones_mask);\n        \n        // Perform rotation\n        __m256i rotated = _mm256_or_si256(\n            _mm256_slli_epi32(_mm256_andnot_si256(rotate_mask, vec_src), rotate_bits),\n            _mm256_srli_epi32(_mm256_and_si256(vec_src, rotate_mask), 32 - rotate_bits)\n        );\n        \n        // Perform bitwise negation\n        __m256i negated = _mm256_xor_si256(vec_src, _mm256_set1_epi32(-1));\n        \n        // Select based on condition\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n        \n        _mm256_storeu_si256((__m256i *)(dst + i * vec_len), result);\n    }\n    \n    // Handle remaining elements\n    for(size_t i = num_vecs * vec_len; i < length; ++i){\n        uint32_t val = src[i];\n        if((val & 0xF) == 0xF){\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        }\n        else{\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const size_t vec_len = 8;\n    const size_t num_vecs = length / vec_len;\n    const __m256i mask = _mm256_set1_epi32(0xF);\n    const __m256i ones = _mm256_set1_epi32(0xFFFFFFFF);\n\n    for(size_t i = 0; i < num_vecs; ++i){\n        __m256i vsrc = _mm256_loadu_si256((__m256i const*)(src + i * vec_len));\n        __m256i low_bits = _mm256_and_si256(vsrc, mask);\n        __m256i all_ones = _mm256_cmpeq_epi32(low_bits, mask);\n\n        // Rotate\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(vsrc, _mm256_set1_epi32(rotate_bits)),\n            _mm256_sllv_epi32(vsrc, _mm256_set1_epi32(32 - rotate_bits))\n        );\n\n        // Negate\n        __m256i negated = _mm256_xor_si256(vsrc, ones);\n\n        // Conditional select\n        __m256i result = _mm256_blendv_epi8(negated, rotated, all_ones);\n\n        _mm256_storeu_si256((__m256i *)(dst + i * vec_len), result);\n    }\n\n    // Handle remaining elements\n    for(size_t i = num_vecs * vec_len; i < length; ++i){\n        if((src[i] & 0xF) == 0xF){\n            dst[i] = (src[i] >> rotate_bits) | (src[i] << (32 - rotate_bits));\n        }\n        else{\n            dst[i] = ~src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const __m256i all_ones_low_4 = _mm256_set1_epi32(0xF);\n    const __m256i mask_all_ones = _mm256_set1_epi32(0xFFFFFFFF);\n    const __m256i rotate_mask = _mm256_set1_epi32((rotate_bits & 0x1F) | ((32 - rotate_bits) << 5));\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8){\n        __m256i vec_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_4_bits = _mm256_and_si256(vec_src, all_ones_low_4);\n        __m256i condition = _mm256_cmpeq_epi32(low_4_bits, all_ones_low_4);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srli_epi32(vec_src, rotate_bits),\n            _mm256_slli_epi32(vec_src, 32 - rotate_bits)\n        );\n        __m256i negated = _mm256_xor_si256(vec_src, mask_all_ones);\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i){\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF){\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        }\n        else{\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const size_t vec_len = 8;\n    const size_t loop_count = length / vec_len;\n    const __m256i mask_low_4bits = _mm256_set1_epi32(0xF);\n    const __m256i all_ones = _mm256_set1_epi32(0xFFFFFFFF);\n    const __m256i rotate_val = _mm256_set1_epi32(rotate_bits);\n\n    for(size_t i = 0; i < loop_count; ++i){\n        __m256i data = _mm256_loadu_si256((__m256i const*)(src + i * vec_len));\n        __m256i low_4bits = _mm256_and_si256(data, mask_low_4bits);\n        __m256i condition = _mm256_cmpeq_epi32(low_4bits, mask_low_4bits);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(data, _mm256_and_si256(rotate_val, _mm256_set1_epi32(31))),\n            _mm256_sllv_epi32(data, _mm256_sub_epi32(_mm256_set1_epi32(32), rotate_val))\n        );\n        __m256i negated = _mm256_xor_si256(data, all_ones);\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n        _mm256_storeu_si256((__m256i *)(dst + i * vec_len), result);\n    }\n\n    // Handle remaining elements\n    for(size_t i = loop_count * vec_len; i < length; ++i){\n        uint32_t val = src[i];\n        if((val & 0xF) == 0xF){\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        }\n        else{\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const __m256i all_ones = _mm256_set1_epi32(0xF);\n    const __m256i mask_low_nibble = _mm256_set1_epi32(0xF);\n    const __m256i rotate_mask = _mm256_set1_epi32((rotate_bits & 0x1F) | ((32 - rotate_bits) << 5));\n    \n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i vec_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low_nibble = _mm256_and_si256(vec_src, mask_low_nibble);\n        __m256i condition = _mm256_cmpeq_epi32(low_nibble, all_ones);\n\n        __m256i rotated = _mm256_or_si256(\n            _mm256_srlv_epi32(vec_src, _mm256_set1_epi32(rotate_bits)),\n            _mm256_sllv_epi32(vec_src, _mm256_set1_epi32(32 - rotate_bits))\n        );\n        __m256i negated = _mm256_xor_si256(vec_src, _mm256_set1_epi32(-1));\n\n        __m256i result = _mm256_blendv_epi8(negated, rotated, condition);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    const __m256 abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi8(1);\n    const __m256i minus_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        // Compare diff > abs(epsilon)\n        __m256 greater_mask = _mm256_cmp_ps(diff, abs_epsilon, _CMP_GT_OQ);\n        __m256i greater_result = _mm256_castps_si256(greater_mask);\n        greater_result = _mm256_and_si256(greater_result, one);\n\n        // Compare diff < -abs(epsilon)\n        __m256 less_mask = _mm256_cmp_ps(diff, _mm256_negate_ps(abs_epsilon), _CMP_LT_OQ);\n        __m256i less_result = _mm256_castps_si256(less_mask);\n        less_result = _mm256_and_si256(less_result, minus_one);\n\n        // Combine results\n        __m256i result = _mm256_or_si256(greater_result, less_result);\n        result = _mm256_add_epi8(result, _mm256_cmpeq_epi32(zero, zero)); // Ensure values are within [-1, 1]\n\n        // Store result\n        __m256i packed_result = _mm256_packs_epi16(result, result);\n        packed_result = _mm256_packus_epi16(packed_result, packed_result);\n        _mm_storel_epi64((__m128i*)(out + i), _mm256_castsi256_si128(packed_result));\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    __m256i zero = _mm256_set1_epi8(0);\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i minus_one = _mm256_set1_epi8(-1);\n\n    for (; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        __m256 greater_mask = _mm256_cmp_ps(diff, abs_epsilon, _CMP_GT_OQ);\n        __m256 less_mask = _mm256_cmp_ps(diff, _mm256_neg_ps(abs_epsilon), _CMP_LT_OQ);\n        __m256 within_mask = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_LE_OQ);\n\n        __m256i result = _mm256_blendv_epi8(\n            _mm256_blendv_epi8(minus_one, zero, _mm256_castps_si256(within_mask)),\n            one, _mm256_castps_si256(greater_mask)\n        );\n\n        __m256i packed_result = _mm256_packs_epi16(result, result);\n        __m128i final_result = _mm_packus_epi16(_mm256_castsi256_si128(packed_result), _mm256_extracti128_si256(packed_result, 1));\n\n        _mm_storel_epi64((__m128i *)(out + i), final_result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <smmintrin.h>\n#include <stdint.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    const __m256 abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    const __m256i zero = _mm256_set1_epi8(0);\n    const __m256i one = _mm256_set1_epi8(1);\n    const __m256i minus_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        __m256 greater = _mm256_cmp_ps(diff, abs_epsilon, _CMP_GT_OQ);\n        __m256 less = _mm256_cmp_ps(diff, _mm256_neg_ps(abs_epsilon), _CMP_LT_OQ);\n        __m256 within = _mm256_cmp_ps(abs_diff, abs_epsilon, _CMP_LE_OQ);\n\n        __m256i mask_greater = _mm256_castps_si256(greater);\n        __m256i mask_less = _mm256_castps_si256(less);\n        __m256i mask_within = _mm256_castps_si256(within);\n\n        __m256i result = _mm256_blendv_epi8(\n            _mm256_blendv_epi8(minus_one, zero, mask_within),\n            one, mask_greater\n        );\n\n        __m256i packed_result = _mm256_cvtepi32_epi8(result);\n        _mm_storeu_si128((__m128i*)(out + i), _mm256_castsi256_si128(packed_result));\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    const __m256 abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    const __m256i zero_i = _mm256_setzero_si256();\n    const __m256i one_i = _mm256_set1_epi8(1);\n    const __m256i minus_one_i = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 greater_mask_f = _mm256_cmp_ps(diff, abs_epsilon, _CMP_GT_OQ);\n        __m256 less_mask_f = _mm256_cmp_ps(diff, _mm256_neg_ps(abs_epsilon), _CMP_LT_OQ);\n\n        __m256i greater_mask_i = _mm256_castps_si256(greater_mask_f);\n        __m256i less_mask_i = _mm256_castps_si256(less_mask_f);\n\n        __m256i out_vec = _mm256_blendv_epi8(_mm256_blendv_epi8(zero_i, one_i, greater_mask_i), minus_one_i, less_mask_i);\n\n        __m256i packed_out = _mm256_packs_epi32(out_vec, out_vec);\n        packed_out = _mm256_packs_epi16(packed_out, packed_out);\n\n        __m128i low = _mm256_castsi256_si128(packed_out);\n        __m128i high = _mm256_extracti128_si256(packed_out, 1);\n        __m128i combined = _mm_packus_epi16(low, high);\n\n        _mm_storeu_si128((__m128i*)(out + i), combined);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 abs_epsilon = _mm256_set1_ps(fabsf(epsilon));\n    __m256i zero = _mm256_set1_epi8(0);\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i minus_one = _mm256_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n        __m256 diff = _mm256_sub_ps(a, b);\n\n        __m256 greater = _mm256_cmp_ps(diff, abs_epsilon, _CMP_GT_OQ);\n        __m256 less = _mm256_cmp_ps(diff, _mm256_neg_ps(abs_epsilon), _CMP_LT_OQ);\n\n        __m256i mask_greater = _mm256_castps_si256(greater);\n        __m256i mask_less = _mm256_castps_si256(less);\n\n        __m256i result = _mm256_blendv_epi8(_mm256_blendv_epi8(zero, minus_one, mask_less), one, mask_greater);\n\n        __m256i packed_result = _mm256_packs_epi32(result, result);\n        packed_result = _mm256_packs_epi16(packed_result, packed_result);\n\n        __m128i low = _mm256_castsi256_si128(packed_result);\n        __m128i high = _mm256_extracti128_si256(packed_result, 1);\n\n        _mm_storeu_si128((__m128i*)(out + i), low);\n        _mm_storeu_si128((__m128i*)(out + i + 4), high);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 max_f = _mm256_set1_ps(INT32_MAX);\n    const __m256 min_f = _mm256_set1_ps(INT32_MIN);\n    const size_t step = 8;\n\n    for (size_t i = 0; i <= size - step; i += step) {\n        __m256 vec_f = _mm256_loadu_ps(&A[i]);\n        __m256 clamped_vec = _mm256_min_ps(vec_f, max_f);\n        clamped_vec = _mm256_max_ps(clamped_vec, min_f);\n        __m256i vec_i = _mm256_cvtps_epi32(clamped_vec);\n        _mm256_storeu_si256((__m256i*)&B[i], vec_i);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size / step) * step; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) val = INT32_MAX;\n        else if (val < INT32_MIN) val = INT32_MIN;\n        B[i] = (int32_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 int32_max = _mm256_set1_ps(INT32_MAX);\n    const __m256 int32_min = _mm256_set1_ps(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256 vec_f = _mm256_loadu_ps(A + i);\n        __m256 vec_clamped = _mm256_min_ps(_mm256_max_ps(vec_f, int32_min), int32_max);\n        __m256i vec_i = _mm256_cvtps_epi32(vec_clamped);\n        _mm256_storeu_si256((__m256i*)(B + i), vec_i);\n    }\n\n    // Handle leftover elements\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) val = INT32_MAX;\n        else if (val < INT32_MIN) val = INT32_MIN;\n        B[i] = static_cast<int32_t>(val);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 f_max = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 f_min = _mm256_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec_f = _mm256_loadu_ps(A + i);\n        __m256 vec_clamped = _mm256_min_ps(_mm256_max_ps(vec_f, f_min), f_max);\n        __m256i vec_i = _mm256_cvtps_epi32(vec_clamped);\n        _mm256_storeu_si256((__m256i *)(B + i), vec_i);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) val = INT32_MAX;\n        else if (val < INT32_MIN) val = INT32_MIN;\n        B[i] = (int32_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 int32_max = _mm256_set1_ps(INT32_MAX);\n    const __m256 int32_min = _mm256_set1_ps(INT32_MIN);\n\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 floats = _mm256_loadu_ps(&A[i]);\n        __m256 clamped = _mm256_min_ps(floats, int32_max);\n        clamped = _mm256_max_ps(clamped, int32_min);\n        __m256i ints = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i*)&B[i], ints);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (A[i] > INT32_MAX) ? INT32_MAX : (A[i] < INT32_MIN) ? INT32_MIN : (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m256 fmax = _mm256_set1_ps(INT32_MAX);\n    const __m256 fmin = _mm256_set1_ps(INT32_MIN);\n    const size_t vec_size = 8;\n    \n    for (size_t i = 0; i <= size - vec_size; i += vec_size) {\n        __m256 floats = _mm256_loadu_ps(A + i);\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(floats, fmin), fmax);\n        __m256i ints = _mm256_cvtps_epi32(clamped);\n        _mm256_storeu_si256((__m256i *)(B + i), ints);\n    }\n    \n    // Handle leftover elements\n    size_t i = size & ~(vec_size - 1);\n    if (i < size) {\n        __m256 floats = _mm256_setzero_ps();\n        for (size_t j = 0; j < vec_size && i + j < size; ++j) {\n            ((float*)&floats)[j] = A[i + j];\n        }\n        __m256 clamped = _mm256_min_ps(_mm256_max_ps(floats, fmin), fmax);\n        __m256i ints = _mm256_cvtps_epi32(clamped);\n        int32_t temp[8];\n        _mm256_storeu_si256((__m256i *)temp, ints);\n        for (size_t j = 0; j < vec_size && i + j < size; ++j) {\n            B[i + j] = temp[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i = 0;\n\n    for (; i + 7 < size; i += 8) {\n        __m256i v_int = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 v_float = _mm256_cvtepi32_ps(v_int);\n        v_float = _mm256_mul_ps(v_float, scale_vec);\n        _mm256_storeu_ps(B + i, v_float);\n    }\n\n    for (; i < size; i++) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256 scale_vec = _mm256_set1_ps(scale);\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v_int = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 v_float = _mm256_cvtepi32_ps(v_int);\n        v_float = _mm256_mul_ps(v_float, scale_vec);\n        _mm256_storeu_ps(B + i, v_float);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i v_scale = _mm256_set1_epi8(scale);\n    const __m256i v_zero = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256i v_int = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i v_scaled = _mm256_mullo_epi32(_mm256_cvtepi8_epi32(v_scale), _mm256_cvtepi32_epi32(v_int));\n        __m256 v_float = _mm256_cvtepi32_ps(_mm256_slli_epi32(v_scaled, 0));\n        _mm256_storeu_ps(B + i, v_float);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256 scale_vec = _mm256_set1_ps((float)scale);\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v_int = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 v_float = _mm256_cvtepi32_ps(v_int);\n        v_float = _mm256_mul_ps(v_float, scale_vec);\n        _mm256_storeu_ps(B + i, v_float);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i broadcasted_scale = _mm256_set1_epi8(scale);\n    const __m256i zero = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i <= size - 8; i += 8) {\n        __m256i v_int = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256i v_sign = _mm256_sign_epi32(v_int, broadcasted_scale);\n        __m256 v_float = _mm256_cvtepi32_ps(v_sign);\n        _mm256_storeu_ps(B + i, v_float);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256i zero_mask = _mm256_set1_epi32(0x80000000);\n    __m256i ones_mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i as_int = _mm256_cvtps_epi32(rounded);\n        as_int = _mm256_and_si256(as_int, ones_mask);\n        as_int = _mm256_or_si256(as_int, zero_mask);\n        _mm256_storeu_si256((__m256i*)&C[i], as_int);\n    }\n\n    for (; i < size; ++i) {\n        float result = std::round(A[i] / B[i]);\n        C[i] = static_cast<int32_t>(result);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    const size_t vec_len = 8; // Number of elements processed per AVX2 operation (256 bits / 32 bits per float)\n    size_t i = 0;\n\n    for (; i <= size - vec_len; i += vec_len) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256 b = _mm256_loadu_ps(B + i);\n\n        __m256 div_result = _mm256_div_ps(a, b);\n        __m256 rounded_result = _mm256_round_ps(div_result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n        __m256i int_result = _mm256_cvtps_epi32(rounded_result);\n        _mm256_storeu_si256((__m256i *)(C + i), int_result);\n    }\n\n    // Handle the remaining elements with scalar code\n    for (; i < size; ++i) {\n        float result = std::round(A[i] / B[i]);\n        C[i] = static_cast<int32_t>(result);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <math.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    const size_t vec_len = 8;\n    const size_t rounds = size / vec_len;\n    const size_t remainder = size % vec_len;\n\n    for (size_t i = 0; i < rounds; ++i) {\n        __m256 a = _mm256_loadu_ps(A + i * vec_len);\n        __m256 b = _mm256_loadu_ps(B + i * vec_len);\n\n        __m256 div = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(div, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n\n        __m256i result = _mm256_cvtps_epi32(rounded);\n        _mm256_storeu_si256((__m256i *)(C + i * vec_len), result);\n    }\n\n    if (remainder > 0) {\n        __m256i mask = _mm256_set_epi32(\n            (remainder > 7) ? -1 : 0,\n            (remainder > 6) ? -1 : 0,\n            (remainder > 5) ? -1 : 0,\n            (remainder > 4) ? -1 : 0,\n            (remainder > 3) ? -1 : 0,\n            (remainder > 2) ? -1 : 0,\n            (remainder > 1) ? -1 : 0,\n            (remainder > 0) ? -1 : 0\n        );\n\n        __m256 a = _mm256_maskload_ps(A + rounds * vec_len, mask);\n        __m256 b = _mm256_maskload_ps(B + rounds * vec_len, mask);\n\n        __m256 div = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(div, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n\n        __m256i result = _mm256_cvtps_epi32(rounded);\n        _mm256_maskstore_epi32(C + rounds * vec_len, mask, result);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    const size_t vec_len = 8; // Number of elements in a YMM register for floats\n    size_t i = 0;\n\n    for (; i <= size - vec_len; i += vec_len) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 div = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i result = _mm256_cvtps_epi32(rounded);\n        _mm256_storeu_si256((__m256i*)&C[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        C[i] = static_cast<int32_t>( nearbyintf(A[i] / B[i]) );\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m256i zero_mask = _mm256_set1_epi32(0x80000000);\n    __m256i ones_mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (; i + 7 < size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256 b = _mm256_loadu_ps(&B[i]);\n        __m256 result = _mm256_div_ps(a, b);\n        __m256 rounded = _mm256_round_ps(result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256i as_int = _mm256_cvtps_epi32(rounded);\n        as_int = _mm256_and_si256(as_int, ones_mask);\n        as_int = _mm256_or_si256(as_int, zero_mask);\n        _mm256_storeu_si256((__m256i*)&C[i], as_int);\n    }\n\n    for (; i < size; ++i) {\n        float res = A[i] / B[i];\n        C[i] = static_cast<int32_t>(std::round(res));\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n#include <math.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 max_val = _mm256_set1_ps(FLT_MAX);\n    const __m256 min_val = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m256d vec_double = _mm256_loadu_pd(A + i);\n        __m256 vec_float = _mm256_cvtpd_ps(vec_double);\n\n        __m256 mask_max = _mm256_cmp_ps(vec_float, max_val, _CMP_GT_OQ);\n        vec_float = _mm256_min_ps(vec_float, max_val);\n\n        __m256 mask_min = _mm256_cmp_ps(vec_float, min_val, _CMP_LT_OQ);\n        vec_float = _mm256_max_ps(vec_float, min_val);\n\n        _mm256_storeu_ps(B + i, vec_float);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256 flt_max = _mm256_set1_ps(FLT_MAX);\n    const __m256 flt_min = _mm256_set1_ps(-FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m256d vec_double = _mm256_loadu_pd(A + i);\n        __m256 vec_float = _mm256_cvtpd_ps(vec_double);\n\n        __m256 vec_float_max = _mm256_max_ps(vec_float, flt_min);\n        __m256 vec_float_saturated = _mm256_min_ps(vec_float_max, flt_max);\n\n        _mm256_storeu_ps(B + i, vec_float_saturated);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = static_cast<float>(val);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_d = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d min_d = _mm256_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d vec_d = _mm256_loadu_pd(A + i);\n        __m256d vec_clamped = _mm256_max_pd(_mm256_min_pd(vec_d, max_d), min_d);\n        __m256 vec_f = _mm256_cvtpd_ps(vec_clamped);\n        _mm256_storeu_ps(B + i, vec_f);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_double = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d min_double = _mm256_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d vec_d = _mm256_loadu_pd(A + i);\n        __m256d vec_clamped = _mm256_max_pd(_mm256_min_pd(vec_d, max_double), min_double);\n        __m256 vec_f = _mm256_cvtpd_ps(vec_clamped);\n        _mm256_storeu_ps(B + i, vec_f);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m256d max_double = _mm256_set1_pd((double)FLT_MAX);\n    const __m256d min_double = _mm256_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m256d vec_d = _mm256_loadu_pd(A + i);\n        __m256d vec_clamped = _mm256_max_pd(_mm256_min_pd(vec_d, max_double), min_double);\n        __m256 vec_f = _mm256_cvtpd_ps(vec_clamped);\n        _mm256_storeu_ps(B + i, vec_f);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    \n    for (; i + 16 <= size; i += 16) {\n        __m256i vec = _mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i)));\n        _mm256_storeu_si256((__m256i *)(B + i), _mm256_unpacklo_epi32(vec, zero));\n        _mm256_storeu_si256((__m256i *)(B + i + 8), _mm256_unpackhi_epi32(vec, zero));\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t vec_size = 16; // Number of int16_t elements in a __m256i\n    size_t i = 0;\n\n    for (; i <= size - vec_size; i += vec_size) {\n        __m256i v16 = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i v32_lo = _mm256_cvtepi16_epi32(v16);\n        __m256i v32_hi = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(v16, 1));\n        _mm256_storeu_si256((__m256i *)(B + 2 * i), v32_lo);\n        _mm256_storeu_si256((__m256i *)(B + 2 * i + 8), v32_hi);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const __m256i zero = _mm256_setzero_si256();\n    size_t i = 0;\n    for (; i + 16 <= size; i += 16) {\n        __m256i low = _mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i)));\n        __m256i high = _mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i + 8)));\n        _mm256_storeu_si256((__m256i *)(B + i), low);\n        _mm256_storeu_si256((__m256i *)(B + i + 8), high);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const __m256i zero = _mm256_setzero_si256();\n    size_t i = 0;\n    for (; i + 16 <= size; i += 16) {\n        __m256i low = _mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i)));\n        __m256i high = _mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i + 8)));\n        _mm256_storeu_si256((__m256i *)(B + i), low);\n        _mm256_storeu_si256((__m256i *)(B + i + 8), high);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    for (; i <= size - 16; i += 16) {\n        __m256i low = _mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i)));\n        __m256i high = _mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i + 8)));\n        _mm256_storeu_si256((__m256i *)(B + i), low);\n        _mm256_storeu_si256((__m256i *)(B + i + 8), high);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m256 min = _mm256_set1_ps(min_val);\n    const __m256 max = _mm256_set1_ps(max_val);\n    const __m256 range = _mm256_sub_ps(max, min);\n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(&A[i]);\n        __m256i ctrl = _mm256_loadu_si256((const __m256i*)&control[i]);\n        __m256 cond = _mm256_cmpgt_epi32(ctrl, _mm256_setzero_si256());\n        __m256 cond_f = _mm256_castsi256_ps(_mm256_and_si256(cond, _mm256_set1_epi32(-1)));\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a, min), range);\n        normalized = _mm256_max_ps(normalized, zero);\n        normalized = _mm256_min_ps(normalized, one);\n\n        __m256 result = _mm256_blendv_ps(a, normalized, cond_f);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (size_t i = (size / 8) * 8; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = (val < 0.0f) ? 0.0f : ((val > 1.0f) ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m256 min_vec = _mm256_set1_ps(min_val);\n    const __m256 max_vec = _mm256_set1_ps(max_val);\n    const __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    const __m256 zero_vec = _mm256_setzero_ps();\n    const __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vals = _mm256_loadu_ps(A + i);\n        __m256i ctrl_vals = _mm256_loadu_si256((__m256i const *)(control + i));\n        __m256 cond = _mm256_cmpgt_epi32(ctrl_vals, _mm256_setzero_si256());\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vals, min_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        __m256 result = _mm256_blendv_ps(a_vals, normalized, _mm256_castsi256_ps(cond));\n        _mm256_storeu_ps(B + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = (val < 0.0f) ? 0.0f : ((val > 1.0f) ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m256 min_val_vec = _mm256_set1_ps(min_val);\n    const __m256 max_val_vec = _mm256_set1_ps(max_val);\n    const __m256 one_vec = _mm256_set1_ps(1.0f);\n    const __m256 zero_vec = _mm256_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i ctrl_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&control[i]));\n        __m256 cond_mask = _mm256_cmpgt_epi32(ctrl_vec, _mm256_setzero_si256());\n        __m256 cond_mask_f = _mm256_castsi256_ps(_mm256_and_si256(cond_mask, _mm256_set1_epi32(-1)));\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), _mm256_sub_ps(max_val_vec, min_val_vec));\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, cond_mask_f);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = (val < 0.0f) ? 0.0f : ((val > 1.0f) ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m256 min_val_v = _mm256_set1_ps(min_val);\n    const __m256 max_val_v = _mm256_set1_ps(max_val);\n    const __m256 range_v = _mm256_sub_ps(max_val_v, min_val_v);\n    const __m256 zero_v = _mm256_setzero_ps();\n    const __m256 one_v = _mm256_set1_ps(1.0f);\n\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8) {\n        __m256 a = _mm256_loadu_ps(A + i);\n        __m256i ctrl = _mm256_loadu_si256((__m256i const *)(control + i));\n        __m256 cond = _mm256_cmpgt_epi32(ctrl, _mm256_setzero_si256());\n\n        __m256 norm = _mm256_div_ps(_mm256_sub_ps(a, min_val_v), range_v);\n        norm = _mm256_max_ps(norm, zero_v);\n        norm = _mm256_min_ps(norm, one_v);\n\n        __m256 result = _mm256_blendv_ps(a, norm, _mm256_castsi256_ps(cond));\n        _mm256_storeu_ps(B + i, result);\n    }\n\n    for (; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = (val < 0.0f) ? 0.0f : ((val > 1.0f) ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m256 min_vec = _mm256_set1_ps(min_val);\n    const __m256 max_vec = _mm256_set1_ps(max_val);\n    const __m256 range_vec = _mm256_sub_ps(max_vec, min_vec);\n    const __m256 zero_vec = _mm256_setzero_ps();\n    const __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i + 8 <= size; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256i ctrl_vec = _mm256_loadu_si256((const __m256i*)&control[i]);\n        __m256 cond_vec = _mm256_cmpgt_epi32(ctrl_vec, _mm256_setzero_si256());\n        __m256 mask_vec = _mm256_castsi256_ps(cond_vec);\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_vec), range_vec);\n        normalized = _mm256_max_ps(normalized, zero_vec);\n        normalized = _mm256_min_ps(normalized, one_vec);\n\n        __m256 result = _mm256_blendv_ps(a_vec, normalized, mask_vec);\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (size_t i = size & ~7; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i zero = _mm256_set1_epi32(0x0000FFFF);\n    for (; i + 7 < size; i += 8) {\n        __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i masked = _mm256_and_si256(a, zero);\n        __m256 converted = _mm256_cvtepi32_ps(masked);\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(&B[i], sqrt_result);\n    }\n    for (; i < size; ++i) {\n        uint32_t val = A[i] & 0xFFFF;\n        B[i] = static_cast<float>(val);\n        B[i] = sqrtf(B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_size = 8; // Number of elements processed per iteration with AVX2\n    size_t i = 0;\n\n    for (; i <= size - vec_size; i += vec_size) {\n        __m256i a_int = _mm256_loadu_si256((__m256i const *)(A + i));\n        __m256i high_bits = _mm256_srli_epi32(a_int, 16); // Shift right to get high 16 bits\n\n        __m256 converted = _mm256_cvtepi32_ps(high_bits); // Convert to float\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);   // Take square root\n\n        _mm256_storeu_ps(B + i, sqrt_result); // Store result\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        uint32_t val = A[i];\n        uint16_t high = val >> 16;\n        float fval = (float)high;\n        B[i] = sqrtf(fval);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_len = 8;\n    size_t i = 0;\n\n    for (; i <= size - vec_len; i += vec_len) {\n        __m256i a_int = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i high_bits = _mm256_srli_epi32(a_int, 16);\n        __m256 converted = _mm256_cvtepi32_ps(_mm256_castsi256_si128(high_bits));\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(&B[i], sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t val = A[i] >> 16;\n        float fval = static_cast<float>(val);\n        B[i] = sqrtf(fval);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_size = 8; // Number of elements processed per iteration with AVX2\n    size_t i = 0;\n\n    for (; i <= size - vec_size; i += vec_size) {\n        __m256i v_int = _mm256_loadu_si256((__m256i const *)(A + i)); // Load 8 uint32_t values\n        __m256i v_high_bits = _mm256_srli_epi32(v_int, 16); // Shift right to extract high 16 bits\n\n        // Convert high 16 bits (uint16_t) to float\n        __m256 v_float = _mm256_cvtepi32_ps(_mm256_cvtepu16_epi32(v_high_bits));\n\n        // Compute square root of the float values\n        __m256 v_sqrt = _mm256_sqrt_ps(v_float);\n\n        // Store the results\n        _mm256_storeu_ps(B + i, v_sqrt);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        uint32_t val = A[i];\n        uint16_t high_bits = val >> 16;\n        float fval = static_cast<float>(high_bits);\n        B[i] = sqrtf(fval);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_len = 8;\n    size_t i = 0;\n\n    for (; i <= size - vec_len; i += vec_len) {\n        __m256i a_int = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i high_bits = _mm256_srli_epi32(a_int, 16);\n        __m256 converted = _mm256_cvtepi32_ps(_mm256_castsi256_si128(high_bits));\n        __m256 sqrt_result = _mm256_sqrt_ps(converted);\n        _mm256_storeu_ps(&B[i], sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t val = A[i] >> 16;\n        float fval = static_cast<float>(val);\n        B[i] = sqrtf(fval);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_size = 8; // Number of elements in a __m256i or __m256 vector\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= size - vec_size; i += vec_size) {\n        __m256i int_vec = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 float_vec = _mm256_castsi256_ps(int_vec);\n        _mm256_storeu_ps(B + i, float_vec);\n    }\n\n    // Handle remaining scalar elements\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_size = 8; // Number of elements processed per AVX operation\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= size - vec_size; i += vec_size) {\n        __m256i loaded_ints = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 converted_floats = _mm256_castsi256_ps(loaded_ints);\n        _mm256_storeu_ps(B + i, converted_floats);\n    }\n\n    // Handle remaining scalars\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i vzero = _mm256_setzero_si256();\n    for (; i <= size - 8; i += 8) {\n        __m256i vint = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 vfloat = _mm256_castsi256_ps(vint);\n        _mm256_storeu_ps(B + i, vfloat);\n    }\n    for (; i < size; i++) {\n        B[i] = *(float*)&(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_size = 8; // Number of elements in a YMM register for uint32_t/float\n    size_t i = 0;\n\n    // Process vectors\n    for (; i <= size - vec_size; i += vec_size) {\n        __m256i loaded_ints = _mm256_loadu_si256((const __m256i*)&A[i]);\n        __m256 converted_floats = _mm256_castsi256_ps(loaded_ints);\n        _mm256_storeu_ps(&B[i], converted_floats);\n    }\n\n    // Handle leftovers with scalar code\n    for (; i < size; ++i) {\n        reinterpret_cast<float&>(B[i]) = reinterpret_cast<const float&>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_size = 8; // Number of elements in a 256-bit register for uint32_t/float\n    const size_t aligned_size = size / vec_size * vec_size;\n\n    // Process vectors\n    for (size_t i = 0; i < aligned_size; i += vec_size) {\n        __m256i load_int = _mm256_loadu_si256((__m256i const*)(A + i));\n        __m256 cast_float = _mm256_castsi256_ps(load_int);\n        _mm256_storeu_ps(B + i, cast_float);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_size; i < size; ++i) {\n        reinterpret_cast<float&>(B[i]) = reinterpret_cast<const float&>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    diff[0] = 0.0f;\n\n    const __m256i zero_i = _mm256_setzero_si256();\n    const __m256 zero_f = _mm256_setzero_ps();\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i a_curr = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i a_prev = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n\n        __m256i diff_i = _mm256_sub_epi32(a_curr, a_prev);\n\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n\n        __m256 mask = _mm256_castsi256_ps(_mm256_set_epi32(\n            cond[i + 7] ? 0xFFFFFFFF : 0,\n            cond[i + 6] ? 0xFFFFFFFF : 0,\n            cond[i + 5] ? 0xFFFFFFFF : 0,\n            cond[i + 4] ? 0xFFFFFFFF : 0,\n            cond[i + 3] ? 0xFFFFFFFF : 0,\n            cond[i + 2] ? 0xFFFFFFFF : 0,\n            cond[i + 1] ? 0xFFFFFFFF : 0,\n            cond[i + 0] ? 0xFFFFFFFF : 0\n        ));\n\n        diff_f = _mm256_and_ps(mask, diff_f);\n        diff_f = _mm256_or_ps(diff_f, _mm256_andnot_ps(mask, zero_f));\n\n        _mm256_storeu_ps(&diff[i], diff_f);\n    }\n\n    for (; i < size; ++i) {\n        diff[i] = cond[i] ? static_cast<float>(A[i] - A[i - 1]) : 0.0f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    const __m256i zero_i = _mm256_setzero_si256();\n    const __m256 zero_f = _mm256_setzero_ps();\n\n    for (size_t i = 1; i < size; i += 8) {\n        size_t j = i + 8 <= size ? 8 : size - i;\n        __m256i a_curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i a_prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i - 1]));\n\n        __m256i diff_i = _mm256_sub_epi32(a_curr, a_prev);\n\n        __m256 cond_v = _mm256_castsi256_ps(_mm256_set_epi32(\n            (i + 7 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 7] : 0,\n            (i + 6 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 6] : 0,\n            (i + 5 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 5] : 0,\n            (i + 4 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 4] : 0,\n            (i + 3 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 3] : 0,\n            (i + 2 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 2] : 0,\n            (i + 1 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 1] : 0,\n            (i + 0 < size) ? reinterpret_cast<const int32_t*>(cond)[i + 0] : 0\n        ));\n\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n        __m256 mask = _mm256_and_ps(cond_v, diff_f);\n        mask = _mm256_or_ps(mask, _mm256_andnot_ps(cond_v, zero_f));\n\n        _mm256_storeu_ps(&diff[i], mask);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n    diff[0] = 0.0f;\n\n    const __m256i zero_i = _mm256_setzero_si256();\n    const __m256 zero_f = _mm256_setzero_ps();\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i - 1]));\n\n        __m256i diff_i = _mm256_sub_epi32(current, prev);\n\n        // Load condition array as mask\n        __m256i cond_mask = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond[i]));\n        cond_mask = _mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(1));\n\n        // Zero out differences where condition is false\n        diff_i = _mm256_and_si256(diff_i, cond_mask);\n\n        // Convert integer differences to float\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n\n        // Set diff[i] to 0.0f where condition is false\n        __m256 cond_mask_f = _mm256_castsi256_ps(_mm256_cmpeq_epi32(cond_mask, zero_i));\n        diff_f = _mm256_or_ps(_mm256_and_ps(cond_mask_f, diff_f), _mm256_andnot_ps(cond_mask_f, zero_f));\n\n        _mm256_storeu_ps(&diff[i], diff_f);\n    }\n\n    for (; i < size; ++i) {\n        diff[i] = cond[i] ? static_cast<float>(A[i] - A[i - 1]) : 0.0f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    const __m256i zero_i = _mm256_setzero_si256();\n    const __m256 zero_f = _mm256_setzero_ps();\n    const __m256i one_i = _mm256_set1_epi32(1);\n\n    size_t i = 1;\n    for (; i + 7 < size; i += 8) {\n        __m256i a_curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i a_prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i - 1]));\n\n        __m256i diff_i = _mm256_sub_epi32(a_curr, a_prev);\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n\n        __m256i cond_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&cond[i]));\n        __m256i cond_mask = _mm256_cmpgt_epi32(cond_vec, zero_i); // cond > 0\n\n        __m256 mask_f = _mm256_castsi256_ps(_mm256_and_si256(cond_mask, one_i));\n        __m256 result = _mm256_blendv_ps(zero_f, diff_f, mask_f);\n\n        _mm256_storeu_ps(&diff[i], result);\n    }\n\n    for (; i < size; ++i) {\n        diff[i] = cond[i] ? static_cast<float>(A[i] - A[i - 1]) : 0.0f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    const __m256i zero_i = _mm256_setzero_si256();\n    const __m256 zero_f = _mm256_setzero_ps();\n    const __m256i one_i = _mm256_set1_epi32(1);\n\n    size_t i = 1;\n    for (; i <= size - 8; i += 8) {\n        __m256i a_curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i]));\n        __m256i a_prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i - 1]));\n\n        __m256i diff_i = _mm256_sub_epi32(a_curr, a_prev);\n\n        // Load condition array as integer mask (true=0xFFFFFFFF, false=0x0)\n        __m256i cond_mask = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(cond + i));\n        cond_mask = _mm256_cmpeq_epi32(cond_mask, _mm256_set1_epi32(-1)); // true if cond[i] is true\n\n        // Set diff_i to 0 where cond[i] is false\n        diff_i = _mm256_and_si256(diff_i, cond_mask);\n\n        // Convert to float\n        __m256 diff_f = _mm256_cvtepi32_ps(diff_i);\n\n        // Store result\n        _mm256_storeu_ps(&diff[i], diff_f);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < size; ++i) {\n        diff[i] = cond[i] ? static_cast<float>(A[i] - A[i - 1]) : 0.0f;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF); // Set high 32 bits to 1\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i low = _mm256_loadu_si256((__m256i*)(src + i)); // Load 8 x 32-bit integers\n        __m256i high = high_bits; // Broadcast the high bits mask\n\n        // Interleave lower and higher parts into 64-bit integers\n        __m256i interleaved_low = _mm256_unpacklo_epi32(low, high);\n        __m256i interleaved_high = _mm256_unpackhi_epi32(low, high);\n\n        // Store the results\n        _mm256_storeu_si256((__m256i*)(dst + i), interleaved_low);\n        _mm256_storeu_si256((__m256i*)(dst + i + 4), interleaved_high);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)(src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m256i low = _mm256_loadu_si256((__m256i const*)(src + i));\n        __m256i high = high_bits;\n        __m256i converted = _mm256_cvtepu32_epi64(low);\n        __m256i result = _mm256_or_si256(converted, _mm256_slli_epi64(high, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)(src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i high_bits = _mm256_set1_epi64x(0xFFFFFFFF00000000);\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        __m256i loaded = _mm256_cvtepu32_epi64(_mm_loadu_si128((__m128i const*)(src + i)));\n        __m256i masked = _mm256_or_si256(loaded, high_bits);\n        _mm256_storeu_si256((__m256i*)(dst + i), masked);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)(src[i]) | 0xFFFFFFFF00000000;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i v = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i low = _mm256_cvtepu32_epi64(v);\n        __m256i high = _mm256_cvtepu32_epi64(_mm256_and_si256(v, high_bits));\n        __m256i result = _mm256_or_si256(low, _mm256_slli_epi64(high, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (uint64_t)(src[i]) | ((uint64_t)0xFFFFFFFF << 32);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m256i low = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&src[i]));\n        __m256i high = high_bits;\n        __m256i result_low = _mm256_cvtepu32_epi64(low);\n        __m256i result_high = _mm256_cvtepu32_epi64(_mm256_srli_epi64(_mm256_slli_epi64(low, 32), 32));\n        __m256i combined_low = _mm256_or_si256(result_low, _mm256_slli_epi64(high, 32));\n        __m256i combined_high = _mm256_or_si256(result_high, _mm256_slli_epi64(high, 32));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i]), combined_low);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&dst[i + 4]), combined_high);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)(src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx_vec = _mm_loadu_si128((__m128i const*)(index + i));\n        __m128i idx_broadcast = _mm_cvtepi32_epi64(_mm_cvtepi16_epi32(idx_vec));\n\n        __m256d gather_vec = _mm256_i64gather_pd(vec, idx_broadcast, 8);\n        sum = _mm256_add_pd(sum, gather_vec);\n    }\n\n    double result = 0.0;\n    __m256d temp = _mm256_permute4x64_pd(sum, 0x93);\n    sum = _mm256_add_pd(sum, temp);\n    temp = _mm256_permute4x64_pd(sum, 0xF);\n    sum = _mm256_add_pd(sum, temp);\n\n    result += _mm256_extract_double_pd(sum, 0);\n    result += _mm256_extract_double_pd(sum, 1);\n\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    __m256i indices;\n    __m256d values;\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i max_index = _mm256_set1_epi32(length - 1);\n\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        indices = _mm256_loadu_si256((const __m256i*)(index + i));\n        indices = _mm256_and_si256(_mm256_cmpgt_epi16(indices, zero), max_index);\n        values = _mm256_i32gather_pd((const double*)vec, indices, 8);\n        sum = _mm256_add_pd(sum, values);\n    }\n\n    double result = 0.0;\n    __m256d temp = _mm256_permute4x64_pd(sum, 0b01001110);\n    sum = _mm256_add_pd(sum, temp);\n    temp = _mm256_permute4x64_pd(sum, 0b11100001);\n    sum = _mm256_add_pd(sum, temp);\n    _mm256_storeu_pd(&result, sum);\n\n    for (; i < length; ++i) {\n        if (index[i] >= 0 && index[i] < (int16_t)length) {\n            result += vec[index[i]];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    const size_t vector_length = 4;\n    size_t i = 0;\n\n    for (; i <= length - vector_length; i += vector_length) {\n        __m256i idx = _mm256_loadu_si256((const __m256i*)(index + i));\n        __m256d gather = _mm256_i32gather_pd((const double*)vec, idx, 8);\n        sum_vec = _mm256_add_pd(sum_vec, gather);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (; i < length; ++i) {\n        total_sum += vec[index[i]];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m256i idx_vec = _mm256_loadu_si256((const __m256i*)(index + i));\n        __m256d gather_vec = _mm256_i32gather_pd((const double*)vec, idx_vec, 8);\n        sum = _mm256_add_pd(sum, gather_vec);\n    }\n\n    double result = 0.0;\n    __m256d temp = _mm256_setzero_pd();\n\n    if (i < length) {\n        __m256i mask = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n        for (size_t j = i; j < length; ++j) {\n            mask = _mm256_or_si256(mask, _mm256_set_epi32(0, 0, 0, 1 << (j - i)));\n        }\n        __m256i idx_vec = _mm256_maskload_epi32((const int*)(index + i), mask);\n        __m256d gather_vec = _mm256_mask_i32gather_pd(temp, (const double*)vec, idx_vec, _mm256_set1_pd(0.0), 8);\n        sum = _mm256_add_pd(sum, gather_vec);\n    }\n\n    __m128d sum_low = _mm256_castpd256_pd128(sum);\n    __m128d sum_high = _mm256_extractf128_pd(sum, 1);\n    __m128d total = _mm_add_pd(sum_low, sum_high);\n\n    double lower = _mm_cvtsd_f64(total);\n    double upper = _mm_cvtsd_f64(_mm_unpackhi_pd(total, total));\n\n    result = lower + upper;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    const size_t vector_length = 4;\n    size_t i = 0;\n\n    for (; i <= length - vector_length; i += vector_length) {\n        __m128i idx_vec = _mm_loadu_si128((__m128i const*)(index + i));\n        __m128i idx_broadcast = _mm_cvtepi32_epi64(_mm_cvtepi16_epi32(idx_vec));\n\n        __m256d gather_vec = _mm256_i32gather_pd(vec, idx_broadcast, 8);\n        sum = _mm256_add_pd(sum, gather_vec);\n    }\n\n    double result = 0.0;\n    __m256d temp = _mm256_permute4x64_pd(sum, 0b11100100);\n    sum = _mm256_add_pd(sum, temp);\n    temp = _mm256_permute4x64_pd(sum, 0b00010000);\n    sum = _mm256_add_pd(sum, temp);\n\n    result += _mm256_cvtsd_f64(sum);\n\n    for (; i < length; ++i) {\n        result += vec[index[i]];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d vzero = _mm256_setzero_pd();\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = vzero;\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; n += 4) {\n                    __m256d vk = _mm256_loadu_pd(&kernel[m * kernel_size + n]);\n                    __m256d vi = _mm256_set_pd(\n                        input[(i + m) * input_size + j + n + 3],\n                        input[(i + m) * input_size + j + n + 2],\n                        input[(i + m) * input_size + j + n + 1],\n                        input[(i + m) * input_size + j + n]\n                    );\n                    __m256d vmul = _mm256_mul_pd(vi, vk);\n                    sum = _mm256_add_pd(sum, vmul);\n                }\n            }\n            sum = _mm256_max_pd(sum, vzero); // ReLU activation\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            output[i * output_size + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d vzero = _mm256_setzero_pd();\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t p = 0; p < kernel_size; ++p) {\n                for (size_t q = 0; q < kernel_size; q += 4) {\n                    __m256d vk = _mm256_loadu_pd(&kernel[p * kernel_size + q]);\n                    __m256d vi = _mm256_set_pd(\n                        input[(i + p) * input_size + (j + q + 3)],\n                        input[(i + p) * input_size + (j + q + 2)],\n                        input[(i + p) * input_size + (j + q + 1)],\n                        input[(i + p) * input_size + (j + q)]\n                    );\n                    __m256d prod = _mm256_mul_pd(vk, vi);\n                    sum = _mm256_add_pd(sum, prod);\n                }\n            }\n            sum = _mm256_max_pd(sum, vzero); // ReLU activation\n            double result[4];\n            _mm256_storeu_pd(result, sum);\n            output[i * output_size + j] = result[0] + result[1] + result[2] + result[3];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d k_vec[4];\n    \n    for(size_t i = 0; i < kernel_size; ++i){\n        for(size_t j = 0; j < kernel_size; j+=4){\n            if(j+3 < kernel_size){\n                k_vec[j/4] = _mm256_loadu_pd(&kernel[i*kernel_size + j]);\n            }\n        }\n    }\n\n    for(size_t i = 0; i < output_size; ++i){\n        for(size_t j = 0; j < output_size; ++j){\n            __m256d sum = _mm256_setzero_pd();\n            for(size_t ki = 0; ki < kernel_size; ++ki){\n                for(size_t kj = 0; kj < kernel_size; kj+=4){\n                    if(kj+3 < kernel_size){\n                        __m256d in = _mm256_loadu_pd(&input[(i+ki)*input_size + j+kj]);\n                        __m256d ker = k_vec[kj/4];\n                        __m256d prod = _mm256_mul_pd(in, ker);\n                        sum = _mm256_add_pd(sum, prod);\n                    }\n                }\n            }\n            double res[4];\n            _mm256_storeu_pd(res, sum);\n            double out_val = res[0] + res[1] + res[2] + res[3];\n            out_val = out_val > 0 ? out_val : 0;\n            output[i*output_size + j] = out_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d k_vec[4];\n    \n    for (size_t i = 0; i < kernel_size; ++i) {\n        for (size_t j = 0; j < kernel_size; j += 4) {\n            if (j + 4 <= kernel_size) {\n                k_vec[j / 4] = _mm256_loadu_pd(&kernel[i * kernel_size + j]);\n            }\n        }\n    }\n\n    for (size_t row = 0; row < output_size; ++row) {\n        for (size_t col = 0; col < output_size; ++col) {\n            __m256d sum = _mm256_setzero_pd();\n            \n            for (size_t i = 0; i < kernel_size; ++i) {\n                for (size_t j = 0; j < kernel_size; j += 4) {\n                    if (j + 4 <= kernel_size) {\n                        __m256d in_vec = _mm256_loadu_pd(&input[(row + i) * input_size + col + j]);\n                        __m256d k_vec_curr = k_vec[j / 4];\n                        __m256d prod = _mm256_mul_pd(in_vec, k_vec_curr);\n                        sum = _mm256_add_pd(sum, prod);\n                    }\n                }\n            }\n\n            double result_arr[4] = {0};\n            _mm256_storeu_pd(result_arr, sum);\n\n            double result = 0.0;\n            for (size_t i = 0; i < 4; ++i) {\n                result += result_arr[i];\n            }\n\n            result = result > 0 ? result : 0;\n            output[row * output_size + col] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m256d k_vec[kernel_size * kernel_size];\n    \n    for (size_t i = 0; i < kernel_size * kernel_size; ++i) {\n        k_vec[i] = _mm256_set1_pd(kernel[i]);\n    }\n\n    for (size_t row = 0; row < output_size; ++row) {\n        for (size_t col = 0; col < output_size; col += 4) {\n            __m256d sum_vec = _mm256_setzero_pd();\n            \n            for (size_t k_row = 0; k_row < kernel_size; ++k_row) {\n                for (size_t k_col = 0; k_col < kernel_size; ++k_col) {\n                    size_t in_idx = ((row + k_row) * input_size + col + k_col);\n                    __m256d in_vec = _mm256_loadu_pd(&input[in_idx]);\n                    __m256d k_val = k_vec[k_row * kernel_size + k_col];\n                    __m256d prod = _mm256_mul_pd(in_vec, k_val);\n                    sum_vec = _mm256_add_pd(sum_vec, prod);\n                }\n            }\n            \n            sum_vec = _mm256_max_pd(sum_vec, _mm256_setzero_pd());\n            _mm256_storeu_pd(&output[row * output_size + col], sum_vec);\n        }\n        \n        // Handle remaining elements if output_size is not a multiple of 4\n        for (size_t col = output_size & ~size_t(3); col < output_size; ++col) {\n            double sum = 0.0;\n            for (size_t k_row = 0; k_row < kernel_size; ++k_row) {\n                for (size_t k_col = 0; k_col < kernel_size; ++k_col) {\n                    size_t in_idx = ((row + k_row) * input_size + col + k_col);\n                    sum += input[in_idx] * kernel[k_row * kernel_size + k_col];\n                }\n            }\n            output[row * output_size + col] = (sum > 0) ? sum : 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n#include <algorithm>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    size_t n = numbers.size();\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n\n    for(size_t i = 0; i < n; ++i){\n        __m256d x = _mm256_set1_pd(numbers[i]);\n        for(size_t j = i + 1; j < n; j += 4){\n            size_t window = std::min(j + 4, n) - j;\n            __m256d y = _mm256_set_pd(\n                window > 3 ? numbers[j+3] : 0,\n                window > 2 ? numbers[j+2] : 0,\n                window > 1 ? numbers[j+1] : 0,\n                window > 0 ? numbers[j+0] : 0\n            );\n            __m256d diff = _mm256_sub_pd(y, x);\n            __m256d abs_diff = _mm256_and_pd(diff, _mm256_set1_pd(0x7FFFFFFFFFFFFFFF));\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n            if(_mm256_movemask_pd(cmp) != 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <algorithm>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    const int n = numbers.size();\n    const int vec_size = 4; // Number of elements in a YMM register for double\n    const int aligned_size = n - (n % vec_size);\n\n    __m256d thresh = _mm256_set1_pd(threshold);\n\n    for(int i = 0; i < aligned_size; i += vec_size){\n        __m256d a = _mm256_loadu_pd(&numbers[i]);\n        for(int j = i + 1; j < aligned_size; j += vec_size){\n            __m256d b = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(a, b);\n            __m256d abs_diff = _mm256_and_pd(diff, _mm256_set1_pd(0x7FFFFFFFFFFFFFFF));\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh, _CMP_LT_OS);\n            if(_mm256_movemask_pd(cmp) != 0){\n                return true;\n            }\n        }\n    }\n\n    // Handle remaining unaligned elements\n    for(int i = aligned_size; i < n; ++i){\n        for(int j = i + 1; j < n; ++j){\n            if(std::fabs(numbers[i] - numbers[j]) < threshold){\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    const int vec_size = 4;\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    size_t i = 0;\n\n    for (; i + vec_size <= numbers.size(); i += vec_size) {\n        __m256d vec_i = _mm256_loadu_pd(&numbers[i]);\n        for (size_t j = i + vec_size; j + vec_size <= numbers.size(); j += vec_size) {\n            __m256d vec_j = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vec_i, vec_j);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n            if (_mm256_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    if (i < numbers.size()) {\n        __m256d vec_i = _mm256_setzero_pd();\n        for (size_t k = 0; k < vec_size && i + k < numbers.size(); ++k) {\n            vec_i = _mm256_insertf128_pd(vec_i, _mm_load_sd(&numbers[i + k]), k / 2);\n        }\n        for (size_t j = i + vec_size; j + vec_size <= numbers.size(); j += vec_size) {\n            __m256d vec_j = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(vec_i, vec_j);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n            if (_mm256_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    const int size = numbers.size();\n    const __m256d thresh_vec = _mm256_set1_pd(threshold);\n\n    for(int i = 0; i < size; ++i){\n        __m256d a = _mm256_set1_pd(numbers[i]);\n        for(int j = i + 1; j < size; j += 4){\n            if(j + 3 < size){\n                __m256d b = _mm256_loadu_pd(&numbers[j]);\n                __m256d diff = _mm256_sub_pd(b, a);\n                __m256d abs_diff = _mm256_and_pd(diff, _mm256_set1_pd(0x7FFFFFFFFFFFFFFF));\n                __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n                if(_mm256_movemask_pd(cmp) != 0){\n                    return true;\n                }\n            }\n            else{\n                for(int k = j; k < size; ++k){\n                    if(std::fabs(numbers[i] - numbers[k]) < threshold){\n                        return true;\n                    }\n                }\n                break;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n#include <algorithm>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    size_t n = numbers.size();\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n\n    for(size_t i = 0; i < n; ++i){\n        __m256d a = _mm256_set1_pd(numbers[i]);\n        for(size_t j = i + 1; j < n; j += 4){\n            size_t k = std::min(j + 3, n - 1);\n            __m256d b = _mm256_loadu_pd(&numbers[j]);\n            __m256d diff = _mm256_sub_pd(a, b);\n            __m256d abs_diff = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diff);\n            __m256d cmp = _mm256_cmp_pd(abs_diff, thresh_vec, _CMP_LT_OS);\n            if(_mm256_movemask_pd(cmp) != 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    const int simd_width = 8;\n    size_t vec_size = operations.size();\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i balance = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x80000000);\n\n    for(size_t i = 0; i < vec_size / simd_width * simd_width; i += simd_width){\n        __m256i op_vec = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, op_vec);\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);\n        __m256i and_res = _mm256_and_si256(cmp, mask);\n        int res = _mm256_movemask_epi8(and_res);\n        if(res != 0) return true;\n    }\n\n    int running_balance = 0;\n    for(size_t i = vec_size / simd_width * simd_width; i < vec_size; ++i){\n        running_balance += operations[i];\n        if(running_balance < 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations) {\n    const int vec_size = 8;\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i balance = _mm256_set1_epi32(0);\n    __m256i cmp_result;\n\n    for (size_t i = 0; i + vec_size <= operations.size(); i += vec_size) {\n        __m256i op_vec = _mm256_loadu_si256((__m256i const*)(&operations[i]));\n        balance = _mm256_add_epi32(balance, op_vec);\n        cmp_result = _mm256_cmpgt_epi32(zero, balance);\n        if (_mm256_movemask_epi8(cmp_result)) {\n            return true;\n        }\n    }\n\n    // Handle leftover elements\n    for (size_t i = (operations.size() / vec_size) * vec_size; i < operations.size(); ++i) {\n        int current_balance = _mm256_extract_epi32(balance, 0) + operations[i];\n        balance = _mm256_alignr_epi8(balance, _mm256_set1_epi32(current_balance), 4);\n        if (current_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nbool below_zero_simd(std::vector<int> operations){\n    const int vec_size = 8;\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i balance = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(INT_MIN);\n\n    for(size_t i = 0; i < operations.size(); i += vec_size){\n        __m256i chunk = _mm256_set_epi32(\n            (i+7 < operations.size()) ? operations[i+7] : 0,\n            (i+6 < operations.size()) ? operations[i+6] : 0,\n            (i+5 < operations.size()) ? operations[i+5] : 0,\n            (i+4 < operations.size()) ? operations[i+4] : 0,\n            (i+3 < operations.size()) ? operations[i+3] : 0,\n            (i+2 < operations.size()) ? operations[i+2] : 0,\n            (i+1 < operations.size()) ? operations[i+1] : 0,\n            (i+0 < operations.size()) ? operations[i+0] : 0\n        );\n        balance = _mm256_add_epi32(balance, chunk);\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);\n        __m256i and_res = _mm256_and_si256(cmp, mask);\n        int res = _mm256_movemask_epi8(and_res);\n        if(res != 0){\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    const int simd_width = 8;\n    size_t vec_size = operations.size();\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i balance = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i < vec_size - simd_width + 1; i += simd_width){\n        __m256i ops = _mm256_loadu_si256((__m256i*)&operations[i]);\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);\n        if(_mm256_movemask_epi8(cmp)){\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    int running_balance = 0;\n    for(size_t i = (vec_size / simd_width) * simd_width; i < vec_size; ++i){\n        running_balance += operations[i];\n        if(running_balance < 0){\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    const int vec_size = 8;\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i balance = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x80000000);\n\n    size_t i = 0;\n    for (; i + vec_size <= operations.size(); i += vec_size) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        balance = _mm256_add_epi32(balance, ops);\n        __m256i cmp = _mm256_cmpgt_epi32(zero, balance);\n        __m256i and_res = _mm256_and_si256(cmp, mask);\n        if (_mm256_movemask_epi8(and_res)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    __m256i tail = _mm256_setzero_si256();\n    for (; i < operations.size(); ++i) {\n        int current_balance = _mm256_extract_epi32(balance, 0);\n        if (current_balance + operations[i] < 0) return true;\n        balance = _mm256_add_epi32(balance, _mm256_set1_epi32(operations[i]));\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the sum using AVX\n    __m256d sum_vec = _mm256_setzero_pd();\n    for(size_t i = 0; i < n - n % 4; i += 4){\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n    for(size_t i = n - n % 4; i < n; ++i){\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Calculate the MAD using AVX\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    for(size_t i = 0; i < n - n % 4; i += 4){\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_and_pd(diff, _mm256_castsi256_pd(_mm256_set1_epi64x(0x7FFFFFFFFFFFFFFF)));\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n    double mad_sum[4];\n    _mm256_storeu_pd(mad_sum, mad_sum_vec);\n    double total_mad_sum = mad_sum[0] + mad_sum[1] + mad_sum[2] + mad_sum[3];\n    for(size_t i = n - n % 4; i < n; ++i){\n        total_mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the mean\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t aligned_size = n / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Calculate MAD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)), diff);\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n\n    double mad_sum[4];\n    _mm256_storeu_pd(mad_sum, mad_sum_vec);\n    double total_mad_sum = mad_sum[0] + mad_sum[1] + mad_sum[2] + mad_sum[3];\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        total_mad_sum += fabs(numbers[i] - mean);\n    }\n\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate mean\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t aligned_size = n / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Calculate MAD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)), diff);\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n\n    double mad_sum[4];\n    _mm256_storeu_pd(mad_sum, mad_sum_vec);\n    double total_mad_sum = mad_sum[0] + mad_sum[1] + mad_sum[2] + mad_sum[3];\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        total_mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the sum using AVX\n    __m256d sum_vec = _mm256_setzero_pd();\n    for(size_t i = 0; i < n - n % 4; i += 4){\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n    for(size_t i = n - n % 4; i < n; ++i){\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Calculate the MAD using AVX\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    for(size_t i = 0; i < n - n % 4; i += 4){\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_castsi256_pd(_mm256_set1_epi64x(0x8000000000000000)), diff);\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n    double mad_sum[4];\n    _mm256_storeu_pd(mad_sum, mad_sum_vec);\n    double total_mad_sum = mad_sum[0] + mad_sum[1] + mad_sum[2] + mad_sum[3];\n    for(size_t i = n - n % 4; i < n; ++i){\n        total_mad_sum += std::abs(numbers[i] - mean);\n    }\n\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the mean\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t aligned_size = n - (n % 4);\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, vec);\n    }\n    double sum[4];\n    _mm256_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1] + sum[2] + sum[3];\n    for (size_t i = aligned_size; i < n; ++i) {\n        total_sum += numbers[i];\n    }\n    double mean = total_sum / n;\n\n    // Calculate MAD\n    __m256d mean_vec = _mm256_set1_pd(mean);\n    __m256d mad_sum_vec = _mm256_setzero_pd();\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m256d vec = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(vec, mean_vec);\n        __m256d abs_diff = _mm256_andnot_pd(_mm256_castsi256_pd(_mm256_cmpeq_epi32(_mm256_castpd_si256(diff), _mm256_setzero_si256())), diff);\n        mad_sum_vec = _mm256_add_pd(mad_sum_vec, abs_diff);\n    }\n    double mad_sum[4];\n    _mm256_storeu_pd(mad_sum, mad_sum_vec);\n    double total_mad_sum = mad_sum[0] + mad_sum[1] + mad_sum[2] + mad_sum[3];\n    for (size_t i = aligned_size; i < n; ++i) {\n        total_mad_sum += fabs(numbers[i] - mean);\n    }\n    return total_mad_sum / n;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    if (numbers.size() <= 1) return numbers;\n\n    size_t original_size = numbers.size();\n    size_t result_size = original_size * 2 - 1;\n    std::vector<int> result(result_size, 0);\n\n    __m256i delimiter_broadcast = _mm256_set1_epi32(delimeter);\n    size_t i = 0;\n\n    for (; i + 7 < original_size; i += 8) {\n        __m256i vec_load = _mm256_loadu_si256((__m256i const*)(&numbers[i]));\n        __m256i vec_next_load = _mm256_loadu_si256((__m256i const*)(&numbers[i + 1]));\n\n        __m256i interleaved_low = _mm256_unpacklo_epi32(vec_load, vec_next_load);\n        __m256i interleaved_high = _mm256_unpackhi_epi32(vec_load, vec_next_load);\n\n        __m256i mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        __m256i interleaved_low_with_delimiter = _mm256_blendv_epi8(interleaved_low, delimiter_broadcast, mask);\n        __m256i interleaved_high_with_delimiter = _mm256_blendv_epi8(interleaved_high, delimiter_broadcast, mask);\n\n        _mm256_storeu_si256((__m256i*)(&result[2 * i]), interleaved_low_with_delimiter);\n        _mm256_storeu_si256((__m256i*)(&result[2 * i + 16]), interleaved_high_with_delimiter);\n    }\n\n    for (; i < original_size - 1; ++i) {\n        result[2 * i] = numbers[i];\n        result[2 * i + 1] = delimeter;\n    }\n    result[2 * i] = numbers[i];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    if (numbers.size() <= 1) {\n        return numbers;\n    }\n\n    size_t N = numbers.size();\n    size_t vec_len = N / 8 * 8;\n\n    __m256i delimiter_broadcast = _mm256_set1_epi32(delimeter);\n\n    for (size_t i = 0; i < vec_len; i += 8) {\n        __m256i nums = _mm256_loadu_si256((__m256i const*)(&numbers[i]));\n        __m256i delimiters = _mm256_broadcast_epi32(delimiter_broadcast);\n\n        __m256i low_nums = _mm256_unpacklo_epi32(nums, delimiters);\n        __m256i high_nums = _mm256_unpackhi_epi32(nums, delimiters);\n\n        _mm256_storeu_si256((__m256i*)(&result[result.size()]), low_nums);\n        result.insert(result.end(), &low_nums[0], &low_nums[8]);\n        _mm256_storeu_si256((__m256i*)(&result[result.size()]), high_nums);\n        result.insert(result.end(), &high_nums[0], &high_nums[8]);\n    }\n\n    for (size_t i = vec_len; i < N - 1; ++i) {\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n    result.push_back(numbers[N - 1]);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){ \n    if (numbers.size() <= 1) return numbers;\n\n    size_t original_size = numbers.size();\n    size_t result_size = original_size * 2 - 1;\n    std::vector<int> result(result_size, 0);\n\n    __m256i delimiter_broadcast = _mm256_set1_epi32(delimeter);\n    __m256i index_mask = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n\n    size_t i = 0;\n    for (; i <= original_size - 8; i += 8) {\n        __m256i vec_load = _mm256_loadu_si256((__m256i const*)(&numbers[i]));\n        __m256i vec_low = _mm256_permutevar8x32_epi32(vec_load, index_mask);\n        __m256i vec_high = _mm256_permutevar8x32_epi32(_mm256_loadu_si256((__m256i const*)(&numbers[i + 4])), index_mask);\n\n        __m256i interleaved_low = _mm256_blend_epi32(vec_low, delimiter_broadcast, 0xF0);\n        __m256i interleaved_high = _mm256_blend_epi32(vec_high, delimiter_broadcast, 0xF0);\n\n        _mm256_storeu_si256((__m256i*)(&result[2 * i]), interleaved_low);\n        _mm256_storeu_si256((__m256i*)(&result[2 * i + 8]), interleaved_high);\n    }\n\n    for (; i < original_size - 1; ++i) {\n        result[2 * i] = numbers[i];\n        result[2 * i + 1] = delimeter;\n    }\n    result[2 * i] = numbers[i];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){ \n    if (numbers.empty()) return {};\n    \n    size_t n = numbers.size();\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n    \n    __m256i delimiter_broadcast = _mm256_set1_epi32(delimeter);\n    \n    size_t i = 0;\n    size_t j = 0;\n    \n    for (; i + 7 < n; i += 8, j += 16) {\n        __m256i vec_numbers_1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i vec_numbers_2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i + 4]));\n        \n        __m256i vec_delimiters_1 = delimiter_broadcast;\n        __m256i vec_delimiters_2 = delimiter_broadcast;\n        \n        __m256i interleaved_1 = _mm256_permutevar8x32_epi32(vec_numbers_1, _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        __m256i interleaved_2 = _mm256_permutevar8x32_epi32(vec_numbers_2, _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        \n        interleaved_1 = _mm256_inserti128_epi32(interleaved_1, _mm256_extracti128_si256(vec_delimiters_1, 0), 1);\n        interleaved_2 = _mm256_inserti128_epi32(interleaved_2, _mm256_extracti128_si256(vec_delimiters_2, 0), 1);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[j]), interleaved_1);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[j + 8]), interleaved_2);\n    }\n    \n    for (; i < n - 1; ++i, ++j) {\n        result[j] = numbers[i];\n        result[j + 1] = delimeter;\n        j++;\n    }\n    \n    if (i == n - 1) {\n        result[j] = numbers[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){ \n    if (numbers.empty()) return {};\n    \n    size_t n = numbers.size();\n    size_t result_size = 2 * n - 1;\n    std::vector<int> result(result_size);\n    \n    __m256i delimiter_broadcast = _mm256_set1_epi32(delimeter);\n    \n    size_t i = 0;\n    size_t j = 0;\n    \n    for (; i + 7 < n; i += 8, j += 16) {\n        __m256i vec_nums_1 = _mm256_loadu_si256((__m256i const*)(&numbers[i]));\n        __m256i vec_nums_2 = _mm256_loadu_si256((__m256i const*)(&numbers[i + 4]));\n        \n        __m256i vec_delim_1 = delimiter_broadcast;\n        __m256i vec_delim_2 = delimiter_broadcast;\n        \n        __m256i interleaved_1 = _mm256_permutevar8x32_epi32(vec_nums_1, _mm256_setr_epi32(0, 8, 1, 8, 2, 8, 3, 8));\n        __m256i interleaved_2 = _mm256_permutevar8x32_epi32(vec_nums_2, _mm256_setr_epi32(0, 8, 1, 8, 2, 8, 3, 8));\n        \n        interleaved_1 = _mm256_blend_epi32(interleaved_1, vec_delim_1, 0xF0);\n        interleaved_2 = _mm256_blend_epi32(interleaved_2, vec_delim_2, 0xF0);\n        \n        _mm256_storeu_si256((__m256i*)&result[j], interleaved_1);\n        _mm256_storeu_si256((__m256i*)&result[j + 8], interleaved_2);\n    }\n    \n    for (; i < n - 1; ++i, ++j) {\n        result[j] = numbers[i];\n        result[j + 1] = delimeter;\n        j++;\n    }\n    \n    if (i == n - 1) {\n        result[j] = numbers[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int n = numbers.size();\n    if (n == 0) return {0, 1};\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi16(1);\n\n    for(int i = 0; i < n; i += 16){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        sum = _mm256_add_epi16(sum, vec);\n        prod = _mm256_mullo_epi16(prod, vec);\n    }\n\n    // Horizontal sum\n    __m128i sum_low = _mm_add_epi16(_mm256_extracti128_si256(sum, 0), _mm256_extracti128_si256(sum, 1));\n    sum_low = _mm_add_epi16(sum_low, _mm_unpackhi_epi64(sum_low, sum_low));\n    sum_low = _mm_add_epi16(sum_low, _mm_shuffle_epi32(sum_low, 0x1B));\n    int32_t hsum = _mm_cvtsi128_si32(sum_low);\n\n    // Horizontal product\n    __m128i prod_low = _mm_mullo_epi16(_mm256_extracti128_si256(prod, 0), _mm256_extracti128_si256(prod, 1));\n    prod_low = _mm_mullo_epi16(prod_low, _mm_unpackhi_epi64(prod_low, prod_low));\n    prod_low = _mm_mullo_epi16(prod_low, _mm_shuffle_epi32(prod_low, 0x1B));\n    int32_t hprod = _mm_cvtsi128_si32(prod_low);\n\n    return {(int64_t)hsum, (int64_t)hprod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int len = numbers.size();\n    if(len == 0){\n        return {0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi16(1);\n\n    for(int i = 0; i <= len - 16; i += 16){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        sum = _mm256_add_epi16(sum, vec);\n        prod = _mm256_mullo_epi16(prod, vec);\n    }\n\n    int leftover = len % 16;\n    if(leftover > 0){\n        alignas(32) int16_t temp[16] = {0};\n        for(int i = 0; i < leftover; ++i){\n            temp[i] = numbers[len - leftover + i];\n        }\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(temp));\n        sum = _mm256_add_epi16(sum, vec);\n        prod = _mm256_mullo_epi16(prod, vec);\n    }\n\n    sum = _mm256_hadd_epi16(sum, sum);\n    sum = _mm256_hadd_epi16(sum, sum);\n    int64_t total_sum = _mm256_extract_epi16(sum, 0) + _mm256_extract_epi16(sum, 8);\n\n    prod = _mm256_hmul_epi16(prod, prod);\n    prod = _mm256_hmul_epi16(prod, prod);\n    int64_t total_prod = _mm256_extract_epi16(prod, 0) * _mm256_extract_epi16(prod, 8);\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int len = numbers.size();\n    if(len == 0){\n        return std::vector<int64_t>{0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi16(1);\n\n    for(int i = 0; i < len - 15; i += 16){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        sum = _mm256_add_epi16(sum, vec);\n        prod = _mm256_mullo_epi16(prod, vec);\n    }\n\n    int64_t total_sum = 0;\n    int64_t total_prod = 1;\n\n    alignas(32) int16_t sum_arr[16];\n    alignas(32) int16_t prod_arr[16];\n\n    _mm256_store_si256((__m256i*)sum_arr, sum);\n    _mm256_store_si256((__m256i*)prod_arr, prod);\n\n    for(int i = 0; i < 16; ++i){\n        total_sum += sum_arr[i];\n        total_prod *= prod_arr[i];\n    }\n\n    for(int i = len - (len % 16); i < len; ++i){\n        total_sum += numbers[i];\n        total_prod *= numbers[i];\n    }\n\n    return std::vector<int64_t>{total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int len = numbers.size();\n    if(len == 0){\n        return std::vector<int64_t>{0, 1};\n    }\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vprod = _mm256_set1_epi16(1);\n\n    for(int i = 0; i < len - 15; i += 16){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        vsum = _mm256_add_epi16(vsum, vec);\n        vprod = _mm256_mullo_epi16(vprod, vec);\n    }\n\n    int16_t temp_sum[16] __attribute__((aligned(32))) = {0};\n    int16_t temp_prod[16] __attribute__((aligned(32))) = {1};\n\n    _mm256_store_si256((__m256i*)temp_sum, vsum);\n    _mm256_store_si256((__m256i*)temp_prod, vprod);\n\n    int64_t sum = 0;\n    int64_t prod = 1;\n\n    for(int i = 0; i < 16; ++i){\n        sum += temp_sum[i];\n        prod *= temp_prod[i];\n    }\n\n    for(int i = len - (len % 16); i < len; ++i){\n        sum += numbers[i];\n        prod *= numbers[i];\n    }\n\n    return std::vector<int64_t>{sum, prod};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int n = numbers.size();\n    if (n == 0) return {0, 1};\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_set1_epi16(1);\n\n    for(int i = 0; i + 16 <= n; i += 16){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        sum = _mm256_add_epi16(sum, vec);\n        prod = _mm256_mullo_epi16(prod, vec);\n    }\n\n    // Horizontal sum\n    int64_t hsum = 0;\n    __m128i vsum1 = _mm_add_epi16(_mm256_extracti128_si256(sum, 0), _mm256_extracti128_si256(sum, 1));\n    vsum1 = _mm_add_epi16(vsum1, _mm_unpackhi_epi16(vsum1, vsum1));\n    vsum1 = _mm_add_epi16(vsum1, _mm_unpackhi_epi16(vsum1, vsum1));\n    hsum += _mm_extract_epi16(vsum1, 0);\n\n    // Horizontal product\n    int64_t hprod = 1;\n    __m128i vprod1 = _mm_mullo_epi16(_mm256_extracti128_si256(prod, 0), _mm256_extracti128_si256(prod, 1));\n    vprod1 = _mm_mullo_epi16(vprod1, _mm_unpackhi_epi16(vprod1, vprod1));\n    vprod1 = _mm_mullo_epi16(vprod1, _mm_unpackhi_epi16(vprod1, vprod1));\n    hprod *= _mm_extract_epi16(vprod1, 0);\n\n    // Process remaining elements\n    for(int i = (n / 16) * 16; i < n; ++i){\n        hsum += numbers[i];\n        hprod *= numbers[i];\n    }\n\n    return {hsum, hprod};\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int vec_size = 8;\n    __m256i current_max = _mm256_set1_epi32(numbers[0]);\n    __m256i temp_max;\n\n    for(size_t i = 0; i < numbers.size(); i += vec_size){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        temp_max = _mm256_max_epi32(data, current_max);\n        current_max = temp_max;\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), current_max);\n    }\n\n    // Handle remaining elements\n    for(size_t i = ((numbers.size() / vec_size) * vec_size); i < numbers.size(); ++i){\n        result[i] = (i == 0) ? numbers[i] : std::max(result[i-1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int vec_size = 8; // Number of integers in a YMM register using AVX2\n    int num_full_vectors = numbers.size() / vec_size;\n    int remainder = numbers.size() % vec_size;\n\n    __m256i current_max = _mm256_set1_epi32(numbers[0]);\n    __m256i temp_max;\n\n    for(int i = 0; i < num_full_vectors * vec_size; i += vec_size){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        temp_max = _mm256_max_epi32(current_max, data);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), current_max);\n        current_max = temp_max;\n    }\n\n    // Handle remaining elements\n    for(int i = num_full_vectors * vec_size; i < numbers.size(); ++i){\n        int new_max = std::max(result[i-1], numbers[i]);\n        result[i] = new_max;\n        current_max = _mm256_set1_epi32(new_max);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int vec_size = 8; // Number of integers in a YMM register with AVX2\n    size_t n = numbers.size();\n    __m256i current_max = _mm256_set1_epi32(numbers[0]);\n    __m256i temp_max;\n\n    for(size_t i = 0; i < n; i += vec_size){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        temp_max = _mm256_max_epi32(current_max, data);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), current_max);\n        current_max = temp_max;\n    }\n\n    // Handle remaining elements\n    size_t i = (n / vec_size) * vec_size;\n    int max_val = current_max.m256i_i32[0];\n    for (; i < n; ++i){\n        max_val = std::max(max_val, numbers[i]);\n        result[i] = max_val;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int vec_size = 8;\n    __m256i current_max = _mm256_set1_epi32(numbers[0]);\n    __m256i temp_max;\n\n    int i = 0;\n    for (; i + vec_size <= numbers.size(); i += vec_size) {\n        __m256i data = _mm256_loadu_si256((__m256i const*)(&numbers[i]));\n        temp_max = _mm256_max_epi32(data, current_max);\n        _mm256_storeu_si256((__m256i*)&result[i], temp_max);\n        current_max = temp_max;\n    }\n\n    for (; i < numbers.size(); ++i) {\n        int max_val = (i == 0) ? numbers[i] : std::max(result[i-1], numbers[i]);\n        result[i] = max_val;\n        current_max = _mm256_set1_epi32(max_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int vec_size = 8; // AVX2 can handle 8 integers (256 bits / 32 bits per integer)\n    __m256i current_max = _mm256_set1_epi32(numbers[0]);\n    __m256i temp_max;\n\n    for(size_t i = 0; i < numbers.size(); i += vec_size){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        temp_max = _mm256_max_epi32(current_max, data);\n        current_max = _mm256_max_epi32(current_max, temp_max);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), current_max);\n\n        // Handle scalar remainder\n        for(int j = 0; j < vec_size && (i + j) < numbers.size(); ++j){\n            result[i + j] = std::max(result[i + j], (i + j > 0) ? result[i + j - 1] : numbers[0]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) {\n        return \"\";\n    }\n    \n    std::string result(len, '0');\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n        __m256i vr = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vr);\n    }\n    for (; i < len; ++i) {\n        result[i] = a[i] == b[i] ? '0' : '1';\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) return \"\";\n    std::string result(len, '0');\n    \n    const size_t vec_len = 32; // AVX2 operates on 256 bits\n    size_t i = 0;\n    for (; i + vec_len <= len; i += vec_len) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n        __m256i vxor = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vxor);\n    }\n    for (; i < len; ++i) {\n        result[i] = (a[i] == '0' ? (b[i] == '0' ? '0' : '1') : (b[i] == '0' ? '1' : '0'));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) return \"\";\n    \n    std::string result(len, '0');\n    size_t i = 0;\n    // Process 32 bytes at a time using AVX2\n    for (; i + 31 < len; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n        __m256i vec_res = _mm256_xor_si256(vec_a, vec_b);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vec_res);\n    }\n    // Process remaining characters\n    for (; i < len; ++i) {\n        result[i] = a[i] == '0' ? (b[i] == '0' ? '0' : '1') : (b[i] == '0' ? '1' : '0');\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) {\n        return \"\";\n    }\n    \n    std::string result(len, '0');\n    size_t i = 0;\n    for (; i + 31 < len; i += 32) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n        __m256i vxor = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vxor);\n    }\n    for (; i < len; ++i) {\n        result[i] = (a[i] == '0' ? (b[i] == '0' ? '0' : '1') : (b[i] == '0' ? '1' : '0'));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) return \"\";\n    \n    std::string result(len, '0');\n    size_t i = 0;\n    const size_t step = 32; // Number of bits processed per iteration\n    \n    // Process 32 characters at a time using AVX2\n    for (; i <= len - step; i += step) {\n        __m256i va = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n        __m256i vres = _mm256_xor_si256(va, vb);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vres);\n    }\n    \n    // Handle remaining characters\n    for (; i < len; ++i) {\n        result[i] = (a[i] == '0' && b[i] == '0') || (a[i] == '1' && b[i] == '1') ? '0' : '1';\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int num_strings = strings.size();\n    int max_index = 0;\n    int max_length = 0;\n\n    // Process in chunks of 8 using AVX2\n    __m256i zero = _mm256_set1_epi32(0);\n    for(int i = 0; i + 7 < num_strings; i += 8){\n        __m256i lengths = _mm256_loadu_si256((__m256i*)&strings[i].length());\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, _mm256_set1_epi32(max_length));\n        int mask = _mm256_movemask_epi8(cmp);\n        if(mask != 0){\n            int new_max_index = i + __builtin_ctz(mask / 8);\n            max_length = strings[new_max_index].length();\n            max_index = new_max_index;\n        }\n    }\n\n    // Handle remaining strings\n    for(int i = (num_strings / 8) * 8; i < num_strings; ++i){\n        if(strings[i].length() > max_length){\n            max_length = strings[i].length();\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <climits>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int num_strings = strings.size();\n    int max_idx = 0;\n    int max_len = 0;\n\n    // Process strings in chunks of 8 for AVX2\n    for(int i = 0; i + 7 < num_strings; i += 8){\n        __m256i lengths = _mm256_set_epi32(\n            strings[i+7].length(), strings[i+6].length(), strings[i+5].length(), strings[i+4].length(),\n            strings[i+3].length(), strings[i+2].length(), strings[i+1].length(), strings[i+0].length()\n        );\n\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, _mm256_set1_epi32(max_len));\n        int mask = _mm256_movemask_epi8(cmp);\n\n        if(mask != 0){\n            int new_max_idx = i + __builtin_ctz(mask / 8);\n            max_len = strings[new_max_idx].length();\n            max_idx = new_max_idx;\n        }\n    }\n\n    // Process remaining strings\n    for(int i = ((num_strings / 8) * 8); i < num_strings; ++i){\n        if(strings[i].length() > max_len){\n            max_len = strings[i].length();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int num_strings = strings.size();\n    int max_index = 0;\n    int max_length = strings[0].length();\n\n    for(int i = 1; i + 7 < num_strings; i += 8){\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i lengths = _mm256_set_epi32(\n            strings[i+7].length(), strings[i+6].length(), \n            strings[i+5].length(), strings[i+4].length(), \n            strings[i+3].length(), strings[i+2].length(), \n            strings[i+1].length(), strings[i].length()\n        );\n        __m256i cmp_res = _mm256_cmpgt_epi32(lengths, _mm256_set1_epi32(max_length));\n        __m256i mask = _mm256_movemask_epi8(cmp_res);\n        \n        if(mask != 0){\n            int new_max_pos = _tzcnt_u32(mask) / 4;\n            max_index = i + new_max_pos;\n            max_length = strings[max_index].length();\n        }\n    }\n\n    for(int i = ((num_strings / 8) * 8); i < num_strings; ++i){\n        if(strings[i].length() > max_length){\n            max_index = i;\n            max_length = strings[i].length();\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int vector_size = 8;\n    size_t max_length = 0;\n    size_t max_index = 0;\n\n    // Process in chunks of 8\n    __m256i zero = _mm256_set1_epi32(0);\n    for(size_t i = 0; i + vector_size <= strings.size(); i += vector_size){\n        __m256i lengths = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&strings[i].length()));\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, zero);\n        __m256i mask = _mm256_movemask_epi8(cmp);\n\n        if(mask != 0){\n            size_t local_max_index = _tzcnt_u32(mask) / 4 + i;\n            if(strings[local_max_index].length() > max_length){\n                max_length = strings[local_max_index].length();\n                max_index = local_max_index;\n            }\n        }\n        zero = _mm256_max_epi32(zero, lengths);\n    }\n\n    // Handle remaining elements\n    for(size_t i = (strings.size() / vector_size) * vector_size; i < strings.size(); ++i){\n        if(strings[i].length() > max_length){\n            max_length = strings[i].length();\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <climits>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int n = strings.size();\n    int max_idx = 0;\n    int max_len = 0;\n\n    // Process in chunks of 8 using AVX2\n    for(int i = 0; i + 7 < n; i += 8){\n        __m256i lengths = _mm256_set_epi32(\n            strings[i+7].size(), strings[i+6].size(), strings[i+5].size(), strings[i+4].size(),\n            strings[i+3].size(), strings[i+2].size(), strings[i+1].size(), strings[i+0].size()\n        );\n        __m256i cmp = _mm256_cmpgt_epi32(lengths, _mm256_set1_epi32(max_len));\n        int mask = _mm256_movemask_epi8(cmp);\n        if(mask != 0){\n            int new_max_idx = i + __builtin_ctz(mask) / 4;\n            max_len = strings[new_max_idx].size();\n            max_idx = new_max_idx;\n        }\n    }\n\n    // Handle remaining strings\n    for(int i = ((n / 8) * 8); i < n; ++i){\n        if(strings[i].size() > max_len){\n            max_len = strings[i].size();\n            max_idx = i;\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i zero = _mm256_setzero_si256();\n    \n    while (_mm256_testz_si256(vb, vb) == 0) {\n        __m256i t = _mm256_min_epu64(va, vb);\n        vb = _mm256_sub_epi64(va, t);\n        va = t;\n    }\n    \n    __m128i result = _mm256_extracti128_si256(va, 0);\n    return _mm_extract_epi64(result, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i zero = _mm256_setzero_si256();\n    \n    while (1) {\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testz_si256(cmp, cmp) == 0) {\n            break;\n        }\n        \n        __m256i tmp = _mm256_blendv_epi8(va, vb, _mm256_cmpgt_epi64(va, vb));\n        vb = _mm256_sub_epi64(tmp, vb);\n        va = tmp;\n    }\n    \n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i zero = _mm256_setzero_si256();\n\n    while (1) {\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testc_si256(cmp, cmp)) {\n            break;\n        }\n        __m256i tmp = _mm256_min_epu64(va, vb);\n        vb = _mm256_sub_epi64(_mm256_max_epu64(va, vb), tmp);\n        va = tmp;\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i va = _mm256_set1_epi64x(a);\n    __m256i vb = _mm256_set1_epi64x(b);\n    __m256i zero = _mm256_setzero_si256();\n\n    while (true) {\n        __m256i cmp = _mm256_cmpeq_epi64(vb, zero);\n        if (_mm256_testc_si256(cmp, cmp)) {\n            break;\n        }\n\n        __m256i temp = vb;\n        vb = _mm256_andnot_si256(_mm256_cmpeq_epi64(va, zero), _mm256_sub_epi64(va, _mm256_mullo_epi64(vb, _mm256_div_epu64(va, vb))));\n        va = temp;\n    }\n\n    return _mm256_extract_epi64(va, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m256i zero = _mm256_set1_epi64x(0);\n    __m256i vec_a = _mm256_set1_epi64x(a);\n    __m256i vec_b = _mm256_set1_epi64x(b);\n\n    while (1) {\n        __m256i cmp = _mm256_cmpeq_epi64(vec_a, vec_b);\n        if (_mm256_testc_si256(cmp, _mm256_set1_epi64x(-1)) != 0) {\n            return _mm256_extract_epi64(vec_a, 0);\n        }\n\n        __m256i mask = _mm256_cmpgt_epi64(vec_a, vec_b);\n        __m256i temp = _mm256_sub_epi64(vec_a, vec_b);\n        vec_a = _mm256_blendv_epi8(vec_a, temp, mask);\n        vec_b = _mm256_blendv_epi8(vec_b, vec_a, _mm256_xor_si256(mask, _mm256_set1_epi64x(-1)));\n    }\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <limits>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    const int n = numbers.size();\n    float min_diff = std::numeric_limits<float>::infinity();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n    __m256 current_min_diff = _mm256_set1_ps(min_diff);\n\n    for(int i = 0; i < n - 1; ++i){\n        __m256 a = _mm256_set1_ps(numbers[i]);\n        for(int j = i + 1; j < n; ++j){\n            __m256 b = _mm256_set1_ps(numbers[j]);\n            __m256 diff = _mm256_abs_ps(_mm256_sub_ps(a, b));\n            current_min_diff = _mm256_min_ps(diff, current_min_diff);\n        }\n        // Horizontal min\n        float diff_vals[8];\n        _mm256_storeu_ps(diff_vals, current_min_diff);\n        float local_min = diff_vals[0];\n        for(int k = 1; k < 8; ++k){\n            if(diff_vals[k] < local_min){\n                local_min = diff_vals[k];\n            }\n        }\n        if(local_min < min_diff){\n            min_diff = local_min;\n            for(int j = i + 1; j < n; ++j){\n                if(std::abs(numbers[i] - numbers[j]) == min_diff){\n                    closest1 = std::min(numbers[i], numbers[j]);\n                    closest2 = std::max(numbers[i], numbers[j]);\n                    break;\n                }\n            }\n        }\n        current_min_diff = _mm256_set1_ps(min_diff);\n    }\n\n    return {closest1, closest2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    const int n = numbers.size();\n    float min_diff = std::abs(numbers[1] - numbers[0]);\n    float closest_pair[2] = {numbers[0], numbers[1]};\n\n    __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n    __m256 current_min_diff_vec = _mm256_set1_ps(min_diff);\n\n    for(int i = 0; i < n - 1; i += 8){\n        __m256 vec_i = _mm256_loadu_ps(&numbers[i]);\n        for(int j = i + 1; j < n; j += 8){\n            __m256 vec_j = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(vec_j, vec_i);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n            \n            __m256 mask = _mm256_cmp_ps(abs_diff, current_min_diff_vec, _CMP_LT_OS);\n            if(_mm256_movemask_ps(mask)){\n                // Find the minimum difference and update closest pair\n                float diffs[8];\n                _mm256_storeu_ps(diffs, abs_diff);\n                for(int k = 0; k < 8; ++k){\n                    if(diffs[k] < min_diff){\n                        min_diff = diffs[k];\n                        closest_pair[0] = numbers[i + (k / 8)];\n                        closest_pair[1] = numbers[j + (k % 8)];\n                    }\n                }\n                current_min_diff_vec = _mm256_set1_ps(min_diff);\n            }\n        }\n    }\n\n    std::vector<float> result(closest_pair, closest_pair + 2);\n    std::sort(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n#include <algorithm>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    const int n = numbers.size();\n    float min_diff = std::abs(numbers[1] - numbers[0]);\n    float closest_pair[2] = {numbers[0], numbers[1]};\n\n    __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n    __m256 closest_pair_vec_1 = _mm256_set1_ps(closest_pair[0]);\n    __m256 closest_pair_vec_2 = _mm256_set1_ps(closest_pair[1]);\n\n    for(int i = 0; i < n - 1; i += 8){\n        __m256 current = _mm256_loadu_ps(&numbers[i]);\n        __m256 next = _mm256_loadu_ps(&numbers[i+1]);\n        \n        __m256 diff = _mm256_sub_ps(next, current);\n        __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n        __m256 mask = _mm256_cmp_ps(abs_diff, min_diff_vec, _CMP_LT_OQ);\n        if(_mm256_movemask_ps(mask)){\n            __m256 new_min_diff = _mm256_min_ps(abs_diff, min_diff_vec);\n            min_diff_vec = new_min_diff;\n\n            __m256 new_closest_pair_1 = _mm256_blendv_ps(closest_pair_vec_1, current, mask);\n            __m256 new_closest_pair_2 = _mm256_blendv_ps(closest_pair_vec_2, next, mask);\n\n            closest_pair_vec_1 = new_closest_pair_1;\n            closest_pair_vec_2 = new_closest_pair_2;\n        }\n    }\n\n    float result[8];\n    _mm256_storeu_ps(result, closest_pair_vec_1);\n    float smallest = result[0];\n    _mm256_storeu_ps(result, closest_pair_vec_2);\n    float largest = result[0];\n\n    for(int i = 0; i < 8; ++i){\n        if(result[i] < smallest && result[i] != -1){\n            largest = smallest;\n            smallest = result[i];\n        }\n        else if(result[i] < largest && result[i] != smallest){\n            largest = result[i];\n        }\n    }\n\n    return {smallest, largest};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    const int n = numbers.size();\n    float min_diff = std::abs(numbers[1] - numbers[0]);\n    float result1 = numbers[0];\n    float result2 = numbers[1];\n\n    __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n    __m256 result1_vec = _mm256_set1_ps(result1);\n    __m256 result2_vec = _mm256_set1_ps(result2);\n\n    for(int i = 0; i < n - 1; i += 8){\n        __m256 vec_i = _mm256_loadu_ps(&numbers[i]);\n        for(int j = i + 1; j < n; j += 8){\n            __m256 vec_j = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(vec_i, vec_j);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n\n            __m256 mask = _mm256_cmp_ps(abs_diff, min_diff_vec, _CMP_LT_OS);\n            if(_mm256_movemask_ps(mask)){\n                __m256 new_result1 = _mm256_min_ps(vec_i, vec_j);\n                __m256 new_result2 = _mm256_max_ps(vec_i, vec_j);\n                min_diff_vec = _mm256_min_ps(abs_diff, min_diff_vec);\n                result1_vec = _mm256_blendv_ps(result1_vec, new_result1, mask);\n                result2_vec = _mm256_blendv_ps(result2_vec, new_result2, mask);\n            }\n        }\n    }\n\n    float res1[8], res2[8], diffs[8];\n    _mm256_storeu_ps(res1, result1_vec);\n    _mm256_storeu_ps(res2, result2_vec);\n    _mm256_storeu_ps(diffs, min_diff_vec);\n\n    float final_min_diff = diffs[0];\n    float final_res1 = res1[0];\n    float final_res2 = res2[0];\n    for(int i = 1; i < 8; ++i){\n        if(diffs[i] < final_min_diff){\n            final_min_diff = diffs[i];\n            final_res1 = res1[i];\n            final_res2 = res2[i];\n        }\n    }\n\n    return {final_res1, final_res2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <float.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    const int n = numbers.size();\n    float min_diff = FLT_MAX;\n    float closest_pair[2] = {0.0f, 0.0f};\n\n    __m256 min_diff_vec = _mm256_set1_ps(min_diff);\n    __m256 current_min_diff_vec = _mm256_set1_ps(FLT_MAX);\n\n    for(int i = 0; i < n - 1; ++i){\n        __m256 a = _mm256_set1_ps(numbers[i]);\n        for(int j = i + 1; j < n; j += 8){\n            __m256 b = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(b, a);\n            __m256 abs_diff = _mm256_andnot_ps(_mm256_set1_ps(-0.0f), diff);\n            \n            __m256 mask = _mm256_cmp_ps(abs_diff, current_min_diff_vec, _CMP_LT_OS);\n            current_min_diff_vec = _mm256_min_ps(abs_diff, current_min_diff_vec);\n\n            // Horizontal min to update scalar min_diff\n            float current_mins[8];\n            _mm256_storeu_ps(current_mins, current_min_diff_vec);\n            for(int k = 0; k < 8; ++k){\n                if(current_mins[k] < min_diff){\n                    min_diff = current_mins[k];\n                    closest_pair[0] = numbers[i];\n                    closest_pair[1] = numbers[j + k];\n                }\n            }\n        }\n    }\n\n    std::vector<float> result(closest_pair, closest_pair + 2);\n    std::sort(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vmax = _mm256_set1_ps(max_val);\n    __m256 vrange = _mm256_div_ps(_mm256_sub_ps(vmax, vmin), _mm256_set1_ps(1.0f));\n\n    size_t aligned_size = numbers.size() / 8 * 8;\n    std::vector<float> result(numbers.size());\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 vnums = _mm256_loadu_ps(&numbers[i]);\n        __m256 vsub = _mm256_sub_ps(vnums, vmin);\n        __m256 vres = _mm256_div_ps(vsub, vrange);\n        _mm256_storeu_ps(&result[i], vres);\n    }\n\n    for (size_t i = aligned_size; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    const float range = max_val - min_val;\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vrange = _mm256_set1_ps(range);\n\n    size_t i = 0;\n    size_t vec_size = numbers.size() / 8 * 8;\n    std::vector<float> result(numbers.size());\n\n    for (; i < vec_size; i += 8) {\n        __m256 vnums = _mm256_loadu_ps(&numbers[i]);\n        __m256 vsub = _mm256_sub_ps(vnums, vmin);\n        __m256 vres = _mm256_div_ps(vsub, vrange);\n        _mm256_storeu_ps(&result[i], vres);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / range;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vrange = _mm256_set1_ps(max_val - min_val);\n\n    size_t aligned_size = numbers.size() / 8 * 8;\n    std::vector<float> result(numbers.size());\n\n    for(size_t i = 0; i < aligned_size; i += 8){\n        __m256 vnums = _mm256_loadu_ps(&numbers[i]);\n        __m256 vsub = _mm256_sub_ps(vnums, vmin);\n        __m256 vres = _mm256_div_ps(vsub, vrange);\n        _mm256_storeu_ps(&result[i], vres);\n    }\n\n    for(size_t i = aligned_size; i < numbers.size(); ++i){\n        result[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    float scale = 1.0f / (max_val - min_val);\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vscale = _mm256_set1_ps(scale);\n\n    size_t aligned_size = numbers.size() / 8 * 8;\n    std::vector<float> result(numbers.size());\n\n    for(size_t i = 0; i < aligned_size; i += 8){\n        __m256 vnum = _mm256_loadu_ps(&numbers[i]);\n        __m256 vsub = _mm256_sub_ps(vnum, vmin);\n        __m256 vmul = _mm256_mul_ps(vsub, vscale);\n        _mm256_storeu_ps(&result[i], vmul);\n    }\n\n    for(size_t i = aligned_size; i < numbers.size(); ++i){\n        result[i] = (numbers[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    __m256 vmin = _mm256_set1_ps(min_val);\n    __m256 vrange = _mm256_set1_ps(max_val - min_val);\n\n    std::vector<float> result(numbers.size());\n    size_t i = 0;\n\n    for (; i + 8 <= numbers.size(); i += 8) {\n        __m256 vnums = _mm256_loadu_ps(&numbers[i]);\n        __m256 vsub = _mm256_sub_ps(vnums, vmin);\n        __m256 vres = _mm256_div_ps(vsub, vrange);\n        _mm256_storeu_ps(&result[i], vres);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i divisors = _mm256_set_epi64x(n-1, n-2, n-3, n-4);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i remainder;\n    uint64_t result = 0;\n\n    for(int i = 0; i < 4; ++i){\n        __m256i mod_results = _mm256_rem_epu64(vec_n, divisors);\n        __m256i cmp_results = _mm256_cmpeq_epi64(mod_results, zero);\n        __m256i mask = _mm256_and_si256(cmp_results, divisors);\n\n        uint64_t temp_results[4];\n        _mm256_storeu_si256((__m256i*)temp_results, mask);\n\n        for(int j = 0; j < 4; ++j){\n            if(temp_results[j] > result){\n                result = temp_results[j];\n            }\n        }\n\n        if(result != 0) break;\n\n        divisors = _mm256_sub_epi64(divisors, _mm256_set1_epi64x(4));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n\n    uint64_t result = 0;\n    const int vec_size = 4;\n    __m256i divisors = _mm256_set_epi64x(n-1, n-2, n-3, n-4);\n    __m256i mod_results;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i n_vec = _mm256_set1_epi64x(n);\n\n    for(int i = 0; i < n-1; i += vec_size){\n        mod_results = _mm256_rem_epu64(divisors, n_vec);\n        __m256i cmp_results = _mm256_cmpeq_epi64(mod_results, zero);\n        int mask = _mm256_movemask_epi8(cmp_results);\n        if(mask != 0){\n            int index = __builtin_ctzll(mask) / 2;\n            result = _mm256_extract_epi64(divisors, index);\n            break;\n        }\n        if(i + 2*vec_size < n){\n            divisors = _mm256_add_epi64(divisors, _mm256_set1_epi64x(vec_size));\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n\n    __m256i divisors = _mm256_set_epi64x(n-1, n-2, n-3, n-4);\n    __m256i mod_results;\n    uint64_t result = 0;\n\n    for(int i = 4; i >=0 ; i--){\n        mod_results = _mm256_rem_epu64(divisors, _mm256_set1_epi64x(n));\n        __m256i is_zero = _mm256_cmpeq_epi64(mod_results, _mm256_setzero_si256());\n        uint32_t mask = _mm256_movemask_epi8(is_zero);\n        \n        if(mask != 0){\n            int index = 31 - __builtin_clz(mask);\n            int vec_index = index / 2;\n            uint64_t* temp = (uint64_t*)&divisors;\n            result = temp[vec_index];\n            break;\n        }\n        if(i > 0){\n            divisors = _mm256_sub_epi64(divisors, _mm256_set1_epi64x(4));\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n\n    __m256i divisors = _mm256_set_epi64x(n-1, n-2, n-3, n-4);\n    __m256i n_vec = _mm256_set1_epi64x(n);\n    uint64_t result = 0;\n\n    for(int i = 4; i > 0; --i){\n        __m256i rem = _mm256_rem_epu64(divisors, n_vec);\n        __m256i mask = _mm256_cmpeq_epi64(rem, _mm256_setzero_si256());\n        __m256i max_val = _mm256_max_epu64(_mm256_and_si256(mask, divisors), _mm256_set1_epi64x(result));\n        result = _mm256_extract_epi64(max_val, 0);\n\n        if(result != 0) break;\n        divisors = _mm256_sub_epi64(divisors, _mm256_set1_epi64x(4));\n    }\n\n    if(result == 0){\n        for(uint64_t j = n - (n % 8); j > 0; --j){\n            if((n % j) == 0){\n                result = j;\n                break;\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    uint64_t result = 0;\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i divisors = _mm256_set_epi64x(n-1, n-2, n-3, n-4);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i remainder;\n\n    for(int i = n-1; i > 1; i -= 4){\n        divisors = _mm256_set_epi64x(i, i-1, i-2, i-3);\n        remainder = _mm256_rem_epu64(vec_n, divisors); // hypothetical intrinsic for SIMD remainder\n        __m256i cmp_result = _mm256_cmpeq_epi64(remainder, zero);\n        int mask = _mm256_movemask_epi8(cmp_result);\n        if(mask){\n            int index = __builtin_ctz(mask) / 2;\n            __m256i temp = _mm256_shuffle_epi32(divisors, _mm256_set1_epi32(index));\n            result = _mm256_extract_epi64(temp, 0);\n            break;\n        }\n    }\n    if(result == 0){\n        result = 1;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    // Factor out powers of 2\n    while (_mm256_testc_si256(_mm256_and_si256(v_n, v_two), v_two)) {\n        factors.push_back(2);\n        v_n = _mm256_sub_epi32(v_n, v_two);\n    }\n\n    __m256i v_limit = _mm256_set1_epi32(static_cast<int>(sqrt(n)));\n    __m256i v_i = _mm256_set1_epi32(3);\n\n    while (!_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_i, v_limit))) {\n        __m256i v_mod = _mm256_rem_epi32(v_n, v_i);\n        while (!_mm256_testc_si256(v_mod, v_mod)) {\n            factors.push_back(_mm256_extract_epi32(v_i, 0));\n            v_n = _mm256_div_epi32(v_n, v_i);\n            v_mod = _mm256_rem_epi32(v_n, v_i);\n        }\n        v_i = _mm256_add_epi32(v_i, v_two);\n    }\n\n    if (!_mm256_testz_si256(v_n, v_n)) {\n        factors.push_back(_mm256_extract_epi32(v_n, 0));\n    }\n\n    return factors;\n}\n```\n\n**Note**: The above code assumes the existence of `_mm256_div_epi32` and `_mm256_rem_epi32` for simplicity, but these are not actual AVX/AVX2 intrinsics. You would need to implement these operations using available instructions or fallback to scalar division/modulus for each lane."}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    if (n <= 1) {\n        if (n == 1) factors.push_back(1);\n        return factors;\n    }\n\n    __m256i vdivisor = _mm256_set_epi32(7, 5, 3, 2, 7, 5, 3, 2);\n    __m256i vone = _mm256_set1_epi32(1);\n    __m256i vn = _mm256_set1_epi32(n);\n\n    for(int divisor = 2; divisor * divisor <= n; divisor += 2){\n        if(divisor == 2) divisor--;\n        __m256i vdiv = _mm256_set1_epi32(divisor);\n        __m256i vrem = _mm256_rem_epi32(vn, vdiv);\n        __m256i vzero = _mm256_setzero_si256();\n        __m256i veq = _mm256_cmpeq_epi32(vrem, vzero);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(veq));\n        if(mask){\n            while(n % divisor == 0){\n                factors.push_back(divisor);\n                n /= divisor;\n            }\n            vn = _mm256_set1_epi32(n);\n        }\n    }\n\n    if(n > 1){\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) {\n        if (n == 1) factors.push_back(1);\n        return factors;\n    }\n\n    __m256i divisors = _mm256_set_epi32(11, 7, 5, 3, 2, 2, 2, 2);\n    __m256i counts = _mm256_setzero_si256();\n\n    for (int i = 0; i < 5; ++i) {\n        int d = ((int*)&divisors)[i];\n        if (n % d == 0) {\n            int count = 0;\n            while (n % d == 0) {\n                n /= d;\n                count++;\n            }\n            for (int j = 0; j < count; ++j) {\n                factors.push_back(d);\n            }\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n < 2) return factors;\n\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    // Handle factor of 2\n    while ((n & 1) == 0) {\n        factors.push_back(2);\n        n >>= 1;\n    }\n    __m256i v_n_current = _mm256_set1_epi32(n);\n\n    // Start from 3 and check odd numbers only\n    int factor = 3;\n    while (factor * factor <= n) {\n        __m256i v_factor = _mm256_set1_epi32(factor);\n        __m256i v_mod = _mm256_rem_epu32(v_n_current, v_factor);\n        int mod = _mm256_extract_epi32(v_mod, 0);\n        if (mod == 0) {\n            factors.push_back(factor);\n            while (n % factor == 0) {\n                n /= factor;\n            }\n            v_n_current = _mm256_set1_epi32(n);\n        }\n        factor += 2;\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_one = _mm256_set1_epi32(1);\n\n    // Handle factor of 2\n    while ((n & 1) == 0) {\n        factors.push_back(2);\n        n >>= 1;\n    }\n\n    __m256i v_limit = _mm256_set1_epi32(static_cast<int>(sqrt(n)));\n    __m256i v_i = _mm256_set_epi32(9, 7, 5, 3, 9, 7, 5, 3);\n\n    for (int i = 3; i <= sqrt(n); i += 2) {\n        if (n % i == 0) {\n            while (n % i == 0) {\n                factors.push_back(i);\n                n /= i;\n            }\n        }\n        v_i = _mm256_add_epi32(v_i, v_two);\n        __m256i v_mod = _mm256_rem_epu32(v_n, v_i); // Hypothetical intrinsic for modulo\n        __m256i v_zero = _mm256_setzero_si256();\n        __m256i v_cmp = _mm256_cmpeq_epi32(v_mod, v_zero);\n        int mask = _mm256_movemask_epi8(v_cmp);\n        if (mask != 0) {\n            int index = __builtin_ctz(mask) / 4;\n            int factor = _mm256_extract_epi32(v_i, index);\n            while (n % factor == 0) {\n                factors.push_back(factor);\n                n /= factor;\n            }\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    size_t size = l.size();\n    if (size == 0) return result;\n\n    size_t aligned_size = size - (size % 8);\n    __m256 zero = _mm256_set1_ps(0.0f);\n    __m256 mask;\n\n    for (size_t i = 0; i < aligned_size; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        vec = _mm256_and_ps(vec, mask);\n\n        float temp[8] = {0};\n        _mm256_storeu_ps(temp, vec);\n\n        for(int j = 0; j < 8; ++j){\n            if(temp[j] > 0.0f){\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (size_t i = aligned_size; i < size; ++i){\n        if(l[i] > 0.0f){\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    size_t size = l.size();\n    if (size == 0) return {};\n\n    const int simd_width = 8;\n    std::vector<float> result;\n    result.reserve(size);\n\n    __m256 zero = _mm256_set1_ps(0.0f);\n    __m256 mask;\n\n    for (size_t i = 0; i < size / simd_width * simd_width; i += simd_width){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        vec = _mm256_and_ps(vec, mask);\n\n        float temp[simd_width];\n        _mm256_storeu_ps(temp, vec);\n\n        for(int j = 0; j < simd_width; ++j){\n            if(temp[j] > 0.0f) result.push_back(temp[j]);\n        }\n    }\n\n    for (size_t i = size / simd_width * simd_width; i < size; ++i){\n        if(l[i] > 0.0f) result.push_back(l[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    size_t size = l.size();\n    if (size == 0) return result;\n\n    size_t aligned_size = size / 8 * 8;\n    __m256 zero = _mm256_set1_ps(0.0f);\n    __m256 mask;\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        __m256 selected = _mm256_and_ps(vec, mask);\n\n        float temp[8] = {0};\n        _mm256_storeu_ps(temp, selected);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] > 0.0f) result.push_back(temp[j]);\n        }\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (l[i] > 0.0f) result.push_back(l[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    size_t size = l.size();\n    if (size == 0) return result;\n\n    size_t aligned_size = size / 8 * 8;\n    __m256 zero = _mm256_setzero_ps();\n    __m256 mask;\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmp_ps(vec, zero, _CMP_GT_OQ);\n        __m256 res = _mm256_and_ps(vec, mask);\n\n        float temp[8] = {0};\n        _mm256_storeu_ps(temp, res);\n\n        for (int j = 0; j < 8; ++j) {\n            if (temp[j] > 0) result.push_back(temp[j]);\n        }\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (l[i] > 0) result.push_back(l[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    size_t size = l.size();\n    if (size == 0) return result;\n\n    size_t aligned_size = size - (size % 8);\n    __m256 zero = _mm256_setzero_ps();\n    __m256 mask;\n\n    for (size_t i = 0; i < aligned_size; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        mask = _mm256_cmpgt_ps(vec, zero);\n        __m256 and_vec = _mm256_and_ps(vec, mask);\n\n        float temp[8] = {0};\n        _mm256_storeu_ps(temp, and_vec);\n\n        for(int j = 0; j < 8; ++j){\n            if(temp[j] != 0.0f){\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    for (size_t i = aligned_size; i < size; ++i){\n        if(l[i] > 0){\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n % 2) == 0 || (n % 3) == 0) return false;\n\n    const int64_t limit = static_cast<int64_t>(std::sqrt(static_cast<double>(n)));\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i zero = _mm256_setzero_si256();\n\n    for(int64_t i = 5; i <= limit; i += 8){\n        __m256i vec_i = _mm256_set_epi64x(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i rem = _mm256_rem_epu64(vec_n, vec_i); // Hypothetical intrinsic for remainder\n        __m256i cmp = _mm256_cmpeq_epi64(rem, zero);\n        if(_mm256_movemask_epi8(cmp) != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    const int64_t limit = (int64_t)(sqrt((double)n)) + 1;\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_two = _mm256_set1_epi64x(2);\n\n    for(int64_t i = 5; i < limit; i += 6){\n        __m256i v_i = _mm256_set_epi64x(i+4, i+3, i+2, i+1, i+0, i-1, i-2, i-3);\n        __m256i v_mod = _mm256_rem_epu64(v_n, v_i); // Hypothetical intrinsic for SIMD remainder\n        __m256i v_zero = _mm256_setzero_si256();\n        __m256i v_cmp = _mm256_cmpeq_epi64(v_mod, v_zero);\n        if(_mm256_movemask_epi8(v_cmp) != 0) return false;\n\n        __m256i v_i_plus_2 = _mm256_add_epi64(v_i, v_two);\n        v_mod = _mm256_rem_epu64(v_n, v_i_plus_2); // Hypothetical intrinsic for SIMD remainder\n        v_cmp = _mm256_cmpeq_epi64(v_mod, v_zero);\n        if(_mm256_movemask_epi8(v_cmp) != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    const int64_t limit = static_cast<int64_t>(std::sqrt(static_cast<double>(n))) + 1;\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i zero = _mm256_setzero_si256();\n\n    for(int64_t i = 5; i < limit; i += 8){\n        __m256i vec_i = _mm256_set_epi64x(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i rem = _mm256_rem_epi64(vec_n, vec_i);\n        __m256i check = _mm256_cmpeq_epi64(rem, zero);\n        int mask = _mm256_movemask_epi8(check);\n        if(mask != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n % 2 == 0) || (n % 3 == 0)) return false;\n\n    const int64_t limit = (int64_t)(sqrt((double)n));\n    __m256i vec_n = _mm256_set1_epi64x(n);\n\n    for(int64_t i = 5; i <= limit; i += 6){\n        __m256i vec_i = _mm256_set_epi64x(0, i+2, 0, i);\n        __m256i rem1 = _mm256_rem_epi64(vec_n, vec_i);\n        __m256i zero = _mm256_setzero_si256();\n        if(!_mm256_testz_si256(rem1, rem1)){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    const int64_t limit = static_cast<int64_t>(sqrt(static_cast<double>(n))) + 1;\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int64_t i = 5; i < limit; i += 8) {\n        __m256i vec_i = _mm256_set_epi64x(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i rem = _mm256_rem_epu64(vec_n, vec_i); // Hypothetical intrinsic for remainder\n        __m256i cmp = _mm256_cmpeq_epi64(rem, zero);\n        if (_mm256_movemask_epi8(cmp) != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s){\n    const int len = s.length();\n    if (len == 0) return \"\";\n\n    // Check if there are any letters in the string\n    bool has_letter = false;\n    for (char c : s) {\n        if ((c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z')) {\n            has_letter = true;\n            break;\n        }\n    }\n\n    if (!has_letter) {\n        // Reverse the string if no letters are present\n        return std::string(s.rbegin(), s.rend());\n    }\n\n    // Allocate result string\n    std::string result(len, '\\0');\n\n    // Process in chunks of 32 bytes (AVX registers)\n    const int vec_size = 32;\n    const int num_full_vecs = len / vec_size;\n\n    for(int i = 0; i < num_full_vecs; ++i){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i * vec_size]));\n        \n        // Create mask for lowercase letters\n        __m256i lower_mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('a' - 1)),\n                                              _mm256_cmple_epi8(vec, _mm256_set1_epi8('z')));\n\n        // Create mask for uppercase letters\n        __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('A' - 1)),\n                                              _mm256_cmple_epi8(vec, _mm256_set1_epi8('Z')));\n\n        // Combine masks\n        __m256i letter_mask = _mm256_or_si256(lower_mask, upper_mask);\n\n        // Toggle case for letters\n        __m256i toggled_case = _mm256_xor_si256(vec, _mm256_and_si256(letter_mask, _mm256_set1_epi8(0x20)));\n\n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i * vec_size]), toggled_case);\n    }\n\n    // Handle remaining characters\n    int remainder = len % vec_size;\n    if(remainder > 0){\n        int start = num_full_vecs * vec_size;\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[start]));\n\n        // Create mask for lowercase letters\n        __m256i lower_mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('a' - 1)),\n                                              _mm256_cmple_epi8(vec, _mm256_set1_epi8('z')));\n\n        // Create mask for uppercase letters\n        __m256i upper_mask = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('A' - 1)),\n                                              _mm256_cmple_epi8(vec, _mm256_set1_epi8('Z')));\n\n        // Combine masks\n        __m256i letter_mask = _mm256_or_si256(lower_mask, upper_mask);\n\n        // Toggle case for letters\n        __m256i toggled_case = _mm256_xor_si256(vec, _mm256_and_si256(letter_mask, _mm256_set1_epi8(0x20)));\n\n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[start]), toggled_case);\n\n        // Fix any out-of-bounds characters\n        for(int i = start + remainder; i < len; ++i){\n            result[i] = s[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s){\n    const int len = s.length();\n    if(len == 0) return \"\";\n    \n    // Determine if the string contains any letters\n    bool has_letter = false;\n    for(char c : s){\n        if( ((c >= 'a') && (c <= 'z')) || ((c >= 'A') && (c <= 'Z')) ){\n            has_letter = true;\n            break;\n        }\n    }\n\n    // If no letters, reverse the string\n    if(!has_letter){\n        return std::string(s.rbegin(), s.rend());\n    }\n\n    // Allocate result buffer\n    std::string result(len, '\\0');\n    \n    // Process in chunks of 32 bytes (AVX registers are 32 bytes)\n    const int chunk_size = 32;\n    for(int i = 0; i < len; i += chunk_size){\n        int process_len = (len - i) > chunk_size ? chunk_size : (len - i);\n\n        // Load data into AVX register\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create masks for lowercase and uppercase letters\n        __m256i lower_mask = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8('a' - 1));\n        __m256i upper_mask = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8('A' - 1));\n        __m256i z_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('z' + 1));\n        __m256i Z_mask = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('Z' + 1));\n\n        lower_mask = _mm256_andnot_si256(z_mask, lower_mask);\n        upper_mask = _mm256_andnot_si256(Z_mask, upper_mask);\n\n        // XOR with 0x20 to flip case\n        __m256i flip_mask = _mm256_set1_epi8(0x20);\n        __m256i flipped = _mm256_xor_si256(vec, flip_mask);\n\n        // Combine results using masks\n        __m256i not_lower = _mm256_andnot_si256(lower_mask, vec);\n        __m256i not_upper = _mm256_andnot_si256(upper_mask, vec);\n        __m256i lower_result = _mm256_and_si256(lower_mask, flipped);\n        __m256i upper_result = _mm256_and_si256(upper_mask, flipped);\n\n        vec = _mm256_or_si256(_mm256_or_si256(not_lower, not_upper), _mm256_or_si256(lower_result, upper_result));\n\n        // Store result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return s;\n\n    const int len = s.length();\n    char result[len + 1];\n    result[len] = '\\0';\n\n    bool has_letter = false;\n    __m256i mask_lower = _mm256_set1_epi8(0x20);\n    __m256i mask_upper = _mm256_set1_epi8(0xDF);\n    __m256i mask_alpha = _mm256_set1_epi8(0x1F);\n\n    for(int i = 0; i < len; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i is_lower = _mm256_cmpgt_epi8(_mm256_and_si256(vec, mask_alpha), _mm256_set1_epi8(0x19));\n        __m256i is_upper = _mm256_cmpgt_epi8(_mm256_and_si256(vec, mask_alpha), _mm256_set1_epi8(0x0F));\n        __m256i is_alpha = _mm256_or_si256(is_lower, is_upper);\n\n        __m256i toggle_case = _mm256_xor_si256(vec, _mm256_and_si256(is_alpha, mask_lower));\n        __m256i final_vec = _mm256_blendv_epi8(vec, toggle_case, is_alpha);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result + i), final_vec);\n\n        __m128i reduced_alpha = _mm256_extracti128_si256(is_alpha, 0);\n        reduced_alpha = _mm_or_si128(reduced_alpha, _mm256_extracti128_si256(is_alpha, 1));\n        if (!_mm_testz_si128(reduced_alpha, reduced_alpha)){\n            has_letter = true;\n        }\n    }\n\n    if (!has_letter){\n        for(int i = 0; i < len; ++i){\n            result[i] = s[len - 1 - i];\n        }\n    }\n\n    return std::string(result, result + len);\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return s;\n\n    const int len = s.length();\n    alignas(32) char result[32];\n    bool has_letter = false;\n\n    // Check for letters and process in chunks of 32 bytes\n    for(int i = 0; i <= len - 32; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i lower = _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), vec);\n        __m256i upper = _mm256_cmpgt_epi8(_mm256_set1_epi8('A'), vec);\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('z')), _mm256_set1_epi8(-1));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('Z')), _mm256_set1_epi8(-1));\n\n        __m256i mask = _mm256_or_si256(is_lower, is_upper);\n        has_letter |= (_mm256_movemask_epi8(mask) != 0);\n\n        __m256i toggle_case = _mm256_xor_si256(vec, _mm256_and_si256(mask, _mm256_set1_epi8(0x20)));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), toggle_case);\n\n        for(int j = 0; j < 32; ++j){\n            s[i + j] = result[j];\n        }\n    }\n\n    // Process remaining characters\n    for(int i = (len / 32) * 32; i < len; ++i){\n        if((s[i] >= 'a' && s[i] <= 'z') || (s[i] >= 'A' && s[i] <= 'Z')){\n            has_letter = true;\n            if(s[i] >= 'a' && s[i] <= 'z') s[i] -= 32;\n            else if(s[i] >= 'A' && s[i] <= 'Z') s[i] += 32;\n        }\n    }\n\n    if(!has_letter){\n        std::reverse(s.begin(), s.end());\n    }\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n#include <cctype>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return s;\n\n    // Check if the string contains any letters\n    bool has_letter = false;\n    for(char c : s){\n        if(std::isalpha(c)){\n            has_letter = true;\n            break;\n        }\n    }\n\n    if(!has_letter){\n        // Reverse the string if no letters are present\n        return std::string(s.rbegin(), s.rend());\n    }\n\n    size_t len = s.size();\n    std::string result(len, '\\0');\n\n    const __m256i lower_mask = _mm256_set1_epi8(0x20);\n    const __m256i cmp_mask = _mm256_set1_epi8(0xDF);\n\n    for(size_t i = 0; i < len; i += 32){\n        size_t chunk_size = std::min(len - i, size_t(32));\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n\n        // Create a mask for alphabetic characters\n        __m256i is_alpha = _mm256_cmpistrm(vec, cmp_mask, _SIDD_CMP_RANGES | _SIDD_UPE);\n\n        // Toggle case for alphabetic characters\n        __m256i toggled = _mm256_xor_si256(vec, _mm256_and_si256(is_alpha, lower_mask));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), toggled);\n\n        // Handle remaining characters if the length is not a multiple of 32\n        if(chunk_size < 32){\n            for(size_t j = chunk_size; j < 32; ++j){\n                result[i + j] = s[i + j];\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return {};\n\n    size_t n = l.size();\n    std::vector<int> indices;\n    \n    for(size_t i = 0; i < n; ++i){\n        if(i % 3 == 0) indices.push_back(i);\n    }\n\n    std::vector<int> divisible_elements;\n    for(auto idx : indices){\n        divisible_elements.push_back(l[idx]);\n    }\n\n    std::sort(divisible_elements.begin(), divisible_elements.end());\n\n    size_t sorted_idx = 0;\n    for(auto idx : indices){\n        l[idx] = divisible_elements[sorted_idx++];\n    }\n\n    // SIMD shuffle to ensure correct placement (optional optimization)\n    const size_t simd_width = 8;\n    size_t aligned_size = n / simd_width * simd_width;\n\n    for(size_t i = 0; i < aligned_size; i += simd_width){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&l[i]), vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return l;\n\n    size_t n = l.size();\n    std::vector<int> third_indices;\n    third_indices.reserve(n / 3 + 1);\n\n    for(size_t i = 0; i < n; ++i){\n        if(i % 3 == 0){\n            third_indices.push_back(l[i]);\n        }\n    }\n\n    std::sort(third_indices.begin(), third_indices.end());\n\n    size_t third_idx = 0;\n    for(size_t i = 0; i < n; ++i){\n        if(i % 3 == 0){\n            l[i] = third_indices[third_idx++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return {};\n\n    size_t n = l.size();\n    std::vector<int> sorted_l = l;\n\n    // Extract elements at indices divisible by 3\n    std::vector<int> div_by_three;\n    for(size_t i = 0; i < n; i += 3){\n        div_by_three.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(div_by_three.begin(), div_by_three.end());\n\n    // Use AVX2 to scatter sorted values back into their positions\n    size_t num_div_by_three = div_by_three.size();\n    __m256i mask = _mm256_set_epi32(7,6,5,4,3,2,1,0); // Index mask\n\n    for(size_t i = 0; i < num_div_by_three; i += 8){\n        size_t j = i;\n        __m256i vec = _mm256_setzero_si256();\n        size_t count = std::min((size_t)8, num_div_by_three - i);\n\n        for(size_t k = 0; k < count; ++k){\n            int val = div_by_three[j + k];\n            vec = _mm256_insert_epi32(vec, val, k);\n        }\n\n        for(size_t k = 0; k < count; ++k){\n            size_t index = (i + k) * 3;\n            if(index < n){\n                sorted_l[index] = _mm256_extract_epi32(vec, k);\n            }\n        }\n    }\n\n    return sorted_l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return l;\n\n    size_t size = l.size();\n    std::vector<int> result = l;\n    std::vector<int> thirds;\n\n    // Collect elements at indices divisible by 3\n    for(size_t i = 0; i < size; i++) {\n        if(i % 3 == 0) thirds.push_back(l[i]);\n    }\n\n    // Sort the collected elements\n    std::sort(thirds.begin(), thirds.end());\n\n    // Use AVX2 to scatter sorted values back into their positions\n    size_t third_idx = 0;\n    for(size_t i = 0; i < size && third_idx < thirds.size(); i += 8) {\n        __m256i vec = _mm256_set1_epi32(0); // Initialize with zeros\n        __m256i mask = _mm256_setr_epi32(-1, 0, 0, -1, 0, 0, -1, 0);\n\n        // Load sorted values into vector\n        if(i/3 < thirds.size()/3){\n            vec = _mm256_setr_epi32(thirds[third_idx], 0, 0, thirds[third_idx+1], 0, 0, thirds[third_idx+2], 0, 0);\n            third_idx += 3;\n        }\n\n        // Scatter the values back into the result vector\n        _mm256_maskstore_epi32(&result[i], mask, vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return {};\n\n    size_t n = l.size();\n    std::vector<int> sorted_l = l;\n\n    // Extract elements at indices divisible by 3\n    std::vector<int> div_by_three;\n    for(size_t i = 0; i < n; i += 3){\n        div_by_three.push_back(l[i]);\n    }\n\n    // Sort the extracted elements\n    std::sort(div_by_three.begin(), div_by_three.end());\n\n    // Use AVX2 to scatter sorted values back into their positions\n    size_t num_div_by_three = div_by_three.size();\n    size_t aligned_count = num_div_by_three - (num_div_by_three % 4);\n\n    __m256i indices = _mm256_set_epi32(11, 8, 5, 2, 10, 7, 4, 1);\n    __m256i zero = _mm256_setzero_si256();\n\n    for(size_t i = 0; i < aligned_count; i += 4){\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(&div_by_three[i]));\n        __m256i gather_idx = _mm256_add_epi32(_mm256_mullo_epi32(vec, _mm256_set1_epi32(3)), indices);\n        __m256i mask = _mm256_cmpgt_epi32(gather_idx, zero);\n        _mm256_maskstore_epi32(&sorted_l[0], mask, vec);\n    }\n\n    // Handle remaining elements that couldn't be processed in AVX2 vectors\n    for(size_t i = aligned_count; i < num_div_by_three; ++i){\n        sorted_l[i * 3] = div_by_three[i];\n    }\n\n    return sorted_l;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nfloat max_element_simd(std::vector<float> l){\n    const int n = l.size();\n    if (n == 0) return NAN;\n\n    __m256 vmax = _mm256_set1_ps(l[0]);\n    const int step = 8;\n    for(int i = 0; i <= n - step; i += step){\n        __m256 v = _mm256_loadu_ps(&l[i]);\n        vmax = _mm256_max_ps(vmax, v);\n    }\n\n    // Handle remaining elements\n    if(n % step != 0){\n        alignas(32) float temp[8] = {0};\n        for(int i = 0; i < n % step; ++i){\n            temp[i] = l[n - n % step + i];\n        }\n        __m256 vremaining = _mm256_loadu_ps(temp);\n        vmax = _mm256_max_ps(vmax, vremaining);\n    }\n\n    // Horizontal max\n    alignas(32) float result[8];\n    _mm256_store_ps(result, vmax);\n    float max_val = result[0];\n    for(int i = 1; i < step; ++i){\n        max_val = std::max(max_val, result[i]);\n    }\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nfloat max_element_simd(std::vector<float> l){\n    const int n = l.size();\n    if (n == 0) return 0; // Assuming vector is non-empty as per problem statement\n\n    __m256 max_vals = _mm256_set1_ps(l[0]);\n    const int vec_size = 8;\n\n    for(int i = 0; i <= n - vec_size; i += vec_size){\n        __m256 current = _mm256_loadu_ps(&l[i]);\n        max_vals = _mm256_max_ps(max_vals, current);\n    }\n\n    float max_val = 0.0f;\n    alignas(32) float temp[8];\n    _mm256_store_ps(temp, max_vals);\n\n    max_val = std::max({temp[0], temp[1], temp[2], temp[3], temp[4], temp[5], temp[6], temp[7]});\n\n    for(int i = (n / vec_size) * vec_size; i < n; ++i){\n        max_val = std::max(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) return NAN;\n\n    size_t size = l.size();\n    float result = -INFINITY;\n\n    __m256 vmax = _mm256_set1_ps(-INFINITY);\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        vmax = _mm256_max_ps(vmax, vec);\n    }\n\n    float reduced_max[8];\n    _mm256_storeu_ps(reduced_max, vmax);\n\n    result = std::max({reduced_max[0], reduced_max[1], reduced_max[2], \n                       reduced_max[3], reduced_max[4], reduced_max[5], \n                       reduced_max[6], reduced_max[7]});\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        result = std::max(result, l[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nfloat max_element_simd(std::vector<float> l){\n    const int n = l.size();\n    if (n == 0) return 0; // Assuming vector is non-empty as per the description\n\n    __m256 vmax = _mm256_set1_ps(l[0]);\n    const int vec_size = 8;\n\n    for(int i = 0; i <= n - vec_size; i += vec_size){\n        __m256 v = _mm256_loadu_ps(&l[i]);\n        vmax = _mm256_max_ps(vmax, v);\n    }\n\n    float max_val = horizontal_max(vmax);\n\n    for(int i = (n / vec_size) * vec_size; i < n; ++i){\n        if(l[i] > max_val) max_val = l[i];\n    }\n\n    return max_val;\n}\n\nfloat horizontal_max(__m256 v){\n    __m128 hi = _mm256_extractf128_ps(v, 1);\n    __m128 lo = _mm256_castps256_ps128(v);\n    __m128 max = _mm_max_ps(lo, hi);\n    max = _mm_max_ps(max, _mm_movehl_ps(max, max));\n    max = _mm_max_ss(max, _mm_shuffle_ps(max, max, 1));\n    float result;\n    _mm_store_ss(&result, max);\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nfloat max_element_simd(std::vector<float> l){\n    const int n = l.size();\n    if (n == 0) return NAN;\n\n    __m256 vmax = _mm256_set1_ps(l[0]);\n    for(int i = 0; i < n; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        vmax = _mm256_max_ps(vmax, vec);\n    }\n\n    // Horizontal max\n    float result[8];\n    _mm256_storeu_ps(result, vmax);\n\n    float max_val = result[0];\n    for(int i = 1; i < 8; ++i){\n        if(result[i] > max_val){\n            max_val = result[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    const int step = 8;\n    int count = 0;\n    __m256i div_11 = _mm256_set1_epi32(11);\n    __m256i div_13 = _mm256_set1_epi32(13);\n    __m256i mod_mask = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i seven = _mm256_set1_epi32('7');\n    __m256i tens = _mm256_set_epi32(100000, 10000, 1000, 100, 10, 1, 0, 0);\n\n    for(int i=0; i<n-step; i+=step){\n        __m256i nums = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i rem_11 = _mm256_and_si256(_mm256_sub_epi32(nums, _mm256_mullo_epi32(_mm256_div_epi32(nums, div_11), div_11)), mod_mask);\n        __m256i rem_13 = _mm256_and_si256(_mm256_sub_epi32(nums, _mm256_mullo_epi32(_mm256_div_epi32(nums, div_13), div_13)), mod_mask);\n        __m256i cmp_11 = _mm256_cmpeq_epi32(rem_11, _mm256_setzero_si256());\n        __m256i cmp_13 = _mm256_cmpeq_epi32(rem_13, _mm256_setzero_si256());\n        __m256i mask = _mm256_or_si256(cmp_11, cmp_13);\n\n        __m256i digits = nums;\n        int lane_count = 0;\n        for(int j=0; j<6; j++){\n            __m256i digit = _mm256_and_si256(_mm256_srli_epi32(digits, j*4), _mm256_set1_epi32(0xF));\n            __m256i is_seven = _mm256_cmpeq_epi32(digit, seven);\n            __m256i masked_seven = _mm256_and_si256(is_seven, mask);\n            lane_count += _mm256_popcnt_epi64(_mm256_movemask_epi8(masked_seven));\n        }\n        count += lane_count;\n    }\n    for(int i=n-(n%step); i<n; i++) {\n        if( (i % 11 == 0 || i % 13 == 0) && i >=0 ){\n            int temp = i;\n            while(temp > 0){\n                if(temp % 10 == 7) count++;\n                temp /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    if (n <= 0) return 0;\n\n    const int simd_width = 8;\n    __m256i vseven = _mm256_set1_epi8('7');\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vten = _mm256_set1_epi8(10);\n    __m256i vone = _mm256_set1_epi8(1);\n\n    int count = 0;\n    int limit = n / simd_width * simd_width;\n\n    for(int i = 0; i < limit; i += simd_width){\n        __m256i vidx = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i vdiv11 = _mm256_or_si256(_mm256_cmpeq_epi32(_mm256_rem_epu32(vidx, _mm256_set1_epi32(11)), vzero), \n                                         _mm256_cmpeq_epi32(_mm256_rem_epu32(vidx, _mm256_set1_epi32(13)), vzero));\n        \n        __m256i vmask = _mm256_cmpgt_epi32(vzero, _mm256_and_si256(vdiv11, _mm256_cmpgt_epi32(vidx, vzero)));\n\n        __m256i vtens = _mm256_set_epi32(10000000, 1000000, 100000, 10000, 1000, 100, 10, 1);\n        __m256i vsevens = _mm256_set1_epi32(7);\n\n        __m256i vtemp = _mm256_setzero_si256();\n        for(int j = 0; j < 8; ++j){\n            __m256i vdigits = _mm256_iextract_epi32(vidx, j);\n            __m256i vmod = _mm256_rem_epu32(vdigits, vten);\n            vtemp = _mm256_add_epi32(vtemp, _mm256_and_si256(_mm256_cmpeq_epi32(vmod, vseven), vmask));\n            vdigits = _mm256_div_epu32(vdigits, vten);\n        }\n        count += _mm256_extract_epi32(_mm256_hadd_epi32(vtemp, vtemp), 0);\n    }\n\n    for(int i = limit; i < n; ++i){\n        if((i % 11 == 0 || i % 13 == 0) && i > 0){\n            int num = i;\n            while(num > 0){\n                if(num % 10 == 7) count++;\n                num /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    const int step = 8;\n    int count = 0;\n    __m256i div_11 = _mm256_set1_epi32(11);\n    __m256i div_13 = _mm256_set1_epi32(13);\n    __m256i seven = _mm256_set1_epi32('7');\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i zero = _mm256_setzero_si256();\n    \n    for(int i=0; i <= n-step; i+=step){\n        __m256i nums = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i rem_11 = _mm256_rem_epi32(nums, div_11);\n        __m256i rem_13 = _mm256_rem_epi32(nums, div_13);\n        __m256i mask_11 = _mm256_cmpeq_epi32(rem_11, zero);\n        __m256i mask_13 = _mm256_cmpeq_epi32(rem_13, zero);\n        __m256i mask = _mm256_or_si256(mask_11, mask_13);\n        \n        __m256i temp = nums;\n        __m256i digits[4];\n        for(int j=0; j<4; ++j){\n            digits[j] = _mm256_i32gather_epi32((const int*)&temp, seven, 1);\n            temp = _mm256_div_epi32(temp, ten);\n        }\n        __m256i matches = _mm256_or_si256(digits[0], digits[1]);\n        matches = _mm256_or_si256(matches, digits[2]);\n        matches = _mm256_or_si256(matches, digits[3]);\n        matches = _mm256_and_si256(matches, mask);\n        count += _mm256_popcnt_epi32(matches);\n    }\n    for(int i=n-(n%step); i<n; ++i){\n        if( (i % 11 == 0 || i % 13 == 0) && i >=0 ){\n            char buf[12];\n            int len = snprintf(buf, sizeof(buf), \"%d\", i);\n            for(int j=0; j<len; ++j){\n                if(buf[j] == '7') count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    if (n <= 0) return 0;\n\n    const int simd_width = 8;\n    __m256i vseven = _mm256_set1_epi32('7');\n    __m256i vten = _mm256_set1_epi32(10);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vcount = _mm256_setzero_si256();\n\n    int count = 0;\n    for(int i=0; i<n; i+=simd_width*2){\n        __m256i vindices1 = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i+0);\n        __m256i vindices2 = _mm256_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8);\n\n        __m256i vmod11_1 = _mm256_rem_epu32(vindices1, _mm256_set1_epi32(11));\n        __m256i vmod13_1 = _mm256_rem_epu32(vindices1, _mm256_set1_epi32(13));\n        __m256i visdiv1_1 = _mm256_or_si256(_mm256_cmpeq_epi32(vmod11_1, vzero), _mm256_cmpeq_epi32(vmod13_1, vzero));\n\n        __m256i vmod11_2 = _mm256_rem_epu32(vindices2, _mm256_set1_epi32(11));\n        __m256i vmod13_2 = _mm256_rem_epu32(vindices2, _mm256_set1_epi32(13));\n        __m256i visdiv1_2 = _mm256_or_si256(_mm256_cmpeq_epi32(vmod11_2, vzero), _mm256_cmpeq_epi32(vmod13_2, vzero));\n\n        __m256i vtemp1 = vindices1;\n        __m256i vtemp2 = vindices2;\n\n        __m256i vmask1 = _mm256_cmpgt_epi32(vzero, vtemp1);\n        __m256i vmask2 = _mm256_cmpgt_epi32(vzero, vtemp2);\n\n        vtemp1 = _mm256_and_si256(vtemp1, _mm256_xor_si256(vmask1, _mm256_set1_epi32(-1)));\n        vtemp2 = _mm256_and_si256(vtemp2, _mm256_xor_si256(vmask2, _mm256_set1_epi32(-1)));\n\n        __m256i vdigits1 = _mm256_set_epi32(\n            vtemp1.m256i_i32[7]%10, vtemp1.m256i_i32[6]%10, \n            vtemp1.m256i_i32[5]%10, vtemp1.m256i_i32[4]%10, \n            vtemp1.m256i_i32[3]%10, vtemp1.m256i_i32[2]%10, \n            vtemp1.m256i_i32[1]%10, vtemp1.m256i_i32[0]%10\n        );\n        __m256i vdigits2 = _mm256_set_epi32(\n            vtemp2.m256i_i32[7]%10, vtemp2.m256i_i32[6]%10, \n            vtemp2.m256i_i32[5]%10, vtemp2.m256i_i32[4]%10, \n            vtemp2.m256i_i32[3]%10, vtemp2.m256i_i32[2]%10, \n            vtemp2.m256i_i32[1]%10, vtemp2.m256i_i32[0]%10\n        );\n\n        __m256i veq1 = _mm256_cmpeq_epi32(vdigits1, vseven);\n        __m256i veq2 = _mm256_cmpeq_epi32(vdigits2, vseven);\n\n        __m256i vand1 = _mm256_and_si256(visdiv1_1, veq1);\n        __m256i vand2 = _mm256_and_si256(visdiv1_2, veq2);\n\n        __m256i vones1 = _mm256_cmpgt_epi32(vand1, vzero);\n        __m256i vones2 = _mm256_cmpgt_epi32(vand2, vzero);\n\n        int mask1 = _mm256_movemask_epi8(vones1);\n        int mask2 = _mm256_movemask_epi8(vones2);\n\n        count += __builtin_popcount(mask1);\n        count += __builtin_popcount(mask2);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    int count = 0;\n    const int vector_size = 8;\n    __m256i sevens = _mm256_set1_epi8('7');\n    __m256i divisors[2] = { _mm256_set1_epi32(11), _mm256_set1_epi32(13) };\n    \n    for(int i = 0; i < n; ++i){\n        if(i % 11 == 0 || i % 13 == 0){\n            char str[12];\n            int temp = i;\n            int index = 0;\n            do{\n                str[index++] = temp % 10 + '0';\n                temp /= 10;\n            }while(temp);\n            str[index] = '\\0';\n            \n            for(int j = 0; j < index; j += vector_size){\n                __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + j));\n                __m256i cmp_res = _mm256_cmpeq_epi8(current, sevens);\n                int mask = _mm256_movemask_epi8(cmp_res);\n                count += __builtin_popcount(mask);\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    size_t n = l.size();\n    std::vector<float> sorted_l = l;\n\n    if (n == 0) return sorted_l;\n\n    // Calculate number of full AVX vectors\n    size_t num_full_vectors = n / 8;\n    size_t remainder = n % 8;\n\n    // Extract even indices into a separate vector\n    std::vector<float> even_indices;\n    for (size_t i = 0; i < n; i += 2) {\n        even_indices.push_back(l[i]);\n    }\n\n    // Sort the even indices using standard sort\n    std::sort(even_indices.begin(), even_indices.end());\n\n    // Reconstruct the vector with sorted even indices\n    size_t even_idx = 0;\n    for (size_t i = 0; i < num_full_vectors; ++i) {\n        __m256 even_vec = _mm256_set_ps(\n            0, even_indices[even_idx+3], 0, even_indices[even_idx+2],\n            0, even_indices[even_idx+1], 0, even_indices[even_idx]\n        );\n        even_idx += 4;\n\n        __m256 orig_vec = _mm256_loadu_ps(&l[i * 8]);\n        __m256 mask = _mm256_setr_ps(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256 odd_vec = _mm256_blendv_ps(even_vec, orig_vec, mask);\n\n        _mm256_storeu_ps(&sorted_l[i * 8], odd_vec);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_full_vectors * 8; i < n; ++i) {\n        if (i % 2 == 0) {\n            sorted_l[i] = even_indices[even_idx++];\n        } else {\n            sorted_l[i] = l[i];\n        }\n    }\n\n    return sorted_l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    if (n == 0) return l;\n\n    std::vector<float> even_elements;\n    even_elements.reserve((n + 1) / 2);\n\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    __m256 vec;\n    size_t aligned_count = even_elements.size() / 8 * 8;\n\n    for (size_t i = 0; i < aligned_count; i += 8) {\n        vec = _mm256_loadu_ps(&even_elements[i]);\n        vec = _mm256_permutevar8x32_ps(vec, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        _mm256_storeu_ps(&even_elements[i], vec);\n    }\n\n    // Sort the remaining elements\n    std::sort(even_elements.begin() + aligned_count, even_elements.end());\n\n    // Merge sorted even elements back into original vector\n    for (size_t i = 0, j = 0; i < n; ++i) {\n        if (i % 2 == 0) {\n            l[i] = even_elements[j++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    size_t n = l.size();\n    if (n == 0) return l;\n\n    // Extract even indices elements\n    std::vector<float> evens;\n    for (size_t i = 0; i < n; i += 2) {\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even indices elements\n    std::sort(evens.begin(), evens.end());\n\n    // Reconstruct the vector with sorted even indices\n    size_t even_idx = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        l[i] = evens[even_idx++];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    size_t n = l.size();\n    if (n == 0) return {};\n\n    // Determine the number of full AVX vectors\n    const size_t vec_size = 8; // 256 bits / 32 bits per float\n    size_t num_full_vectors = n / vec_size;\n\n    // Create a copy of even indices\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    // Sort the even elements\n    std::sort(even_elements.begin(), even_elements.end());\n\n    // Reconstruct the vector with sorted even indices\n    size_t even_idx = 0;\n    for (size_t i = 0; i < num_full_vectors; ++i) {\n        __m256 even_vec = _mm256_set_ps(\n            even_elements[even_idx + 7], 0, even_elements[even_idx + 6], 0,\n            even_elements[even_idx + 5], 0, even_elements[even_idx + 4], 0,\n            even_elements[even_idx + 3], 0, even_elements[even_idx + 2], 0,\n            even_elements[even_idx + 1], 0, even_elements[even_idx + 0], 0\n        );\n        __m256 odd_vec = _mm256_loadu_ps(&l[2 * i * vec_size + 1]);\n        __m256 interleaved = _mm256_blend_ps(odd_vec, even_vec, 0xAA);\n        _mm256_storeu_ps(&l[2 * i * vec_size], interleaved);\n        even_idx += vec_size;\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_full_vectors * vec_size * 2; i < n; ++i) {\n        if (i % 2 == 0) {\n            l[i] = even_elements[even_idx++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    size_t n = l.size();\n    std::vector<float> sorted_l = l;\n\n    if (n == 0) return sorted_l;\n\n    size_t aligned_size = n / 8 * 8;\n    for(size_t i = 0; i < aligned_size; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 mask = _mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1);\n        __m256 even = _mm256_andnot_ps(mask, vec);\n\n        float even_arr[8];\n        _mm256_storeu_ps(even_arr, even);\n\n        std::vector<float> evens(even_arr, even_arr + 8);\n        evens.erase(std::remove_if(evens.begin(), evens.end(), [](float x){ return x == 0; }), evens.end());\n        std::sort(evens.begin(), evens.end());\n\n        size_t j = 0;\n        for(size_t k = i; k < i + 8 && j < evens.size(); k += 2){\n            sorted_l[k] = evens[j++];\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = aligned_size; i < n; ++i){\n        if(i % 2 == 0){\n            sorted_l[i] = l[i];\n        }\n    }\n\n    return sorted_l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    if(l.size() < 3) return false;\n\n    size_t n = l.size();\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i < n - 2; ++i){\n        __m256i a = _mm256_set1_epi32(l[i]);\n        for(size_t j = i + 1; j < n - 1; ++j){\n            __m256i b = _mm256_set1_epi32(l[j]);\n            __m256i sum_ab = _mm256_add_epi32(a, b);\n            \n            size_t k_start = (j + 1) / 8 * 8;\n            if((j + 1) % 8 != 0){\n                for(size_t k = j + 1; k < k_start && k < n; ++k){\n                    if(l[k] + l[i] + l[j] == 0) return true;\n                }\n            }\n\n            for(size_t k = k_start; k <= n - 8; k += 8){\n                __m256i c = _mm256_loadu_si256((__m256i const*)(l.data() + k));\n                __m256i sum_abc = _mm256_add_epi32(sum_ab, c);\n                __m256i cmp = _mm256_cmpeq_epi32(sum_abc, zero);\n                if(_mm256_movemask_epi8(cmp)){\n                    return true;\n                }\n            }\n\n            for(size_t k = n / 8 * 8; k < n; ++k){\n                if(l[k] + l[i] + l[j] == 0) return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    if(l.size() < 3) return false;\n\n    std::sort(l.begin(), l.end());\n\n    int n = static_cast<int>(l.size());\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for(int i = 0; i < n - 2; ++i){\n        if(i > 0 && l[i] == l[i-1]) continue;\n        int left = i + 1;\n        int right = n - 1;\n\n        while(left < right){\n            __m256i a = _mm256_set1_epi32(l[i]);\n            __m256i b = _mm256_set1_epi32(l[left]);\n            __m256i c = _mm256_set1_epi32(l[right]);\n\n            __m256i sum = _mm256_add_epi32(_mm256_add_epi32(a, b), c);\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n\n            if(_mm256_movemask_epi8(cmp)){\n                return true;\n            }\n\n            if(_mm256_extract_epi32(sum, 0) < 0){\n                ++left;\n            }\n            else{\n                --right;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    if(n < 3) return false;\n\n    std::sort(l.begin(), l.end());\n\n    for(int i = 0; i < n - 2; ++i){\n        if(i > 0 && l[i] == l[i-1]) continue;\n        int target = -l[i];\n        __m256i vtarget = _mm256_set1_epi32(target);\n        int left = i + 1, right = n - 1;\n\n        while(left < right){\n            __m256i vleft = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[left]));\n            __m256i vright = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[right]));\n\n            __m256i vsum = _mm256_add_epi32(vleft, vright);\n            __m256i vcmp = _mm256_cmpeq_epi32(vsum, vtarget);\n\n            int mask = _mm256_movemask_epi8(vcmp);\n            if(mask != 0){\n                return true;\n            }\n\n            if(_mm256_cmpgt_epi32(vsum, vtarget).m256_i32[0]){\n                right -= 8;\n            }\n            else{\n                left += 8;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < n - 2; ++i) {\n        for (size_t j = i + 1; j < n - 1; ++j) {\n            __m256i sum_ij = _mm256_set1_epi32(l[i] + l[j]);\n            size_t k = j + 1;\n            for (; k + 7 < n; k += 8) {\n                __m256i v_k = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[k]));\n                __m256i sums = _mm256_add_epi32(sum_ij, v_k);\n                __m256i cmp = _mm256_cmpeq_epi32(sums, zero);\n                if (_mm256_movemask_epi8(cmp)) {\n                    return true;\n                }\n            }\n            for (; k < n; ++k) {\n                if (l[i] + l[j] + l[k] == 0) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    if(l.size() < 3) return false;\n\n    std::vector<int> sorted_l = l;\n    std::sort(sorted_l.begin(), sorted_l.end());\n\n    size_t n = sorted_l.size();\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i < n - 2; ++i){\n        int a = sorted_l[i];\n        if(i > 0 && a == sorted_l[i-1]) continue;\n\n        size_t left = i + 1;\n        size_t right = n - 1;\n\n        while(left < right){\n            if(right < left + 8 || right - left < 8){\n                // Fallback to scalar for small ranges\n                while(left < right){\n                    int b = sorted_l[left];\n                    int c = sorted_l[right];\n                    if(a + b + c == 0) return true;\n                    else if(a + b + c < 0) ++left;\n                    else --right;\n                }\n                break;\n            }\n\n            __m256i vb = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&sorted_l[left]));\n            __m256i vc = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&sorted_l[right - 7]));\n\n            __m256i va = _mm256_set1_epi32(a);\n            __m256i sum = _mm256_add_epi32(_mm256_add_epi32(va, vb), vc);\n\n            __m256i cmp = _mm256_cmpeq_epi32(sum, zero);\n            if(_mm256_movemask_epi8(cmp) != 0) return true;\n\n            int mask = _mm256_movemask_epi8(_mm256_cmpgt_epi32(zero, sum));\n            if(mask != 0){\n                int shift = __builtin_ctz(mask) / 4;\n                left += shift;\n            }\n            else{\n                left += 8;\n            }\n\n            mask = _mm256_movemask_epi8(_mm256_cmpgt_epi32(sum, zero));\n            if(mask != 0){\n                int shift = __builtin_ctz(mask) / 4;\n                right -= (8 - shift);\n            }\n            else{\n                right -= 8;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    const int increment = 1;\n    __m256i vincr = _mm256_set1_epi32(increment);\n    \n    for (size_t i = 0; i + 8 <= size; i += 8) {\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i vres = _mm256_add_epi32(vdata, vincr);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vres);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size / 8) * 8; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    \n    const int increment = 1;\n    __m256i vincr = _mm256_set1_epi32(increment);\n    \n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i res = _mm256_add_epi32(vec, vincr);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), res);\n    }\n    \n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    \n    const int increment = 1;\n    __m256i vincr = _mm256_set1_epi32(increment);\n    \n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i vres = _mm256_add_epi32(vdata, vincr);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), vres);\n    }\n    \n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    \n    const int increment = 1;\n    __m256i vincr = _mm256_set1_epi32(increment);\n    \n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i res = _mm256_add_epi32(vec, vincr);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), res);\n    }\n    \n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    \n    const int increment = 1;\n    __m256i vincr = _mm256_set1_epi32(increment);\n    \n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i res = _mm256_add_epi32(vec, vincr);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), res);\n    }\n    \n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l) {\n    if (l.size() < 2) return false;\n\n    const int N = 8;\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i negs = _mm256_setzero_si256();\n    __m256i vec, nvec;\n    std::unordered_set<int> seen;\n\n    for (size_t i = 0; i + N <= l.size(); i += N) {\n        vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        nvec = _mm256_sub_epi32(zero, vec);\n\n        __m256i cmp = _mm256_cmpeq_epi32(vec, nvec);\n        if (_mm256_movemask_epi8(cmp)) return true;\n\n        __m256i shuf = vec;\n        for (int j = 0; j < N; ++j) {\n            int val = _mm256_extract_epi32(shuf, j);\n            if (seen.find(-val) != seen.end()) return true;\n            seen.insert(val);\n        }\n    }\n\n    for (size_t i = (l.size() / N) * N; i < l.size(); ++i) {\n        if (seen.find(-l[i]) != seen.end()) return true;\n        seen.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    std::unordered_set<int> complements;\n\n    size_t i = 0;\n    for (; i + 7 < l.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i neg_vec = _mm256_sub_epi32(zero, vec);\n\n        int data[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(data), neg_vec);\n\n        for (int j = 0; j < 8; ++j) {\n            if (complements.find(data[j]) != complements.end()) {\n                return true;\n            }\n            complements.insert(l[i + j]);\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (complements.find(-l[i]) != complements.end()) {\n            return true;\n        }\n        complements.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    std::unordered_set<int> seen;\n\n    size_t i = 0;\n    for (; i + 7 < l.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i neg_vec = _mm256_sub_epi32(zero, vec);\n\n        int data[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(data), neg_vec);\n\n        for(int j = 0; j < 8; ++j){\n            if(seen.find(data[j]) != seen.end()){\n                return true;\n            }\n        }\n\n        for(int j = 0; j < 8; ++j){\n            seen.insert(l[i+j]);\n        }\n    }\n\n    for (; i < l.size(); ++i){\n        if(seen.find(-l[i]) != seen.end()){\n            return true;\n        }\n        seen.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(const std::vector<int>& l){\n    if (l.size() < 2) return false;\n\n    const int vector_size = 8;\n    size_t aligned_size = l.size() / vector_size * vector_size;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i neg_mask = _mm256_set1_epi32(-1);\n\n    std::unordered_set<int> complements;\n\n    for(size_t i = 0; i < aligned_size; i += vector_size){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i neg_vec = _mm256_sub_epi32(zero, vec);\n\n        int data[8];\n        _mm256_storeu_si256((__m256i*)data, neg_vec);\n\n        for(int j = 0; j < vector_size; ++j){\n            if(complements.find(data[j]) != complements.end()){\n                return true;\n            }\n        }\n\n        for(int j = 0; j < vector_size; ++j){\n            complements.insert(l[i + j]);\n        }\n    }\n\n    for(size_t i = aligned_size; i < l.size(); ++i){\n        if(complements.find(-l[i]) != complements.end()){\n            return true;\n        }\n        complements.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_set>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if(l.size() < 2) return false;\n\n    const int vector_size = 8;\n    size_t n = l.size();\n    __m256i zero = _mm256_set1_epi32(0);\n    std::unordered_set<int> complements;\n\n    for(size_t i = 0; i < n - vector_size + 1; i += vector_size){\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(&l[i]));\n        __m256i neg_vec = _mm256_sub_epi32(zero, vec);\n\n        int data[8];\n        _mm256_storeu_si256((__m256i*)data, neg_vec);\n\n        for(int j = 0; j < vector_size; ++j){\n            if(complements.find(data[j]) != complements.end()){\n                return true;\n            }\n        }\n\n        for(int j = 0; j < vector_size; ++j){\n            complements.insert(l[i+j]);\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = (n / vector_size) * vector_size; i < n; ++i){\n        if(complements.find(-l[i]) != complements.end()){\n            return true;\n        }\n        complements.insert(l[i]);\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 9) return \"\";\n\n    std::ostringstream oss;\n    bool is_negative = false;\n\n    if (x == 0) {\n        return \"0\";\n    }\n\n    if (x < 0) {\n        is_negative = true;\n        x = -x;\n    }\n\n    alignas(32) char digits[32];\n    int count = 0;\n\n    __m256i bcast_base = _mm256_set1_epi8(base);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i remainder;\n    __m256i dividen;\n\n    while(x > 0){\n        int rem = x % base;\n        x /= base;\n        digits[count++] = '0' + rem;\n    }\n\n    if(is_negative){\n        digits[count++] = '-';\n    }\n\n    for(int i = 0; i < count / 2; ++i){\n        char temp = digits[i];\n        digits[i] = digits[count - i - 1];\n        digits[count - i - 1] = temp;\n    }\n\n    digits[count] = '\\0';\n    return std::string(digits, count);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <array>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 9) return \"\";\n\n    std::string result;\n    bool is_negative = false;\n\n    if (x == 0) {\n        return \"0\";\n    }\n\n    if (x < 0) {\n        is_negative = true;\n        x = -x;\n    }\n\n    const int vector_size = 8;\n    alignas(32) std::array<int64_t, vector_size> digits{0};\n    int count = 0;\n\n    __m256i vbase = _mm256_set1_epi8(base);\n    __m256i vx = _mm256_set1_epi64x(x);\n\n    while (x > 0) {\n        __m256i remainder = _mm256_setzero_si256();\n        for (int i = 0; i < vector_size; ++i) {\n            __m256i vtemp = _mm256_div_epi64(vx, vbase);\n            remainder = _mm256_sub_epi64(vx, _mm256_mullo_epi64(vtemp, vbase));\n            vx = vtemp;\n            digits[count++] = _mm256_extract_epi64(remainder, 0);\n            x /= base;\n            if (x == 0) break;\n        }\n    }\n\n    if (is_negative) {\n        result += \"-\";\n    }\n\n    for (int i = count - 1; i >= 0; --i) {\n        result += '0' + digits[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <array>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 9) return \"\";\n\n    std::string result;\n    bool is_negative = false;\n\n    if (x == 0) {\n        return \"0\";\n    }\n\n    if (x < 0) {\n        is_negative = true;\n        x = -x;\n    }\n\n    const int chars_per_vec = 32;\n    alignas(32) std::array<char, chars_per_vec> buffer;\n    __m256i vbase = _mm256_set1_epi8(base);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vrem = _mm256_setzero_si256();\n    __m256i vquot = _mm256_setzero_si256();\n\n    int idx = chars_per_vec;\n    while (x > 0) {\n        int rem = x % base;\n        x /= base;\n        buffer[--idx] = '0' + rem;\n    }\n\n    result.assign(buffer.begin() + idx, buffer.begin() + chars_per_vec);\n\n    if (is_negative) {\n        result.insert(result.begin(), '-');\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <array>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 10) return \"\";\n\n    std::string result;\n    bool is_negative = false;\n\n    __m256i vbase = _mm256_set1_epi8(base);\n    __m256i vx = _mm256_set1_epi64x(x);\n\n    if (x == 0) return \"0\";\n\n    if (x < 0){\n        is_negative = true;\n        x = -x;\n        vx = _mm256_sub_epi64(_mm256_setzero_si256(), vx);\n    }\n\n    std::array<char, 64> digits;\n    size_t digit_count = 0;\n\n    while (x > 0){\n        __m256i vrem = _mm256_set1_epi64x(x % base);\n        __m256i vdiv = _mm256_set1_epi64x(x / base);\n        digits[digit_count++] = static_cast<char>(x % base) + '0';\n        x /= base;\n        vx = vdiv;\n    }\n\n    if (is_negative){\n        result += '-';\n    }\n\n    for (size_t i = digit_count; i > 0; --i){\n        result += digits[i - 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 9) return \"\";\n\n    std::string result;\n    bool is_negative = false;\n\n    if (x == 0) {\n        return \"0\";\n    }\n\n    if (x < 0) {\n        is_negative = true;\n        x = -x;\n    }\n\n    __m256i digits = _mm256_setzero_si256();\n    __m256i digit_count = _mm256_set1_epi8(0);\n    __m256i remainder = _mm256_set1_epi64x(x);\n    __m256i divisor = _mm256_set1_epi8(base);\n\n    const int max_digits = 64;\n    char buffer[max_digits];\n    int count = 0;\n\n    while (x > 0) {\n        __m256i rem = _mm256_rem_epu32(remainder, divisor);\n        __m256i quot = _mm256_div_epu32(remainder, divisor);\n\n        int r = _mm256_extract_epi32(rem, 0);\n        buffer[count++] = '0' + r;\n\n        x = _mm256_extract_epi64(quot, 0);\n        remainder = quot;\n    }\n\n    if (is_negative) {\n        buffer[count++] = '-';\n    }\n\n    result.assign(buffer, count);\n    std::reverse(result.begin(), result.end());\n\n    return result;\n}\n```\n\n**Note:** The above code assumes the existence of `_mm256_rem_epu32` and `_mm256_div_epu32` for simplicity, but these are not actual AVX/AVX2 intrinsics. You would need to implement or approximate division and remainder operations using available SIMD instructions."}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t half_len = len / 2;\n    const char* data = text.c_str();\n\n    size_t i = 0;\n    for (; i + 31 < half_len; i += 32) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + len - i - 32));\n\n        __m256i rev_right = _mm256_shuffle_epi8(right, _mm256_setr_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0,\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n\n        __m256i cmp = _mm256_cmpeq_epi8(left, rev_right);\n        if (!_mm256_movemask_epi8(cmp)) {\n            return false;\n        }\n    }\n\n    for (; i < half_len; ++i) {\n        if (data[i] != data[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t half_len = len / 2;\n    const char* data = text.c_str();\n\n    size_t vec_len = len / 32 * 32; // Process in chunks of 32 bytes\n\n    for (size_t i = 0; i < vec_len; i += 32) {\n        __m256i left = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i right = _mm256_loadu_si256((__m256i*)(data + len - i - 32));\n\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        __m256i mask = _mm256_set1_epi8(0xFF);\n\n        __m256i result = _mm256_and_si256(cmp, mask);\n        int check = _mm256_movemask_epi8(result);\n\n        if (check != 0xFFFFFFFF) return false;\n    }\n\n    for (size_t i = vec_len; i < half_len; ++i) {\n        if (data[i] != data[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t half_len = len / 2;\n    const char* data = text.c_str();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask = _mm256_set_epi8(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0);\n\n    for(size_t i = 0; i < half_len; i += 32){\n        size_t j = len - i - 32;\n        if(j < i + 32) break;\n\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[j]));\n\n        right = _mm256_shuffle_epi8(right, mask);\n\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        cmp = _mm256_andnot_si256(zero, cmp);\n\n        if(_mm256_movemask_epi8(cmp) != 0xFFFF){\n            return false;\n        }\n    }\n\n    for(size_t i = half_len; i < len - half_len; ++i){\n        if(data[i] != data[len - i - 1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t half_len = len / 2;\n    const char* data = text.c_str();\n\n    size_t vec_size = 32; // AVX2 vector size in bytes\n    size_t aligned_len = len / vec_size * vec_size;\n\n    for (size_t i = 0; i < aligned_len; i += vec_size) {\n        __m256i left = _mm256_loadu_si256((__m256i const*)(data + i));\n        __m256i right = _mm256_loadu_si256((__m256i const*)(data + len - i - vec_size));\n\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        if (!_mm256_testc_si256(cmp, _mm256_set1_epi8(-1))) {\n            return false;\n        }\n    }\n\n    for (size_t i = aligned_len; i < half_len; ++i) {\n        if (data[i] != data[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t half_len = len / 2;\n    const char* data = text.c_str();\n\n    __m256i zero = _mm256_setzero_si256();\n    size_t i = 0;\n\n    for (; i + 31 < half_len; i += 32) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + len - i - 32));\n\n        __m256i cmp = _mm256_cmpeq_epi8(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < half_len; ++i) {\n        if (data[i] != data[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod_base = 26;\n    const char base_char = 'a';\n    const char min_char = 'a';\n    const char max_char = 'z';\n\n    size_t len = s.length();\n    std::string out(len, '\\0');\n    size_t vec_len = len / 32 * 32;\n\n    __m256i shift_vec = _mm256_set1_epi8(shift);\n    __m256i base_vec = _mm256_set1_epi8(base_char);\n    __m256i min_vec = _mm256_set1_epi8(min_char);\n    __m256i max_vec = _mm256_set1_epi8(max_char);\n    __m256i mod_base_vec = _mm256_set1_epi8(mod_base);\n\n    for (size_t i = 0; i < vec_len; i += 32) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i adjusted = _mm256_add_epi8(data, shift_vec);\n        __m256i below_min = _mm256_cmpgt_epi8(min_vec, adjusted);\n        __m256i wrap_adjust = _mm256_add_epi8(_mm256_and_si256(below_min, mod_base_vec), adjusted);\n        __m256i clamped = _mm256_max_epi8(wrap_adjust, min_vec);\n        clamped = _mm256_min_epi8(clamped, max_vec);\n        clamped = _mm256_add_epi8(clamped, base_vec);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), clamped);\n    }\n\n    for (size_t i = vec_len; i < len; ++i) {\n        int w = ((int)s[i] - 5 - (int)'a') % 26 + (int)'a';\n        if (w < 'a') w += 26;\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod_base = 26;\n    const char base_char = 'a';\n    const int vector_size = 32; // AVX2: 256 bits = 32 bytes\n    const int len = s.length();\n    std::string out(len, '\\0');\n\n    __m256i shift_val = _mm256_set1_epi8(shift);\n    __m256i mod_base_val = _mm256_set1_epi8(mod_base);\n    __m256i base_char_val = _mm256_set1_epi8(base_char);\n    __m256i zero_val = _mm256_setzero_si256();\n\n    for(int i = 0; i <= len - vector_size; i += vector_size){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i sub = _mm256_sub_epi8(vec, base_char_val);\n        __m256i add = _mm256_add_epi8(sub, shift_val);\n\n        // Compute modulo 26 using a clamp-and-adjust approach\n        __m256i cmp_ge = _mm256_cmpgt_epi8(add, mod_base_val); // Compare each byte >= 26\n        __m256i mod_adj = _mm256_and_si256(cmp_ge, mod_base_val); // Masked adjustment\n        __m256i wrap = _mm256_sub_epi8(add, mod_adj); // Wrap around if necessary\n\n        __m256i result = _mm256_add_epi8(wrap, base_char_val);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Handle remaining characters (if any)\n    for(int i = (len / vector_size) * vector_size; i < len; ++i){\n        int w = ((int)s[i] + shift - (int)base_char) % mod_base;\n        if(w < 0) w += mod_base;\n        out[i] = (char)(w + base_char);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod_base = 26;\n    const char base_char = 'a';\n    const int vector_size = 32; // AVX2: 256 bits = 32 bytes\n    const int len = s.length();\n    std::string out(len, '\\0');\n    \n    __m256i shift_val = _mm256_set1_epi8(shift);\n    __m256i mod_base_val = _mm256_set1_epi8(mod_base);\n    __m256i base_char_val = _mm256_set1_epi8(base_char);\n    __m256i lower_bound = _mm256_set1_epi8('a');\n    __m256i upper_bound = _mm256_set1_epi8('z');\n    \n    for(int i = 0; i < len; i += vector_size){\n        int chunk_size = (len - i) > vector_size ? vector_size : (len - i);\n        \n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i adjusted = _mm256_add_epi8(data, shift_val);\n        \n        __m256i below_a = _mm256_cmpgt_epi8(lower_bound, adjusted);\n        __m256i wrap_around = _mm256_add_epi8(adjusted, mod_base_val);\n        adjusted = _mm256_blendv_epi8(adjusted, wrap_around, below_a);\n        \n        __m256i clamped = _mm256_max_epi8(adjusted, lower_bound);\n        clamped = _mm256_min_epi8(clamped, upper_bound);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), clamped);\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod_base = 26;\n    const char base_char = 'a';\n    const int vector_size = 32; // AVX2 uses 256-bit registers, which can hold 32 chars\n    int length = s.length();\n    std::string out(length, '\\0');\n    \n    __m256i shift_vec = _mm256_set1_epi8(shift);\n    __m256i base_vec = _mm256_set1_epi8(base_char);\n    __m256i mod_base_vec = _mm256_set1_epi8(mod_base);\n\n    for(int i = 0; i <= length - vector_size; i += vector_size){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i sub = _mm256_sub_epi8(data, base_vec);\n        __m256i add_shift = _mm256_add_epi8(sub, shift_vec);\n        \n        // Compute ((x + shift) % 26)\n        __m256i cmp_ge_mod = _mm256_cmpgt_epi8(add_shift, mod_base_vec); // x >= 26\n        __m256i mod_mask = _mm256_and_si256(cmp_ge_mod, mod_base_vec);\n        __m256i wrapped = _mm256_sub_epi8(add_shift, mod_mask);\n        \n        __m256i result = _mm256_add_epi8(wrapped, base_vec);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n\n    // Handle remaining characters (if any)\n    for(int i = (length / vector_size) * vector_size; i < length; ++i){\n        int w = ((int)s[i] + shift - (int)base_char) % mod_base;\n        if(w < 0) w += mod_base;\n        out[i] = (char)(w + base_char);\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod_base = 26;\n    const char base_char = 'a';\n    const int vector_size = 32; // AVX-512 uses 512-bit registers, but assuming AVX2 here (256-bit)\n    int length = s.length();\n    std::string out(length, '\\0');\n    \n    __m256i v_shift = _mm256_set1_epi8(shift);\n    __m256i v_mod_base = _mm256_set1_epi8(mod_base);\n    __m256i v_base_char = _mm256_set1_epi8(base_char);\n    __m256i v_a = _mm256_set1_epi8('a');\n    __m256i v_z = _mm256_set1_epi8('z');\n    \n    for(int i = 0; i < length; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i adjusted = _mm256_add_epi8(vec, v_shift);\n        \n        // Clamp values between 'a' and 'z'\n        __m256i below_a = _mm256_cmpgt_epi8(v_a, adjusted);\n        adjusted = _mm256_add_epi8(adjusted, _mm256_and_si256(below_a, v_mod_base));\n        \n        __m256i above_z = _mm256_cmpgt_epi8(adjusted, v_z);\n        adjusted = _mm256_sub_epi8(adjusted, _mm256_and_si256(above_z, v_mod_base));\n        \n        __m256i result = _mm256_add_epi8(adjusted, v_base_char);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), result);\n    }\n    \n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    const int n = l.size();\n    const int t_broadcast = t;\n    const __m256i threshold = _mm256_set1_epi32(t_broadcast);\n    const int num_lanes = 8;\n    int mask = 0;\n\n    for (size_t i = 0; i <= n - num_lanes; i += num_lanes) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        mask |= _mm256_movemask_epi8(cmp);\n    }\n\n    // Handle remaining elements\n    const size_t remainder = n % num_lanes;\n    if (remainder > 0) {\n        alignas(32) int temp[num_lanes] = {0};\n        for (size_t i = 0; i < remainder; ++i) {\n            temp[i] = l[n - remainder + i];\n        }\n        __m256i vec = _mm256_load_si256(reinterpret_cast<const __m256i*>(temp));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        mask |= _mm256_movemask_epi8(cmp);\n    }\n\n    return mask == 0;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    const int n = static_cast<int>(l.size());\n    const int t_broadcast[8] = {t, t, t, t, t, t, t, t};\n    __m256i threshold = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(t_broadcast));\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    for(int i = 0; i <= n - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        mask = _mm256_and_si256(mask, cmp);\n    }\n\n    // Check remaining elements\n    for(int i = n - (n % 8); i < n; ++i){\n        if(l[i] >= t) return false;\n    }\n\n    // Check if any element was greater than or equal to t\n    int32_t result[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), mask);\n    for(int i = 0; i < 8; ++i){\n        if(result[i] != -1) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    __m256i threshold = _mm256_set1_epi32(t);\n    size_t aligned_size = l.size() / 8 * 8;\n\n    bool result = true;\n    const int* data = l.data();\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (_mm256_movemask_epi8(cmp)) {\n            result = false;\n            break;\n        }\n    }\n\n    for (size_t i = aligned_size; i < l.size(); ++i) {\n        if (l[i] >= t) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_threshold_simd(const std::vector<int>& l, int t){\n    if (l.empty()) return true;\n\n    const int n = static_cast<int>(l.size());\n    const int t_broadcast[8] = {t, t, t, t, t, t, t, t};\n    __m256i threshold = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(t_broadcast));\n\n    bool result = true;\n    int i = 0;\n\n    for (; i <= n - 8; i += 8) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(data, threshold);\n        uint32_t mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        if (mask != 0) {\n            result = false;\n            break;\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] >= t) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    const int num_elements = l.size();\n    const int simd_width = 8; // AVX/AVX2 processes 8 integers (256 bits / 32 bits per int)\n    const int aligned_size = num_elements / simd_width * simd_width;\n    __m256i threshold = _mm256_set1_epi32(t);\n\n    bool result = true;\n    size_t i = 0;\n\n    // Process vectorized elements\n    for (; i <= aligned_size - simd_width; i += simd_width) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold); // Compare vec > threshold\n        if (_mm256_movemask_epi8(cmp) != 0) { // Check if any element is greater than threshold\n            result = false;\n            break;\n        }\n    }\n\n    // Process remaining scalar elements\n    for (; i < num_elements; ++i) {\n        if (l[i] >= t) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int len = brackets.length();\n    if (len == 0) return true;\n\n    const __m256i opening = _mm256_set1_epi8('<');\n    const __m256i closing = _mm256_set1_epi8('>');\n    const __m256i zero = _mm256_setzero_si256();\n\n    int balance = 0;\n    for(int i = 0; i < len; i += 32){\n        if(i + 32 > len){\n            // Handle remaining characters with scalar code\n            for(int j = i; j < len; ++j){\n                if(brackets[j] == '<') balance++;\n                else if(brackets[j] == '>'){\n                    balance--;\n                    if(balance < 0) return false;\n                }\n            }\n            break;\n        }\n\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i eq_opening = _mm256_cmpeq_epi8(chunk, opening);\n        __m256i eq_closing = _mm256_cmpeq_epi8(chunk, closing);\n\n        int open_count = _mm256_popcnt_epi8(eq_opening);\n        int close_count = _mm256_popcnt_epi8(eq_closing);\n\n        balance += open_count - close_count;\n        if(balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n\n// Helper function to count set bits in a 256-bit vector\nint _mm256_popcnt_epi8(__m256i v){\n    alignas(32) uint8_t temp[32];\n    _mm256_store_si256((__m256i*)temp, v);\n    int count = 0;\n    for(int i = 0; i < 32; ++i){\n        count += (temp[i] != 0);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int len = brackets.length();\n    if(len == 0) return true;\n\n    const int vec_size = 32; // AVX2 vector size in bytes\n    const int elem_size = sizeof(char);\n    const int num_elem = vec_size / elem_size;\n\n    __m256i zero = _mm256_set1_epi8(0);\n    __m256i open = _mm256_set1_epi8('<');\n    __m256i close = _mm256_set1_epi8('>');\n    __m256i balance = _mm256_setzero_si256();\n\n    for(int i = 0; i < len; i += num_elem){\n        int chunk_size = (len - i) > num_elem ? num_elem : (len - i);\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i cmp_open = _mm256_cmpeq_epi8(data, open);\n        __m256i cmp_close = _mm256_cmpeq_epi8(data, close);\n\n        __m256i inc = _mm256_and_si256(cmp_open, _mm256_set1_epi8(1));\n        __m256i dec = _mm256_andnot_si256(cmp_open, cmp_close);\n        dec = _mm256_and_si256(dec, _mm256_set1_epi8(-1));\n\n        __m256i delta = _mm256_add_epi8(inc, dec);\n        balance = _mm256_add_epi8(balance, delta);\n\n        __m256i negative_check = _mm256_cmpgt_epi8(zero, balance);\n        int mask = _mm256_movemask_epi8(negative_check);\n        if(mask != 0) return false;\n    }\n\n    int total_balance = 0;\n    alignas(32) char temp[32];\n    _mm256_store_si256((__m256i*)temp, balance);\n    for(int i = 0; i < 32; ++i){\n        total_balance += static_cast<int>(temp[i]);\n    }\n    return total_balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstdint>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int len = brackets.length();\n    if(len == 0) return true;\n\n    const __m256i opening = _mm256_set1_epi8('<');\n    const __m256i closing = _mm256_set1_epi8('>');\n    const __m256i zero = _mm256_setzero_si256();\n\n    int balance = 0;\n    for(size_t i = 0; i < len; i += 32){\n        if(i + 32 > len){\n            // Handle the remaining characters\n            for(size_t j = i; j < len; ++j){\n                if(brackets[j] == '<') balance++;\n                else if(brackets[j] == '>'){\n                    balance--;\n                    if(balance < 0) return false;\n                }\n            }\n            break;\n        }\n\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i eq_opening = _mm256_cmpeq_epi8(chunk, opening);\n        __m256i eq_closing = _mm256_cmpeq_epi8(chunk, closing);\n\n        int count_opening = _mm256_popcnt_epi8(eq_opening);\n        int count_closing = _mm256_popcnt_epi8(eq_closing);\n\n        balance += count_opening - count_closing;\n        if(balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n\n// Helper function to count set bits in a 256-bit vector\nint _mm256_popcnt_epi8(__m256i v){\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i sum = _mm256_sad_epu8(v, zero);\n    uint32_t lo, hi;\n    _mm256_storeu_si256((__m256i*)&lo, sum);\n    hi = lo >> 32;\n    return (lo & 0xFFFFFFFF) + (hi & 0xFFFFFFFF);\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int len = brackets.length();\n    if(len == 0) return true;\n\n    const int simd_width = 32;\n    __m256i zero = _mm256_set1_epi8(0);\n    __m256i open = _mm256_set1_epi8('<');\n    __m256i close = _mm256_set1_epi8('>');\n    int balance = 0;\n\n    for(int i = 0; i < len; i += simd_width){\n        int current_balance = balance;\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i cmp_open = _mm256_cmpeq_epi8(chunk, open);\n        __m256i cmp_close = _mm256_cmpeq_epi8(chunk, close);\n        int mask_open = _mm256_movemask_epi8(cmp_open);\n        int mask_close = _mm256_movemask_epi8(cmp_close);\n\n        current_balance += __builtin_popcount(mask_open);\n        current_balance -= __builtin_popcount(mask_close);\n\n        if(current_balance < 0){\n            return false;\n        }\n        balance = current_balance;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <limits>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int len = brackets.length();\n    if(len == 0) return true;\n\n    const __m256i opening = _mm256_set1_epi8('<');\n    const __m256i closing = _mm256_set1_epi8('>');\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i ones = _mm256_set1_epi8(1);\n\n    int balance = 0;\n    for(size_t i = 0; i < len; i += 32){\n        if(balance < 0) return false;\n\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i is_open = _mm256_cmpeq_epi8(chunk, opening);\n        __m256i is_close = _mm256_cmpeq_epi8(chunk, closing);\n\n        __m256i open_count = _mm256_sad_epu8(is_open, zero);\n        __m256i close_count = _mm256_sad_epu8(is_close, zero);\n\n        int open = _mm256_extract_epi32(open_count, 0) >> 24;\n        int close = _mm256_extract_epi32(close_count, 0) >> 24;\n\n        balance += open - close;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int len = brackets.length();\n    if (len == 0) return true;\n\n    alignas(32) int counts[8] = {0};\n    const int chunks = len / 32;\n    const int remainder = len % 32;\n\n    for (int i = 0; i < chunks; ++i) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i * 32]));\n        __m256i open = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        __m256i close = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(')'));\n\n        int open_count = _mm256_popcnt_epi8(open);\n        int close_count = _mm256_popcnt_epi8(close);\n\n        counts[i % 8] += open_count - close_count;\n\n        if (counts[i % 8] < 0) return false;\n    }\n\n    if (remainder > 0) {\n        __m256i mask = _mm256_set_epi8(\n            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n            -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n        );\n        __m256i chunk = _mm256_maskload_epi8(\n            reinterpret_cast<const __m256i*>(&brackets[chunks * 32]), mask\n        );\n        __m256i open = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        __m256i close = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(')'));\n\n        int open_count = _mm256_popcnt_epi8(open);\n        int close_count = _mm256_popcnt_epi8(close);\n\n        counts[chunks % 8] += open_count - close_count;\n\n        if (counts[chunks % 8] < 0) return false;\n    }\n\n    int total = 0;\n    for (int i = 0; i < 8; ++i) total += counts[i];\n    return total == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int len = brackets.length();\n    if (len == 0) return true;\n\n    alignas(32) int counts[8] = {0};\n    const int chunks = len / 32;\n    const int remainder = len % 32;\n\n    for(int i = 0; i < chunks; ++i){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i * 32]));\n        __m256i ones = _mm256_set1_epi8('(');\n        __m256i zeros = _mm256_setzero_si256();\n\n        __m256i cmp_eq_open = _mm256_cmpeq_epi8(vec, ones);\n        __m256i cmp_eq_close = _mm256_cmpeq_epi8(vec, zeros);\n\n        __m256i open_masks = _mm256_and_si256(cmp_eq_open, ones);\n        __m256i close_masks = _mm256_andnot_si256(cmp_eq_close, ones);\n\n        __m256i diff = _mm256_sub_epi8(open_masks, close_masks);\n\n        __m128i sum1 = _mm_add_epi32(_mm256_extracti128_si256(diff, 0), _mm256_extracti128_si256(diff, 1));\n        __m128i sums = _mm_hadd_epi32(sum1, sum1);\n        __m128i total = _mm_hadd_epi32(sums, sums);\n\n        int temp_counts[4];\n        _mm_storeu_si128((__m128i*)temp_counts, total);\n\n        counts[0] += temp_counts[0];\n        if(counts[0] < 0) return false;\n    }\n\n    for(int i = chunks * 32; i < len; ++i){\n        if(brackets[i] == '('){\n            counts[0]++;\n        }\n        else if(brackets[i] == ')'){\n            counts[0]--;\n            if(counts[0] < 0) return false;\n        }\n    }\n\n    return counts[0] == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int size = brackets.size();\n    if (size == 0) return true;\n\n    const int vec_size = 32; // AVX-512 vector size in bytes\n    const int elements_per_vec = vec_size / sizeof(char);\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i balance = _mm256_setzero_si256();\n    __m256i open = _mm256_set1_epi8('(');\n    __m256i close = _mm256_set1_epi8(')');\n    __m256i one = _mm256_set1_epi8(1);\n    __m256i neg_one = _mm256_set1_epi8(-1);\n\n    int remainder = 0;\n    for(int i = 0; i < size; i += elements_per_vec){\n        int chunk_size = (i + elements_per_vec <= size) ? elements_per_vec : (size - i);\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i is_open = _mm256_cmpeq_epi8(data, open);\n        __m256i is_close = _mm256_cmpeq_epi8(data, close);\n\n        __m256i inc = _mm256_and_si256(is_open, one);\n        __m256i dec = _mm256_and_si256(is_close, neg_one);\n\n        __m256i delta = _mm256_add_epi8(inc, dec);\n        balance = _mm256_add_epi8(balance, delta);\n\n        __m256i negative_check = _mm256_cmpgt_epi8(zero, balance);\n        __m256i mask = _mm256_movemask_epi8(negative_check);\n        if(mask != 0){\n            return false;\n        }\n\n        if(i + elements_per_vec > size){\n            int horizontal_sum = 0;\n            __m256i temp = balance;\n            for(int j = 0; j < elements_per_vec; j += 16){\n                __m128i low = _mm256_extracti128_si256(temp, 0);\n                __m128i high = _mm256_extracti128_si256(temp, 1);\n                horizontal_sum += _mm_cvtsi128_si32(low) + _mm_cvtsi128_si32(high);\n                temp = _mm256_alignr_epi8(temp, temp, 8);\n            }\n            remainder += horizontal_sum;\n        }\n    }\n\n    return remainder == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int len = brackets.length();\n    if (len == 0) return true;\n\n    alignas(32) int counts[8] = {0};\n    const int chunks = len / 32;\n    int remainder = len % 32;\n\n    for (int i = 0; i < chunks; ++i) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i * 32]));\n        __m256i open = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        __m256i close = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(')'));\n\n        int open_count = _mm256_popcnt_epi64(_mm256_movemask_epi8(open));\n        int close_count = _mm256_popcnt_epi64(_mm256_movemask_epi8(close));\n\n        counts[i % 8] += open_count - close_count;\n        if (counts[i % 8] < 0) return false;\n    }\n\n    if (remainder > 0) {\n        alignas(32) char temp[32] = {0};\n        memcpy(temp, &brackets[chunks * 32], remainder);\n        __m256i chunk = _mm256_load_si256(reinterpret_cast<const __m256i*>(temp));\n        __m256i open = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('('));\n        __m256i close = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8(')'));\n\n        int open_count = _mm256_popcnt_epi64(_mm256_movemask_epi8(open));\n        int close_count = _mm256_popcnt_epi64(_mm256_movemask_epi8(close));\n\n        counts[chunks % 8] += open_count - close_count;\n        if (counts[chunks % 8] < 0) return false;\n    }\n\n    int total = 0;\n    for (int i = 0; i < 8; ++i) total += counts[i];\n    return total == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    if (brackets.empty()) return true;\n\n    const int n = brackets.size();\n    const int vec_size = 32; // AVX-512 vector size in bytes\n    const int chunk_size = vec_size / sizeof(char);\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i count = _mm256_setzero_si256();\n    __m256i open = _mm256_set1_epi8('(');\n    __m256i close = _mm256_set1_epi8(')');\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i neg_ones = _mm256_set1_epi8(-1);\n\n    int remainder = 0;\n    for(int i = 0; i < n; i += chunk_size){\n        int limit = std::min(i + chunk_size, n);\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n\n        __m256i is_open = _mm256_cmpeq_epi8(vec, open);\n        __m256i is_close = _mm256_cmpeq_epi8(vec, close);\n\n        __m256i inc = _mm256_and_si256(is_open, ones);\n        __m256i dec = _mm256_and_si256(is_close, neg_ones);\n\n        __m256i delta = _mm256_add_epi8(inc, dec);\n        count = _mm256_add_epi8(count, delta);\n\n        __m256i min_check = _mm256_min_epi8(count, zero);\n        __m256i cmp = _mm256_cmpeq_epi8(min_check, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        if(mask != 0xFFFF){\n            return false;\n        }\n\n        if(limit == n && (limit - i) < chunk_size){\n            int temp_count = 0;\n            for(int j = i; j < limit; ++j){\n                temp_count += (brackets[j] == '(') ? 1 : ((brackets[j] == ')') ? -1 : 0);\n                if(temp_count < 0) return false;\n            }\n            remainder = temp_count;\n        }\n    }\n\n    int hsum = 0;\n    alignas(32) char buffer[32];\n    _mm256_store_si256((__m256i*)buffer, count);\n    for(int i = 0; i < 32; ++i){\n        hsum += static_cast<int8_t>(buffer[i]);\n    }\n\n    return (hsum + remainder) == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n    \n    std::vector<float> result(n - 1, 0.0f);\n    \n    size_t aligned_size = (n - 1) / 8 * 8;\n    __m256 indices = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);\n    \n    for(size_t i = 0; i < aligned_size; i += 8){\n        __m256 coeffs = _mm256_loadu_ps(&xs[i+1]);\n        __m256 mult = _mm256_mul_ps(coeffs, indices);\n        _mm256_storeu_ps(&result[i], mult);\n        indices = _mm256_add_ps(indices, _mm256_set1_ps(8.0f));\n    }\n    \n    for(size_t i = aligned_size; i < n - 1; ++i){\n        result[i] = xs[i+1] * (i + 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256 indices = _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f);\n    __m256 coeffs;\n\n    for(size_t i = 0; i + 8 <= n - 1; i += 8){\n        coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 mul_res = _mm256_mul_ps(coeffs, indices);\n        _mm256_storeu_ps(&result[i], mul_res);\n        indices = _mm256_add_ps(indices, _mm256_set1_ps(8.0f));\n    }\n\n    for(size_t i = (n - 1) / 8 * 8; i < n - 1; ++i){\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256 indices = _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f);\n    size_t i = 0;\n\n    for (; i + 8 <= n - 1; i += 8){\n        __m256 coeffs = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 idx = _mm256_add_ps(indices, _mm256_set1_ps(i));\n        __m256 prod = _mm256_mul_ps(coeffs, idx);\n        _mm256_storeu_ps(&result[i], prod);\n    }\n\n    for (; i < n - 1; ++i){\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256 indices_vec = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);\n\n    for (size_t i = 0; i < n - 1; i += 8) {\n        __m256 xs_vec = _mm256_loadu_ps(&xs[i + 1]);\n        __m256 mul_vec = _mm256_mul_ps(xs_vec, indices_vec);\n        _mm256_storeu_ps(&result[i], mul_vec);\n        indices_vec = _mm256_add_ps(indices_vec, _mm256_set1_ps(8.0f));\n    }\n\n    // Handle remaining elements\n    size_t i = (n - 1) / 8 * 8;\n    for (; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1);\n    __m256 indices_vec = _mm256_set_ps(7.0f, 6.0f, 5.0f, 4.0f, 3.0f, 2.0f, 1.0f, 0.0f);\n    __m256 coeffs_vec;\n\n    for(size_t i = 0; i + 8 <= n - 1; i += 8){\n        coeffs_vec = _mm256_loadu_ps(&xs[i + 1]);\n        coeffs_vec = _mm256_mul_ps(coeffs_vec, indices_vec);\n        _mm256_storeu_ps(&result[i], coeffs_vec);\n        indices_vec = _mm256_add_ps(indices_vec, _mm256_set1_ps(8.0f));\n    }\n\n    for(size_t i = ((n - 1) / 8) * 8; i < n - 1; ++i){\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nint vowels_count_simd(const std::string& s) {\n    if (s.empty()) return 0;\n\n    const __m256i vowels = _mm256_set_epi8(\n        'a', 'e', 'i', 'o', 'u',\n        'A', 'E', 'I', 'O', 'U',\n        -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1\n    );\n\n    int count = 0;\n    size_t len = s.length();\n    const unsigned char* data = reinterpret_cast<const unsigned char*>(s.c_str());\n\n    for (size_t i = 0; i + 31 < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp = _mm256_cmpestrm(vowels, 10, vec, 32, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining characters\n    for (size_t i = (len / 32) * 32; i < len - 1; ++i) {\n        if (strchr(\"aeiouAEIOU\", s[i])) count++;\n    }\n\n    // Check last character for 'y' or 'Y'\n    if (strchr(\"yY\", s[len - 1])) count++;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s){\n    int count = 0;\n    const int len = s.length();\n    if (len == 0) return 0;\n\n    // Define vowel masks for 'a', 'e', 'i', 'o', 'u' (both lowercase and uppercase)\n    __m256i vowels_mask1 = _mm256_set_epi8('U','O','I','E','A','u','o','i','e','a',0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);\n    __m256i vowels_mask2 = _mm256_set_epi8('Y','y',0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0);\n\n    for(int i = 0; i <= len - 32; i += 32){\n        __m256i data1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i data2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i+16]));\n\n        __m256i cmp1 = _mm256_cmpestrm(vowels_mask1, 10, data1, 16, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_MASK | _SIDD_NEGATIVE_POLARITY);\n        __m256i cmp2 = _mm256_cmpestrm(vowels_mask1, 10, data2, 16, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_MASK | _SIDD_NEGATIVE_POLARITY);\n\n        __m256i cmp3 = _mm256_cmpestrm(vowels_mask2, 2, data1, 16, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_MASK | _SIDD_NEGATIVE_POLARITY);\n        __m256i cmp4 = _mm256_cmpestrm(vowels_mask2, 2, data2, 16, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_MASK | _SIDD_NEGATIVE_POLARITY);\n\n        __m256i res1 = _mm256_or_si256(cmp1, cmp3);\n        __m256i res2 = _mm256_or_si256(cmp2, cmp4);\n\n        __m256i ones = _mm256_set1_epi8(1);\n        __m256i sum1 = _mm256_sad_epu8(res1, _mm256_setzero_si256());\n        __m256i sum2 = _mm256_sad_epu8(res2, _mm256_setzero_si256());\n\n        int temp1 = _mm256_extract_epi16(sum1, 0) + _mm256_extract_epi16(sum1, 8);\n        int temp2 = _mm256_extract_epi16(sum2, 0) + _mm256_extract_epi16(sum2, 8);\n\n        count += (temp1 + temp2) / 255;\n    }\n\n    // Handle remaining characters\n    for(int i = len - (len % 32); i < len; ++i){\n        char c = std::tolower(s[i]);\n        if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' || (c == 'y' && i == len - 1)){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nint vowels_count_simd(std::string s){\n    if (s.empty()) return 0;\n\n    // Convert string to uppercase for easier comparison\n    std::transform(s.begin(), s.end(), s.begin(), ::toupper);\n\n    const __m256i vowels = _mm256_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'U', 'O', 'I', 'E', 'A', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n\n    int count = 0;\n    size_t length = s.length();\n    size_t aligned_length = length / 32 * 32;\n\n    for (size_t i = 0; i < aligned_length; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i cmp = _mm256_cmpestrm(vec, 32, vowels, 25, _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining characters\n    for (size_t i = aligned_length; i < length - 1; ++i) {\n        if (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U') {\n            count++;\n        }\n    }\n\n    // Check the last character for 'Y'\n    if (s[length - 1] == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nint vowels_count_simd(std::string s){\n    int count = 0;\n    if(s.empty()) return 0;\n\n    // Convert string to uppercase for uniformity\n    std::transform(s.begin(), s.end(), s.begin(), ::toupper);\n\n    const __m256i vowels = _mm256_set_epi8('U', 'O', 'I', 'E', 'A', \n                                           'U', 'O', 'I', 'E', 'A', \n                                           'U', 'O', 'I', 'E', 'A', \n                                           'U', 'O', 'I', 'E', 'A', \n                                           -1, -1, -1, -1, -1, -1, -1, -1);\n    const int length = s.length();\n    const int vector_length = 32;\n\n    int i = 0;\n    for (; i + vector_length <= length; i += vector_length) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i cmp = _mm256_cmpestrm(data, vector_length, vowels, vector_length, _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK | _SIDD_UWORD_OPS | _SIDD_NEGATIVE_POLARITY);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining characters\n    for (; i < length; ++i) {\n        char c = s[i];\n        if (c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U' || (c == 'Y' && i == length - 1)) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nint vowels_count_simd(std::string s){\n    if(s.empty()) return 0;\n\n    // Convert string to uppercase for easier comparison\n    std::transform(s.begin(), s.end(), s.begin(), ::toupper);\n\n    const __m256i vowels = _mm256_set_epi8(\n        'A', 'E', 'I', 'O', 'U', -1, -1, -1, \n        'A', 'E', 'I', 'O', 'U', -1, -1, -1,\n        'A', 'E', 'I', 'O', 'U', -1, -1, -1,\n        'A', 'E', 'I', 'O', 'U', -1, -1, -1\n    );\n\n    int count = 0;\n    size_t length = s.length();\n    const unsigned char* data = reinterpret_cast<const unsigned char*>(s.data());\n\n    for(size_t i = 0; i + 31 < length; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp = _mm256_cmpeq_epi8(vec, vowels);\n        __m256i mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining characters\n    for(size_t i = (length / 32) * 32; i < length - 1; ++i){\n        if(data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U'){\n            count++;\n        }\n    }\n\n    // Check last character for 'Y'\n    if(length > 0 && (data[length - 1] == 'Y')){\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    const int length = s.length();\n    if (length == 0) return 0;\n\n    alignas(32) unsigned char buffer[32];\n    __m256i sum = _mm256_setzero_si256();\n    __m256i upper_mask = _mm256_set1_epi8(0);\n\n    for(int i = 0; i < length; i += 32){\n        int chunk_size = (length - i > 32) ? 32 : length - i;\n        memset(buffer, 0, 32);\n        memcpy(buffer, s.c_str() + i, chunk_size);\n\n        __m256i data = _mm256_loadu_si256((__m256i*)buffer);\n        __m256i cmp_result = _mm256_cmpgt_epi8(_mm256_and_si256(data, _mm256_set1_epi8(0xDF)), _mm256_set1_epi8('Z'));\n        __m256i masked_data = _mm256_and_si256(data, cmp_result);\n\n        sum = _mm256_add_epi8(sum, masked_data);\n    }\n\n    __m256i vsum1 = _mm256_hadd_epi16(sum, sum);\n    __m256i vsum2 = _mm256_hadd_epi16(vsum1, vsum1);\n    __m128i vsum3 = _mm_add_epi32(_mm256_extracti128_si256(vsum2, 0), _mm256_extracti128_si256(vsum2, 1));\n    __m128i vsum4 = _mm_hadd_epi32(vsum3, vsum3);\n    __m128i vsum5 = _mm_hadd_epi32(vsum4, vsum4);\n\n    int result = 0;\n    _mm_storeu_si128((__m128i*)&result, vsum5);\n    return result;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i upper_mask = _mm256_set1_epi8(0x40); // 'A'-'Z' have bit 6 set\n    __m256i ascii_mask = _mm256_set1_epi8(0x7F);\n\n    for(size_t i = 0; i + 31 < len; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n        vec = _mm256_and_si256(vec, ascii_mask);\n        __m256i cmp = _mm256_cmpgt_epi8(vec, _mm256_sub_epi8(upper_mask, _mm256_set1_epi8(1)));\n        vec = _mm256_and_si256(vec, cmp);\n        vsum = _mm256_add_epi32(_mm256_sad_epu8(vec, _mm256_setzero_si256()), vsum);\n    }\n\n    int temp[8] = {0};\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for(size_t i = (len / 32) * 32; i < len; ++i){\n        if(str[i] >= 'A' && str[i] <= 'Z'){\n            sum += static_cast<unsigned char>(str[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i upper_mask = _mm256_set1_epi8(0x40); // 'A'-'Z' have bit 6 set\n    __m256i ascii_mask = _mm256_set1_epi8(0x7F);\n\n    for(size_t i = 0; i + 31 < len; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n        vec = _mm256_and_si256(vec, ascii_mask);\n        __m256i is_upper = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8('A'-1));\n        is_upper = _mm256_and_si256(is_upper, _mm256_cmpgt_epi8(_mm256_set1_epi8('Z'), vec));\n        vec = _mm256_and_si256(vec, is_upper);\n        vsum = _mm256_add_epi32(_mm256_sad_epu8(vec, _mm256_setzero_si256()), vsum);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    for(size_t i = (len / 32) * 32; i < len; ++i){\n        if(str[i] >= 'A' && str[i] <= 'Z'){\n            sum += static_cast<unsigned char>(str[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const int length = s.length();\n    const int vec_length = 32; // AVX2: 256 bits = 32 bytes\n    const int num_full_vecs = length / vec_length;\n\n    __m256i upper_bound = _mm256_set1_epi8('Z');\n    __m256i lower_bound = _mm256_set1_epi8('A');\n    __m256i ascii_val = _mm256_setzero_si256();\n\n    for(int i=0; i<num_full_vecs*vec_length; i+=vec_length){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i is_upper = _mm256_and_si256(_mm256_cmpgt_epi8(data, lower_bound), _mm256_cmple_epi8(data, upper_bound));\n        __m256i masked_data = _mm256_and_si256(data, is_upper);\n        ascii_val = _mm256_add_epi8(ascii_val, masked_data);\n    }\n\n    int temp[32] = {0};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), ascii_val);\n    for(int i=0;i<32;i++) sum += temp[i];\n\n    for(int i=num_full_vecs*vec_length; i<length; i++) {\n        if(s[i] >= 'A' && s[i] <= 'Z') sum += s[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const int length = s.length();\n    const int vectorLength = 32; // AVX2 operates on 256 bits, which is 32 bytes\n    const int numFullVectors = length / vectorLength;\n\n    alignas(32) unsigned char buffer[vectorLength];\n    __m256i ascii_A = _mm256_set1_epi8('A');\n    __m256i ascii_Z = _mm256_set1_epi8('Z');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i total = _mm256_setzero_si256();\n\n    for(int i = 0; i < numFullVectors * vectorLength; i += vectorLength){\n        memcpy(buffer, s.c_str() + i, vectorLength);\n        __m256i vec = _mm256_load_si256((__m256i*)buffer);\n        __m256i ge_A = _mm256_cmpgt_epi8(vec, ascii_A);\n        __m256i le_Z = _mm256_cmple_epi8(vec, ascii_Z);\n        __m256i is_upper = _mm256_and_si256(ge_A, le_Z);\n        __m256i masked_vec = _mm256_and_si256(vec, is_upper);\n        total = _mm256_add_epi8(total, masked_vec);\n    }\n\n    // Horizontal sum of the total vector\n    __m128i low = _mm256_castsi256_si128(total);\n    __m128i high = _mm256_extracti128_si256(total, 1);\n    __m128i sum128 = _mm_add_epi8(low, high);\n    unsigned char sums[16];\n    _mm_storeu_si128((__m128i*)sums, sum128);\n    for(int i = 0; i < 16; ++i){\n        sum += sums[i];\n    }\n\n    // Process remaining characters\n    for(int i = numFullVectors * vectorLength; i < length; ++i){\n        if(s[i] >= 'A' && s[i] <= 'Z'){\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    std::vector<int> result;\n    if (arr.empty()) return result;\n\n    const int vec_size = 8;\n    __m256i min_val = _mm256_set1_epi32(0x7FFFFFFF);\n    __m256i min_idx = _mm256_set1_epi32(-1);\n\n    int remainder = arr.size() % vec_size;\n    int aligned_size = arr.size() - remainder;\n\n    for (int i = 0; i < aligned_size; i += vec_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i even_mask = _mm256_andnot_si256(_mm256_srli_epi32(data, 31), _mm256_cmpeq_epi32(data, data));\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i is_even = _mm256_cmpeq_epi32(even_mask, data);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i current_min_val = _mm256_min_epi32(_mm256_blendv_epi8(min_val, data, is_even), min_val);\n        __m256i current_min_idx = _mm256_blendv_epi8(min_idx, indices, _mm256_and_si256(is_even, _mm256_cmpeq_epi32(current_min_val, data)));\n        min_val = current_min_val;\n        min_idx = current_min_idx;\n    }\n\n    int scalar_min_val = 0x7FFFFFFF;\n    int scalar_min_idx = -1;\n\n    alignas(32) int min_vals[8];\n    alignas(32) int min_indices[8];\n    _mm256_store_si256((__m256i*)min_vals, min_val);\n    _mm256_store_si256((__m256i*)min_indices, min_idx);\n\n    for (int i = 0; i < vec_size; ++i) {\n        if ((min_vals[i] & 1) == 0 && min_vals[i] < scalar_min_val) {\n            scalar_min_val = min_vals[i];\n            scalar_min_idx = min_indices[i];\n        }\n    }\n\n    for (int i = aligned_size; i < arr.size(); ++i) {\n        if ((arr[i] & 1) == 0 && arr[i] < scalar_min_val) {\n            scalar_min_val = arr[i];\n            scalar_min_idx = i;\n        }\n    }\n\n    if (scalar_min_idx != -1) {\n        result = {scalar_min_val, scalar_min_idx};\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    std::vector<int> result;\n    if (arr.empty()) return result;\n\n    const int vec_size = 8;\n    int n = arr.size();\n    int remainder = n % vec_size;\n    int aligned_n = n - remainder;\n\n    __m256i min_val = _mm256_set1_epi32(0x7FFFFFFF);\n    __m256i min_idx = _mm256_set1_epi32(-1);\n\n    for(int i = 0; i < aligned_n; i += vec_size){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i even_mask = _mm256_andnot_si256(_mm256_srli_epi32(data, 31), _mm256_cmpeq_epi32(data, _mm256_add_epi32(data, data)));\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i valid_data = _mm256_and_si256(data, even_mask);\n        \n        __m256i current_min_val = _mm256_min_epi32(valid_data, min_val);\n        __m256i val_cmp = _mm256_cmplt_epi32(current_min_val, min_val);\n        min_idx = _mm256_blendv_epi8(min_idx, indices, _mm256_and_si256(val_cmp, even_mask));\n        min_val = current_min_val;\n    }\n\n    int scalar_min_val = 0x7FFFFFFF;\n    int scalar_min_idx = -1;\n\n    int temp_vals[8];\n    int temp_idxs[8];\n    _mm256_storeu_si256((__m256i*)temp_vals, min_val);\n    _mm256_storeu_si256((__m256i*)temp_idxs, min_idx);\n\n    for(int i = 0; i < vec_size; ++i){\n        if(temp_vals[i] < scalar_min_val && temp_vals[i] != 0){\n            scalar_min_val = temp_vals[i];\n            scalar_min_idx = temp_idxs[i];\n        }\n    }\n\n    for(int i = aligned_n; i < n; ++i){\n        if( (arr[i] & 1) == 0 && arr[i] < scalar_min_val ){\n            scalar_min_val = arr[i];\n            scalar_min_idx = i;\n        }\n    }\n\n    if(scalar_min_idx != -1){\n        result.push_back(scalar_min_val);\n        result.push_back(scalar_min_idx);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    const int vec_size = 8;\n    size_t n = arr.size();\n    size_t aligned_size = n - (n % vec_size);\n\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i min_idx = _mm256_set1_epi32(-1);\n    __m256i curr_idx = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (size_t i = 0; i < aligned_size; i += vec_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i even_mask = _mm256_andnot_si256(_mm256_srli_epi32(data, 31), _mm256_cmpeq_epi32(_mm256_and_si256(data, _mm256_set1_epi32(1)), _mm256_setzero_si256()));\n        __m256i valid_data = _mm256_and_si256(data, even_mask);\n        __m256i valid_idx = _mm256_and_si256(_mm256_add_epi32(curr_idx, _mm256_set1_epi32(i)), even_mask);\n\n        __m256i cmp_mask = _mm256_cmplt_epi32(valid_data, min_val);\n        min_val = _mm256_min_epi32(min_val, valid_data);\n        min_idx = _mm256_blendv_epi8(min_idx, valid_idx, cmp_mask);\n\n        // Handle duplicate minimum values by index comparison\n        __m256i eq_mask = _mm256_cmpeq_epi32(valid_data, min_val);\n        __m256i smaller_idx = _mm256_cmplt_epi32(valid_idx, min_idx);\n        __m256i update_mask = _mm256_and_si256(eq_mask, smaller_idx);\n        min_idx = _mm256_blendv_epi8(min_idx, valid_idx, update_mask);\n    }\n\n    int smallest_val = std::numeric_limits<int>::max();\n    int smallest_idx = -1;\n\n    alignas(32) int temp_vals[8];\n    alignas(32) int temp_idxs[8];\n\n    _mm256_store_si256((__m256i*)temp_vals, min_val);\n    _mm256_store_si256((__m256i*)temp_idxs, min_idx);\n\n    for (int i = 0; i < vec_size; ++i) {\n        if (temp_vals[i] < smallest_val || (temp_vals[i] == smallest_val && temp_idxs[i] < smallest_idx)) {\n            smallest_val = temp_vals[i];\n            smallest_idx = temp_idxs[i];\n        }\n    }\n\n    // Process remaining elements\n    for (size_t i = aligned_size; i < n; ++i) {\n        if (arr[i] < smallest_val || (arr[i] == smallest_val && static_cast<int>(i) < smallest_idx)) {\n            if (arr[i] % 2 == 0) {\n                smallest_val = arr[i];\n                smallest_idx = static_cast<int>(i);\n            }\n        }\n    }\n\n    if (smallest_idx != -1) {\n        return {smallest_val, smallest_idx};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) return {};\n\n    const int vec_size = 8;\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i min_idx = _mm256_set1_epi32(-1);\n    __m256i current_idx = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i mask_even = _mm256_set1_epi32(0xAAAAAAAA);\n\n    size_t i = 0;\n    for (; i + vec_size <= arr.size(); i += vec_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i idx = _mm256_add_epi32(current_idx, _mm256_set1_epi32(i));\n        \n        // Check even numbers\n        __m256i is_even = _mm256_and_si256(data, mask_even);\n        __m256i cmp_res = _mm256_cmpeq_epi32(is_even, _mm256_setzero_si256());\n        __m256i valid_data = _mm256_and_si256(data, cmp_res);\n        __m256i valid_idx = _mm256_and_si256(idx, cmp_res);\n\n        // Compare and find minimum\n        __m256i curr_min_val = _mm256_min_epi32(valid_data, min_val);\n        __m256i curr_min_idx = _mm256_blendv_epi8(min_idx, valid_idx, _mm256_cmpeq_epi32(curr_min_val, valid_data));\n\n        min_val = curr_min_val;\n        min_idx = curr_min_idx;\n    }\n\n    // Horizontal min\n    int min_value = std::numeric_limits<int>::max();\n    int min_index = -1;\n    alignas(32) int min_vals[8];\n    alignas(32) int min_idxs[8];\n    _mm256_store_si256((__m256i*)min_vals, min_val);\n    _mm256_store_si256((__m256i*)min_idxs, min_idx);\n\n    for (int j = 0; j < vec_size; ++j) {\n        if (min_vals[j] < min_value && min_vals[j] != 0) {\n            min_value = min_vals[j];\n            min_index = min_idxs[j];\n        }\n    }\n\n    // Process remaining elements\n    for (; i < arr.size(); ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_value) {\n            min_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index == -1) return {};\n    return {min_value, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) return {};\n\n    const int vec_size = 8;\n    __m256i min_val = _mm256_set1_epi32(std::numeric_limits<int>::max());\n    __m256i min_idx = _mm256_set1_epi32(-1);\n    __m256i idx_vec = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n\n    int n = arr.size();\n    int i = 0;\n\n    for (; i <= n - vec_size; i += vec_size) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i even_mask = _mm256_and_si256(data, _mm256_set1_epi32(1));\n        even_mask = _mm256_xor_si256(even_mask, _mm256_set1_epi32(1));\n        even_mask = _mm256_cmpeq_epi32(even_mask, _mm256_setzero_si256());\n\n        __m256i current_idx = _mm256_add_epi32(idx_vec, _mm256_set1_epi32(i));\n        __m256i valid_data = _mm256_and_si256(data, even_mask);\n        __m256i valid_idx = _mm256_and_si256(current_idx, even_mask);\n\n        __m256i cmp_val = _mm256_min_epi32(valid_data, min_val);\n        __m256i val_changed_mask = _mm256_cmpeq_epi32(cmp_val, valid_data);\n        __m256i cmp_idx = _mm256_blendv_epi8(min_idx, valid_idx, _mm256_and_si256(val_changed_mask, even_mask));\n\n        min_val = cmp_val;\n        min_idx = cmp_idx;\n    }\n\n    int min_value = std::numeric_limits<int>::max();\n    int min_index = -1;\n\n    for (int j = 0; j < vec_size; ++j) {\n        int value = _mm256_extract_epi32(min_val, j);\n        if (value < min_value) {\n            min_value = value;\n            min_index = _mm256_extract_epi32(min_idx, j);\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (arr[i] % 2 == 0 && arr[i] < min_value) {\n            min_value = arr[i];\n            min_index = i;\n        }\n    }\n\n    if (min_index != -1) {\n        return {min_value, min_index};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 255) max_val = 255;\n\n    __m256i freq = _mm256_setzero_si256();\n    for(size_t i = 0; i < lst.size(); i += 8){\n        __m256i vec = _mm256_set_epi32(\n            (i+7 < lst.size()) ? lst[i+7] : 0,\n            (i+6 < lst.size()) ? lst[i+6] : 0,\n            (i+5 < lst.size()) ? lst[i+5] : 0,\n            (i+4 < lst.size()) ? lst[i+4] : 0,\n            (i+3 < lst.size()) ? lst[i+3] : 0,\n            (i+2 < lst.size()) ? lst[i+2] : 0,\n            (i+1 < lst.size()) ? lst[i+1] : 0,\n            (i+0 < lst.size()) ? lst[i+0] : 0\n        );\n        __m256i cmp = _mm256_cmpeq_epi32(vec, _mm256_set1_epi32(1));\n        freq = _mm256_add_epi32(freq, _mm256_and_si256(cmp, _mm256_set1_epi32(1)));\n        for(int j = 2; j <= max_val; ++j){\n            cmp = _mm256_cmpeq_epi32(vec, _mm256_set1_epi32(j));\n            freq = _mm256_add_epi32(freq, _mm256_and_si256(cmp, _mm256_set1_epi32(1)));\n        }\n    }\n\n    int result = -1;\n    for(int i = 1; i <= max_val; ++i){\n        int count = 0;\n        __m256i f = _mm256_srli_si256(freq, 4*(i-1));\n        f = _mm256_permutevar8x32_epi32(f, _mm256_set_epi32(0,0,0,0,0,0,0,0));\n        count += _mm256_extract_epi32(f, 0);\n        if(i <= count){\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_map>\n#include <climits>\n\nint search_simd(std::vector<int> lst){\n    std::unordered_map<int, int> freq;\n    for(auto num : lst) freq[num]++;\n    \n    int max_val = -1;\n    __m256i res = _mm256_set1_epi32(-1);\n    __m256i zero = _mm256_set1_epi32(0);\n    \n    for(auto &[key, count] : freq){\n        if(key > 0 && count >= key){\n            __m256i current = _mm256_set1_epi32(key);\n            __m256i cmp = _mm256_cmpgt_epi32(current, res);\n            res = _mm256_blendv_epi8(res, current, cmp);\n        }\n    }\n    \n    alignas(32) int out[8];\n    _mm256_store_si256((__m256i*)out, res);\n    int result = -1;\n    for(int i = 0; i < 8; ++i){\n        if(out[i] > result) result = out[i];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 255) max_val = 255;\n\n    __m256i counts = _mm256_setzero_si256();\n    for(size_t i = 0; i < lst.size(); i += 8){\n        __m256i data = _mm256_set_epi32(\n            (i+7 < lst.size()) ? lst[i+7] : 0,\n            (i+6 < lst.size()) ? lst[i+6] : 0,\n            (i+5 < lst.size()) ? lst[i+5] : 0,\n            (i+4 < lst.size()) ? lst[i+4] : 0,\n            (i+3 < lst.size()) ? lst[i+3] : 0,\n            (i+2 < lst.size()) ? lst[i+2] : 0,\n            (i+1 < lst.size()) ? lst[i+1] : 0,\n            (i+0 < lst.size()) ? lst[i+0] : 0\n        );\n        __m256i mask = _mm256_cmpeq_epi32(data, _mm256_set1_epi32(1));\n        counts = _mm256_add_epi32(counts, _mm256_and_si256(mask, _mm256_set1_epi32(1)));\n    }\n\n    int result = -1;\n    int count_arr[8];\n    _mm256_storeu_si256((__m256i*)count_arr, counts);\n    for(int i = 1; i <= max_val; ++i){\n        int total = 0;\n        for(int j = 0; j < 8; ++j){\n            if(i == 1) total += count_arr[j];\n        }\n        if(total >= i && i > result){\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    std::unordered_map<int, int> freq;\n    const int vec_size = 8;\n    size_t n = lst.size();\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i + vec_size <= n; i += vec_size){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i cmp = _mm256_cmpeq_epi32(vec, vec);\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(cmp));\n        if(mask != 0){\n            for(int j = 0; j < vec_size; ++j){\n                if(mask & (1 << j)){\n                    freq[lst[i + j]]++;\n                }\n            }\n        }\n    }\n\n    for(size_t i = ((n / vec_size) * vec_size); i < n; ++i){\n        freq[lst[i]]++;\n    }\n\n    int result = -1;\n    for(const auto& [key, value] : freq){\n        if(value >= key && key > result){\n            result = key;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <unordered_map>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    std::unordered_map<int, int> freq;\n    const int *data = lst.data();\n    size_t size = lst.size();\n    \n    // Count frequencies using AVX2\n    __m256i zero = _mm256_set1_epi32(0);\n    for(size_t i = 0; i + 7 < size; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        for(int j = 0; j < 8; ++j){\n            int val = _mm256_extract_epi32(vec, j);\n            freq[val]++;\n        }\n    }\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        freq[data[i]]++;\n    }\n\n    int result = -1;\n    for(const auto &pair : freq){\n        if(pair.second >= pair.first && pair.first > 0){\n            result = std::max(result, pair.first);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    bool is_palindrome = true;\n\n    // Check for palindrome using AVX2\n    __m256i zero = _mm256_setzero_si256();\n    size_t i = 0;\n    for (; i + 7 < n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i const*)(&q[i]));\n        __m256i right = _mm256_loadu_si256((__m256i const*)(&q[n - i - 8]));\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Check remaining elements\n    for (; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if (!is_palindrome) return false;\n\n    // Calculate sum using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(&q[i]));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal sum\n    alignas(32) int sum_arr[8] = {0};\n    _mm256_store_si256((__m256i*)sum_arr, sum_vec);\n    int total_sum = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum += sum_arr[j];\n    }\n\n    // Add remaining elements\n    for (; i < n; ++i) {\n        total_sum += q[i];\n    }\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w){\n    size_t n = q.size();\n    if(n == 0) return false;\n\n    // Check if the vector is a palindrome\n    bool is_palindrome = true;\n    __m256i zero = _mm256_setzero_si256();\n    for(size_t i = 0; i < (n / 2) / 8; ++i){\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i * 8]));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[n - (i+1) * 8]));\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        if(_mm256_movemask_epi8(cmp) != 0xFFFF){\n            is_palindrome = false;\n            break;\n        }\n    }\n    for(size_t i = (n / 2) / 8 * 8; i < n / 2; ++i){\n        if(q[i] != q[n - i - 1]){\n            is_palindrome = false;\n            break;\n        }\n    }\n    if(!is_palindrome) return false;\n\n    // Calculate the sum of elements\n    __m256i sum_vec = _mm256_setzero_si256();\n    for(size_t i = 0; i < n / 8; ++i){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i * 8]));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n    int sum_array[8] = {0};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum_array), sum_vec);\n    int sum = 0;\n    for(int i = 0; i < 8; ++i){\n        sum += sum_array[i];\n    }\n    for(size_t i = n / 8 * 8; i < n; ++i){\n        sum += q[i];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w){\n    size_t n = q.size();\n    bool is_palindrome = true;\n\n    // Check for palindrome using AVX2\n    __m256i zero = _mm256_setzero_si256();\n    size_t i = 0;\n    for (; i + 7 < n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[n - i - 8]));\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Scalar check for remaining elements\n    for (; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if (!is_palindrome) return false;\n\n    // Calculate sum using AVX2\n    __m256i vsum = _mm256_setzero_si256();\n    i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        vsum = _mm256_add_epi32(vsum, vec);\n    }\n\n    // Horizontal sum of vsum\n    __m128i sum_low = _mm256_castsi256_si128(vsum);\n    __m128i sum_high = _mm256_extracti128_si256(vsum, 1);\n    sum_low = _mm_add_epi32(sum_low, sum_high);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    int total_sum = _mm_cvtsi128_si32(sum_low);\n\n    // Add remaining scalar elements\n    for (; i < n; ++i) {\n        total_sum += q[i];\n    }\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w){\n    size_t n = q.size();\n    if(n == 0) return false;\n\n    // Check if the vector is a palindrome\n    bool is_palindrome = true;\n    __m256i zero = _mm256_set1_epi32(0);\n    for(size_t i = 0; i < (n / 2) / 8; ++i){\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i * 8]));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[n - (i + 1) * 8]));\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        if(_mm256_movemask_epi8(cmp) != 0xFFFF){\n            is_palindrome = false;\n            break;\n        }\n    }\n    for(size_t i = (n / 2) / 8 * 8; i < n / 2; ++i){\n        if(q[i] != q[n - i - 1]){\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if(!is_palindrome) return false;\n\n    // Calculate the sum of elements using AVX2\n    __m256i vsum = _mm256_setzero_si256();\n    for(size_t i = 0; i < n / 8; ++i){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i * 8]));\n        vsum = _mm256_add_epi32(vsum, vec);\n    }\n    int sum_array[8] = {0};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum_array), vsum);\n    int sum = 0;\n    for(int i = 0; i < 8; ++i){\n        sum += sum_array[i];\n    }\n    for(size_t i = n / 8 * 8; i < n; ++i){\n        sum += q[i];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w){\n    size_t n = q.size();\n    bool is_palindrome = true;\n\n    // Check for palindrome using AVX2\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i mask = _mm256_set_epi32(-1, -1, -1, -1, -1, -1, -1, -1);\n    size_t i = 0;\n    for (; i + 7 < n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[n - i - 8]));\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        if (_mm256_movemask_epi8(cmp) != 0xFFFF) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    // Check remaining elements\n    for (; i < n / 2; ++i) {\n        if (q[i] != q[n - i - 1]) {\n            is_palindrome = false;\n            break;\n        }\n    }\n\n    if (!is_palindrome) return false;\n\n    // Calculate sum using AVX2\n    __m256i sum_vec = _mm256_setzero_si256();\n    i = 0;\n    for (; i + 7 < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    // Horizontal add\n    __m128i sum_low = _mm_add_epi32(_mm256_extracti128_si256(sum_vec, 0), _mm256_extracti128_si256(sum_vec, 1));\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    int sum = _mm_cvtsi128_si32(sum_low);\n\n    // Add remaining elements\n    for (; i < n; ++i) {\n        sum += q[i];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    const int simd_width = 8; // AVX2 operates on 256 bits, so we can compare 8 integers at once\n\n    for (; i + simd_width <= n / 2; i += simd_width) {\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[n - i - simd_width]));\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        __m256i mask = _mm256_xor_si256(_mm256_set1_epi32(-1), cmp);\n\n        int bitmask = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n        changes += __builtin_popcount(bitmask);\n    }\n\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    const int *arr_data = arr.data();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for (; i + 7 < n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)(arr_data + i));\n        __m256i right = _mm256_loadu_si256((__m256i*)(arr_data + n - i - 1));\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        __m256i mask = _mm256_xor_si256(cmp, _mm256_set1_epi32(-1));\n        __m256i diff = _mm256_and_si256(mask, ones);\n\n        int temp_changes = _mm256 horizontal_add(diff); // Horizontal add implementation needed\n        changes += temp_changes;\n    }\n\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n``` \n\n// Note: The `_mm256_horizontal_add` function needs to be implemented separately for summing the elements of a `__m256i` vector."}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    const int* data = arr.data();\n\n    for (; i + 8 <= n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&data[n - i - 8]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        __m256i mask = _mm256_set1_epi32(-1);\n        __m256i diff = _mm256_xor_si256(cmp, mask);\n\n        int bits = _mm256_movemask_ps(_mm256_castsi256_ps(diff));\n        changes += __builtin_popcount(bits);\n    }\n\n    for (; i < n / 2; ++i) {\n        if (data[i] != data[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    // Process in chunks of 8 using AVX2\n    for (; i + 8 <= n / 2; i += 8) {\n        __m256i left = _mm256_loadu_si256((__m256i const*)(&arr[i]));\n        __m256i right = _mm256_loadu_si256((__m256i const*)(&arr[n - i - 8]));\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        __m256i mask = _mm256_xor_si256(cmp, _mm256_set1_epi32(-1));\n\n        int bitmask = _mm256_movemask_ps(_mm256_castsi256_ps(mask));\n        changes += __builtin_popcount(bitmask);\n    }\n\n    // Handle remaining elements\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n    int j = n - 1;\n\n    const int simd_width = 8;\n    __m256i vmask = _mm256_set1_epi32(1);\n\n    while(j - i + 1 >= simd_width){\n        __m256i vleft = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i vright = _mm256_loadu_si256((__m256i*)&arr[j]);\n\n        __m256i veq = _mm256_cmpeq_epi32(vleft, vright);\n        __m256i vneq = _mm256_xor_si256(vmask, veq);\n\n        int mask = _mm256_movemask_ps(_mm256_castsi256_ps(vneq));\n        changes += __builtin_popcount(mask);\n\n        i += simd_width;\n        j -= simd_width;\n    }\n\n    while(i <= j){\n        if(arr[i] != arr[j]){\n            changes++;\n        }\n        i++;\n        j--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    auto count_chars = [](const std::vector<std::string> &lst) -> size_t {\n        size_t total = 0;\n        __m256i vsum = _mm256_setzero_si256();\n        __m256i vones = _mm256_set1_epi32(1);\n\n        for(const auto& str : lst){\n            const char* data = str.data();\n            size_t len = str.size();\n            size_t i = 0;\n\n            // Process 8 characters at a time using AVX2\n            for(; i + 31 < len; i += 32){\n                __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n                __m256i mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), vec);\n                __m256i cnt = _mm256_and_si256(mask, vones);\n                vsum = _mm256_add_epi32(vsum, _mm256_sad_epu8(cnt, _mm256_setzero_si256()));\n            }\n\n            // Process remaining characters\n            for(; i < len; ++i){\n                if(data[i] != '\\0') total++;\n            }\n        }\n\n        // Horizontal add of vsum\n        int sum_arr[8];\n        _mm256_storeu_si256((__m256i*)sum_arr, vsum);\n        for(int i = 0; i < 8; ++i){\n            total += sum_arr[i];\n        }\n\n        return total;\n    };\n\n    size_t total1 = count_chars(lst1);\n    size_t total2 = count_chars(lst2);\n\n    if(total1 <= total2){\n        return lst1;\n    }\n    else{\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    auto count_chars = [](const std::vector<std::string> &lst) -> size_t {\n        size_t total = 0;\n        for (const auto& str : lst) {\n            size_t len = str.length();\n            if (len >= 32) {\n                size_t i = 0;\n                __m256i vsum = _mm256_setzero_si256();\n                for (; i + 31 < len; i += 32) {\n                    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.data() + i));\n                    __m256i mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), vec);\n                    vsum = _mm256_add_epi8(vsum, mask);\n                }\n                alignas(32) unsigned char temp[32];\n                _mm256_store_si256((__m256i*)temp, vsum);\n                for (size_t j = 0; j < 32; ++j) {\n                    total += temp[j] ? 1 : 0;\n                }\n            }\n            for (; i < len; ++i) {\n                total += str[i] != '\\0';\n            }\n        }\n        return total;\n    };\n\n    size_t sum1 = count_chars(lst1);\n    size_t sum2 = count_chars(lst2);\n\n    return sum1 <= sum2 ? lst1 : lst2;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    uint64_t sum1 = 0, sum2 = 0;\n\n    if(lst1.empty() && lst2.empty()){\n        return {};\n    }\n\n    // Calculate sum of lengths for lst1 using AVX2\n    size_t len1 = lst1.size();\n    size_t aligned_len1 = len1 - (len1 % 8);\n    __m256i vsum1 = _mm256_setzero_si256();\n    for(size_t i = 0; i < aligned_len1; i += 8){\n        __m256i vlengths = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i].length()));\n        vsum1 = _mm256_add_epi64(vsum1, vlengths);\n    }\n    uint64_t temp_sum1[4] = {0};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_sum1), vsum1);\n    sum1 = temp_sum1[0] + temp_sum1[1] + temp_sum1[2] + temp_sum1[3];\n    for(size_t i = aligned_len1; i < len1; ++i){\n        sum1 += lst1[i].length();\n    }\n\n    // Calculate sum of lengths for lst2 using AVX2\n    size_t len2 = lst2.size();\n    size_t aligned_len2 = len2 - (len2 % 8);\n    __m256i vsum2 = _mm256_setzero_si256();\n    for(size_t i = 0; i < aligned_len2; i += 8){\n        __m256i vlengths = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i].length()));\n        vsum2 = _mm256_add_epi64(vsum2, vlengths);\n    }\n    uint64_t temp_sum2[4] = {0};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_sum2), vsum2);\n    sum2 = temp_sum2[0] + temp_sum2[1] + temp_sum2[2] + temp_sum2[3];\n    for(size_t i = aligned_len2; i < len2; ++i){\n        sum2 += lst2[i].length();\n    }\n\n    if(sum1 <= sum2){\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    auto count_chars = [](const std::vector<std::string> &lst) -> size_t {\n        size_t total = 0;\n        for(const auto& str : lst){\n            size_t len = str.length();\n            if(len >= 32){\n                size_t i = 0;\n                for(; i + 31 < len; i += 32){\n                    __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n                    __m256i zeros = _mm256_setzero_si256();\n                    __m256i cmp = _mm256_cmpeq_epi8(v, zeros);\n                    int mask = _mm256_movemask_epi8(cmp);\n                    total += 32 - __builtin_popcount(mask);\n                }\n                total += str.substr(i).size();\n            }\n            else{\n                total += len;\n            }\n        }\n        return total;\n    };\n\n    size_t total1 = count_chars(lst1);\n    size_t total2 = count_chars(lst2);\n\n    if(total1 <= total2){\n        return lst1;\n    }\n    else{\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    auto sum_chars = [](const std::vector<std::string> &lst) -> size_t {\n        size_t total = 0;\n        for (const auto& str : lst) {\n            size_t len = str.length();\n            if (len > 31) {\n                total += len;\n                continue;\n            }\n            __m256i v_mask = _mm256_set1_epi8(-1);\n            for(size_t i = 0; i < len; i+=32){\n                __m256i v_data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str.c_str() + i));\n                __m256i v_cmp = _mm256_cmpeq_epi8(v_data, v_mask);\n                int cmp_res = _mm256_movemask_epi8(v_cmp);\n                total += __builtin_popcount(~cmp_res);\n            }\n        }\n        return total;\n    };\n\n    size_t sum1 = sum_chars(lst1);\n    size_t sum2 = sum_chars(lst2);\n\n    if(sum1 <= sum2){\n        return lst1;\n    }\n    return lst2;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n\n    __m256i current = _mm256_set1_epi64x(1);\n    __m256i limit = _mm256_set1_epi64x(x + 1);\n\n    bool result = false;\n\n    for(int i = 0; i < 64 && !result; ++i){\n        __m256i next = _mm256_mul_epu32(current, vn);\n        __m256i cmp = _mm256_cmpeq_epi64(next, vx);\n        if(_mm256_movemask_epi8(cmp) != 0){\n            result = true;\n        }\n        current = next;\n        if(_mm256_cmpgt_epi64(next, limit)) break;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n\n    __m256i one = _mm256_set1_epi64x(1);\n    __m256i current = one;\n\n    for(int i = 0; i < 64; ++i){\n        __m256i cmp = _mm256_cmpgt_epi64(current, vx);\n        int mask = _mm256_movemask_epi8(cmp);\n        if(mask != 0){\n            break;\n        }\n        __m256i equal = _mm256_cmpeq_epi64(current, vx);\n        mask = _mm256_movemask_epi8(equal);\n        if(mask != 0){\n            return true;\n        }\n        current = _mm256_mullo_epi64(current, vn);\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n\n    uint64_t result = 1;\n    uint64_t power = n;\n\n    while (power <= x){\n        __m256i vpower = _mm256_set1_epi64x(power);\n        __m256i veq = _mm256_cmpeq_epi64(vpower, vx);\n        if (_mm256_movemask_epi8(veq) != 0) return true;\n        if (power > UINT64_MAX / n) break;\n        power *= n;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n\n    uint64_t result = 1;\n    uint64_t power = n;\n\n    while (power <= x){\n        __m256i vpower = _mm256_set1_epi64x(power);\n        __m256i veq = _mm256_cmpeq_epi64(vpower, vx);\n        if (_mm256_testz_si256(_mm256_xor_si256(veq, _mm256_set1_epi64x(-1)), _mm256_set1_epi64x(-1))){\n            return true;\n        }\n        if (power > UINT64_MAX / n) break;\n        power *= n;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m256i vx = _mm256_set1_epi64x(x);\n    __m256i vn = _mm256_set1_epi64x(n);\n\n    uint64_t power = n;\n    while (power <= x){\n        __m256i vpower = _mm256_set1_epi64x(power);\n        __m256i veq = _mm256_cmpeq_epi64(vpower, vx);\n        if (_mm256_movemask_epi8(veq) != 0) return true;\n        if (power > x / n) break;\n        power *= n;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m256i prime_mask = _mm256_set1_epi8(0x92); // Binary: 10010010 -> primes at positions 2,3,5,7,B,D\n    const int len = num.length();\n    int count = 0;\n\n    for(int i = 0; i <= len - 32; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n        __m256i cmp = _mm256_cmpeq_epi8(vec, prime_mask);\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i res = _mm256_and_si256(cmp, vec);\n        __m256i packed = _mm256_packus_epi16(res, res);\n        __m128i masked = _mm256_extracti128_si256(packed, 0);\n        int temp_count = _mm_popcnt_u32(_mm_movemask_epi8(masked));\n        count += temp_count;\n    }\n\n    for(int i = len - (len % 32); i < len; ++i){\n        unsigned char c = num[i];\n        if(c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D'){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const uint8_t prime_mask[16] = {0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0};\n    alignas(32) uint8_t lookup_table[256] = {0};\n    for(size_t i = 0; i < 16; ++i){\n        lookup_table['0' + i] = prime_mask[i];\n        if(i < 6){\n            lookup_table['A' + i] = prime_mask[10 + i];\n        }\n    }\n\n    int count = 0;\n    size_t len = num.length();\n    const uint8_t *data = reinterpret_cast<const uint8_t*>(num.data());\n\n    __m256i vcount = _mm256_setzero_si256();\n    __m256i vlookup = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lookup_table));\n\n    for(size_t i = 0; i + 31 < len; i += 32){\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i vresult = _mm256_shuffle_epi8(vlookup, vdata);\n        vcount = _mm256_add_epi8(vcount, vresult);\n    }\n\n    uint8_t temp[32];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vcount);\n    for(int i = 0; i < 32; ++i){\n        count += temp[i];\n    }\n\n    for(size_t i = (len / 32) * 32; i < len; ++i){\n        count += lookup_table[data[i]];\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m256i primes_mask = _mm256_set1_epi8(\n        0b00000000 | ('2' & 0b01111111) |\n        0b00000000 | ('3' & 0b01111111) |\n        0b00000000 | ('5' & 0b01111111) |\n        0b00000000 | ('7' & 0b01111111) |\n        0b00000000 | ('B' & 0b01111111) |\n        0b00000000 | ('D' & 0b01111111)\n    );\n\n    int count = 0;\n    size_t len = num.length();\n    const unsigned char *data = reinterpret_cast<const unsigned char*>(num.c_str());\n\n    for (size_t i = 0; i + 31 < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp = _mm256_cmpeq_epi8(vec, primes_mask);\n        __m256i mask = _mm256_and_si256(cmp, primes_mask);\n        __m256i ones = _mm256_set1_epi8(0x01);\n        __m256i res = _mm256_cmpgt_epi8(mask, _mm256_setzero_si256());\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n    }\n\n    for (size_t i = (len / 32) * 32; i < len; ++i) {\n        unsigned char c = data[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m256i primes_mask = _mm256_set_epi8(\n        0,1,1,0, 0,0,0,1, 0,1,1,0, 1,0,0,0,\n        0,1,1,0, 0,0,0,1, 0,1,1,0, 1,0,0,0\n    );\n\n    int count = 0;\n    size_t len = num.length();\n    const unsigned char *data = reinterpret_cast<const unsigned char *>(num.c_str());\n\n    for (size_t i = 0; i + 31 < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp = _mm256_cmpeq_epi8(vec, primes_mask);\n        __m256i ones = _mm256_and_si256(cmp, _mm256_set1_epi8(0x1));\n        __m256i sum = _mm256_sad_epu8(ones, _mm256_setzero_si256());\n        count += _mm256_extract_epi16(sum, 0) + _mm256_extract_epi16(sum, 8);\n    }\n\n    for (size_t i = ((len / 32) * 32); i < len; ++i) {\n        unsigned char c = data[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    alignas(32) const char prime_mask[16] = {\n        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0\n    };\n\n    int count = 0;\n    size_t len = num.length();\n    const char *data = num.c_str();\n\n    __m256i mask = _mm256_load_si256((__m256i*)prime_mask);\n\n    for (size_t i = 0; i + 31 < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('0'));\n        __m256i res = _mm256_and_si256(cmp, mask);\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n\n        cmp = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('A'));\n        res = _mm256_and_si256(cmp, _mm256_srli_epi64(mask, 10));\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n\n        cmp = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('B'));\n        res = _mm256_and_si256(cmp, _mm256_srli_epi64(mask, 11));\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n\n        cmp = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('C'));\n        res = _mm256_and_si256(cmp, _mm256_srli_epi64(mask, 12));\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n\n        cmp = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('D'));\n        res = _mm256_and_si256(cmp, _mm256_srli_epi64(mask, 13));\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n\n        cmp = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('E'));\n        res = _mm256_and_si256(cmp, _mm256_srli_epi64(mask, 14));\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n\n        cmp = _mm256_cmpeq_epi8(vec, _mm256_set1_epi8('F'));\n        res = _mm256_and_si256(cmp, _mm256_srli_epi64(mask, 15));\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(res));\n    }\n\n    for (size_t i = len - len % 32; i < len; ++i) {\n        char c = data[i];\n        if (c >= '0' && c <= '9') {\n            if (prime_mask[c - '0']) count++;\n        } else if (c >= 'A' && c <= 'F') {\n            if (prime_mask[10 + c - 'A']) count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if(len < 3) return false;\n\n    const char *data = s.c_str();\n    __m256i vnot_equal_mask = _mm256_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n\n    for(size_t i = 0; i <= len - 16; i += 16){\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n        __m256i v3 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 2));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(v1, v2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(v2, v3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(v1, v3);\n\n        __m256i or1 = _mm256_or_si256(cmp1, cmp2);\n        __m256i or2 = _mm256_or_si256(or1, cmp3);\n\n        __m256i and_result = _mm256_andnot_si256(vnot_equal_mask, or2);\n\n        if(_mm256_movemask_epi8(and_result) != 0){\n            return false;\n        }\n    }\n\n    for(size_t i = len - (len % 16); i <= len - 3; ++i){\n        if(data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    const char *data = s.c_str();\n    __m256i v0, v1, v2;\n    __m256i cmp1, cmp2;\n\n    for (size_t i = 0; i <= len - 3; i += 8) {\n        v0 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n        v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 2));\n\n        cmp1 = _mm256_cmpeq_epi8(v0, v1);\n        cmp2 = _mm256_cmpeq_epi8(v1, v2);\n\n        // Check if any of the comparisons are true\n        if (_mm256_movemask_epi8(cmp1) || _mm256_movemask_epi8(cmp2)) {\n            return false;\n        }\n    }\n\n    // Handle remaining elements that couldn't fit into the SIMD registers\n    for (size_t i = ((len - 3) / 8) * 8; i <= len - 3; ++i) {\n        if (data[i] == data[i + 1] || data[i + 1] == data[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    const char *data = s.c_str();\n    size_t vec_iters = len - 2;\n\n    for (size_t i = 0; i <= vec_iters - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n        __m256i c = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 2));\n\n        __m256i cmp_ab = _mm256_cmpeq_epi8(a, b);\n        __m256i cmp_bc = _mm256_cmpeq_epi8(b, c);\n        __m256i cmp_ac = _mm256_cmpeq_epi8(a, c);\n\n        __m256i mask = _mm256_or_si256(cmp_ab, _mm256_or_si256(cmp_bc, cmp_ac));\n        if (_mm256_movemask_epi8(mask)) return false;\n    }\n\n    for (size_t i = vec_iters - 32; i <= len - 3; ++i) {\n        if (data[i] == data[i + 1] || data[i] == data[i + 2] || data[i + 1] == data[i + 2]) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    const char *data = s.c_str();\n    size_t vec_iters = len - 2;\n\n    for (size_t i = 0; i <= vec_iters - 32; i += 32) {\n        __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n        __m256i c = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 2));\n\n        __m256i eq_ab = _mm256_cmpeq_epi8(a, b);\n        __m256i eq_bc = _mm256_cmpeq_epi8(b, c);\n        __m256i eq_ac = _mm256_cmpeq_epi8(a, c);\n\n        __m256i any_eq = _mm256_or_si256(_mm256_or_si256(eq_ab, eq_bc), eq_ac);\n\n        if (_mm256_movemask_epi8(any_eq) != 0) return false;\n    }\n\n    for (size_t i = vec_iters - 32 + 1; i <= vec_iters; ++i) {\n        if (data[i] == data[i + 1] || data[i + 1] == data[i + 2] || data[i] == data[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if(len < 3) return false;\n\n    const char *data = s.c_str();\n\n    for(size_t i = 0; i <= len - 3; i+=4){\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 1));\n        __m256i v3 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i + 2));\n\n        __m256i cmp1 = _mm256_cmpeq_epi8(v1, v2);\n        __m256i cmp2 = _mm256_cmpeq_epi8(v2, v3);\n        __m256i cmp3 = _mm256_cmpeq_epi8(v1, v3);\n\n        __m256i mask1 = _mm256_or_si256(cmp1, cmp2);\n        __m256i mask = _mm256_or_si256(mask1, cmp3);\n\n        if(_mm256_testz_si256(mask, mask) == 0){\n            return false;\n        }\n    }\n\n    // Check remaining elements\n    for(size_t i = (len / 4) * 4; i <= len - 3; ++i){\n        if(data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if (length <= 1) return false;\n    if (length == 2 || length == 3) return true;\n    if ((length & 1) == 0) return false;\n\n    const int limit = static_cast<int>(std::sqrt(length));\n    __m256i vec_length = _mm256_set1_epi32(length);\n    __m256i zero = _mm256_setzero_si256();\n\n    for(int i = 5; i <= limit; i += 8){\n        __m256i vec_i = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i rem = _mm256_rem_epu32(vec_length, vec_i);\n        __m256i mask = _mm256_cmpeq_epi32(rem, zero);\n        if(_mm256_movemask_epi8(mask) != 0){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n & 1) == 0 || n % 3 == 0) return false;\n    int sqrt_n = static_cast<int>(std::sqrt(n));\n    for (int i = 5; i <= sqrt_n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string &str) {\n    const int len = static_cast<int>(str.length());\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n    __m256i v_mods = _mm256_set_epi32(6, 6, 6, 6, 6, 6, 6, 6);\n    __m256i v_len = _mm256_set1_epi32(len);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_three = _mm256_set1_epi32(3);\n\n    if (_mm256_testc_si256(_mm256_and_si256(_mm256_xor_si256(v_len, v_two), v_two), v_two) ||\n        _mm256_testc_si256(_mm256_and_si256(_mm256_xor_si256(v_len, v_three), v_three), v_three)) {\n        return false;\n    }\n\n    int sqrt_n = static_cast<int>(std::sqrt(len));\n    for (int i = 5; i <= sqrt_n; i += 6) {\n        __m256i v_i = _mm256_set1_epi32(i);\n        __m256i v_i2 = _mm256_set1_epi32(i + 2);\n        if (!_mm256_testz_si256(_mm256_and_si256(_mm256_cmpeq_epi32(_mm256_rem_epu32(v_len, v_i), _mm256_setzero_si256()), _mm256_set1_epi32(-1)) ||\n            !_mm256_testz_si256(_mm256_and_si256(_mm256_cmpeq_epi32(_mm256_rem_epu32(v_len, v_i2), _mm256_setzero_si256()), _mm256_set1_epi32(-1)))) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n & 1) == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str){\n    int len = static_cast<int>(str.length());\n    __m256i vlen = _mm256_set1_epi32(len);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i three = _mm256_set1_epi32(3);\n    __m256i five = _mm256_set1_epi32(5);\n    __m256i six = _mm256_set1_epi32(6);\n\n    if (len <= 1) return false;\n    if (len <= 3) return true;\n\n    __m256i mod_two = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_rem_epu32(vlen, two), _mm256_setzero_si256()), _mm256_set1_epi32(-1));\n    __m256i mod_three = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_rem_epu32(vlen, three), _mm256_setzero_si256()), _mm256_set1_epi32(-1));\n    if (_mm256_movemask_epi8(_mm256_or_si256(mod_two, mod_three)) != 0) return false;\n\n    for (int i = 5; i * i <= len; i += 6) {\n        __m256i vi = _mm256_set1_epi32(i);\n        __m256i vi2 = _mm256_set1_epi32(i + 2);\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(_mm256_rem_epu32(vlen, vi), _mm256_setzero_si256())) != 0 ||\n            _mm256_movemask_epi8(_mm256_cmpeq_epi32(_mm256_rem_epu32(vlen, vi2), _mm256_setzero_si256())) != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n    for (int i = 5; i * i <= n; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str){\n    int length = static_cast<int>(str.length());\n    __m256i vec_length = _mm256_set1_epi32(length);\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_three = _mm256_set1_epi32(3);\n    __m256i vec_six = _mm256_set1_epi32(6);\n\n    if (length <= 1) return false;\n    if (length == 2 || length == 3) return true;\n    if (length % 2 == 0 || length % 3 == 0) return false;\n\n    int sqrt_n = static_cast<int>(std::sqrt(length));\n    for (int i = 5; i <= sqrt_n; i += 6) {\n        __m256i vec_i = _mm256_set1_epi32(i);\n        __m256i vec_i_plus_2 = _mm256_add_epi32(vec_i, vec_two);\n\n        __m256i mod_i = _mm256_rem_epi32(vec_length, vec_i);\n        __m256i mod_i_plus_2 = _mm256_rem_epi32(vec_length, vec_i_plus_2);\n\n        if (_mm256_testz_si256(mod_i, mod_i) || _mm256_testz_si256(mod_i_plus_2, mod_i_plus_2)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n & 1) == 0 || n % 3 == 0) return false;\n\n    __m256i divisors = _mm256_set_epi32(11, 7, 5, 3, 13, 17, 19, 23);\n    __m256i n_vec = _mm256_set1_epi32(n);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (int i = 0; i < 8; ++i) {\n        __m256i remainder = _mm256_rem_epu32(n_vec, _mm256_shuffle_epi32(divisors, _MM_SHUFFLE(i,i,i,i)));\n        if (_mm256_testc_si256(_mm256_cmpeq_epi32(remainder, zero), _mm256_set1_epi32(-1))) {\n            return false;\n        }\n    }\n\n    for (int i = 29; i * i <= n; i += 2) {\n        if (n % i == 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string & str){\n    int length = static_cast<int>(str.length());\n    return is_prime(length);\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N){\n    uint32_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i constant_10 = _mm256_set1_epi32(10);\n\n    while(N > 0){\n        __m256i digit = _mm256_set1_epi32(N % 10);\n        vsum = _mm256_add_epi32(vsum, digit);\n        N /= 10;\n    }\n\n    // Horizontal sum\n    __m128i low = _mm256_castsi256_si128(vsum);\n    __m128i high = _mm256_extractf128_si256(vsum, 1);\n    low = _mm_add_epi32(low, high);\n    low = _mm_hadd_epi32(low, low);\n    low = _mm_hadd_epi32(low, low);\n    sum = _mm_cvtsi128_si32(low);\n\n    std::string binary_representation = std::bitset<32>(sum).to_string();\n    // Remove leading zeros\n    size_t first_one = binary_representation.find_first_of('1');\n    if(first_one != std::string::npos){\n        return binary_representation.substr(first_one);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string solve_simd(uint64_t N){\n    uint32_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vN = _mm256_set1_epi32(N);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i tens = _mm256_set1_epi32(10);\n\n    for(int i=0; i<10; ++i){\n        __m256i remainder = _mm256_cvtepi32_epi64(_mm256_and_si256(_mm256_sub_epi32(vN, ones), tens));\n        __m256i digit = _mm256_cvtepi64_epi32(_mm256_srli_epi64(remainder, 60));\n        vsum = _mm256_add_epi32(vsum, digit);\n        vN = _mm256_div_epi32(vN, tens);\n    }\n\n    uint32_t res[8];\n    _mm256_storeu_si256((__m256i*)res, vsum);\n    for(int i=0; i<8; ++i){\n        sum += res[i];\n    }\n\n    std::string binary = std::bitset<32>(sum).to_string();\n    // Remove leading zeros\n    size_t first_one = binary.find_first_of('1');\n    if(first_one != std::string::npos){\n        return binary.substr(first_one);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N){\n    uint32_t sum = 0;\n    const uint32_t digits[10] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 0};\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vdigits = _mm256_loadu_si256((__m256i*)digits);\n    __m256i vones = _mm256_set1_epi32(1);\n\n    while(N > 0){\n        uint64_t digit = N % 10;\n        N /= 10;\n\n        __m256i vd = _mm256_set1_epi32(digit);\n        __m256i mask = _mm256_cmpeq_epi32(vd, vdigits);\n        __m256i add = _mm256_and_si256(mask, vones);\n        vsum = _mm256_add_epi32(vsum, add);\n    }\n\n    uint32_t res[8];\n    _mm256_storeu_si256((__m256i*)res, vsum);\n    for(int i=0;i<8;i++) sum += res[i];\n\n    std::string binary = std::bitset<32>(sum).to_string();\n    // Remove leading zeros\n    size_t first_one = binary.find_first_of('1');\n    if(first_one != std::string::npos){\n        return binary.substr(first_one);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N){\n    uint32_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i constant_10 = _mm256_set1_epi32(10);\n\n    while(N > 0){\n        __m256i digit = _mm256_set1_epi32(N % 10);\n        vsum = _mm256_add_epi32(vsum, digit);\n        N /= 10;\n    }\n\n    // Horizontal sum\n    __m128i vsum_low = _mm_add_epi32(_mm256_castsi256_si128(vsum), \n                                      _mm256_extractf128_si256(vsum, 1));\n    vsum_low = _mm_hadd_epi32(vsum_low, vsum_low);\n    vsum_low = _mm_hadd_epi32(vsum_low, vsum_low);\n    sum = _mm_cvtsi128_si32(vsum_low);\n\n    std::string binary_representation = std::bitset<32>(sum).to_string();\n    // Remove leading zeros\n    size_t first_one = binary_representation.find_first_of('1');\n    if(first_one != std::string::npos){\n        return binary_representation.substr(first_one);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <bitset>\n#include <string>\n\nstd::string solve_simd(uint64_t N){\n    uint32_t sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i constant_10 = _mm256_set1_epi32(10);\n\n    while(N > 0){\n        __m256i digit = _mm256_set1_epi32(N % 10);\n        vsum = _mm256_add_epi32(vsum, digit);\n        N /= 10;\n    }\n\n    // Horizontal sum\n    __m128i vlow = _mm256_castsi256_si128(vsum);\n    __m128i vhigh = _mm256_extracti128_si256(vsum, 1);\n    vlow = _mm_add_epi32(vlow, vhigh);\n\n    uint32_t low_sum = _mm_cvtsi128_si32(vlow);\n    uint32_t high_sum = _mm_extract_epi32(vlow, 1);\n    sum = low_sum + high_sum;\n\n    std::string binary_representation = std::bitset<32>(sum).to_string();\n    // Remove leading zeros\n    size_t first_one = binary_representation.find_first_of('1');\n    if(first_one != std::string::npos){\n        binary_representation = binary_representation.substr(first_one);\n    } else {\n        binary_representation = \"0\";\n    }\n    return binary_representation;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    size_t size = lst.size();\n    if(size < 2) return sum;\n\n    alignas(32) int temp[8];\n    __m256i zero = _mm256_setzero_si256();\n    __m256i total = _mm256_setzero_si256();\n\n    for(size_t i = 1; i < size; i += 2){\n        if(i + 8 <= size){\n            __m256i vec = _mm256_loadu_si256((__m256i const*)(&lst[i]));\n            __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_and_si256(vec, _mm256_set1_epi32(1)));\n            __m256i even_elements = _mm256_and_si256(vec, even_mask);\n            total = _mm256_add_epi32(total, even_elements);\n            i += 7;\n        }\n        else{\n            break;\n        }\n    }\n\n    _mm256_store_si256((__m256i*)temp, total);\n    for(int j = 0; j < 8; ++j){\n        sum += temp[j];\n    }\n\n    for(size_t i = 1; i < size; i += 2){\n        if(lst[i] % 2 == 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    size_t size = lst.size();\n    alignas(32) int temp[8] = {0};\n\n    if (size <= 1) return 0;\n\n    for (size_t i = 1; i < size; i += 2) {\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mask = _mm256_and_si256(_mm256_srli_epi32(indices, 1), _mm256_set1_epi32(1));\n        __m256i data = _mm256_i32gather_epi32((const int*)&lst[0], indices, sizeof(int));\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(data, _mm256_set1_epi32(1)), _mm256_setzero_si256());\n        __m256i valid_data = _mm256_and_si256(data, even_mask);\n        _mm256_store_si256((__m256i*)temp, valid_data);\n        sum += temp[1] + temp[3] + temp[5] + temp[7];\n        i += 7;\n    }\n\n    // Handle remaining elements\n    for (size_t i = 1; i < size; i += 2) {\n        if (i >= size) break;\n        if ((i & 1) && !(lst[i] & 1)) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    size_t size = lst.size();\n    if(size < 2) return sum;\n\n    alignas(32) int temp[8] = {0};\n    __m256i zero = _mm256_setzero_si256();\n    __m256i total = _mm256_setzero_si256();\n\n    for(size_t i = 1; i < size; i += 2){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i even_mask = _mm256_cmpeq_epi32(_mm256_and_si256(vec, _mm256_set1_epi32(1)), zero);\n        __m256i even_elements = _mm256_and_si256(vec, even_mask);\n        total = _mm256_add_epi32(total, even_elements);\n    }\n\n    _mm256_storeu_si256((__m256i*)temp, total);\n\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    size_t size = lst.size();\n    \n    if (size == 0) return 0;\n\n    alignas(32) int temp[8] = {0};\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i mask_even = _mm256_set1_epi32(1); // To check even numbers\n    __m256i mask_odd_idx = _mm256_set_epi32(1, 0, 1, 0, 1, 0, 1, 0); // Odd indices mask\n    __m256i vsum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 8 <= size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i is_even = _mm256_cmpeq_epi32(_mm256_and_si256(vec, mask_even), zero);\n        __m256i odd_idx = _mm256_and_si256(is_even, mask_odd_idx);\n        __m256i filtered = _mm256_and_si256(vec, odd_idx);\n        vsum = _mm256_add_epi32(vsum, filtered);\n    }\n\n    _mm256_store_si256((__m256i*)temp, vsum);\n\n    sum = temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n\n    for (size_t i = (size / 8) * 8; i < size; ++i) {\n        if (i % 2 != 0 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    if(lst.size() <= 1) return sum;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i mask_even = _mm256_set1_epi32(1); // To check even numbers (using AND)\n    __m256i mask_odd_indices = _mm256_set_epi32(1,0,1,0,1,0,1,0); // Odd indices mask\n    __m256i total = _mm256_set1_epi32(0);\n\n    size_t aligned_size = lst.size() / 8 * 8;\n    int* data = lst.data();\n\n    for(size_t i = 0; i < aligned_size; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        \n        // Create a mask for odd indices\n        __m256i is_odd = _mm256_and_si256(indices, mask_odd_indices);\n        \n        // Check if elements are even\n        __m256i is_even = _mm256_xor_si256(_mm256_and_si256(vec, mask_even), mask_even);\n        \n        // Combine masks: even values at odd indices\n        __m256i valid = _mm256_and_si256(is_even, is_odd);\n        \n        // Zero out invalid elements\n        __m256i filtered = _mm256_and_si256(valid, vec);\n        \n        // Add to total\n        total = _mm256_add_epi32(total, filtered);\n    }\n\n    // Horizontal sum\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, total);\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    // Handle remaining elements\n    for(size_t i = aligned_size; i < lst.size(); ++i){\n        if(i % 2 == 1 && lst[i] % 2 == 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int mod = 26;\n    const char base = 'a';\n    std::string result;\n    result.reserve(s.size());\n\n    const __m256i vshift = _mm256_set1_epi8(shift);\n    const __m256i vmod = _mm256_set1_epi8(mod);\n    const __m256i vbase = _mm256_set1_epi8(base);\n    const __m256i vlower_a = _mm256_set1_epi8('a');\n    const __m256i vupper_a = _mm256_set1_epi8('A');\n    const __m256i vlower_z = _mm256_set1_epi8('z');\n    const __m256i vupper_z = _mm256_set1_epi8('Z');\n\n    size_t i = 0;\n    for (; i + 31 <= s.size(); i += 32) {\n        __m256i vchar = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i vis_lower = _mm256_cmpgt_epi8(vchar, vlower_a);\n        __m256i vis_upper = _mm256_cmpgt_epi8(vchar, vupper_a);\n\n        __m256i vadjusted = _mm256_add_epi8(vchar, vshift);\n\n        __m256i vlower_wrap = _mm256_cmpgt_epi8(vadjusted, vlower_z);\n        __m256i vupper_wrap = _mm256_cmpgt_epi8(vadjusted, vupper_z);\n\n        __m256i vlower_sub = _mm256_and_si256(vlower_wrap, _mm256_sub_epi8(vadjusted, vmod));\n        __m256i vupper_sub = _mm256_and_si256(vupper_wrap, _mm256_sub_epi8(vadjusted, vmod));\n\n        vadjusted = _mm256_blendv_epi8(vadjusted, vlower_sub, vis_lower);\n        vadjusted = _mm256_blendv_epi8(vadjusted, vupper_sub, vis_upper);\n\n        __m256i vresult = _mm256_blendv_epi8(vadjusted, vchar, _mm256_xor_si256(vis_lower, vis_upper));\n\n        char temp[32];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vresult);\n        result.append(temp, temp + 32);\n    }\n\n    for (; i < s.size(); ++i) {\n        if (s[i] >= 'a' && s[i] <= 'z') {\n            result += (s[i] - 'a' + shift) % mod + 'a';\n        } else if (s[i] >= 'A' && s[i] <= 'Z') {\n            result += (s[i] - 'A' + shift) % mod + 'A';\n        } else {\n            result += s[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.length();\n    std::string result(length, ' ');\n\n    const __m256i lower_bound = _mm256_set1_epi8('a');\n    const __m256i upper_bound = _mm256_set1_epi8('z');\n    const __m256i alphabet_size = _mm256_set1_epi8(26);\n    const __m256i shift_value = _mm256_set1_epi8(shift);\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i in_range = _mm256_and_si256(\n            _mm256_cmpgt_epi8(vec, lower_bound),\n            _mm256_cmple_epi8(vec, upper_bound)\n        );\n        __m256i shifted = _mm256_add_epi8(vec, shift_value);\n        __m256i wrapped = _mm256_sub_epi8(\n            _mm256_add_epi8(vec, _mm256_and_si256(in_range, shift_value)),\n            _mm256_and_si256(_mm256_cmpgt_epi8(shifted, upper_bound), alphabet_size)\n        );\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), wrapped);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.length();\n    std::string result(length, ' ');\n\n    const __m256i lower_bound = _mm256_set1_epi8('a');\n    const __m256i upper_bound = _mm256_set1_epi8('z');\n    const __m256i alphabet_size = _mm256_set1_epi8(26);\n    const __m256i shift_value = _mm256_set1_epi8(shift);\n\n    for(int i = 0; i < length; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i in_range = _mm256_and_si256(\n            _mm256_cmpgt_epi8(vec, lower_bound),\n            _mm256_cmple_epi8(vec, upper_bound)\n        );\n        __m256i shifted = _mm256_add_epi8(vec, shift_value);\n        __m256i wrapped = _mm256_sub_epi8(\n            _mm256_add_epi8(\n                _mm256_and_si256(shifted, in_range),\n                _mm256_and_si256(\n                    _mm256_mullo_epi16(\n                        _mm256_set1_epi8(26),\n                        _mm256_cmplt_epi8(shifted, lower_bound)\n                    ),\n                    in_range\n                )\n            ),\n            _mm256_and_si256(\n                _mm256_mullo_epi16(\n                    _mm256_set1_epi8(26),\n                    _mm256_cmpgt_epi8(shifted, upper_bound)\n                ),\n                in_range\n            )\n        );\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), wrapped);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.length();\n    std::string result(length, ' ');\n\n    const __m256i lower_bound = _mm256_set1_epi8('a');\n    const __m256i upper_bound = _mm256_set1_epi8('z');\n    const __m256i alphabet_size = _mm256_set1_epi8(26);\n    const __m256i shift_value = _mm256_set1_epi8(shift);\n\n    for(size_t i = 0; i < length; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i in_range_mask = _mm256_and_si256(\n            _mm256_cmpgt_epi8(vec, lower_bound),\n            _mm256_cmple_epi8(vec, upper_bound)\n        );\n        __m256i shifted = _mm256_add_epi8(vec, shift_value);\n        __m256i wrapped = _mm256_sub_epi8(\n            _mm256_add_epi8(vec, _mm256_and_si256(in_range_mask, shift_value)),\n            _mm256_and_si256(_mm256_cmpgt_epi8(shifted, upper_bound), alphabet_size)\n        );\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), wrapped);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.length();\n    std::string result(length, ' ');\n\n    const __m256i lower_bound = _mm256_set1_epi8('a');\n    const __m256i upper_bound = _mm256_set1_epi8('z');\n    const __m256i alphabet_size = _mm256_set1_epi8(26);\n    const __m256i shift_val = _mm256_set1_epi8(shift);\n\n    for(int i = 0; i < length; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i in_range = _mm256_and_si256(\n            _mm256_cmpgt_epi8(vec, lower_bound),\n            _mm256_cmple_epi8(vec, upper_bound)\n        );\n        __m256i shifted = _mm256_add_epi8(vec, shift_val);\n        __m256i wrapped = _mm256_sub_epi8(\n            _mm256_add_epi8(shifted, _mm256_and_si256(_mm256_cmplt_epi8(shifted, lower_bound), alphabet_size)),\n            _mm256_and_si256(_mm256_cmpgt_epi8(shifted, upper_bound), alphabet_size)\n        );\n        __m256i encrypted = _mm256_blendv_epi8(vec, wrapped, in_range);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encrypted);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    const char vowels_lower[] = { 'a', 'e', 'i', 'o', 'u' };\n    const char vowels_upper[] = { 'A', 'E', 'I', 'O', 'U' };\n    const char replace_lower[] = { 'c', 'g', 'k', 'q', 'w' };\n    const char replace_upper[] = { 'C', 'G', 'K', 'Q', 'W' };\n\n    __m256i vlower = _mm256_loadu_si256((__m256i*)vowels_lower);\n    __m256i vupper = _mm256_loadu_si256((__m256i*)vowels_upper);\n    __m256i rlower = _mm256_loadu_si256((__m256i*)replace_lower);\n    __m256i rupper = _mm256_loadu_si256((__m256i*)replace_upper);\n\n    std::string result;\n    result.reserve(message.size());\n\n    for(size_t i = 0; i < message.size(); i += 32) {\n        size_t chunk = std::min<size_t>(message.size() - i, 32);\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Swap case\n        __m256i lower_mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), _mm256_set1_epi8('Z'));\n        __m256i is_lower = _mm256_and_si256(_mm256_cmpgt_epi8(data, _mm256_set1_epi8('A')), _mm256_cmple_epi8(data, _mm256_set1_epi8('z')));\n        __m256i is_upper = _mm256_andnot_si256(is_lower, lower_mask);\n        __m256i swap_case = _mm256_xor_si256(data, _mm256_or_si256(_mm256_and_si256(is_lower, _mm256_set1_epi8(0x20)), _mm256_and_si256(is_upper, _mm256_set1_epi8(0x20))));\n\n        // Replace vowels\n        __m256i is_vowel_lower = _mm256_testnzc_si256(_mm256_cmpeq_epi8(data, vlower), _mm256_setzero_si256());\n        __m256i is_vowel_upper = _mm256_testnzc_si256(_mm256_cmpeq_epi8(data, vupper), _mm256_setzero_si256());\n        __m256i replace = _mm256_blendv_epi8(swap_case, _mm256_or_si256(_mm256_and_si256(is_vowel_lower, rlower), _mm256_and_si256(is_vowel_upper, rupper)), _mm256_or_si256(is_vowel_lower, is_vowel_upper));\n\n        // Store result\n        alignas(32) char temp[32];\n        _mm256_store_si256(reinterpret_cast<__m256i*>(temp), replace);\n        result.append(temp, chunk);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    const int len = message.size();\n    std::string result(len, ' ');\n\n    // Define vowels and their replacements\n    const char vowels[5] = {'a', 'e', 'i', 'o', 'u'};\n    const char vowel_replacements[5] = {'c', 'g', 'k', 'q', 'w'};\n    \n    // Create a mask for vowels and their replacements\n    __m256i v_vowels = _mm256_loadu_si256((__m256i*)vowels);\n    __m256i v_replacements = _mm256_loadu_si256((__m256i*)vowel_replacements);\n\n    for(int i = 0; i < len; i += 32){\n        int limit = (i + 32 > len) ? len - i : 32;\n        __m256i v_data = _mm256_loadu_si256((__m256i*)&message[i]);\n\n        // Swap case\n        __m256i v_lower_mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), _mm256_set1_epi8('A'));\n        __m256i v_case_mask = _mm256_xor_si256(v_data, v_lower_mask);\n        __m256i v_swapped_case = _mm256_sub_epi8(_mm256_add_epi8(v_data, v_case_mask), v_case_mask);\n\n        // Replace vowels\n        __m256i v_is_vowel = _mm256_cmpeq_epi8(v_swapped_case, v_vowels);\n        __m256i v_replaced = _mm256_blendv_epi8(v_swapped_case, v_replacements, v_is_vowel);\n\n        // Store result\n        _mm256_storeu_si256((__m256i*)&result[i], v_replaced);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cstring>\n\nstd::string encode_simd(const std::string & message){\n    const int length = message.size();\n    std::string result(length, ' ');\n\n    // Define vowels and their replacements (shifted by 2)\n    const char vowels[10] = {'a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U'};\n    const char replacements[10] = {'c', 'g', 'k', 'q', 'w', 'C', 'G', 'K', 'Q', 'W'};\n\n    // Load vowel-replacement pairs into a vector for comparison\n    __m256i v_vowels = _mm256_loadu_si256((__m256i*)vowels);\n    __m256i v_replacements = _mm256_loadu_si256((__m256i*)replacements);\n\n    // Process in chunks of 32 bytes (256 bits)\n    for(int i = 0; i < length; i += 32){\n        int chunk_size = std::min(32, length - i);\n\n        // Load characters into a SIMD register\n        __m256i v_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Create a mask to identify vowels\n        __m256i v_mask = _mm256_cmpeq_epi8(v_chars, v_vowels);\n\n        // Replace vowels with corresponding replacements\n        __m256i v_result = _mm256_blendv_epi8(v_chars, v_replacements, v_mask);\n\n        // Swap case for non-vowel alphabetic characters\n        __m256i v_upper = _mm256_and_si256(v_chars, _mm256_set1_epi8(0xDF)); // Uppercase mask\n        __m256i v_lower = _mm256_or_si256(_mm256_and_si256(v_chars, _mm256_set1_epi8(0x5F)), _mm256_set1_epi8(0x20)); // Lowercase mask\n\n        __m256i v_is_alpha = _mm256_or_si256(\n            _mm256_and_si256(_mm256_cmpgt_epi8(v_chars, _mm256_set1_epi8('A' - 1)),\n            _mm256_and_si256(_mm256_cmplt_epi8(v_chars, _mm256_set1_epi8('z' + 1)))\n        );\n\n        __m256i v_swapped = _mm256_blendv_epi8(v_lower, v_upper, v_is_alpha);\n\n        // Combine replaced vowels and swapped case\n        v_result = _mm256_blendv_epi8(v_result, v_swapped, _mm256_xor_si256(v_mask, _mm256_set1_epi8(-1)));\n\n        // Store the result back into the string\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), v_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    const int len = message.size();\n    std::string result(len, ' ');\n\n    // Define vowels and their replacements\n    const char vowels[5] = {'a', 'e', 'i', 'o', 'u'};\n    const char vowel_replacements[5] = {'c', 'g', 'k', 'q', 'w'};\n    \n    // Create a mask for vowels and their replacements\n    __m256i vowel_mask = _mm256_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1, \n        -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1\n    );\n    for(int i = 0; i < 5; ++i){\n        vowel_mask = _mm256_or_si256(vowel_mask, _mm256_set1_epi8(vowels[i]));\n    }\n\n    // Process in chunks of 32 bytes\n    for(int i = 0; i < len; i += 32){\n        int chunk_size = (len - i > 32) ? 32 : len - i;\n        \n        // Load data\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Swap case\n        __m256i lower_to_upper = _mm256_sub_epi8(vec, _mm256_set1_epi8(32));\n        __m256i upper_to_lower = _mm256_add_epi8(vec, _mm256_set1_epi8(32));\n        __m256i is_lower = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), vec);\n        __m256i is_upper = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8('Z'));\n        __m256i swapped_case = _mm256_blendv_epi8(lower_to_upper, upper_to_lower, is_upper);\n\n        // Replace vowels\n        __m256i is_vowel = _mm256_cmpeq_epi8(vec, vowel_mask);\n        __m256i replacement = _mm256_set1_epi8(' ');\n        for(int j = 0; j < 5; ++j){\n            __m256i current_vowel = _mm256_set1_epi8(vowels[j]);\n            __m256i current_replacement = _mm256_set1_epi8(vowel_replacements[j]);\n            __m256i match = _mm256_cmpeq_epi8(vec, current_vowel);\n            replacement = _mm256_blendv_epi8(replacement, current_replacement, match);\n        }\n        __m256i final_vec = _mm256_blendv_epi8(swapped_case, replacement, is_vowel);\n\n        // Store result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    const int len = message.size();\n    std::string result(len, ' ');\n\n    // Define vowels and their replacements (shifted by 2)\n    const char vowels[10] = {'a','e','i','o','u','A','E','I','O','U'};\n    const char repls[10] = {'c','g','k','q','w','C','G','K','Q','W'};\n\n    // Load vowels and replacements into vectors\n    __m256i v_vowels = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(vowels));\n    __m256i v_repls = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(repls));\n\n    // Process in chunks of 32 bytes (256 bits)\n    for(int i = 0; i < len; i += 32){\n        int chunk_size = ((len - i) > 32) ? 32 : (len - i);\n\n        // Load data into vector\n        __m256i v_data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n\n        // Create mask for lowercase letters\n        __m256i v_lower_mask = _mm256_cmpgt_epi8(v_data, _mm256_set1_epi8('a' - 1));\n        v_lower_mask = _mm256_and_si256(v_lower_mask, _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), v_data));\n\n        // Create mask for uppercase letters\n        __m256i v_upper_mask = _mm256_cmpgt_epi8(v_data, _mm256_set1_epi8('A' - 1));\n        v_upper_mask = _mm256_and_si256(v_upper_mask, _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), v_data));\n\n        // Swap case\n        __m256i v_swapped = _mm256_add_epi8(v_data, _mm256_or_si256(\n            _mm256_and_si256(v_lower_mask, _mm256_set1_epi8(-32)),\n            _mm256_and_si256(v_upper_mask, _mm256_set1_epi8(32))\n        ));\n\n        // Find vowels and replace them\n        __m256i v_is_vowel = _mm256_cmpeq_epi8(v_data, v_vowels);\n        for(int j = 1; j < 10; j += 2){\n            v_is_vowel = _mm256_or_si256(v_is_vowel, _mm256_cmpeq_epi8(v_data, _mm256_set1_epi8(vowels[j])));\n        }\n        __m256i v_replaced = _mm256_blendv_epi8(v_swapped, v_repls, v_is_vowel);\n\n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), v_replaced);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n#include <climits>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first = true;\n    bool is_all_upper = false;\n    bool is_all_lower = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        if (key.empty()) continue;\n\n        bool current_is_upper = true;\n        bool current_is_lower = true;\n\n        size_t len = key.length();\n        size_t vec_len = len / 32 * 32; // Process in chunks of 32 bytes\n\n        for (size_t i = 0; i < vec_len; i += 32) {\n            __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key.c_str() + i));\n            __m256i lower_bound = _mm256_set1_epi8('a');\n            __m256i upper_bound = _mm256_set1_epi8('z');\n            __m256i lower_check = _mm256_cmpgt_epi8(chunk, lower_bound);\n            __m256i upper_check = _mm256_cmple_epi8(chunk, upper_bound);\n            __m256i is_lower = _mm256_and_si256(lower_check, upper_check);\n\n            __m256i A_lower_bound = _mm256_set1_epi8('A');\n            __m256i Z_upper_bound = _mm256_set1_epi8('Z');\n            __m256i A_lower_check = _mm256_cmpgt_epi8(chunk, A_lower_bound);\n            __m256i Z_upper_check = _mm256_cmple_epi8(chunk, Z_upper_bound);\n            __m256i is_upper = _mm256_and_si256(A_lower_check, Z_upper_check);\n\n            int mask_lower = _mm256_movemask_epi8(is_lower);\n            int mask_upper = _mm256_movemask_epi8(is_upper);\n\n            if ((mask_lower & 0xFFFF) != 0xFFFF) current_is_lower = false;\n            if ((mask_upper & 0xFFFF) != 0xFFFF) current_is_upper = false;\n        }\n\n        for (size_t i = vec_len; i < len; ++i) {\n            unsigned char c = key[i];\n            if (!(c >= 'a' && c <= 'z')) current_is_lower = false;\n            if (!(c >= 'A' && c <= 'Z')) current_is_upper = false;\n        }\n\n        if (first) {\n            is_all_upper = current_is_upper;\n            is_all_lower = current_is_lower;\n            first = false;\n        } else {\n            if (current_is_upper && !is_all_upper) return false;\n            if (current_is_lower && !is_all_lower) return false;\n            if (!current_is_upper && !current_is_lower) return false;\n        }\n    }\n\n    return is_all_upper || is_all_lower;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n#include <algorithm>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first_key = true;\n    bool is_all_lower = false;\n    bool is_all_upper = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        if (key.empty() || !std::all_of(key.begin(), key.end(), ::isalpha)) {\n            return false;\n        }\n\n        __m256i lower_mask = _mm256_set1_epi8(0);\n        __m256i upper_mask = _mm256_set1_epi8(0);\n\n        size_t i = 0;\n        for (; i + 31 < key.size(); i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&key[i]));\n            __m256i cmp_lower = _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), vec);\n            __m256i cmp_upper = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8('Z'));\n\n            lower_mask = _mm256_or_si256(lower_mask, cmp_lower);\n            upper_mask = _mm256_or_si256(upper_mask, cmp_upper);\n        }\n\n        bool current_is_lower = true;\n        bool current_is_upper = true;\n\n        if (!_mm256_testz_si256(lower_mask, lower_mask)) {\n            current_is_lower = false;\n        }\n        if (!_mm256_testz_si256(upper_mask, upper_mask)) {\n            current_is_upper = false;\n        }\n\n        for (; i < key.size(); ++i) {\n            if (key[i] >= 'a' && key[i] <= 'z') {\n                current_is_upper = false;\n            } else if (key[i] >= 'A' && key[i] <= 'Z') {\n                current_is_lower = false;\n            } else {\n                return false;\n            }\n        }\n\n        if (first_key) {\n            is_all_lower = current_is_lower;\n            is_all_upper = current_is_upper;\n            first_key = false;\n        } else {\n            if ((current_is_lower && !is_all_lower) || (current_is_upper && !is_all_upper)) {\n                return false;\n            }\n        }\n    }\n\n    return is_all_lower || is_all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first_key_processed = false;\n    bool is_all_lower = false;\n    bool is_all_upper = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        if (key.empty()) continue;\n\n        bool key_is_lower = true;\n        bool key_is_upper = true;\n\n        size_t len = key.length();\n        size_t vec_len = len / 32 * 32; // Process in chunks of 32 characters\n\n        // AVX2 registers can hold 32 bytes (256 bits)\n        __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n        __m256i upper_mask = _mm256_set1_epi8('z' + 1);\n        __m256i upper_a = _mm256_set1_epi8('A');\n        __m256i upper_z = _mm256_set1_epi8('Z');\n\n        for (size_t i = 0; i < vec_len; i += 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&key[i]));\n\n            // Check lowercase range\n            __m256i cmp1 = _mm256_cmpgt_epi8(lower_mask, vec); // x < 'a'\n            __m256i cmp2 = _mm256_cmple_epi8(vec, upper_mask); // x <= 'z'\n            __m256i is_lower = _mm256_andnot_si256(cmp1, cmp2);\n            if (!_mm256_testc_si256(_mm256_set1_epi8(0xFF), is_lower)) {\n                key_is_lower = false;\n            }\n\n            // Check uppercase range\n            __m256i cmp3 = _mm256_cmpgt_epi8(upper_mask, vec); // x > 'Z'\n            __m256i cmp4 = _mm256_cmple_epi8(vec, upper_a); // x <= 'A'\n            __m256i is_upper = _mm256_andnot_si256(cmp3, cmp4);\n            if (!_mm256_testc_si256(_mm256_set1_epi8(0xFF), is_upper)) {\n                key_is_upper = false;\n            }\n        }\n\n        // Process remaining characters\n        for (size_t i = vec_len; i < len; ++i) {\n            unsigned char c = key[i];\n            if (!(std::islower(c) || std::isupper(c))) {\n                key_is_lower = false;\n                key_is_upper = false;\n                break;\n            }\n            if (!std::islower(c)) key_is_lower = false;\n            if (!std::isupper(c)) key_is_upper = false;\n        }\n\n        if (!first_key_processed) {\n            is_all_lower = key_is_lower;\n            is_all_upper = key_is_upper;\n            first_key_processed = true;\n        } else {\n            if ((key_is_lower && !is_all_lower) || (key_is_upper && !is_all_upper)) {\n                return false;\n            }\n            is_all_lower = is_all_lower && key_is_lower;\n            is_all_upper = is_all_upper && key_is_upper;\n        }\n    }\n\n    return is_all_lower || is_all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n#include <algorithm>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    const int vector_size = 32; // AVX2: 256 bits = 32 bytes\n    alignas(32) char buffer[vector_size];\n    __m256i lower_mask = _mm256_set1_epi8('a' - 1);\n    __m256i upper_mask = _mm256_set1_epi8('A' - 1);\n    __m256i nine_mask = _mm256_set1_epi8('9');\n    __m256i zero_mask = _mm256_set1_epi8('0');\n\n    for (const auto& [key, value] : dict){\n        if (key.empty()) continue;\n\n        size_t len = key.length();\n        bool is_lower = true;\n        bool is_upper = true;\n\n        for (size_t i = 0; i < len; i += vector_size){\n            size_t chunk = std::min(len - i, (size_t)vector_size);\n            memset(buffer, 0, vector_size);\n            memcpy(buffer, key.c_str() + i, chunk);\n\n            __m256i vec = _mm256_load_si256(reinterpret_cast<const __m256i*>(buffer));\n            __m256i cmp_lower = _mm256_cmpgt_epi8(vec, lower_mask);\n            __m256i cmp_upper = _mm256_cmpgt_epi8(vec, upper_mask);\n            __m256i cmp_nine = _mm256_cmple_epi8(vec, nine_mask);\n            __m256i cmp_zero = _mm256_cmpgt_epi8(vec, zero_mask);\n\n            __m256i is_digit = _mm256_and_si256(cmp_nine, cmp_zero);\n            __m256i not_digit = _mm256_xor_si256(is_digit, _mm256_set1_epi8(-1));\n\n            __m256i lower_check = _mm256_and_si256(cmp_lower, not_digit);\n            __m256i upper_check = _mm256_and_si256(cmp_upper, not_digit);\n\n            int mask_lower = _mm256_movemask_epi8(lower_check);\n            int mask_upper = _mm256_movemask_epi8(upper_check);\n\n            if (mask_lower != (1 << vector_size) - 1){\n                is_lower = false;\n            }\n            if (mask_upper != (1 << vector_size) - 1){\n                is_upper = false;\n            }\n        }\n\n        all_lower &= is_lower;\n        all_upper &= is_upper;\n\n        if (!all_lower && !all_upper){\n            return false;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n#include <algorithm>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    auto it = dict.begin();\n    const char* first_key = it->first.c_str();\n    bool is_upper = true, is_lower = true;\n\n    // Check the case of the first key\n    for (__m256i vec; (*first_key); first_key += 32) {\n        vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(first_key));\n        __m256i lower_mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('a'), _mm256_set1_epi8('z'));\n        __m256i upper_mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('A'), _mm256_set1_epi8('Z'));\n\n        __m256i is_lower_vec = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('a')), \n                                               _mm256_cmple_epi8(vec, _mm256_set1_epi8('z')));\n        __m256i is_upper_vec = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('A')), \n                                               _mm256_cmple_epi8(vec, _mm256_set1_epi8('Z')));\n\n        is_lower &= (_mm256_movemask_epi8(is_lower_vec) == 0xFFFF);\n        is_upper &= (_mm256_movemask_epi8(is_upper_vec) == 0xFFFF);\n    }\n\n    if (!is_lower && !is_upper) return false;\n\n    for (++it; it != dict.end(); ++it) {\n        const char* key = it->first.c_str();\n        bool current_is_lower = true, current_is_upper = true;\n\n        for (__m256i vec; (*key); key += 32) {\n            vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key));\n\n            __m256i is_lower_vec = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('a')), \n                                                   _mm256_cmple_epi8(vec, _mm256_set1_epi8('z')));\n            __m256i is_upper_vec = _mm256_and_si256(_mm256_cmpgt_epi8(vec, _mm256_set1_epi8('A')), \n                                                   _mm256_cmple_epi8(vec, _mm256_set1_epi8('Z')));\n\n            current_is_lower &= (_mm256_movemask_epi8(is_lower_vec) == 0xFFFF);\n            current_is_upper &= (_mm256_movemask_epi8(is_upper_vec) == 0xFFFF);\n        }\n\n        if ((is_lower && !current_is_lower) || (is_upper && !current_is_upper)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    const int size = n;\n    const int bytes = (size + 7) / 8;\n    __m256i zero = _mm256_setzero_si256();\n    alignas(32) unsigned char sieve[bytes];\n    for(int i = 0; i < bytes; ++i) sieve[i] = 0xFF;\n\n    // Mark 0 and 1 as non-prime\n    if(size > 0) sieve[0] &= ~1;\n    if(size > 1) sieve[0] &= ~2;\n\n    int limit = (int)std::sqrt((double)size);\n    for(int p = 2; p <= limit; ++p){\n        if(sieve[p/8] & (1 << (p%8))){\n            int start = p * p;\n            if(start >= size) break;\n            for(int multiple = start; multiple < size; multiple += p){\n                sieve[multiple/8] &= ~(1 << (multiple%8));\n            }\n        }\n    }\n\n    // Collect primes using AVX2\n    __m256i vone = _mm256_set1_epi8(1);\n    for(int i = 2; i < size; i++){\n        if(sieve[i/8] & (1 << (i%8))){\n            primes.push_back(i);\n            if(primes.size() >= 8){\n                __m256i vprimes = _mm256_loadu_si256((__m256i*)&primes[primes.size()-8]);\n                __m256i vmask = _mm256_cmpgt_epi8(vprimes, zero);\n                __m256i vresult = _mm256_and_si256(vprimes, vmask);\n                _mm256_storeu_si256((__m256i*)&primes[primes.size()-8], vresult);\n            }\n        }\n    }\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    int limit = n;\n    int size = (limit + 7) / 8;\n    __m256i vone = _mm256_set1_epi8(1);\n    __m256i vmask = _mm256_set_epi8(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1);\n\n    std::vector<char> sieve(size, 0);\n    sieve[0] = 1;\n\n    for(int i = 2; i*i < limit; ++i){\n        if(!sieve[i/8] & (1 << (i%8))){\n            for(int j = i*i; j < limit; j += i){\n                sieve[j/8] |= (1 << (j%8));\n            }\n        }\n    }\n\n    int idx = 0;\n    int simd_size = 32;\n    int num_chunks = (size + simd_size - 1) / simd_size;\n\n    for(int chunk = 0; chunk < num_chunks; ++chunk){\n        __m256i vbyte = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&sieve[chunk * simd_size]));\n        __m256i vbits = _mm256_andnot_si256(vbyte, vmask);\n        __m256i vpairs = _mm256_shuffle_epi8(vbits, _mm256_set_epi8(\n            15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0,\n            15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0));\n\n        int mask = _mm256_movemask_epi8(vpairs);\n        for(int bit = 0; bit < 256; ++bit){\n            if(mask & (1 << bit)){\n                int prime = chunk * 256 + bit;\n                if(prime < n){\n                    primes.push_back(prime);\n                }\n            }\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    const int size = n + 7; // Align to multiple of 8 for AVX2\n    unsigned char* sieve = new unsigned char[size];\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i zeros = _mm256_setzero_si256();\n\n    // Initialize sieve\n    for(int i = 0; i < size; i += 32){\n        _mm256_storeu_si256((__m256i*)&sieve[i], ones);\n    }\n    sieve[0] = 0;\n    sieve[1] = 0;\n\n    int limit = static_cast<int>(std::sqrt(n));\n\n    for(int p = 2; p <= limit; ++p){\n        if(sieve[p]){\n            int start = p * p;\n            if(start > n) break;\n            for(int j = start; j < n; j += p){\n                int idx = j & ~31;\n                __m256i mask = _mm256_set_epi8(\n                    (j+0 == idx ? 0 : sieve[j+0]),\n                    (j+1 == idx ? 0 : sieve[j+1]),\n                    (j+2 == idx ? 0 : sieve[j+2]),\n                    (j+3 == idx ? 0 : sieve[j+3]),\n                    (j+4 == idx ? 0 : sieve[j+4]),\n                    (j+5 == idx ? 0 : sieve[j+5]),\n                    (j+6 == idx ? 0 : sieve[j+6]),\n                    (j+7 == idx ? 0 : sieve[j+7]),\n                    (j+8 == idx ? 0 : sieve[j+8]),\n                    (j+9 == idx ? 0 : sieve[j+9]),\n                    (j+10 == idx ? 0 : sieve[j+10]),\n                    (j+11 == idx ? 0 : sieve[j+11]),\n                    (j+12 == idx ? 0 : sieve[j+12]),\n                    (j+13 == idx ? 0 : sieve[j+13]),\n                    (j+14 == idx ? 0 : sieve[j+14]),\n                    (j+15 == idx ? 0 : sieve[j+15]),\n                    (j+16 == idx ? 0 : sieve[j+16]),\n                    (j+17 == idx ? 0 : sieve[j+17]),\n                    (j+18 == idx ? 0 : sieve[j+18]),\n                    (j+19 == idx ? 0 : sieve[j+19]),\n                    (j+20 == idx ? 0 : sieve[j+20]),\n                    (j+21 == idx ? 0 : sieve[j+21]),\n                    (j+22 == idx ? 0 : sieve[j+22]),\n                    (j+23 == idx ? 0 : sieve[j+23]),\n                    (j+24 == idx ? 0 : sieve[j+24]),\n                    (j+25 == idx ? 0 : sieve[j+25]),\n                    (j+26 == idx ? 0 : sieve[j+26]),\n                    (j+27 == idx ? 0 : sieve[j+27]),\n                    (j+28 == idx ? 0 : sieve[j+28]),\n                    (j+29 == idx ? 0 : sieve[j+29]),\n                    (j+30 == idx ? 0 : sieve[j+30]),\n                    (j+31 == idx ? 0 : sieve[j+31])\n                );\n                __m256i old_val = _mm256_loadu_si256((__m256i*)&sieve[idx]);\n                __m256i updated_val = _mm256_andnot_si256(mask, old_val);\n                _mm256_storeu_si256((__m256i*)&sieve[idx], updated_val);\n            }\n        }\n    }\n\n    for(int i = 2; i < n; ++i){\n        if(sieve[i]) primes.push_back(i);\n    }\n\n    delete[] sieve;\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    const int size = n;\n    const int simd_width = 8; // AVX2 can handle 256 bits, which is 8 ints\n    __m256i zero = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi32(1);\n\n    // Sieve of Eratosthenes using AVX2\n    std::vector<__m256i> sieve((size + simd_width - 1) / simd_width, _mm256_setallone_si256());\n    sieve[0] = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0); // 0 and 1 are not prime\n\n    for(int i = 2; i * i < size; ++i){\n        if(_mm256_testz_si256(sieve[i / simd_width], _mm256_set1_epi32(1 << (i % simd_width))) == 0){\n            for(int j = i * i; j < size; j += i){\n                int idx = j / simd_width;\n                int bit = j % simd_width;\n                sieve[idx] = _mm256_andnot_si256(_mm256_set1_epi32(1 << bit), sieve[idx]);\n            }\n        }\n    }\n\n    // Collect primes\n    for(int i = 2; i < size; ++i){\n        int idx = i / simd_width;\n        int bit = i % simd_width;\n        if(_mm256_testz_si256(_mm256_and_si256(sieve[idx], _mm256_set1_epi32(1 << bit)), _mm256_setzero_si256()) == 0){\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n <= 1) return primes;\n\n    int size = n / 32 + 1;\n    std::vector<__m256i> sieve(size, _mm256_setzero_si256());\n\n    __m256i one = _mm256_set1_epi32(1);\n\n    for(int i = 2; i * i < n; ++i){\n        if(!(_mm256_testc_epi32(sieve[i/32], _mm256_set1_epi32(1 << (i%32))))) {\n            for(int j = i * i; j < n; j += i){\n                __m256i mask = _mm256_set1_epi32(1 << (j%32));\n                sieve[j/32] = _mm256_or_si256(sieve[j/32], mask);\n            }\n        }\n    }\n\n    for(int i = 2; i < n; ++i){\n        if(!(_mm256_testc_epi32(sieve[i/32], _mm256_set1_epi32(1 << (i%32))))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s){\n    const int len = s.size();\n    if (len == 0) return 0;\n\n    const __m256i uppercase_vowels = _mm256_set1_epi8('A' | 'E' | 'I' | 'O' | 'U');\n    const __m256i indices_mask = _mm256_setr_epi8(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1);\n    const __m256i even_mask = _mm256_set1_epi8(0x00);\n    const __m256i odd_mask = _mm256_set1_epi8(0xFF);\n\n    int count = 0;\n    size_t i = 0;\n\n    for (; i + 31 < len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i even_indices = _mm256_andnot_si256(indices_mask, vec);\n        __m256i is_upper_vowel = _mm256_cmpeq_epi8(even_indices, uppercase_vowels);\n        __m256i masked = _mm256_and_si256(is_upper_vowel, _mm256_xor_si256(indices_mask, odd_mask));\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(masked));\n    }\n\n    for (; i < len; ++i) {\n        if ((i % 2 == 0) && (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    int count = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n    __m256i mask = _mm256_set1_epi8(0x41 | (0x45 << 8) | (0x49 << 16) | (0x4F << 24) | (0x55 << 32)); // A, E, I, O, U\n    __m256i ones = _mm256_set1_epi8(1);\n\n    for (size_t i = 0; i < len; i += 32) {\n        size_t j;\n        for (j = i; j < len && (j % 2) != 0; ++j); // Find next even index\n        if (j >= len) break;\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + j));\n        __m256i cmp = _mm256_cmpeq_epi8(vec, mask);\n        __m256i res = _mm256_and_si256(cmp, ones);\n        __m256i sum = _mm256_sad_epu8(res, _mm256_setzero_si256());\n        count += _mm256_extract_epi16(sum, 0);\n        j += 32;\n        if (j >= len) break;\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s){\n    if (s.empty()) return 0;\n\n    const char* str = s.c_str();\n    int length = s.length();\n    int count = 0;\n\n    const __m256i even_mask = _mm256_set1_epi32(0x80808080); // Mask for even indices\n    const __m256i uppercase_vowels = _mm256_set1_epi8('A' - ('A'^'E')); // AEIOU pattern\n    const __m256i cmp_mask = _mm256_set1_epi8(0xFF);\n\n    for(int i = 0; i + 31 < length; i += 32){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n        __m256i idx_mask = _mm256_and_si256(_mm256_set_epi32(15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0), even_mask);\n        __m256i shuffled = _mm256_shuffle_epi8(vec, idx_mask);\n        __m256i eq = _mm256_cmpeq_epi8(shuffled, uppercase_vowels);\n        __m256i masked = _mm256_and_si256(eq, cmp_mask);\n        __m256i res = _mm256_or_si256(res, masked);\n    }\n\n    int remainder = length % 32;\n    if(remainder > 0){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[length - remainder]));\n        __m256i idx_mask = _mm256_and_si256(_mm256_set_epi32(15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0), even_mask);\n        __m256i shuffled = _mm256_shuffle_epi8(vec, idx_mask);\n        __m256i eq = _mm256_cmpeq_epi8(shuffled, uppercase_vowels);\n        __m256i masked = _mm256_and_si256(eq, cmp_mask);\n        __m256i res = _mm256_or_si256(res, masked);\n    }\n\n    // Horizontal sum of results\n    __m256i res = _mm256_setzero_si256();\n    res = _mm256_sad_epu8(res, res);\n    uint32_t sums[8];\n    _mm256_storeu_si256((__m256i*)sums, res);\n    count = sums[0] + sums[1] + sums[2] + sums[3] + sums[4] + sums[5] + sums[6] + sums[7];\n    return count / 255;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    if (s.empty()) return count;\n\n    const char* str = s.c_str();\n    size_t length = s.length();\n    __m256i uppercase_vowels = _mm256_set1_epi8('A' ^ 'E' ^ 'I' ^ 'O' ^ 'U');\n    __m256i mask_even_indices = _mm256_setr_epi8(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n    for (size_t i = 0; i + 31 < length; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n        __m256i cmp_res = _mm256_cmpeq_epi8(vec, uppercase_vowels);\n        __m256i even_masked = _mm256_and_si256(cmp_res, mask_even_indices);\n        __m256i sum = _mm256_sad_epu8(even_masked, _mm256_setzero_si256());\n        count += _mm256_extract_epi16(sum, 0) / 255;\n    }\n\n    for (size_t i = (length / 32) * 32; i < length; ++i) {\n        if (i % 2 == 0 && (str[i] == 'A' || str[i] == 'E' || str[i] == 'I' || str[i] == 'O' || str[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    int count = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n    __m256i mask = _mm256_set1_epi8(0x41 | 0x4F); // Mask for uppercase vowels A E I O U\n    __m256i cmp_mask = _mm256_set1_epi8(0x0);\n\n    for (size_t i = 0; i < len; i += 32) {\n        if (i % 2 != 0) continue; // Ensure even index\n        if (len - i < 32) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n            __m256i res = _mm256_cmpeq_epi8(vec, mask);\n            res = _mm256_and_si256(res, cmp_mask);\n            int temp_count = _mm256_movemask_epi8(res);\n            count += __builtin_popcount(temp_count);\n        } else {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n            __m256i res = _mm256_cmpeq_epi8(vec, mask);\n            res = _mm256_and_si256(res, cmp_mask);\n            int temp_count = _mm256_movemask_epi8(res);\n            count += __builtin_popcount(temp_count);\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_init = _mm256_set_epi32(n + 6, n + 4, n + 2, n, n + 6, n + 4, n + 2, n);\n\n    int remainder = n % 8;\n    int aligned_count = n - remainder;\n\n    for(int i = 0; i < aligned_count; i += 8){\n        __m256i current = vec_init;\n        _mm256_storeu_si256((__m256i*)&result[i], current);\n        vec_init = _mm256_add_epi32(vec_init, vec_two);\n    }\n\n    if(remainder > 0){\n        __m256i mask = _mm256_set_epi32(-1, -1, -1, -1, -1, -1, -1, -1);\n        __m256i last_vec = vec_init;\n        __m256i zero = _mm256_setzero_si256();\n        mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(remainder), _mm256_set_epi32(7,6,5,4,3,2,1,0));\n        last_vec = _mm256_and_si256(mask, last_vec);\n        _mm256_storeu_si256((__m256i*)&result[aligned_count], last_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_is_odd = _mm256_set1_epi32(1);\n\n    __m256i current = vec_n;\n    __m256i increment = _mm256_and_si256(vec_two, _mm256_xor_si256(vec_n, vec_is_odd));\n\n    int idx = 0;\n    for (; idx <= n - 8; idx += 8){\n        _mm256_storeu_si256((__m256i*)&result[idx], current);\n        current = _mm256_add_epi32(current, vec_two);\n    }\n\n    for (; idx < n; ++idx){\n        result[idx] = n + 2 * idx;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_inc = _mm256_set1_epi32(2);\n    __m256i vec_num = _mm256_set1_epi32(n);\n\n    int remainder = n % 8;\n    int main_loop_count = n - remainder;\n\n    for(int i = 0; i < main_loop_count; i += 8){\n        _mm256_storeu_si256((__m256i*)&result[i], vec_num);\n        vec_num = _mm256_add_epi32(vec_num, vec_inc);\n    }\n\n    for(int i = main_loop_count; i < n; ++i){\n        result[i] = n + 2 * (i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_inc = _mm256_set1_epi32((n % 2 == 0) ? 2 : 2);\n    __m256i vec_curr = vec_n;\n\n    int idx = 0;\n    const int step = 8; // Number of integers processed per AVX operation\n\n    for (; idx <= n - step; idx += step) {\n        _mm256_storeu_si256((__m256i*)&result[idx], vec_curr);\n        vec_curr = _mm256_add_epi32(vec_curr, vec_inc);\n    }\n\n    // Handle remaining elements\n    for (; idx < n; ++idx) {\n        result[idx] = n + 2 * idx;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_two = _mm256_set1_epi32(2);\n    __m256i vec_zero = _mm256_setzero_si256();\n    __m256i is_odd = _mm256_and_si256(_mm256_srli_epi32(vec_n, 31), vec_one);\n\n    __m256i current = _mm256_add_epi32(vec_n, _mm256_and_si256(is_odd, vec_one));\n    int idx = 0;\n\n    for (; idx <= n - 8; idx += 8) {\n        _mm256_storeu_si256((__m256i*)&result[idx], current);\n        current = _mm256_add_epi32(current, _mm256_add_epi32(vec_two, vec_two));\n    }\n\n    // Handle remaining elements\n    for (; idx < n; ++idx) {\n        result[idx] = n + 2 * idx + (n % 2);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) return \"-1\";\n\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_m = _mm256_set1_epi64x(m);\n\n    __m256i v_sum = _mm256_add_epi64(_mm256_mullo_epi64(v_n, _mm256_set1_epi64x(m - n + 1)), \n                                     _mm256_mullo_epi64(_mm256_set1_epi64x(m), _mm256_set1_epi64x(m + 1)));\n    v_sum = _mm256_sub_epi64(v_sum, _mm256_mullo_epi64(v_n, _mm256_set1_epi64x(n - 1)));\n    v_sum = _mm256_srli_epi64(v_sum, 1);\n\n    __m256i v_count = _mm256_set1_epi64x(m - n + 1);\n    __m256i v_avg = _mm256_div_epi64(v_sum, v_count); // Hypothetical intrinsic for division\n\n    int64_t avg = _mm256_extract_epi64(v_avg, 0);\n    std::string binary = std::bitset<64>(avg).to_string();\n    // Remove leading zeros\n    size_t first_one = binary.find_first_of('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n    return binary;\n}\n```\n\n**Note:** `_mm256_div_epi64` is not a real AVX2 intrinsic. You would need to implement integer division using other methods or intrinsics if required."}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bit>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) return \"-1\";\n    \n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_m = _mm256_set1_epi64x(m);\n\n    __m256i v_sum = _mm256_add_epi64(_mm256_mullo_epi64(v_n, _mm256_set1_epi64x(m - n + 1)), _mm256_mullo_epi64(v_m, _mm256_set1_epi64x(m - n + 1)));\n    __m256i v_count = _mm256_set1_epi64x(m - n + 1);\n\n    __m256i v_avg = _mm256_div_epi64(v_sum, _mm256_slli_epi64(v_count, 1)); // Simplified avg calculation\n\n    int64_t avg = _mm256_extract_epi64(v_avg, 0);\n    std::string binary = std::bitset<64>(avg).to_string();\n    binary.erase(0, binary.find_first_of('1'));\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) return \"-1\";\n\n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_m = _mm256_set1_epi64x(m);\n\n    __m256i v_sum = _mm256_sub_epi64(v_m, v_n);\n    __m256i v_count = _mm256_add_epi64(v_sum, _mm256_set1_epi64x(1));\n    __m256i v_total = _mm256_mullo_epi64(_mm256_add_epi64(v_n, v_m), _mm256_srlv_epi64(v_count, _mm256_set1_epi64x(1)));\n\n    int64_t sum = _mm256_extract_epi64(v_total, 0);\n    int64_t count = _mm256_extract_epi64(v_count, 0);\n\n    int64_t avg = (sum + (count / 2)) / count;\n\n    std::string binary = std::bitset<64>(avg).to_string();\n    // Remove leading zeros\n    size_t first_one = binary.find_first_of('1');\n    if (first_one != std::string::npos) {\n        binary = binary.substr(first_one);\n    } else {\n        binary = \"0\";\n    }\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <bitset>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) return \"-1\";\n    \n    __m256i v_n = _mm256_set1_epi64x(n);\n    __m256i v_m = _mm256_set1_epi64x(m);\n    \n    __m256i v_sum = _mm256_sub_epi64(v_m, v_n);\n    v_sum = _mm256_add_epi64(v_sum, _mm256_set1_epi64x(1));\n    __m256i v_count = _mm256_add_epi64(v_sum, _mm256_set1_epi64x(n + m - 1));\n    v_sum = _mm256_mullo_epi32(v_sum, _mm256_set1_epi64x(n + m));\n    v_sum = _mm256_srli_epi64(v_sum, 1);\n    \n    int64_t sum = _mm256_extract_epi64(v_sum, 0);\n    int64_t count = _mm256_extract_epi64(v_count, 0);\n    \n    int64_t avg = sum / count;\n    std::string binary = std::bitset<64>(avg).to_string();\n    // Trim leading zeros\n    const auto loc = binary.find_first_of('1');\n    return (loc != std::string::npos) ? binary.substr(loc) : \"0\";\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) return \"-1\";\n    \n    __m256i vn = _mm256_set1_epi64x(n);\n    __m256i vm = _mm256_set1_epi64x(m);\n    \n    __m256i vcount = _mm256_sub_epi64(vm, vn);\n    vcount = _mm256_add_epi64(vcount, _mm256_set1_epi64x(1));\n    \n    __m256i vsum = _mm256_mullo_epi32(vn, _mm256_set1_epi64x(m - n + 1));\n    vsum = _mm256_add_epi64(vsum, _mm256_set1_epi64x((m - n + 1) * (m - n) / 2));\n    \n    __m256i vavg = _mm256_div_epi32(vsum, _mm256_set1_epi64x(m - n + 1));\n    \n    int64_t avg;\n    _mm256_storeu_si256((__m256i*)&avg, _mm256_cvtepi32_epi64(_mm256_extracti128_si256(vavg, 0)));\n    \n    std::string binary = \"\";\n    for(int i = 63; i >= 0; --i){\n        binary += ((avg >> i) & 1LL) ? '1' : '0';\n    }\n    \n    // Remove leading zeros\n    size_t start = binary.find_first_of(\"1\");\n    if(start != std::string::npos){\n        return binary.substr(start);\n    }\n    return \"0\";\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 1);\n    if (n <= 0) return result;\n\n    const int mod = 10000;\n    __m256i vec_mod = _mm256_set1_epi32(mod);\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 2 == 0){\n            int factorial = 1;\n            for(int j = 1; j <= i; ++j){\n                factorial = (factorial * j) % mod;\n            }\n            result[i-1] = factorial;\n        }\n        else{\n            int sum = (i * (i + 1)) / 2;\n            result[i-1] = sum % mod;\n        }\n    }\n\n    // SIMD optimization for even indices (factorials)\n    for(int i = 2; i <= n; i += 8){\n        __m256i vec_i = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i vec_fact = _mm256_set1_epi32(1);\n\n        for(int j = 1; j <= i+7; ++j){\n            __m256i vec_j = _mm256_set1_epi32(j);\n            vec_fact = _mm256_mullo_epi32(vec_fact, vec_j);\n            vec_fact = _mm256_and_si256(vec_fact, vec_mod);\n        }\n\n        _mm256_storeu_si256((__m256i*)&result[i-2], vec_fact);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    const int mod = 10000;\n    int i = 1;\n    __m256i vec_mod = _mm256_set1_epi32(mod);\n\n    while(i + 7 <= n){\n        __m256i vec_i = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i vec_res = _mm256_setzero_si256();\n        __m256i mask_even = _mm256_and_si256(_mm256_srli_epi32(vec_i, 31), _mm256_set1_epi32(0xAAAAAAAA));\n        \n        for(int j=1; j<=8; ++j){\n            __m256i cmp_mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(j), vec_i);\n            __m256i temp = _mm256_mullo_epi32(vec_res, _mm256_or_si256(_mm256_set1_epi32(j), cmp_mask));\n            vec_res = _mm256_andnot_si256(mask_even, _mm256_add_epi32(vec_res, _mm256_set1_epi32(j)));\n            vec_res = _mm256_or_si256(vec_res, _mm256_and_si256(mask_even, _mm256_mullo_epi32(temp, _mm256_set1_epi32(j))));\n            vec_res = _mm256_and_si256(vec_res, _mm256_cmplt_epi32(vec_res, vec_mod));\n        }\n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, vec_res);\n        for(int j=0; j<8; ++j){\n            result[i-1+j] = arr[j];\n        }\n        i += 8;\n    }\n\n    for(; i < n; ++i){\n        if(i % 2 == 0){\n            int fact = 1;\n            for(int j=1; j<=i; ++j){\n                fact = (fact * j) % mod;\n            }\n            result[i-1] = fact;\n        }\n        else{\n            int sum = (i * (i + 1)) / 2;\n            result[i-1] = sum;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 1);\n    if (n <= 0) return result;\n\n    const int mod = 10000;\n    __m256i vec_mod = _mm256_set1_epi32(mod);\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 2 == 0){\n            int factorial = 1;\n            for(int j = 1; j <= i; ++j){\n                factorial = (factorial * j) % mod;\n            }\n            result[i-1] = factorial;\n        }\n        else{\n            int sum = (i * (i + 1)) / 2;\n            result[i-1] = sum % mod;\n        }\n    }\n\n    // SIMD optimization for even indices (factorial modulo)\n    for(int i = 2; i <= n; i += 8){\n        __m256i vec_i = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i vec_fact = _mm256_set1_epi32(1);\n        for(int j = 1; j <= i+7; ++j){\n            __m256i vec_j = _mm256_set1_epi32(j);\n            vec_fact = _mm256_mullo_epi32(vec_fact, vec_j);\n            vec_fact = _mm256_rem_epu32(vec_fact, vec_mod); // Assume this intrinsic exists or use a workaround\n        }\n        _mm256_storeu_si256((__m256i*)&result[i-2], vec_fact);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    const int mod = 10000;\n    int k = n / 8 + 1;\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_mod = _mm256_set1_epi32(mod);\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 8 == 0){\n            __m256i v_i = _mm256_set_epi32(i-7, i-6, i-5, i-4, i-3, i-2, i-1, i);\n            __m256i v_res = v_one;\n            __m256i v_sum = v_one;\n\n            for(int j = 1; j <= i; ++j){\n                __m256i v_j = _mm256_set1_epi32(j);\n                __m256i v_even_mask = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_and_si256(v_i, _mm256_set1_epi32(1)), v_zero), v_one);\n                __m256i v_odd_mask = _mm256_xor_si256(v_one, v_even_mask);\n\n                v_res = _mm256_mullo_epi32(v_res, v_j);\n                v_res = _mm256_and_si256(v_res, v_even_mask);\n\n                v_sum = _mm256_add_epi32(v_sum, v_j);\n                v_sum = _mm256_and_si256(v_sum, _mm256_xor_si256(v_one, v_even_mask));\n\n                v_res = _mm256_or_si256(v_res, v_sum);\n                v_res = _mm256_rem_epi32(v_res, v_mod);\n            }\n\n            int data[8];\n            _mm256_storeu_si256((__m256i*)data, v_res);\n            for(int m = 0; m < 8 && (i - 7 + m) < n; ++m){\n                result[i - 7 + m] = data[m];\n            }\n        }\n        else{\n            int res = 1;\n            int sum = 1;\n            if(i % 2 == 0){\n                for(int j = 1; j <= i; ++j){\n                    res = (res * j) % mod;\n                }\n                result[i-1] = res;\n            }\n            else{\n                for(int j = 1; j <= i; ++j){\n                    sum += j;\n                }\n                result[i-1] = sum;\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    const int mod = 10000;\n    int i = 1;\n    __m256i vec_mod = _mm256_set1_epi32(mod);\n\n    for (; i + 7 <= n; i += 8) {\n        __m256i vec_idx = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i vec_even_mask = _mm256_and_si256(_mm256_srli_epi32(vec_idx, 1), _mm256_set1_epi32(1));\n        vec_even_mask = _mm256_cmpeq_epi32(vec_even_mask, _mm256_setzero_si256());\n\n        __m256i vec_result = _mm256_setzero_si256();\n        __m256i vec_factorial = _mm256_set1_epi32(1);\n        __m256i vec_sum = _mm256_set1_epi32(0);\n\n        for (int j = 1; j <= i+7; ++j) {\n            __m256i vec_j = _mm256_set1_epi32(j);\n            vec_factorial = _mm256_mullo_epi32(vec_factorial, vec_j);\n            vec_factorial = _mm256_and_si256(vec_factorial, vec_mod);\n            vec_sum = _mm256_add_epi32(vec_sum, vec_j);\n\n            __m256i vec_j_mask = _mm256_cmpgt_epi32(vec_j, vec_idx);\n            vec_factorial = _mm256_blendv_epi8(vec_factorial, _mm256_setzero_si256(), vec_j_mask);\n            vec_sum = _mm256_blendv_epi8(vec_sum, _mm256_setzero_si256(), vec_j_mask);\n\n            __m256i vec_even_result = _mm256_and_si256(vec_factorial, vec_even_mask);\n            __m256i vec_odd_result = _mm256_andnot_si256(vec_even_mask, vec_sum);\n            vec_result = _mm256_or_si256(vec_even_result, vec_odd_result);\n        }\n\n        int arr[8];\n        _mm256_storeu_si256((__m256i*)arr, vec_result);\n        for (int k = 0; k < 8; ++k) {\n            result[i - 1 + k] = arr[k];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (i % 2 == 0) {\n            int factorial = 1;\n            for (int j = 1; j <= i; ++j) {\n                factorial = (factorial * j) % mod;\n            }\n            result[i-1] = factorial;\n        } else {\n            int sum = (i * (i + 1)) / 2;\n            result[i-1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    const int vector_size = 8;\n    __m256i cmp_mask = _mm256_set1_epi32(n);\n    \n    for(int i = 1; i <= n; i += vector_size){\n        __m256i vec = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i cmp_res = _mm256_cmpgt_epi32(vec, cmp_mask);\n        __m256i mask = _mm256_andnot_si256(cmp_res, vec);\n        \n        // Check if palindrome\n        __m256i rev_vec = _mm256_shuffle_epi8(mask, _mm256_set_epi8(0xF,0xE,0xD,0xC,0xB,0xA,9,8,7,6,5,4,3,2,1,0,0xF,0xE,0xD,0xC,0xB,0xA,9,8));\n        __m256i is_palindrome = _mm256_cmpeq_epi32(mask, rev_vec);\n        \n        // Check if even or odd\n        __m256i mod2 = _mm256_and_si256(mask, _mm256_set1_epi32(1));\n        __m256i is_even = _mm256_cmpeq_epi32(mod2, _mm256_setzero_si256());\n        __m256i is_odd = _mm256_cmpeq_epi32(mod2, _mm256_set1_epi32(1));\n        \n        // Combine with palindrome check\n        __m256i final_even = _mm256_and_si256(is_palindrome, is_even);\n        __m256i final_odd = _mm256_and_si256(is_palindrome, is_odd);\n        \n        // Extract results\n        int temp_even[8], temp_odd[8];\n        _mm256_storeu_si256((__m256i*)temp_even, final_even);\n        _mm256_storeu_si256((__m256i*)temp_odd, final_odd);\n        \n        for(int j = 0; j < vector_size; ++j){\n            if(_mm256_testz_si256(_mm256_set1_epi32(-1), cmp_res) || (i + j > n)) continue;\n            if(temp_even[j]) even++;\n            if(temp_odd[j]) odd++;\n        }\n    }\n    return std::vector<int>{even, odd};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    const int step = 8;\n    __m256i cmp_mask = _mm256_set1_epi32(n);\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for(int i=1; i <= n; i += step){\n        __m256i vec = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mask = _mm256_cmpgt_epi32(vec, cmp_mask);\n        vec = _mm256_andnot_si256(mask, vec);\n\n        // Check palindrome property and even/odd\n        __m256i reversed_vec = _mm256_shuffle_epi8(vec, _mm256_set_epi8(\n            0xF,0xF,0xF,0xF,0xF,0xF,0xF,3,0xF,0xF,0xF,0xF,0xF,0xF,0xF,2,\n            0xF,0xF,0xF,0xF,0xF,0xF,0xF,1,0xF,0xF,0xF,0xF,0xF,0xF,0xF,0));\n        \n        __m256i is_palindrome = _mm256_cmpeq_epi32(vec, reversed_vec);\n        __m256i even_check = _mm256_and_si256(is_palindrome, _mm256_testn_epi32(vec, ones));\n        __m256i odd_check = _mm256_and_si256(is_palindrome, _mm256_test_epi32(vec, ones));\n\n        even += _mm256_popcnt_epi32(even_check);\n        odd += _mm256_popcnt_epi32(odd_check);\n    }\n\n    return std::vector<int>{even, odd};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    __m256i cmp_mask = _mm256_set1_epi32(n);\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    \n    for(int i=1; i <= n; i+=8){\n        __m256i vec_i = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mask = _mm256_cmpgt_epi32(vec_i, cmp_mask);\n        vec_i = _mm256_andnot_si256(mask, vec_i);\n\n        // Reverse digits and check palindrome\n        __m256i tens = _mm256_set_epi32(10000000,1000000,100000,10000,1000,100,10,1);\n        __m256i reversed = _mm256_setzero_si256();\n        __m256i remainder = vec_i;\n\n        for(int j=0; j<8; ++j){\n            __m256i digit = _mm256_cvtepi32_epi8(_mm256_div_epu32(remainder, tens));\n            reversed = _mm256_add_epi32(reversed, _mm256_mullo_epi32(digit, _mm256_shuffle_epi32(tens, _MM_SHUFFLE(j,j,j,j))));\n            remainder = _mm256_sub_epi32(remainder, _mm256_mullo_epi32(digit, tens));\n            tens = _mm256_slli_epi32(tens, 1);\n        }\n\n        __m256i is_palindrome = _mm256_cmpeq_epi32(vec_i, reversed);\n        __m256i even_mask = _mm256_and_si256(is_palindrome, _mm256_test_epi32(vec_i, ones));\n        __m256i odd_mask = _mm256_andnot_si256(even_mask, is_palindrome);\n\n        even += _mm256_popcnt_epi32(even_mask);\n        odd += _mm256_popcnt_epi32(odd_mask);\n    }\n    return std::vector<int>{even, odd};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    const int vec_size = 8;\n    __m256i cmp_mask = _mm256_set1_epi32(n);\n    \n    for(int i = 1; i <= n; i += vec_size){\n        __m256i vec_i = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i cmp_res = _mm256_cmpgt_epi32(vec_i, cmp_mask);\n        __m256i mask = _mm256_andnot_si256(cmp_res, _mm256_set1_epi32(0xFFFFFFFF));\n        \n        // Reverse digits and check palindrome\n        __m256i rev_vec = _mm256_set_epi32(\n            reverse_int(i+7), reverse_int(i+6), reverse_int(i+5), reverse_int(i+4),\n            reverse_int(i+3), reverse_int(i+2), reverse_int(i+1), reverse_int(i)\n        );\n        __m256i is_palindrome = _mm256_cmpeq_epi32(vec_i, rev_vec);\n        is_palindrome = _mm256_and_si256(is_palindrome, mask);\n\n        // Check even or odd\n        __m256i modulo_two = _mm256_and_si256(vec_i, _mm256_set1_epi32(1));\n        __m256i is_even = _mm256_cmpeq_epi32(modulo_two, _mm256_setzero_si256());\n        is_even = _mm256_and_si256(is_even, is_palindrome);\n        __m256i is_odd = _mm256_andnot_si256(is_even, is_palindrome);\n\n        // Extract results\n        int temp_even[vec_size], temp_odd[vec_size];\n        _mm256_storeu_si256((__m256i*)temp_even, is_even);\n        _mm256_storeu_si256((__m256i*)temp_odd, is_odd);\n\n        for(int j = 0; j < vec_size; ++j){\n            if(temp_even[j] == 0xFFFFFFFF) even++;\n            if(temp_odd[j] == 0xFFFFFFFF) odd++;\n        }\n    }\n    return {even, odd};\n}\n\ninline int reverse_int(int x){\n    int rev = 0;\n    while(x > 0){\n        rev = rev * 10 + (x % 10);\n        x /= 10;\n    }\n    return rev;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    const int step = 8;\n    __m256i cmp_mask = _mm256_set1_epi32(n);\n    \n    for(int i = 1; i <= n; i += step){\n        int upper_bound = (i + step - 1 > n) ? n - i + 1 : step;\n        \n        __m256i vec_i = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mask = _mm256_cmpgt_epi32(vec_i, cmp_mask);\n        vec_i = _mm256_andnot_si256(mask, vec_i);\n        \n        __m256i reversed_vec = _mm256_set_epi32(\n            reverse_int(i+7), reverse_int(i+6), reverse_int(i+5), reverse_int(i+4),\n            reverse_int(i+3), reverse_int(i+2), reverse_int(i+1), reverse_int(i)\n        );\n        \n        __m256i eq_mask = _mm256_cmpeq_epi32(vec_i, reversed_vec);\n        __m256i parity_mask = _mm256_and_si256(_mm256_srli_epi32(vec_i, 31), eq_mask);\n        \n        int count_eq = _mm256_popcnt_epi32(eq_mask);\n        int count_odd = _mm256_popcnt_epi32(parity_mask);\n        \n        even += count_eq - count_odd;\n        odd += count_odd;\n    }\n    return {even, odd};\n}\n\ninline int reverse_int(int x){\n    int rev = 0;\n    while(x > 0){\n        rev = rev * 10 + x % 10;\n        x /= 10;\n    }\n    return rev;\n}\n\ninline int _mm256_popcnt_epi32(__m256i v){\n    int res = 0;\n    alignas(32) int arr[8];\n    _mm256_store_si256((__m256i*)arr, v);\n    for(int i = 0; i < 8; ++i){\n        res += __builtin_popcount(arr[i]);\n    }\n    return res;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    const int size = n.size();\n    int count = 0;\n    const int simd_width = 8; // AVX2 can handle 8 integers at once (256 bits / 32 bits per int)\n    int aligned_size = size / simd_width * simd_width;\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    for(int i = 0; i < aligned_size; i += simd_width){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_vec = _mm256_andnot_si256(sign_mask, vec); // Absolute value\n        __m256i signs = _mm256_cmpgt_epi32(zero, vec); // Signs of original numbers\n\n        __m256i sum_digits = zero;\n        __m256i temp = abs_vec;\n\n        for(int j = 0; j < 9; ++j){\n            __m256i digits = _mm256_cvtepi32_epi8(_mm256_and_si256(temp, ten)); // Extract last digit\n            __m256i extended_signs = _mm256_cvtepi8_epi32(digits);\n            __m256i signed_digits = _mm256_sub_epi32(zero, extended_signs);\n            signed_digits = _mm256_blendv_epi8(signed_digits, digits, signs);\n            sum_digits = _mm256_add_epi32(sum_digits, signed_digits);\n            temp = _mm256_div_epi32(temp, ten); // Divide by 10\n        }\n\n        __m256i cmp_result = _mm256_cmpgt_epi32(sum_digits, zero);\n        count += _mm256_popcnt_epi32(cmp_result);\n    }\n\n    for(int i = aligned_size; i < size; ++i){\n        int sum = 0;\n        int num = n[i];\n        bool is_negative = num < 0;\n        num = std::abs(num);\n        while(num > 0){\n            int digit = num % 10;\n            if(is_negative && num >= 10) digit *= -1;\n            sum += digit;\n            num /= 10;\n        }\n        if(sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    int count = 0;\n    size_t size = n.size();\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(&n[i]));\n        __m256i abs_vec = _mm256_andnot_si256(sign_mask, vec);\n        __m256i rem = vec;\n        __m256i sum = _mm256_setzero_si256();\n\n        for(int j = 0; j < 10; ++j){\n            __m256i digit = _mm256_srai_epi32(_mm256_and_si256(rem, sign_mask), 31);\n            __m256i mod = _mm256_abs_epi32(_mm256_sub_epi32(rem, _mm256_mullo_epi32(_mm256_set1_epi32(10), _mm256_div_epi32(rem, ten))));\n            digit = _mm256_or_si256(digit, _mm256_andnot_si256(sign_mask, mod));\n            sum = _mm256_add_epi32(sum, digit);\n            rem = _mm256_div_epi32(rem, ten);\n        }\n\n        __m256i cmp = _mm256_cmpgt_epi32(sum, zero);\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(cmp));\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        int sum = 0;\n        int num = n[i];\n        if(num < 0){\n            sum += -((num / 10) < 0 ? -(num / 10) % 10 : (num / 10) % 10);\n            num = -num % 10;\n        }\n        while(num){\n            sum += num % 10;\n            num /= 10;\n        }\n        if(sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    const int size = n.size();\n    int count = 0;\n    const int simd_width = 8; // AVX2 operates on 256 bits, so 8 integers at a time\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    for(int i = 0; i < size; i += simd_width){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i abs_vec = _mm256_andnot_si256(sign_mask, vec); // Absolute value\n        __m256i signs = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec); // Signs (-1 for negative, 0 otherwise)\n\n        __m256i sum_digits = zero;\n        for(int j = 0; j < 4; ++j){\n            __m256i remainder = _mm256_abs_epi32(_mm256_srai_epi32(vec, 31 & (j * 8))); // Extract signed digits\n            __m256i digit = _mm256_and_si256(remainder, ten);\n            digit = _mm256_sub_epi32(digit, ten);\n            digit = _mm256_add_epi32(digit, _mm256_blendv_epi8(zero, ten, signs));\n            sum_digits = _mm256_add_epi32(sum_digits, digit);\n            vec = _mm256_sub_epi32(vec, digit);\n            vec = _mm256_mullo_epi32(vec, _mm256_set1_epi32(0xCCCCCCCD)); // Approximate division by 10\n        }\n\n        __m256i cmp = _mm256_cmpgt_epi32(sum_digits, zero);\n        int mask = _mm256_movemask_epi8(cmp);\n        count += __builtin_popcount(mask) / 4;\n    }\n\n    for(int i = ((size / simd_width) * simd_width); i < size; ++i){\n        int num = n[i];\n        int sum = 0;\n        if(num < 0){\n            sum -= (num / 10 == 0 ? 0 : num / 10);\n            num = -num % 10;\n        }\n        while(num > 0){\n            sum += num % 10;\n            num /= 10;\n        }\n        if(sum > 0) count++;\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    int count = 0;\n    size_t size = n.size();\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(&n[i]));\n        __m256i abs_vec = _mm256_andnot_si256(sign_mask, vec);\n        __m256i quotient = _mm256_abs_epi32(vec);\n        __m256i sum = _mm256_setzero_si256();\n\n        for(int j = 0; j < 4; ++j){\n            __m256i remainder = _mm256_srai_epi32(quotient, 8 * j);\n            __m256i digit = _mm256_and_si256(remainder, _mm256_set1_epi32(0xFF));\n            sum = _mm256_add_epi32(sum, digit);\n        }\n\n        __m256i cmp_result = _mm256_cmpgt_epi32(sum, zero);\n        count += _mm256_popcnt_epi64(_mm256_movemask_epi8(cmp_result)) / 4;\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        int num = n[i];\n        int sum = 0;\n        bool is_negative = false;\n        if(num < 0){\n            is_negative = true;\n            num = -num;\n        }\n        while(num > 0){\n            int digit = num % 10;\n            if(is_negative && (num / 10 == 0)){\n                digit = -digit;\n            }\n            sum += digit;\n            num /= 10;\n        }\n        if(sum > 0){\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    int count = 0;\n    const int size = n.size();\n    const int aligned_size = size - (size % 8);\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i sign_mask = _mm256_set1_epi32(0x80000000);\n\n    for(int i = 0; i < aligned_size; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i abs_vec = _mm256_andnot_si256(sign_mask, vec);\n        __m256i neg_mask = _mm256_cmpgt_epi32(zero, vec);\n\n        __m256i sum_digits = _mm256_setzero_si256();\n\n        for(int j = 0; j < 4; ++j){\n            __m256i remainder = _mm256_abs_epi32(_mm256_srai_epi32(abs_vec, j * 8));\n            __m256i digit = _mm256_and_si256(remainder, ten);\n            digit = _mm256_sub_epi32(digit, ten);\n            digit = _mm256_add_epi32(digit, ten);\n            sum_digits = _mm256_add_epi32(sum_digits, digit);\n            abs_vec = _mm256_sub_epi32(abs_vec, digit);\n        }\n\n        sum_digits = _mm256_sub_epi32(sum_digits, _mm256_and_si256(neg_mask, ten));\n        __m256i cmp_result = _mm256_cmpgt_epi32(sum_digits, zero);\n        count += _mm256_movemask_epi8(cmp_result) & 0xFF;\n    }\n\n    for(int i = aligned_size; i < size; ++i){\n        int num = n[i];\n        int sum = 0;\n        bool is_negative = false;\n        if(num < 0){\n            is_negative = true;\n            num = -num;\n        }\n        while(num > 0){\n            int digit = num % 10;\n            sum += (is_negative && digit != 0) ? -digit : digit;\n            num /= 10;\n        }\n        if(sum > 0) count++;\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Find the index where the array breaks the non-decreasing order\n    int break_point = -1;\n    for(int i = 0; i < N; ++i){\n        if(arr[i] > arr[(i+1)%N]){\n            break_point = i;\n            break;\n        }\n    }\n\n    if(break_point == -1) return true;\n\n    // Check if the array can be sorted by right shifts\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i prev = _mm256_set1_epi32(arr[(break_point + 1) % N]);\n\n    for(int i = 1; i < N; ++i){\n        int current_idx = (break_point + 1 + i) % N;\n        __m256i current = _mm256_set1_epi32(arr[current_idx]);\n        __m256i cmp_result = _mm256_cmpgt_epi32(prev, current);\n        int check = _mm256_movemask_epi8(cmp_result);\n        if(check != 0) return false;\n        prev = current;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    const int N = arr.size();\n    if (N == 0) return true;\n\n    // Create a new vector with double the size to simulate circular buffer\n    std::vector<int> extended_arr(2 * N);\n    for(int i = 0; i < 2 * N; ++i){\n        extended_arr[i] = arr[i % N];\n    }\n\n    int drop_count = 0;\n    int max_drop_count = 0;\n\n    __m256i prev_vec = _mm256_set1_epi32(extended_arr[0]);\n    for(int i = 1; i < 2 * N; i += 8){\n        __m256i current_vec = _mm256_loadu_si256((__m256i const*)(extended_arr.data() + i));\n        __m256i cmp_result = _mm256_cmpgt_epi32(prev_vec, current_vec);\n\n        int mask = _mm256_movemask_epi8(cmp_result);\n        int count = __builtin_popcount(mask);\n\n        drop_count += count;\n        if(drop_count > max_drop_count){\n            max_drop_count = drop_count;\n        }\n        if(count == 0){\n            drop_count = 0;\n        }\n\n        prev_vec = current_vec;\n    }\n\n    return max_drop_count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if (n == 0) return true;\n\n    // Find the index where the array breaks the non-decreasing order\n    int break_count = 0;\n    int break_index = -1;\n\n    const int simd_width = 8; // AVX2 can handle 8 integers at once for 256-bit registers\n    int i = 0;\n\n    __m256i cmp_mask, curr_vec, next_vec;\n\n    for (; i <= n - simd_width; i += simd_width) {\n        curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        next_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i + 1]));\n\n        cmp_mask = _mm256_cmpgt_epi32(next_vec, curr_vec); // Compare if next > current\n        int temp_cmp = _mm256_movemask_ps(_mm256_castsi256_ps(cmp_mask)); // Get bitmask\n\n        if (temp_cmp != 0xFF) { // If not all are in order\n            for (int j = 0; j < simd_width; ++j) {\n                if ((temp_cmp & (1 << j)) == 0) { // Check which element breaks order\n                    break_index = i + j;\n                    break_count++;\n                    if (break_count > 1) return false;\n                }\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n - 1; ++i) {\n        if (arr[i] > arr[i + 1]) {\n            break_index = i;\n            break_count++;\n            if (break_count > 1) return false;\n        }\n    }\n\n    // Check wrap-around condition\n    if (break_count == 0) return true;\n    if (break_count == 1 && arr[n - 1] <= arr[0]) return true;\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    if (arr.empty()) return true;\n\n    int N = arr.size();\n    std::vector<int> arr_extended = arr;\n    arr_extended.insert(arr_extended.end(), arr.begin(), arr.end());\n\n    __m256i cmp_mask = _mm256_set1_epi32(0xFFFFFFFF);\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for(int i = 0; i < N; ++i){\n        int count = 0;\n        bool valid = true;\n\n        for(int j = 0; j < N - 1 && valid; j += 8){\n            __m256i vec_curr = _mm256_loadu_si256((__m256i const*)(arr_extended.data() + i + j));\n            __m256i vec_next = _mm256_loadu_si256((__m256i const*)(arr_extended.data() + i + j + 1));\n\n            __m256i cmp_result = _mm256_cmpgt_epi32(vec_next, vec_curr);\n            __m256i mask = _mm256_cmpeq_epi32(cmp_result, cmp_mask);\n\n            if(_mm256_movemask_epi8(mask) != 0xFFFF){\n                valid = false;\n            }\n        }\n\n        if(valid){\n            for(int j = N - 1 - (N % 8); j < N - 1 && valid; ++j){\n                if(arr_extended[i + j] > arr_extended[i + j + 1]){\n                    valid = false;\n                }\n            }\n        }\n\n        if(valid){\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    size_t N = arr.size();\n    if (N == 0) return true;\n\n    // Find the minimum element and its index\n    int min_val = arr[0];\n    size_t min_idx = 0;\n    for(size_t i = 1; i < N; ++i){\n        if(arr[i] < min_val){\n            min_val = arr[i];\n            min_idx = i;\n        }\n    }\n\n    // Rotate the array logically to start from the minimum element\n    __m256i mask = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n    size_t rotations = N - min_idx;\n    if(rotations == N) rotations = 0;\n\n    // Prepare the rotated array in SIMD registers\n    for(size_t i = 0; i < N; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i const*)(&arr[i]));\n        __m256i aligned_vec = _mm256_alignr_epi8(vec, vec, rotations * 4);\n        \n        // Check if the sequence is non-decreasing\n        __m256i prev = _mm256_srli_si256(aligned_vec, 4);\n        __m256i cmp_res = _mm256_cmpgt_epi32(prev, aligned_vec);\n\n        // If any comparison is true, then it's not sorted\n        if(_mm256_movemask_epi8(cmp_res) != 0){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    size_t N1 = lst1.size();\n    size_t N2 = lst2.size();\n\n    // Process lst1 to count odd numbers\n    for(size_t i = 0; i + 7 < N1; i += 8){\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i and_res = _mm256_and_si256(v, ones);\n        __m256i cmp_res = _mm256_cmpgt_epi32(and_res, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp_res);\n        odd_count_lst1 += __builtin_popcount(mask);\n    }\n    for(size_t i = (N1 / 8) * 8; i < N1; ++i){\n        if(lst1[i] % 2 != 0) odd_count_lst1 += 1;\n    }\n\n    // Process lst2 to count even numbers\n    for(size_t i = 0; i + 7 < N2; i += 8){\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i and_res = _mm256_andnot_si256(ones, v);\n        __m256i cmp_res = _mm256_cmpgt_epi32(and_res, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp_res);\n        even_count_lst2 += __builtin_popcount(mask);\n    }\n    for(size_t i = (N2 / 8) * 8; i < N2; ++i){\n        if(lst2[i] % 2 == 0) even_count_lst2 += 1;\n    }\n\n    return (even_count_lst2 >= odd_count_lst1) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    size_t N1 = lst1.size();\n    size_t N2 = lst2.size();\n\n    // Process lst1 to count odd numbers\n    for(size_t i = 0; i + 8 <= N1; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mods = _mm256_and_si256(vec, ones);\n        __m256i cmp = _mm256_cmpeq_epi32(mods, ones);\n        int mask = _mm256_movemask_epi8(cmp);\n        odd_count_lst1 += __builtin_popcount(mask);\n    }\n    for(size_t i = (N1 / 8) * 8; i < N1; ++i){\n        if(lst1[i] % 2 != 0) odd_count_lst1 += 1;\n    }\n\n    // Process lst2 to count even numbers\n    for(size_t i = 0; i + 8 <= N2; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i mods = _mm256_and_si256(vec, ones);\n        __m256i cmp = _mm256_cmpeq_epi32(mods, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp);\n        even_count_lst2 += __builtin_popcount(mask);\n    }\n    for(size_t i = (N2 / 8) * 8; i < N2; ++i){\n        if(lst2[i] % 2 == 0) even_count_lst2 += 1;\n    }\n\n    return (even_count_lst2 >= odd_count_lst1) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    // Count odd numbers in lst1\n    for(size_t i = 0; i < lst1.size(); i += 8){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i mask = _mm256_and_si256(data, _mm256_set1_epi32(1));\n        __m256i cmp = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256());\n        int temp = _mm256_movemask_epi8(cmp);\n        odd_count_lst1 += __builtin_popcount(~temp);\n    }\n\n    // Count even numbers in lst2\n    for(size_t i = 0; i < lst2.size(); i += 8){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i mask = _mm256_and_si256(data, _mm256_set1_epi32(1));\n        __m256i cmp = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256());\n        int temp = _mm256_movemask_epi8(cmp);\n        even_count_lst2 += __builtin_popcount(temp);\n    }\n\n    if(even_count_lst2 >= odd_count_lst1){\n        return \"YES\";\n    }\n    else{\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    size_t n1 = lst1.size();\n    size_t n2 = lst2.size();\n\n    // Process lst1 to count odd numbers\n    for (size_t i = 0; i + 7 < n1; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i and_res = _mm256_and_si256(vec, ones);\n        __m256i cmp_res = _mm256_cmpgt_epi32(and_res, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp_res);\n        odd_count_lst1 += __builtin_popcount(mask);\n    }\n    for (size_t i = (n1 / 8) * 8; i < n1; ++i) {\n        if (lst1[i] % 2 != 0) odd_count_lst1++;\n    }\n\n    // Process lst2 to count even numbers\n    for (size_t i = 0; i + 7 < n2; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i and_res = _mm256_and_si256(vec, ones);\n        __m256i cmp_res = _mm256_cmpeq_epi32(and_res, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(cmp_res);\n        even_count_lst2 += __builtin_popcount(mask);\n    }\n    for (size_t i = (n2 / 8) * 8; i < n2; ++i) {\n        if (lst2[i] % 2 == 0) even_count_lst2++;\n    }\n\n    return (even_count_lst2 >= odd_count_lst1) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int count_odd_lst1 = 0;\n    int count_even_lst2 = 0;\n\n    size_t N1 = lst1.size();\n    size_t N2 = lst2.size();\n\n    // Process lst1 to count odd numbers\n    for (size_t i = 0; i + 7 < N1; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i mask = _mm256_and_si256(v, _mm256_set1_epi32(1));\n        __m256i cmp = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256());\n        int bits = _mm256_movemask_epi8(cmp);\n        count_odd_lst1 += __builtin_popcount(~bits);\n    }\n    for (size_t i = (N1 / 8) * 8; i < N1; ++i) {\n        if (lst1[i] % 2 != 0) count_odd_lst1++;\n    }\n\n    // Process lst2 to count even numbers\n    for (size_t i = 0; i + 7 < N2; i += 8) {\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n        __m256i mask = _mm256_and_si256(v, _mm256_set1_epi32(1));\n        __m256i cmp = _mm256_cmpeq_epi32(mask, _mm256_setzero_si256());\n        int bits = _mm256_movemask_epi8(cmp);\n        count_even_lst2 += __builtin_popcount(bits);\n    }\n    for (size_t i = (N2 / 8) * 8; i < N2; ++i) {\n        if (lst2[i] % 2 == 0) count_even_lst2++;\n    }\n\n    return count_even_lst2 >= count_odd_lst1 ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    const __m256i mask = _mm256_set1_epi8(0x01);\n    const __m256i ones = _mm256_set1_epi8(0x31); // '1'\n    const __m256i zeros = _mm256_setzero_si256();\n    std::vector<std::string> result;\n\n    for (const auto& str : lst) {\n        int count = 0;\n        size_t i = 0;\n        size_t len = str.length();\n\n        for (; i + 31 < len; i += 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&str[i]));\n            __m256i cmp_res = _mm256_cmpgt_epi8(_mm256_and_si256(data, mask), zeros);\n            __m256i temp = _mm256_sub_epi8(ones, cmp_res);\n            __m256i sum = _mm256_sad_epu8(temp, zeros);\n            count += _mm256_extract_epi16(sum, 0) / 256;\n        }\n\n        for (; i < len; ++i) {\n            if ((str[i] - '0') & 1) {\n                count++;\n            }\n        }\n\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(count);\n        output += \"n the str\";\n        output += std::to_string(count);\n        output += \"ng \";\n        output += std::to_string(count);\n        output += \" of the \";\n        output += std::to_string(count);\n        output += \"nput.\";\n        result.push_back(output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    const __m256i ones = _mm256_set1_epi8('1');\n    const __m256i zeros = _mm256_setzero_si256();\n    const __m256i mask = _mm256_set1_epi8(0x0F);\n    const char format[] = \"the number of odd elements %dn the str%ding %d of the %dnput.\";\n    \n    for (const auto& s : lst) {\n        int count = 0;\n        size_t i = 0;\n        size_t len = s.size();\n        \n        for (; i + 31 < len; i += 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n            __m256i low_nibbles = _mm256_and_si256(data, mask);\n            __m256i cmp_res = _mm256_cmpeq_epi8(low_nibbles, ones);\n            __m256i packed = _mm256-packuswb_epi16(cmp_res, zeros);\n            __m128i sum = _mm256_extracti128_si256(packed, 0);\n            sum = _mm_add_epi8(sum, _mm256_extracti128_si256(packed, 1));\n            int temp[16];\n            _mm_storeu_si128((__m128i*)temp, sum);\n            for (int j = 0; j < 16; ++j) {\n                count += temp[j] >> 7;\n            }\n        }\n        \n        for (; i < len; ++i) {\n            if ((s[i] - '0') & 1) {\n                count++;\n            }\n        }\n        \n        char output[100];\n        snprintf(output, sizeof(output), format, count, count, count, count);\n        result.emplace_back(output);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    const char* format = \"the number of odd elements %dn the str%ding %d of the %dnput.\";\n    char buffer[256];\n    \n    for (const auto& s : lst) {\n        int count = 0;\n        size_t len = s.length();\n        \n        // Handle unaligned start\n        size_t i = 0;\n        if (len >= 32) {\n            __m256i zero = _mm256_set1_epi8('0');\n            __m256i eight = _mm256_set1_epi8('8');\n            __m256i two = _mm256_set1_epi8('2');\n            \n            for (; i <= len - 32; i += 32) {\n                __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n                __m256i sub = _mm256_sub_epi8(data, zero);\n                __m256i and_v = _mm256_and_si256(sub, two);\n                __m256i cmp = _mm256_cmpeq_epi8(and_v, two);\n                count += _mm256_popcnt_epi64(cmp);\n            }\n        }\n        \n        // Handle remaining characters\n        for (; i < len; ++i) {\n            if ((s[i] - '0') & 1) {\n                count++;\n            }\n        }\n        \n        // Construct result string\n        snprintf(buffer, sizeof(buffer), format, count, count, count, count);\n        result.emplace_back(buffer);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    const __m256i ones = _mm256_set1_epi8('1');\n    const __m256i threes = _mm256_set1_epi8('3');\n    const __m256i fives = _mm256_set1_epi8('5');\n    const __m256i sevens = _mm256_set1_epi8('7');\n    const __m256i nines = _mm256_set1_epi8('9');\n    const __m256i zero = _mm256_set1_epi8('0');\n    \n    std::vector<std::string> result;\n    result.reserve(lst.size());\n    \n    for(const auto& str : lst){\n        size_t count = 0;\n        size_t len = str.length();\n        size_t vec_len = len / 32 * 32;\n        \n        const unsigned char* data = (const unsigned char*)str.c_str();\n        \n        for(size_t i=0; i<vec_len; i+=32){\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n            __m256i mask = _mm256_or_si256(_mm256_cmpeq_epi8(vec, ones),\n                                          _mm256_or_si256(_mm256_cmpeq_epi8(vec, threes),\n                                          _mm256_or_si256(_mm256_cmpeq_epi8(vec, fives),\n                                          _mm256_or_si256(_mm256_cmpeq_epi8(vec, sevens),\n                                                        _mm256_cmpeq_epi8(vec, nines))));\n            count += _mm256_popcnt_epi64(_mm256_movemask_epi8(mask));\n        }\n        \n        for(size_t i=vec_len; i<len; ++i){\n            if(data[i] == '1' || data[i] == '3' || data[i] == '5' || data[i] == '7' || data[i] == '9'){\n                count++;\n            }\n        }\n        \n        std::string output = \"the number of odd elements \";\n        output += std::to_string(count);\n        output += \"n the str\";\n        output += std::to_string(count);\n        output += \"ng \";\n        output += std::to_string(count);\n        output += \" of the \";\n        output += std::to_string(count);\n        output += \"nput.\";\n        result.push_back(output);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    const __m256i mask_odd = _mm256_set1_epi8(0x01);\n    const __m256i vec_zero = _mm256_setzero_si256();\n    const char* format = \"the number of odd elements %dn the str%ding %d of the %dnput.\";\n    std::vector<std::string> result;\n\n    for (const auto& s : lst) {\n        size_t count = 0;\n        size_t len = s.size();\n        for (size_t i = 0; i < len; i += 32) {\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n            __m256i sub = _mm256_sub_epi8(data, _mm256_set1_epi8('0'));\n            __m256i and_res = _mm256_and_si256(sub, mask_odd);\n            __m256i cmp = _mm256_cmpeq_epi8(and_res, mask_odd);\n            __m256i sum = _mm256_sad_epu8(cmp, vec_zero);\n            uint32_t temp_sum[8];\n            _mm256_storeu_si256((__m256i*)temp_sum, sum);\n            count += temp_sum[0] / 255;\n        }\n        char buffer[100];\n        snprintf(buffer, sizeof(buffer), format, count, count, count, count);\n        result.emplace_back(buffer);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int vec_size = 4;\n    __m256i current_min = _mm256_set1_epi64x(nums[0]);\n    __m256i global_min = _mm256_set1_epi64x(nums[0]);\n\n    for(size_t i = 1; i + vec_size <= nums.size(); i += vec_size){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i prev_min = _mm256_add_epi64(vec, current_min);\n        __m256i new_current_min = _mm256_min_epi64(prev_min, vec);\n\n        current_min = new_current_min;\n        global_min = _mm256_min_epi64(global_min, current_min);\n    }\n\n    // Horizontal min for the remaining vector\n    alignas(32) int64_t temp[4];\n    _mm256_store_si256((__m256i*)temp, global_min);\n    int64_t result = temp[0];\n    for(int i = 1; i < vec_size; ++i){\n        if(temp[i] < result) result = temp[i];\n    }\n\n    // Process leftover elements\n    for(size_t i = ((nums.size() / vec_size) * vec_size); i < nums.size(); ++i){\n        int64_t curr = nums[i];\n        result = std::min(result, curr);\n        curr = std::min(curr, result);\n        result = std::min(result, curr);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int vec_size = 4;\n    const int num_elements = nums.size();\n    int64_t min_sum = INT64_MAX;\n    int64_t current_min = 0;\n\n    __m256i v_min_sum = _mm256_set1_epi64x(INT64_MAX);\n    __m256i v_current_min = _mm256_setzero_si256();\n\n    for(int i = 0; i + vec_size <= num_elements; i += vec_size){\n        __m256i v_nums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        v_current_min = _mm256_add_epi64(v_nums, _mm256_min_epi64(v_current_min, v_nums));\n        v_min_sum = _mm256_min_epi64(v_min_sum, v_current_min);\n    }\n\n    // Horizontal min for the vector\n    __m128i v_min_low = _mm256_extracti128_si256(v_min_sum, 0);\n    __m128i v_min_high = _mm256_extracti128_si256(v_min_sum, 1);\n    v_min_low = _mm_min_epi64(v_min_low, v_min_high);\n    int64_t min_val[2];\n    _mm_storeu_si128((__m128i*)min_val, v_min_low);\n    min_sum = std::min({min_sum, min_val[0], min_val[1]});\n\n    // Handle remaining elements\n    for(int i = ((num_elements / vec_size) * vec_size); i < num_elements; ++i){\n        current_min = std::min((int64_t)nums[i], current_min + nums[i]);\n        min_sum = std::min(min_sum, current_min);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int len = nums.size();\n    const int vec_len = 4; // Number of elements in a YMM register for int64_t\n    const int simd_blocks = len / vec_len;\n    const int remainder = len % vec_len;\n\n    __m256i current_min_vec = _mm256_set1_epi64x(INT64_MAX);\n    __m256i global_min_vec = _mm256_set1_epi64x(INT64_MAX);\n    __m256i temp_vec;\n\n    for(int i = 0; i < simd_blocks * vec_len; i += vec_len){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        current_min_vec = _mm256_add_epi64(data, current_min_vec);\n        current_min_vec = _mm256_min_epi64(current_min_vec, data);\n        global_min_vec = _mm256_min_epi64(global_min_vec, current_min_vec);\n    }\n\n    // Horizontal min for the vector\n    temp_vec = _mm256_permute4x64_epi64(global_min_vec, 0xE); // Swap high and low dwords\n    global_min_vec = _mm256_min_epi64(global_min_vec, temp_vec);\n    temp_vec = _mm256_shuffle_epi32(global_min_vec, 0x1B); // Swap adjacent lanes\n    global_min_vec = _mm256_min_epi64(global_min_vec, temp_vec);\n\n    int64_t global_min = _mm256_extract_epi64(global_min_vec, 0);\n\n    // Process remaining elements\n    int64_t current_min = INT64_MAX;\n    int64_t local_min = INT64_MAX;\n    for(int i = simd_blocks * vec_len; i < len; ++i){\n        current_min = std::min(nums[i], current_min + nums[i]);\n        local_min = std::min(local_min, current_min);\n    }\n    global_min = std::min(global_min, local_min);\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int len = nums.size();\n    const int vec_len = 4; // Number of elements processed per SIMD operation\n    const int num_iters = len / vec_len;\n    int64_t remainder = len % vec_len;\n\n    __m256i current_min_vec = _mm256_set1_epi64x(INT64_MAX);\n    __m256i global_min_vec = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_sum_vec = _mm256_setzero_si256();\n\n    for(int i = 0; i < num_iters * vec_len; i += vec_len){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        current_sum_vec = _mm256_add_epi64(current_sum_vec, vec);\n        __m256i temp_min = _mm256_min_epi64(current_sum_vec, vec);\n        current_min_vec = _mm256_min_epi64(current_min_vec, temp_min);\n        global_min_vec = _mm256_min_epi64(global_min_vec, current_min_vec);\n        current_sum_vec = _mm256_min_epi64(current_sum_vec, vec);\n    }\n\n    int64_t global_min = INT64_MAX;\n    __m256i res = _mm256_permute4x64_epi64(global_min_vec, 0x00);\n    global_min = std::min(global_min, _mm256_extract_epi64(res, 0));\n    res = _mm256_permute4x64_epi64(global_min_vec, 0x01);\n    global_min = std::min(global_min, _mm256_extract_epi64(res, 0));\n    res = _mm256_permute4x64_epi64(global_min_vec, 0x02);\n    global_min = std::min(global_min, _mm256_extract_epi64(res, 0));\n    res = _mm256_permute4x64_epi64(global_min_vec, 0x03);\n    global_min = std::min(global_min, _mm256_extract_epi64(res, 0));\n\n    for(int i = num_iters * vec_len; i < len; ++i){\n        int64_t current_sum = nums[i];\n        if(current_sum < global_min){\n            global_min = current_sum;\n        }\n        current_sum += global_min;\n        if(current_sum < global_min){\n            global_min = current_sum;\n        }\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int vec_size = 4;\n    __m256i current_min = _mm256_set1_epi64x(nums[0]);\n    __m256i global_min = _mm256_set1_epi64x(nums[0]);\n\n    for(size_t i = 1; i + vec_size <= nums.size(); i += vec_size){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        current_min = _mm256_add_epi64(current_min, vec);\n        __m256i reset_check = _mm256_cmpgt_epi64(vec, current_min);\n        current_min = _mm256_blendv_epi8(current_min, vec, reset_check);\n        global_min = _mm256_min_epi64(global_min, current_min);\n    }\n\n    // Horizontal min for the remaining vector\n    int64_t min_vals[4];\n    _mm256_storeu_si256((__m256i*)min_vals, global_min);\n\n    int64_t result = min_vals[0];\n    for(int i = 1; i < vec_size; ++i){\n        result = std::min(result, min_vals[i]);\n    }\n\n    // Handle leftover elements\n    for(size_t i = nums.size() - nums.size() % vec_size; i < nums.size(); ++i){\n        int64_t current = nums[i];\n        result = std::min(result, current);\n        current_min = _mm256_set1_epi64x(std::min(current + _mm256_extract_epi64(current_min, 0), current));\n        global_min = _mm256_set1_epi64x(std::min(_mm256_extract_epi64(global_min, 0), _mm256_extract_epi64(current_min, 0)));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    const int vec_size = 256 / 8; // 32 bytes / 8 bits per byte = 32 elements\n    int total_drops = 0;\n\n    for (const auto& row : grid) {\n        int sum = 0;\n        const int* data = row.data();\n        int size = row.size();\n\n        // Handle full AVX vectors\n        for (int i = 0; i <= size - vec_size; i += vec_size) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n            __m256i ones = _mm256_set1_epi8(-1); // All ones\n            __m256i cmp = _mm256_cmpeq_epi8(vec, ones); // Compare each byte to 1\n            __m256i mask = _mm256_and_si256(cmp, ones); // Create mask of 1s and 0s\n            int popcnt = _mm256_popcnt_epi64(mask); // Population count (number of 1s)\n            sum += popcnt;\n        }\n\n        // Handle remaining elements\n        for (int i = (size / vec_size) * vec_size; i < size; ++i) {\n            sum += data[i];\n        }\n\n        total_drops += (sum + capacity - 1) / capacity;\n    }\n\n    return total_drops;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    const int vec_size = 256 / 8; // 32 bytes / 8 bits per byte\n    int total_drops = 0;\n    \n    for(const auto& row : grid){\n        int sum = 0;\n        int leftover = row.size() % vec_size;\n        int vec_iters = row.size() / vec_size;\n        \n        const int* row_data = row.data();\n        \n        __m256i vsum = _mm256_setzero_si256();\n        __m256i ones = _mm256_set1_epi8(-1);\n        \n        for(int i = 0; i < vec_iters; ++i){\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row_data + i * vec_size));\n            __m256i cmp = _mm256_cmpeq_epi8(data, ones);\n            vsum = _mm256_add_epi8(vsum, cmp);\n        }\n        \n        uint32_t intermediate_sum = 0;\n        __m256i vsum_lo = _mm256_extracti128_si256(vsum, 0);\n        __m256i vsum_hi = _mm256_extracti128_si256(vsum, 1);\n        \n        intermediate_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(vsum_lo));\n        intermediate_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(_mm_srli_si128(vsum_lo, 4)));\n        intermediate_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(vsum_hi));\n        intermediate_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(_mm_srli_si128(vsum_hi, 4)));\n        \n        sum += intermediate_sum;\n        \n        if(leftover > 0){\n            __m256i tail = _mm256_setzero_si256();\n            tail = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(row_data + vec_iters * vec_size));\n            __m256i mask = _mm256_set_epi8(0,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,\n                                           0,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1);\n            tail = _mm256_and_si256(tail, mask);\n            __m256i cmp = _mm256_cmpeq_epi8(tail, ones);\n            vsum = cmp;\n            \n            uint32_t tail_sum = 0;\n            __m128i vsum_lo = _mm256_extracti128_si256(vsum, 0);\n            __m128i vsum_hi = _mm256_extracti128_si256(vsum, 1);\n            \n            tail_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(vsum_lo));\n            tail_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(_mm_srli_si128(vsum_lo, 4)));\n            tail_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(vsum_hi));\n            tail_sum += _mm_popcnt_u32(_mm_cvtsi128_si32(_mm_srli_si128(vsum_hi, 4)));\n            \n            sum += tail_sum;\n        }\n        \n        total_drops += (sum + capacity - 1) / capacity;\n    }\n    \n    return total_drops;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    if (grid.empty() || grid[0].empty()) return 0;\n\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    int total_water = 0;\n\n    const int simd_width = 256 / 8; // AVX2: 32 bytes -> 32 units of water\n    const __m256i ones = _mm256_set1_epi8(-1); // All bits set to 1\n    const __m256i zero = _mm256_setzero_si256();\n\n    for(int r = 0; r < rows; ++r){\n        int row_sum = 0;\n        int c = 0;\n\n        // Process in chunks of 32\n        for(; c <= cols - simd_width; c += simd_width){\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[r][c]));\n            __m256i mask = _mm256_cmpeq_epi8(vec, ones);\n            int count = _mm256_popcnt_epi64(_mm256_movemask_epi8(mask));\n            row_sum += count;\n        }\n\n        // Tail processing\n        for(; c < cols; ++c){\n            row_sum += grid[r][c];\n        }\n\n        total_water += row_sum;\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    const int rows = grid.size();\n    if (rows == 0) return 0;\n    const int cols = grid[0].size();\n    int total_drops = 0;\n\n    // Process in chunks of 8 using AVX2\n    const int simd_width = 8;\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i cap = _mm256_set1_epi32(capacity);\n\n    for (int i = 0; i < rows; ++i) {\n        int sum = 0;\n        for (int j = 0; j < cols; j += simd_width) {\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[i][j]));\n            __m256i mask = _mm256_cmpeq_epi32(vec, ones);\n            __m256i popcnt = _mm256_popcnt_epi32(mask);\n            int partial_sum = _mm256_extract_epi32(_mm256_hadd_epi32(popcnt, popcnt), 0);\n            sum += partial_sum;\n        }\n        // Handle remaining elements\n        for (int j = ((cols / simd_width) * simd_width); j < cols; ++j) {\n            sum += grid[i][j];\n        }\n        total_drops += (sum + capacity - 1) / capacity;\n    }\n\n    return total_drops;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    const int rows = grid.size();\n    if (rows == 0) return 0;\n    const int cols = grid[0].size();\n    int total_units = 0;\n\n    const int simd_width = 256 / 8; // AVX2: 32 bytes -> 32 units per vector\n    __m256i ones = _mm256_set1_epi8(-1); // All bits set to 1\n    __m256i zero = _mm256_setzero_si256();\n\n    for(int r = 0; r < rows; ++r){\n        const int *data = reinterpret_cast<const int*>(&grid[r][0]);\n        int remainder = cols % simd_width;\n        int aligned_cols = cols - remainder;\n\n        int sum = 0;\n        for(int c = 0; c < aligned_cols; c += simd_width){\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + c));\n            __m256i cmp = _mm256_cmpeq_epi8(vec, ones);\n            __m256i mask = _mm256_and_si256(cmp, _mm256_set1_epi8(1));\n            int count = _mm256_extract_epi32(_mm256_movemask_epi8(mask), 0);\n            sum += __builtin_popcount(count);\n        }\n\n        // Handle remaining elements\n        for(int c = aligned_cols; c < cols; ++c){\n            sum += data[c];\n        }\n\n        total_units += sum;\n    }\n\n    return (total_units + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    if(lst.size() == 0) return sum;\n\n    const int len = lst.size();\n    const int aligned_len = len - (len % 8);\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i mask = _mm256_setr_epi16(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n    const int16_t* data = lst.data();\n\n    for(int i = 0; i < aligned_len; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i even_pos = _mm256_and_si256(vec, mask);\n        __m256i is_odd = _mm256_cmpgt_epi16(_mm256_and_si256(even_pos, _mm256_set1_epi16(1)), _mm256_setzero_si256());\n        __m256i odd_elements = _mm256_and_si256(even_pos, is_odd);\n        vsum = _mm256_add_epi16(vsum, odd_elements);\n    }\n\n    int16_t temp[16];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    for(int i = aligned_len; i < len; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    if(lst.size() == 0) return sum;\n\n    const int16_t* data = lst.data();\n    size_t size = lst.size();\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask_odd = _mm256_set1_epi16(1);\n    __m256i mask_even_idx = _mm256_set_epi16(1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0);\n    __m256i vsum = _mm256_setzero_si256();\n\n    for(size_t i = 0; i + 16 <= size; i += 16){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i idx_mask = _mm256_and_si256(_mm256_srli_epi16(vec, 15), mask_even_idx);\n        __m256i is_odd = _mm256_cmpgt_epi16(_mm256_and_si256(vec, mask_odd), zero);\n        __m256i filtered = _mm256_and_si256(vec, _mm256_and_si256(is_odd, idx_mask));\n        vsum = _mm256_add_epi16(vsum, filtered);\n    }\n\n    // Horizontal sum\n    __m128i vsum_low = _mm_add_epi16(_mm256_extracti128_si256(vsum, 0), _mm256_extracti128_si256(vsum, 1));\n    vsum_low = _mm_add_epi16(vsum_low, _mm_unpackhi_epi64(vsum_low, vsum_low));\n    vsum_low = _mm_add_epi16(vsum_low, _mm_shuffle_epi32(vsum_low, 0x1B));\n\n    sum += _mm_extract_epi16(vsum_low, 0) + _mm_extract_epi16(vsum_low, 4);\n\n    // Handle remaining elements\n    for(size_t i = (size / 16) * 16; i < size; ++i){\n        if(i % 2 == 0 && (data[i] & 1)){\n            sum += data[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    const int len = lst.size();\n    if(len == 0) return sum;\n\n    alignas(32) int16_t temp[16];\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi16(1);\n    __m256i mask_even_pos = _mm256_set_epi16(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1);\n    __m256i mask_odd = _mm256_set1_epi16(1);\n\n    for(int i=0; i <= (len - 16); i += 16){\n        _mm256_store_si256((__m256i*)temp, _mm256_loadu_si256((__m256i*)&lst[i]));\n        __m256i vec = _mm256_load_si256((__m256i*)temp);\n        \n        // Check even positions\n        __m256i even_positions = _mm256_and_si256(vec, mask_even_pos);\n        \n        // Check if values are odd\n        __m256i mod_check = _mm256_and_si256(even_positions, ones);\n        __m256i is_odd = _mm256_cmpeq_epi16(mod_check, ones);\n        \n        // Select odd numbers at even positions\n        __m256i selected = _mm256_and_si256(is_odd, even_positions);\n        \n        // Horizontal sum\n        __m256i sum_vec = _mm256_sad_epu8(selected, zero);\n        uint32_t high = _mm256_extract_epi32(sum_vec, 7);\n        uint32_t low = _mm256_extract_epi32(sum_vec, 3);\n        sum += high + low;\n    }\n\n    // Handle remaining elements\n    for(int i = (len / 16) * 16; i < len; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t size = lst.size();\n    if(size == 0) return sum;\n\n    alignas(32) int16_t temp[16];\n    __m256i mask_even_idx = _mm256_set_epi16(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1);\n    __m256i mask_odd_val = _mm256_set1_epi16(1);\n    __m256i vsum = _mm256_setzero_si256();\n\n    for(size_t i = 0; i + 16 <= size; i += 16){\n        _mm256_store_si256((__m256i*)temp, _mm256_loadu_si256((const __m256i*)&lst[i]));\n        __m256i vec = _mm256_load_si256((const __m256i*)temp);\n        __m256i even_idx = _mm256_and_si256(vec, mask_even_idx);\n        __m256i odd_val = _mm256_and_si256(vec, mask_odd_val);\n        __m256i is_odd = _mm256_cmpgt_epi16(odd_val, _mm256_setzero_si256());\n        __m256i filtered = _mm256_and_si256(even_idx, is_odd);\n        vsum = _mm256_add_epi16(vsum, filtered);\n    }\n\n    int16_t res[16];\n    _mm256_storeu_si256((__m256i*)res, vsum);\n    for(int i = 0; i < 16; ++i){\n        sum += res[i];\n    }\n\n    for(size_t i = (size / 16) * 16; i < size; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t size = lst.size();\n    if(size == 0) return sum;\n\n    alignas(32) int16_t temp[16];\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi16(1);\n    __m256i mask_odd = _mm256_set1_epi16(0xAAAA);\n    __m256i res = _mm256_setzero_si256();\n\n    for(size_t i = 0; i + 16 <= size; i += 16){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i even_idx_mask = _mm256_setr_epi16(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1);\n        __m256i even_elements = _mm256_and_si256(vec, even_idx_mask);\n\n        __m256i is_odd = _mm256_and_si256(even_elements, mask_odd);\n        __m256i odd_elements = _mm256_and_si256(even_elements, _mm256_cmpeq_epi16(is_odd, mask_odd));\n\n        res = _mm256_add_epi16(res, odd_elements);\n    }\n\n    _mm256_store_si256((__m256i*)temp, res);\n    for(int i = 0; i < 16; ++i){\n        sum += temp[i];\n    }\n\n    for(size_t i = (size / 16) * 16; i < size; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    if(k <= 0) return sum;\n\n    const int limit = 100;\n    const int num_elements = k < arr.size() ? k : arr.size();\n    const int aligned_num = num_elements - (num_elements % 8);\n\n    __m256i vlimit = _mm256_set1_epi32(limit);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vsum = _mm256_setzero_si256();\n\n    for(int i = 0; i < aligned_num; i += 8){\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i vcond = _mm256_cmpgt_epi32(vlimit, _mm256_abs_epi32(vdata));\n        __m256i vmasked = _mm256_and_si256(vdata, vcond);\n        vsum = _mm256_add_epi32(vsum, vmasked);\n    }\n\n    int temp_sum[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_sum), vsum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp_sum[i];\n    }\n\n    for(int i = aligned_num; i < num_elements; ++i){\n        if(arr[i] >= -limit && arr[i] <= limit){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = 100;\n    int num_elements = k;\n\n    // Handle edge case where there are fewer than 8 elements\n    if (num_elements < 8) {\n        for(int i = 0; i < num_elements; ++i){\n            if(arr[i] >= -limit && arr[i] < limit){\n                sum += arr[i];\n            }\n        }\n        return sum;\n    }\n\n    __m256i vlimit = _mm256_set1_epi32(limit);\n    __m256i vneglimit = _mm256_set1_epi32(-limit);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vsum = _mm256_setzero_si256();\n\n    for(int i = 0; i + 8 <= num_elements; i += 8){\n        __m256i vdata = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i vcond1 = _mm256_cmpgt_epi32(vneglimit, vdata); // vdata < -limit\n        __m256i vcond2 = _mm256_cmplt_epi32(vdata, vlimit);     // vdata < limit\n        __m256i vcond = _mm256_andnot_si256(vcond1, vcond2);    // -limit <= vdata < limit\n        __m256i vmasked = _mm256_and_si256(vdata, vcond);       // Apply mask\n        vsum = _mm256_add_epi32(vsum, vmasked);                 // Accumulate\n    }\n\n    // Horizontal sum of vsum\n    alignas(32) int temp[8];\n    _mm256_store_si256((__m256i*)temp, vsum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    // Process remaining elements\n    for(int i = (num_elements / 8) * 8; i < num_elements; ++i){\n        if(arr[i] >= -limit && arr[i] < limit){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    if(k <= 0) return sum;\n\n    const int threshold = 99;\n    const int abs_mask = 0x7FFFFFFF;\n    const int limit = (k > arr.size()) ? arr.size() : k;\n    const int aligned_limit = limit - (limit % 8);\n\n    __m256i v_threshold = _mm256_set1_epi32(threshold);\n    __m256i v_abs_mask = _mm256_set1_epi32(abs_mask);\n    __m256i v_sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < aligned_limit; i += 8){\n        __m256i v_data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i v_abs_data = _mm256_and_si256(v_data, v_abs_mask);\n        __m256i v_cmp_result = _mm256_cmpgt_epi32(v_threshold, v_abs_data);\n        __m256i v_filtered = _mm256_and_si256(v_data, v_cmp_result);\n        v_sum = _mm256_add_epi32(v_sum, v_filtered);\n    }\n\n    int temp_sum[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_sum), v_sum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp_sum[i];\n    }\n\n    for(int i = aligned_limit; i < limit; ++i){\n        if(arr[i] >= -99 && arr[i] <= 99){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = k - (k % 8);\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(99);\n\n    for(int i = 0; i < limit; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(mask, vec); // Compare if each element <= 99\n        __m256i sel = _mm256_and_si256(vec, cmp); // Select elements <= 99\n        vsum = _mm256_add_epi32(vsum, sel); // Add selected elements to vsum\n    }\n\n    int temp[8];\n    _mm256_storeu_si256((__m256i*)temp, vsum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    for(int i = limit; i < k; ++i){\n        if(arr[i] <= 99){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = k - (k % 8);\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i abs_mask = _mm256_set1_epi32(0x7FFFFFFF);\n    __m256i threshold = _mm256_set1_epi32(99);\n\n    for(int i = 0; i < limit; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_and_si256(vec, abs_mask);\n        __m256i cmp_res = _mm256_cmpgt_epi32(threshold, abs_vec);\n        __m256i mask = _mm256_and_si256(vec, cmp_res);\n        vsum = _mm256_add_epi32(vsum, mask);\n    }\n\n    int temp[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp), vsum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    for(int i = limit; i < k; ++i){\n        if(arr[i] >= -99 && arr[i] <= 99){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    size_t n = lst.size();\n    size_t aligned_size = n - (n % 8);\n\n    __m256i prev_vec = _mm256_set1_epi32(lst[0]);\n    __m256i count_vec = _mm256_setzero_si256();\n\n    for (size_t i = 1; i < aligned_size; i += 8) {\n        __m256i current_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i cmp_gt = _mm256_cmpgt_epi32(current_vec, prev_vec);\n        __m256i cmp_eq = _mm256_cmpeq_epi32(current_vec, prev_vec);\n\n        // Check if any element is not greater than the previous one\n        if (_mm256_movemask_epi8(cmp_gt) != 0xFFFF) {\n            return false;\n        }\n\n        // Count duplicates\n        count_vec = _mm256_add_epi32(count_vec, cmp_eq);\n\n        // Update prev_vec for next iteration\n        prev_vec = current_vec;\n    }\n\n    // Check if any duplicate count exceeds 1\n    int counts[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(counts), count_vec);\n    for (int i = 0; i < 8; ++i) {\n        if (counts[i] > 1) return false;\n    }\n\n    // Process remaining elements\n    for (size_t i = aligned_size; i < n - 1; ++i) {\n        if (lst[i + 1] <= lst[i]) return false;\n        if (lst[i + 1] == lst[i]) {\n            int count = 1;\n            for (size_t j = i + 2; j < n; ++j) {\n                if (lst[j] == lst[i]) ++count;\n                else break;\n            }\n            if (count > 1) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() < 2) return true;\n\n    const int *data = lst.data();\n    size_t size = lst.size();\n\n    __m256i prev_vec = _mm256_set1_epi32(data[0]);\n    __m256i count_vec = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for (size_t i = 1; i + 7 < size; i += 8) {\n        __m256i current_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp_ge = _mm256_cmpgt_epi32(current_vec, prev_vec);\n        __m256i cmp_eq = _mm256_cmpeq_epi32(current_vec, prev_vec);\n\n        __m256i mask_ge = _mm256_andnot_si256(cmp_ge, ones);\n        count_vec = _mm256_add_epi32(count_vec, _mm256_and_si256(mask_ge, cmp_eq));\n\n        __m256i any_gt = _mm256_cmpgt_epi32(prev_vec, current_vec);\n        if (_mm256_movemask_epi8(any_gt)) {\n            return false;\n        }\n\n        prev_vec = current_vec;\n    }\n\n    int counts[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(counts), count_vec);\n    for (int i = 0; i < 8; ++i) {\n        if (counts[i] > 1) return false;\n    }\n\n    for (size_t i = ((size - 1) / 8) * 8 + 1; i < size; ++i) {\n        if (data[i] < data[i - 1]) return false;\n        if (data[i] == data[i - 1]) {\n            int count = 0;\n            for (size_t j = i; j < size && data[j] == data[i]; ++j) ++count;\n            if (count > 2) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    const int *data = lst.data();\n    size_t size = lst.size();\n\n    __m256i prev = _mm256_set1_epi32(data[0]);\n    __m256i count = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for (size_t i = 1; i + 7 < size; i += 8){\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp_ge = _mm256_cmpgt_epi32(prev, current);\n        __m256i cmp_eq = _mm256_cmpeq_epi32(prev, current);\n\n        __m256i mask_ge = _mm256_or_si256(cmp_ge, cmp_eq);\n        if (_mm256_movemask_epi8(mask_ge)){\n            return false;\n        }\n\n        __m256i new_count = _mm256_add_epi32(count, cmp_eq);\n        __m256i cmp_count = _mm256_cmpgt_epi32(new_count, ones);\n        if (_mm256_movemask_epi8(cmp_count)){\n            return false;\n        }\n        count = new_count;\n\n        prev = current;\n    }\n\n    for (size_t i = ((size / 8) * 8); i < size - 1; ++i){\n        if (data[i] > data[i + 1]){\n            return false;\n        }\n        if (data[i] == data[i + 1]){\n            int j = i - 1;\n            while(j >=0 && data[j] != data[i]){\n                --j;\n            }\n            if(i - j > 1){\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    size_t n = lst.size();\n    if (n <= 1) return true;\n\n    const int *data = lst.data();\n    __m256i prev_vec = _mm256_set1_epi32(data[0]);\n    __m256i count_vec = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for(size_t i = 0; i <= n - 8; i += 8){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&data[i]);\n        __m256i cmp_ge = _mm256_cmpgt_epi32(current_vec, prev_vec);\n        __m256i cmp_eq = _mm256_cmpeq_epi32(current_vec, prev_vec);\n\n        // Check if any element is less than previous\n        if(_mm256_movemask_epi8(cmp_ge) != 0xFFFF){\n            return false;\n        }\n\n        // Count duplicates\n        count_vec = _mm256_add_epi32(count_vec, cmp_eq);\n        count_vec = _mm256_sub_epi32(count_vec, ones); // Subtract one for allowed duplicates\n\n        // Check if any duplicate count exceeds 1\n        if(_mm256_movemask_epi8(_mm256_cmpgt_epi32(count_vec, _mm256_setzero_si256())) != 0){\n            return false;\n        }\n\n        prev_vec = current_vec;\n    }\n\n    // Process remaining elements\n    for(size_t i = (n / 8) * 8; i < n - 1; ++i){\n        if(data[i + 1] < data[i] || (data[i + 1] == data[i] && data[i] == data[i - 1])){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    size_t n = lst.size();\n    const int *data = lst.data();\n\n    __m256i prev_vec = _mm256_set1_epi32(data[0]);\n    __m256i count_vec = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi32(1);\n\n    for(size_t i = 1; i + 7 < n; i += 8){\n        __m256i current_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i cmp_gt = _mm256_cmpgt_epi32(current_vec, prev_vec);\n        __m256i cmp_eq = _mm256_cmpeq_epi32(current_vec, prev_vec);\n\n        __m256i mask_gt = _mm256_andnot_si256(cmp_eq, cmp_gt);\n        __m256i mask_eq = cmp_eq;\n\n        count_vec = _mm256_add_epi32(count_vec, mask_eq);\n        __m256i check_count = _mm256_cmpgt_epi32(count_vec, ones);\n        if(_mm256_movemask_epi8(check_count)){\n            return false;\n        }\n\n        __m256i all_ones = _mm256_set1_epi32(-1);\n        __m256i valid = _mm256_or_si256(mask_gt, mask_eq);\n        if(_mm256_movemask_epi8(_mm256_xor_si256(valid, all_ones)) != 0){\n            return false;\n        }\n\n        prev_vec = current_vec;\n    }\n\n    for(size_t i = ((n - 1) / 8) * 8 + 1; i < n; ++i){\n        if(lst[i] < lst[i-1]) return false;\n        if(lst[i] == lst[i-1]){\n            if(i > 1 && lst[i] == lst[i-2]) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i product_sign = _mm256_set1_epi32(1);\n\n    const int len = arr.size();\n    const int aligned_len = len - (len % 8);\n\n    for(int i = 0; i < aligned_len; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n\n        // Update product_sign with the current vector's signs\n        product_sign = _mm256_mullo_epi32(product_sign, signs);\n\n        // Add absolute values to sum\n        sum = _mm256_add_epi32(sum, abs_vals);\n\n        // Check for zero elements in the vector\n        __m256i zero_check = _mm256_cmpeq_epi32(vec, _mm256_setzero_si256());\n        int mask = _mm256_movemask_epi8(zero_check);\n        if(mask != 0) return 0;\n    }\n\n    // Horizontal sum of the absolute values\n    alignas(32) int intermediate_sum[8];\n    _mm256_store_si256((__m256i*)intermediate_sum, sum);\n    int total_sum = 0;\n    for(int i = 0; i < 8; ++i){\n        total_sum += intermediate_sum[i];\n    }\n\n    // Horizontal product of the signs\n    alignas(32) int intermediate_product_sign[8];\n    _mm256_store_si256((__m256i*)intermediate_product_sign, product_sign);\n    int total_product_sign = 1;\n    for(int i = 0; i < 8; ++i){\n        total_product_sign *= intermediate_product_sign[i];\n    }\n\n    // Process remaining elements\n    for(int i = aligned_len; i < len; ++i){\n        if(arr[i] == 0) return 0;\n        total_sum += std::abs(arr[i]);\n        total_product_sign *= (arr[i] > 0) ? 1 : -1;\n    }\n\n    return total_sum * total_product_sign;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vprod_sign = _mm256_set1_epi32(1);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i vone = _mm256_set1_epi32(1);\n\n    for(size_t i = 0; i + 8 <= arr.size(); i += 8){\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i vabs = _mm256_abs_epi32(v);\n        __m256i vsgn = _mm256_sign_epi32(vone, v);\n\n        // Check for zero elements\n        __m256i vcmp = _mm256_cmpeq_epi32(v, vzero);\n        int mask = _mm256_movemask_epi8(vcmp);\n        if(mask != 0) return 0;\n\n        vsum = _mm256_add_epi32(vsum, vabs);\n        vprod_sign = _mm256_mullo_epi32(vprod_sign, vsgn);\n    }\n\n    int sum[8];\n    _mm256_storeu_si256((__m256i*)sum, vsum);\n\n    int prod_sign[8];\n    _mm256_storeu_si256((__m256i*)prod_sign, vprod_sign);\n\n    int total_sum = 0;\n    int total_prod_sign = 1;\n    for(int i = 0; i < 8; ++i){\n        total_sum += sum[i];\n        total_prod_sign *= prod_sign[i];\n    }\n\n    for(size_t i = arr.size() & ~size_t(7); i < arr.size(); ++i){\n        if(arr[i] == 0) return 0;\n        total_sum += abs(arr[i]);\n        total_prod_sign *= (arr[i] > 0) ? 1 : -1;\n    }\n\n    return total_sum * total_prod_sign;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod_sign = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n\n    int arr_size = arr.size();\n    int i = 0;\n\n    for (; i <= arr_size - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i cmp_zero = _mm256_cmpgt_epi32(vec, zero); // Signs as 0/-1/1\n        __m256i signs = _mm256_sub_epi32(_mm256_setzero_si256(), cmp_zero); // Convert to -1 for negative\n        signs = _mm256_or_si256(signs, _mm256_and_si256(_mm256_cmpeq_epi32(vec, zero), _mm256_set1_epi32(-1))); // Handle zeros\n\n        // Update product of signs\n        prod_sign = _mm256_mullo_epi32(prod_sign, signs);\n\n        // Compute absolute values\n        __m256i abs_vals = _mm256_abs_epi32(vec);\n\n        // Accumulate sum of magnitudes\n        sum = _mm256_add_epi32(sum, abs_vals);\n    }\n\n    // Horizontal product of signs\n    int32_t hprod_sign = 1;\n    alignas(32) int32_t temp[8];\n    _mm256_store_si256((__m256i*)temp, prod_sign);\n    for (int j = 0; j < 8; ++j) {\n        if (temp[j] == 0) hprod_sign = 0;\n        hprod_sign *= (temp[j] > 0) ? 1 : -1;\n    }\n\n    // Horizontal sum of magnitudes\n    int32_t hsum = 0;\n    _mm256_store_si256((__m256i*)temp, sum);\n    for (int j = 0; j < 8; ++j) {\n        hsum += temp[j];\n    }\n\n    // Process remaining elements\n    for (; i < arr_size; ++i) {\n        if (arr[i] == 0) {\n            hprod_sign = 0;\n        } else {\n            hprod_sign *= (arr[i] > 0) ? 1 : -1;\n        }\n        hsum += std::abs(arr[i]);\n    }\n\n    return hprod_sign * hsum;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i product_sign = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i abs_mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    const int *data = arr.data();\n    size_t size = arr.size();\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        product_sign = _mm256_mullo_epi32(product_sign, signs);\n\n        __m256i abs_vec = _mm256_and_si256(vec, abs_mask);\n        sum = _mm256_add_epi32(sum, abs_vec);\n\n        // Check for zero elements\n        __m256i cmp = _mm256_cmpeq_epi32(vec, zero);\n        __m256i zero_mask = _mm256_movemask_epi8(cmp);\n        if(zero_mask != 0){\n            return 0;\n        }\n    }\n\n    // Horizontal product of signs\n    int prod_sign_val = 1;\n    int signs_arr[8];\n    _mm256_storeu_si256((__m256i*)signs_arr, product_sign);\n    for(int i = 0; i < 8; ++i){\n        prod_sign_val *= signs_arr[i];\n    }\n\n    // Horizontal sum\n    int sum_arr[8];\n    _mm256_storeu_si256((__m256i*)sum_arr, sum);\n    int total_sum = 0;\n    for(int i = 0; i < 8; ++i){\n        total_sum += sum_arr[i];\n    }\n\n    // Tail elements\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        int val = data[i];\n        if(val == 0) return 0;\n        prod_sign_val *= (val > 0) ? 1 : -1;\n        total_sum += abs(val);\n    }\n\n    return total_sum * prod_sign_val;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    const size_t N = arr.size();\n    const size_t aligned_size = N / 8 * 8;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n\n        // Compute absolute values\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n\n        // Sum of magnitudes\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n\n        // Compute signs (-1, 0, 1)\n        __m256i cmp_zero = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i cmp_neg = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i sign_vec = _mm256_sub_epi32(cmp_zero, cmp_neg);\n\n        // Update product of signs\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, sign_vec);\n    }\n\n    int sum_arr[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sum_arr), sum_vec);\n\n    int sum = 0;\n    for (int i = 0; i < 8; ++i) {\n        sum += sum_arr[i];\n    }\n\n    int sign_prod_arr[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(sign_prod_arr), sign_prod_vec);\n\n    int sign_prod = 1;\n    for (int i = 0; i < 8; ++i) {\n        sign_prod *= sign_prod_arr[i];\n    }\n\n    for (size_t i = aligned_size; i < N; ++i) {\n        if (arr[i] > 0) {\n            sign_prod *= 1;\n        } else if (arr[i] < 0) {\n            sign_prod *= -1;\n        } else {\n            sign_prod = 0;\n            break;\n        }\n        sum += std::abs(arr[i]);\n    }\n\n    return sum * sign_prod;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    int min_val = INT_MAX;\n    int start_x = 0, start_y = 0;\n\n    // Find the minimum starting value using AVX2\n    __m256i vmin = _mm256_set1_epi32(INT_MAX);\n    for(int i = 0; i < N; ++i){\n        for(int j = 0; j < N; j += 8){\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[i][std::min(j, N-1)]));\n            vmin = _mm256_min_epi32(vmin, vec);\n        }\n    }\n\n    int min_start = _mm256_extract_epi32(vmin, 0);\n    for(int i = 0; i < N && min_start > 0; ++i){\n        for(int j = 0; j < N && min_start > 0; ++j){\n            if(grid[i][j] == min_start){\n                start_x = i;\n                start_y = j;\n                min_start = 0;\n            }\n        }\n    }\n\n    result.push_back(grid[start_x][start_y]);\n    int current_x = start_x, current_y = start_y;\n\n    for(int step = 1; step < k; ++step){\n        int next_val = INT_MAX;\n        int next_x = current_x, next_y = current_y;\n\n        // Check neighbors\n        int dx[4] = {-1, 1, 0, 0};\n        int dy[4] = {0, 0, -1, 1};\n\n        for(int dir = 0; dir < 4; ++dir){\n            int nx = current_x + dx[dir];\n            int ny = current_y + dy[dir];\n\n            if(nx >= 0 && nx < N && ny >= 0 && ny < N){\n                if(grid[nx][ny] < next_val){\n                    next_val = grid[nx][ny];\n                    next_x = nx;\n                    next_y = ny;\n                }\n            }\n        }\n\n        result.push_back(grid[next_x][next_y]);\n        current_x = next_x;\n        current_y = next_y;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> & grid, int k){\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    const int INF = INT_MAX;\n    __m256i vmin = _mm256_set1_epi32(INF);\n    __m256i best_path = _mm256_setzero_si256();\n\n    for(int start_row = 0; start_row < N; ++start_row){\n        for(int start_col = 0; start_col < N; ++start_col){\n            __m256i current_min = _mm256_set1_epi32(grid[start_row][start_col]);\n            __m256i path = _mm256_insert_epi32(_mm256_setzero_si256(), grid[start_row][start_col], 0);\n\n            int r = start_row, c = start_col;\n            for(int step = 1; step < k; ++step){\n                int next_val = INF;\n                int next_r = r, next_c = c;\n\n                // Explore neighbors\n                if(r > 0 && grid[r-1][c] < next_val){\n                    next_val = grid[r-1][c];\n                    next_r = r-1;\n                    next_c = c;\n                }\n                if(r < N-1 && grid[r+1][c] < next_val){\n                    next_val = grid[r+1][c];\n                    next_r = r+1;\n                    next_c = c;\n                }\n                if(c > 0 && grid[r][c-1] < next_val){\n                    next_val = grid[r][c-1];\n                    next_r = r;\n                    next_c = c-1;\n                }\n                if(c < N-1 && grid[r][c+1] < next_val){\n                    next_val = grid[r][c+1];\n                    next_r = r;\n                    next_c = c+1;\n                }\n\n                __m256i next_val_vec = _mm256_set1_epi32(next_val);\n                current_min = _mm256_min_epi32(current_min, next_val_vec);\n                path = _mm256_insert_epi32(path, next_val, step);\n\n                r = next_r;\n                c = next_c;\n            }\n\n            __m256i mask = _mm256_cmpgt_epi32(vmin, current_min);\n            vmin = _mm256_min_epi32(vmin, current_min);\n            best_path = _mm256_blendv_epi8(best_path, path, mask);\n        }\n    }\n\n    for(int i = 0; i < k; ++i){\n        int value = _mm256_extract_epi32(best_path, i);\n        result.push_back(value);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int N = grid.size();\n    std::vector<int> result;\n    if (k == 0) return result;\n\n    int best_val = INT32_MAX;\n    std::vector<int> best_path;\n\n    for(int start_row = 0; start_row < N; ++start_row){\n        for(int start_col = 0; start_col < N; ++start_col){\n            __m256i current_min = _mm256_set1_epi32(INT32_MAX);\n            std::vector<int> path;\n            path.push_back(grid[start_row][start_col]);\n            \n            // Use AVX2 to process multiple paths in parallel\n            for(int step = 1; step < k; ++step){\n                __m256i candidates = _mm256_set_epi32(\n                    (start_row > 0) ? grid[start_row-1][start_col] : INT32_MAX,\n                    (start_col > 0) ? grid[start_row][start_col-1] : INT32_MAX,\n                    (start_row < N-1) ? grid[start_row+1][start_col] : INT32_MAX,\n                    (start_col < N-1) ? grid[start_row][start_col+1] : INT32_MAX,\n                    (start_row > 0) ? grid[start_row-1][start_col] : INT32_MAX,\n                    (start_col > 0) ? grid[start_row][start_col-1] : INT32_MAX,\n                    (start_row < N-1) ? grid[start_row+1][start_col] : INT32_MAX,\n                    (start_col < N-1) ? grid[start_row][start_col+1] : INT32_MAX\n                );\n                __m256i min_candidate = _mm256_min_epi32(candidates, current_min);\n                int min_val = _mm256_extract_epi32(min_candidate, 0);\n                \n                path.push_back(min_val);\n                current_min = _mm256_set1_epi32(min_val);\n            }\n\n            if(path < best_path || best_path.empty()){\n                best_path = path;\n            }\n        }\n    }\n\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    int min_val = INT_MAX;\n    int start_x = 0, start_y = 0;\n\n    // Find the minimum starting value using AVX2\n    __m256i vmin = _mm256_set1_epi32(INT_MAX);\n    for(int i = 0; i < N; ++i){\n        for(int j = 0; j < N; j += 8){\n            __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[i][std::min(j, N-1)]));\n            vmin = _mm256_min_epi32(vmin, data);\n        }\n    }\n\n    int min_val_arr[8];\n    _mm256_storeu_si256((__m256i*)min_val_arr, vmin);\n    for(int i = 0; i < 8; ++i){\n        if(min_val_arr[i] < min_val){\n            min_val = min_val_arr[i];\n        }\n    }\n\n    // Find all positions with the minimum value\n    for(int i = 0; i < N; ++i){\n        for(int j = 0; j < N; ++j){\n            if(grid[i][j] == min_val){\n                start_x = i;\n                start_y = j;\n                break;\n            }\n        }\n    }\n\n    // Perform DFS to find the minimum path\n    std::vector<int> current_path;\n    current_path.push_back(grid[start_x][start_y]);\n    std::vector<std::vector<int>> best_path(1, current_path);\n\n    for(int step = 1; step < k; ++step){\n        std::vector<std::vector<int>> new_best_path;\n        for(auto &path : best_path){\n            int last_x = path.size() >= 2 ? path[path.size()-2] : start_x;\n            int last_y = path.size() >= 2 ? path[path.size()-1] : start_y;\n            int directions[4][2] = {{-1,0}, {1,0}, {0,-1}, {0,1}};\n            for(auto dir : directions){\n                int nx = last_x + dir[0];\n                int ny = last_y + dir[1];\n                if(nx >=0 && nx < N && ny >=0 && ny < N){\n                    std::vector<int> new_path = path;\n                    new_path.push_back(grid[nx][ny]);\n                    if(new_best_path.empty() || lexicographical_compare(new_path.begin(), new_path.end(), new_best_path[0].begin(), new_best_path[0].end())){\n                        new_best_path.clear();\n                        new_best_path.push_back(new_path);\n                    }\n                }\n            }\n        }\n        best_path = new_best_path;\n    }\n\n    if(!best_path.empty()){\n        result = best_path[0];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    int min_val = INT_MAX;\n    int start_x = 0, start_y = 0;\n\n    __m256i min_vec = _mm256_set1_epi32(INT_MAX);\n    __m256i comp_vec;\n\n    for(int i=0; i<N; ++i){\n        for(int j=0; j<N; ++j){\n            comp_vec = _mm256_set1_epi32(grid[i][j]);\n            min_vec = _mm256_min_epi32(min_vec, comp_vec);\n            if(grid[i][j] < min_val){\n                min_val = grid[i][j];\n                start_x = i;\n                start_y = j;\n            }\n        }\n    }\n\n    result.push_back(min_val);\n    int current_x = start_x;\n    int current_y = start_y;\n\n    for(int step=1; step<k; ++step){\n        int next_val = INT_MAX;\n        int next_x = current_x, next_y = current_y;\n\n        // Directions: up, down, left, right\n        int dx[4] = {-1, 1, 0, 0};\n        int dy[4] = {0, 0, -1, 1};\n\n        __m256i current_min = _mm256_set1_epi32(INT_MAX);\n        __m256i val_vec;\n\n        for(int dir=0; dir<4; ++dir){\n            int nx = current_x + dx[dir];\n            int ny = current_y + dy[dir];\n\n            if(nx >=0 && nx <N && ny >=0 && ny <N){\n                val_vec = _mm256_set1_epi32(grid[nx][ny]);\n                current_min = _mm256_min_epi32(current_min, val_vec);\n                if(grid[nx][ny] < next_val){\n                    next_val = grid[nx][ny];\n                    next_x = nx;\n                    next_y = ny;\n                }\n            }\n        }\n\n        result.push_back(next_val);\n        current_x = next_x;\n        current_y = next_y;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i mask_odd = _mm256_set1_epi8(0x01);\n    __m256i mask_even = _mm256_set1_epi8(0x00);\n    __m256i product = _mm256_set1_epi16(1);\n\n    uint64_t result = 1;\n    int has_odd = 0;\n\n    while (n > 0) {\n        __m256i digits = _mm256_set_epi8(\n            (n % 10), (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10,\n            (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10,\n            (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10,\n            (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10,\n            (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10,\n            (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10,\n            (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10, (n /= 10) % 10\n        );\n\n        __m256i is_odd = _mm256_and_si256(digits, mask_odd);\n        __m256i odd_digits = _mm256_and_si256(digits, _mm256_cmpeq_epi8(is_odd, mask_odd));\n\n        __m256i zeroed = _mm256_blendv_epi8(ones, odd_digits, _mm256_cmpeq_epi8(is_odd, mask_even));\n        product = _mm256_mullo_epi16(product, _mm256_cvtepu8_epi16(zeroed));\n\n        int temp[16];\n        _mm256_storeu_si256((__m256i*)temp, product);\n\n        for(int i = 0; i < 16; ++i){\n            if(temp[i] != 1){\n                has_odd = 1;\n                result *= temp[i];\n            }\n        }\n\n        product = _mm256_set1_epi16(1);\n    }\n\n    return has_odd ? result : 0;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    uint64_t product = 1;\n    uint64_t has_odd = 0;\n\n    __m256i vzero = _mm256_set1_epi8('0');\n    __m256i vnine = _mm256_set1_epi8('9');\n    __m256i vtwo = _mm256_set1_epi8('2');\n    __m256i vone = _mm256_set1_epi8(1);\n    __m256i vmod = _mm256_set1_epi8(10);\n\n    char buffer[32];\n    int count = 0;\n\n    while (n > 0 || count > 0) {\n        if (n > 0) {\n            uint64_t digit = n % 10;\n            buffer[count++] = '0' + digit;\n            n /= 10;\n        }\n\n        if (count >= 32) {\n            __m256i vbuffer = _mm256_loadu_si256((__m256i*)buffer);\n            __m256i vgeq_zero = _mm256_cmpgt_epi8(vbuffer, vzero);\n            __m256i vleq_nine = _mm256_cmple_epi8(vbuffer, vnine);\n            __m256i vis_digit = _mm256_and_si256(vgeq_zero, vleq_nine);\n            __m256i vodd = _mm256_and_si256(_mm256_cmpgt_epi8(_mm256_sub_epi8(vbuffer, vtwo), _mm256_setzero_si256()), vis_digit);\n            __m256i vdigit = _mm256_sub_epi8(vbuffer, vzero);\n            __m256i vrem = _mm256_and_si256(_mm256_set1_epi8(1), vdigit);\n            __m256i vis_odd = _mm256_cmpeq_epi8(vrem, vone);\n            __m256i vodd_digits = _mm256_and_si256(vodd, vdigit);\n\n            __m256i vmask = _mm256_cmpgt_epi8(vodd_digits, _mm256_setzero_si256());\n            __m256i vprod = _mm256_set1_epi8(1);\n            for(int i=0;i<32;i++) {\n                if(_mm256_testc_epi8(_mm256_shuffle_epi8(vmask, _mm256_set1_epi8(i)), vmask)) {\n                    uint8_t d = ((uint8_t*)&vodd_digits)[i];\n                    if(d !=0 ){\n                        product *= d;\n                        has_odd =1;\n                    }\n                }\n            }\n            count -=32;\n            _mm256_storeu_si256((__m256i*)buffer, _mm256_setzero_si256());\n        }\n    }\n\n    if (!has_odd) return 0;\n    return product;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i mask_odd = _mm256_set1_epi8(0x0F);\n    __m256i is_odd_mask = _mm256_set1_epi8(0x01);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi16(1);\n\n    uint64_t result = 1;\n    int has_odd = 0;\n\n    while (n > 0) {\n        __m256i digits = _mm256_set1_epi64x(n);\n        __m256i last_digits = _mm256_and_si256(digits, mask_odd);\n        __m256i is_odd = _mm256_cmpgt_epi8(_mm256_and_si256(last_digits, is_odd_mask), zero);\n\n        __m256i odd_digits = _mm256_and_si256(last_digits, is_odd);\n        __m256i current_product = _mm256_mullo_epi16(product, _mm256_slli_epi16(odd_digits, 8));\n\n        has_odd |= _mm256_movemask_epi8(is_odd);\n        product = current_product;\n\n        n /= 10;\n    }\n\n    if (!has_odd) return 0;\n\n    uint16_t prod_arr[16];\n    _mm256_storeu_si256((__m256i*)prod_arr, product);\n\n    result = 1;\n    for(int i = 0; i < 16; ++i){\n        if(prod_arr[i] != 0){\n            result *= (prod_arr[i] >> 8);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i mask_odd = _mm256_set1_epi8(0x01);\n    __m256i mask_even = _mm256_set1_epi8(0x00);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi16(1);\n\n    uint64_t temp = n;\n    uint8_t digits[32];\n    int count = 0;\n\n    while(temp > 0){\n        uint8_t digit = temp % 10;\n        digits[count++] = digit;\n        temp /= 10;\n    }\n\n    for(int i=0; i<count; i+=32){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&digits[i]);\n        __m256i odd_mask = _mm256_and_si256(vec, mask_odd);\n        __m256i is_odd = _mm256_cmpeq_epi8(odd_mask, mask_odd);\n        __m256i odd_digits = _mm256_and_si256(vec, is_odd);\n        __m256i valid = _mm256_cmpgt_epi8(odd_digits, zero);\n\n        __m256i mul_lo = _mm256_maddubs_epi16(valid, product);\n        __m256i mul_hi = _mm256_maddubs_epi16(_mm256_shuffle_epi8(valid, _mm256_set_epi8(\n            15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0,\n            15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0)), product);\n\n        product = _mm256_mullo_epi16(mul_lo, mul_hi);\n    }\n\n    uint16_t prod_arr[16];\n    _mm256_storeu_si256((__m256i*)prod_arr, product);\n\n    uint64_t result = 1;\n    for(int i=0;i<16;i++) result *= prod_arr[i];\n\n    return (result == 1 && n%2 == 0) ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i mask = _mm256_set1_epi8(0x0F);\n    __m256i odd_mask = _mm256_set1_epi8(0x55);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi16(1);\n\n    uint64_t result = 1;\n    int has_odd = 0;\n\n    while (n > 0) {\n        __m256i digits = _mm256_set_epi8(\n            (n >> 63) & 0xF, (n >> 60) & 0xF, (n >> 57) & 0xF, (n >> 54) & 0xF,\n            (n >> 51) & 0xF, (n >> 48) & 0xF, (n >> 45) & 0xF, (n >> 42) & 0xF,\n            (n >> 39) & 0xF, (n >> 36) & 0xF, (n >> 33) & 0xF, (n >> 30) & 0xF,\n            (n >> 27) & 0xF, (n >> 24) & 0xF, (n >> 21) & 0xF, (n >> 18) & 0xF,\n            (n >> 15) & 0xF, (n >> 12) & 0xF, (n >> 9) & 0xF, (n >> 6) & 0xF,\n            (n >> 3) & 0xF, (n >> 0) & 0xF, 0, 0, 0, 0, 0, 0, 0, 0\n        );\n        n /= 100;\n\n        __m256i is_odd = _mm256_and_si256(_mm256_cmpeq_epi8(_mm256_and_si256(digits, mask), odd_mask), mask);\n        __m256i filtered = _mm256_and_si256(is_odd, digits);\n\n        __m256i zeros = _mm256_setzero_si256();\n        __m256i cmp = _mm256_cmpeq_epi8(filtered, zeros);\n        __m256i sel = _mm256_blendv_epi8(ones, filtered, cmp);\n\n        __m256i prod_lo = _mm256_maddubs_epi16(sel, _mm256_set1_epi8(1));\n        __m256i prod_hi = _mm256_madd_epi16(prod_lo, _mm256_set1_epi16(1));\n\n        int temp[16];\n        _mm256_storeu_si256((__m256i*)temp, prod_hi);\n\n        for(int i = 0; i < 16; ++i){\n            if(temp[i] != 0){\n                has_odd = 1;\n                result *= temp[i] & 0xFF;\n            }\n        }\n    }\n\n    return has_odd ? result : 0;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    alignas(32) float temp[8];\n    __m256i rounded_ints;\n    __m256 squares;\n    __m256 sums = _mm256_setzero_ps();\n\n    for(size_t i = 0; i < size / 8 * 8; i += 8){\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_data = _mm256_ceil_ps(data);\n        rounded_ints = _mm256_cvtps_epi32(ceil_data);\n        squares = _mm256_cvtepi32_ps(_mm256_mullo_epi32(rounded_ints, rounded_ints));\n        sums = _mm256_add_ps(sums, squares);\n    }\n\n    _mm256_storeu_ps(temp, sums);\n    total_sum += static_cast<int>(temp[0]) + static_cast<int>(temp[1]) + \n                 static_cast<int>(temp[2]) + static_cast<int>(temp[3]) +\n                 static_cast<int>(temp[4]) + static_cast<int>(temp[5]) +\n                 static_cast<int>(temp[6]) + static_cast<int>(temp[7]);\n\n    for(size_t i = size / 8 * 8; i < size; ++i){\n        int rounded_val = static_cast<int>(std::ceil(lst[i]));\n        total_sum += rounded_val * rounded_val;\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    alignas(32) float temp[8];\n    __m256i sum_mask = _mm256_set1_epi32(0x7FFFFFFF);\n    __m256 ceil_val = _mm256_set1_ps(0.999999f);\n    __m256 zero_val = _mm256_setzero_ps();\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_data = _mm256_add_ps(data, ceil_val);\n        __m256 floored_data = _mm256_floor_ps(ceil_data);\n        __m256 squared_data = _mm256_mul_ps(floored_data, floored_data);\n\n        __m256i int_data = _mm256_cvttps_epi32(squared_data);\n        int_data = _mm256_and_si256(int_data, sum_mask);\n        sum_vec = _mm256_add_epi32(sum_vec, int_data);\n    }\n\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for(int i = 0; i < 8; ++i){\n        total_sum += temp[i];\n    }\n\n    for(size_t i = size - (size % 8); i < size; ++i){\n        float val = std::ceil(lst[i]);\n        total_sum += static_cast<uint64_t>(val * val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n#include <stdint.h>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    uint64_t total_sum = 0;\n    alignas(32) float ceil_values[8];\n    __m256i sum_vec = _mm256_setzero_si256();\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        vec = _mm256_ceil_ps(vec); // Apply ceiling to each element\n        _mm256_store_ps(ceil_values, vec);\n\n        __m256i int_vec = _mm256_cvtps_epi32(vec); // Convert to integer\n        __m256i squared_vec = _mm256_mullo_epi32(int_vec, int_vec); // Square the integers\n\n        sum_vec = _mm256_add_epi32(sum_vec, squared_vec); // Accumulate the sum\n    }\n\n    // Horizontal sum of the accumulated vector\n    alignas(32) int32_t temp[8];\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    for(int i = 0; i < 8; ++i){\n        total_sum += temp[i];\n    }\n\n    // Handle remaining elements\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        int val = static_cast<int>(std::ceil(lst[i]));\n        total_sum += val * val;\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    alignas(32) float temp[8];\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i zero = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < size / 8 * 8; i += 8){\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        __m256 ceil_vec = _mm256_ceil_ps(vec);\n        __m256i int_vec = _mm256_cvtps_epi32(ceil_vec);\n        __m256i squared_vec = _mm256_mullo_epi32(int_vec, int_vec);\n\n        __m256i high_part = _mm256_srli_epi64(squared_vec, 32);\n        __m256i sum_parts = _mm256_add_epi32(squared_vec, high_part);\n\n        _mm256_storeu_si256((__m256i*)temp, sum_parts);\n        total_sum += temp[0] + temp[1] + temp[2] + temp[3] + temp[4] + temp[5] + temp[6] + temp[7];\n    }\n\n    for (size_t i = size / 8 * 8; i < size; ++i){\n        float val = std::ceil(lst[i]);\n        total_sum += static_cast<int>(val) * static_cast<int>(val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return total_sum;\n\n    alignas(32) float temp[8];\n    __m256i rounded;\n    __m256 squares, values;\n\n    for(size_t i = 0; i < size / 8 * 8; i += 8){\n        values = _mm256_loadu_ps(&lst[i]);\n        rounded = _mm256_cvttps_epi32(_mm256_add_ps(values, _mm256_set1_ps(0.5f)));\n        squares = _mm256_mul_ps(_mm256_cvtepi32_ps(rounded), _mm256_cvtepi32_ps(rounded));\n        _mm256_storeu_ps(temp, squares);\n        total_sum += static_cast<uint64_t>(temp[0]) + static_cast<uint64_t>(temp[1]) + \n                    static_cast<uint64_t>(temp[2]) + static_cast<uint64_t>(temp[3]) +\n                    static_cast<uint64_t>(temp[4]) + static_cast<uint64_t>(temp[5]) +\n                    static_cast<uint64_t>(temp[6]) + static_cast<uint64_t>(temp[7]);\n    }\n\n    for(size_t i = size / 8 * 8; i < size; ++i){\n        int val = static_cast<int>(std::ceil(lst[i]));\n        total_sum += static_cast<uint64_t>(val * val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int result = -1;\n    size_t n = arr.size();\n    size_t aligned_size = n - (n % 8);\n\n    __m256i indices = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n    __m256i max_index = _mm256_set1_epi32(-1);\n\n    for(size_t i = 0; i < aligned_size - 1; i += 8){\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i+1]));\n\n        __m256i cmp = _mm256_cmpgt_epi32(next, current);\n        __m256i mask = _mm256_xor_si256(cmp, _mm256_set1_epi32(-1));\n\n        __m256i current_indices = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n        __m256i violating_indices = _mm256_and_si256(mask, current_indices);\n\n        __m256i max_violating = _mm256_max_epu32(violating_indices, max_index);\n        max_index = max_violating;\n    }\n\n    int max_index_val = -1;\n    alignas(32) int max_index_array[8];\n    _mm256_store_si256((__m256i*)max_index_array, max_index);\n    for(int i = 0; i < 8; ++i){\n        if(max_index_array[i] > max_index_val){\n            max_index_val = max_index_array[i];\n        }\n    }\n\n    result = max_index_val;\n\n    for(size_t i = aligned_size; i < n-1; ++i){\n        if(arr[i+1] < arr[i]){\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n <=1 ) return -1;\n\n    int max_idx = -1;\n    __m256i cmp_res = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    for(int i=0; i < n-1; i+=8){\n        __m256i vec_curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i vec_next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i+1]));\n\n        __m256i res = _mm256_cmpgt_epi32(vec_next, vec_curr);\n        cmp_res = _mm256_or_si256(cmp_res, res);\n\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i masked_indices = _mm256_andnot_si256(res, indices);\n        __m256i current_max = _mm256_max_epi32(masked_indices, _mm256_set1_epi32(max_idx));\n        max_idx = _mm256_extract_epi32(current_max, 0);\n        for(int j=1; j<8; ++j){\n            max_idx = std::max(max_idx, _mm256_extract_epi32(current_max, j));\n        }\n    }\n\n    // Check remaining elements\n    for(int i = (n/8)*8; i < n-1; ++i){\n        if(arr[i+1] < arr[i]){\n            max_idx = std::max(max_idx, i);\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int result = -1;\n    size_t i = 0;\n    size_t vec_size = arr.size() / 8 * 8;\n\n    __m256i indices = _mm256_setr_epi32(0,1,2,3,4,5,6,7);\n\n    for (; i <= vec_size - 8; i += 8){\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i + 1]));\n\n        __m256i cmp_res = _mm256_cmpgt_epi32(next, current);\n        __m256i mask = _mm256_xor_si256(cmp_res, _mm256_set1_epi32(-1));\n\n        __m256i masked_indices = _mm256_and_si256(mask, indices);\n        __m256i max_index = _mm256_max_epi32(masked_indices, _mm256_set1_epi32(result + i));\n\n        int temp_result = _mm256_extract_epi32(max_index, 7);\n        result = std::max(result, temp_result - i);\n    }\n\n    for (; i < arr.size() - 1; ++i){\n        if (arr[i] > arr[i+1]){\n            result = i;\n        }\n    }\n\n    return result == -1 ? -1 : result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int result = -1;\n    size_t n = arr.size();\n    size_t aligned_size = n - (n % 8);\n\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i max_index = _mm256_set1_epi32(-1);\n\n    for(size_t i = 0; i < aligned_size - 1; i += 8){\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i+1]));\n\n        __m256i cmp = _mm256_cmpgt_epi32(next, current);\n        __m256i mask = _mm256_xor_si256(cmp, _mm256_set1_epi32(-1));\n\n        __m256i idx = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n        __m256i masked_idx = _mm256_and_si256(mask, idx);\n        __m256i max_idx = _mm256_max_epi32(max_index, masked_idx);\n        max_index = max_idx;\n    }\n\n    int max_val = -1;\n    alignas(32) int res[8];\n    _mm256_store_si256((__m256i*)res, max_index);\n    for(int i = 0; i < 8; ++i){\n        if(res[i] > max_val) max_val = res[i];\n    }\n    result = max_val;\n\n    for(size_t i = aligned_size; i < n-1; ++i){\n        if(arr[i+1] < arr[i]){\n            result = i+1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int n = arr.size();\n    __m256i indices = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i max_index = _mm256_set1_epi32(-1);\n\n    for(int i = 0; i + 8 <= n - 1; i += 8){\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i + 1]));\n\n        __m256i cmp = _mm256_cmpgt_epi32(next, current); // next > current\n        __m256i mask = _mm256_xor_si256(cmp, _mm256_set1_epi32(-1)); // Invert the comparison\n\n        __m256i current_indices = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n        __m256i masked_indices = _mm256_and_si256(mask, current_indices);\n\n        __m256i max_masked = _mm256_max_epi32(masked_indices, max_index);\n        max_index = max_masked;\n    }\n\n    int result = -1;\n    alignas(32) int max_index_array[8];\n    _mm256_store_si256((__m256i*)max_index_array, max_index);\n    for(int i = 0; i < 8; ++i){\n        if(max_index_array[i] != -1 && max_index_array[i] > result){\n            result = max_index_array[i];\n        }\n    }\n\n    for(int i = ((n - 1) / 8) * 8; i < n - 1; ++i){\n        if(arr[i + 1] <= arr[i]){\n            result = std::max(result, i);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    int max_neg = INT_MIN;\n    int min_pos = INT_MAX;\n\n    size_t size = lst.size();\n    const int* data = lst.data();\n\n    __m256i v_max_neg = _mm256_set1_epi32(INT_MIN);\n    __m256i v_min_pos = _mm256_set1_epi32(INT_MAX);\n\n    for(size_t i = 0; i <= size - 8; i += 8){\n        __m256i v_data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        \n        __m256i v_neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), v_data);\n        __m256i v_pos_mask = _mm256_cmpgt_epi32(v_data, _mm256_setzero_si256());\n\n        __m256i v_neg = _mm256_and_si256(v_neg_mask, v_data);\n        __m256i v_pos = _mm256_and_si256(v_pos_mask, v_data);\n\n        v_max_neg = _mm256_max_epi32(v_max_neg, v_neg);\n        v_min_pos = _mm256_min_epi32(v_min_pos, v_pos);\n    }\n\n    // Horizontal max for negatives\n    int negs[8];\n    _mm256_storeu_si256((__m256i*)negs, v_max_neg);\n    for(int i = 0; i < 8; ++i){\n        if(negs[i] > max_neg) max_neg = negs[i];\n    }\n\n    // Horizontal min for positives\n    int poss[8];\n    _mm256_storeu_si256((__m256i*)poss, v_min_pos);\n    for(int i = 0; i < 8; ++i){\n        if(poss[i] < min_pos) min_pos = poss[i];\n    }\n\n    // Handle remaining elements\n    for(size_t i = size - (size % 8); i < size; ++i){\n        if(data[i] < 0 && data[i] > max_neg) max_neg = data[i];\n        if(data[i] > 0 && data[i] < min_pos) min_pos = data[i];\n    }\n\n    if(max_neg == INT_MIN) max_neg = 0;\n    if(min_pos == INT_MAX) min_pos = 0;\n\n    return {max_neg, min_pos};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    const int simd_width = 8;\n    size_t n = lst.size();\n    size_t aligned_size = n / simd_width * simd_width;\n\n    __m256i max_neg = _mm256_set1_epi32(INT_MIN);\n    __m256i min_pos = _mm256_set1_epi32(INT_MAX);\n\n    for(size_t i = 0; i < aligned_size; i += simd_width){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        \n        // Create mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i neg_nums = _mm256_and_si256(vec, neg_mask);\n        max_neg = _mm256_max_epi32(max_neg, neg_nums);\n\n        // Create mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i pos_nums = _mm256_and_si256(vec, pos_mask);\n        min_pos = _mm256_min_epi32(min_pos, pos_nums);\n    }\n\n    // Horizontal max for negatives\n    int max_neg_h[8];\n    _mm256_storeu_si256((__m256i*)max_neg_h, max_neg);\n    int max_neg_val = INT_MIN;\n    for(int i = 0; i < 8; ++i){\n        if(max_neg_h[i] > max_neg_val) max_neg_val = max_neg_h[i];\n    }\n\n    // Horizontal min for positives\n    int min_pos_h[8];\n    _mm256_storeu_si256((__m256i*)min_pos_h, min_pos);\n    int min_pos_val = INT_MAX;\n    for(int i = 0; i < 8; ++i){\n        if(min_pos_h[i] < min_pos_val && min_pos_h[i] > 0) min_pos_val = min_pos_h[i];\n    }\n\n    // Process remaining elements\n    for(size_t i = aligned_size; i < n; ++i){\n        if(lst[i] < 0 && lst[i] > max_neg_val) max_neg_val = lst[i];\n        if(lst[i] > 0 && lst[i] < min_pos_val) min_pos_val = lst[i];\n    }\n\n    if(max_neg_val == INT_MIN) max_neg_val = 0;\n    if(min_pos_val == INT_MAX) min_pos_val = 0;\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    __m256i max_neg = _mm256_set1_epi32(0x80000000);\n    __m256i min_pos = _mm256_set1_epi32(0x7FFFFFFF);\n\n    const int* data = lst.data();\n    size_t size = lst.size();\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        \n        // Create mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i neg_values = _mm256_and_si256(vec, neg_mask);\n        max_neg = _mm256_max_epi32(max_neg, neg_values);\n\n        // Create mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i pos_values = _mm256_and_si256(vec, pos_mask);\n        min_pos = _mm256_min_epi32(min_pos, pos_values);\n    }\n\n    // Horizontal max for negatives\n    int max_neg_arr[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(max_neg_arr), max_neg);\n    int max_neg_val = *std::max_element(max_neg_arr, max_neg_arr + 8);\n    if(max_neg_val == 0x80000000) max_neg_val = 0;\n\n    // Horizontal min for positives\n    int min_pos_arr[8];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(min_pos_arr), min_pos);\n    int min_pos_val = *std::min_element(min_pos_arr, min_pos_arr + 8);\n    if(min_pos_val == 0x7FFFFFFF) min_pos_val = 0;\n\n    // Process remaining elements\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        if(data[i] < 0 && data[i] > max_neg_val) max_neg_val = data[i];\n        if(data[i] > 0 && data[i] < min_pos_val) min_pos_val = data[i];\n    }\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    __m256i max_neg = _mm256_set1_epi32(0x80000000);\n    __m256i min_pos = _mm256_set1_epi32(0x7FFFFFFF);\n\n    bool has_neg = false;\n    bool has_pos = false;\n\n    for(size_t i = 0; i + 7 < lst.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i cmp_neg = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i cmp_pos = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n\n        __m256i negs = _mm256_and_si256(cmp_neg, vec);\n        __m256i poss = _mm256_and_si256(cmp_pos, vec);\n\n        __m256i mask_neg = _mm256_movemask_epi8(cmp_neg);\n        __m256i mask_pos = _mm256_movemask_epi8(cmp_pos);\n\n        if(mask_neg){\n            has_neg = true;\n            max_neg = _mm256_max_epi32(max_neg, negs);\n        }\n        if(mask_pos){\n            has_pos = true;\n            min_pos = _mm256_min_epi32(min_pos, poss);\n        }\n    }\n\n    int max_neg_val = 0;\n    int min_pos_val = 0;\n\n    if(has_neg){\n        int data[8];\n        _mm256_storeu_si256((__m256i*)data, max_neg);\n        max_neg_val = *std::max_element(data, data + 8);\n    }\n\n    if(has_pos){\n        int data[8];\n        _mm256_storeu_si256((__m256i*)data, min_pos);\n        min_pos_val = *std::min_element(data, data + 8);\n    }\n\n    for(size_t i = lst.size() - (lst.size() % 8); i < lst.size(); ++i){\n        if(lst[i] < 0 && (!has_neg || lst[i] > max_neg_val)){\n            max_neg_val = lst[i];\n            has_neg = true;\n        }\n        if(lst[i] > 0 && (!has_pos || lst[i] < min_pos_val)){\n            min_pos_val = lst[i];\n            has_pos = true;\n        }\n    }\n\n    if(!has_neg) max_neg_val = 0;\n    if(!has_pos) min_pos_val = 0;\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if(lst.empty()){\n        return {0, 0};\n    }\n\n    __m256i max_neg = _mm256_set1_epi32(INT_MIN);\n    __m256i min_pos = _mm256_set1_epi32(INT_MAX);\n\n    const int* data = lst.data();\n    size_t size = lst.size();\n\n    for(size_t i = 0; i < size / 8 * 8; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        \n        // Create mask for negative numbers\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n        __m256i neg_vals = _mm256_and_si256(vec, neg_mask);\n        max_neg = _mm256_max_epi32(max_neg, neg_vals);\n\n        // Create mask for positive numbers\n        __m256i pos_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i pos_vals = _mm256_and_si256(vec, pos_mask);\n        min_pos = _mm256_min_epi32(min_pos, pos_vals);\n    }\n\n    // Horizontal max for negatives\n    int max_neg_arr[8];\n    _mm256_storeu_si256((__m256i*)max_neg_arr, max_neg);\n    int a = INT_MIN;\n    for(int i = 0; i < 8; ++i){\n        if(max_neg_arr[i] < 0 && max_neg_arr[i] > a){\n            a = max_neg_arr[i];\n        }\n    }\n\n    // Horizontal min for positives\n    int min_pos_arr[8];\n    _mm256_storeu_si256((__m256i*)min_pos_arr, min_pos);\n    int b = INT_MAX;\n    for(int i = 0; i < 8; ++i){\n        if(min_pos_arr[i] > 0 && min_pos_arr[i] < b){\n            b = min_pos_arr[i];\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = size / 8 * 8; i < size; ++i){\n        if(data[i] < 0 && data[i] > a){\n            a = data[i];\n        }\n        if(data[i] > 0 && data[i] < b){\n            b = data[i];\n        }\n    }\n\n    if(a == INT_MIN) a = 0;\n    if(b == INT_MAX) b = 0;\n\n    return {a, b};\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if (n == 0 || n == 1) return 1;\n\n    __m256i vresult = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            factorial *= j;\n        }\n        __m256i vf = _mm256_set1_epi64x(factorial);\n        vresult = _mm256_mul_epi32(vresult, vf);\n    }\n\n    uint64_t intermediate_results[4];\n    _mm256_storeu_si256((__m256i*)intermediate_results, vresult);\n\n    result *= intermediate_results[0];\n    result *= intermediate_results[1];\n    result *= intermediate_results[2];\n    result *= intermediate_results[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if(n == 0 || n == 1) return 1;\n    \n    __m256i vresult = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            factorial *= j;\n        }\n        result *= factorial;\n        \n        // Vectorize the multiplication if possible\n        __m256i vf = _mm256_set1_epi64x(factorial);\n        vresult = _mm256_mul_epi32(vresult, vf);\n    }\n\n    uint64_t r[4];\n    _mm256_storeu_si256((__m256i*)r, vresult);\n\n    uint64_t final_result = r[0] * r[1] * r[2] * r[3] * result;\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if (n == 0 || n == 1) return 1;\n\n    __m256i vresult = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            factorial *= j;\n        }\n        __m256i vf = _mm256_set1_epi64x(factorial);\n        vresult = _mm256_mul_epi32(vresult, vf);\n        \n        uint64_t temp[4];\n        _mm256_storeu_si256((__m256i*)temp, vresult);\n        result = temp[0] * temp[1] * temp[2] * temp[3];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if (n == 0 || n == 1) return 1;\n\n    __m256i vresult = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            factorial *= j;\n        }\n        __m256i vf = _mm256_set1_epi64x(factorial);\n        vresult = _mm256_mul_epi32(vresult, vf);\n    }\n\n    uint64_t intermediate_results[4] __attribute__((aligned(32)));\n    _mm256_store_si256((__m256i*)intermediate_results, vresult);\n\n    result *= intermediate_results[0];\n    result *= intermediate_results[1];\n    result *= intermediate_results[2];\n    result *= intermediate_results[3];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if (n == 0 || n == 1) return 1;\n\n    __m256i vresult = _mm256_set1_epi64x(1);\n    uint64_t result = 1;\n\n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            factorial *= j;\n        }\n        __m256i vf = _mm256_set1_epi64x(factorial);\n        vresult = _mm256_mul_epi32(vresult, vf);\n        uint64_t temp_result[4];\n        _mm256_storeu_si256((__m256i*)temp_result, vresult);\n        result = temp_result[0] * temp_result[1] * temp_result[2] * temp_result[3];\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if(lst.empty()) return 0;\n\n    const int n = lst.size();\n    alignas(32) int temp[8];\n    __m256i sum = _mm256_setzero_si256();\n    __m256i v32ones = _mm256_set1_epi32(1);\n\n    for(int i = 0; i < n; i += 8){\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mod3 = _mm256_andnot_si256(_mm256_srli_epi32(indices, 1), v32ones); // Check if multiple of 3\n        __m256i mod4 = _mm256_andnot_si256(_mm256_srli_epi32(indices, 2), v32ones); // Check if multiple of 4\n\n        __m256i mask_mod3 = _mm256_cmpeq_epi32(mod3, v32ones);\n        __m256i mask_mod4 = _mm256_cmpeq_epi32(mod4, v32ones);\n        __m256i mask_mod3_not_mod4 = _mm256_andnot_si256(mask_mod3, mask_mod4);\n\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(squared, vec);\n\n        __m256i result = _mm256_blendv_epi8(vec, cubed, mask_mod3_not_mod4);\n        result = _mm256_blendv_epi8(result, squared, mask_mod3);\n\n        sum = _mm256_add_epi32(sum, result);\n    }\n\n    _mm256_store_si256((__m256i*)temp, sum);\n    int total_sum = 0;\n    for(int i = 0; i < 8; ++i){\n        total_sum += temp[i];\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if(lst.empty()) return 0;\n\n    const int n = lst.size();\n    alignas(32) int temp[8];\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i indices_vec = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n\n    for(int i=0;i<n;i+=8){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i indices = _mm256_add_epi32(indices_vec, _mm256_set1_epi32(i));\n\n        // Mask for multiple of 3\n        __m256i mod3 = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_rem_epu32(indices, _mm256_set1_epi32(3)), _mm256_setzero_si256());\n        __m256i square_mask = _mm256_cmpgt_epi32(mod3, _mm256_setzero_si256());\n\n        // Mask for multiple of 4 and not multiple of 3\n        __m256i mod4 = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_rem_epu32(indices, _mm256_set1_epi32(4)), _mm256_setzero_si256());\n        __m256i cube_mask = _mm256_andnot_si256(mod3, mod4);\n\n        // Square the values where index is multiple of 3\n        __m256i squared = _mm256_mullo_epi32(data, data);\n        data = _mm256_blendv_epi8(data, squared, square_mask);\n\n        // Cube the values where index is multiple of 4 and not multiple of 3\n        __m256i cubed = _mm256_mullo_epi32(squared, data);\n        data = _mm256_blendv_epi8(data, cubed, cube_mask);\n\n        sum_vec = _mm256_add_epi32(sum_vec, data);\n    }\n\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    int sum = 0;\n    for(int i=0;i<8;i++) sum += temp[i];\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if (lst.empty()) return 0;\n\n    const int N = lst.size();\n    alignas(32) int temp[8];\n    __m256i sum = _mm256_setzero_si256();\n    __m256i indices = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n\n    for(int i = 0; i <= N - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i idx = _mm256_add_epi32(indices, _mm256_set1_epi32(i));\n\n        // Mask for multiple of 3\n        __m256i mod3 = _mm256_andnot_si256(_mm256_cmpeq_epi32(_mm256_set1_epi32(0), \n                                _mm256_i32gather_epi32(temp, reinterpret_cast<const int*>(idx), 4)), \n                                _mm256_cmpeq_epi32(_mm256_set1_epi32(0), \n                                _mm256_mod_epi32(idx, _mm256_set1_epi32(3))));\n        __m256i sq_vec = _mm256_mullo_epi32(vec, vec);\n        __m256i sq_masked = _mm256_blendv_epi8(vec, sq_vec, mod3);\n\n        // Mask for multiple of 4 and not multiple of 3\n        __m256i mod4 = _mm256_cmpeq_epi32(_mm256_setzero_si256(), \n                            _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_set1_epi32(0), \n                            _mm256_mod_epi32(idx, _mm256_set1_epi32(4))), \n                            _mm256_xor_si256(mod3, _mm256_set1_epi32(-1))));\n        __m256i cb_vec = _mm256_mullo_epi32(sq_vec, vec);\n        __m256i cb_masked = _mm256_blendv_epi8(sq_masked, cb_vec, mod4);\n\n        sum = _mm256_add_epi32(sum, cb_masked);\n    }\n\n    int total_sum = 0;\n    __m256i sum_h = _mm256_hadd_epi32(sum, sum);\n    sum_h = _mm256_hadd_epi32(sum_h, sum_h);\n    _mm256_storeu_si256((__m256i*)&temp, sum_h);\n    total_sum += temp[0] + temp[4];\n\n    for(int i = N - N%8; i < N; ++i){\n        if(i % 3 == 0){\n            total_sum += lst[i] * lst[i];\n        }\n        else if(i % 4 == 0){\n            total_sum += lst[i] * lst[i] * lst[i];\n        }\n        else{\n            total_sum += lst[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if (lst.empty()) return 0;\n\n    const int n = lst.size();\n    int remainder_sum = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i thirtytwos = _mm256_set1_epi32(32);\n\n    for(int i = 0; i < n; i += 8){\n        if(i + 8 <= n){\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n            __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n\n            // Mask for multiple of 3\n            __m256i mod3 = _mm256_and_si256(_mm256_cmpeq_epi32(_mm256_mullo_epi32(indices, _mm256_set1_epi32(922337203)), _mm256_setzero_si256()), _mm256_set1_epi32(-1));\n            __m256i squared = _mm256_mullo_epi32(vec, vec);\n            vec = _mm256_blendv_epi8(vec, squared, mod3);\n\n            // Mask for multiple of 4 and not multiple of 3\n            __m256i mod4 = _mm256_andnot_si256(mod3, _mm256_cmpeq_epi32(_mm256_mullo_epi32(indices, _mm256_set1_epi32(536870913)), _mm256_setzero_si256()));\n            __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(vec, vec), vec);\n            vec = _mm256_blendv_epi8(vec, cubed, mod4);\n\n            sum_vec = _mm256_add_epi32(sum_vec, vec);\n        }\n        else{\n            for(int j = i; j < n; ++j){\n                if(j % 3 == 0){\n                    remainder_sum += lst[j] * lst[j];\n                }\n                else if(j % 4 == 0){\n                    remainder_sum += lst[j] * lst[j] * lst[j];\n                }\n                else{\n                    remainder_sum += lst[j];\n                }\n            }\n        }\n    }\n\n    __m128i sum_low = _mm_add_epi32(_mm256_castsi256_si128(sum_vec), _mm256_extracti128_si256(sum_vec, 1));\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    int total_sum = _mm_cvtsi128_si32(sum_low) + remainder_sum;\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    if (n == 0) return 0;\n\n    alignas(32) int temp[8];\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i indices_vec = _mm256_set_epi32(7,6,5,4,3,2,1,0);\n\n    for(int i = 0; i < n; i += 8){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i indices = _mm256_add_epi32(indices_vec, _mm256_set1_epi32(i));\n\n        // Mask for multiple of 3\n        __m256i mod3 = _mm256_and_si256(_mm256_cmpeq_epi32(indices, _mm256_set1_epi32(0)), \n                                        _mm256_cmpeq_epi32(_mm256_mod_epi32(indices, 3), _mm256_setzero_si256()));\n        __m256i square_mask = _mm256_cmpgt_epi32(mod3, _mm256_setzero_si256());\n\n        // Mask for multiple of 4 and not multiple of 3\n        __m256i mod4 = _mm256_andnot_si256(_mm256_cmpeq_epi32(indices, _mm256_set1_epi32(0)), \n                                           _mm256_cmpeq_epi32(_mm256_mod_epi32(indices, 4), _mm256_setzero_si256()));\n        __m256i cube_mask = _mm256_cmpgt_epi32(mod4, _mm256_setzero_si256());\n\n        // Square the values where index is multiple of 3\n        __m256i squared = _mm256_mullo_epi32(data, data);\n        __m256i result = _mm256_blendv_epi8(data, squared, square_mask);\n\n        // Cube the values where index is multiple of 4 and not multiple of 3\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(data, data), data);\n        result = _mm256_blendv_epi8(result, cubed, cube_mask);\n\n        // Add to sum\n        sum_vec = _mm256_add_epi32(sum_vec, _mm256_hadd_epi32(result, result));\n    }\n\n    _mm256_store_si256((__m256i*)temp, sum_vec);\n    int sum = 0;\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int threshold = 10;\n    const int vectorSize = 8;\n    __m256i vt = _mm256_set1_epi32(threshold);\n    size_t i = 0;\n\n    auto is_odd_first_last_digit = [](int x) -> int {\n        if (x == 0) return 0;\n        int abs_x = (x > 0) ? x : -x;\n        int first_digit = 0, last_digit = abs_x % 10;\n        while (abs_x >= 10) {\n            abs_x /= 10;\n            first_digit = abs_x % 10;\n        }\n        return ((first_digit & 1) && (last_digit & 1));\n    };\n\n    // Process in chunks of 8\n    for (; i <= nums.size() - vectorSize; i += vectorSize) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i cmp_res = _mm256_cmpgt_epi32(vec, vt); // vec > 10\n\n        int mask = _mm256_movemask_epi8(cmp_res); // Get mask for numbers > 10\n        int temp_count = 0;\n\n        for (int j = 0; j < vectorSize; ++j) {\n            if (mask & (1 << (4*j))) { // Check if the j-th element is greater than 10\n                if (is_odd_first_last_digit(nums[i + j])) {\n                    temp_count++;\n                }\n            }\n        }\n        count += temp_count;\n    }\n\n    // Process remaining elements\n    for (; i < nums.size(); ++i) {\n        if (nums[i] > 10 && is_odd_first_last_digit(nums[i])) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    const int threshold = 10;\n    __m256i v_threshold = _mm256_set1_epi32(threshold);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_ten = _mm256_set1_epi32(10);\n\n    for(size_t i = 0; i + 8 <= n; i += 8){\n        __m256i v_nums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i v_cmp_gt = _mm256_cmpgt_epi32(v_nums, v_threshold);\n\n        // Extract absolute values\n        __m256i v_abs = _mm256_abs_epi32(v_nums);\n\n        // Compute last digit\n        __m256i v_last_digit = _mm256_and_si256(_mm256_abs_epi32(_mm256_rem_epu32(v_abs, v_ten)), v_one);\n\n        // Compute first digit\n        __m256i v_first_digit_mask = _mm256_setr_epi32(1,10,100,1000,10000,100000,1000000,10000000);\n        __m256i v_first_digit = _mm256_and_si256(_mm256_div_epi32(v_abs, v_first_digit_mask), v_one);\n\n        // Check if both first and last digits are odd\n        __m256i v_odd_check = _mm256_and_si256(v_last_digit, v_first_digit);\n\n        // Combine conditions\n        __m256i v_final_check = _mm256_and_si256(v_cmp_gt, v_odd_check);\n\n        // Count the number of valid elements\n        __m256i v_ones = _mm256_set1_epi32(1);\n        __m256i v_count = _mm256_cmpeq_epi32(v_final_check, v_final_check);\n        v_count = _mm256_and_si256(v_count, v_ones);\n\n        int temp_count[8];\n        _mm256_storeu_si256((__m256i*)temp_count, v_count);\n        count += temp_count[0] + temp_count[1] + temp_count[2] + temp_count[3] \n               + temp_count[4] + temp_count[5] + temp_count[6] + temp_count[7];\n    }\n\n    // Handle remaining elements\n    for(size_t i = (n / 8) * 8; i < n; ++i){\n        int num = nums[i];\n        if(num > 10){\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            int first_digit = abs_num;\n            while(first_digit >= 10) first_digit /= 10;\n            if(last_digit % 2 == 1 && first_digit % 2 == 1){\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int n = nums.size();\n    if (n == 0) return 0;\n\n    __m256i ten = _mm256_set1_epi32(10);\n    __m256i mask_odd = _mm256_set1_epi32(0x55555555);\n    __m256i mask_pos = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for(size_t i = 0; i + 7 < n; i += 8){\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i abs_data = _mm256_and_si256(data, mask_pos);\n        \n        // Check if greater than 10\n        __m256i cmp_gt_10 = _mm256_cmpgt_epi32(abs_data, ten);\n        \n        // Extract last digit\n        __m256i last_digit = _mm256_and_si256(abs_data, _mm256_set1_epi32(0xF));\n        __m256i is_last_odd = _mm256_and_si256(last_digit, mask_odd);\n        \n        // Extract first digit\n        __m256i div_10 = _mm256_set1_epi32(10);\n        __m256i first_digit_mask = _mm256_setr_epi32(1,10,100,1000,10000,100000,1000000,10000000);\n        __m256i first_digits = _mm256_div_epu32(abs_data, first_digit_mask);\n        __m256i is_first_odd = _mm256_and_si256(first_digits, mask_odd);\n        \n        // Combine conditions\n        __m256i combined = _mm256_and_si256(cmp_gt_10, _mm256_and_si256(is_last_odd, is_first_odd));\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i res = _mm256_cmpeq_epi32(combined, _mm256_setzero_si256());\n        __m256i cnt = _mm256_sub_epi32(_mm256_set1_epi32(8), _mm256_movemask_epi8(res));\n        count += _mm256_extract_epi32(cnt, 0);\n    }\n\n    for(size_t i = (n / 8) * 8; i < n; ++i){\n        int num = nums[i];\n        if(num > 10 || -num > 10){\n            int abs_num = num > 0 ? num : -num;\n            int last_digit = abs_num % 10;\n            int first_digit = 0;\n            if(abs_num >= 10000000){\n                first_digit = abs_num / 10000000;\n            }\n            else if(abs_num >= 1000000){\n                first_digit = (abs_num / 1000000) % 10;\n            }\n            else if(abs_num >= 100000){\n                first_digit = (abs_num / 100000) % 10;\n            }\n            else if(abs_num >= 10000){\n                first_digit = (abs_num / 10000) % 10;\n            }\n            else if(abs_num >= 1000){\n                first_digit = (abs_num / 1000) % 10;\n            }\n            else if(abs_num >= 100){\n                first_digit = (abs_num / 100) % 10;\n            }\n            else if(abs_num >= 10){\n                first_digit = (abs_num / 10) % 10;\n            }\n            else{\n                first_digit = abs_num;\n            }\n            if((first_digit & 1) && (last_digit & 1)){\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    size_t n = nums.size();\n    if (n == 0) return 0;\n\n    __m256i v_ten = _mm256_set1_epi32(10);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_nine = _mm256_set1_epi32(9);\n    __m256i v_mask_odd = _mm256_set1_epi32(0x55555555);\n\n    for(size_t i = 0; i + 8 <= n; i += 8){\n        __m256i v_nums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i v_cmp_gt_10 = _mm256_cmpgt_epi32(v_nums, v_ten);\n\n        // Absolute value\n        __m256i v_abs = _mm256_abs_epi32(v_nums);\n\n        // Extract last digit\n        __m256i v_last_digit = _mm256_and_si256(_mm256_cvtepi32_epi8(v_abs), v_mask_odd);\n        __m256i v_last_odd = _mm256_and_si256(v_last_digit, v_one);\n\n        // Extract first digit\n        __m256i v_divider = _mm256_setr_epi32(1,10,100,1000,10000,100000,1000000,10000000);\n        __m256i v_first_digit = _mm256_max_epi32(_mm256_setzero_si256(), _mm256_sub_epi32(v_abs, _mm256_mullo_epi32(_mm256_div_epi32(v_abs, v_divider), v_divider)));\n        __m256i v_first_odd = _mm256_and_si256(_mm256_cvtepi32_epi8(v_first_digit), v_mask_odd);\n\n        // Combine conditions\n        __m256i v_combined = _mm256_and_si256(v_cmp_gt_10, _mm256_and_si256(v_last_odd, v_first_odd));\n        __m256i v_ones = _mm256_set1_epi32(1);\n        __m256i v_count = _mm256_cmpeq_epi32(v_combined, v_ones);\n\n        // Horizontal sum\n        int temp_count[8];\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp_count), v_count);\n        for(int j=0; j<8; ++j){\n            count += (temp_count[j] != 0);\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = (n / 8) * 8; i < n; ++i){\n        int num = nums[i];\n        if(num > 10){\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            int first_digit = 0;\n            while(abs_num >= 10){\n                abs_num /= 10;\n            }\n            first_digit = abs_num;\n            if((last_digit & 1) && (first_digit & 1)){\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int threshold = 10;\n    const int odd_digits_mask = 0x15555555; // Mask for first and last digits being odd\n\n    size_t i = 0;\n    size_t vec_size = nums.size();\n\n    __m256i cmp_result, mask_result, valid_numbers, ones;\n\n    for (; i + 7 < vec_size; i += 8) {\n        __m256i data = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i abs_data = _mm256_abs_epi32(data);\n\n        // Check if numbers are greater than 10\n        cmp_result = _mm256_cmpgt_epi32(abs_data, _mm256_set1_epi32(threshold));\n\n        // Extract first and last digits\n        __m256i div_10 = _mm256_set1_epi32(10);\n        __m256i last_digit = _mm256_and_si256(_mm256_set1_epi32(odd_digits_mask), _mm256_rem_epu32(abs_data, div_10));\n        __m256i first_digit = _mm256_and_si256(_mm256_set1_epi32(odd_digits_mask), _mm256_div_epu32(abs_data, _mm256_set1_epi32(1)));\n\n        // Check if both first and last digits are odd\n        mask_result = _mm256_and_si256(last_digit, first_digit);\n\n        // Combine conditions\n        valid_numbers = _mm256_and_si256(cmp_result, mask_result);\n\n        // Count the number of valid elements\n        ones = _mm256_set1_epi32(1);\n        __m256i valid_count = _mm256_cmpeq_epi32(valid_numbers, _mm256_setzero_si256());\n        valid_count = _mm256_sub_epi32(_mm256_set1_epi32(8), valid_count);\n        count += _mm256_extract_epi32(_mm256_hadd_epi32(valid_count, valid_count), 0);\n    }\n\n    for (; i < vec_size; ++i) {\n        int num = std::abs(nums[i]);\n        if (num > 10) {\n            int first_digit = num;\n            while (first_digit >= 10) first_digit /= 10;\n            int last_digit = num % 10;\n            if ((first_digit & 1) && (last_digit & 1)) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if (n < 3) return 0;\n\n    const uint64_t mod[3] = {0};\n    __m256i vthree = _mm256_set1_epi64x(3);\n    __m256i vone = _mm256_set1_epi64x(1);\n\n    uint64_t count_mod[3] = {0};\n    for(uint64_t i = 1; i <= n; ++i){\n        uint64_t val = i * i - i + 1;\n        __m256i vval = _mm256_set1_epi64x(val);\n        __m256i vrem = _mm256_rem_epu64(vval, vthree); // Hypothetical intrinsic for modulo operation\n        uint64_t rem = _mm256_extract_epi64(vrem, 0);\n        count_mod[rem]++;\n    }\n\n    uint64_t triples = 0;\n    triples += count_mod[0] * (count_mod[0] - 1) * (count_mod[0] - 2) / 6;\n    triples += count_mod[1] * count_mod[1] * count_mod[1] / 6;\n    triples += count_mod[2] * count_mod[2] * count_mod[2] / 6;\n    triples += count_mod[0] * count_mod[1] * count_mod[2];\n\n    return triples;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const uint64_t len = n;\n    uint64_t count[3] = {0};\n\n    __m256i vec_three = _mm256_set1_epi64x(3);\n    __m256i vec_one = _mm256_set1_epi64x(1);\n\n    for(uint64_t i=1; i<=len; ++i){\n        uint64_t val = i * i - i + 1;\n        __m256i vec_val = _mm256_set1_epi64x(val);\n        __m256i mod_result = _mm256_cvtepi64_epi8(_mm256_and_si256(_mm256_sub_epi64(vec_val, vec_one), vec_three));\n        uint8_t rem = (uint8_t)_mm256_extract_epi8(mod_result, 0);\n        count[rem]++;\n    }\n\n    uint64_t result = count[0] * (count[0]-1) * (count[0]-2) / 6 \n                    + count[0] * count[1] * count[2];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const uint64_t len = n;\n    uint64_t count[3] = {0};\n    __m256i v3 = _mm256_set1_epi64x(3);\n    __m256i v1 = _mm256_set1_epi64x(1);\n\n    for(uint64_t i = 1; i <= len; ++i){\n        uint64_t val = i * i - i + 1;\n        __m256i vval = _mm256_set1_epi64x(val);\n        __m256i mod = _mm256_and_si256(_mm256_sub_epi64(vval, v1), v3);\n        uint64_t mod_val = _mm256_extract_epi64(mod, 0);\n        count[mod_val]++;\n    }\n\n    uint64_t res = count[0] / 3 + count[0] * count[1] * count[2];\n    if(count[1] >= 3){\n        __m256i vc1 = _mm256_set1_epi64x(count[1]);\n        __m256i three = _mm256_set1_epi64x(3);\n        __m256i div = _mm256_div_epi64(vc1, three);\n        uint64_t div_val = _mm256_extract_epi64(div, 0);\n        res += div_val * (div_val - 1) * (div_val - 2) / 6;\n    }\n    if(count[2] >= 3){\n        __m256i vc2 = _mm256_set1_epi64x(count[2]);\n        __m256i three = _mm256_set1_epi64x(3);\n        __m256i div = _mm256_div_epi64(vc2, three);\n        uint64_t div_val = _mm256_extract_epi64(div, 0);\n        res += div_val * (div_val - 1) * (div_val - 2) / 6;\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const int vec_size = 8;\n    uint64_t count = 0;\n    __m256i mod_0 = _mm256_setzero_si256();\n    __m256i mod_1 = _mm256_setzero_si256();\n    __m256i mod_2 = _mm256_setzero_si256();\n\n    uint64_t *a = (uint64_t *)_mm_malloc(n * sizeof(uint64_t), 32);\n\n    for(uint64_t i = 1; i <= n; ++i){\n        a[i-1] = i * i - i + 1;\n    }\n\n    for(uint64_t i = 0; i < n; i += vec_size){\n        uint64_t limit = (i + vec_size > n) ? n - i : vec_size;\n        __m256i vec_a = _mm256_loadu_si256((__m256i *)(a + i));\n        __m256i rem = _mm256_and_si256(_mm256_sub_epi64(vec_a, _mm256_set1_epi64x(1)), _mm256_set1_epi64x(2));\n\n        __m256i eq_0 = _mm256_cmpeq_epi64(rem, _mm256_setzero_si256());\n        __m256i eq_1 = _mm256_cmpeq_epi64(rem, _mm256_set1_epi64x(1));\n        __m256i eq_2 = _mm256_cmpeq_epi64(rem, _mm256_set1_epi64x(2));\n\n        mod_0 = _mm256_add_epi64(mod_0, _mm256_and_si256(eq_0, _mm256_set1_epi64x(-1)));\n        mod_1 = _mm256_add_epi64(mod_1, _mm256_and_si256(eq_1, _mm256_set1_epi64x(-1)));\n        mod_2 = _mm256_add_epi64(mod_2, _mm256_and_si256(eq_2, _mm256_set1_epi64x(-1)));\n    }\n\n    uint64_t sum_mod_0 = 0, sum_mod_1 = 0, sum_mod_2 = 0;\n\n    sum_mod_0 += _mm256_extract_epi64(mod_0, 0) + _mm256_extract_epi64(mod_0, 1) +\n                 _mm256_extract_epi64(mod_0, 2) + _mm256_extract_epi64(mod_0, 3);\n    sum_mod_1 += _mm256_extract_epi64(mod_1, 0) + _mm256_extract_epi64(mod_1, 1) +\n                 _mm256_extract_epi64(mod_1, 2) + _mm256_extract_epi64(mod_1, 3);\n    sum_mod_2 += _mm256_extract_epi64(mod_2, 0) + _mm256_extract_epi64(mod_2, 1) +\n                 _mm256_extract_epi64(mod_2, 2) + _mm256_extract_epi64(mod_2, 3);\n\n    count += (sum_mod_0 * (sum_mod_0 - 1) * (sum_mod_0 - 2)) / 6;\n    count += (sum_mod_1 * (sum_mod_1 - 1) * (sum_mod_1 - 2)) / 6;\n    count += (sum_mod_2 * (sum_mod_2 - 1) * (sum_mod_2 - 2)) / 6;\n    count += sum_mod_0 * sum_mod_1 * sum_mod_2;\n\n    _mm_free(a);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const uint64_t len = n;\n    uint32_t *a = (uint32_t*) _mm_malloc(len * sizeof(uint32_t), 32);\n\n    __m256i v_i = _mm256_set_epi32(8,7,6,5,4,3,2,1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_mod3 = _mm256_set1_epi32(3);\n\n    for(uint64_t i=0;i<len;i+=8){\n        __m256i idx = _mm256_add_epi32(v_i, _mm256_set1_epi32(i));\n        __m256i sq = _mm256_mullo_epi32(idx, idx);\n        __m256i val = _mm256_sub_epi32(sq, _mm256_add_epi32(idx, v_one));\n        _mm256_storeu_si256((__m256i*)&a[i], val);\n    }\n\n    uint64_t count[3] = {0};\n    for(uint64_t i=0;i<len;i++) count[a[i] % 3]++;\n\n    uint64_t res = count[0]*(count[0]-1)*(count[0]-2)/6 \n                 + count[0]*count[1]*count[2]\n                 + count[1]*(count[1]-1)*(count[1]-2)/6 \n                 + count[2]*(count[2]-1)*(count[2]-2)/6;\n\n    _mm_free(a);\n    return res;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    int sqrt_n = (int)(sqrt((double)n));\n    int is_prime = 1;\n\n    for(int i=2; i<=sqrt_n; i++){\n        __m256i vec_i = _mm256_set1_epi32(i);\n        __m256i vec_mod = _mm256_rem_epi32(vec_n, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi32(vec_mod, vec_zero);\n        int mask = _mm256_movemask_epi8(vec_cmp);\n        if(mask != 0){\n            is_prime = 0;\n            break;\n        }\n    }\n\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n\n    __m256i vn = _mm256_set1_epi32(n);\n    __m256i vone = _mm256_set1_epi32(1);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i is_prime = _mm256_set1_epi32(1);\n\n    int limit = (int)(sqrt((double)n) + 1e-8);\n    for(int i=2;i<=limit;i++){\n        __m256i vi = _mm256_set1_epi32(i);\n        __m256i cmp_res = _mm256_cmpeq_epi32(_mm256_rem_epu32(vn, vi), vzero);\n        __m256i and_res = _mm256_and_si256(cmp_res, _mm256_set1_epi32(1));\n        is_prime = _mm256_andnot_si256(and_res, is_prime);\n    }\n\n    int mask = -_mm256_movemask_epi8(is_prime);\n    return (mask & 0xFFFFFFFF) ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n    __m256i vn = _mm256_set1_epi32(n);\n    __m256i vone = _mm256_set1_epi32(1);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i is_prime = vone;\n\n    for(int i=2; i*i <=n; ++i){\n        __m256i vi = _mm256_set1_epi32(i);\n        __m256i remainder = _mm256_rem_epi32(vn, vi);\n        __m256i cmp_result = _mm256_cmpeq_epi32(remainder, vzero);\n        is_prime = _mm256_andnot_si256(cmp_result, is_prime);\n    }\n\n    int mask = _mm256_movemask_epi8(is_prime);\n    if(mask == 0xFFFF){\n        return x;\n    }\n    return y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n\n    __m256i vn = _mm256_set1_epi32(n);\n    __m256i vone = _mm256_set1_epi32(1);\n    __m256i vzero = _mm256_setzero_si256();\n    __m256i is_prime = _mm256_set1_epi32(1);\n\n    int limit = (int)(sqrt((double)n) + 1e-8);\n    for(int i=2;i<=limit;i++){\n        __m256i vi = _mm256_set1_epi32(i);\n        __m256i cmp_res = _mm256_cmpeq_epi32(_mm256_rem_epi32(vn, vi), vzero);\n        __m256i mask = _mm256_andnot_si256(cmp_res, is_prime);\n        is_prime = mask;\n    }\n\n    int prime_check = _mm256_testz_si256(is_prime, is_prime);\n    if(prime_check){\n        return x;\n    }\n    else{\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```cpp\n#include <immintrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n\n    __m256i vec_n = _mm256_set1_epi32(n);\n    __m256i vec_one = _mm256_set1_epi32(1);\n    __m256i vec_zero = _mm256_setzero_si256();\n\n    int limit = (int)_mm256_cvtsi256_si32(_mm256_sqrt_epi32(_mm256_mullo_epi32(vec_n, vec_n)));\n    limit = (limit * limit <= n) ? limit : limit - 1;\n\n    for(int i = 2; i <= limit; ++i){\n        __m256i vec_i = _mm256_set1_epi32(i);\n        __m256i remainder = _mm256_sub_epi32(vec_n, _mm256_mullo_epi32(_mm256_div_epi32(vec_n, vec_i), vec_i));\n        __m256i is_zero = _mm256_cmpeq_epi32(remainder, vec_zero);\n        if(_mm256_testc_si256(is_zero, is_zero)){\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    if(lst.empty()) return 0;\n\n    const float *data = lst.data();\n    size_t size = lst.size();\n\n    __m256 zero = _mm256_set1_ps(0.0f);\n    __m256 two = _mm256_set1_ps(2.0f);\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256 vec = _mm256_loadu_ps(data + i);\n        __m256 floored = _mm256_floor_ps(vec); // Floor the values\n        __m256 diff = _mm256_sub_ps(vec, floored); // Check if they are integers\n        __m256 is_int = _mm256_cmp_ps(diff, zero, _CMP_EQ_OQ); // Compare to zero\n\n        __m256 abs_val = _mm256_andnot_ps(_mm256_cmp_ps(vec, zero, _CMP_LT_OQ), vec); // Get absolute value\n        __m256 is_odd_mask = _mm256_cmp_ps(_mm256_and_ps(abs_val, _mm256_set1_ps(1.0f)), _mm256_set1_ps(1.0f), _CMP_EQ_OQ); // Check if odd\n\n        __m256 mask = _mm256_and_ps(is_int, is_odd_mask); // Combine masks\n        __m256 squared = _mm256_mul_ps(vec, vec); // Square the values\n        __m256 selected = _mm256_and_ps(mask, squared); // Select squares of odd integers\n\n        __m256 doubled_diff = _mm256_mul_ps(selected, two); // Double the difference\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 0);\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 1);\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 2);\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 3);\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 4);\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 5);\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 6);\n        sum += _mm256_extract_epi32(_mm256_cvtps_epi32(doubled_diff), 7);\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        if(lst[i] > 0 && lst[i] == static_cast<int>(lst[i]) && static_cast<int>(lst[i]) % 2 != 0){\n            sum += static_cast<int>(lst[i]) * static_cast<int>(lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if (lst.empty()) return 0;\n\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    size_t aligned_size = size - (size % 8);\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i mask_odd = _mm256_set1_epi32(0xAAAAAAAA);\n    __m256i mask_sign = _mm256_set1_epi32(0x80000000);\n\n    for(size_t i = 0; i < aligned_size; i += 8){\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        __m256i data_int = _mm256_cvttps_epi32(data);\n\n        // Check for negative numbers\n        __m256i sign_bit = _mm256_and_si256(data_int, mask_sign);\n        __m256i non_negative = _mm256_cmpeq_epi32(sign_bit, zero);\n\n        // Check for odd numbers\n        __m256i odd_check = _mm256_and_si256(data_int, one);\n        __m256i is_odd = _mm256_cmpeq_epi32(odd_check, one);\n\n        // Combine masks: positive and odd\n        __m256i combined_mask = _mm256_and_si256(non_negative, is_odd);\n\n        // Compute squares\n        __m256 squared = _mm256_mul_ps(data, data);\n\n        // Extract integers from floats\n        __m256i squared_int = _mm256_cvttps_epi32(squared);\n\n        // Apply mask to squared values\n        __m256i masked_squares = _mm256_and_si256(squared_int, _mm256_castps_si256(_mm256_castsi256_ps(combined_mask)));\n\n        // Horizontal sum\n        __m128i low = _mm256_castsi256_si128(masked_squares);\n        __m128i high = _mm256_extracti128_si256(masked_squares, 1);\n        __m128i sum_128 = _mm_add_epi32(low, high);\n        uint32_t sums[4];\n        _mm_storeu_si128((__m128i*)sums, sum_128);\n        for(int j = 0; j < 4; ++j){\n            sum += sums[j];\n        }\n    }\n\n    for(size_t i = aligned_size; i < size; ++i){\n        int val = static_cast<int>(lst[i]);\n        if(val > 0 && (val & 1)){\n            sum += val * val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if (lst.empty()) return 0;\n\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    size_t aligned_size = size - (size % 8);\n\n    __m256i zero = _mm256_set1_epi32(0);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i one = _mm256_set1_epi32(1);\n    __m256i mask_odd = _mm256_set1_epi32(0x55555555);\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256 data = _mm256_loadu_ps(&lst[i]);\n        __m256i data_int = _mm256_cvttps_epi32(data);\n\n        // Check if numbers are non-negative\n        __m256i is_non_negative = _mm256_cmpgt_epi32(data_int, zero);\n\n        // Check if numbers are odd\n        __m256i is_odd = _mm256_and_si256(_mm256_xor_si256(data_int, _mm256_add_epi32(data_int, one)), mask_odd);\n\n        // Combine conditions: non-negative and odd\n        __m256i valid = _mm256_and_si256(is_non_negative, is_odd);\n\n        // Square the original float values\n        __m256 squared = _mm256_mul_ps(data, data);\n\n        // Mask out invalid values\n        __m256 masked = _mm256_and_ps(_mm256_castsi256_ps(valid), squared);\n\n        // Horizontal sum of the valid squares\n        __m128 lo = _mm256_extractf128_ps(masked, 0);\n        __m128 hi = _mm256_extractf128_ps(masked, 1);\n        __m128 sum_4 = _mm_add_ps(lo, hi);\n        sum_4 = _mm_hadd_ps(sum_4, sum_4);\n        sum_4 = _mm_hadd_ps(sum_4, sum_4);\n        float temp_sum = _mm_cvtss_f32(sum_4);\n        sum += static_cast<uint64_t>(temp_sum);\n    }\n\n    // Process remaining elements\n    for (size_t i = aligned_size; i < size; ++i) {\n        int val = static_cast<int>(lst[i]);\n        if (val >= 0 && (val & 1)) {\n            sum += val * val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    if(lst.empty()) return sum;\n\n    const float *data = lst.data();\n    size_t size = lst.size();\n\n    __m256i zero_mask = _mm256_set1_epi32(0);\n    __m256i odd_mask = _mm256_set1_epi32(0xAAAAAAAA);\n    __m256i ones = _mm256_set1_epi32(1);\n\n    __m256i total_sum = _mm256_setzero_si256();\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256 vec = _mm256_loadu_ps(data + i);\n        __m256i vec_int = _mm256_cvttps_epi32(vec); // Convert to int\n        __m256i cmp_ge_zero = _mm256_cmpgt_epi32(vec_int, zero_mask); // Check >= 0\n\n        __m256i and_odd = _mm256_and_si256(vec_int, odd_mask); // Check odd\n        __m256i is_odd = _mm256_testc_si256(and_odd, odd_mask);\n\n        __m256i valid = _mm256_and_si256(cmp_ge_zero, is_odd); // Combine conditions\n\n        __m256i squared = _mm256_mullo_epi32(vec_int, vec_int); // Square the values\n        __m256i masked_squared = _mm256_and_si256(squared, _mm256_castps_si256(_mm256_castsi256_ps(valid))); // Mask with valid\n\n        total_sum = _mm256_add_epi32(total_sum, masked_squared); // Accumulate\n    }\n\n    uint64_t temp[8];\n    _mm256_storeu_si256((__m256i*)temp, total_sum);\n\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        int val = static_cast<int>(lst[i]);\n        if(val >= 0 && (val & 1)){\n            sum += val * val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstdint>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    if(lst.empty()) return 0;\n\n    const float *data = lst.data();\n    size_t size = lst.size();\n\n    __m256i zero_mask = _mm256_set1_epi32(0x80000000);\n    __m256i odd_mask = _mm256_set1_epi32(0x55555555);\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m256 vec = _mm256_loadu_ps(data + i);\n        __m256i vec_int = _mm256_cvttps_epi32(vec); // Convert to int\n        __m256i signs = _mm256_and_si256(vec_int, zero_mask);\n        __m256i is_positive = _mm256_cmpeq_epi32(signs, _mm256_setzero_si256());\n        __m256i and_odd = _mm256_and_si256(vec_int, odd_mask);\n        __m256i is_odd = _mm256_cmpeq_epi32(and_odd, vec_int);\n        __m256i valid = _mm256_and_si256(is_positive, is_odd);\n\n        __m256 squared = _mm256_mul_ps(vec, vec);\n        __m256 masked = _mm256_and_ps(_mm256_castsi256_ps(valid), squared);\n\n        __m256 sums = _mm256_hadd_ps(masked, masked);\n        sums = _mm256_hadd_ps(sums, sums);\n\n        float temp[8];\n        _mm256_storeu_ps(temp, sums);\n        for(int j = 0; j < 8; ++j){\n            sum += static_cast<uint64_t>(temp[j]);\n        }\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        if(lst[i] > 0 && static_cast<int>(lst[i]) % 2 == 1){\n            sum += static_cast<uint64_t>(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess) {\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n\n    if (size == 0) return result;\n\n    // Process in chunks of 8 elements using AVX2\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n\n    if (size == 0) return result;\n\n    size_t aligned_size = size / 8 * 8;\n\n    const int* game_data = game.data();\n    const int* guess_data = guess.data();\n    int* result_data = result.data();\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(game_data + i));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(guess_data + i));\n\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        __m256i cmp_eq = _mm256_cmpeq_epi32(game_vec, guess_vec);\n        __m256i zero = _mm256_setzero_si256();\n        __m256i res = _mm256_blendv_epi8(abs_diff, zero, cmp_eq);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result_data + i), res);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        result[i] = (game[i] == guess[i]) ? 0 : std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n    \n    if (size == 0) return result;\n\n    // Handle unaligned data\n    size_t i = 0;\n    for (; i + 8 <= size; i += 8){\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i){\n        result[i] = std::abs(game[i] - guess[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n    \n    if (size == 0) return result;\n\n    size_t aligned_size = size - (size % 8);\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), abs_diff);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        result[i] = (game[i] > guess[i]) ? (game[i] - guess[i]) : (guess[i] - game[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t size = game.size();\n    std::vector<int> result(size);\n\n    if (size == 0) return result;\n\n    size_t aligned_size = size / 8 * 8;\n\n    __m256i zero = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < aligned_size; i += 8) {\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i neg_diff = _mm256_sub_epi32(guess_vec, game_vec);\n\n        __m256i abs_diff = _mm256_max_epi32(diff, neg_diff);\n\n        __m256i mask = _mm256_cmpeq_epi32(game_vec, guess_vec);\n        __m256i res = _mm256_blendv_epi8(abs_diff, zero, mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), res);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = (diff < 0) ? -diff : diff;\n    }\n\n    return result;\n}\n```"}

{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 1 <= length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i*)&dst[i]);\n        \n        // Create a mask vector\n        __m128i cond = _mm_set_epi8(\n            mask[i+1] ? -1 : 0,\n            0, 0, 0, 0, 0, 0, 0,\n            mask[i] ? -1 : 0,\n            0, 0, 0, 0, 0, 0, 0\n        );\n        \n        // Use blendv to conditionally move\n        dst_vec = _mm_blendv_epi8(dst_vec, src_vec, cond);\n        \n        _mm_storeu_si128((__m128i*)&dst[i], dst_vec);\n    }\n    // Handle any remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 2 <= length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const *)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i const *)&dst[i]);\n        \n        // Create mask vector\n        __m128i mask_vec = _mm_set_epi8(\n            (mask[i+1] ? -1 : 0),\n            0, 0, 0, 0, 0, 0, 0, 0,\n            (mask[i] ? -1 : 0),\n            0, 0, 0, 0, 0, 0, 0\n        );\n        \n        __m128i cond_mov = _mm_or_si128(_mm_and_si128(mask_vec, src_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_storeu_si128((__m128i *)&dst[i], cond_mov);\n    }\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n\n    // Process 2 elements per iteration using SSE2\n    for (; i + 1 <= length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const *)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const *)(dst + i));\n\n        // Create a mask vector where each 64-bit element is either 0 or -1\n        __m128i mask_vec = _mm_castps_si128(_mm_loadl_pi(_mm_setzero_ps(), (__m64 const *)(mask + i)));\n        mask_vec = _mm_cmpeq_epi32(mask_vec, _mm_set1_epi32(-1)); // Convert to all bits set if true\n\n        // Perform blend based on mask\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mask_vec, src_vec), \n            _mm_andnot_si128(mask_vec, dst_vec)\n        );\n\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 2 <= length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const *)(src + i));\n        __m128i dst_vec = _mm_loadu_si128((__m128i const *)(dst + i));\n        \n        // Create a mask vector\n        __m128i mask_vec = _mm_set_epi8(\n            (mask[i+1] ? -1 : 0),\n            0, 0, 0, 0, 0, 0, 0, \n            (mask[i] ? -1 : 0),\n            0, 0, 0, 0, 0, 0, 0\n        );\n        \n        __m128i masked_src = _mm_and_si128(src_vec, mask_vec);\n        __m128i inverted_mask = _mm_andnot_si128(mask_vec, dst_vec);\n        __m128i result = _mm_or_si128(masked_src, inverted_mask);\n\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 2 <= length; i += 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i dst_vec = _mm_loadu_si128((__m128i*)&dst[i]);\n        \n        // Create a mask vector\n        __m128i cond_mask = _mm_set_epi8(\n            (mask[i+1] ? -1 : 0), 0, 0, 0, 0, 0, 0, 0,\n            (mask[i] ? -1 : 0), 0, 0, 0, 0, 0, 0, 0\n        );\n        \n        // Use blendv to conditionally move\n        dst_vec = _mm_blendv_epi8(dst_vec, src_vec, cond_mask);\n        \n        _mm_storeu_si128((__m128i*)&dst[i], dst_vec);\n    }\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m128i shift_right = _mm_set1_epi32(3);\n    const __m128i shift_left = _mm_set1_epi32(-3); // Equivalent to left shift by 3\n    const __m128i divisor = _mm_set1_epi32(7);\n    const __m128i scale_vec = _mm_set1_epi32(scale);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n\n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n\n        __m128i product = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, product);\n\n        __m128i result = _mm_div_epi32(sum, divisor); // Assuming a custom intrinsic for division\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    __m128i v_seven = _mm_set1_epi32(7);\n    size_t i = 0;\n\n    for (; i <= length - 4; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i const *)(src + i));\n        __m128i v_mul = _mm_mullo_epi32(v_src, v_scale);\n\n        // Calculate tmp1: (element * scale) >> 3\n        __m128i v_tmp1 = _mm_srai_epi32(v_mul, 3);\n\n        // Calculate tmp2: (element * scale) << 3\n        __m128i v_tmp2 = _mm_slli_epi32(v_mul, 3);\n\n        // Compute result: (element * scale + tmp1 * tmp2) / 7\n        __m128i v_product = _mm_mullo_epi32(v_tmp1, v_tmp2);\n        __m128i v_sum = _mm_add_epi32(v_mul, v_product);\n        __m128i v_result = _mm_div_epi32(v_sum, v_seven); // Assuming a custom div function\n\n        _mm_storeu_si128((__m128i *)(dst + i), v_result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m128i shift_right = _mm_set1_epi32(3);\n    const __m128i shift_left = _mm_set1_epi32(-3);\n    const __m128i div_const = _mm_set1_epi32(7);\n    const __m128i scale_vec = _mm_set1_epi32(scale);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const *)(src + i));\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n\n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n\n        __m128i product = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, product);\n\n        __m128i result = _mm_div_epi32(sum, div_const); // Assuming a custom or intrinsic div function\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m128i shift_right = _mm_set1_epi32(3);\n    const __m128i shift_left = _mm_set1_epi32(-3); // Equivalent to left shift by 3\n    const __m128i div_const = _mm_set1_epi32(7);\n    const __m128i scale_vec = _mm_set1_epi32(scale);\n\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i vec_src = _mm_loadu_si128((__m128i const *)(src + i));\n        __m128i vec_mul = _mm_mullo_epi32(vec_src, scale_vec);\n\n        __m128i tmp1 = _mm_sra_epi32(vec_mul, shift_right);\n        __m128i tmp2 = _mm_sll_epi32(vec_mul, _mm_cvtepi32_epi64(shift_left));\n\n        __m128i tmp_mul = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i result = _mm_add_epi32(vec_mul, tmp_mul);\n\n        result = _mm_sra_epi32(result, _mm_cvtepi32_epi64(div_const));\n\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    const __m128i scale_vec = _mm_set1_epi32(scale);\n    const __m128i seven_vec = _mm_set1_epi32(7);\n    const __m128i three_vec = _mm_set1_epi32(3);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n        \n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n        \n        __m128i product = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, product);\n        \n        __m128i result = _mm_div_epi32(sum, seven_vec); // Assuming a custom intrinsic for division\n        \n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 1 < rows; i += 2) {\n        __m128d data = _mm_loadu_pd(&src[i * cols + stride]);\n        _mm_storeu_pd(&dst[i], data);\n    }\n    if (i < rows) {\n        double temp = src[i * cols + stride];\n        dst[i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 1 < rows; i += 2) {\n        __m128d data = _mm_loadu_pd(&src[i * cols + stride]);\n        _mm_storeu_pd(&dst[i], data);\n    }\n    if (i < rows) {\n        double temp = src[i * cols + stride];\n        dst[i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i = 0;\n    __m128d vdata;\n\n    // Handle rows in multiples of 2 for SIMD processing\n    for (; i <= rows - 2; i += 2) {\n        size_t src_index = i * cols + stride;\n        vdata = _mm_loadu_pd(&src[src_index]); // Load two doubles unaligned\n        _mm_storeu_pd(&dst[i], vdata); // Store two doubles unaligned\n    }\n\n    // Handle remaining rows (if any)\n    for (; i < rows; ++i) {\n        size_t src_index = i * cols + stride;\n        dst[i] = src[src_index];\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 1 < rows; i += 2) {\n        __m128d data = _mm_loadu_pd(&src[i * cols + stride]);\n        _mm_storeu_pd(&dst[i], data);\n    }\n    if (i < rows) {\n        double temp = src[i * cols + stride];\n        dst[i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t i;\n    for (i = 0; i + 1 < rows; i += 2) {\n        __m128d data = _mm_loadu_pd(&src[i * cols + stride]);\n        _mm_storeu_pd(&dst[i], data);\n    }\n    if (i < rows) {\n        double temp = src[i * cols + stride];\n        dst[i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    for (; i + simd_width <= length; i += simd_width) {\n        __m128 indices_vec = _mm_loadu_ps((const float *)(indices + i));\n        __m128 src_vals = _mm_setzero_ps();\n\n        // Gather operation (manual gather since SSE does not have a direct gather intrinsic)\n        src_vals = _mm_set_epi32(\n            *(float *)((char *)src + (int)(indices[i + 3] * sizeof(float))),\n            *(float *)((char *)src + (int)(indices[i + 2] * sizeof(float))),\n            *(float *)((char *)src + (int)(indices[i + 1] * sizeof(float))),\n            *(float *)((char *)src + (int)(indices[i + 0] * sizeof(float)))\n        );\n\n        // Store gathered values into destination array\n        _mm_storeu_ps(dst + i, src_vals);\n\n        // Scatter operation (manual scatter since SSE does not have a direct scatter intrinsic)\n        *(float *)((char *)dst + (int)(indices[i + 0] * sizeof(float))) = dst[i + 0];\n        *(float *)((char *)dst + (int)(indices[i + 1] * sizeof(float))) = dst[i + 1];\n        *(float *)((char *)dst + (int)(indices[i + 2] * sizeof(float))) = dst[i + 2];\n        *(float *)((char *)dst + (int)(indices[i + 3] * sizeof(float))) = dst[i + 3];\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m128 indices_f;\n    __m128 src_vec;\n    __m128 dst_vec;\n\n    for (i = 0; i + 4 <= length; i += 4) {\n        // Load indices\n        indices_f = _mm_loadu_ps((const float *)(indices + i));\n\n        // Gather operation: load from src using indices\n        src_vec = _mm_set_ps(src[(int)indices_f.m128_f32[3]], \n                             src[(int)indices_f.m128_f32[2]], \n                             src[(int)indices_f.m128_f32[1]], \n                             src[(int)indices_f.m128_f32[0]]);\n        \n        // Store gathered values into destination array\n        _mm_storeu_ps(dst + i, src_vec);\n\n        // Scatter operation: copy values from src to dst using indices as write locations\n        dst_vec = _mm_loadu_ps(src + i);\n        _mm_storeu_ps((float *)(src + (size_t)indices_f.m128_f32[0]), dst_vec);\n        _mm_storeu_ps((float *)(src + (size_t)indices_f.m128_f32[1]), dst_vec);\n        _mm_storeu_ps((float *)(src + (size_t)indices_f.m128_f32[2]), dst_vec);\n        _mm_storeu_ps((float *)(src + (size_t)indices_f.m128_f32[3]), dst_vec);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]]; // gather\n        dst[indices[i]] = src[i]; // scatter\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128 indices_vec_f = _mm_loadu_ps((const float *)(indices + i));\n        __m128i indices_vec_i = _mm_cvtps_epi32(indices_vec_f);\n\n        // Gather operation\n        __m128 gather_vec = _mm_setzero_ps();\n        gather_vec = _mm_shuffle_epi8(_mm_loadu_ps(src + _mm_extract_epi32(indices_vec_i, 0)), _mm_set_epi8(0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x00));\n        gather_vec = _mm_shuffle_epi8(_mm_loadu_ps(src + _mm_extract_epi32(indices_vec_i, 1)), _mm_set_epi8(0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x00, 0xFF));\n        gather_vec = _mm_shuffle_epi8(_mm_loadu_ps(src + _mm_extract_epi32(indices_vec_i, 2)), _mm_set_epi8(0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x00, 0xFF, 0xFF, 0xFF));\n        gather_vec = _mm_shuffle_epi8(_mm_loadu_ps(src + _mm_extract_epi32(indices_vec_i, 3)), _mm_set_epi8(0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0x00, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF));\n\n        _mm_storeu_ps(dst + i, gather_vec);\n\n        // Scatter operation\n        __m128 src_vec = _mm_loadu_ps(src + i);\n        _mm_store_ss((float *)(src + _mm_extract_epi32(indices_vec_i, 0)), _mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(0, 0, 0, 0)));\n        _mm_store_ss((float *)(src + _mm_extract_epi32(indices_vec_i, 1)), _mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(1, 1, 1, 1)));\n        _mm_store_ss((float *)(src + _mm_extract_epi32(indices_vec_i, 2)), _mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(2, 2, 2, 2)));\n        _mm_store_ss((float *)(src + _mm_extract_epi32(indices_vec_i, 3)), _mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(3, 3, 3, 3)));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = src[indices[i]]; // Gather\n        src[indices[i]] = src[i]; // Scatter\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    for (size_t i = 0; i + 4 <= length; i += 4) {\n        __m128 indices_vec_f = _mm_loadu_ps((const float *)(indices + i));\n        __m128i indices_vec_i = _mm_cvtps_epi32(indices_vec_f);\n\n        // Gather operation\n        __m128 gather_vec = _mm_setzero_ps();\n        gather_vec = _mm_shuffle_epi8(_mm_loadu_ps(src + _mm_extract_epi32(indices_vec_i, 0)), _mm_srli_epi32(indices_vec_i, 2));\n        _mm_storeu_ps(dst + i, gather_vec);\n\n        // Scatter operation\n        __m128 src_vec = _mm_loadu_ps(src + i);\n        _mm_stream_si32((int *)(dst + _mm_extract_epi32(indices_vec_i, 0)), _mm_cvtps_epi32(_mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(0, 0, 0, 0))));\n        _mm_stream_si32((int *)(dst + _mm_extract_epi32(indices_vec_i, 1)), _mm_cvtps_epi32(_mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(1, 1, 1, 1))));\n        _mm_stream_si32((int *)(dst + _mm_extract_epi32(indices_vec_i, 2)), _mm_cvtps_epi32(_mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(2, 2, 2, 2))));\n        _mm_stream_si32((int *)(dst + _mm_extract_epi32(indices_vec_i, 3)), _mm_cvtps_epi32(_mm_shuffle_ps(src_vec, src_vec, _MM_SHUFFLE(3, 3, 3, 3))));\n    }\n\n    // Handle remaining elements\n    for (size_t i = (length / 4) * 4; i < length; ++i) {\n        dst[i] = src[indices[i]]; // Gather\n        dst[indices[i]] = src[i]; // Scatter\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        // Gather operation\n        __m128 idx_vec = _mm_loadu_ps((const float *)(indices + i));\n        __m128 src_vec = _mm_setzero_ps();\n        src_vec = _mm_shuffle_epi8(_mm_castps_si128(src_vec), _mm_castps_si128(idx_vec)); // Simulate gather\n        src_vec = _mm_loadu_ps(src + (int)(idx_vec.m128_f32[0])); // Simplified gather\n        _mm_storeu_ps(dst + i, src_vec);\n\n        // Scatter operation\n        __m128 dst_vec = _mm_loadu_ps(src + i);\n        _mm_stream_ps(dst + (size_t)(idx_vec.m128_f32[0]), dst_vec); // Simplified scatter\n    }\n    for (; i < length; i++) {\n        // Gather operation (scalar fallback)\n        dst[i] = src[indices[i]];\n        // Scatter operation (scalar fallback)\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || start >= (uint64_t)(-1) / 2 || end >= (uint64_t)(-1) / 2) return;\n\n    uint64_t length = end - start + 1;\n    uint64_t half_length = length / 2;\n\n    for (uint64_t i = 0; i < half_length / 8 * 8; i += 8) {\n        __m128i left = _mm_loadu_si128((__m128i *)(arr + start + i));\n        __m128i right = _mm_loadu_si128((__m128i *)(arr + end - i));\n\n        __m128i shuffle_mask = _mm_set_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n        __m128i swapped_left = _mm_shuffle_epi8(right, shuffle_mask);\n        __m128i swapped_right = _mm_shuffle_epi8(left, shuffle_mask);\n\n        _mm_storeu_si128((__m128i *)(arr + start + i), swapped_left);\n        _mm_storeu_si128((__m128i *)(arr + end - i), swapped_right);\n    }\n\n    for (uint64_t i = half_length / 8 * 8; i < half_length; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t len = end - start + 1;\n    uint64_t half_len = len / 2;\n\n    for (uint64_t i = 0; i < half_len; i += 8) {\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[start + i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[end - i]);\n\n        __m128i shuffle_mask = _mm_set_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0\n        );\n\n        __m128i reversed_left = _mm_shuffle_epi8(right, shuffle_mask);\n        __m128i reversed_right = _mm_shuffle_epi8(left, shuffle_mask);\n\n        _mm_storeu_si128((__m128i*)&arr[start + i], reversed_right);\n        _mm_storeu_si128((__m128i*)&arr[end - i], reversed_left);\n    }\n\n    // Handle remaining elements if any\n    for (uint64_t i = half_len * 8; i < len / 2; ++i) {\n        int16_t temp = arr[start + i];\n        arr[start + i] = arr[end - i];\n        arr[end - i] = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || arr == NULL) return;\n\n    uint64_t len = end - start + 1;\n    uint64_t half_len = len / 2;\n\n    int16_t* left = arr + start;\n    int16_t* right = arr + end;\n\n    for (uint64_t i = 0; i < half_len / 8; ++i) {\n        __m128i vleft = _mm_loadu_si128((__m128i*)left);\n        __m128i vright = _mm_loadu_si128((__m128i*)right);\n\n        __m128i swapped = _mm_shufflelo_epi16(vleft, _MM_SHUFFLE(3, 2, 1, 0));\n        swapped = _mm_shufflehi_epi16(swapped, _MM_SHUFFLE(3, 2, 1, 0));\n\n        __m128i swapped_right = _mm_shufflelo_epi16(vright, _MM_SHUFFLE(3, 2, 1, 0));\n        swapped_right = _mm_shufflehi_epi16(swapped_right, _MM_SHUFFLE(3, 2, 1, 0));\n\n        __m128i temp = swapped;\n        swapped = swapped_right;\n        swapped_right = temp;\n\n        _mm_storeu_si128((__m128i*)left, swapped_right);\n        _mm_storeu_si128((__m128i*)right, swapped);\n\n        left += 8;\n        right -= 8;\n    }\n\n    // Handle remaining elements with scalar code\n    for (uint64_t i = half_len / 8 * 8; i < half_len; ++i) {\n        int16_t tmp = left[i];\n        left[i] = right[-i - 1];\n        right[-i - 1] = tmp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end || arr == NULL) return;\n\n    uint64_t i = start;\n    uint64_t j = end;\n\n    while (j - i + 1 >= 8) {\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[j - 7]);\n\n        __m128i shuffle_mask = _mm_set_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8,\n            7, 6, 5, 4, 3, 2, 1, 0\n        );\n\n        __m128i reversed_left = _mm_shuffle_epi8(right, shuffle_mask);\n        __m128i reversed_right = _mm_shuffle_epi8(left, shuffle_mask);\n\n        _mm_storeu_si128((__m128i*)&arr[i], reversed_right);\n        _mm_storeu_si128((__m128i*)&arr[j - 7], reversed_left);\n\n        i += 8;\n        j -= 8;\n    }\n\n    while (i < j) {\n        int16_t temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n        i++;\n        j--;\n    }\n\n    if (i == j) {\n        // No action needed for the middle element when count is odd\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    uint64_t length = end - start + 1;\n    uint64_t half_length = length / 2;\n\n    for (uint64_t i = 0; i < half_length; i += 8) {\n        __m128i left = _mm_loadu_si128((__m128i *)(arr + start + i));\n        __m128i right = _mm_loadu_si128((__m128i *)(arr + end - i));\n\n        __m128i swapped = _mm_shufflelo_epi16(right, _MM_SHUFFLE(3, 2, 1, 0));\n        swapped = _mm_shufflehi_epi16(swapped, _MM_SHUFFLE(3, 2, 1, 0));\n\n        __m128i swapped_right = _mm_shufflelo_epi16(left, _MM_SHUFFLE(3, 2, 1, 0));\n        swapped_right = _mm_shufflehi_epi16(swapped_right, _MM_SHUFFLE(3, 2, 1, 0));\n\n        _mm_storeu_si128((__m128i *)(arr + start + i), swapped);\n        _mm_storeu_si128((__m128i *)(arr + end - i), swapped_right);\n    }\n\n    // Handle remaining elements if any\n    uint64_t remainder = half_length % 8;\n    if (remainder > 0) {\n        uint64_t i = half_length - remainder;\n        __m128i left = _mm_loadu_si128((__m128i *)(arr + start + i));\n        __m128i right = _mm_loadu_si128((__m128i *)(arr + end - i));\n\n        __m128i mask = _mm_set_epi16(\n            (remainder >= 8) ? 0xFFFF : 0,\n            (remainder >= 7) ? 0xFFFF : 0,\n            (remainder >= 6) ? 0xFFFF : 0,\n            (remainder >= 5) ? 0xFFFF : 0,\n            (remainder >= 4) ? 0xFFFF : 0,\n            (remainder >= 3) ? 0xFFFF : 0,\n            (remainder >= 2) ? 0xFFFF : 0,\n            (remainder >= 1) ? 0xFFFF : 0\n        );\n\n        __m128i swapped = _mm_shufflelo_epi16(right, _MM_SHUFFLE(3, 2, 1, 0));\n        swapped = _mm_shufflehi_epi16(swapped, _MM_SHUFFLE(3, 2, 1, 0));\n\n        __m128i swapped_right = _mm_shufflelo_epi16(left, _MM_SHUFFLE(3, 2, 1, 0));\n        swapped_right = _mm_shufflehi_epi16(swapped_right, _MM_SHUFFLE(3, 2, 1, 0));\n\n        __m128i masked_left = _mm_or_si128(_mm_and_si128(mask, swapped), _mm_andnot_si128(mask, left));\n        __m128i masked_right = _mm_or_si128(_mm_and_si128(mask, swapped_right), _mm_andnot_si128(mask, right));\n\n        _mm_storeu_si128((__m128i *)(arr + start + i), masked_left);\n        _mm_storeu_si128((__m128i *)(arr + end - i), masked_right);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i = 0;\n    __m128i vzero = _mm_setzero_si128();\n    \n    for (; i <= (dim1 * dim2 - 16); i += 16) {\n        __m128i vdata = _mm_loadu_si128((const __m128i *)(src + i));\n        _mm_storeu_si128((__m128i *)(slice + i), vdata);\n    }\n    \n    for (; i < dim1 * dim2; ++i) {\n        slice[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i, j;\n    for (i = 0; i < dim1; i++) {\n        for (j = 0; j + 15 < dim2; j += 16) {\n            __m128i v = _mm_loadu_si128((__m128i *)(src + i * dim2 * dim3 + j));\n            _mm_storeu_si128((__m128i *)(slice + i * dim2 + j), v);\n        }\n        for (; j < dim2; j++) {\n            slice[i * dim2 + j] = src[i * dim2 * dim3 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    size_t i, j;\n    for (i = 0; i < dim1; i++) {\n        for (j = 0; j + 15 < dim2; j += 16) {\n            __m128i v = _mm_loadu_si128((__m128i *)(src + i * dim2 * dim3 + j));\n            _mm_storeu_si128((__m128i *)(slice + i * dim2 + j), v);\n        }\n        for (; j < dim2; j++) {\n            slice[i * dim2 + j] = src[i * dim2 * dim3 + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    \n    for (size_t i = 0; i < dim1 * dim2; i += 16) {\n        __m128i v = _mm_loadu_si128((__m128i *)(src + i));\n        _mm_storeu_si128((__m128i *)(slice + i), v);\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t slice_offset = slice_idx * dim1 * dim2;\n    const uint8_t *src = tensor + slice_offset;\n    size_t total_elements = dim1 * dim2;\n    size_t i = 0;\n\n    __m128i vzero = _mm_setzero_si128();\n\n    for (; i + 15 < total_elements; i += 16) {\n        __m128i vdata = _mm_loadu_si128((__m128i *)(src + i));\n        _mm_storeu_si128((__m128i *)(slice + i), vdata);\n    }\n\n    for (; i < total_elements; ++i) {\n        slice[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t k = 0; k < block_size && (i + k) < rows; ++k) {\n                const int16_t *row_ptr = src + (i + k) * cols + j;\n                __m128i row_data = _mm_loadu_si128((__m128i *)row_ptr);\n                for (size_t l = 0; l < block_size && (j + l) < cols; ++l) {\n                    __m128i col_data = _mm_shuffle_epi32(row_data, _MM_SHUFFLE(l, l, l, l));\n                    int16_t value = _mm_cvtsi128_si32(col_data);\n                    dst[(j + l) * rows + i + k] = value;\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t k = 0; k < block_size; k++) {\n                for (size_t l = 0; l < block_size; l += 8) {\n                    __m128i row = _mm_loadu_si128((__m128i const *)(src + ((i + k) * cols + j + l)));\n                    _mm_storeu_si128((__m128i *)(dst + ((j + l) * rows + i + k)), row);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ++ii) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    __m128i v = _mm_loadu_si128((__m128i const *)(src + ii * cols + jj));\n                    _mm_storeu_si128((__m128i *)(dst + jj * rows + ii), v);\n                }\n            }\n            for (size_t jj = j; jj < j + block_size && jj < cols; ++jj) {\n                for (size_t ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                    __m128i v = _mm_loadu_si128((__m128i const *)(src + ii * cols + jj));\n                    _mm_storeu_si128((__m128i *)(dst + jj * rows + ii), v);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ii++) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    __m128i v = _mm_loadu_si128((__m128i const *)(src + ii * cols + jj));\n                    _mm_storeu_si128((__m128i *)(dst + jj * rows + ii), v);\n                }\n            }\n            for (size_t jj = j; jj < j + block_size && jj < cols; jj++) {\n                for (size_t ii = i; ii < i + block_size && ii < rows; ii += 8) {\n                    __m128i v = _mm_loadu_si128((__m128i const *)(src + ii * cols + jj));\n                    _mm_storeu_si128((__m128i *)(dst + jj * rows + ii), v);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t i = 0; i < rows; i += block_size) {\n        for (size_t j = 0; j < cols; j += block_size) {\n            for (size_t ii = i; ii < i + block_size && ii < rows; ii++) {\n                for (size_t jj = j; jj < j + block_size && jj < cols; jj += 8) {\n                    __m128i row = _mm_loadu_si128((__m128i const *)(src + ii * cols + jj));\n                    __m128i col0 = _mm_shuffle_epi32(row, _MM_SHUFFLE(0, 0, 0, 0));\n                    __m128i col1 = _mm_shuffle_epi32(row, _MM_SHUFFLE(1, 1, 1, 1));\n                    __m128i col2 = _mm_shuffle_epi32(row, _MM_SHUFFLE(2, 2, 2, 2));\n                    __m128i col3 = _mm_shuffle_epi32(row, _MM_SHUFFLE(3, 3, 3, 3));\n\n                    __m128i col01 = _mm_unpacklo_epi16(col0, col1);\n                    __m128i col23 = _mm_unpacklo_epi16(col2, col3);\n\n                    __m128i result = _mm_unpacklo_epi32(col01, col23);\n\n                    size_t dst_index = jj * rows + ii;\n                    _mm_storeu_si128((__m128i *)(dst + dst_index), result);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i;\n    for (i = 0; i + 3 < dim; i += 4) {\n        size_t idx = i * (dim * dim + dim + 1);\n        __m128 vec = _mm_loadu_ps(&array[idx]);\n        sum = _mm_add_ps(sum, vec);\n    }\n    float result = 0.0f;\n    for (; i < dim; ++i) {\n        size_t idx = i * (dim * dim + dim + 1);\n        result += array[idx];\n    }\n    __m128 sum_h = _mm_hadd_ps(sum, sum);\n    sum_h = _mm_hadd_ps(sum_h, sum_h);\n    float temp[4] = {0};\n    _mm_storeu_ps(temp, sum_h);\n    result += temp[0] + temp[1] + temp[2] + temp[3];\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i, limit = dim & (~3);\n\n    for (i = 0; i < limit; i += 4) {\n        size_t index = i * (dim * dim + dim + 1);\n        __m128 vec = _mm_loadu_ps(&array[index]);\n        sum = _mm_add_ps(sum, vec);\n    }\n\n    float result[4] __attribute__((aligned(16)));\n    _mm_store_ps(result, sum);\n\n    float total = result[0] + result[1] + result[2] + result[3];\n\n    for (; i < dim; i++) {\n        size_t index = i * (dim * dim + dim + 1);\n        total += array[index];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i;\n\n    for (i = 0; i < dim && i + 4 <= dim; i += 4) {\n        size_t idx = i * (dim * dim + dim + 1);\n        __m128 vec = _mm_loadu_ps(&array[idx]);\n        sum = _mm_add_ps(sum, vec);\n    }\n\n    float result[4] __attribute__((aligned(16)));\n    _mm_store_ps(result, sum);\n\n    float total = result[0] + result[1] + result[2] + result[3];\n\n    for (; i < dim; ++i) {\n        size_t idx = i * (dim * dim + dim + 1);\n        total += array[idx];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i;\n\n    for (i = 0; i + 3 < dim; i += 4) {\n        size_t idx = i * (dim * dim + dim + 1);\n        __m128 v = _mm_loadu_ps(&array[idx]);\n        sum = _mm_add_ps(sum, v);\n    }\n\n    for (; i < dim; ++i) {\n        size_t idx = i * (dim * dim + dim + 1);\n        __m128 v = _mm_load_ss(&array[idx]);\n        sum = _mm_add_ss(sum, v);\n    }\n\n    sum = _mm_hadd_ps(sum, sum);\n    sum = _mm_hadd_ps(sum, sum);\n\n    float result[4];\n    _mm_storeu_ps(result, sum);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum = _mm_setzero_ps();\n    size_t i;\n\n    for (i = 0; i + 3 < dim; i += 4) {\n        size_t idx = i * (dim * dim + dim + 1);\n        __m128 vec = _mm_loadu_ps(&array[idx]);\n        sum = _mm_add_ps(sum, vec);\n    }\n\n    for (; i < dim; i++) {\n        size_t idx = i * (dim * dim + dim + 1);\n        __m128 vec = _mm_load_ss(&array[idx]);\n        sum = _mm_add_ss(sum, vec);\n    }\n\n    sum = _mm_hadd_ps(sum, sum);\n    sum = _mm_hadd_ps(sum, sum);\n\n    float result;\n    _mm_store_ss(&result, sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d one_vec = _mm_set1_pd(1.0);\n\n    size_t total_elements = rows * cols;\n    size_t aligned_elements = total_elements / 2 * 2;\n\n    for (size_t i = 0; i < aligned_elements; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(src + i);\n        __m128d cmp_result = _mm_cmpgt_pd(src_vec, threshold_vec);\n        __m128d mask = _mm_and_pd(cmp_result, _mm_sub_pd(scale_vec, one_vec));\n        __m128d result = _mm_add_pd(src_vec, _mm_mul_pd(mask, _mm_sub_pd(src_vec, one_vec)));\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d one_vec = _mm_set1_pd(1.0);\n\n    size_t total_elements = rows * cols;\n    size_t aligned_elements = total_elements / 2 * 2;\n\n    for (size_t i = 0; i < aligned_elements; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(src + i);\n        __m128d cmp_result = _mm_cmpgt_pd(src_vec, threshold_vec);\n        __m128d mask = _mm_and_pd(cmp_result, _mm_sub_pd(scale_vec, one_vec));\n        __m128d result = _mm_add_pd(src_vec, _mm_mul_pd(mask, src_vec));\n        _mm_storeu_pd(dst + i, result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    size_t length = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 2 <= length; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(&src[i]);\n        __m128d cmp_result = _mm_cmpgt_pd(src_vec, threshold_vec); // Compare greater than\n        __m128d scaled = _mm_mul_pd(src_vec, scale_vec); // Scale the source vector\n\n        // Blend using comparison result as mask\n        __m128d result = _mm_or_pd(_mm_and_pd(cmp_result, scaled), _mm_andnot_pd(cmp_result, src_vec));\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = (src[i] > threshold) ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d one_vec = _mm_set1_pd(1.0);\n\n    size_t total = rows * cols;\n    size_t i = 0;\n\n    // Process 2 elements per iteration using SIMD\n    for (; i + 1 < total; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(&src[i]);\n        __m128d cmp_result = _mm_cmpgt_pd(src_vec, threshold_vec); // Compare src > threshold\n        __m128d mask = _mm_and_pd(cmp_result, scale_vec);         // Mask where scaling applies\n        __m128d no_mask = _mm_andnot_pd(cmp_result, one_vec);     // Mask where no scaling applies\n        __m128d scaled = _mm_mul_pd(src_vec, mask);               // Scale where applicable\n        __m128d unchanged = _mm_mul_pd(src_vec, no_mask);         // Keep original where not scaled\n        __m128d result = _mm_add_pd(scaled, unchanged);          // Combine scaled and unchanged\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle remaining scalar element if any\n    if (i < total) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d one_vec = _mm_set1_pd(1.0);\n\n    size_t length = rows * cols;\n    size_t i;\n\n    for (i = 0; i + 1 <= length; i += 2) {\n        __m128d src_vec = _mm_loadu_pd(&src[i]);\n        __m128d cmp_result = _mm_cmpgt_pd(src_vec, threshold_vec);\n        __m128d mask = _mm_and_pd(cmp_result, _mm_sub_pd(scale_vec, one_vec));\n        mask = _mm_add_pd(mask, one_vec);\n        __m128d result = _mm_mul_pd(src_vec, mask);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    if (i < length) {\n        dst[i] = src[i] > threshold ? src[i] * scale : src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        const double *src_row = src + idx * cols;\n        double *dst_row = dst + i * cols;\n\n        for (j = 0; j + 2 <= cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, vec);\n        }\n\n        // Handle any remaining elements\n        for (; j < cols; ++j) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        const double *src_row = src + idx * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t k = 0;\n        for (; k <= cols - 4; k += 4) {\n            __m128d vec1 = _mm_loadu_pd(src_row + k);\n            __m128d vec2 = _mm_loadu_pd(src_row + k + 2);\n            _mm_storeu_pd(dst_row + k, vec1);\n            _mm_storeu_pd(dst_row + k + 2, vec2);\n        }\n\n        for (; k < cols; ++k) {\n            dst_row[k] = src_row[k];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        const double *src_row = src + idx * cols;\n        double *dst_row = dst + i * cols;\n\n        for (j = 0; j + 2 <= cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(src_row + j);\n            _mm_storeu_pd(dst_row + j, vec);\n        }\n\n        if (j < cols) {\n            dst_row[j] = src_row[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t idx = indices[i];\n        const double *src_row = src + idx * cols;\n        double *dst_row = dst + i * cols;\n\n        size_t k = 0;\n        for (; k + 2 <= cols; k += 2) {\n            __m128d data = _mm_loadu_pd(src_row + k);\n            _mm_storeu_pd(dst_row + k, data);\n        }\n        for (; k < cols; ++k) {\n            dst_row[k] = src_row[k];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        size_t src_row = indices[i];\n        const double *src_ptr = src + src_row * cols;\n        double *dst_ptr = dst + i * cols;\n\n        for (j = 0; j + 2 <= cols; j += 2) {\n            __m128d vec = _mm_loadu_pd(src_ptr + j);\n            _mm_storeu_pd(dst_ptr + j, vec);\n        }\n\n        if (j < cols) {\n            dst_ptr[j] = src_ptr[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <emmintrin.h>\n#include <stdint.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    const __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    size_t i = 0;\n\n    for (; i + 1 <= length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i const *)(src1 + i));\n        __m128i vec2 = _mm_loadu_si128((__m128i const *)(src2 + i));\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n\n        // Divide by 2 (round toward zero)\n        __m128i mask = _mm_set1_epi64x(0x8000000000000000);\n        __m128i sign = _mm_and_si128(sum, mask);\n        __m128i abs_val = _mm_xor_si128(sum, sign);\n        abs_val = _mm_srli_epi64(abs_val, 1);\n        __m128i result = _mm_or_si128(abs_val, sign);\n\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements (if any)\n    if (i < length) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    const __m128i scalar_vec = _mm_set1_epi64x(scalar);\n    size_t i = 0;\n\n    for (; i + 1 <= length; i += 2) {\n        __m128i vec1 = _mm_loadu_si128((__m128i const *)(src1 + i));\n        __m128i vec2 = _mm_loadu_si128((__m128i const *)(src2 + i));\n\n        __m128i sum = _mm_add_epi64(vec1, vec2);\n        sum = _mm_add_epi64(sum, scalar_vec);\n\n        // Divide by 2 (round toward zero)\n        __m128i mask = _mm_set1_epi64x(0x8000000000000000);\n        __m128i sign = _mm_and_si128(sum, mask);\n        __m128i abs_val = _mm_xor_si128(sum, sign);\n        abs_val = _mm_srl_epi64(abs_val, _mm_set1_epi64x(1));\n        __m128i result = _mm_xor_si128(abs_val, sign);\n\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements if any\n    if (i < length) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m128i vscalar = _mm_set1_epi64x(scalar);\n\n    for (; i + 1 <= length; i += 2) {\n        __m128i va = _mm_loadu_si128((__m128i const *)&src1[i]);\n        __m128i vb = _mm_loadu_si128((__m128i const *)&src2[i]);\n        \n        __m128i sum = _mm_add_epi64(va, vb);\n        __m128i sum_scalar = _mm_add_epi64(sum, vscalar);\n        \n        // Divide by 2 (right shift by 1)\n        __m128i result = _mm_sra_epi64(sum_scalar, _mm_set1_epi64x(1));\n        \n        _mm_storeu_si128((__m128i *)&dst[i], result);\n    }\n\n    // Handle remaining elements if any\n    if (i < length) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m128i vscalar = _mm_set1_epi64x(scalar);\n\n    for (; i + 1 < length; i += 2) {\n        __m128i va = _mm_loadu_si128((const __m128i*)&src1[i]);\n        __m128i vb = _mm_loadu_si128((const __m128i*)&src2[i]);\n        __m128i vsum = _mm_add_epi64(va, vb);\n        vsum = _mm_add_epi64(vsum, vscalar);\n        __m128i vresult = _mm_srai_epi64(vsum, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], vresult);\n    }\n\n    if (i < length) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m128i vscalar = _mm_set1_epi64x(scalar);\n\n    for (; i + 1 <= length; i += 2) {\n        __m128i va = _mm_loadu_si128((__m128i const *)(src1 + i));\n        __m128i vb = _mm_loadu_si128((__m128i const *)(src2 + i));\n        \n        __m128i sum = _mm_add_epi64(va, vb);\n        __m128i sum_scalar = _mm_add_epi64(sum, vscalar);\n        \n        __m128i div_result = _mm_sra_epi64(sum_scalar, _mm_set1_epi64x(1));\n        \n        _mm_storeu_si128((__m128i *)(dst + i), div_result);\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n#include <stdint.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0 || src == NULL) return -1;\n\n    const size_t aligned_length = length / 4 * 4;\n    __m128i min_val = _mm_set1_epi32(INT_MAX);\n    __m128i mask = _mm_setr_epi32(0, -1, 0, -1);\n\n    for(size_t i = 0; i < aligned_length; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even = _mm_and_si128(vec, mask);\n        min_val = _mm_min_epi32(min_val, even);\n    }\n\n    int remaining_min = INT_MAX;\n    for(size_t i = aligned_length; i < length; ++i){\n        if((i % 2) == 0 && src[i] < remaining_min){\n            remaining_min = src[i];\n        }\n    }\n\n    __m128i min_h = _mm_min_epi32(min_val, _mm_shuffle_epi32(min_val, 1));\n    __m128i min_lh = _mm_min_epi32(min_h, _mm_shuffle_epi32(min_h, 2));\n    int simd_min = _mm_cvtsi128_si32(min_lh);\n\n    return (simd_min < remaining_min) ? simd_min : remaining_min;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    size_t aligned_length = length / 4 * 4;\n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const *)(src + i));\n        __m128i even_vec = _mm_and_si128(vec, mask);\n        __m128i curr_min = _mm_min_epi32(even_vec, min_val);\n        min_val = curr_min;\n    }\n\n    int min_result = 0x7FFFFFFF;\n    __m128i shuf = _mm_shuffle_epi32(min_val, _MM_SHUFFLE(0, 0, 2, 0));\n    min_val = _mm_min_epi32(min_val, shuf);\n    shuf = _mm_shuffle_epi32(min_val, _MM_SHUFFLE(0, 0, 0, 1));\n    min_val = _mm_min_epi32(min_val, shuf);\n    _mm_store_si128((__m128i *)&min_result, min_val);\n\n    for (size_t i = aligned_length; i < length; i++) {\n        if ((i % 2) == 0 && src[i] < min_result) {\n            min_result = src[i];\n        }\n    }\n\n    return min_result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    size_t aligned_length = length / 4 * 4;\n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n\n    for(size_t i = 0; i < aligned_length; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i even_vec = _mm_and_si128(vec, mask);\n        __m128i curr_min = _mm_min_epi32(even_vec, min_val);\n        min_val = curr_min;\n    }\n\n    int min_result = 0x7FFFFFFF;\n    min_result = _mm_extract_epi32(min_val, 0);\n    min_result = (min_result < _mm_extract_epi32(min_val, 2)) ? min_result : _mm_extract_epi32(min_val, 2);\n\n    for(size_t i = aligned_length; i < length; ++i){\n        if((i % 2) == 0 && src[i] < min_result){\n            min_result = src[i];\n        }\n    }\n\n    return (min_result == 0x7FFFFFFF) ? -1 : min_result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    size_t aligned_length = length / 4 * 4;\n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask = _mm_set_epi32(0, -1, 0, -1);\n\n    for (size_t i = 0; i < aligned_length; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even_vec = _mm_and_si128(vec, mask);\n        __m128i curr_min = _mm_min_epi32(even_vec, min_val);\n        min_val = curr_min;\n    }\n\n    int result = 0x7FFFFFFF;\n    __m128i shuf = _mm_shuffle_epi32(min_val, _MM_SHUFFLE(0, 0, 0, 0));\n    result = _mm_cvtsi128_si32(shuf);\n\n    for (size_t i = aligned_length; i < length; ++i){\n        if ((i % 2) == 0 && src[i] < result){\n            result = src[i];\n        }\n    }\n\n    return (result == 0x7FFFFFFF) ? -1 : result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if (length == 0) return -1;\n\n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i mask = _mm_set1_epi32(0xAAAAAAAA);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i even_mask = _mm_andnot_si128(mask, _mm_set1_epi32(1));\n        __m128i is_even = _mm_cmpeq_epi32(_mm_and_si128(vec, _mm_set1_epi32(1)), even_mask);\n        __m128i candidate = _mm_and_si128(vec, is_even);\n        __m128i current_min = _mm_min_epi32(candidate, min_val);\n        min_val = _mm_or_si128(current_min, _mm_andnot_si128(is_even, min_val));\n    }\n\n    int min_result = 0x7FFFFFFF;\n    __m128i shuf = _mm_shuffle_epi32(min_val, _MM_SHUFFLE(0, 0, 0, 0));\n    min_result = _mm_cvtsi128_si32(shuf);\n    shuf = _mm_shuffle_epi32(min_val, _MM_SHUFFLE(1, 1, 1, 1));\n    min_result = (int)((_mm_cvtsi128_si32(shuf) < min_result) ? _mm_cvtsi128_si32(shuf) : min_result);\n    shuf = _mm_shuffle_epi32(min_val, _MM_SHUFFLE(2, 2, 2, 2));\n    min_result = (int)((_mm_cvtsi128_si32(shuf) < min_result) ? _mm_cvtsi128_si32(shuf) : min_result);\n    shuf = _mm_shuffle_epi32(min_val, _MM_SHUFFLE(3, 3, 3, 3));\n    min_result = (int)((_mm_cvtsi128_si32(shuf) < min_result) ? _mm_cvtsi128_si32(shuf) : min_result);\n\n    for (; i < length; ++i){\n        if ((src[i] & 1) == 0 && src[i] < min_result){\n            min_result = src[i];\n        }\n    }\n\n    return (min_result == 0x7FFFFFFF) ? -1 : min_result;\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t aligned_length = length / 4 * 4;\n    __m128i shift_amount = _mm_set1_epi32(shift);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i const *)(src + i));\n\n        // Arithmetic right shift\n        __m128i arith_shift = _mm_sra_epi32(src_vec, shift_amount);\n\n        // Logical right shift: convert to unsigned, shift, then convert back to signed\n        __m128i logical_shift = _mm_srl_epi32(_mm_castps_si128(_mm_castsi128_ps(src_vec)), shift_amount);\n\n        // Add arithmetic and logical shifts\n        __m128i result = _mm_add_epi32(arith_shift, logical_shift);\n\n        // Store the result\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_length; i < length; ++i) {\n        int val = src[i];\n        int arith_shift = val >> shift;\n        int logical_shift = static_cast<int>(static_cast<unsigned int>(val) >> shift);\n        dst[i] = arith_shift + logical_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    __m128i shft = _mm_set1_epi32(shift);\n\n    for (; i <= length - simd_width; i += simd_width) {\n        __m128i v_src = _mm_loadu_si128((__m128i const *)(src + i));\n\n        // Arithmetic right shift\n        __m128i v_ashr = _mm_sra_epi32(v_src, shft);\n\n        // Logical right shift: convert to unsigned, shift, then convert back to signed\n        __m128i v_lshr = _mm_srl_epi32(_mm_castps_si128(_mm_castsi128_ps(v_src)), shft);\n\n        // Add arithmetic and logical shifts\n        __m128i v_res = _mm_add_epi32(v_ashr, v_lshr);\n\n        // Store the result\n        _mm_storeu_si128((__m128i *)(dst + i), v_res);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        int val = src[i];\n        int ashift = val >> shift;\n        int lshift = static_cast<int>(static_cast<unsigned int>(val) >> shift);\n        dst[i] = ashift + lshift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t aligned_length = length / 4 * 4;\n    __m128i shift_amount = _mm_set1_epi32(shift);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i const *)(src + i));\n\n        // Arithmetic right shift\n        __m128i v_ashift = _mm_sra_epi32(v_src, shift_amount);\n\n        // Logical right shift: convert to unsigned, shift, then convert back to signed\n        __m128i v_lshift = _mm_srl_epi32(_mm_castps_si128(_mm_castsi128_ps(v_src)), shift_amount);\n\n        // Add results of arithmetic and logical shifts\n        __m128i v_result = _mm_add_epi32(v_ashift, v_lshift);\n\n        // Store the result\n        _mm_storeu_si128((__m128i *)(dst + i), v_result);\n    }\n\n    // Handle remaining elements (if any)\n    for (size_t i = aligned_length; i < length; ++i) {\n        int ashift = src[i] >> shift;\n        unsigned int lshift = ((unsigned int)src[i]) >> shift;\n        dst[i] = ashift + (int)lshift;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t aligned_length = length / 4 * 4;\n    __m128i shifts = _mm_set1_epi32(shift);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const *)(src + i));\n\n        // Arithmetic right shift\n        __m128i ars = _mm_sra_epi32(v, shifts);\n\n        // Logical right shift: convert to unsigned, shift, then convert back to signed\n        __m128i v_unsigned = _mm_castps_si128(_mm_castsi128_ps(v)); // reinterpret as unsigned\n        __m128i lrs = _mm_srl_epi32(v_unsigned, shifts);\n\n        // Add arithmetic and logical shift results\n        __m128i result = _mm_add_epi32(ars, lrs);\n\n        // Store the result\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_length; i < length; ++i) {\n        int val = src[i];\n        int ars = val >> shift;\n        int lrs = static_cast<int>(static_cast<unsigned int>(val) >> shift);\n        dst[i] = ars + lrs;\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    const size_t aligned_length = length / 4 * 4;\n    __m128i zero = _mm_setzero_si128();\n    __m128i sh = _mm_cvtsi32_si128(shift);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const *)(src + i));\n        \n        // Arithmetic right shift\n        __m128i arith_shift = _mm_srai_epi32(v, shift);\n        \n        // Logical right shift: convert to unsigned, shift, then convert back\n        __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n        __m128i u = _mm_and_si128(v, mask); // Make unsigned\n        __m128i logic_shift = _mm_srl_epi32(u, sh);\n        \n        // Add arithmetic and logical shifts\n        __m128i result = _mm_add_epi32(arith_shift, logic_shift);\n        \n        // Store the result\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_length; i < length; ++i) {\n        int val = src[i];\n        int arith_shift = val >> shift;\n        int logic_shift = ((unsigned int)val) >> shift;\n        dst[i] = arith_shift + logic_shift;\n    }\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    __m128i sum = _mm_setzero_si128();\n    __m128i temp;\n\n    size_t i = 0;\n    for (; i <= total - 4; i += 4) {\n        temp = _mm_loadu_si128((__m128i*)&matrix[i]);\n        sum = _mm_add_epi32(sum, temp);\n    }\n\n    int intermediate[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i*)intermediate, sum);\n\n    int scalar_sum = intermediate[0] + intermediate[1] + intermediate[2] + intermediate[3];\n\n    for (; i < total; ++i) {\n        scalar_sum += matrix[i];\n    }\n\n    return scalar_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    __m128i sum = _mm_setzero_si128();\n    __m128i v;\n\n    for (size_t i = 0; i <= total - 4; i += 4) {\n        v = _mm_loadu_si128((__m128i *)(matrix + i));\n        sum = _mm_add_epi32(sum, v);\n    }\n\n    int temp[4] __attribute__((aligned(16)));\n    _mm_store_si128((__m128i *)temp, sum);\n\n    int64_t result = (int64_t)(temp[0] + temp[1] + temp[2] + temp[3]);\n\n    if (total & 3) {\n        for (size_t i = total & ~3; i < total; ++i) {\n            result += matrix[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    __m128i sum = _mm_setzero_si128();\n    __m128i v;\n\n    for (size_t i = 0; i + 4 <= total; i += 4) {\n        v = _mm_loadu_si128((__m128i*)&matrix[i]);\n        sum = _mm_add_epi32(sum, v);\n    }\n\n    // Horizontal sum of the SSE register\n    __m128i sum_low = _mm_unpacklo_epi32(sum, sum);\n    __m128i sum_high = _mm_unpackhi_epi32(sum, sum);\n    __m128i sum_combined = _mm_add_epi32(sum_low, sum_high);\n    __m128i sum_final = _mm_add_epi32(sum_combined, _mm_shuffle_epi32(sum_combined, _MM_SHUFFLE(2, 3, 0, 1)));\n\n    int64_t result = _mm_cvtsi128_si32(sum_final);\n\n    // Add any remaining elements\n    for (size_t i = (total / 4) * 4; i < total; ++i) {\n        result += matrix[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t length = rows * cols;\n    __m128i sum = _mm_setzero_si128();\n    __m128i v;\n\n    for (size_t i = 0; i + 4 <= length; i += 4) {\n        v = _mm_loadu_si128((__m128i *)(matrix + i));\n        sum = _mm_add_epi32(sum, v);\n    }\n\n    // Horizontal sum of the SSE register\n    __m128i sum_low = _mm_unpacklo_epi32(sum, sum);\n    __m128i sum_high = _mm_unpackhi_epi32(sum, sum);\n    sum = _mm_add_epi32(sum_low, sum_high);\n\n    sum_low = _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 0, 3, 2));\n    sum = _mm_add_epi32(sum, sum_low);\n\n    int result = _mm_cvtsi128_si32(sum);\n\n    // Add remaining elements if any\n    for (size_t i = (length / 4) * 4; i < length; ++i) {\n        result += matrix[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    size_t total = rows * cols;\n    __m128i sum = _mm_setzero_si128();\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= total; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const *)(matrix + i));\n        sum = _mm_add_epi32(sum, vec);\n    }\n\n    // Horizontal sum of the remaining SIMD register\n    __m128i sum_high = _mm_unpackhi_epi32(sum, sum);\n    sum = _mm_add_epi32(sum, sum_high);\n    __m128i sum_final = _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 0, 0, 0));\n    sum = _mm_add_epi32(sum, sum_final);\n\n    int result = _mm_cvtsi128_si32(sum);\n\n    // Add any leftover elements\n    for (; i < total; ++i) {\n        result += matrix[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m128i *simd_src = (const __m128i *)src;\n    size_t max_idx = 0;\n    int8_t max_val = src[0];\n\n    size_t i = 0;\n    __m128i current_max = _mm_set1_epi8(src[0]);\n    __m128i current_idx = _mm_set_epi32(0, 0, 0, 0);\n    __m128i idx_step = _mm_set_epi32(16, 16, 16, 16);\n\n    for (; i <= length - 16; i += 16) {\n        __m128i vec = _mm_loadu_si128(simd_src++);\n        __m128i cmp = _mm_cmpeq_epi8(vec, current_max);\n        __m128i mask = _mm_cmpgt_epi8(vec, current_max);\n        current_max = _mm_max_epu8(current_max, vec);\n        current_idx = _mm_blendv_epi8(current_idx, _mm_add_epi32(current_idx, idx_step), mask);\n    }\n\n    int8_t temp[16];\n    _mm_storeu_si128((__m128i *)temp, current_max);\n    for (int j = 0; j < 16; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n            max_idx = ((int *)(&current_idx))[j];\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m128i *simd_src = (const __m128i *)src;\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m128i vec = _mm_loadu_si128(simd_src + (i / 16));\n        __m128i cur_max = _mm_max_epi8(vec, _mm_set1_epi8(max_value));\n\n        __m128i cmp = _mm_cmpeq_epi8(cur_max, vec);\n        __m128i mask = _mm_cmpgt_epi8(cur_max, _mm_set1_epi8(max_value));\n\n        int mask_val = _mm_movemask_epi8(mask);\n        if (mask_val) {\n            max_value = _mm_extract_epi8(cur_max, __builtin_ctz(mask_val));\n            max_index = i + __builtin_ctz(_mm_movemask_epi8(cmp));\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m128i *simd_src = (const __m128i *)src;\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n    __m128i current_max = _mm_set1_epi8(src[0]);\n    __m128i current_index = _mm_set_epi32(0, 0, 0, 0);\n    __m128i index_step = _mm_set_epi32(4, 3, 2, 1);\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16, simd_src++) {\n        __m128i lo = _mm_loadu_si128(simd_src);\n        __m128i hi = _mm_loadu_si128(simd_src + 1);\n\n        __m128i cmp_lo = _mm_cmpgt_epi8(lo, current_max);\n        __m128i cmp_hi = _mm_cmpgt_epi8(hi, current_max);\n\n        __m128i new_max_lo = _mm_max_epu8(lo, current_max);\n        __m128i new_max_hi = _mm_max_epu8(hi, current_max);\n\n        __m128i mask_lo = _mm_movemask_epi8(cmp_lo);\n        __m128i mask_hi = _mm_movemask_epi8(cmp_hi);\n\n        if (mask_lo) {\n            current_max = new_max_lo;\n            current_index = _mm_add_epi32(current_index, _mm_and_si128(_mm_set1_epi32(mask_lo), index_step));\n            max_index = i + _mm_extract_epi32(current_index, 0);\n        }\n        if (mask_hi) {\n            current_max = new_max_hi;\n            current_index = _mm_add_epi32(current_index, _mm_and_si128(_mm_set1_epi32(mask_hi), index_step));\n            max_index = i + 8 + _mm_extract_epi32(current_index, 0);\n        }\n    }\n\n    for (; i < length; i++) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m128i *simd_src = (const __m128i *)src;\n    size_t max_idx = 0;\n    int8_t max_val = src[0];\n    __m128i current_max = _mm_set1_epi8(src[0]);\n    __m128i current_idx = _mm_set_epi32(0, 0, 0, 0);\n    __m128i idx_step = _mm_set_epi32(4, 3, 2, 1);\n\n    for (size_t i = 0; i + 16 <= length; i += 16, simd_src++) {\n        __m128i vec = _mm_loadu_si128(simd_src);\n        __m128i cmp = _mm_cmpgt_epi8(vec, current_max);\n        __m128i new_max = _mm_blendv_epi8(current_max, vec, cmp);\n        __m128i new_idx = _mm_blendv_epi8(current_idx, _mm_add_epi32(current_idx, idx_step), cmp);\n        current_max = new_max;\n        current_idx = new_idx;\n\n        // Check horizontally\n        int max_in_vec = _mm_extract_epi8(current_max, 0);\n        for (int j = 1; j < 16; ++j) {\n            int val = _mm_extract_epi8(current_max, j);\n            if (val > max_in_vec) {\n                max_in_vec = val;\n                max_idx = _mm_extract_epi32(current_idx, j / 4) + (j % 4);\n            }\n        }\n    }\n\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m128i *simd_src = (const __m128i *)src;\n    size_t max_index = 0;\n    int8_t max_value = src[0];\n\n    size_t i = 0;\n    __m128i current_max = _mm_set1_epi8(src[0]);\n    __m128i current_index = _mm_set_epi32(0, 0, 0, 0);\n    __m128i index_mask = _mm_set_epi32(3, 2, 1, 0);\n\n    for (; i <= length - 16; i += 16) {\n        __m128i vec = _mm_loadu_si128(simd_src + i / 16);\n        __m128i cmp = _mm_cmpgt_epi8(vec, current_max);\n        __m128i new_max = _mm_max_epu8(current_max, vec);\n\n        __m128i indices = _mm_add_epi32(_mm_and_si128(cmp, index_mask), current_index);\n        __m128i mask = _mm_movemask_epi8(cmp);\n\n        if (mask) {\n            int lane = __builtin_ctz(mask);\n            max_value = ((int8_t *)&new_max)[lane];\n            max_index = i + lane;\n            current_max = _mm_set1_epi8(max_value);\n            current_index = _mm_set_epi32(i + 12, i + 8, i + 4, i);\n        } else {\n            current_index = _mm_add_epi32(current_index, _mm_set1_epi32(4));\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i <= length - 8; i += 8) {\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i m = _mm_set1_epi32(mask);\n        __m128i and_result = _mm_and_si128(indices, m);\n        __m128i cmp_result = _mm_cmpeq_epi32(and_result, zero);\n        __m128i src1_val = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_val = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i blended = _mm_or_si128(_mm_andnot_si128(cmp_result, src1_val), _mm_and_si128(cmp_result, src2_val));\n        _mm_storeu_si128((__m128i*)&dst[i], blended);\n    }\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i msk = _mm_set1_epi32(mask);\n\n    for (; i + 7 < length; i += 8) {\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i and_res = _mm_and_si128(indices, msk);\n        __m128i cmp_res = _mm_cmpeq_epi32(and_res, zero);\n\n        __m128i s1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i s2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i blend = _mm_or_si128(\n            _mm_and_si128(cmp_res, s2),\n            _mm_andnot_si128(cmp_res, s1)\n        );\n\n        _mm_storeu_si128((__m128i*)&dst[i], blend);\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i <= length - 8; i += 8) {\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i m = _mm_set1_epi32(mask);\n        __m128i and_res = _mm_and_si128(indices, m);\n        __m128i cmp_res = _mm_cmpeq_epi32(and_res, zero);\n        __m128i src1_val = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_val = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i blend_res = _mm_or_si128(_mm_andnot_si128(cmp_res, src1_val), _mm_and_si128(cmp_res, src2_val));\n        _mm_storeu_si128((__m128i*)&dst[i], blend_res);\n    }\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_cmpeq_epi16(zero, zero);\n\n    for (; i <= length - simd_width; i += simd_width) {\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i m = _mm_set1_epi32(mask);\n        __m128i and_result = _mm_and_si128(indices, m);\n        __m128i cmp_result = _mm_cmpgt_epi32(and_result, zero);\n\n        __m128i src1_vec = _mm_loadu_si128((const __m128i*)&src1[i]);\n        __m128i src2_vec = _mm_loadu_si128((const __m128i*)&src2[i]);\n\n        __m128i blend_mask = _mm_packus_epi32(cmp_result, cmp_result);\n        __m128i result = _mm_or_si128(_mm_and_si128(blend_mask, src1_vec), _mm_andnot_si128(blend_mask, src2_vec));\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i <= length - 8; i += 8) {\n        __m128i idx = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i msk = _mm_set1_epi32(mask);\n        __m128i and_res = _mm_and_si128(idx, msk);\n        __m128i cmp_res = _mm_cmpeq_epi32(and_res, zero);\n        __m128i src1_val = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i src2_val = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i blend_res = _mm_or_si128(_mm_andnot_si128(cmp_res, src1_val), _mm_and_si128(cmp_res, src2_val));\n        _mm_storeu_si128((__m128i*)&dst[i], blend_res);\n    }\n    for (; i < length; ++i) {\n        if ((i & mask) != 0) {\n            dst[i] = src1[i];\n        } else {\n            dst[i] = src2[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const __m128i mask1 = _mm_set1_epi32(0x55555555);\n    const __m128i mask2 = _mm_set1_epi32(0x33333333);\n    const __m128i mask4 = _mm_set1_epi32(0x0f0f0f0f);\n    const __m128i mask8 = _mm_set1_epi32(0x00ff00ff);\n    const __m128i count3 = _mm_set1_epi32(3);\n\n    for (size_t i = 0; i + 4 <= length; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i*)&src[i]);\n\n        __m128i v = data;\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 1), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 2), mask2));\n        v = _mm_mullo_epi32(v, count3);\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 4), mask4));\n        v = _mm_add_epi32(_mm_and_si128(v, mask8), _mm_and_si128(_mm_srli_epi32(v, 8), mask8));\n        v = _mm_add_epi32(v, _mm_srli_epi32(v, 16));\n\n        _mm_storeu_si128((__m128i*)&dst[i], v);\n    }\n\n    for (size_t i = (length / 4) * 4; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const __m128i ones = _mm_set1_epi8(0x01);\n    const __m128i counts1 = _mm_set_epi8(0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4);\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i count = _mm_setzero_si128();\n\n        v = _mm_and_si128(v, _mm_set1_epi32(0x55555555));\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n        v = _mm_srli_epi32(v, 1);\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n\n        v = _mm_and_si128(_mm_srli_epi32(v, 1), _mm_set1_epi32(0x55555555));\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n        v = _mm_srli_epi32(v, 1);\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n\n        v = _mm_and_si128(_mm_srli_epi32(v, 1), _mm_set1_epi32(0x55555555));\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n        v = _mm_srli_epi32(v, 1);\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n\n        v = _mm_and_si128(_mm_srli_epi32(v, 1), _mm_set1_epi32(0x55555555));\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n        v = _mm_srli_epi32(v, 1);\n        count = _mm_add_epi8(count, _mm_shuffle_epi8(counts1, _mm_and_si128(v, ones)));\n\n        count = _mm_sad_epu8(count, _mm_setzero_si128());\n        _mm_storeu_si128((__m128i *)(dst + i), _mm_packus_epi16(_mm_unpacklo_epi8(count, _mm_setzero_si128()), _mm_unpackhi_epi8(count, _mm_setzero_si128())));\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const __m128i mask0 = _mm_set1_epi8(0x55);\n    const __m128i mask1 = _mm_set1_epi8(0x33);\n    const __m128i mask2 = _mm_set1_epi8(0x0F);\n    const __m128i count1 = _mm_set1_epi8(0x01);\n    const __m128i count2 = _mm_set1_epi8(0x03);\n    const __m128i count4 = _mm_set1_epi8(0x0F);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&src[i]);\n\n        __m128i v0 = _mm_and_si128(v, mask0);\n        __m128i v1 = _mm_and_si128(_mm_srli_epi32(v, 1), mask0);\n        v = _mm_add_epi8(v0, v1);\n\n        v0 = _mm_and_si128(v, mask1);\n        v1 = _mm_and_si128(_mm_srli_epi32(v, 2), mask1);\n        v = _mm_add_epi8(v0, v1);\n\n        v0 = _mm_and_si128(v, mask2);\n        v1 = _mm_and_si128(_mm_srli_epi32(v, 4), mask2);\n        v = _mm_add_epi8(v0, v1);\n\n        v = _mm_shuffle_epi8(count4, v);\n        v = _mm_sad_epu8(v, _mm_setzero_si128());\n\n        _mm_storeu_si128((__m128i*)&dst[i], _mm_cvtepi8_epi32(v));\n    }\n\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const __m128i mask1 = _mm_set1_epi32(0x55555555);\n    const __m128i mask2 = _mm_set1_epi32(0x33333333);\n    const __m128i mask4 = _mm_set1_epi32(0x0f0f0f0f);\n    const __m128i mask8 = _mm_set1_epi32(0x00ff00ff);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(src + i));\n\n        v = _mm_and_si128(v, mask1);\n        __m128i t = _mm_srli_epi32(v, 1);\n        t = _mm_and_si128(t, mask1);\n        v = _mm_add_epi32(v, t);\n\n        t = _mm_srli_epi32(v, 2);\n        v = _mm_and_si128(v, mask2);\n        t = _mm_and_si128(t, mask2);\n        v = _mm_add_epi32(v, t);\n\n        t = _mm_srli_epi32(v, 4);\n        v = _mm_and_si128(v, mask4);\n        t = _mm_and_si128(t, mask4);\n        v = _mm_add_epi32(v, t);\n\n        t = _mm_srli_epi32(v, 8);\n        v = _mm_and_si128(v, mask8);\n        t = _mm_and_si128(t, mask8);\n        v = _mm_add_epi32(v, t);\n\n        t = _mm_srli_epi32(v, 16);\n        v = _mm_add_epi32(v, t);\n\n        _mm_storeu_si128((__m128i *)(dst + i), v);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    const __m128i mask0 = _mm_set1_epi32(0x55555555);\n    const __m128i mask1 = _mm_set1_epi32(0x33333333);\n    const __m128i mask2 = _mm_set1_epi32(0x0F0F0F0F);\n    const __m128i mask3 = _mm_set1_epi32(0x00FF00FF);\n    const __m128i mask4 = _mm_set1_epi32(0x0000FFFF);\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i v = _mm_loadu_si128((const __m128i*)&src[i]);\n        \n        v = _mm_sub_epi32(_mm_and_si128(v, mask0), _mm_and_si128(_mm_srli_epi32(v, 1), mask0));\n        v = _mm_add_epi32(_mm_and_si128(v, mask1), _mm_and_si128(_mm_srli_epi32(v, 2), mask1));\n        v = _mm_add_epi32(_mm_and_si128(v, mask2), _mm_and_si128(_mm_srli_epi32(v, 4), mask2));\n        v = _mm_add_epi32(_mm_and_si128(v, mask3), _mm_and_si128(_mm_srli_epi32(v, 8), mask3));\n        v = _mm_add_epi32(_mm_and_si128(v, mask4), _mm_and_si128(_mm_srli_epi32(v, 16), mask4));\n\n        _mm_storeu_si128((__m128i*)&dst[i], v);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    for (; i <= length - simd_width; i += simd_width) {\n        __m128i vec1 = _mm_loadu_si128((__m128i const*)&src1[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i const*)&src2[i]);\n        __m128i result = _mm_adds_epu16(vec1, vec2);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] > UINT16_MAX) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 7 < length; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i result = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n    for (; i < length; ++i) {\n        dst[i] = (uint16_t)((src1[i] + src2[i]) > UINT16_MAX ? UINT16_MAX : (src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    for (; i <= length - simd_width; i += simd_width) {\n        __m128i a = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i result = _mm_adds_epu16(a, b);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] > UINT16_MAX) ? UINT16_MAX : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i <= length - simd_width; i += simd_width) {\n        __m128i v1 = _mm_loadu_si128((__m128i const*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((__m128i const*)(src2 + i));\n        __m128i result = _mm_adds_epu16(v1, v2);\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint16_t)((src1[i] + src2[i]) > UINT16_MAX ? UINT16_MAX : src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n#include <stddef.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    __m128i max_val = _mm_set1_epi16(0xFFFF);\n\n    for (; i <= length - simd_width; i += simd_width) {\n        __m128i vec1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i vec2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i sum = _mm_adds_epu16(vec1, vec2);\n\n        _mm_storeu_si128((__m128i*)&dst[i], sum);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src1[i] + src2[i] > 0xFFFF) ? 0xFFFF : src1[i] + src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < n; j += 2) {\n                __m128d a_val = _mm_loadu_pd(&A[i * n + j]);\n                __m128d b_val = _mm_loadu_pd(&B[j * p + k]);\n\n                __m128d in_range_a = _mm_and_ps(_mm_cmpge_pd(a_val, lower_bound), _mm_cmple_pd(a_val, upper_bound));\n                __m128d in_range_b = _mm_and_ps(_mm_cmpge_pd(b_val, lower_bound), _mm_cmple_pd(b_val, upper_bound));\n\n                __m128d mask = _mm_and_pd(in_range_a, in_range_b);\n                __m128d product = _mm_mul_pd(a_val, b_val);\n\n                product = _mm_and_pd(product, mask);\n\n                sum += _mm_cvtsd_f64(product);\n                __m128d high_part = _mm_shuffle_pd(product, product, 1);\n                sum += _mm_cvtsd_f64(high_part);\n            }\n            C[i * p + k] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m128d a_val = _mm_loadu_pd(&A[i * n + k]);\n            __m128d in_range_a = _mm_and_ps(_mm_cmpge_pd(a_val, lower_bound), _mm_cmple_pd(a_val, upper_bound));\n            in_range_a = _mm_and_ps(in_range_a, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n\n            for (size_t j = 0; j < p; j += 2) {\n                __m128d b_val = _mm_loadu_pd(&B[k * p + j]);\n                __m128d in_range_b = _mm_and_ps(_mm_cmpge_pd(b_val, lower_bound), _mm_cmple_pd(b_val, upper_bound));\n                in_range_b = _mm_and_ps(in_range_b, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n\n                __m128d in_range = _mm_and_pd(in_range_a, in_range_b);\n                __m128d prod = _mm_mul_pd(a_val, b_val);\n                prod = _mm_and_pd(prod, in_range);\n\n                __m128d c_val = _mm_loadu_pd(&C[i * p + j]);\n                c_val = _mm_add_pd(c_val, prod);\n                _mm_storeu_pd(&C[i * p + j], c_val);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; k += 2) {\n            __m128d a_vec = _mm_loadu_pd(&A[i * n + k]);\n            __m128d in_range_a = _mm_and_ps(_mm_cmpge_pd(a_vec, lower_bound), _mm_cmple_pd(a_vec, upper_bound));\n            a_vec = _mm_and_pd(a_vec, in_range_a);\n\n            for (size_t j = 0; j < p; j += 2) {\n                __m128d c_vec = _mm_loadu_pd(&C[i * p + j]);\n                __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n                \n                __m128d in_range_b = _mm_and_ps(_mm_cmpge_pd(b_vec, lower_bound), _mm_cmple_pd(b_vec, upper_bound));\n                b_vec = _mm_and_pd(b_vec, in_range_b);\n\n                __m128d product = _mm_mul_pd(a_vec, b_vec);\n                c_vec = _mm_add_pd(c_vec, product);\n\n                _mm_storeu_pd(&C[i * p + j], c_vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m128d a_val = _mm_loadu_pd(&A[i * n + k]);\n            __m128d in_range_a = _mm_and_pd(_mm_cmpge_pd(a_val, lower_bound), _mm_cmple_pd(a_val, upper_bound));\n            a_val = _mm_and_pd(a_val, in_range_a);\n\n            for (size_t j = 0; j < p; j += 2) {\n                __m128d b_val = _mm_loadu_pd(&B[k * p + j]);\n                __m128d in_range_b = _mm_and_pd(_mm_cmpge_pd(b_val, lower_bound), _mm_cmple_pd(b_val, upper_bound));\n                b_val = _mm_and_pd(b_val, in_range_b);\n\n                __m128d product = _mm_mul_pd(a_val, b_val);\n\n                __m128d c_val = _mm_loadu_pd(&C[i * p + j]);\n                c_val = _mm_add_pd(c_val, product);\n                _mm_storeu_pd(&C[i * p + j], c_val);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m128d lower_bound = _mm_set1_pd(-100.0);\n    __m128d upper_bound = _mm_set1_pd(100.0);\n    __m128d zero = _mm_setzero_pd();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m128d a_val = _mm_loadu_pd(&A[i * n + k]);\n            __m128d in_range_a = _mm_and_ps(_mm_cmpge_pd(a_val, lower_bound), _mm_cmple_pd(a_val, upper_bound));\n            a_val = _mm_and_pd(a_val, in_range_a);\n\n            for (size_t j = 0; j < p; j += 2) {\n                __m128d b_val = _mm_loadu_pd(&B[k * p + j]);\n                __m128d in_range_b = _mm_and_ps(_mm_cmpge_pd(b_val, lower_bound), _mm_cmple_pd(b_val, upper_bound));\n                b_val = _mm_and_pd(b_val, in_range_b);\n\n                __m128d product = _mm_mul_pd(a_val, b_val);\n\n                __m128d c_val = _mm_loadu_pd(&C[i * p + j]);\n                c_val = _mm_add_pd(c_val, product);\n                _mm_storeu_pd(&C[i * p + j], c_val);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <emmintrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i result = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle any remaining elements\n    if (i < total_elements) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t aligned_elements = total_elements - (total_elements % 2);\n\n    for (size_t i = 0; i < aligned_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n\n    if (aligned_elements < total_elements) {\n        C[aligned_elements] = A[aligned_elements] + B[aligned_elements];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n\n    if (i < total_elements) {\n        __m128i a = _mm_loadl_epi64((const __m128i*)&A[i]);\n        __m128i b = _mm_loadl_epi64((const __m128i*)&B[i]);\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storel_epi64((__m128i*)&C[i], c);\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 1 < total_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n\n    if (i < total_elements) {\n        __m128i a = _mm_loadl_epi64((__m128i*)&A[i]);\n        __m128i b = _mm_loadl_epi64((__m128i*)&B[i]);\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storel_epi64((__m128i*)&C[i], c);\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t aligned_elements = total_elements - (total_elements % 2);\n\n    for (size_t i = 0; i < aligned_elements; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i c = _mm_add_epi64(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], c);\n    }\n\n    if (aligned_elements < total_elements) {\n        C[aligned_elements] = A[aligned_elements] + B[aligned_elements];\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    \n    for (; i <= length - 4; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i);\n        __m128 vec2 = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(vec1, vec2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    \n    for (; i + 3 < length; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i);\n        __m128 vec2 = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(vec1, vec2);\n        __m128 rounded = _mm_cvtepi32_ps(_mm_cvtps_epi32(sum));\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; i++) {\n        dst[i] = (float)((int)(src1[i] + src2[i] + 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 rounding_mode = _mm_set_ps(0.5f, 0.5f, 0.5f, 0.5f);\n\n    for (; i + 3 < length; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i);\n        __m128 vec2 = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(vec1, vec2);\n        __m128 rounded = _mm_add_ps(sum, rounding_mode);\n        __m128 result = _mm_cvtepi32_ps(_mm_cvttps_epi32(rounded));\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (float)((int)(src1[i] + src2[i] + 0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    \n    for (; i + 3 < length; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i);\n        __m128 vec2 = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(vec1, vec2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n    \n    for (; i < length; ++i) {\n        dst[i] = nearbyintf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    \n    for (; i + 4 <= length; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i);\n        __m128 vec2 = _mm_loadu_ps(src2 + i);\n        __m128 sum = _mm_add_ps(vec1, vec2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        _mm_storeu_ps(dst + i, rounded);\n    }\n\n    // Handle remaining elements\n    if (i < length) {\n        __m128 vec1 = _mm_loadu_ps(src1 + i);\n        __m128 vec2 = _mm_loadu_ps(src2 + i);\n        __m128 mask = _mm_cmplt_ps(_mm_set1_ps(i + 4 - length), _mm_set1_ps(4.0f));\n        __m128 sum = _mm_add_ps(vec1, vec2);\n        __m128 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m128 prev = _mm_loadu_ps(dst + i);\n        __m128 result = _mm_or_ps(_mm_and_ps(mask, rounded), _mm_andnot_ps(mask, prev));\n        _mm_storeu_ps(dst + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    __m128 v50 = _mm_set1_ps(50.0f);\n    __m128 vmin = _mm_set1_ps(-1.0f);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        if ((i & 1) == 0) {\n            __m128 vsrc1 = _mm_loadu_ps(src1 + i);\n            __m128 vsrc2 = _mm_loadu_ps(src2 + i);\n\n            __m128 diff = _mm_sub_ps(vsrc1, vsrc2);\n            __m128 abs_diff = _mm_andnot_ps(_mm_castsi128_ps(_mm_cmplt_epi32(_mm_castps_si128(diff), _mm_setzero_si128())), diff);\n            __m128 cond = _mm_cmple_ps(abs_diff, v50);\n\n            __m128 mul_res = _mm_mul_ps(vsrc1, vsrc2);\n            __m128 ceil_res = _mm_ceil_ps(mul_res);\n\n            __m128 res = _mm_blendv_ps(vmin, ceil_res, cond);\n            _mm_storeu_ps(dst + i, res);\n        } else {\n            __m128 res = _mm_set1_ps(-1.0f);\n            _mm_storeu_ps(dst + i, res);\n        }\n    }\n\n    for (; i < length; ++i) {\n        if ((i & 1) == 0) {\n            float diff = src1[i] - src2[i];\n            if (diff < 0) diff = -diff;\n            if (diff <= 50.0f) {\n                dst[i] = std::ceil(src1[i] * src2[i]);\n            } else {\n                dst[i] = -1.0f;\n            }\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    __m128 v50 = _mm_set1_ps(50.0f);\n    __m128 vmin = _mm_set1_ps(-1.0f);\n    __m128i mask;\n\n    for(size_t i = 0; i + 4 <= length; i += 4){\n        if(i % 2 == 0){\n            __m128 va = _mm_loadu_ps(src1 + i);\n            __m128 vb = _mm_loadu_ps(src2 + i);\n\n            __m128 diff = _mm_sub_ps(va, vb);\n            __m128 abs_diff = _mm_andnot_ps(_mm_castsi128_ps(_mm_cmplt_epi32(_mm_castps_si128(diff), _mm_setzero_si128())), diff);\n            __m128 cond = _mm_cmple_ps(abs_diff, v50);\n\n            __m128 mul_res = _mm_mul_ps(va, vb);\n            __m128 ceil_res = _mm_ceil_ps(mul_res);\n\n            mask = _mm_castps_si128(cond);\n            mask = _mm_cmpeq_epi32(mask, _mm_set1_epi32(-1));\n            __m128 res = _mm_blendv_ps(vmin, ceil_res, _mm_castsi128_ps(mask));\n\n            _mm_storeu_ps(dst + i, res);\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = (length / 4) * 4; i < length; ++i){\n        if(i % 2 == 0){\n            if(std::fabs(src1[i] - src2[i]) <= 50.0f){\n                dst[i] = std::ceil(src1[i] * src2[i]);\n            }\n            else{\n                dst[i] = -1.0f;\n            }\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    __m128 v50 = _mm_set1_ps(50.0f);\n    __m128 vNeg1 = _mm_set1_ps(-1.0f);\n    __m128i vTwo = _mm_set1_epi32(2);\n\n    for(size_t i = 0; i + 4 <= length; i += 4){\n        __m128 va = _mm_loadu_ps(src1 + i);\n        __m128 vb = _mm_loadu_ps(src2 + i);\n        __m128 diff = _mm_sub_ps(va, vb);\n        __m128 abs_diff = _mm_max_ps(_mm_mul_ps(diff, _mm_set1_ps(-1.0f)), diff);\n        __m128 cond1 = _mm_cmple_ps(abs_diff, v50);\n        \n        __m128 indices = _mm_set_ps(i+3, i+2, i+1, i);\n        __m128i indices_int = _mm_cvtps_epi32(indices);\n        __m128i mod_res = _mm_andnot_si128(_mm_rem_epu32(indices_int, vTwo), _mm_set1_epi32(-1));\n        __m128 cond2 = _mm_castsi128_ps(mod_res);\n\n        __m128 cond = _mm_and_ps(cond1, _mm_castsi128_ps(mod_res));\n        __m128 mul_res = _mm_mul_ps(va, vb);\n        __m128 ceil_res = _mm_ceil_ps(mul_res);\n        __m128 res = _mm_blendv_ps(vNeg1, ceil_res, cond);\n        _mm_storeu_ps(dst + i, res);\n    }\n\n    for(size_t i = (length / 4) * 4; i < length; ++i){\n        if( fabs(src1[i] - src2[i]) <= 50.0 && (i % 2 == 0) ){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    __m128 v50 = _mm_set1_ps(50.0f);\n    __m128 vNeg1 = _mm_set1_ps(-1.0f);\n    __m128i vTwo = _mm_set1_epi32(2);\n\n    for(size_t i = 0; i + 4 <= length; i += 4){\n        __m128 vSrc1 = _mm_loadu_ps(src1 + i);\n        __m128 vSrc2 = _mm_loadu_ps(src2 + i);\n\n        // Calculate absolute difference\n        __m128 vDiff = _mm_sub_ps(vSrc1, vSrc2);\n        __m128 vAbsDiff = _mm_andnot_ps(_mm_castsi128_ps(_mm_slli_epi32(_mm_castps_si128(vDiff), 31)), vDiff);\n\n        // Check if abs(diff) <= 50\n        __m128 cond1 = _mm_cmple_ps(vAbsDiff, v50);\n\n        // Check if index is multiple of 2\n        __m128i indices = _mm_add_epi32(_mm_set_epi32(i+3, i+2, i+1, i+0), vTwo);\n        __m128i modRes = _mm_and_si128(indices, _mm_set1_epi32(1));\n        __m128 cond2 = _mm_castsi128_ps(_mm_cmpeq_epi32(modRes, _mm_setzero_si128()));\n\n        // Combine conditions\n        __m128 mask = _mm_and_ps(cond1, cond2);\n\n        // Perform multiplication\n        __m128 vMul = _mm_mul_ps(vSrc1, vSrc2);\n\n        // Round up toward positive infinity\n        __m128 vRounded = _mm_add_ps(vMul, _mm_set1_ps(0.999999f));\n\n        // Select -1 or rounded result based on mask\n        __m128 vResult = _mm_or_ps(_mm_and_ps(mask, vRounded), _mm_andnot_ps(mask, vNeg1));\n\n        // Store the result\n        _mm_storeu_ps(dst + i, vResult);\n    }\n\n    // Handle remaining elements\n    for(size_t i = (length / 4) * 4; i < length; ++i){\n        if( (fabs(src1[i] - src2[i]) <= 50.0f) && ((i % 2) == 0) ){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    __m128 v50 = _mm_set1_ps(50.0f);\n    __m128 vmin = _mm_set1_ps(-50.0f);\n    __m128 vminusone = _mm_set1_ps(-1.0f);\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        if (i % 2 == 0) {\n            __m128 va = _mm_loadu_ps(src1 + i);\n            __m128 vb = _mm_loadu_ps(src2 + i);\n            __m128 diff = _mm_sub_ps(va, vb);\n            __m128 cond1 = _mm_and_ps(_mm_cmple_ps(diff, v50), _mm_cmpge_ps(diff, vmin));\n            __m128 mul = _mm_mul_ps(va, vb);\n            __m128 ceil_val = _mm_ceil_ps(mul);\n            __m128 result = _mm_blendv_ps(vminusone, ceil_val, cond1);\n            _mm_storeu_ps(dst + i, result);\n        } else {\n            __m128 vminusone_rep = _mm_set1_ps(-1.0f);\n            _mm_storeu_ps(dst + i, vminusone_rep);\n        }\n    }\n\n    for (; i < length; i++) {\n        if (i % 2 == 0) {\n            float diff = src1[i] - src2[i];\n            if (diff >= -50.0f && diff <= 50.0f) {\n                dst[i] = std::ceil(src1[i] * src2[i]);\n            } else {\n                dst[i] = -1.0f;\n            }\n        } else {\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < total; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]);\n        __m128d result = _mm_mul_pd(vec, scalar_vec);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle remaining element if total is odd\n    if (total & 1) {\n        dst[total - 1] = mat[total - 1] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < total; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]);\n        __m128d result = _mm_mul_pd(vec, scalar_vec);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle remaining element if total is odd\n    if (total & 1) {\n        dst[total - 1] = mat[total - 1] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    size_t i = 0;\n    for (; i + 1 <= total; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]);\n        __m128d result = _mm_mul_pd(vec, scalar_vec);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    if (i < total) {\n        dst[i] = mat[i] * scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d s = _mm_set1_pd(scalar);\n\n    size_t i = 0;\n    for (; i + 1 <= total; i += 2) {\n        __m128d v = _mm_loadu_pd(&mat[i]);\n        __m128d r = _mm_mul_pd(v, s);\n        _mm_storeu_pd(&dst[i], r);\n    }\n\n    if (i < total) {\n        __m128d v = _mm_set_sd(mat[i]);\n        __m128d r = _mm_mul_sd(v, s);\n        dst[i] = _mm_cvtsd_f64(r);\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    __m128d sc = _mm_set1_pd(scalar);\n\n    for (size_t i = 0; i < total; i += 2) {\n        __m128d vec = _mm_loadu_pd(&mat[i]);\n        __m128d res = _mm_mul_pd(vec, sc);\n        _mm_storeu_pd(&dst[i], res);\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        size_t j = 0;\n\n        __m128 sum = _mm_setzero_ps();\n\n        // Compute the squared sum using SIMD\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(&row[j]);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum = _mm_add_ps(sum, squared);\n        }\n\n        // Horizontal add of the SIMD sum\n        float sum_arr[4];\n        _mm_storeu_ps(sum_arr, sum);\n        float squared_sum = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n\n        // Process remaining elements\n        for (; j < n; ++j) {\n            squared_sum += row[j] * row[j];\n        }\n\n        float norm = sqrtf(squared_sum);\n\n        if (norm > 1e-6f) {\n            norm = 1.0f / norm;\n\n            // Normalize the row using SIMD\n            j = 0;\n            for (; j + 4 <= n; j += 4) {\n                __m128 vec = _mm_loadu_ps(&row[j]);\n                __m128 normalized = _mm_mul_ps(vec, _mm_set1_ps(norm));\n                _mm_storeu_ps(&row[j], normalized);\n            }\n\n            // Normalize remaining elements\n            for (; j < n; ++j) {\n                row[j] *= norm;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m128 sum = _mm_setzero_ps();\n        \n        // Compute the sum of squares using SIMD\n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(&row[j]);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum = _mm_add_ps(sum, squared);\n        }\n        \n        // Horizontal sum of the SIMD result\n        float sum_arr[4];\n        _mm_storeu_ps(sum_arr, sum);\n        float norm_sq = sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3];\n        \n        // Process remaining elements\n        for (; j < n; ++j) {\n            norm_sq += row[j] * row[j];\n        }\n        \n        // Avoid division by zero\n        float norm = (norm_sq > 0.0f) ? sqrtf(norm_sq) : 1.0f;\n        \n        // Normalize the row\n        __m128 inv_norm = _mm_set1_ps(1.0f / norm);\n        j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(&row[j]);\n            vec = _mm_mul_ps(vec, inv_norm);\n            _mm_storeu_ps(&row[j], vec);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            row[j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = 0;\n        __m128 sum = _mm_setzero_ps();\n        \n        // Compute L2 norm using SIMD\n        for (; j + 3 < n; j += 4) {\n            __m128 vec = _mm_loadu_ps(mat + i * n + j);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum = _mm_add_ps(sum, squared);\n        }\n        \n        float remainder_sum = 0.0f;\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            remainder_sum += val * val;\n        }\n        \n        float norm_squared[4];\n        _mm_storeu_ps(norm_squared, sum);\n        float norm = sqrt(norm_squared[0] + norm_squared[1] + norm_squared[2] + norm_squared[3] + remainder_sum);\n        \n        if (norm == 0.0f) continue; // Avoid division by zero\n        \n        __m128 inv_norm = _mm_set1_ps(1.0f / norm);\n        \n        // Normalize row using SIMD\n        j = 0;\n        for (; j + 3 < n; j += 4) {\n            __m128 vec = _mm_loadu_ps(mat + i * n + j);\n            vec = _mm_mul_ps(vec, inv_norm);\n            _mm_storeu_ps(mat + i * n + j, vec);\n        }\n        \n        // Normalize remaining elements\n        for (; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <math.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        float* row = &mat[i * n];\n        __m128 sum = _mm_setzero_ps();\n        \n        size_t j = 0;\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(&row[j]);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum = _mm_add_ps(sum, squared);\n        }\n        \n        float remainder_sum = 0.0f;\n        for (; j < n; ++j) {\n            remainder_sum += row[j] * row[j];\n        }\n        \n        float norm_squared = 0.0f;\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n        _mm_store_ss(&norm_squared, sum);\n        norm_squared += remainder_sum;\n        float norm = sqrtf(norm_squared);\n        \n        if (norm > 1e-6f) {\n            __m128 inv_norm = _mm_set1_ps(1.0f / norm);\n            j = 0;\n            for (; j + 4 <= n; j += 4) {\n                __m128 vec = _mm_loadu_ps(&row[j]);\n                vec = _mm_mul_ps(vec, inv_norm);\n                _mm_storeu_ps(&row[j], vec);\n            }\n            for (; j < n; ++j) {\n                row[j] /= norm;\n            }\n        } else {\n            // Handle zero or near-zero norm case (optional based on requirements)\n            for (j = 0; j < n; ++j) {\n                row[j] = 0.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        size_t j = 0;\n        __m128 sum = _mm_setzero_ps();\n        \n        // Calculate L2 norm using SSE\n        for (; j + 4 <= n; j += 4) {\n            __m128 vec = _mm_loadu_ps(mat + i * n + j);\n            __m128 squared = _mm_mul_ps(vec, vec);\n            sum = _mm_add_ps(sum, squared);\n        }\n        \n        float remainder_sum = 0.0f;\n        for (; j < n; ++j) {\n            float val = mat[i * n + j];\n            remainder_sum += val * val;\n        }\n        \n        float sums[4];\n        _mm_storeu_ps(sums, sum);\n        float total_sum = sums[0] + sums[1] + sums[2] + sums[3] + remainder_sum;\n        float norm = sqrtf(total_sum);\n        \n        if (norm != 0.0f) {\n            norm = 1.0f / norm;\n            \n            j = 0;\n            __m128 inv_norm = _mm_set1_ps(norm);\n            for (; j + 4 <= n; j += 4) {\n                __m128 vec = _mm_loadu_ps(mat + i * n + j);\n                vec = _mm_mul_ps(vec, inv_norm);\n                _mm_storeu_ps(mat + i * n + j, vec);\n            }\n            \n            for (; j < n; ++j) {\n                mat[i * n + j] *= norm;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum1 = _mm_setzero_pd();\n            __m128d sum2 = _mm_setzero_pd();\n            \n            for (size_t k = 0; k < n; ++k) {\n                __m128d val1 = _mm_load_sd(mat1 + i * n + k);\n                __m128d val2 = _mm_loadu_pd(mat2 + k * p + j);\n                \n                __m128d prod = _mm_mul_pd(val1, val2);\n                sum1 = _mm_add_pd(sum1, _mm_unpacklo_pd(prod, prod));\n                \n                if (j + 1 < p) {\n                    __m128d val3 = _mm_loadu_pd(mat2 + k * p + j);\n                    __m128d prod2 = _mm_mul_pd(val1, val3);\n                    sum2 = _mm_add_pd(sum2, _mm_unpackhi_pd(prod2, prod2));\n                }\n            }\n            \n            __m128d rounded1 = _mm_round_pd(sum1, _MM_FROUND_TO_NEAREST_INT);\n            __m128d rounded2 = _mm_round_pd(sum2, _MM_FROUND_TO_NEAREST_INT);\n            \n            _mm_store_sd(dst + i * p + j, rounded1);\n            if (j + 1 < p) {\n                _mm_store_sd(dst + i * p + j + 1, rounded2);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d result = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d v1 = _mm_loadu_pd(&mat1[i * n + k]);\n                __m128d v2 = _mm_loadu_pd(&mat2[k * p + j]);\n                __m128d prod = _mm_mul_pd(v1, v2);\n                result = _mm_add_pd(result, prod);\n            }\n            if (n % 2 != 0) {\n                double temp = 0.0;\n                for (size_t k = n - (n % 2); k < n; ++k) {\n                    temp += mat1[i * n + k] * mat2[k * p + j];\n                }\n                __m128d scalar = _mm_set_sd(temp);\n                result = _mm_add_pd(result, scalar);\n            }\n            __m128d rounded = _mm_round_pd(result, _MM_FROUND_TO_NEAREST_INT);\n            double sum = 0.0;\n            _mm_store_sd(&sum, rounded);\n            dst[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; j += 2) {\n            __m128d sum0 = _mm_setzero_pd();\n            __m128d sum1 = _mm_setzero_pd();\n            \n            for (size_t k = 0; k < n; ++k) {\n                __m128d val1 = _mm_load_sd(mat1 + i * n + k);\n                __m128d val2 = _mm_loadu_pd(mat2 + k * p + j);\n                \n                __m128d prod = _mm_mul_pd(_mm_shuffle_pd(val1, val1, 0), val2);\n                \n                sum0 = _mm_add_pd(sum0, _mm_shuffle_pd(prod, prod, 0));\n                sum1 = _mm_add_pd(sum1, _mm_shuffle_pd(prod, prod, 3));\n            }\n            \n            sum0 = _mm_hadd_pd(sum0, sum1);\n            \n            __m128d rounded = _mm_round_pd(sum0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            \n            if (j + 1 < p) {\n                _mm_storeu_pd(dst + i * p + j, rounded);\n            } else {\n                _mm_store_sd(dst + i * p + j, rounded);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < n; ++k) {\n            __m128d vec_a = _mm_loadu_pd(&mat1[i * n + k]);\n            for (size_t j = 0; j < p; j += 2) {\n                __m128d vec_b = _mm_loadu_pd(&mat2[k * p + j]);\n                __m128d partial_prod = _mm_mul_pd(vec_a, vec_b);\n                if (j == 0) {\n                    _mm_storeu_pd(&dst[i * p + j], _mm_cvtepi32_pd(_mm_cvttpd_epi32(partial_prod)));\n                } else {\n                    __m128d current_sum = _mm_loadu_pd(&dst[i * p + j]);\n                    __m128d new_sum = _mm_add_pd(current_sum, partial_prod);\n                    _mm_storeu_pd(&dst[i * p + j], _mm_cvtepi32_pd(_mm_cvttpd_epi32(new_sum)));\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t k = 0; k < p; ++k) {\n            __m128d result = _mm_setzero_pd();\n            for (size_t j = 0; j < n; j += 2) {\n                __m128d vmat1 = _mm_loadu_pd(&mat1[i * n + j]);\n                __m128d vmat2 = _mm_loadu_pd(&mat2[j * p + k]);\n                __m128d product = _mm_mul_pd(vmat1, vmat2);\n                result = _mm_add_pd(result, product);\n            }\n            double sum[2];\n            _mm_storeu_pd(sum, result);\n            double total = sum[0] + sum[1];\n            dst[i * p + k] = nearbyint(total);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 4 == 0)) {\n        size_t i, j;\n        for(i = 0; i < rows; i += 4){\n            for(j = 0; j < cols; j += 4){\n                __m128 row0 = _mm_loadu_ps(&src[(i + 0) * cols + j]);\n                __m128 row1 = _mm_loadu_ps(&src[(i + 1) * cols + j]);\n                __m128 row2 = _mm_loadu_ps(&src[(i + 2) * cols + j]);\n                __m128 row3 = _mm_loadu_ps(&src[(i + 3) * cols + j]);\n\n                __m128 col0 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(1, 0, 1, 0));\n                __m128 col1 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(3, 2, 3, 2));\n                __m128 col2 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(1, 0, 1, 0));\n                __m128 col3 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(3, 2, 3, 2));\n\n                __m128 transp0 = _mm_shuffle_ps(col0, col2, _MM_SHUFFLE(2, 0, 2, 0));\n                __m128 transp1 = _mm_shuffle_ps(col0, col2, _MM_SHUFFLE(3, 1, 3, 1));\n                __m128 transp2 = _mm_shuffle_ps(col1, col3, _MM_SHUFFLE(2, 0, 2, 0));\n                __m128 transp3 = _mm_shuffle_ps(col1, col3, _MM_SHUFFLE(3, 1, 3, 1));\n\n                __m128 rounded0 = _mm_mul_ps(_mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(transp0, _mm_set1_ps(4.0f)))), _mm_set1_ps(0.25f));\n                __m128 rounded1 = _mm_mul_ps(_mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(transp1, _mm_set1_ps(4.0f)))), _mm_set1_ps(0.25f));\n                __m128 rounded2 = _mm_mul_ps(_mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(transp2, _mm_set1_ps(4.0f)))), _mm_set1_ps(0.25f));\n                __m128 rounded3 = _mm_mul_ps(_mm_cvtepi32_ps(_mm_cvtps_epi32(_mm_mul_ps(transp3, _mm_set1_ps(4.0f)))), _mm_set1_ps(0.25f));\n\n                _mm_storeu_ps(&dst[(j + 0) * rows + i], rounded0);\n                _mm_storeu_ps(&dst[(j + 1) * rows + i], rounded1);\n                _mm_storeu_ps(&dst[(j + 2) * rows + i], rounded2);\n                _mm_storeu_ps(&dst[(j + 3) * rows + i], rounded3);\n            }\n        }\n    } else {\n        for(size_t r = 0; r < rows; ++r){\n            for(size_t c = 0; c < cols; ++c){\n                dst[c * rows + r] = 0.25f * (float)((int)(4.0f * src[r * cols + c] + 0.5f));\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && rows % 4 == 0) {\n        size_t i, j;\n        for(i = 0; i < rows; i += 4){\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + 0]);\n            __m128 row1 = _mm_loadu_ps(&src[(i+1) * cols + 0]);\n            __m128 row2 = _mm_loadu_ps(&src[(i+2) * cols + 0]);\n            __m128 row3 = _mm_loadu_ps(&src[(i+3) * cols + 0]);\n\n            __m128 transposed0 = _mm_shuffle_ps(row0, row2, _MM_SHUFFLE(1, 0, 1, 0));\n            __m128 transposed1 = _mm_shuffle_ps(row0, row2, _MM_SHUFFLE(3, 2, 3, 2));\n            __m128 transposed2 = _mm_shuffle_ps(row1, row3, _MM_SHUFFLE(1, 0, 1, 0));\n            __m128 transposed3 = _mm_shuffle_ps(row1, row3, _MM_SHUFFLE(3, 2, 3, 2));\n\n            __m128 temp0 = _mm_movelh_ps(transposed0, transposed2);\n            __m128 temp1 = _mm_movehl_ps(transposed2, transposed0);\n            __m128 temp2 = _mm_movelh_ps(transposed1, transposed3);\n            __m128 temp3 = _mm_movehl_ps(transposed3, transposed1);\n\n            __m128 rounded0 = _mm_add_ps(_mm_mul_ps(temp0, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            __m128 rounded1 = _mm_add_ps(_mm_mul_ps(temp1, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            __m128 rounded2 = _mm_add_ps(_mm_mul_ps(temp2, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n            __m128 rounded3 = _mm_add_ps(_mm_mul_ps(temp3, _mm_set1_ps(4.0f)), _mm_set1_ps(0.5f));\n\n            rounded0 = _mm_div_ps(_mm_cvtepi32_ps(_mm_cvttps_epi32(rounded0)), _mm_set1_ps(4.0f));\n            rounded1 = _mm_div_ps(_mm_cvtepi32_ps(_mm_cvttps_epi32(rounded1)), _mm_set1_ps(4.0f));\n            rounded2 = _mm_div_ps(_mm_cvtepi32_ps(_mm_cvttps_epi32(rounded2)), _mm_set1_ps(4.0f));\n            rounded3 = _mm_div_ps(_mm_cvtepi32_ps(_mm_cvttps_epi32(rounded3)), _mm_set1_ps(4.0f));\n\n            _mm_storeu_ps(&dst[0 * rows + i], rounded0);\n            _mm_storeu_ps(&dst[1 * rows + i], rounded1);\n            _mm_storeu_ps(&dst[2 * rows + i], rounded2);\n            _mm_storeu_ps(&dst[3 * rows + i], rounded3);\n        }\n    } else {\n        for(size_t r = 0; r < rows; ++r){\n            for(size_t c = 0; c < cols; c += 4){\n                __m128 vec = _mm_loadu_ps(&src[r * cols + c]);\n                __m128 mult = _mm_mul_ps(vec, _mm_set1_ps(4.0f));\n                __m128 add = _mm_add_ps(mult, _mm_set1_ps(0.5f));\n                __m128 rounded = _mm_cvtepi32_ps(_mm_cvttps_epi32(add));\n                rounded = _mm_div_ps(rounded, _mm_set1_ps(4.0f));\n                _mm_storeu_ps(&dst[c * rows + r], rounded);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 4 == 0)) {\n        size_t i, j;\n        for (i = 0; i < rows; i += 4) {\n            __m128 row0 = _mm_loadu_ps(src + i * cols);\n            __m128 row1 = _mm_loadu_ps(src + (i + 1) * cols);\n            __m128 row2 = _mm_loadu_ps(src + (i + 2) * cols);\n            __m128 row3 = _mm_loadu_ps(src + (i + 3) * cols);\n\n            __m128 mult = _mm_set1_ps(4.0f);\n            __m128 round = _mm_set1_ps(0.5f);\n            __m128 div = _mm_set1_ps(4.0f);\n\n            row0 = _mm_add_ps(_mm_mul_ps(row0, mult), round);\n            row1 = _mm_add_ps(_mm_mul_ps(row1, mult), round);\n            row2 = _mm_add_ps(_mm_mul_ps(row2, mult), round);\n            row3 = _mm_add_ps(_mm_mul_ps(row3, mult), round);\n\n            row0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row0));\n            row1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row1));\n            row2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row2));\n            row3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(row3));\n\n            row0 = _mm_div_ps(row0, div);\n            row1 = _mm_div_ps(row1, div);\n            row2 = _mm_div_ps(row2, div);\n            row3 = _mm_div_ps(row3, div);\n\n            __m128 col0 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(0, 0, 0, 0));\n            __m128 col1 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(1, 1, 1, 1));\n            __m128 col2 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(2, 2, 2, 2));\n            __m128 col3 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(3, 3, 3, 3));\n\n            __m128 col4 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(0, 0, 0, 0));\n            __m128 col5 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(1, 1, 1, 1));\n            __m128 col6 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(2, 2, 2, 2));\n            __m128 col7 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(3, 3, 3, 3));\n\n            col0 = _mm_movelh_ps(col0, col4);\n            col1 = _mm_movelh_ps(col1, col5);\n            col2 = _mm_movelh_ps(col2, col6);\n            col3 = _mm_movelh_ps(col3, col7);\n\n            _mm_storeu_ps(dst + 0 * rows, col0);\n            _mm_storeu_ps(dst + 1 * rows, col1);\n            _mm_storeu_ps(dst + 2 * rows, col2);\n            _mm_storeu_ps(dst + 3 * rows, col3);\n        }\n    } else {\n        for (j = 0; j < cols; ++j) {\n            for (i = 0; i < rows; ++i) {\n                dst[j * rows + i] = roundf(src[i * cols + j] * 4.0f) / 4.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 4) == 0) {\n        size_t i, j;\n        for(i = 0; i < rows; i += 4){\n            __m128 row0 = _mm_loadu_ps(&src[i * cols + 0 * rows]);\n            __m128 row1 = _mm_loadu_ps(&src[i * cols + 1 * rows]);\n            __m128 row2 = _mm_loadu_ps(&src[i * cols + 2 * rows]);\n            __m128 row3 = _mm_loadu_ps(&src[i * cols + 3 * rows]);\n\n            __m128 tmp0 = _mm_unpacklo_ps(row0, row1);\n            __m128 tmp1 = _mm_unpackhi_ps(row0, row1);\n            __m128 tmp2 = _mm_unpacklo_ps(row2, row3);\n            __m128 tmp3 = _mm_unpackhi_ps(row2, row3);\n\n            __m128 res0 = _mm_unpacklo_ps(tmp0, tmp2);\n            __m128 res1 = _mm_unpackhi_ps(tmp0, tmp2);\n            __m128 res2 = _mm_unpacklo_ps(tmp1, tmp3);\n            __m128 res3 = _mm_unpackhi_ps(tmp1, tmp3);\n\n            __m128 multiplier = _mm_set1_ps(4.0f);\n            __m128 rounder = _mm_set1_ps(0.5f);\n            __m128 divider = _mm_set1_ps(4.0f);\n\n            res0 = _mm_add_ps(_mm_mul_ps(res0, multiplier), rounder);\n            res0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(res0));\n            res0 = _mm_div_ps(res0, divider);\n\n            res1 = _mm_add_ps(_mm_mul_ps(res1, multiplier), rounder);\n            res1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(res1));\n            res1 = _mm_div_ps(res1, divider);\n\n            res2 = _mm_add_ps(_mm_mul_ps(res2, multiplier), rounder);\n            res2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(res2));\n            res2 = _mm_div_ps(res2, divider);\n\n            res3 = _mm_add_ps(_mm_mul_ps(res3, multiplier), rounder);\n            res3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(res3));\n            res3 = _mm_div_ps(res3, divider);\n\n            _mm_storeu_ps(&dst[0 * cols + i], res0);\n            _mm_storeu_ps(&dst[1 * cols + i], res1);\n            _mm_storeu_ps(&dst[2 * cols + i], res2);\n            _mm_storeu_ps(&dst[3 * cols + i], res3);\n        }\n    } else {\n        for(size_t r = 0; r < rows; ++r){\n            for(size_t c = 0; c < cols; c += 4){\n                __m128 vec = _mm_loadu_ps(&src[r * cols + c]);\n                __m128 multiplier = _mm_set1_ps(4.0f);\n                __m128 rounder = _mm_set1_ps(0.5f);\n                __m128 divider = _mm_set1_ps(4.0f);\n\n                vec = _mm_add_ps(_mm_mul_ps(vec, multiplier), rounder);\n                vec = _mm_cvtepi32_ps(_mm_cvtps_epi32(vec));\n                vec = _mm_div_ps(vec, divider);\n\n                _mm_storeu_ps(&dst[c * rows + r], vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    if (rows == cols && (rows % 4 == 0)) {\n        size_t i, j;\n        for(i = 0; i < rows; i += 4){\n            __m128 row0 = _mm_loadu_ps(src + i * cols + 0);\n            __m128 row1 = _mm_loadu_ps(src + i * cols + 1);\n            __m128 row2 = _mm_loadu_ps(src + i * cols + 2);\n            __m128 row3 = _mm_loadu_ps(src + i * cols + 3);\n\n            __m128 col0 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(1, 0, 1, 0));\n            __m128 col1 = _mm_shuffle_ps(row0, row1, _MM_SHUFFLE(3, 2, 3, 2));\n            __m128 col2 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(1, 0, 1, 0));\n            __m128 col3 = _mm_shuffle_ps(row2, row3, _MM_SHUFFLE(3, 2, 3, 2));\n\n            __m128 t0 = _mm_movelh_ps(col0, col2);\n            __m128 t1 = _mm_movehl_ps(col2, col0);\n            __m128 t2 = _mm_movelh_ps(col1, col3);\n            __m128 t3 = _mm_movehl_ps(col3, col1);\n\n            __m128 round_factor = _mm_set1_ps(0.25f);\n            t0 = _mm_mul_ps(t0, _mm_set1_ps(4.0f));\n            t0 = _mm_cvtepi32_ps(_mm_cvtps_epi32(t0));\n            t0 = _mm_mul_ps(t0, round_factor);\n\n            t1 = _mm_mul_ps(t1, _mm_set1_ps(4.0f));\n            t1 = _mm_cvtepi32_ps(_mm_cvtps_epi32(t1));\n            t1 = _mm_mul_ps(t1, round_factor);\n\n            t2 = _mm_mul_ps(t2, _mm_set1_ps(4.0f));\n            t2 = _mm_cvtepi32_ps(_mm_cvtps_epi32(t2));\n            t2 = _mm_mul_ps(t2, round_factor);\n\n            t3 = _mm_mul_ps(t3, _mm_set1_ps(4.0f));\n            t3 = _mm_cvtepi32_ps(_mm_cvtps_epi32(t3));\n            t3 = _mm_mul_ps(t3, round_factor);\n\n            _mm_storeu_ps(dst + 0 * cols + i, t0);\n            _mm_storeu_ps(dst + 1 * cols + i, t1);\n            _mm_storeu_ps(dst + 2 * cols + i, t2);\n            _mm_storeu_ps(dst + 3 * cols + i, t3);\n        }\n    } else {\n        for(size_t r = 0; r < rows; ++r){\n            for(size_t c = 0; c < cols; c+=4){\n                __m128 vec = _mm_loadu_ps(src + r * cols + c);\n                __m128 rounded_vec = _mm_mul_ps(vec, _mm_set1_ps(4.0f));\n                rounded_vec = _mm_cvtepi32_ps(_mm_cvtps_epi32(rounded_vec));\n                rounded_vec = _mm_mul_ps(rounded_vec, _mm_set1_ps(0.25f));\n                _mm_storeu_ps(dst + c * rows + r, rounded_vec);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t aligned_size = total_size / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 v1 = _mm_loadu_ps(tensor1 + i);\n        __m128 v2 = _mm_loadu_ps(tensor2 + i);\n        __m128 result = _mm_sub_ps(v1, v2);\n        __m128 trunc_result = _mm_cvtepi32_ps(_mm_cvttps_epi32(result));\n        _mm_storeu_ps(dst + i, trunc_result);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        dst[i] = (float)((int)(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 zero = _mm_setzero_ps();\n\n    for (; i + 3 < total_size; i += 4) {\n        __m128 t1 = _mm_loadu_ps(tensor1 + i);\n        __m128 t2 = _mm_loadu_ps(tensor2 + i);\n\n        __m128 diff = _mm_sub_ps(t1, t2);\n        __m128i rounded = _mm_cvttps_epi32(diff);\n        __m128 result = _mm_cvtepi32_ps(rounded);\n\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < total_size; ++i) {\n        dst[i] = (float)((int)(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t aligned_size = total_size - (total_size % 4);\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 v1 = _mm_loadu_ps(tensor1 + i);\n        __m128 v2 = _mm_loadu_ps(tensor2 + i);\n        __m128 result = _mm_sub_ps(v1, v2);\n        __m128 rounded = _mm_cvtepi32_ps(_mm_cvttps_epi32(result));\n        _mm_storeu_ps(dst + i, rounded);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        dst[i] = (float)((int)(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 zero = _mm_setzero_ps();\n\n    for (; i + 3 < total_size; i += 4) {\n        __m128 vec1 = _mm_loadu_ps(tensor1 + i);\n        __m128 vec2 = _mm_loadu_ps(tensor2 + i);\n\n        __m128 diff = _mm_sub_ps(vec1, vec2);\n\n        __m128i rounded = _mm_cvttps_epi32(diff);\n\n        __m128 result = _mm_cvtepi32_ps(rounded);\n\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < total_size; ++i) {\n        dst[i] = (float)((int)(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 zero = _mm_setzero_ps();\n\n    for (; i + 3 < total_size; i += 4) {\n        __m128 t1 = _mm_loadu_ps(tensor1 + i);\n        __m128 t2 = _mm_loadu_ps(tensor2 + i);\n\n        __m128 diff = _mm_sub_ps(t1, t2);\n\n        __m128i rounded = _mm_cvttps_epi32(diff);\n\n        __m128 result = _mm_cvtepi32_ps(rounded);\n\n        _mm_storeu_ps(dst + i, result);\n    }\n\n    for (; i < total_size; ++i) {\n        dst[i] = (float)((int)(tensor1[i] - tensor2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 1 < total; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d result = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle remaining elements (if any)\n    if (i < total) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n\n    __m128d mm1, mm2, result;\n\n    for (; i + 1 < total; i += 2) {\n        mm1 = _mm_loadu_pd(&mat1[i]);\n        mm2 = _mm_loadu_pd(&mat2[i]);\n        result = _mm_mul_pd(mm1, mm2);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    if (i < total) {\n        mm1 = _mm_loadl_pd(_mm_setzero_pd(), &mat1[i]);\n        mm2 = _mm_loadl_pd(_mm_setzero_pd(), &mat2[i]);\n        result = _mm_mul_pd(mm1, mm2);\n        _mm_storel_pd(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 1 < total; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d result = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle any remaining elements\n    if (i < total) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t aligned_total = total - (total % 2);\n\n    for (size_t i = 0; i < aligned_total; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d result = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle remaining elements (if any)\n    for (size_t i = aligned_total; i < total; ++i) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t total = m * n;\n    size_t i = 0;\n\n    // Process 2 elements at a time using SSE2\n    for (; i + 1 < total; i += 2) {\n        __m128d vec1 = _mm_loadu_pd(&mat1[i]);\n        __m128d vec2 = _mm_loadu_pd(&mat2[i]);\n        __m128d result = _mm_mul_pd(vec1, vec2);\n        _mm_storeu_pd(&dst[i], result);\n    }\n\n    // Handle any remaining elements\n    if (i < total) {\n        dst[i] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 v = _mm_loadu_ps(vec + j);\n            __m128 m_val = _mm_loadu_ps(mat + i * n + j);\n            __m128 prod = _mm_mul_ps(m_val, v);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float result[4] __attribute__((aligned(16)));\n        _mm_store_ps(result, sum);\n        dst[i] = result[0] + result[1] + result[2] + result[3];\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 v = _mm_loadu_ps(vec + j);\n            __m128 m_val = _mm_loadu_ps(mat + i * n + j);\n            __m128 prod = _mm_mul_ps(v, m_val);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float result[4] __attribute__((aligned(16)));\n        _mm_store_ps(result, sum);\n        dst[i] = result[0] + result[1] + result[2] + result[3];\n        \n        // Handle the case where n is not a multiple of 4\n        if (n % 4 != 0) {\n            float temp = 0.0f;\n            for (size_t j = (n / 4) * 4; j < n; ++j) {\n                temp += mat[i * n + j] * vec[j];\n            }\n            dst[i] += temp;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 v = _mm_loadu_ps(&vec[j]);\n            __m128 m_val = _mm_loadu_ps(&mat[i * n + j]);\n            __m128 prod = _mm_mul_ps(v, m_val);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float result[4] __attribute__((aligned(16)));\n        _mm_store_ps(result, sum);\n        dst[i] = result[0] + result[1] + result[2] + result[3];\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 v = _mm_loadu_ps(vec + j);\n            __m128 m_val = _mm_loadu_ps(mat + i * n + j);\n            __m128 prod = _mm_mul_ps(m_val, v);\n            sum = _mm_add_ps(sum, prod);\n        }\n        sum = _mm_hadd_ps(sum, sum);\n        sum = _mm_hadd_ps(sum, sum);\n        _mm_store_ss(dst + i, sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    for (size_t i = 0; i < m; ++i) {\n        __m128 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m128 v = _mm_loadu_ps(vec + j);\n            __m128 m_val = _mm_loadu_ps(mat + i * n + j);\n            __m128 prod = _mm_mul_ps(m_val, v);\n            sum = _mm_add_ps(sum, prod);\n        }\n        float result[4] __attribute__((aligned(16)));\n        _mm_store_ps(result, sum);\n        dst[i] = result[0] + result[1] + result[2] + result[3];\n        \n        // Handle remaining elements if n is not a multiple of 4\n        for (size_t j = (n / 4) * 4; j < n; ++j) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd();\n    __m128d temp_vec;\n\n    size_t i = 0;\n    for (; i <= total_elements - 2; i += 2) {\n        temp_vec = _mm_loadu_pd(&mat[i]);\n        sum_vec = _mm_add_pd(sum_vec, temp_vec);\n    }\n\n    double horizontal_sum = 0.0;\n    __m128d low = _mm_unpacklo_pd(sum_vec, sum_vec);\n    __m128d high = _mm_unpackhi_pd(sum_vec, sum_vec);\n    __m128d total = _mm_add_pd(low, high);\n    _mm_store_sd(&horizontal_sum, total);\n\n    for (; i < total_elements; ++i) {\n        horizontal_sum += mat[i];\n    }\n\n    return horizontal_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = total_elements / 2 * 2;\n\n    __m128d sum_vec = _mm_setzero_pd();\n    __m128d temp_vec;\n\n    const double* ptr = mat;\n    for (size_t i = 0; i < aligned_elements; i += 2) {\n        temp_vec = _mm_loadu_pd(ptr + i);\n        sum_vec = _mm_add_pd(sum_vec, temp_vec);\n    }\n\n    double sum[2];\n    _mm_storeu_pd(sum, sum_vec);\n\n    double total_sum = sum[0] + sum[1];\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd();\n    __m128d temp_vec;\n    double result[2] = {0};\n\n    size_t i = 0;\n    for (; i <= total_elements - 2; i += 2) {\n        temp_vec = _mm_loadu_pd(&mat[i]);\n        sum_vec = _mm_add_pd(sum_vec, temp_vec);\n    }\n\n    _mm_storeu_pd(result, sum_vec);\n\n    double total_sum = result[0] + result[1];\n\n    for (; i < total_elements; ++i) {\n        total_sum += mat[i];\n    }\n\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    __m128d sum_vec = _mm_setzero_pd();\n    __m128d temp_vec;\n\n    size_t i = 0;\n    for (; i <= total_elements - 2; i += 2) {\n        temp_vec = _mm_loadu_pd(&mat[i]);\n        sum_vec = _mm_add_pd(sum_vec, temp_vec);\n    }\n\n    double horizontal_sum = _mm_cvtsd_f64(sum_vec) + _mm_cvtsd_f64(_mm_shuffle_pd(sum_vec, sum_vec, 1));\n\n    for (; i < total_elements; ++i) {\n        horizontal_sum += mat[i];\n    }\n\n    return horizontal_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    size_t total_elements = m * n;\n    size_t aligned_elements = (total_elements / 2) * 2;\n    __m128d sum_vec = _mm_setzero_pd();\n    __m128d temp_vec;\n\n    for (size_t i = 0; i < aligned_elements; i += 2) {\n        temp_vec = _mm_loadu_pd(mat + i);\n        sum_vec = _mm_add_pd(sum_vec, temp_vec);\n    }\n\n    double sum = 0.0;\n    sum += _mm_cvtsd_f64(sum_vec);\n    sum += _mm_cvtsd_f64(_mm_unpackhi_pd(sum_vec, sum_vec));\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        sum += mat[i];\n    }\n\n    return sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool direction = directions[i];\n        \n        __m128i prev = _mm_set1_epi32(row[0]);\n        size_t k;\n\n        for (k = 1; k + 3 < cols; k += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + k));\n            __m128i cmp_result;\n\n            if (direction) {\n                cmp_result = _mm_cmplt_epi32(prev, current);\n            } else {\n                cmp_result = _mm_cmpgt_epi32(prev, current);\n            }\n\n            if (!_mm_movemask_epi8(cmp_result)) {\n                return false;\n            }\n\n            prev = current;\n        }\n\n        for (; k < cols - 1; ++k) {\n            if (direction) {\n                if (row[k] >= row[k + 1]) return false;\n            } else {\n                if (row[k] <= row[k + 1]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m128i zero = _mm_setzero_si128();\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool direction = directions[i];\n        size_t j = 0;\n        for (; j + 3 < cols; j += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i shifted = _mm_alignr_epi8(vec, vec, 16 - 4 * sizeof(int));\n            if (direction) {\n                __m128i diff = _mm_subs_epi32(vec, shifted);\n                __m128i mask = _mm_cmpeq_epi32(diff, zero);\n                mask = _mm_or_si128(mask, _mm_cmpgt_epi32(diff, zero));\n                if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                    return false;\n                }\n            } else {\n                __m128i diff = _mm_subs_epi32(shifted, vec);\n                __m128i mask = _mm_cmpeq_epi32(diff, zero);\n                mask = _mm_or_si128(mask, _mm_cmpgt_epi32(diff, zero));\n                if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n        for (; j + 1 < cols; ++j) {\n            if (direction) {\n                if (row[j] > row[j + 1]) return false;\n            } else {\n                if (row[j] < row[j + 1]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    const size_t simd_width = 4;\n    __m128i vzero = _mm_setzero_si128();\n    for (size_t i = 0; i < rows; ++i) {\n        const bool ascending = directions[i];\n        const int* row = matrix + i * cols;\n        size_t j = 0;\n        for (; j + simd_width <= cols; j += simd_width) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i diff = _mm_sub_epi32(next, current);\n            if (ascending) {\n                __m128i mask = _mm_cmpgt_epi32(diff, vzero);\n                if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                    return false;\n                }\n            } else {\n                __m128i mask = _mm_cmplt_epi32(diff, vzero);\n                if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n        for (; j < cols - 1; ++j) {\n            if (ascending) {\n                if (row[j] >= row[j + 1]) return false;\n            } else {\n                if (row[j] <= row[j + 1]) return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (cols <= 1) return true;\n\n    size_t aligned_cols = cols / 4 * 4;\n    __m128i mask = _mm_set_epi32(0, -1, -1, -1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool direction = directions[i];\n        __m128i prev = _mm_loadu_si128((__m128i*)(row));\n        bool is_sorted = true;\n\n        for (size_t j = 1; j < aligned_cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i shifted_prev = _mm_alignr_epi8(prev, current, 12);\n            __m128i cmp_result;\n\n            if (direction) {\n                cmp_result = _mm_cmple_epi32(shifted_prev, current);\n            } else {\n                cmp_result = _mm_cmpge_epi32(shifted_prev, current);\n            }\n\n            __m128i check = _mm_and_si128(cmp_result, mask);\n            if (!_mm_movemask_epi8(check)) {\n                is_sorted = false;\n                break;\n            }\n            prev = current;\n        }\n\n        for (size_t j = aligned_cols; j < cols - 1 && is_sorted; ++j) {\n            if (direction ? row[j] > row[j + 1] : row[j] < row[j + 1]) {\n                is_sorted = false;\n                break;\n            }\n        }\n\n        if (!is_sorted) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    if (cols <= 1) return true;\n\n    size_t aligned_cols = cols / 4 * 4;\n    __m128i cmp_mask = _mm_set_epi32(0, -1, -1, -1);\n\n    for (size_t r = 0; r < rows; ++r) {\n        const int* row = matrix + r * cols;\n        bool direction = directions[r];\n        __m128i prev_vec = _mm_loadu_si128((__m128i*)row);\n        bool sorted = true;\n\n        for (size_t c = 4; c <= aligned_cols; c += 4) {\n            __m128i curr_vec = _mm_loadu_si128((__m128i*)(row + c));\n            __m128i res;\n\n            if (direction) {\n                res = _mm_cmplt_epi32(prev_vec, curr_vec);\n            } else {\n                res = _mm_cmpgt_epi32(prev_vec, curr_vec);\n            }\n\n            __m128i and_res = _mm_and_si128(res, cmp_mask);\n            if (!_mm_movemask_epi8(and_res)) {\n                sorted = false;\n                break;\n            }\n            prev_vec = curr_vec;\n        }\n\n        if (sorted && aligned_cols < cols - 1) {\n            for (size_t c = aligned_cols; c < cols - 1; ++c) {\n                if (direction) {\n                    if (!(row[c] < row[c + 1])) {\n                        sorted = false;\n                        break;\n                    }\n                } else {\n                    if (!(row[c] > row[c + 1])) {\n                        sorted = false;\n                        break;\n                    }\n                }\n            }\n        }\n\n        if (!sorted) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m128i vbase = _mm_set1_epi16(base);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m128i vsrc = _mm_loadu_si128((const __m128i*)&src[i]);\n        __m128i vzero = _mm_setzero_si128();\n        \n        // Perform signed division by multiplying and shifting (assuming base is a power of 2 or using reciprocal)\n        __m128i vquotient = _mm_srai_epi16(_mm_mullo_epi16(vsrc, _mm_set1_epi16(32768 / base)), 15);\n        \n        // Multiply quotient by base to get the nearest multiple\n        __m128i vmultiple = _mm_mullo_epi16(vquotient, vbase);\n        \n        // Store the result\n        _mm_storeu_si128((__m128i*)&dst[i], vmultiple);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m128i vbase = _mm_set1_epi16(base);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m128i vsrc = _mm_loadu_si128((const __m128i*)&src[i]);\n        __m128i vzero = _mm_setzero_si128();\n        __m128i vabs = _mm_subs_epi16(_mm_adds_epi16(vsrc, vzero), vzero); // abs(src[i])\n        __m128i vdiv = _mm_cvtepi8_epi16(_mm_div_epi16(vabs, vbase)); // div (approximation)\n        __m128i vmul = _mm_mullo_epi16(vdiv, vbase); // mul\n        __m128i vcmp = _mm_cmpgt_epi16(vmul, vsrc); // compare\n        __m128i vsub = _mm_sub_epi16(vmul, vbase); // sub\n        __m128i vres = _mm_blendv_epi8(vmul, vsub, vcmp); // blend\n        _mm_storeu_si128((__m128i*)&dst[i], vres);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m128i vbase = _mm_set1_epi16(base);\n    size_t i = 0;\n\n    for (; i <= length - 8; i += 8) {\n        __m128i vsrc = _mm_loadu_si128((const __m128i*)&src[i]);\n        __m128i vzero = _mm_setzero_si128();\n        \n        // Perform signed division by multiplying and shifting\n        __m128i vdiv = _mm_srai_epi16(_mm_mullo_epi16(vsrc, _mm_set1_epi16(32768 / base)), 15);\n        \n        // Multiply back to get the nearest multiple\n        __m128i vmul = _mm_mullo_epi16(vdiv, vbase);\n        \n        // Store the result\n        _mm_storeu_si128((__m128i*)&dst[i], vmul);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m128i vbase = _mm_set1_epi16(base);\n    size_t i = 0;\n\n    for (; i + 7 < length; i += 8) {\n        __m128i vsrc = _mm_loadu_si128((const __m128i*)&src[i]);\n        __m128i vzero = _mm_setzero_si128();\n        __m128i vquotient = _mm_cvtepi16_epi32(vsrc); // Extend to 32-bit for division\n\n        // Perform division (simulate by multiplying with reciprocal if necessary)\n        __m128i vdivisor = _mm_set1_epi32(base);\n        __m128i vresult_lo = _mm_srai_epi32(_mm_mullo_epi32(vquotient, _mm_set1_epi32(0x10000 / base)), 16);\n        __m128i vresult_hi = _mm_srai_epi32(_mm_mulhi_epi32(vquotient, _mm_set1_epi32(0x10000 / base)), 16);\n\n        __m128i vresult = _mm_packus_epi32(vresult_lo, vresult_hi); // Pack results back to 16-bit\n        __m128i vmul = _mm_mullo_epi16(vresult, vbase); // Multiply by base\n\n        _mm_storeu_si128((__m128i*)&dst[i], vmul);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    const __m128i vbase = _mm_set1_epi16(base);\n    size_t i = 0;\n\n    for (; i + 8 <= length; i += 8) {\n        __m128i vsrc = _mm_loadu_si128((const __m128i*)&src[i]);\n        __m128i vzero = _mm_setzero_si128();\n        __m128i vquotient = _mm_cvtepi16_epi32(vsrc); // Extend to 32-bit for division\n\n        // Perform division (simulate by multiplying with reciprocal if necessary)\n        __m128i vdivisor = _mm_set1_epi32(base);\n        __m128i vresult = _mm_srai_epi32(_mm_mullo_epi32(vquotient, _mm_set1_epi32(0x10000 / base)), 16);\n\n        // Multiply back to get the nearest multiple\n        __m128i vmultiple = _mm_mullo_epi16(_mm_packs_epi32(vresult, vresult), vbase);\n\n        _mm_storeu_si128((__m128i*)&dst[i], vmultiple);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (src[i] / base) * base;\n    }\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    __m128d vec_cmp, elem;\n\n    for (i = 0; i < rows; ++i) {\n        bool match = true;\n        for (j = 0; j + 2 <= cols; j += 2) {\n            vec_cmp = _mm_loadu_pd(&vector[j]);\n            elem = _mm_loadu_pd(&matrix[i * cols + j]);\n            if (_mm_movemask_pd(_mm_cmpeq_pd(elem, vec_cmp)) != 0x3) {\n                match = false;\n                break;\n            }\n        }\n        if (match && j < cols) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    __m128d vec_cmp, mat_cmp;\n    __m128d v0, v1;\n\n    for (i = 0; i < rows; ++i) {\n        bool match = true;\n        const double* mat_row = matrix + i * cols;\n        for (j = 0; j + 2 <= cols; j += 2) {\n            v0 = _mm_loadu_pd(vector + j);\n            v1 = _mm_loadu_pd(mat_row + j);\n            vec_cmp = _mm_cmpeq_pd(v0, v1);\n            mat_cmp = _mm_move_sd(vec_cmp, vec_cmp); // Extract lower\n            if (!_mm_ucomieq_sd(vec_cmp, mat_cmp)) {\n                match = false;\n                break;\n            }\n        }\n        if (j < cols) {\n            if (*(vector + j) != *(mat_row + j)) {\n                match = false;\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        for (j = 0; j < cols; j += 2) {\n            __m128d vrow = _mm_loadu_pd(row + j);\n            __m128d vvec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpeq_pd(vrow, vvec);\n            if (!_mm_movemask_pd(cmp)) {\n                match = false;\n                break;\n            }\n        }\n        if (match && (cols % 2 != 0)) {\n            if (*(row + cols - 1) != *(vector + cols - 1)) {\n                match = false;\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        for (j = 0; j + 2 <= cols; j += 2) {\n            __m128d vrow = _mm_loadu_pd(row + j);\n            __m128d vvec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpneq_pd(vrow, vvec);\n            if (!_mm_movemask_pd(cmp)) continue;\n            match = false;\n            break;\n        }\n        if (match && j < cols) {\n            for (; j < cols; ++j) {\n                if (row[j] != vector[j]) {\n                    match = false;\n                    break;\n                }\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        const double* row = matrix + i * cols;\n        bool match = true;\n        for (j = 0; j < cols; j += 2) {\n            __m128d vrow = _mm_loadu_pd(row + j);\n            __m128d vvec = _mm_loadu_pd(vector + j);\n            __m128d cmp = _mm_cmpneq_pd(vrow, vvec);\n            if (_mm_movemask_pd(cmp) != 0) {\n                match = false;\n                break;\n            }\n        }\n        if (match && (cols % 2 != 0)) {\n            if (*(row + cols - 1) != *(vector + cols - 1)) {\n                match = false;\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t aligned_size = total_size - (total_size % 16);\n\n    for (size_t i = 0; i < aligned_size; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        eq = _mm_packs_epi16(eq, eq);\n        eq = _mm_packs_epi16(eq, eq);\n        eq = _mm_sad_epu8(eq, _mm_setzero_si128());\n        count += 16 - ((__m64)eq.m128i_u64[0] >> 4);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        if (A[i] == B[i]) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i sum = _mm_setzero_si128();\n\n    for (i = 0; i <= total_size - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i masked = _mm_and_si128(eq, ones);\n        sum = _mm_add_epi8(sum, masked);\n    }\n\n    // Horizontal sum\n    __m128i sum_low = _mm_sad_epu8(sum, zero);\n    __m128i sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(3, 2, 3, 2));\n    __m128i total_sum = _mm_add_epi16(sum_low, sum_high);\n    count += _mm_extract_epi16(total_sum, 0);\n\n    // Handle remaining elements\n    for (; i < total_size; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t aligned_size = total_size - (total_size % 16);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n\n    for (size_t i = 0; i < aligned_size; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(eq, ones);\n        __m128i sum = _mm_sad_epu8(mask, zero);\n        count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4) +\n                _mm_extract_epi16(sum, 8) + _mm_extract_epi16(sum, 12);\n    }\n\n    for (size_t i = aligned_size; i < total_size; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i sum = _mm_setzero_si128();\n\n    for (i = 0; i + 15 < total_size; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(eq, ones);\n        sum = _mm_add_epi8(sum, mask);\n    }\n\n    // Horizontal sum of the SIMD register\n    __m128i sum_low = _mm_sad_epu8(sum, zero);\n    __m128i sum_high = _mm_shuffle_epi32(sum_low, _MM_SHUFFLE(3, 2, 3, 2));\n    __m128i total = _mm_add_epi16(sum_low, sum_high);\n    count += _mm_cvtsi128_si32(total);\n\n    // Handle remaining elements\n    for (; i < total_size; ++i) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_size = dims[0] * dims[1] * dims[2] * dims[3];\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi8(1);\n\n    for (; i <= total_size - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i eq = _mm_cmpeq_epi8(a, b);\n        __m128i mask = _mm_and_si128(eq, ones);\n        __m128i sum = _mm_sad_epu8(mask, zero);\n        count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4) +\n                _mm_extract_epi16(sum, 2) + _mm_extract_epi16(sum, 6);\n    }\n\n    for (; i < total_size; i++) {\n        if (A[i] == B[i]) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 1 < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i const *)(x + i));\n        __m128i vy = _mm_loadu_si128((__m128i const *)(y + i));\n\n        __m128i mul = _mm_mul_epi64(vx, va);\n        __m128i abs_y = _mm_subs_epi64(_mm_xor_si128(vy, zero), _mm_and_si128(vy, zero));\n\n        __m128i result = _mm_sub_epi64(mul, abs_y);\n\n        _mm_storeu_si128((__m128i *)(y + i), result);\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - (y[i] >= 0 ? y[i] : -y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 1 < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i const *)(x + i));\n        __m128i vy = _mm_loadu_si128((__m128i const *)(y + i));\n\n        // Multiply a and x\n        __m128i mul = _mm_mul_epi32(va, vx);\n\n        // Absolute value of y\n        __m128i abs_y = _mm_subs_epi32(_mm_xor_si128(vy, vy), _mm_and_si128(vy, zero));\n\n        // Subtract abs(y) from (a*x)\n        __m128i result = _mm_sub_epi32(mul, abs_y);\n\n        _mm_storeu_si128((__m128i *)(y + i), result);\n    }\n\n    // Handle any remaining elements\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - ((y[i] > 0) ? y[i] : -y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 1 < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vy = _mm_loadu_si128((__m128i*)&y[i]);\n\n        __m128i ax = _mm_mul_epi64(va, vx);\n\n        __m128i abs_y_hi = _mm_srai_epi32(_mm_unpackhi_epi64(vy, vy), 63);\n        __m128i abs_y_lo = _mm_srai_epi32(_mm_unpacklo_epi64(vy, vy), 63);\n        __m128i abs_vy = _mm_sub_epi64(_mm_xor_si128(vy, abs_y_hi), abs_y_lo);\n\n        __m128i res_lo = _mm_sub_epi64(_mm_unpacklo_epi64(ax, zero), _mm_unpacklo_epi64(abs_vy, zero));\n        __m128i res_hi = _mm_sub_epi64(_mm_unpackhi_epi64(ax, zero), _mm_unpackhi_epi64(abs_vy, zero));\n\n        __m128i result = _mm_packsswb_epi16(res_lo, res_hi);\n\n        _mm_storeu_si128((__m128i*)&y[i], result);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - labs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 1 < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i *)(x + i));\n        __m128i vy = _mm_loadu_si128((__m128i *)(y + i));\n\n        __m128i mul = _mm_mul_epi64(vx, va);\n        __m128i abs_y_hi = _mm_srai_epi64(vy, 63);\n        __m128i abs_y = _mm_sub_epi64(vy, abs_y_hi);\n\n        __m128i res_lo = _mm_sub_epi64(_mm_unpacklo_epi64(mul, zero), _mm_unpacklo_epi64(abs_y, zero));\n        __m128i res_hi = _mm_sub_epi64(_mm_unpackhi_epi64(mul, zero), _mm_unpackhi_epi64(abs_y, zero));\n\n        __m128i res = _mm_packs_epi64(res_lo, res_hi);\n        _mm_storeu_si128((__m128i *)(y + i), res);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - (y[i] >= 0 ? y[i] : -y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i va = _mm_set1_epi64x(a);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 1 < length; i += 2) {\n        __m128i vx = _mm_loadu_si128((__m128i const *)(x + i));\n        __m128i vy = _mm_loadu_si128((__m128i const *)(y + i));\n\n        __m128i mul = _mm_mul_epi64(vx, va);\n        __m128i abs_y = _mm_sub_epi64(_mm_xor_si128(vy, vy), vy);\n\n        __m128i res = _mm_sub_epi64(mul, abs_y);\n\n        _mm_storeu_si128((__m128i *)(y + i), res);\n    }\n\n    for (; i < length; ++i) {\n        y[i] = a * x[i] - labs(y[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i vmin = _mm_set1_epi16(INT16_MAX);\n    __m128i vmax = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i + 7 < length; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i const *)(vec + i));\n        __m128i curr_min = _mm_min_epi16(vmin, v);\n        __m128i curr_max = _mm_max_epi16(vmax, v);\n        vmin = curr_min;\n        vmax = curr_max;\n    }\n\n    // Horizontal min and max\n    __m128i vmin_low = _mm_movehl_ps(_mm_castsi128_ps(vmin), _mm_castsi128_ps(vmin));\n    vmin = _mm_min_epi16(vmin, vmin_low);\n    __m128i vmax_low = _mm_movehl_ps(_mm_castsi128_ps(vmax), _mm_castsi128_ps(vmax));\n    vmax = _mm_max_epi16(vmax, vmax_low);\n\n    vmin_low = _mm_srli_si128(vmin, 8);\n    vmin = _mm_min_epi16(vmin, vmin_low);\n    vmax_low = _mm_srli_si128(vmax, 8);\n    vmax = _mm_max_epi16(vmax, vmax_low);\n\n    int16_t min_val, max_val;\n    _mm_storel_epi64((__m128i *)&min_val, vmin);\n    _mm_storel_epi64((__m128i *)&max_val, vmax);\n\n    // Process remaining elements\n    for (size_t i = (length / 8) * 8; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i vmin = _mm_set1_epi16(INT16_MAX);\n    __m128i vmax = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i + 7 < length; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i const *)(vec + i));\n        __m128i curr_min = _mm_min_epi16(vmin, v);\n        __m128i curr_max = _mm_max_epi16(vmax, v);\n        \n        vmin = curr_min;\n        vmax = curr_max;\n    }\n\n    // Horizontal min and max\n    __m128i vmin_low = _mm_unpacklo_epi16(vmin, vmin);\n    __m128i vmin_high = _mm_unpackhi_epi16(vmin, vmin);\n    vmin = _mm_min_epi16(vmin_low, vmin_high);\n\n    __m128i vmax_low = _mm_unpacklo_epi16(vmax, vmax);\n    __m128i vmax_high = _mm_unpackhi_epi16(vmax, vmax);\n    vmax = _mm_max_epi16(vmax_low, vmax_high);\n\n    int16_t min_val = _mm_cvtsi128_si32(vmin);\n    int16_t max_val = _mm_cvtsi128_si32(vmax);\n\n    // Process remaining elements\n    for (size_t i = (length / 8) * 8; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i vmin = _mm_set1_epi16(vec[0]);\n    __m128i vmax = _mm_set1_epi16(vec[0]);\n\n    for(size_t i = 0; i + 8 <= length; i += 8){\n        __m128i v = _mm_loadu_si128((__m128i const*)(vec + i));\n        __m128i curr_min = _mm_min_epi16(vmin, v);\n        __m128i curr_max = _mm_max_epi16(vmax, v);\n        vmin = curr_min;\n        vmax = curr_max;\n    }\n\n    // Horizontal min and max\n    __m128i vmin_low = _mm_movehl_ps(_mm_castsi128_ps(vmin), _mm_castsi128_ps(vmin)); \n    vmin_low = _mm_unpacklo_epi8(vmin, vmin);\n    vmin = _mm_min_epi16(vmin, _mm_shuffle_epi32(vmin, 1));\n    vmin = _mm_min_epi16(vmin, _mm_shuffle_epi32(vmin, 2));\n\n    __m128i vmax_low = _mm_movehl_ps(_mm_castsi128_ps(vmax), _mm_castsi128_ps(vmax)); \n    vmax_low = _mm_unpacklo_epi8(vmax, vmax);\n    vmax = _mm_max_epi16(vmax, _mm_shuffle_epi32(vmax, 1));\n    vmax = _mm_max_epi16(vmax, _mm_shuffle_epi32(vmax, 2));\n\n    int16_t min_val, max_val;\n    _mm_storel_epi64((__m128i*)&min_val, vmin);\n    _mm_storel_epi64((__m128i*)&max_val, vmax);\n\n    // Process remaining elements\n    for(size_t i = (length / 8) * 8; i < length; ++i){\n        if(vec[i] < min_val) min_val = vec[i];\n        if(vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i vmin = _mm_set1_epi16(vec[0]);\n    __m128i vmax = _mm_set1_epi16(vec[0]);\n\n    for (size_t i = 0; i + 8 <= length; i += 8, vec += 8) {\n        __m128i v = _mm_loadu_si128((__m128i*)vec);\n        vmin = _mm_min_epi16(vmin, v);\n        vmax = _mm_max_epi16(vmax, v);\n    }\n\n    // Horizontal min and max within registers\n    __m128i vminh = _mm_min_epi16(vmin, _mm_unpackhi_epi64(vmin, vmin));\n    __m128i vmaxh = _mm_max_epi16(vmax, _mm_unpackhi_epi64(vmax, vmax));\n\n    vminh = _mm_min_epi16(vminh, _mm_shuffle_epi32(vminh, _MM_SHUFFLE(3, 3, 1, 1)));\n    vmaxh = _mm_max_epi16(vmaxh, _mm_shuffle_epi32(vmaxh, _MM_SHUFFLE(3, 3, 1, 1)));\n\n    vminh = _mm_min_epi16(vminh, _mm_shufflelo_epi16(vminh, _MM_SHUFFLE(1, 1, 3, 3)));\n    vmaxh = _mm_max_epi16(vmaxh, _mm_shufflelo_epi16(vmaxh, _MM_SHUFFLE(1, 1, 3, 3)));\n\n    int16_t min_val = _mm_cvtsi128_si32(vminh);\n    int16_t max_val = _mm_cvtsi128_si32(vmaxh);\n\n    // Process remaining elements\n    for (size_t i = (length / 8) * 8; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    return (MinMaxPair){min_val, max_val};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i vmin = _mm_set1_epi16(INT16_MAX);\n    __m128i vmax = _mm_set1_epi16(INT16_MIN);\n\n    for (size_t i = 0; i + 7 < length; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i const *)(vec + i));\n        vmin = _mm_min_epi16(vmin, v);\n        vmax = _mm_max_epi16(vmax, v);\n    }\n\n    int16_t min_arr[8] __attribute__((aligned(16)));\n    int16_t max_arr[8] __attribute__((aligned(16)));\n\n    _mm_store_si128((__m128i *)min_arr, vmin);\n    _mm_store_si128((__m128i *)max_arr, vmax);\n\n    int16_t min_val = min_arr[0];\n    int16_t max_val = max_arr[0];\n\n    for (int i = 1; i < 8; ++i) {\n        if (min_arr[i] < min_val) min_val = min_arr[i];\n        if (max_arr[i] > max_val) max_val = max_arr[i];\n    }\n\n    for (size_t i = (length / 8) * 8; i < length; ++i) {\n        if (vec[i] < min_val) min_val = vec[i];\n        if (vec[i] > max_val) max_val = vec[i];\n    }\n\n    MinMaxPair result;\n    result.min_num = min_val;\n    result.max_num = max_val;\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    size_t aligned_cols = cols / 4 * 4;\n    __m128i ones = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool is_increasing = true;\n\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i diff = _mm_sub_epi32(next, current);\n            __m128i cmp = _mm_cmpeq_epi32(diff, ones);\n            __m128i mask = _mm_cmpgt_epi32(diff, _mm_setzero_si128());\n            if (_mm_movemask_epi8(cmp) != 0 || _mm_movemask_epi8(mask) != 0xFFFF) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        for (size_t j = aligned_cols; j < cols - 1; ++j) {\n            if (row[j + 1] - row[j] <= 1) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        if (!is_increasing) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t num_elements_per_row = cols;\n    const size_t aligned_cols = cols / 4 * 4;\n\n    for (size_t r = 0; r < rows; ++r) {\n        const int* row = matrix + r * num_elements_per_row;\n        __m128i prev = _mm_set1_epi32(row[0]);\n        bool increasing = true;\n\n        for (size_t c = 1; c < aligned_cols; c += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + c));\n            __m128i shifted_prev = _mm_shuffle_epi32(prev, _MM_SHUFFLE(2, 3, 2, 3));\n            __m128i cmp = _mm_cmpgt_epi32(current, shifted_prev);\n            __m128i mask = _mm_cmpeq_epi32(cmp, _mm_set1_epi32(-1));\n            if (_mm_movemask_epi8(mask) != 0xFFFF) {\n                increasing = false;\n                break;\n            }\n            prev = current;\n        }\n\n        for (size_t c = aligned_cols; c < num_elements_per_row && increasing; ++c) {\n            if (row[c] <= row[c - 1]) {\n                increasing = false;\n                break;\n            }\n        }\n\n        if (!increasing) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t simd_width = 4;\n    __m128i broadcasted_val;\n    __m128i next_val;\n    __m128i mask;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool is_increasing = true;\n\n        for (size_t j = 0; j + simd_width <= cols; j += simd_width) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i shifted = _mm_alignr_epi8(current, current, 4); // Shift right by one element\n            __m128i comparison = _mm_cmpgt_epi32(shifted, current); // Compare shifted > current\n\n            mask = _mm_cmpeq_epi32(comparison, _mm_set1_epi32(0xFFFFFFFF)); // Check all true\n            if (!_mm_movemask_epi8(mask)) { // If any comparison failed\n                is_increasing = false;\n                break;\n            }\n        }\n\n        // Handle remaining elements\n        for (size_t j = ((cols / simd_width) * simd_width); j < cols - 1; ++j) {\n            if (!(row[j] < row[j + 1])) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        if (!is_increasing) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t aligned_cols = cols / 4 * 4;\n    __m128i mask = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool is_increasing = true;\n\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(current, next);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        for (size_t j = aligned_cols; j < cols - 1; ++j) {\n            if (!(row[j] < row[j + 1])) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        if (!is_increasing) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const size_t aligned_cols = cols / 4 * 4;\n    __m128i mask = _mm_set1_epi32(-1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row = matrix + i * cols;\n        bool is_increasing = true;\n\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)(row + j));\n            __m128i next = _mm_loadu_si128((__m128i*)(row + j + 1));\n            __m128i cmp = _mm_cmplt_epi32(current, next);\n            if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        for (size_t j = aligned_cols; j < cols - 1; ++j) {\n            if (!(row[j] < row[j + 1])) {\n                is_increasing = false;\n                break;\n            }\n        }\n\n        if (!is_increasing) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks_row = cols / block_size;\n    size_t num_blocks_col = rows / block_size;\n\n    const int* first_block = matrix;\n    size_t block_area = block_size * block_size;\n\n    for (size_t b = 1; b < num_blocks_row * num_blocks_col; ++b) {\n        size_t block_row = b / num_blocks_row;\n        size_t block_col = b % num_blocks_row;\n\n        const int* current_block = matrix + (block_row * block_size * cols) + (block_col * block_size);\n\n        for (size_t i = 0; i < block_area; i += 4) {\n            __m128i vec_first = _mm_loadu_si128((__m128i*)(first_block + i));\n            __m128i vec_curr = _mm_loadu_si128((__m128i*)(current_block + i));\n\n            __m128i cmp = _mm_cmpeq_epi32(vec_first, vec_curr);\n            if (!_mm_movemask_epi8(cmp) == 0xFFFF) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    const int* first_block = matrix;\n    __m128i first_block_vec[4]; // Assuming block_size <= 4 for SIMD loading\n\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            first_block_vec[j / 4] = _mm_loadu_si128((__m128i*)&first_block[i * cols + j]);\n        }\n    }\n\n    for (size_t rb = 0; rb < num_row_blocks; ++rb) {\n        for (size_t cb = 0; cb < num_col_blocks; ++cb) {\n            if (rb == 0 && cb == 0) continue;\n\n            const int* current_block = &matrix[rb * block_size * cols + cb * block_size];\n            bool equal = true;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 4) {\n                    __m128i current_block_vec = _mm_loadu_si128((__m128i*)&current_block[i * cols + j]);\n                    if (_mm_movemask_epi8(_mm_cmpeq_epi32(current_block_vec, first_block_vec[j / 4])) != 0xFFFF) {\n                        equal = false;\n                        goto not_equal;\n                    }\n                }\n            }\n\n            not_equal:\n            if (!equal) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    const int* first_block = matrix;\n    __m128i first_block_vec[4]; // Assuming block_size >= 4 for SIMD\n\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 4) {\n            if (j + 4 > block_size) break;\n            first_block_vec[j / 4] = _mm_loadu_si128((__m128i*)&first_block[i * cols + j]);\n        }\n    }\n\n    for (size_t rb = 0; rb < num_row_blocks; ++rb) {\n        for (size_t cb = 0; cb < num_col_blocks; ++cb) {\n            if (rb == 0 && cb == 0) continue;\n\n            const int* current_block = &matrix[rb * block_size * cols + cb * block_size];\n            bool mismatch = false;\n\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 4) {\n                    if (j + 4 > block_size) break;\n                    __m128i current_block_vec = _mm_loadu_si128((__m128i*)&current_block[i * cols + j]);\n                    __m128i diff = _mm_xor_si128(current_block_vec, first_block_vec[j / 4]);\n                    if (_mm_movemask_epi8(diff) != 0) {\n                        mismatch = true;\n                        goto end_check;\n                    }\n                }\n            }\n\n        end_check:\n            if (mismatch) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    size_t block_area = block_size * block_size;\n    __m128i* first_block = (__m128i*)matrix;\n\n    for (size_t i = 0; i < num_row_blocks; ++i) {\n        for (size_t j = 0; j < num_col_blocks; ++j) {\n            const int* current_block_ptr = matrix + (i * block_size * cols) + (j * block_size);\n            __m128i* current_block = (__m128i*)current_block_ptr;\n\n            for (size_t k = 0; k < block_area / 4; ++k) {\n                __m128i v1 = _mm_loadu_si128(first_block + k);\n                __m128i v2 = _mm_loadu_si128(current_block + k);\n\n                __m128i cmp = _mm_cmpeq_epi32(v1, v2);\n                int mask = _mm_movemask_epi8(cmp);\n\n                if (mask != 0xFFFF) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n\n    size_t num_row_blocks = rows / block_size;\n    size_t num_col_blocks = cols / block_size;\n\n    const int* first_block = matrix;\n    size_t block_area = block_size * block_size;\n\n    for (size_t i = 1; i < num_row_blocks * num_col_blocks; ++i) {\n        size_t row_idx = (i / num_col_blocks) * block_size;\n        size_t col_idx = (i % num_col_blocks) * block_size;\n        const int* current_block = matrix + row_idx * cols + col_idx;\n\n        for (size_t j = 0; j < block_area; j += 4) {\n            __m128i vec_first = _mm_loadu_si128((__m128i*)(first_block + j));\n            __m128i vec_curr = _mm_loadu_si128((__m128i*)(current_block + j));\n\n            if (_mm_movemask_epi8(_mm_cmpeq_epi32(vec_first, vec_curr)) != 0xFFFF) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdbool.h>\n#include <stddef.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length == 0 || block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks <= 1) {\n        return true;\n    }\n\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n    __m128d first_block[2];\n    first_block[0] = _mm_loadu_pd(&vec[0]);\n    first_block[1] = _mm_loadu_pd(&vec[2]);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        size_t offset = i * block_size;\n        bool equal = true;\n\n        for (size_t j = 0; j < block_size && equal; j += 4) {\n            __m128d current_block_0 = _mm_loadu_pd(&vec[offset + j]);\n            __m128d current_block_1 = _mm_loadu_pd(&vec[offset + j + 2]);\n\n            __m128d diff_0 = _mm_sub_pd(current_block_0, first_block[0]);\n            __m128d diff_1 = _mm_sub_pd(current_block_1, first_block[1]);\n\n            __m128d abs_diff_0 = _mm_andnot_pd(_mm_castsi128_pd(_mm_cmplt_epi32(_mm_castpd_si128(diff_0), _mm_setzero_si128())), diff_0);\n            __m128d abs_diff_1 = _mm_andnot_pd(_mm_castsi128_pd(_mm_cmplt_epi32(_mm_castpd_si128(diff_1), _mm_setzero_si128())), diff_1);\n\n            __m128d cmp_0 = _mm_cmpgt_pd(abs_diff_0, tol_vec);\n            __m128d cmp_1 = _mm_cmpgt_pd(abs_diff_1, tol_vec);\n\n            int mask_0 = _mm_movemask_pd(cmp_0);\n            int mask_1 = _mm_movemask_pd(cmp_1);\n\n            if (mask_0 || mask_1) {\n                equal = false;\n            }\n        }\n\n        if (!equal) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            __m128d first_block = _mm_loadu_pd(&vec[j]);\n            __m128d current_block = _mm_loadu_pd(&vec[i * block_size + j]);\n\n            __m128d diff = _mm_sub_pd(first_block, current_block);\n            __m128d abs_diff = _mm_or_pd(_mm_and_pd(diff, _mm_castsi128_pd(_mm_set_epi64x(0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF))), _mm_andnot_pd(_mm_castsi128_pd(_mm_set_epi64x(0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF)), diff));\n\n            __m128d cmp = _mm_cmpgt_pd(tol, abs_diff);\n\n            if (!_mm_movemask_pd(cmp)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length == 0 || block_size == 0 || block_size > length || length % block_size != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            __m128d first_block = _mm_loadu_pd(&vec[j]);\n            __m128d current_block = _mm_loadu_pd(&vec[i * block_size + j]);\n\n            __m128d diff = _mm_sub_pd(first_block, current_block);\n            __m128d abs_diff = _mm_and_pd(diff, _mm_castsi128_pd(_mm_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF)));\n\n            __m128d cmp = _mm_cmpgt_pd(tol_vec, abs_diff);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 3) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == nullptr || length == 0 || block_size == 0 || block_size > length || block_size % 2 != 0) {\n        return false;\n    }\n\n    size_t num_blocks = length / block_size;\n    if (num_blocks < 2) {\n        return true;\n    }\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    __m128d first_block[2];\n    first_block[0] = _mm_loadu_pd(&vec[0]);\n    first_block[1] = _mm_loadu_pd(&vec[2]);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        size_t offset = i * block_size;\n        __m128d current_block[2];\n        current_block[0] = _mm_loadu_pd(&vec[offset]);\n        current_block[1] = _mm_loadu_pd(&vec[offset + 2]);\n\n        __m128d diff0 = _mm_sub_pd(current_block[0], first_block[0]);\n        __m128d diff1 = _mm_sub_pd(current_block[1], first_block[1]);\n\n        __m128d abs_diff0 = _mm_andnot_pd(_mm_castsi128_pd(_mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000)), diff0);\n        __m128d abs_diff1 = _mm_andnot_pd(_mm_castsi128_pd(_mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000)), diff1);\n\n        __m128d cmp0 = _mm_cmpgt_pd(tol, abs_diff0);\n        __m128d cmp1 = _mm_cmpgt_pd(tol, abs_diff1);\n\n        if (!_mm_movemask_pd(cmp0) == 0b11 || !_mm_movemask_pd(cmp1) == 0b11) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (block_size == 0 || length < block_size || vec == NULL) return false;\n\n    size_t num_blocks = length / block_size;\n    __m128d tol_vec = _mm_set1_pd(tolerance);\n\n    for (size_t i = 1; i < num_blocks; ++i) {\n        for (size_t j = 0; j < block_size; j += 2) {\n            size_t idx_ref = j;\n            size_t idx_curr = i * block_size + j;\n\n            __m128d ref = _mm_loadu_pd(&vec[idx_ref]);\n            __m128d curr = _mm_loadu_pd(&vec[idx_curr]);\n\n            __m128d diff = _mm_sub_pd(ref, curr);\n            __m128d abs_diff = _mm_or_pd(_mm_and_pd(diff, _mm_castsi128_pd(_mm_set_epi32(-1, -1, 0xFFFFFFFF, 0xFFFFFFFF))), _mm_andnot_pd(_mm_castsi128_pd(_mm_set_epi32(-1, -1, 0xFFFFFFFF, 0xFFFFFFFF)), diff));\n\n            __m128d cmp = _mm_cmpgt_pd(tol_vec, abs_diff);\n            int mask = _mm_movemask_pd(cmp);\n            if (mask != 3) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        size_t j = 0;\n        \n        for (; j + 3 < cols; j += 4) {\n            __m128d v1 = _mm_loadu_pd(&matrix[i * cols + j]);\n            __m128d v2 = _mm_loadu_pd(&matrix[i * cols + j + 2]);\n            __m128d sq1 = _mm_mul_pd(v1, v1);\n            __m128d sq2 = _mm_mul_pd(v2, v2);\n            sum = _mm_add_pd(sum, _mm_add_pd(sq1, sq2));\n        }\n        \n        for (; j < cols; ++j) {\n            double temp = matrix[i * cols + j];\n            sum = _mm_add_sd(sum, _mm_set_sd(temp * temp));\n        }\n        \n        double norm_sq = _mm_cvtsd_f64(sum) + _mm_cvtsd_f64(_mm_unpackhi_pd(sum, sum));\n        double threshold_sq = thresholds[i] * thresholds[i];\n        result[i] = (norm_sq > threshold_sq);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    for (size_t i = 0; i < rows; ++i) {\n        double sum = 0.0;\n        size_t j = 0;\n        for (; j + 3 <= cols; j += 4) {\n            __m128d v1 = _mm_loadu_pd(&matrix[i * cols + j]);\n            __m128d v2 = _mm_loadu_pd(&matrix[i * cols + j + 2]);\n            v1 = _mm_mul_pd(v1, v1);\n            v2 = _mm_mul_pd(v2, v2);\n            v1 = _mm_add_pd(v1, v2);\n            v1 = _mm_hadd_pd(v1, v1);\n            sum += _mm_cvtsd_f64(v1) + _mm_cvtsd_f64(_mm_shuffle_pd(v1, v1, 1));\n        }\n        for (; j < cols; ++j) {\n            sum += matrix[i * cols + j] * matrix[i * cols + j];\n        }\n        result[i] = (sum > thresholds[i] * thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m128d sum = _mm_setzero_pd();\n    __m128d temp;\n\n    for (i = 0; i < rows; ++i) {\n        sum = _mm_setzero_pd();\n        for (j = 0; j + 2 <= cols; j += 2) {\n            temp = _mm_loadu_pd(&matrix[i * cols + j]);\n            temp = _mm_mul_pd(temp, temp);\n            sum = _mm_add_pd(sum, temp);\n        }\n        double row_sum = 0.0;\n        if (cols % 2 != 0) {\n            double last_elem = matrix[i * cols + cols - 1];\n            row_sum += last_elem * last_elem;\n        }\n        row_sum += _mm_extract_double(sum, 0) + _mm_extract_double(sum, 1);\n        row_sum = sqrt(row_sum);\n        result[i] = (row_sum > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        __m128d sum = _mm_setzero_pd();\n        const double* row_start = matrix + i * cols;\n        size_t aligned_cols = cols - (cols % 2);\n\n        for (j = 0; j < aligned_cols; j += 2) {\n            __m128d vals = _mm_loadu_pd(row_start + j);\n            __m128d squared = _mm_mul_pd(vals, vals);\n            sum = _mm_add_pd(sum, squared);\n        }\n\n        double temp_sum[2] __attribute__((aligned(16)));\n        _mm_store_pd(temp_sum, sum);\n        double scalar_sum = temp_sum[0] + temp_sum[1];\n\n        for (; j < cols; ++j) {\n            scalar_sum += row_start[j] * row_start[j];\n        }\n\n        result[i] = (scalar_sum > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t i, j;\n    __m128d sum = _mm_setzero_pd();\n    __m128d temp;\n\n    for (i = 0; i < rows; ++i) {\n        sum = _mm_setzero_pd();\n        const double* row = matrix + i * cols;\n        size_t col = 0;\n\n        // Process 2 elements at a time using SIMD\n        for (; col <= cols - 2; col += 2) {\n            temp = _mm_loadu_pd(row + col);\n            temp = _mm_mul_pd(temp, temp);\n            sum = _mm_add_pd(sum, temp);\n        }\n\n        // Horizontal add of the two values in sum\n        __m128d hsum = _mm_hadd_pd(sum, sum);\n        double row_sum = _mm_cvtsd_f64(hsum);\n\n        // Add any remaining elements\n        for (; col < cols; ++col) {\n            row_sum += row[col] * row[col];\n        }\n\n        // Compare with threshold\n        result[i] = (row_sum > thresholds[i] * thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m128i *a = (__m128i*)A;\n    __m128i *b = (__m128i*)B;\n    __m128i *c = (__m128i*)C;\n\n    for (; i + 4 <= length; i += 4) {\n        _mm_storeu_si128(c++, _mm_or_si128(_mm_loadu_si128(a++), _mm_loadu_si128(b++)));\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* simd_A = (__m128i*)A;\n    __m128i* simd_B = (__m128i*)B;\n    __m128i* simd_C = (__m128i*)C;\n\n    for (; i <= length - 4; i += 4) {\n        _mm_storeu_si128(simd_C++, _mm_or_si128(_mm_loadu_si128(simd_A++), _mm_loadu_si128(simd_B++)));\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m128i *a = (__m128i*)A;\n    __m128i *b = (__m128i*)B;\n    __m128i *c = (__m128i*)C;\n\n    for (; i + 4 <= length; i += 4) {\n        _mm_storeu_si128(c++, _mm_or_si128(_mm_loadu_si128(a++), _mm_loadu_si128(b++)));\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m128i *a = (__m128i*)A;\n    __m128i *b = (__m128i*)B;\n    __m128i *c = (__m128i*)C;\n\n    for (; i + 4 <= length; i += 4) {\n        _mm_storeu_si128(c++, _mm_or_si128(_mm_loadu_si128(a++), _mm_loadu_si128(b++)));\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m128i *a = (__m128i*)A;\n    __m128i *b = (__m128i*)B;\n    __m128i *c = (__m128i*)C;\n\n    for (; i <= length - 4; i += 4) {\n        _mm_storeu_si128(c++, _mm_or_si128(_mm_loadu_si128(a++), _mm_loadu_si128(b++)));\n    }\n\n    for (; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const *)(B + i));\n\n        // Compute squares of A and B\n        __m128i a_squared = _mm_mullo_epi16(a, a);\n        __m128i b_squared = _mm_mullo_epi16(b, b);\n\n        // Sum of squares\n        __m128i ab_squares = _mm_add_epi32(_mm_unpacklo_epi16(a_squared, b_squared),\n                                          _mm_unpackhi_epi16(a_squared, b_squared));\n        sum_squares = _mm_add_epi32(sum_squares, ab_squares);\n\n        // Compute products of A and B\n        __m128i ab_products = _mm_mullo_epi16(a, b);\n        ab_products = _mm_add_epi32(_mm_unpacklo_epi16(ab_products, _mm_setzero_si128()),\n                                   _mm_unpackhi_epi16(ab_products, _mm_setzero_si128()));\n        sum_products = _mm_add_epi32(sum_products, ab_products);\n    }\n\n    // Horizontal sum for squares\n    int64_t total_squares = 0;\n    __m128i temp = _mm_add_epi32(sum_squares, _mm_shuffle_epi32(sum_squares, _MM_SHUFFLE(1, 0, 3, 2)));\n    temp = _mm_add_epi32(temp, _mm_shuffle_epi32(temp, _MM_SHUFFLE(0, 3, 2, 1)));\n    _mm_store_si128((__m128i *)&total_squares, temp);\n\n    // Horizontal sum for products\n    int64_t total_products = 0;\n    temp = _mm_add_epi32(sum_products, _mm_shuffle_epi32(sum_products, _MM_SHUFFLE(1, 0, 3, 2)));\n    temp = _mm_add_epi32(temp, _mm_shuffle_epi32(temp, _MM_SHUFFLE(0, 3, 2, 1)));\n    _mm_store_si128((__m128i *)&total_products, temp);\n\n    // Handle remaining elements\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        total_squares += A[i] * A[i] + B[i] * B[i];\n        total_products += A[i] * B[i];\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        // Compute squares of A and B\n        __m128i a_squared = _mm_mullo_epi16(a, a);\n        __m128i b_squared = _mm_mullo_epi16(b, b);\n\n        // Compute products of A and B\n        __m128i ab_product = _mm_mullo_epi16(a, b);\n\n        // Sum the squares\n        sum_squares = _mm_add_epi32(sum_squares, _mm_unpacklo_epi16(a_squared, b_squared));\n        sum_squares = _mm_add_epi32(sum_squares, _mm_unpackhi_epi16(a_squared, b_squared));\n\n        // Sum the products\n        sum_products = _mm_add_epi32(sum_products, _mm_unpacklo_epi16(ab_product, _mm_setzero_si128()));\n        sum_products = _mm_add_epi32(sum_products, _mm_unpackhi_epi16(ab_product, _mm_setzero_si128()));\n    }\n\n    // Horizontal sum for squares\n    __m128i temp_squares = _mm_add_epi32(_mm_shuffle_epi32(sum_squares, _MM_SHUFFLE(0, 1, 2, 3)), sum_squares);\n    temp_squares = _mm_add_epi32(_mm_shuffle_epi32(temp_squares, _MM_SHUFFLE(0, 0, 1, 1)), temp_squares);\n    int64_t total_squares = _mm_cvtsi128_si32(temp_squares);\n\n    // Horizontal sum for products\n    __m128i temp_products = _mm_add_epi32(_mm_shuffle_epi32(sum_products, _MM_SHUFFLE(0, 1, 2, 3)), sum_products);\n    temp_products = _mm_add_epi32(_mm_shuffle_epi32(temp_products, _MM_SHUFFLE(0, 0, 1, 1)), temp_products);\n    int64_t total_products = _mm_cvtsi128_si32(temp_products);\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        total_squares += A[i] * A[i] + B[i] * B[i];\n        total_products += A[i] * B[i];\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 15 <= length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const *)(B + i));\n\n        // Compute squares of A and B\n        __m128i a_squared = _mm_mullo_epi16(a, a);\n        __m128i b_squared = _mm_mullo_epi16(b, b);\n\n        // Sum of squares\n        __m128i squares = _mm_add_epi32(_mm_unpacklo_epi16(a_squared, b_squared), _mm_unpackhi_epi16(a_squared, b_squared));\n        sum_squares = _mm_add_epi32(sum_squares, squares);\n\n        // Compute products of A and B\n        __m128i product = _mm_mullo_epi16(a, b);\n\n        // Sum of products\n        __m128i products = _mm_add_epi32(_mm_unpacklo_epi16(product, _mm_setzero_si128()), _mm_unpackhi_epi16(product, _mm_setzero_si128()));\n        sum_products = _mm_add_epi32(sum_products, products);\n    }\n\n    // Horizontal add for sum_squares\n    __m128i temp_squares = _mm_add_epi32(_mm_shuffle_epi32(sum_squares, _MM_SHUFFLE(0, 0, 3, 2)), sum_squares);\n    temp_squares = _mm_add_epi32(_mm_shuffle_epi32(temp_squares, _MM_SHUFFLE(0, 0, 1, 0)), temp_squares);\n    int64_t total_squares = _mm_cvtsi128_si32(temp_squares);\n\n    // Horizontal add for sum_products\n    __m128i temp_products = _mm_add_epi32(_mm_shuffle_epi32(sum_products, _MM_SHUFFLE(0, 0, 3, 2)), sum_products);\n    temp_products = _mm_add_epi32(_mm_shuffle_epi32(temp_products, _MM_SHUFFLE(0, 0, 1, 0)), temp_products);\n    int64_t total_products = _mm_cvtsi128_si32(temp_products);\n\n    // Handle remaining elements\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        total_squares += (int64_t)(A[i]) * (int64_t)(A[i]) + (int64_t)(B[i]) * (int64_t)(B[i]);\n        total_products += (int64_t)(A[i]) * (int64_t)(B[i]);\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const *)(B + i));\n\n        // Compute squares of A and B\n        __m128i a_squares = _mm_mullo_epi16(a, a);\n        __m128i b_squares = _mm_mullo_epi16(b, b);\n\n        // Sum of squares\n        __m128i ab_squares = _mm_add_epi32(_mm_unpacklo_epi16(a_squares, b_squares),\n                                          _mm_unpackhi_epi16(a_squares, b_squares));\n        sum_squares = _mm_add_epi32(sum_squares, ab_squares);\n\n        // Compute products of A and B\n        __m128i ab_products = _mm_mullo_epi16(a, b);\n        ab_products = _mm_add_epi32(_mm_unpacklo_epi16(ab_products, _mm_setzero_si128()),\n                                   _mm_unpackhi_epi16(ab_products, _mm_setzero_si128()));\n        sum_products = _mm_add_epi32(sum_products, ab_products);\n    }\n\n    // Horizontal add for sum_squares\n    __m128i temp_squares = _mm_add_epi32(_mm_shuffle_epi32(sum_squares, 0x1B), sum_squares);\n    temp_squares = _mm_add_epi32(_mm_shuffle_epi32(temp_squares, 0x4E), temp_squares);\n    int64_t total_squares = _mm_cvtsi128_si32(temp_squares);\n\n    // Horizontal add for sum_products\n    __m128i temp_products = _mm_add_epi32(_mm_shuffle_epi32(sum_products, 0x1B), sum_products);\n    temp_products = _mm_add_epi32(_mm_shuffle_epi32(temp_products, 0x4E), temp_products);\n    int64_t total_products = _mm_cvtsi128_si32(temp_products);\n\n    // Handle remaining elements\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        total_squares += A[i] * A[i] + B[i] * B[i];\n        total_products += A[i] * B[i];\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_squares = _mm_setzero_si128();\n    __m128i sum_products = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 15 <= length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        // Compute squares of A and B\n        __m128i a_squares = _mm_mullo_epi16(a, a);\n        __m128i b_squares = _mm_mullo_epi16(b, b);\n\n        // Sum of squares\n        __m128i ab_squares = _mm_add_epi32(_mm_unpacklo_epi16(a_squares, b_squares), \n                                          _mm_unpackhi_epi16(a_squares, b_squares));\n        sum_squares = _mm_add_epi32(sum_squares, ab_squares);\n\n        // Compute products of A and B\n        __m128i ab_products = _mm_mullo_epi16(a, b);\n\n        // Sum of products\n        __m128i ab_sum = _mm_add_epi32(_mm_unpacklo_epi16(ab_products, _mm_setzero_si128()), \n                                       _mm_unpackhi_epi16(ab_products, _mm_setzero_si128()));\n        sum_products = _mm_add_epi32(sum_products, ab_sum);\n    }\n\n    // Horizontal add for sum_squares\n    __m128i temp_squares = _mm_add_epi32(_mm_unpacklo_epi32(sum_squares, _mm_setzero_si128()), \n                                         _mm_unpackhi_epi32(sum_squares, _mm_setzero_si128()));\n    int64_t total_squares = _mm_cvtsi128_si32(temp_squares) + ((uint32_t)_mm_extract_epi32(temp_squares, 1));\n\n    // Horizontal add for sum_products\n    __m128i temp_products = _mm_add_epi32(_mm_unpacklo_epi32(sum_products, _mm_setzero_si128()), \n                                          _mm_unpackhi_epi32(sum_products, _mm_setzero_si128()));\n    int64_t total_products = _mm_cvtsi128_si32(temp_products) + ((uint32_t)_mm_extract_epi32(temp_products, 1));\n\n    // Handle remaining elements\n    for (size_t i = (length / 16) * 16; i < length; ++i) {\n        total_squares += A[i] * A[i] + B[i] * B[i];\n        total_products += A[i] * B[i];\n    }\n\n    return total_squares - total_products;\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i xmm_zero = _mm_setzero_si128();\n    for (; i + 1 <= length; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i not_b = _mm_andnot_si128(b, _mm_cmpeq_epi64(xmm_zero, xmm_zero));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 1 < length; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i not_b = _mm_andnot_si128(b, _mm_cmpeq_epi64(zero, zero));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    if (i < length) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 1 <= length; i += 2) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i not_b = _mm_xor_si128(b, _mm_cmpeq_epi64(zero, zero));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i *)(C + i), result);\n    }\n    // Handle any remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* a_aligned = (__m128i*)A;\n    __m128i* b_aligned = (__m128i*)B;\n    __m128i* c_aligned = (__m128i*)C;\n\n    for (; i + 2 <= length; i += 2) {\n        __m128i a = _mm_loadu_si128(&a_aligned[i]);\n        __m128i b = _mm_loadu_si128(&b_aligned[i]);\n        __m128i result = _mm_andnot_si128(b, a);\n        _mm_storeu_si128(&c_aligned[i], result);\n    }\n\n    for (; i < length; i++) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 2 <= length; i += 2) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i not_b = _mm_xor_si128(b, _mm_cmpeq_epi64(zero, zero));\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & ~B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n\n        __m128i pos_a = _mm_cmpgt_epi8(a, zero);\n        __m128i pos_b = _mm_cmpgt_epi8(b, zero);\n        __m128i both_pos = _mm_and_si128(pos_a, pos_b);\n\n        __m128i neg_a = _mm_cmplt_epi8(a, zero);\n        __m128i neg_b = _mm_cmplt_epi8(b, zero);\n        __m128i both_neg = _mm_and_si128(neg_a, neg_b);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, one),\n            _mm_and_si128(both_neg, neg_one)\n        );\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi8(0x80);\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n\n        __m128i sign_a = _mm_and_si128(a, sign_mask);\n        __m128i sign_b = _mm_and_si128(b, sign_mask);\n\n        __m128i both_positive = _mm_and_si128(_mm_cmpgt_epi8(a, zero), _mm_cmpgt_epi8(b, zero));\n        __m128i both_negative = _mm_and_si128(_mm_cmplt_epi8(a, zero), _mm_cmplt_epi8(b, zero));\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_positive, one),\n            _mm_and_si128(both_negative, neg_one)\n        );\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i pos_mask = _mm_set1_epi8(0x00);\n    __m128i neg_mask = _mm_set1_epi8(0xFF);\n    __m128i zero = _mm_set1_epi8(0);\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n\n        __m128i sign_a = _mm_sign_epi8(a, neg_mask);\n        __m128i sign_b = _mm_sign_epi8(b, neg_mask);\n\n        __m128i both_pos = _mm_and_si128(_mm_cmpeq_epi8(a, zero), _mm_cmpeq_epi8(b, zero));\n        __m128i both_neg = _mm_and_si128(_mm_cmpeq_epi8(sign_a, neg_mask), _mm_cmpeq_epi8(sign_b, neg_mask));\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, one),\n            _mm_and_si128(both_neg, minus_one)\n        );\n\n        _mm_storeu_si128((__m128i *)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i one = _mm_set1_epi8(1);\n    const __m128i neg_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 15 < length; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n\n        __m128i pos_a = _mm_cmpgt_epi8(a, zero);\n        __m128i pos_b = _mm_cmpgt_epi8(b, zero);\n        __m128i both_pos = _mm_and_si128(pos_a, pos_b);\n\n        __m128i neg_a = _mm_cmplt_epi8(a, zero);\n        __m128i neg_b = _mm_cmplt_epi8(b, zero);\n        __m128i both_neg = _mm_and_si128(neg_a, neg_b);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, one),\n            _mm_and_si128(both_neg, neg_one)\n        );\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m128i pos_mask = _mm_set1_epi8(0x7F);\n    __m128i neg_mask = _mm_set1_epi8(0x80);\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n    __m128i zero = _mm_set1_epi8(0);\n\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n\n        __m128i a_pos = _mm_and_si128(a, pos_mask);\n        __m128i b_pos = _mm_and_si128(b, pos_mask);\n        __m128i both_pos = _mm_cmpeq_epi8(_mm_add_epi8(a_pos, b_pos), _mm_add_epi8(a_pos, pos_mask));\n\n        __m128i a_neg = _mm_and_si128(a, neg_mask);\n        __m128i b_neg = _mm_and_si128(b, neg_mask);\n        __m128i both_neg = _mm_cmpeq_epi8(_mm_add_epi8(a_neg, b_neg), _mm_add_epi8(a_neg, neg_mask));\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_pos, one),\n            _mm_and_si128(both_neg, minus_one)\n        );\n        result = _mm_or_si128(result, _mm_andnot_si128(_mm_or_si128(both_pos, both_neg), zero));\n\n        _mm_storeu_si128((__m128i *)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    // Process elements before the midpoint with XOR\n    for (; i <= midpoint - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining scalar elements before midpoint\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Copy the midpoint element from A to C\n    if (midpoint < length) {\n        C[midpoint] = A[midpoint];\n        ++midpoint;\n    }\n\n    // Process elements after the midpoint with AND\n    for (i = midpoint; i <= length - 8; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining scalar elements after midpoint\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Handle elements before the midpoint with XOR\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Handle the midpoint element\n    C[midpoint] = A[midpoint];\n\n    // Handle elements after the midpoint with AND\n    i = midpoint + 1;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint with XOR\n    for (; i + 7 < midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Copy the element at the midpoint from A to C\n    if (midpoint < length) {\n        C[midpoint] = A[midpoint];\n        ++midpoint;\n    }\n\n    // Process elements after the midpoint with AND\n    for (i = midpoint; i + 7 < length; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    // Process elements before the midpoint with XOR\n    for (; i + 8 <= midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i result = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    for (; i < midpoint; i++) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Process the midpoint element\n    if (midpoint < length) {\n        C[midpoint] = A[midpoint];\n        midpoint++;\n    }\n\n    // Process elements after the midpoint with AND\n    for (i = midpoint; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i const*)(B + i));\n        __m128i result = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n    for (; i < length; i++) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t midpoint = length / 2;\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    // Process elements before the midpoint with XOR\n    for (; i + 7 < midpoint; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i res = _mm_xor_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], res);\n    }\n    for (; i < midpoint; ++i) {\n        C[i] = A[i] ^ B[i];\n    }\n\n    // Copy the midpoint element from A to C\n    if (midpoint < length) {\n        C[midpoint] = A[midpoint];\n        ++midpoint;\n    }\n\n    // Process elements after the midpoint with AND\n    for (i = midpoint; i + 7 < length; i += 8) {\n        __m128i a = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((const __m128i*)&B[i]);\n        __m128i res = _mm_and_si128(a, b);\n        _mm_storeu_si128((__m128i*)&C[i], res);\n    }\n    for (; i < length; ++i) {\n        C[i] = A[i] & B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i true_mask = _mm_set1_epi32(-1);\n    __m128i false_mask = _mm_setzero_si128();\n\n    for (; i + 3 < total_size; i += 4) {\n        // Load conditions\n        __m128i c1 = _mm_loadu_si128((const __m128i*)&cond1[i]);\n        __m128i c2 = _mm_loadu_si128((const __m128i*)&cond2[i]);\n\n        // Convert boolean to mask (true -> -1, false -> 0)\n        __m128i cond1_mask = _mm_cmpeq_epi32(c1, true_mask);\n        __m128i cond2_mask = _mm_cmpeq_epi32(c2, true_mask);\n\n        // Logical AND and OR masks\n        __m128i both_true = _mm_and_si128(cond1_mask, cond2_mask);\n        __m128i c1_true_c2_false = _mm_andnot_si128(cond2_mask, cond1_mask);\n        __m128i c1_false_c2_true = _mm_andnot_si128(cond1_mask, cond2_mask);\n        __m128i both_false = _mm_andnot_si128(_mm_or_si128(cond1_mask, cond2_mask), true_mask);\n\n        // Load tensors\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        // Select based on conditions\n        __m128 result = _mm_or_ps(_mm_or_ps(\n            _mm_or_ps(\n                _mm_and_ps(_mm_castsi128_ps(both_true), a),\n                _mm_and_ps(_mm_castsi128_ps(c1_true_c2_false), b)),\n            _mm_and_ps(_mm_castsi128_ps(c1_false_c2_true), c)),\n            _mm_and_ps(_mm_castsi128_ps(both_false), d));\n\n        // Store result\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 true_vec = _mm_set1_ps(1.0f);\n    __m128 false_vec = _mm_set1_ps(0.0f);\n\n    for (; i + 4 <= total_size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128i cond1_vec = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i cond2_vec = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128 cond1_f = _mm_castsi128_ps(cond1_vec);\n        __m128 cond2_f = _mm_castsi128_ps(cond2_vec);\n\n        __m128 mask1 = _mm_and_ps(cond1_f, cond2_f); // cond1 && cond2\n        __m128 mask2 = _mm_andnot_ps(cond2_f, cond1_f); // cond1 && !cond2\n        __m128 mask3 = _mm_andnot_ps(cond1_f, cond2_f); // !cond1 && cond2\n        __m128 mask4 = _mm_xor_ps(_mm_xor_ps(mask1, mask2), mask3); // !(mask1 || mask2 || mask3)\n\n        __m128 out_vec = _mm_or_ps(_mm_and_ps(mask1, a),\n                                  _mm_or_ps(_mm_and_ps(mask2, b),\n                                            _mm_or_ps(_mm_and_ps(mask3, c),\n                                                     _mm_and_ps(mask4, d))));\n\n        _mm_storeu_ps(&out[i], out_vec);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    __m128 true_vec = _mm_set1_ps(1.0f);\n    __m128 false_vec = _mm_set1_ps(0.0f);\n\n    for (size_t i = 0; i < total_size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128i cond1_vec = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i cond2_vec = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128 cond1_f = _mm_and_ps(_mm_castsi128_ps(cond1_vec), true_vec);\n        __m128 cond2_f = _mm_and_ps(_mm_castsi128_ps(cond2_vec), true_vec);\n\n        __m128 not_cond1_f = _mm_xor_ps(cond1_f, true_vec);\n        __m128 not_cond2_f = _mm_xor_ps(cond2_f, true_vec);\n\n        __m128 cond1_and_cond2 = _mm_and_ps(cond1_f, cond2_f);\n        __m128 cond1_and_not_cond2 = _mm_and_ps(cond1_f, not_cond2_f);\n        __m128 not_cond1_and_cond2 = _mm_and_ps(not_cond1_f, cond2_f);\n        __m128 not_cond1_and_not_cond2 = _mm_and_ps(not_cond1_f, not_cond2_f);\n\n        __m128 result1 = _mm_or_ps(_mm_and_ps(cond1_and_cond2, a), _mm_and_ps(cond1_and_not_cond2, b));\n        __m128 result2 = _mm_or_ps(_mm_and_ps(not_cond1_and_cond2, c), _mm_and_ps(not_cond1_and_not_cond2, d));\n\n        __m128 result = _mm_or_ps(result1, result2);\n\n        _mm_storeu_ps(&out[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i true_mask = _mm_set1_epi32(-1);\n    __m128i false_mask = _mm_setzero_si128();\n\n    for (; i <= total_size - 4; i += 4) {\n        // Load conditions\n        __m128i c1 = _mm_loadu_si128((const __m128i*)&cond1[i]);\n        __m128i c2 = _mm_loadu_si128((const __m128i*)&cond2[i]);\n\n        // Convert boolean to mask (true -> -1, false -> 0)\n        __m128i cond1_mask = _mm_and_si128(c1, true_mask);\n        __m128i cond2_mask = _mm_and_si128(c2, true_mask);\n\n        // Compute combined masks\n        __m128i both_true = _mm_and_si128(cond1_mask, cond2_mask);\n        __m128i c1_true_c2_false = _mm_andnot_si128(cond2_mask, cond1_mask);\n        __m128i c1_false_c2_true = _mm_andnot_si128(cond1_mask, cond2_mask);\n        __m128i both_false = _mm_andnot_si128(_mm_or_si128(cond1_mask, cond2_mask), true_mask);\n\n        // Load values from tensors\n        __m128 a_val = _mm_loadu_ps(&A[i]);\n        __m128 b_val = _mm_loadu_ps(&B[i]);\n        __m128 c_val = _mm_loadu_ps(&C[i]);\n        __m128 d_val = _mm_loadu_ps(&D[i]);\n\n        // Select based on conditions\n        __m128 result = _mm_or_ps(\n            _mm_or_ps(\n                _mm_or_ps(\n                    _mm_and_ps(_mm_castsi128_ps(both_true), a_val),\n                    _mm_and_ps(_mm_castsi128_ps(c1_true_c2_false), b_val)\n                ),\n                _mm_and_ps(_mm_castsi128_ps(c1_false_c2_true), c_val)\n            ),\n            _mm_and_ps(_mm_castsi128_ps(both_false), d_val)\n        );\n\n        // Store the result\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    // Handle remaining elements with scalar code\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i true_mask = _mm_set1_epi32(-1);\n    __m128i false_mask = _mm_setzero_si128();\n\n    for (; i <= total_size - 4; i += 4) {\n        __m128 a_val = _mm_loadu_ps(&A[i]);\n        __m128 b_val = _mm_loadu_ps(&B[i]);\n        __m128 c_val = _mm_loadu_ps(&C[i]);\n        __m128 d_val = _mm_loadu_ps(&D[i]);\n\n        __m128i cond1_vec = _mm_loadu_si128((__m128i*)&cond1[i]);\n        __m128i cond2_vec = _mm_loadu_si128((__m128i*)&cond2[i]);\n\n        __m128i cond1_true = _mm_cmpeq_epi32(cond1_vec, true_mask);\n        __m128i cond2_true = _mm_cmpeq_epi32(cond2_vec, true_mask);\n\n        __m128 cond1_true_f = _mm_castsi128_ps(cond1_true);\n        __m128 cond2_true_f = _mm_castsi128_ps(cond2_true);\n\n        __m128 cond1_false_f = _mm_xor_ps(cond1_true_f, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n        __m128 cond2_false_f = _mm_xor_ps(cond2_true_f, _mm_castsi128_ps(_mm_set1_epi32(-1)));\n\n        __m128 cond1_and_cond2 = _mm_and_ps(cond1_true_f, cond2_true_f);\n        __m128 cond1_and_not_cond2 = _mm_and_ps(cond1_true_f, cond2_false_f);\n        __m128 not_cond1_and_cond2 = _mm_and_ps(cond1_false_f, cond2_true_f);\n        __m128 not_cond1_and_not_cond2 = _mm_and_ps(cond1_false_f, cond2_false_f);\n\n        __m128 selected_a = _mm_and_ps(cond1_and_cond2, a_val);\n        __m128 selected_b = _mm_and_ps(cond1_and_not_cond2, b_val);\n        __m128 selected_c = _mm_and_ps(not_cond1_and_cond2, c_val);\n        __m128 selected_d = _mm_and_ps(not_cond1_and_not_cond2, d_val);\n\n        __m128 result = _mm_or_ps(selected_a, _mm_or_ps(selected_b, _mm_or_ps(selected_c, selected_d)));\n\n        _mm_storeu_ps(&out[i], result);\n    }\n\n    for (; i < total_size; ++i) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    __m128i ones = _mm_set1_epi8(0x01);\n    __m128i count_mask = _mm_set_epi8(0x80, 0x40, 0x20, 0x10, 0x08, 0x04, 0x02, 0x01, 0x80, 0x40, 0x20, 0x10, 0x08, 0x04, 0x02, 0x01);\n\n    for (size_t i = 0; i < total_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i count = _mm_setzero_si128();\n\n        for (int j = 0; j < 32; ++j) {\n            __m128i cmp = _mm_and_si128(_mm_cmpeq_epi8(data, _mm_slli_epi32(ones, j)), ones);\n            count = _mm_add_epi8(count, cmp);\n        }\n\n        __m128i packed = _mm_packs_epi16(_mm_packus_epi16(_mm_cvtepi32_epi16(count), _mm_cvtepi32_epi16(_mm_srli_si128(count, 8))), _mm_setzero_si128());\n        _mm_storel_epi64((__m128i *)(out + i), packed);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    const size_t aligned_elements = total_elements / 4 * 4;\n\n    __m128i ones = _mm_set1_epi8(0x01);\n    __m128i count_mask = _mm_set_epi8(0x80, 0x40, 0x20, 0x10, 0x08, 0x04, 0x02, 0x01, 0x80, 0x40, 0x20, 0x10, 0x08, 0x04, 0x02, 0x01);\n\n    for (size_t i = 0; i < aligned_elements; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i count = _mm_sad_epu8(_mm_and_si128(data, count_mask), _mm_setzero_si128());\n        count = _mm_shuffle_epi32(count, _MM_SHUFFLE(0, 0, 0, 0));\n        count = _mm_add_epi8(count, _mm_shuffle_epi32(count, _MM_SHUFFLE(1, 1, 1, 1)));\n        count = _mm_add_epi8(count, _mm_shuffle_epi32(count, _MM_SHUFFLE(2, 2, 2, 2)));\n        count = _mm_add_epi8(count, _mm_shuffle_epi32(count, _MM_SHUFFLE(3, 3, 3, 3)));\n\n        _mm_storel_pi((__m64*)(out + i), _mm_castsi128_ps(count));\n    }\n\n    for (size_t i = aligned_elements; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    const __m128i ones = _mm_set1_epi8(0x01);\n    for (size_t i = 0; i < total_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i count = _mm_sad_epu8(_mm_and_si128(vec, ones), _mm_setzero_si128());\n        count = _mm_add_epi16(count, _mm_slli_epi16(count, 4));\n        count = _mm_add_epi16(count, _mm_slli_epi16(count, 8));\n        count = _mm_add_epi16(count, _mm_slli_epi16(count, 12));\n        __m128i result = _mm_packs_epi16(count, count);\n        _mm_storel_epi64((__m128i *)(out + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    __m128i ones = _mm_set1_epi8(0x01);\n    __m128i two = _mm_set1_epi8(0x02);\n    __m128i four = _mm_set1_epi8(0x04);\n    __m128i eight = _mm_set1_epi8(0x08);\n    __m128i sixteen = _mm_set1_epi8(0x10);\n    __m128i thirtytwo = _mm_set1_epi8(0x20);\n    __m128i sixtyfour = _mm_set1_epi8(0x40);\n    __m128i onetwentyeight = _mm_set1_epi8(0x80);\n\n    for (size_t i = 0; i < total_size / 4; ++i) {\n        __m128i v = _mm_loadu_si128((__m128i const *)(A + 4 * i));\n        __m128i count = _mm_setzero_si128();\n\n        __m128i comp = _mm_and_si128(v, _mm_set1_epi32(0x55555555));\n        count = _mm_add_epi8(count, _mm_and_si128(comp, ones));\n        comp = _mm_srli_epi32(comp, 1);\n        count = _mm_add_epi8(count, _mm_and_si128(comp, ones));\n\n        comp = _mm_and_si128(v, _mm_set1_epi32(0x33333333));\n        count = _mm_add_epi8(count, _mm_and_si128(comp, two));\n        comp = _mm_srli_epi32(comp, 2);\n        count = _mm_add_epi8(count, _mm_and_si128(comp, two));\n\n        comp = _mm_and_si128(v, _mm_set1_epi32(0x0f0f0f0f));\n        count = _mm_add_epi8(count, _mm_and_si128(comp, four));\n        comp = _mm_srli_epi32(comp, 4);\n        count = _mm_add_epi8(count, _mm_and_si128(comp, four));\n\n        comp = _mm_and_si128(v, _mm_set1_epi32(0x00ff00ff));\n        count = _mm_add_epi8(count, _mm_and_si128(comp, eight));\n        comp = _mm_srli_epi32(comp, 8);\n        count = _mm_add_epi8(count, _mm_and_si128(comp, eight));\n\n        comp = _mm_and_si128(v, _mm_set1_epi32(0x0000ffff));\n        count = _mm_add_epi8(count, _mm_and_si128(comp, sixteen));\n        comp = _mm_srli_epi32(comp, 16);\n        count = _mm_add_epi8(count, _mm_and_si128(comp, sixteen));\n\n        count = _mm_add_epi8(count, _mm_and_si128(v, thirtytwo));\n        count = _mm_add_epi8(count, _mm_and_si128(_mm_srli_epi32(v, 32), thirtytwo));\n\n        count = _mm_add_epi8(count, _mm_and_si128(v, sixtyfour));\n        count = _mm_add_epi8(count, _mm_and_si128(_mm_srli_epi32(v, 64), sixtyfour));\n\n        count = _mm_add_epi8(count, _mm_and_si128(v, onetwentyeight));\n        count = _mm_add_epi8(count, _mm_and_si128(_mm_srli_epi32(v, 128), onetwentyeight));\n\n        _mm_storeu_si128((__m128i *)(out + 4 * i), count);\n    }\n\n    for (size_t i = total_size / 4 * 4; i < total_size; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_size = dim1 * dim2 * dim3;\n    __m128i ones = _mm_set1_epi8(0x01);\n    __m128i count_mask = _mm_set_epi8(0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1);\n\n    for (size_t i = 0; i < total_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i cnt = _mm_setzero_si128();\n\n        __m128i shifted = data;\n        for (int s = 0; s < 32; s += 8) {\n            __m128i byte = _mm_and_si128(_mm_srli_epi32(shifted, s), ones);\n            cnt = _mm_add_epi8(cnt, byte);\n            shifted = _mm_srli_epi32(shifted, 1);\n        }\n\n        cnt = _mm_shuffle_epi8(cnt, count_mask);\n        __m128i sum = _mm_sad_epu8(cnt, _mm_setzero_si128());\n        __m128i result = _mm_packus_epi16(_mm_srli_epi16(sum, 4), _mm_srli_epi16(sum, 4));\n\n        _mm_storel_epi64((__m128i *)(out + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t length = rows * cols;\n    size_t aligned_length = length / 4 * 4;\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i three = _mm_set1_epi32(3);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i rem = _mm_slli_epi32(vec, 30); // Multiply by 30 to handle division by 3\n        rem = _mm_srli_epi32(rem, 30);         // Approximate remainder check\n        __m128i div_by_3 = _mm_cmpeq_epi32(rem, zero);\n        __m128i abs_val = _mm_andnot_epi32(_mm_cmplt_epi32(vec, zero), vec);\n        __m128i result = _mm_blendv_epi8(vec, abs_val, div_by_3);\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_length; i < length; ++i) {\n        if ((A[i] % 3) == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t length = rows * cols;\n    size_t aligned_length = length / 4 * 4;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i three = _mm_set1_epi32(3);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i rem = _mm_slli_epi32(vec, 30); // Multiply by 30 to simulate modulo 3\n        rem = _mm_srli_epi32(rem, 30);         // Keep only the remainder bits\n        __m128i cond = _mm_cmpeq_epi32(rem, zero); // Check if remainder is zero\n\n        __m128i abs_vec = _mm_abs_epi32(vec); // Compute absolute values\n        __m128i result = _mm_blendv_epi8(vec, abs_vec, cond); // Select based on condition\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_length; i < length; ++i) {\n        if ((A[i] % 3) == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t length = rows * cols;\n    size_t aligned_length = length - (length % 4);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        \n        __m128i three = _mm_set1_epi32(3);\n        __m128i mod_vec = _mm_and_si128(_mm_slli_epi32(_mm_srli_epi32(vec, 31), 31), three);\n        __m128i div_check = _mm_cmpeq_epi32(mod_vec, _mm_setzero_si128());\n        \n        __m128i result = _mm_or_si128(_mm_and_si128(div_check, abs_vec), _mm_andnot_si128(div_check, vec));\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_length; i < length; ++i) {\n        if ((A[i] % 3) == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t length = rows * cols;\n    size_t aligned_length = length - (length % 4);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i zero = _mm_setzero_si128();\n        __m128i abs_vec = _mm_sub_epi32(_mm_xor_si128(vec, zero), _mm_and_si128(vec, zero));\n\n        __m128i three = _mm_set1_epi32(3);\n        __m128i rem = _mm_abs_epi32(_mm_srai_epi32(_mm_mullo_epi32(vec, three), 31));\n        __m128i div_three = _mm_cmpeq_epi32(_mm_rem_epi32(vec, three), zero);\n\n        __m128i cond = _mm_and_si128(div_three, abs_vec);\n        __m128i result = _mm_or_si128(cond, _mm_andnot_si128(div_three, vec));\n\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_length; i < length; ++i) {\n        if (A[i] % 3 == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t length = rows * cols;\n    size_t aligned_length = length - (length % 4);\n\n    for (size_t i = 0; i < aligned_length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i zero = _mm_setzero_si128();\n        __m128i three = _mm_set1_epi32(3);\n        \n        // Compute A % 3 == 0\n        __m128i rem = _mm_srai_epi32(_mm_and_si128(_mm_sub_epi32(vec, _mm_mullo_epi32(_mm_div_epi32(vec, three), three)), 31);\n        __m128i mask = _mm_cmpeq_epi32(rem, zero);\n\n        // Compute abs(A)\n        __m128i abs_vec = _mm_sub_epi32(zero, vec);\n        abs_vec = _mm_min_epi32(abs_vec, vec);\n\n        // Select between abs(A) and A based on mask\n        __m128i result = _mm_blendv_epi8(vec, abs_vec, mask);\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = aligned_length; i < length; ++i) {\n        if ((A[i] % 3) == 0) {\n            B[i] = abs(A[i]);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const size_t vec_length = length / 4;\n    const __m128i mask_f = _mm_set1_epi32(0xF);\n    const __m128i mask_not_f = _mm_set1_epi32(~0xF);\n    const __m128i rot_mask = _mm_set1_epi32((rotate_bits % 32));\n    \n    for(size_t i = 0; i < vec_length; ++i){\n        __m128i v_src = _mm_loadu_si128((__m128i const*)(src + 4 * i));\n        __m128i low_bits = _mm_and_si128(v_src, mask_f);\n        __m128i condition = _mm_cmpeq_epi32(low_bits, mask_f);\n\n        // Rotate right by rotate_bits\n        __m128i rotated = _mm_or_si128(\n            _mm_bsrli_epi128(v_src, rotate_bits),\n            _mm_bslli_epi128(v_src, 32 - rotate_bits)\n        );\n\n        // Negate the source\n        __m128i negated = _mm_xor_si128(v_src, mask_not_f);\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated, condition);\n        \n        _mm_storeu_si128((__m128i *)(dst + 4 * i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const size_t vec_len = length / 4;\n    const __m128i mask_low_4bits = _mm_set1_epi32(0xF);\n    const __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF);\n    const __m128i zero = _mm_setzero_si128();\n    const int rot_val = rotate_bits;\n\n    for(size_t i = 0; i < vec_len; ++i){\n        __m128i vec_src = _mm_loadu_si128((__m128i*)&src[i*4]);\n        __m128i low_4bits = _mm_and_si128(vec_src, mask_low_4bits);\n        __m128i condition = _mm_cmpeq_epi32(low_4bits, mask_low_4bits);\n\n        __m128i rotated = _mm_or_si128(\n            _mm_slli_epi32(vec_src, rot_val),\n            _mm_srli_epi32(vec_src, 32 - rot_val)\n        );\n\n        __m128i negated = _mm_xor_si128(vec_src, all_ones);\n\n        __m128i result = _mm_blendv_epi8(negated, rotated, condition);\n\n        _mm_storeu_si128((__m128i*)&dst[i*4], result);\n    }\n\n    // Handle remaining elements if any\n    for(size_t i = vec_len * 4; i < length; ++i){\n        uint32_t val = src[i];\n        if((val & 0xF) == 0xF){\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        }\n        else{\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const __m128i mask_low_4bits = _mm_set1_epi32(0xF); // Mask for low 4 bits\n    const __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF); // All ones for bitwise negation\n    const __m128i zero = _mm_setzero_si128(); // Zero vector\n\n    size_t i = 0;\n    for (; i + 3 < length; i += 4) {\n        __m128i vec_src = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i low_4bits = _mm_and_si128(vec_src, mask_low_4bits);\n        __m128i condition = _mm_cmpeq_epi32(low_4bits, mask_low_4bits);\n\n        // Rotate right by rotate_bits\n        __m128i rotated = _mm_or_si128(\n            _mm_srli_epi32(vec_src, rotate_bits),\n            _mm_slli_epi32(vec_src, 32 - rotate_bits)\n        );\n\n        // Bitwise negation\n        __m128i negated = _mm_xor_si128(vec_src, all_ones);\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated, condition);\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const size_t simd_length = 4;\n    __m128i mask_low_4bits_set = _mm_set1_epi32(0xF);\n    __m128i mask_low_4bits_check = _mm_set1_epi32(0xF);\n    __m128i rotate_amount = _mm_set1_epi32(rotate_bits);\n\n    for (size_t i = 0; i < length / simd_length * simd_length; i += simd_length) {\n        __m128i vec_src = _mm_loadu_si128((__m128i const *)(src + i));\n        \n        // Check if low 4 bits are all 1s\n        __m128i low_4bits = _mm_and_si128(vec_src, mask_low_4bits_check);\n        __m128i condition = _mm_cmpeq_epi32(low_4bits, mask_low_4bits_set);\n\n        // Rotate right by rotate_bits\n        __m128i rotated = _mm_or_si128(\n            _mm_srli_epi32(vec_src, rotate_bits),\n            _mm_slli_epi32(vec_src, 32 - rotate_bits)\n        );\n\n        // Negate the vector\n        __m128i negated = _mm_xor_si128(vec_src, _mm_set1_epi32(0xFFFFFFFF));\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated, condition);\n\n        _mm_storeu_si128((__m128i *)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (length / simd_length * simd_length); i < length; ++i) {\n        uint32_t val = src[i];\n        if ((val & 0xF) == 0xF) {\n            dst[i] = (val >> rotate_bits) | (val << (32 - rotate_bits));\n        } else {\n            dst[i] = ~val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const __m128i mask = _mm_set1_epi32(0xF); // Mask for low 4 bits\n    const __m128i rot_mask = _mm_set1_epi32((rotate_bits % 32)); // Rotation amount\n    const __m128i all_ones = _mm_set1_epi32(0xFFFFFFFF); // For bitwise negation\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)&src[i]);\n        __m128i low_bits = _mm_and_si128(vec, mask); // Extract low 4 bits\n        __m128i cond = _mm_cmpeq_epi32(low_bits, mask); // Check if low 4 bits are all 1\n\n        // Perform rotation\n        __m128i rotated = _mm_or_si128(\n            _mm_slli_epi32(vec, rotate_bits),\n            _mm_srli_epi32(vec, 32 - rotate_bits)\n        );\n\n        // Perform bitwise negation\n        __m128i negated = _mm_xor_si128(vec, all_ones);\n\n        // Select based on condition\n        __m128i result = _mm_blendv_epi8(negated, rotated, cond);\n\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 abs_epsilon = _mm_set1_ps(fabsf(epsilon));\n    __m128i zero = _mm_set1_epi8(0);\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), diff); // abs(diff)\n\n        __m128 greater = _mm_cmpgt_ps(diff, abs_epsilon);\n        __m128 less = _mm_cmplt_ps(diff, _mm_sub_ps(_mm_setzero_ps(), abs_epsilon));\n\n        __m128 mask = _mm_or_ps(greater, _mm_andnot_ps(greater, less));\n\n        __m128i result = _mm_setzero_si128();\n        result = _mm_or_si128(result, _mm_and_si128(_mm_castps_si128(greater), one));\n        result = _mm_or_si128(result, _mm_and_si128(_mm_castps_si128(less), minus_one));\n\n        __m128i packed_result = _mm_packs_epi16(result, result);\n        __m128i final_result = _mm_packus_epi16(packed_result, packed_result);\n\n        _mm_storel_epi64((__m128i *)(out + i), final_result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 abs_epsilon = _mm_set1_ps(fabsf(epsilon));\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 greater = _mm_cmpgt_ps(diff, abs_epsilon);\n        __m128 less = _mm_cmplt_ps(diff, _mm_sub_ps(_mm_setzero_ps(), abs_epsilon));\n\n        __m128i mask_greater = _mm_castps_si128(greater);\n        __m128i mask_less = _mm_castps_si128(less);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mask_greater, _mm_shuffle_epi32(one, _MM_SHUFFLE(0, 0, 0, 0))),\n            _mm_and_si128(mask_less, _mm_shuffle_epi32(minus_one, _MM_SHUFFLE(0, 0, 0, 0)))\n        );\n\n        result = _mm_packs_epi32(result, zero);\n        result = _mm_packus_epi16(result, zero);\n\n        _mm_storel_epi64((__m128i*)(out + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if (A[i] - B[i] > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 abs_epsilon = _mm_set1_ps(fabsf(epsilon));\n    __m128i zero = _mm_set1_epi8(0);\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_andnot_ps(_mm_castsi128_ps(_mm_set1_epi32(0x80000000)), diff);\n\n        __m128 greater = _mm_cmpgt_ps(diff, abs_epsilon);\n        __m128 less = _mm_cmplt_ps(diff, _mm_sub_ps(_mm_setzero_ps(), abs_epsilon));\n\n        __m128i mask_greater = _mm_castps_si128(greater);\n        __m128i mask_less = _mm_castps_si128(less);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mask_greater, one),\n            _mm_and_si128(mask_less, minus_one)\n        );\n\n        __m128i final_result = _mm_or_si128(\n            _mm_and_si128(_mm_xor_si128(mask_greater, mask_less), zero),\n            result\n        );\n\n        _mm_storel_epi64((__m128i*)(out + i), final_result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 abs_epsilon = _mm_set1_ps(fabsf(epsilon));\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 neg_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_set1_ps(0.0f);\n\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_andnot_ps(_mm_castsi128_ps(_mm_cmplt_epi32(_mm_setzero_si128(), _mm_castps_si128(diff))), diff);\n\n        __m128 greater = _mm_cmpgt_ps(diff, abs_epsilon);\n        __m128 less = _mm_cmplt_ps(diff, _mm_sub_ps(zero, abs_epsilon));\n\n        __m128 result = _mm_or_ps(\n            _mm_and_ps(greater, one),\n            _mm_and_ps(less, neg_one)\n        );\n\n        __m128i result_int = _mm_cvtps_epi32(result);\n        __m128i packed_result = _mm_packs_epi32(result_int, result_int);\n        __m128i final_result = _mm_packus_epi16(packed_result, packed_result);\n\n        _mm_storel_epi64((__m128i*)&out[i], final_result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m128 abs_epsilon = _mm_set1_ps(fabsf(epsilon));\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi8(1);\n    __m128i minus_one = _mm_set1_epi8(-1);\n\n    size_t i = 0;\n    for (; i + 4 <= length; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 diff = _mm_sub_ps(a, b);\n        __m128 abs_diff = _mm_and_ps(_mm_sub_ps(_mm_max_ps(diff, _mm_setzero_ps()), _mm_min_ps(diff, _mm_setzero_ps())), _mm_cmpgt_ps(diff, abs_epsilon));\n\n        __m128i cmp_gt = _mm_castps_si128(_mm_cmpgt_ps(diff, abs_epsilon));\n        __m128i cmp_lt = _mm_castps_si128(_mm_cmplt_ps(diff, _mm_sub_ps(_mm_setzero_ps(), abs_epsilon)));\n\n        __m128i mask_gt = _mm_and_si128(cmp_gt, one);\n        __m128i mask_lt = _mm_and_si128(cmp_lt, minus_one);\n        __m128i result = _mm_or_si128(mask_gt, mask_lt);\n\n        __m128i packed_result = _mm_packs_epi16(result, zero);\n        _mm_storel_epi64((__m128i *)(out + i), packed_result);\n    }\n\n    for (; i < length; ++i) {\n        float diff = A[i] - B[i];\n        if (diff > fabsf(epsilon)) {\n            out[i] = 1;\n        } else if (diff < -fabsf(epsilon)) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_f = _mm_set1_ps(INT32_MAX);\n    const __m128 min_f = _mm_set1_ps(INT32_MIN);\n    const size_t aligned_size = size / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(A + i);\n        __m128 clamped = _mm_max_ps(vec, min_f);\n        clamped = _mm_min_ps(clamped, max_f);\n        __m128i result = _mm_cvtps_epi32(clamped);\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) val = INT32_MAX;\n        else if (val < INT32_MIN) val = INT32_MIN;\n        B[i] = (int32_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_float = _mm_set1_ps(INT32_MAX);\n    const __m128 min_float = _mm_set1_ps(INT32_MIN);\n    const size_t aligned_size = size / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(A + i);\n        vec = _mm_max_ps(vec, min_float);\n        vec = _mm_min_ps(vec, max_float);\n        __m128i result = _mm_cvtps_epi32(vec);\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) val = INT32_MAX;\n        else if (val < INT32_MIN) val = INT32_MIN;\n        B[i] = (int32_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_f = _mm_set1_ps(INT32_MAX);\n    const __m128 min_f = _mm_set1_ps(INT32_MIN);\n    const __m128i max_i = _mm_set1_epi32(INT32_MAX);\n    const __m128i min_i = _mm_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(A + i);\n        __m128 clamped = _mm_max_ps(_mm_min_ps(vec, max_f), min_f);\n        __m128i result = _mm_cvtps_epi32(clamped);\n        result = _mm_max_epi32(result, min_i);\n        result = _mm_min_epi32(result, max_i);\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_f = _mm_set1_ps(INT32_MAX);\n    const __m128 min_f = _mm_set1_ps(INT32_MIN);\n    const __m128i max_i = _mm_set1_epi32(INT32_MAX);\n    const __m128i min_i = _mm_set1_epi32(INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128 vec = _mm_loadu_ps(A + i);\n        __m128 clamped = _mm_max_ps(vec, min_f);\n        clamped = _mm_min_ps(clamped, max_f);\n        __m128i converted = _mm_cvtps_epi32(clamped);\n        __m128i saturated = _mm_max_epi32(converted, min_i);\n        saturated = _mm_min_epi32(saturated, max_i);\n        _mm_storeu_si128((__m128i *)(B + i), saturated);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = static_cast<int32_t>(val);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <limits.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    const __m128 max_f = _mm_set1_ps((float)INT32_MAX);\n    const __m128 min_f = _mm_set1_ps((float)INT32_MIN);\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(A + i);\n        vec = _mm_max_ps(vec, min_f);\n        vec = _mm_min_ps(vec, max_f);\n        __m128i result = _mm_cvtps_epi32(vec);\n        _mm_storeu_si128((__m128i *)(B + i), result);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i];\n        if (val > INT32_MAX) val = INT32_MAX;\n        else if (val < INT32_MIN) val = INT32_MIN;\n        B[i] = (int32_t)val;\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    size_t i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i v_int = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i v_scaled = _mm_mullo_epi32(v_int, v_scale);\n\n        __m128 v_low = _mm_cvtepi32_ps(_mm_unpacklo_epi32(v_scaled, _mm_setzero_si128()));\n        __m128 v_high = _mm_cvtepi32_ps(_mm_unpackhi_epi32(v_scaled, _mm_setzero_si128()));\n\n        __m128 v_result = _mm_shuffle_ps(v_low, v_high, _MM_SHUFFLE(2, 0, 2, 0));\n        _mm_storeu_ps(B + i, v_result);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128i v_int = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i v_scaled = _mm_mullo_epi32(v_int, v_scale);\n        __m128 v_float = _mm_cvtepi32_ps(v_scaled);\n        _mm_storeu_ps(B + i, v_float);\n    }\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128i v_int = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128i v_scaled = _mm_mullo_epi32(v_int, v_scale);\n        __m128 v_float = _mm_cvtepi32_ps(v_scaled);\n        _mm_storeu_ps(B + i, v_float);\n    }\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i v_scale = _mm_set1_epi32(scale);\n    size_t i = 0;\n\n    for (; i <= size - 4; i += 4) {\n        __m128i v_int = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i v_scaled = _mm_mullo_epi32(v_int, v_scale);\n\n        // Extract high and low parts of the 64-bit results\n        __m128i v_low = _mm_cvtepi32_ps(_mm_unpacklo_epi32(v_scaled, _mm_setzero_si128()));\n        __m128i v_high = _mm_cvtepi32_ps(_mm_unpackhi_epi32(v_scaled, _mm_setzero_si128()));\n\n        __m128 v_float_low = _mm_cvtepi32_ps(_mm_shuffle_epi32(v_low, _MM_SHUFFLE(0, 0, 0, 0)));\n        __m128 v_float_high = _mm_cvtepi32_ps(_mm_shuffle_epi32(v_high, _MM_SHUFFLE(0, 0, 0, 0)));\n\n        __m128 v_combined = _mm_shuffle_ps(v_float_low, v_float_high, _MM_SHUFFLE(2, 0, 2, 0));\n        _mm_storeu_ps(B + i, v_combined);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (float)(A[i] * scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m128i v_scale = _mm_set1_epi8(scale);\n    __m128 v_scale_f = _mm_cvtepi32_ps(_mm_cvtepi8_epi32(v_scale));\n    \n    size_t i = 0;\n    for (; i <= size - 4; i += 4) {\n        __m128i v_int = _mm_loadu_si128((__m128i const *)(A + i));\n        __m128 v_float = _mm_cvtepi32_ps(v_int);\n        __m128 result = _mm_mul_ps(v_float, v_scale_f);\n        _mm_storeu_ps(B + i, result);\n    }\n    \n    for (; i < size; ++i) {\n        B[i] = A[i] * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 half = _mm_set1_ps(0.5f);\n    __m128 neg_half = _mm_set1_ps(-0.5f);\n\n    for (; i <= size - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        \n        // Round to nearest integer\n        __m128 pos_mask = _mm_cmpge_ps(div, _mm_setzero_ps());\n        __m128 rounded = _mm_add_ps(_mm_and_ps(pos_mask, half), _mm_andnot_ps(pos_mask, neg_half));\n        __m128 result = _mm_add_ps(div, rounded);\n        \n        // Convert to integer\n        __m128i res_int = _mm_cvtps_epi32(result);\n        _mm_storeu_si128((__m128i*)&C[i], res_int);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        float div = A[i] / B[i];\n        C[i] = (int32_t)(div + (div >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    for (; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128 rounded = _mm_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m128i result = _mm_cvtps_epi32(rounded);\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n    for (; i < size; ++i) {\n        C[i] = (int32_t)lroundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n\n    for (; i + 4 <= size; i += 4) {\n        __m128 a = _mm_loadu_ps(A + i);\n        __m128 b = _mm_loadu_ps(B + i);\n        __m128 div = _mm_div_ps(a, b);\n        __m128 rounded = _mm_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m128i result = _mm_cvtps_epi32(rounded);\n        result = _mm_and_si128(result, mask); // Clamp to 32-bit range\n        _mm_storeu_si128((__m128i *)(C + i), result);\n    }\n\n    for (; i < size; ++i) {\n        float val = A[i] / B[i];\n        C[i] = static_cast<int32_t>(val + (val >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n\n    for (; i + 3 < size; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 result = _mm_div_ps(a, b);\n        __m128 rounded = _mm_round_ps(result, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m128i converted = _mm_cvtps_epi32(rounded);\n        __m128i clamped = _mm_and_si128(converted, mask);\n        _mm_storeu_si128((__m128i*)&C[i], clamped);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <smmintrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 zero = _mm_setzero_ps();\n    __m128i mask = _mm_set1_epi32(0x7FFFFFFF);\n\n    for (; i <= size - 4; i += 4) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 div = _mm_div_ps(a, b);\n        __m128 rounded = _mm_round_ps(div, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m128i result = _mm_cvtps_epi32(rounded);\n        result = _mm_and_si128(result, mask); // Clamp to 32-bit integer range\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    for (; i < size; ++i) {\n        C[i] = (int32_t)(A[i] / B[i] + 0.5f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n#include <math.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d max_double = _mm_set1_pd((double)FLT_MAX);\n    const __m128d min_double = _mm_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 1 <= size; i += 2) {\n        __m128d vec = _mm_loadu_pd(A + i);\n        __m128d clamped_max = _mm_min_pd(vec, max_double);\n        __m128d clamped = _mm_max_pd(clamped_max, min_double);\n        \n        // Convert to float\n        __m128 converted = _mm_cvtpd_ps(clamped);\n        \n        // Store the result\n        _mm_storeu_ps(B + i, converted);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d max_d = _mm_set1_pd((double)FLT_MAX);\n    const __m128d min_d = _mm_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128d vec_double = _mm_loadu_pd(A + i);\n        __m128d vec_clamped = _mm_max_pd(vec_double, min_d);\n        vec_clamped = _mm_min_pd(vec_clamped, max_d);\n\n        __m128 vec_float = _mm_cvtpd_ps(vec_clamped);\n        __m128d high_double = _mm_unpackhi_pd(vec_double, vec_double);\n        __m128 high_float = _mm_cvtpd_ps(high_double);\n\n        __m128 combined = _mm_shuffle_ps(vec_float, high_float, _MM_SHUFFLE(2, 3, 0, 1));\n        _mm_storeu_ps(B + i, combined);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d max_d = _mm_set1_pd((double)FLT_MAX);\n    const __m128d min_d = _mm_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128d vec_double = _mm_loadu_pd(&A[i]);\n        __m128d vec_clamped = _mm_max_pd(_mm_min_pd(vec_double, max_d), min_d);\n        \n        __m128 vec_float = _mm_cvtpd_ps(vec_clamped);\n        \n        // Handle the remaining two elements of the quadword\n        __m128d vec_double_high = _mm_unpackhi_pd(vec_double, vec_double);\n        __m128d vec_clamped_high = _mm_max_pd(_mm_min_pd(vec_double_high, max_d), min_d);\n        __m128 vec_float_high = _mm_cvtpd_ps(vec_clamped_high);\n        \n        __m128 combined = _mm_shuffle_ps(vec_float, vec_float_high, _MM_SHUFFLE(3, 2, 1, 0));\n        _mm_storeu_ps(&B[i], combined);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d max_d = _mm_set1_pd((double)FLT_MAX);\n    const __m128d min_d = _mm_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 1 <= size; i += 2) {\n        __m128d vec_d = _mm_loadu_pd(&A[i]);\n        __m128d clamped_max = _mm_min_pd(vec_d, max_d);\n        __m128d clamped_min = _mm_max_pd(clamped_max, min_d);\n        __m128 vec_f = _mm_cvtpd_ps(clamped_min);\n        _mm_storeu_ps(&B[i], vec_f);\n    }\n\n    // Handle remaining elements (if any)\n    if (i < size) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <float.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    const __m128d max_double = _mm_set1_pd((double)FLT_MAX);\n    const __m128d min_double = _mm_set1_pd(-(double)FLT_MAX);\n\n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128d vec_double = _mm_loadu_pd(&A[i]);\n        __m128d vec_double_clamped = _mm_max_pd(vec_double, min_double);\n        vec_double_clamped = _mm_min_pd(vec_double_clamped, max_double);\n\n        __m128 vec_float = _mm_cvtpd_ps(vec_double_clamped);\n        __m128d vec_double_high = _mm_unpackhi_pd(vec_double_clamped, vec_double_clamped);\n        __m128 vec_float_high = _mm_cvtpd_ps(vec_double_high);\n\n        __m128 result = _mm_movelh_ps(vec_float, vec_float_high);\n        _mm_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; ++i) {\n        double val = A[i];\n        if (val > FLT_MAX) val = FLT_MAX;\n        else if (val < -FLT_MAX) val = -FLT_MAX;\n        B[i] = (float)val;\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i <= size - 8; i += 8) {\n        __m128i low = _mm_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i)));\n        __m128i high = _mm_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i + 4)));\n        _mm_storeu_si128((__m128i *)(B + i), low);\n        _mm_storeu_si128((__m128i *)(B + i + 4), high);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i data = _mm_loadu_si128((const __m128i*)&A[i]);\n        __m128i converted_low = _mm_cvtepi16_epi32(data);\n        __m128i converted_high = _mm_cvtepi16_epi32(_mm_unpackhi_epi64(data, data));\n        _mm_storeu_si128((__m128i*)&B[i], converted_low);\n        _mm_storeu_si128((__m128i*)&B[i + 4], converted_high);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const __m128i zero = _mm_setzero_si128();\n    size_t i = 0;\n    for (; i + 7 < size; i += 8) {\n        __m128i low = _mm_cvtepi16_epi32(_mm_loadu_si128((__m128i const*)(A + i)));\n        __m128i high = _mm_cvtepi16_epi32(_mm_loadu_si128((__m128i const*)(A + i + 4)));\n        _mm_storeu_si128((__m128i *)(B + i), low);\n        _mm_storeu_si128((__m128i *)(B + i + 4), high);\n    }\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t simd_width = 8;\n    size_t i = 0;\n\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i v_low = _mm_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i)));\n        __m128i v_high = _mm_cvtepi16_epi32(_mm_loadu_si128((__m128i const *)(A + i + 4)));\n\n        _mm_storeu_si128((__m128i *)(B + i), v_low);\n        _mm_storeu_si128((__m128i *)(B + i + 4), v_high);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    const size_t step = 8; // Process 8 elements per iteration (4 for each 128-bit register)\n    size_t i = 0;\n\n    for (; i <= size - step; i += step) {\n        __m128i low = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i));\n        __m128i high = _mm_loadu_si128(reinterpret_cast<const __m128i*>(A + i + 4));\n\n        __m128i converted_low = _mm_cvtepi16_epi32(low);\n        __m128i converted_high = _mm_cvtepi16_epi32(high);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(B + i), converted_low);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(B + i + 4), converted_high);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    __m128 vrange = _mm_sub_ps(vmax, vmin);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128i vcontrol = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 vcond = _mm_castsi128_ps(_mm_cmpgt_epi32(vcontrol, _mm_setzero_si128()));\n\n        // Normalize when control > 0\n        __m128 vnorm = _mm_div_ps(_mm_sub_ps(va, vmin), vrange);\n        vnorm = _mm_max_ps(vnorm, zero);\n        vnorm = _mm_min_ps(vnorm, one);\n\n        // Select between normalized value and original value based on control\n        __m128 vresult = _mm_or_ps(_mm_and_ps(vcond, vnorm), _mm_andnot_ps(vcond, va));\n        _mm_storeu_ps(&B[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    __m128 vrange = _mm_sub_ps(vmax, vmin);\n    __m128 vzero = _mm_setzero_ps();\n    __m128 vone = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128i vcontrol = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 vcond = _mm_castsi128_ps(_mm_cmpgt_epi32(vcontrol, _mm_setzero_si128()));\n\n        __m128 vnorm = _mm_div_ps(_mm_sub_ps(va, vmin), vrange);\n        __m128 vclamped = _mm_max_ps(_mm_min_ps(vnorm, vone), vzero);\n        __m128 vresult = _mm_or_ps(_mm_and_ps(vcond, vclamped), _mm_andnot_ps(vcond, va));\n\n        _mm_storeu_ps(&B[i], vresult);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    const __m128 one = _mm_set1_ps(1.0f);\n    const __m128 zero = _mm_set1_ps(0.0f);\n    const __m128 min_val_vec = _mm_set1_ps(min_val);\n    const __m128 max_val_vec = _mm_set1_ps(max_val);\n    const __m128 range = _mm_set1_ps(max_val - min_val);\n\n    for (size_t i = 0; i <= size - 4; i += 4) {\n        __m128 a_vals = _mm_loadu_ps(&A[i]);\n        __m128i ctrl_vals = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 mask = _mm_castsi128_ps(_mm_cmpgt_epi32(ctrl_vals, _mm_setzero_si128()));\n\n        // Normalize values\n        __m128 normalized = _mm_div_ps(_mm_sub_ps(a_vals, min_val_vec), range);\n        normalized = _mm_max_ps(normalized, zero);\n        normalized = _mm_min_ps(normalized, one);\n\n        // Select between normalized and original based on control\n        __m128 result = _mm_or_ps(_mm_and_ps(mask, normalized), _mm_andnot_ps(mask, a_vals));\n\n        _mm_storeu_ps(&B[i], result);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size / 4) * 4; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = (val < 0.0f) ? 0.0f : ((val > 1.0f) ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    __m128 vrange = _mm_sub_ps(vmax, vmin);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i + 4 <= size; i += 4) {\n        __m128 va = _mm_loadu_ps(&A[i]);\n        __m128i vcontrol = _mm_loadu_si128((__m128i*)&control[i]);\n        __m128 vcond = _mm_castsi128_ps(_mm_cmpgt_epi32(vcontrol, _mm_setzero_si128()));\n\n        // Normalize: (va - vmin) / vrange\n        __m128 vnorm = _mm_div_ps(_mm_sub_ps(va, vmin), vrange);\n\n        // Clamp to [0, 1]\n        __m128 vclamp = _mm_max_ps(zero, _mm_min_ps(one, vnorm));\n\n        // Select between normalized and original based on control\n        __m128 vresult = _mm_or_ps(_mm_and_ps(vcond, vclamp), _mm_andnot_ps(vcond, va));\n\n        _mm_storeu_ps(&B[i], vresult);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size / 4) * 4; i < size; ++i) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = (val < 0.0f) ? 0.0f : ((val > 1.0f) ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    __m128 vrange = _mm_sub_ps(vmax, vmin);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m128 va = _mm_loadu_ps(A + i);\n        __m128i vcontrol = _mm_loadu_si128((__m128i const*)(control + i));\n        __m128 vcond = _mm_castsi128_ps(_mm_cmpgt_epi32(vcontrol, _mm_setzero_si128()));\n\n        __m128 normalized = _mm_div_ps(_mm_sub_ps(va, vmin), vrange);\n        __m128 clamped = _mm_max_ps(zero, _mm_min_ps(normalized, one));\n        __m128 result = _mm_or_ps(_mm_and_ps(vcond, clamped), _mm_andnot_ps(vcond, va));\n\n        _mm_storeu_ps(B + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    __m128i mask = _mm_set1_epi32(0xFFFF0000);\n\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i val = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i shifted = _mm_and_si128(val, mask);\n        __m128 converted = _mm_cvtepi32_ps(_mm_srai_epi32(shifted, 16));\n        __m128 sqrt_result = _mm_sqrt_ps(converted);\n        _mm_storeu_ps(B + i, sqrt_result);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t v = A[i];\n        uint32_t high_word = (v >> 16) & 0xFFFF;\n        float fval = static_cast<float>(high_word);\n        B[i] = std::sqrtf(fval);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    for (; i + simd_width <= size; i += simd_width) {\n        __m128i v_int = _mm_loadu_si128((__m128i const*)&A[i]);\n        __m128i v_high = _mm_srli_epi32(v_int, 16);\n        __m128 v_float = _mm_cvtepi32_ps(v_high);\n        __m128 v_sqrt = _mm_sqrt_ps(v_float);\n        _mm_storeu_ps(&B[i], v_sqrt);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t val = A[i] >> 16;\n        B[i] = static_cast<float>(val);\n        B[i] = sqrtf(B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t vec_size = 4;\n    size_t i = 0;\n\n    __m128i mask = _mm_set1_epi32(0xFFFF0000);\n\n    for (; i <= size - vec_size; i += vec_size) {\n        __m128i loaded = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i shifted = _mm_srli_epi32(loaded, 16);\n        __m128 converted = _mm_cvtepi32_ps(shifted);\n        __m128 sqrted = _mm_sqrt_ps(converted);\n        _mm_storeu_ps(B + i, sqrted);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t val = A[i] >> 16;\n        B[i] = sqrtf((float)val);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const __m128i mask = _mm_set1_epi32(0xFFFF0000);\n    size_t i = 0;\n\n    for (; i + 4 <= size; i += 4) {\n        __m128i val = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i high_bits = _mm_and_si128(val, mask);\n        __m128 as_float = _mm_cvtepi32_ps(_mm_srai_epi32(high_bits, 16));\n        __m128 sqrt_result = _mm_sqrt_ps(as_float);\n        _mm_storeu_ps(&B[i], sqrt_result);\n    }\n\n    for (; i < size; i++) {\n        uint32_t high_bits = (A[i] >> 16);\n        B[i] = sqrtf((float)high_bits);\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i v_int = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i v_high = _mm_srli_epi32(v_int, 16);\n        __m128 v_float = _mm_cvtepi32_ps(v_high);\n        __m128 v_sqrt = _mm_sqrt_ps(v_float);\n        _mm_storeu_ps(B + i, v_sqrt);\n    }\n\n    for (; i < size; ++i) {\n        uint32_t val = A[i] >> 16;\n        B[i] = static_cast<float>(val);\n        B[i] = sqrtf(B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    // Process in SIMD chunks\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i int_data = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 float_data = _mm_castsi128_ps(int_data);\n        _mm_storeu_ps(B + i, float_data);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        *(reinterpret_cast<float*>(B + i)) = reinterpret_cast<const float*>(A + i)[0];\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    // Process elements in batches of 4 using SSE\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i int_data = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 float_data = _mm_castsi128_ps(int_data);\n        _mm_storeu_ps(B + i, float_data);\n    }\n\n    // Handle any remaining elements\n    for (; i < size; ++i) {\n        *(reinterpret_cast<float*>(B + i)) = reinterpret_cast<const float*>(A + i)[0];\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    // Process in SIMD chunks\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i int_data = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 float_data = _mm_castsi128_ps(int_data);\n        _mm_storeu_ps(B + i, float_data);\n    }\n\n    // Handle remaining elements (scalar fallback)\n    for (; i < size; ++i) {\n        *(reinterpret_cast<float*>(B + i)) = reinterpret_cast<const float*>(A + i)[0];\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i int_values = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128 float_values = _mm_castsi128_ps(int_values);\n        _mm_storeu_ps(&B[i], float_values);\n    }\n\n    for (; i < size; ++i) {\n        B[i] = *(float*)&A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    // Process in SIMD chunks\n    for (; i <= size - simd_width; i += simd_width) {\n        __m128i int_data = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128 float_data = _mm_castsi128_ps(int_data);\n        _mm_storeu_ps(B + i, float_data);\n    }\n\n    // Handle remaining elements (if any)\n    for (; i < size; ++i) {\n        *(reinterpret_cast<float*>(B + i)) = reinterpret_cast<const float*>(A + i)[0];\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    // Set the first element to 0.0f\n    diff[0] = 0.0f;\n\n    if (size == 1) return;\n\n    const __m128i zero_i = _mm_setzero_si128();\n    const __m128 zero_f = _mm_setzero_ps();\n\n    for (size_t i = 1; i < size; i += 4) {\n        size_t j = i + 4;\n        if (j > size) j = size;\n\n        __m128i current = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i prev = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        __m128i diff_i = _mm_sub_epi32(current, prev);\n\n        __m128 diff_f = _mm_cvtepi32_ps(diff_i);\n\n        __m128 mask = _mm_set_ps(\n            (j > i + 3 && cond[i + 3]) ? 1.0f : 0.0f,\n            (j > i + 2 && cond[i + 2]) ? 1.0f : 0.0f,\n            (j > i + 1 && cond[i + 1]) ? 1.0f : 0.0f,\n            (cond[i]) ? 1.0f : 0.0f\n        );\n\n        diff_f = _mm_and_ps(diff_f, mask);\n\n        _mm_storeu_ps(&diff[i], diff_f);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    __m128i vzero = _mm_setzero_si128();\n    __m128 fzero = _mm_setzero_ps();\n    diff[0] = 0.0f;\n\n    size_t i = 1;\n    for (; i + 3 < size; i += 4) {\n        __m128i vA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vA_prev = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        __m128i vdiff = _mm_sub_epi32(vA, vA_prev);\n\n        __m128 vdiff_f = _mm_cvtepi32_ps(vdiff);\n\n        __m128i vcond = _mm_loadu_si128((__m128i*)&cond[i]);\n        __m128i vmask = _mm_and_si128(_mm_cmpeq_epi32(vcond, vzero), _mm_set1_epi32(0xFFFFFFFF));\n        __m128 vmask_f = _mm_castsi128_ps(vmask);\n\n        __m128 result = _mm_or_ps(_mm_and_ps(vmask_f, fzero), _mm_andnot_ps(vmask_f, vdiff_f));\n\n        _mm_storeu_ps(&diff[i], result);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    diff[0] = 0.0f;\n    if (size == 1) return;\n\n    const __m128i zero = _mm_setzero_si128();\n    const __m128 zero_f = _mm_setzero_ps();\n\n    size_t i = 1;\n    for (; i + 3 < size; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i const*)(A + i));\n        __m128i prev = _mm_loadu_si128((__m128i const*)(A + i - 1));\n\n        __m128i diff_i = _mm_sub_epi32(current, prev);\n\n        __m128 diff_f = _mm_cvtepi32_ps(diff_i);\n\n        __m128i cond_i = _mm_loadu_si128((__m128i const*)(cond + i));\n        __m128 cond_f = _mm_and_ps(_mm_castsi128_ps(cond_i), _mm_castsi128_ps(_mm_cmpeq_epi32(cond_i, _mm_set1_epi32(1))));\n\n        diff_f = _mm_and_ps(diff_f, cond_f);\n        diff_f = _mm_or_ps(diff_f, _mm_andnot_ps(cond_f, zero_f));\n\n        _mm_storeu_ps(diff + i, diff_f);\n    }\n\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = (float)(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    __m128i vzero = _mm_set1_epi32(0);\n    __m128 fzero = _mm_set1_ps(0.0f);\n\n    // Set first element to 0.0f\n    diff[0] = 0.0f;\n\n    size_t i = 1;\n    for (; i + 3 < size; i += 4) {\n        __m128i vA_curr = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vA_prev = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        __m128i vdiff = _mm_sub_epi32(vA_curr, vA_prev);\n\n        // Load condition array as integer mask\n        __m128i vcond = _mm_loadu_si128((__m128i*)&cond[i]);\n        __m128i vmask = _mm_and_si128(vcond, _mm_set1_epi32(-1));\n\n        // Apply mask to differences\n        vdiff = _mm_or_si128(_mm_and_si128(vdiff, vmask), _mm_andnot_si128(vmask, vzero));\n\n        // Convert to float\n        __m128 vf_diff = _mm_cvtepi32_ps(_mm_castsi128_epi32(vdiff));\n\n        // Store result\n        _mm_storeu_ps(&diff[i], vf_diff);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        diff[i] = cond[i] ? static_cast<float>(A[i] - A[i - 1]) : 0.0f;\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    if (size == 0) return;\n\n    // Set the first element to 0.0f\n    diff[0] = 0.0f;\n\n    if (size == 1) return;\n\n    __m128i vone = _mm_set1_epi32(1);\n    __m128 zero = _mm_setzero_ps();\n\n    for (size_t i = 1; i < size; i += 4) {\n        size_t j = i + 4;\n        if (j > size) j = size;\n\n        __m128i vA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vA_prev = _mm_loadu_si128((__m128i*)&A[i - 1]);\n\n        __m128i vdiff = _mm_sub_epi32(vA, vA_prev);\n\n        __m128 vdiff_f = _mm_cvtepi32_ps(vdiff);\n\n        __m128i vcond = _mm_loadu_si128((__m128i*)&cond[i]);\n        __m128i vcond_mask = _mm_and_si128(vcond, vone);\n        __m128 vcond_mask_f = _mm_castsi128_ps(vcond_mask);\n\n        __m128 result = _mm_or_ps(_mm_and_ps(vdiff_f, vcond_mask_f), _mm_andnot_ps(vcond_mask_f, zero));\n\n        _mm_storeu_ps(&diff[i], result);\n\n        i += 3;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m128i high_bits = _mm_set1_epi32(0xFFFFFFFF); // Set high 32 bits to 1\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i low = _mm_loadu_si128((__m128i*)&src[i]); // Load 4 uint32_t values\n        __m128i high = high_bits; // Broadcast the high_bits mask\n\n        __m128i low64 = _mm_cvtepu32_epi64(low); // Extend low 4x32-bit to 2x64-bit\n        __m128i high64 = _mm_cvtepu32_epi64(_mm_shuffle_epi32(low, _MM_SHUFFLE(3, 2, 3, 2))); // Extend high 4x32-bit to 2x64-bit\n\n        __m128i result_low = _mm_or_si128(low64, _mm_slli_si128(high64, 8)); // Combine low with shifted high\n        __m128i result_high = _mm_or_si128(_mm_srli_si128(low64, 8), high64); // Combine shifted low with high\n\n        _mm_storeu_si128((__m128i*)&dst[i], result_low); // Store first 2x64-bit\n        _mm_storeu_si128((__m128i*)&dst[i + 2], result_high); // Store second 2x64-bit\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (uint64_t)src[i] | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const size_t simd_width = 4;\n    size_t i = 0;\n\n    __m128i high_bits = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (; i <= length - simd_width; i += simd_width) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i low = _mm_cvtepi32_epi64(src_vec);\n        __m128i high = _mm_cvtepi32_epi64(_mm_and_si128(src_vec, high_bits));\n        high = _mm_or_si128(high, _mm_slli_epi64(_mm_set1_epi32(-1), 32));\n        _mm_storeu_si128((__m128i*)&dst[i], low);\n        _mm_storeu_si128((__m128i*)&dst[i + 2], high);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m128i high_bits = _mm_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i low = _mm_loadu_si128((__m128i const*)(src + i));\n        __m128i high = _mm_and_si128(high_bits, _mm_set1_epi32(-1));\n        __m128i combined = _mm_unpacklo_epi32(low, high);\n        _mm_storeu_si128((__m128i *)(dst + i), combined);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m128i high_bits = _mm_set1_epi32(0xFFFFFFFF); // Set all high 32 bits to 1\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i low = _mm_loadu_si128((__m128i const*)(src + i)); // Load 4 x 32-bit values\n        __m128i high = _mm_and_si128(high_bits, _mm_srli_si128(low, 0)); // Broadcast high bits\n        __m128i combined_low = _mm_unpacklo_epi32(low, high); // Interleave low and high (lower 64-bit)\n        __m128i combined_high = _mm_unpackhi_epi32(low, high); // Interleave low and high (upper 64-bit)\n\n        __m128i result = _mm_unpacklo_epi64(combined_low, combined_high); // Combine into 128-bit result\n        _mm_storeu_si128((__m128i *)(dst + i), result); // Store the result\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    const __m128i high_bits = _mm_set1_epi32(0xFFFFFFFF);\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i low_part = _mm_cvtepu32_epi64(src_vec);\n        __m128i high_part = _mm_cvtepu32_epi64(_mm_shuffle_epi32(src_vec, _MM_SHUFFLE(3, 2, 1, 0)));\n        __m128i high_bits_low = _mm_unpacklo_epi32(high_bits, high_bits);\n        __m128i high_bits_high = _mm_unpackhi_epi32(high_bits, high_bits);\n        low_part = _mm_or_si128(low_part, _mm_slli_si128(high_bits_low, 4));\n        high_part = _mm_or_si128(high_part, _mm_slli_si128(high_bits_high, 4));\n        _mm_storeu_si128((__m128i*)&dst[i * 2], low_part);\n        _mm_storeu_si128((__m128i*)&dst[i * 2 + 2], high_part);\n    }\n\n    for (; i < length; ++i) {\n        dst[i] = (uint64_t)src[i] | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    // Process 2 indices per iteration using SSE2\n    for (; i + 1 < length; i += 2) {\n        __m128i idx = _mm_loadu_si128((__m128i const *)&index[i]);\n        __m128i idx_lo = _mm_unpacklo_epi16(idx, _mm_setzero_si128());\n        __m128i idx_hi = _mm_unpackhi_epi16(idx, _mm_setzero_si128());\n\n        __m128d val_lo = _mm_set_pd(vec[_mm_extract_epi32(idx_lo, 1)], vec[_mm_extract_epi32(idx_lo, 0)]);\n        __m128d val_hi = _mm_set_pd(vec[_mm_extract_epi32(idx_hi, 1)], vec[_mm_extract_epi32(idx_hi, 0)]);\n\n        __m128d vals = _mm_add_pd(val_lo, val_hi);\n        sum = _mm_add_pd(sum, vals);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        double temp = vec[index[i]];\n        sum = _mm_add_sd(sum, _mm_load_sd(&temp));\n    }\n\n    // Horizontal add of the two elements in sum\n    __m128d high = _mm_shuffle_pd(sum, sum, 1);\n    sum = _mm_add_sd(sum, high);\n\n    double result;\n    _mm_store_sd(&result, sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v1 = _mm_set_sd(vec[_mm_extract_epi16(idx, 0)]);\n        __m128d v2 = _mm_set_sd(vec[_mm_extract_epi16(idx, 1)]);\n        __m128d v3 = _mm_set_sd(vec[_mm_extract_epi16(idx, 2)]);\n        __m128d v4 = _mm_set_sd(vec[_mm_extract_epi16(idx, 3)]);\n        \n        __m128d vs = _mm_add_pd(_mm_add_pd(v1, v2), _mm_add_pd(v3, v4));\n        sum = _mm_add_pd(sum, vs);\n    }\n\n    for (; i < length; ++i) {\n        double val = vec[index[i]];\n        __m128d temp = _mm_load_sd(&val);\n        sum = _mm_add_pd(sum, temp);\n    }\n\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v1 = _mm_set_pd(vec[_mm_extract_epi16(idx, 3)], vec[_mm_extract_epi16(idx, 2)]);\n        __m128d v2 = _mm_set_pd(vec[_mm_extract_epi16(idx, 1)], vec[_mm_extract_epi16(idx, 0)]);\n        __m128d v = _mm_add_pd(v1, v2);\n        sum = _mm_add_pd(sum, v);\n    }\n\n    double result = 0.0;\n    result += ((double*)&sum)[0];\n    result += ((double*)&sum)[1];\n\n    for (; i < length; ++i) {\n        if (index[i] >= 0 && index[i] < (int16_t)length) {\n            result += vec[index[i]];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 4 <= length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i const*)(index + i));\n        __m128d v1 = _mm_set_pd(vec[_mm_extract_epi16(idx, 3)], vec[_mm_extract_epi16(idx, 2)]);\n        __m128d v2 = _mm_set_pd(vec[_mm_extract_epi16(idx, 1)], vec[_mm_extract_epi16(idx, 0)]);\n        __m128d vs = _mm_add_pd(v1, v2);\n        sum = _mm_add_pd(sum, vs);\n    }\n\n    double scalar_sum = 0.0;\n    for (; i < length; ++i) {\n        scalar_sum += vec[index[i]];\n    }\n\n    sum = _mm_hadd_pd(sum, sum);\n    double result[2];\n    _mm_storeu_pd(result, sum);\n\n    return result[0] + result[1] + scalar_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    for (; i + 3 < length; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v1 = _mm_set_pd(vec[_mm_extract_epi16(idx, 3)], vec[_mm_extract_epi16(idx, 2)]);\n        __m128d v2 = _mm_set_pd(vec[_mm_extract_epi16(idx, 1)], vec[_mm_extract_epi16(idx, 0)]);\n        __m128d v = _mm_add_pd(v1, v2);\n        sum = _mm_add_pd(sum, v);\n    }\n\n    double result = 0.0;\n    for (; i < length; ++i) {\n        if (index[i] >= 0 && index[i] < (int16_t)length) {\n            result += vec[index[i]];\n        }\n    }\n\n    __m128d lo = sum;\n    __m128d hi = _mm_shuffle_pd(sum, sum, 1);\n    __m128d total = _mm_add_sd(lo, hi);\n    result += _mm_cvtsd_f64(total);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d k0, k1, k2, k3;\n    __m128d zero = _mm_setzero_pd();\n\n    // Load kernel values into registers\n    k0 = _mm_loadu_pd(&kernel[0]);\n    k1 = _mm_loadu_pd(&kernel[2]);\n    k2 = _mm_loadu_pd(&kernel[4]);\n    k3 = _mm_loadu_pd(&kernel[6]);\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            \n            for (size_t p = 0; p < kernel_size; ++p) {\n                const double* inp_row = &input[(i + p) * input_size + j];\n                \n                __m128d row0 = _mm_loadu_pd(inp_row);\n                __m128d row1 = _mm_loadu_pd(inp_row + input_size);\n                __m128d row2 = _mm_loadu_pd(inp_row + 2 * input_size);\n                __m128d row3 = _mm_loadu_pd(inp_row + 3 * input_size);\n\n                __m128d prod0 = _mm_mul_pd(row0, k0);\n                __m128d prod1 = _mm_mul_pd(row1, k1);\n                __m128d prod2 = _mm_mul_pd(row2, k2);\n                __m128d prod3 = _mm_mul_pd(row3, k3);\n\n                __m128d temp = _mm_add_pd(prod0, prod1);\n                temp = _mm_add_pd(temp, prod2);\n                temp = _mm_add_pd(temp, prod3);\n\n                sum = _mm_add_pd(sum, temp);\n            }\n\n            sum = _mm_max_pd(sum, zero); // ReLU activation\n            _mm_storeu_pd(&output[(i * output_size + j / 2) * 2], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d zero = _mm_setzero_pd();\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            \n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    __m128d inp = _mm_loadu_pd(&input[(i + m) * input_size + j + n]);\n                    __m128d ker = _mm_set1_pd(kernel[m * kernel_size + n]);\n                    __m128d prod = _mm_mul_pd(inp, ker);\n                    sum = _mm_add_pd(sum, prod);\n                }\n            }\n            sum = _mm_max_pd(sum, zero);\n            _mm_storeu_pd(&output[(i * output_size + j / 2) * 2], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d k0 = _mm_loadu_pd(&kernel[0]);\n    __m128d k1 = _mm_loadu_pd(&kernel[2]);\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            \n            for (size_t p = 0; p < kernel_size; ++p) {\n                __m128d row_data = _mm_loadu_pd(&input[(i + p) * input_size + j]);\n                __m128d prod0 = _mm_mul_pd(row_data, k0);\n                __m128d prod1 = _mm_mul_pd(_mm_shuffle_pd(row_data, row_data, 1), k1);\n                sum = _mm_add_pd(sum, _mm_add_pd(prod0, prod1));\n                k0 = _mm_loadu_pd(&kernel[p * kernel_size + 2]);\n                k1 = _mm_loadu_pd(&kernel[p * kernel_size + 4]);\n            }\n            \n            sum = _mm_max_pd(sum, _mm_setzero_pd());\n            _mm_storeu_pd(&output[i * output_size + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d k0 = _mm_loadu_pd(&kernel[0]);\n    __m128d k1 = _mm_loadu_pd(&kernel[2]);\n    \n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t p = 0; p < kernel_size; ++p) {\n                size_t row_offset = (i + p) * input_size;\n                __m128d in0 = _mm_loadu_pd(&input[row_offset + j]);\n                __m128d in1 = _mm_loadu_pd(&input[row_offset + j + 2]);\n                __m128d prod0 = _mm_mul_pd(in0, k0);\n                __m128d prod1 = _mm_mul_pd(in1, k1);\n                sum = _mm_add_pd(sum, _mm_add_pd(prod0, prod1));\n                if ((j + 3) < output_size) {\n                    __m128d in2 = _mm_loadu_pd(&input[row_offset + j + 4]);\n                    __m128d prod2 = _mm_mul_pd(in2, k0);\n                    sum = _mm_add_pd(sum, prod2);\n                }\n            }\n            sum = _mm_hadd_pd(sum, sum);\n            double res[2];\n            _mm_storeu_pd(res, sum);\n            double out_val = (res[0] + res[1]) > 0 ? (res[0] + res[1]) : 0;\n            output[i * output_size + j] = out_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d k0 = _mm_loadu_pd(&kernel[0]);\n    __m128d k1 = _mm_loadu_pd(&kernel[2]);\n    __m128d k2 = _mm_loadu_pd(&kernel[kernel_size * 2]);\n    __m128d k3 = _mm_loadu_pd(&kernel[kernel_size * 2 + 2]);\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; j += 2) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t p = 0; p < kernel_size; ++p) {\n                __m128d in0 = _mm_loadu_pd(&input[(i + p) * input_size + j]);\n                __m128d in1 = _mm_loadu_pd(&input[(i + p) * input_size + j + 2]);\n                __m128d prod0 = _mm_mul_pd(in0, k0);\n                __m128d prod1 = _mm_mul_pd(in1, k1);\n                __m128d prod2 = _mm_mul_pd(in0, k2);\n                __m128d prod3 = _mm_mul_pd(in1, k3);\n                sum = _mm_add_pd(sum, _mm_add_pd(prod0, prod1));\n                sum = _mm_add_pd(sum, _mm_add_pd(prod2, prod3));\n            }\n            sum = _mm_max_pd(sum, _mm_setzero_pd());\n            _mm_storeu_pd(&output[(i * output_size) + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    size_t n = numbers.size();\n\n    for (size_t i = 0; i < n; ++i) {\n        __m128d x = _mm_set1_pd(numbers[i]);\n        for (size_t j = i + 1; j < n; j += 2) {\n            __m128d y = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(y, x);\n            __m128d abs_diff = _mm_and_pd(diff, _mm_castsi128_pd(_mm_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF)));\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    size_t n = numbers.size();\n    __m128d thresh = _mm_set1_pd(threshold);\n\n    for (size_t i = 0; i < n; ++i) {\n        __m128d x = _mm_set1_pd(numbers[i]);\n        for (size_t j = i + 1; j < n; j += 2) {\n            if (j + 1 < n) {\n                __m128d y = _mm_loadu_pd(&numbers[j]);\n                __m128d diff = _mm_sub_pd(y, x);\n                __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_set_epi32(0x80000000, 0x80000000, 0, 0)), diff);\n                __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n                if (_mm_movemask_pd(cmp)) {\n                    return true;\n                }\n            } else {\n                double diff = std::fabs(numbers[j] - numbers[i]);\n                if (diff < threshold) {\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    const __m128d thresh = _mm_set1_pd(threshold);\n    size_t n = numbers.size();\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m128d x = _mm_set1_pd(numbers[i]);\n        for (size_t j = i + 1; j < n; j += 2) {\n            size_t k = std::min(j + 1, n);\n            __m128d y = _mm_set_pd(numbers[j], numbers[j < n - 1 ? j + 1 : j]);\n            __m128d diff = _mm_sub_pd(y, x);\n            __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_set1_epi32(0x80000000)), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    size_t n = numbers.size();\n\n    for (size_t i = 0; i < n; ++i) {\n        __m128d x = _mm_set1_pd(numbers[i]);\n        for (size_t j = i + 1; j < n; j += 2) {\n            __m128d y = _mm_loadu_pd(&numbers[j]);\n            __m128d diff = _mm_sub_pd(y, x);\n            __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_set1_epi32(0x80000000)), diff);\n            __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n            if (_mm_movemask_pd(cmp)) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    if (numbers.size() < 2) return false;\n\n    __m128d thresh = _mm_set1_pd(threshold);\n    size_t n = numbers.size();\n\n    for (size_t i = 0; i < n - 1; ++i){\n        __m128d x = _mm_set1_pd(numbers[i]);\n        for (size_t j = i + 1; j < n; j += 2){\n            if (j + 1 < n){\n                __m128d y = _mm_loadu_pd(&numbers[j]);\n                __m128d diff = _mm_sub_pd(y, x);\n                __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_set1_epi32(0x80000000)), diff);\n                __m128d cmp = _mm_cmplt_pd(abs_diff, thresh);\n                if (_mm_movemask_pd(cmp)){\n                    return true;\n                }\n            }\n            else{\n                double diff = std::fabs(numbers[i] - numbers[j]);\n                if (diff < threshold){\n                    return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    __m128i neg_one = _mm_set1_epi32(-1);\n    int n = operations.size();\n    int i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i const*)(&operations[i]));\n        balance = _mm_add_epi32(balance, op);\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        __m128i mask = _mm_and_si128(cmp, neg_one);\n        int result = _mm_movemask_ps(_mm_castsi128_ps(mask));\n        if (result != 0) return true;\n    }\n\n    int remaining_balance = 0;\n    for (; i < n; ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i balance = _mm_set1_epi32(0);\n    __m128i mask = _mm_set1_epi32(-1);\n\n    for(size_t i = 0; i + 4 <= operations.size(); i += 4){\n        __m128i current_ops = _mm_loadu_si128((__m128i*)&operations[i]);\n        balance = _mm_add_epi32(balance, current_ops);\n        __m128i cmp_result = _mm_cmplt_epi32(balance, zero);\n        __m128i and_result = _mm_and_si128(cmp_result, mask);\n        int check = _mm_movemask_epi8(and_result);\n        if(check != 0) return true;\n    }\n\n    // Handle remaining elements\n    for(size_t i = operations.size() - operations.size() % 4; i < operations.size(); ++i){\n        if((int)(operations[i] + (*(__m128i*)&balance)[0]) < 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(-1);\n\n    size_t i = 0;\n    for (; i + 3 < operations.size(); i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i const*)(&operations[i]));\n        balance = _mm_add_epi32(balance, op);\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        __m128i and_res = _mm_and_si128(cmp, mask);\n        int result = _mm_movemask_epi8(and_res);\n        if (result != 0) return true;\n    }\n\n    int remaining_balance = 0;\n    for (; i < operations.size(); ++i) {\n        remaining_balance += operations[i];\n        if (remaining_balance < 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    __m128i balance = _mm_setzero_si128();\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(-1);\n    int n = operations.size();\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i const*)(&operations[i]));\n        balance = _mm_add_epi32(balance, op);\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        __m128i and_res = _mm_and_si128(cmp, mask);\n        int result = _mm_movemask_epi8(and_res);\n        if (result != 0) return true;\n    }\n\n    // Handle remaining elements\n    __m128i temp_balance = balance;\n    for (; i < n; ++i) {\n        temp_balance = _mm_add_epi32(temp_balance, _mm_set1_epi32(operations[i]));\n        if (_mm_cvtsi128_si32(_mm_cmplt_epi32(temp_balance, zero)) != 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations){\n    if (operations.empty()) return false;\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i balance = _mm_set1_epi32(0);\n    __m128i neg_mask = _mm_set1_epi32(0x80000000);\n\n    for (size_t i = 0; i + 4 <= operations.size(); i += 4) {\n        __m128i op = _mm_loadu_si128((__m128i const*)(&operations[i]));\n        balance = _mm_add_epi32(balance, op);\n        __m128i cmp = _mm_cmplt_epi32(balance, zero);\n        __m128i and_res = _mm_and_si128(cmp, neg_mask);\n        int mask = _mm_movemask_epi8(and_res);\n        if (mask != 0) return true;\n    }\n\n    // Handle remaining elements\n    for (size_t i = operations.size() - operations.size() % 4; i < operations.size(); ++i) {\n        if ((int)(operations[i] + (*(__m128i*)&balance)[0]) < 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the sum using SIMD\n    __m128d sum_vec = _mm_setzero_pd();\n    for(size_t i = 0; i + 1 < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum_vec = _mm_add_pd(sum_vec, data);\n    }\n    double sum[2] = {0};\n    _mm_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1];\n    if(n % 2 != 0){\n        total_sum += numbers[n-1];\n    }\n\n    double mean = total_sum / n;\n\n    // Calculate the absolute deviations using SIMD\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d abs_dev_sum = _mm_setzero_pd();\n    for(size_t i = 0; i + 1 < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean_vec);\n        __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_cmplt_pd(diff, _mm_setzero_pd())), diff);\n        abs_dev_sum = _mm_add_pd(abs_dev_sum, abs_diff);\n    }\n    double abs_sum[2] = {0};\n    _mm_storeu_pd(abs_sum, abs_dev_sum);\n    double total_abs_dev = abs_sum[0] + abs_sum[1];\n    if(n % 2 != 0){\n        double last_diff = fabs(numbers[n-1] - mean);\n        total_abs_dev += last_diff;\n    }\n\n    return total_abs_dev / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the mean\n    __m128d sum = _mm_setzero_pd();\n    for(size_t i = 0; i + 1 < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, data);\n    }\n    double total_sum[2] = {0};\n    _mm_storeu_pd(total_sum, sum);\n    double mean = (total_sum[0] + total_sum[1] + (n % 2 ? numbers[n-1] : 0)) / n;\n\n    // Calculate absolute deviations\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d abs_sum = _mm_setzero_pd();\n    for(size_t i = 0; i + 1 < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean_vec);\n        __m128d abs_diff = _mm_andnot_pd(_mm_cmplt_pd(diff, _mm_setzero_pd()), diff);\n        abs_sum = _mm_add_pd(abs_sum, abs_diff);\n    }\n\n    double abs_total[2] = {0};\n    _mm_storeu_pd(abs_total, abs_sum);\n    double mad = (abs_total[0] + abs_total[1]);\n    if(n % 2){\n        mad += fabs(numbers[n-1] - mean);\n    }\n    return mad / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    if (numbers.empty()) return 0.0;\n\n    size_t n = numbers.size();\n    __m128d sum = _mm_setzero_pd();\n    __m128d count = _mm_set1_pd(n);\n\n    // Compute mean\n    for(size_t i = 0; i < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, data);\n    }\n    __m128d mean = _mm_div_pd(sum, _mm_set1_pd(n));\n\n    // Extract mean\n    double mean_arr[2];\n    _mm_storeu_pd(mean_arr, mean);\n    double x_mean = (mean_arr[0] + mean_arr[1]) / 2.0;\n\n    // Compute MAD\n    sum = _mm_setzero_pd();\n    for(size_t i = 0; i < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, _mm_set1_pd(x_mean));\n        __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_cmplt_epi32(_mm_castpd_si128(diff), _mm_setzero_si128())), diff);\n        sum = _mm_add_pd(sum, abs_diff);\n    }\n\n    // Horizontal add\n    double result_arr[2];\n    _mm_storeu_pd(result_arr, sum);\n    double mad = (result_arr[0] + result_arr[1]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    size_t n = numbers.size();\n    if (n == 0) return 0.0;\n\n    // Calculate the sum using SIMD\n    __m128d sum_vec = _mm_setzero_pd();\n    for(size_t i = 0; i < n - 1; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum_vec = _mm_add_pd(sum_vec, data);\n    }\n    double sum[2] = {0};\n    _mm_storeu_pd(sum, sum_vec);\n    double total_sum = sum[0] + sum[1];\n\n    // Add remaining elements if any\n    for(size_t i = n - (n % 2); i < n; ++i){\n        total_sum += numbers[i];\n    }\n\n    double mean = total_sum / n;\n\n    // Calculate absolute deviations using SIMD\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d abs_dev_sum_vec = _mm_setzero_pd();\n\n    for(size_t i = 0; i < n - 1; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean_vec);\n        __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_cmplt_pd(diff, _mm_setzero_pd())), diff);\n        abs_dev_sum_vec = _mm_add_pd(abs_dev_sum_vec, abs_diff);\n    }\n\n    double abs_dev_sum[2] = {0};\n    _mm_storeu_pd(abs_dev_sum, abs_dev_sum_vec);\n    double total_abs_dev_sum = abs_dev_sum[0] + abs_dev_sum[1];\n\n    // Add remaining elements if any\n    for(size_t i = n - (n % 2); i < n; ++i){\n        total_abs_dev_sum += std::abs(numbers[i] - mean);\n    }\n\n    return total_abs_dev_sum / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cmath>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    if (numbers.empty()) return 0.0;\n\n    size_t n = numbers.size();\n    __m128d sum = _mm_setzero_pd();\n    __m128d count = _mm_set1_pd(1.0);\n\n    for(size_t i = 0; i + 1 < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, data);\n        count = _mm_add_pd(count, _mm_set1_pd(1.0));\n    }\n\n    double total_sum[2] = {0};\n    _mm_storeu_pd(total_sum, sum);\n    double mean = (total_sum[0] + total_sum[1]) / n;\n\n    __m128d mean_vec = _mm_set1_pd(mean);\n    __m128d mad_sum = _mm_setzero_pd();\n\n    for(size_t i = 0; i + 1 < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean_vec);\n        __m128d abs_diff = _mm_andnot_pd(_mm_castsi128_pd(_mm_cmplt_epi32(_mm_castpd_si128(diff), _mm_setzero_si128())), diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double mad_total[2] = {0};\n    _mm_storeu_pd(mad_total, mad_sum);\n    return (mad_total[0] + mad_total[1]) / n;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    std::vector<int> result;\n    if (numbers.size() <= 1) {\n        return numbers;\n    }\n\n    size_t N = numbers.size() / 4 * 4;\n    __m128i v_delimeter = _mm_set1_epi32(delimeter);\n\n    for (size_t i = 0; i < N; i += 4) {\n        __m128i v_nums = _mm_loadu_si128((__m128i const*)(&numbers[i]));\n        __m128i v_nums_high = _mm_shuffle_epi32(v_nums, _MM_SHUFFLE(3, 3, 2, 2));\n        __m128i v_nums_low = _mm_shuffle_epi32(v_nums, _MM_SHUFFLE(1, 1, 0, 0));\n\n        __m128i v_res_low = _mm_unpacklo_epi32(v_nums_low, v_delimeter);\n        __m128i v_res_high = _mm_unpackhi_epi32(v_nums_high, v_delimeter);\n\n        _mm_storeu_si128((__m128i*)(&result[result.size()]), v_res_low);\n        result.insert(result.end(), &result[result.size()], 8);\n        _mm_storeu_si128((__m128i*)(&result[result.size()]), v_res_high);\n        result.insert(result.end(), &result[result.size()], 8);\n    }\n\n    for (size_t i = N; i < numbers.size() - 1; ++i) {\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n    if (numbers.size() > 0) {\n        result.push_back(numbers[numbers.size()-1]);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    if (numbers.size() <= 1) return numbers;\n\n    size_t original_size = numbers.size();\n    size_t new_size = (original_size * 2) - 1;\n    std::vector<int> result(new_size, 0);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n    __m128i zero = _mm_setzero_si128();\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 4 <= original_size; i += 4, j += 8) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&numbers[i]));\n        __m128i low = _mm_shuffle_epi32(vec, _MM_SHUFFLE(1, 0, 1, 0));\n        __m128i high = _mm_shuffle_epi32(vec, _MM_SHUFFLE(3, 2, 3, 2));\n\n        __m128i interleaved_low = _mm_unpacklo_epi32(low, delimiter_vec);\n        __m128i interleaved_high = _mm_unpackhi_epi32(high, delimiter_vec);\n\n        _mm_storeu_si128((__m128i*)(&result[j]), interleaved_low);\n        _mm_storeu_si128((__m128i*)(&result[j + 4]), interleaved_high);\n    }\n\n    // Handle remaining elements\n    for (; i < original_size; ++i, ++j) {\n        result[j] = numbers[i];\n        if (i != original_size - 1) {\n            result[++j] = delimeter;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){ \n    if (numbers.size() <= 1) return numbers;\n\n    size_t original_size = numbers.size();\n    size_t new_size = 2 * original_size - 1;\n    std::vector<int> result(new_size, delimeter);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 4 <= original_size; i += 4, j += 8) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&numbers[i]));\n        __m128i interleaved = _mm_unpacklo_epi32(vec, delimiter_vec);\n        _mm_storeu_si128((__m128i*)&result[j], interleaved);\n\n        if (i + 8 <= original_size) {\n            __m128i vec_hi = _mm_loadu_si128((__m128i const*)(&numbers[i + 4]));\n            __m128i interleaved_hi = _mm_unpackhi_epi32(vec, delimiter_vec);\n            _mm_storeu_si128((__m128i*)&result[j + 4], interleaved_hi);\n        }\n    }\n\n    for (; i < original_size; ++i, ++j) {\n        result[j] = numbers[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){ \n    if (numbers.size() <= 1) return numbers;\n\n    size_t original_size = numbers.size();\n    size_t new_size = original_size + (original_size - 1);\n    std::vector<int> result(new_size, delimeter);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n\n    size_t i = 0;\n    size_t j = 0;\n\n    for (; i + 4 <= original_size; i += 4, j += 8) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&numbers[i]));\n        __m128i interleaved = _mm_shuffle_epi32(vec, _MM_SHUFFLE(3, 3, 2, 2));\n        interleaved = _mm_insert_epi32(interleaved, _mm_extract_epi32(vec, 1), 3);\n        interleaved = _mm_insert_epi32(interleaved, _mm_extract_epi32(vec, 0), 5);\n\n        __m128i delimiters = _mm_load1_epi32(&delimeter);\n        delimiters = _mm_shuffle_epi32(delimiters, _MM_SHUFFLE(0, 0, 0, 0));\n        interleaved = _mm_or_si128(interleaved, _mm_and_si128(delimiters, _mm_set_epi32(0, -1, 0, -1)));\n\n        _mm_storeu_si128((__m128i*)&result[j], interleaved);\n    }\n\n    for (; i < original_size; ++i, ++j) {\n        result[j] = numbers[i];\n        if (i != original_size - 1) {\n            result[j + 1] = delimeter;\n            j += 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){ \n    std::vector<int> result;\n    if (numbers.size() <= 1) {\n        return numbers;\n    }\n\n    size_t N = numbers.size() / 4;\n    size_t remainder = numbers.size() % 4;\n\n    result.reserve(numbers.size() * 2 - 1);\n\n    __m128i delimiter_vec = _mm_set1_epi32(delimeter);\n\n    for (size_t i = 0; i < N; ++i) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&numbers[i * 4]);\n        __m128i low = _mm_shuffle_epi32(vec, _MM_SHUFFLE(1, 0, 1, 0));\n        __m128i high = _mm_shuffle_epi32(vec, _MM_SHUFFLE(3, 2, 3, 2));\n\n        __m128i low_delim = _mm_insert_epi32(low, delimeter, 2);\n        __m128i high_delim = _mm_insert_epi32(high, delimeter, 2);\n\n        _mm_storeu_si128((__m128i*)&result[i * 8], low_delim);\n        _mm_storeu_si128((__m128i*)&result[i * 8 + 4], high_delim);\n    }\n\n    for (size_t i = N * 4; i < numbers.size() - 1; ++i) {\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n    result.push_back(numbers.back());\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int n = numbers.size();\n    if (n == 0) return {0, 1};\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set_epi64x(1, 1);\n\n    for(int i = 0; i < n; i += 8){\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i lo = _mm_unpacklo_epi16(data, _mm_setzero_si128());\n        __m128i hi = _mm_unpackhi_epi16(data, _mm_setzero_si128());\n\n        __m128i sum_lo = _mm_add_epi32(sum, lo);\n        __m128i sum_hi = _mm_add_epi32(sum, hi);\n\n        __m128i prod_lo = _mm_mul_epi32(lo, prod);\n        __m128i prod_hi = _mm_mul_epi32(hi, prod);\n\n        sum = _mm_add_epi32(sum_lo, sum_hi);\n        prod = _mm_mul_epi32(prod_lo, prod_hi);\n    }\n\n    sum = _mm_hadd_epi32(sum, sum);\n    sum = _mm_hadd_epi32(sum, sum);\n\n    prod = _mm_hmul_epi32(prod, prod);\n    prod = _mm_hmul_epi32(prod, prod);\n\n    int64_t total_sum = _mm_cvtsi128_si64(sum);\n    int64_t total_prod = _mm_cvtsi128_si64(prod);\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int n = numbers.size();\n    if (n == 0) return {0, 1};\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i product = _mm_set_epi64x(1, 1);\n\n    for(int i = 0; i < n; i += 8){\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i lo = _mm_unpacklo_epi16(data, _mm_setzero_si128());\n        __m128i hi = _mm_unpackhi_epi16(data, _mm_setzero_si128());\n\n        // Sum\n        sum = _mm_add_epi32(sum, _mm_add_epi32(_mm_cvtepi16_epi32(lo), _mm_cvtepi16_epi32(hi)));\n\n        // Product\n        __m128i prod_lo = _mm_mullo_epi32(_mm_cvtepi16_epi32(lo), _mm_shuffle_epi32(product, _MM_SHUFFLE(0,0,0,0)));\n        __m128i prod_hi = _mm_mullo_epi32(_mm_cvtepi16_epi32(hi), _mm_shuffle_epi32(product, _MM_SHUFFLE(1,1,1,1)));\n        product = _mm_add_epi64(_mm_unpacklo_epi32(prod_lo, prod_hi), _mm_unpackhi_epi32(prod_lo, prod_hi));\n    }\n\n    // Horizontal add for sum\n    __m128i sum_h1 = _mm_hadd_epi32(sum, sum);\n    __m128i sum_h2 = _mm_hadd_epi32(sum_h1, sum_h1);\n    int64_t total_sum = _mm_extract_epi32(sum_h2, 0) + _mm_extract_epi32(sum_h2, 1);\n\n    // Horizontal product for product\n    __m128i prod_h1 = _mm_hadd_epi32(product, product);\n    __m128i prod_h2 = _mm_hadd_epi32(prod_h1, prod_h1);\n    int64_t total_product = _mm_extract_epi32(prod_h2, 0) * _mm_extract_epi32(prod_h2, 1);\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int n = numbers.size();\n    if (n == 0) return {0, 1};\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set_epi64x(1, 1);\n\n    for(int i = 0; i < n; i += 8){\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i zero = _mm_setzero_si128();\n        __m128i mask = _mm_cmpeq_epi16(data, zero);\n\n        // Calculate sum\n        sum = _mm_add_epi16(sum, data);\n\n        // Calculate product\n        __m128i ones = _mm_set1_epi16(1);\n        data = _mm_or_si128(data, ones); // Avoid multiplying by zero\n        prod = _mm_mullo_epi16(prod, data);\n    }\n\n    // Horizontal sum\n    __m128i sum_h1 = _mm_hadd_epi16(sum, sum);\n    __m128i sum_h2 = _mm_hadd_epi16(sum_h1, sum_h1);\n    int64_t total_sum = _mm_cvtsi128_si32(sum_h2);\n\n    // Horizontal product\n    __m128i prod_h1 = _mm_hadd_epi16(prod, prod);\n    __m128i prod_h2 = _mm_hadd_epi16(prod_h1, prod_h1);\n    int64_t total_prod = _mm_cvtsi128_si32(prod_h2);\n\n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int n = numbers.size();\n    if (n == 0) return {0, 1};\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i prod = _mm_set1_epi16(1);\n\n    for(int i = 0; i < n; i += 8){\n        __m128i data = _mm_loadu_si128((__m128i const*)(&numbers[i]));\n        sum = _mm_add_epi16(sum, data);\n        prod = _mm_mullo_epi16(prod, data);\n    }\n\n    // Horizontal sum\n    int64_t hsum = 0;\n    int16_t temp_sum[8];\n    _mm_storeu_si128((__m128i*)temp_sum, sum);\n    for(int i = 0; i < 8; ++i){\n        hsum += temp_sum[i];\n    }\n\n    // Horizontal product\n    int64_t hprod = 1;\n    int16_t temp_prod[8];\n    _mm_storeu_si128((__m128i*)temp_prod, prod);\n    for(int i = 0; i < 8; ++i){\n        hprod *= temp_prod[i];\n    }\n\n    return {hsum, hprod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int n = numbers.size();\n    if (n == 0) return {0, 1};\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i product = _mm_set_epi64x(1, 1);\n\n    for(int i = 0; i < n; i += 8){\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i lo = _mm_unpacklo_epi16(data, _mm_setzero_si128());\n        __m128i hi = _mm_unpackhi_epi16(data, _mm_setzero_si128());\n\n        __m128i lo_sum = _mm_add_epi32(sum, lo);\n        __m128i hi_sum = _mm_add_epi32(sum, hi);\n\n        __m128i lo_prod = _mm_mullo_epi32(product, lo);\n        __m128i hi_prod = _mm_mullo_epi32(product, hi);\n\n        sum = _mm_add_epi32(lo_sum, hi_sum);\n        product = _mm_mullo_epi32(lo_prod, hi_prod);\n    }\n\n    int64_t total_sum = 0;\n    int64_t total_product = 1;\n\n    int sum_arr[4];\n    _mm_storeu_si128((__m128i*)sum_arr, sum);\n    for(int i = 0; i < 4; ++i){\n        total_sum += sum_arr[i];\n    }\n\n    int prod_arr[4];\n    _mm_storeu_si128((__m128i*)prod_arr, product);\n    for(int i = 0; i < 4; ++i){\n        total_product *= prod_arr[i];\n    }\n\n    return {total_sum, total_product};\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int simd_width = 4;\n    __m128i current_max = _mm_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); ++i) {\n        __m128i new_val = _mm_set1_epi32(numbers[i]);\n        current_max = _mm_max_epi32(current_max, new_val);\n        result[i] = _mm_extract_epi32(current_max, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int N = 4; // Number of elements processed per SIMD operation\n    __m128i current_max = _mm_set1_epi32(numbers[0]);\n    __m128i temp_max;\n\n    for(size_t i = 0; i < numbers.size(); i += N){\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        temp_max = _mm_max_epi32(data, current_max);\n        current_max = _mm_max_epi32(temp_max, current_max);\n\n        _mm_storeu_si128((__m128i*)&result[i], current_max);\n    }\n\n    // Handle remaining scalar elements\n    for(size_t i = (numbers.size() / N) * N; i < numbers.size(); ++i){\n        result[i] = std::max(result[i - 1], numbers[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int simd_width = 4;\n    __m128i current_max = _mm_set1_epi32(numbers[0]);\n    result[0] = numbers[0];\n\n    for (size_t i = 1; i < numbers.size(); ++i) {\n        __m128i new_val = _mm_set1_epi32(numbers[i]);\n        current_max = _mm_max_epi32(current_max, new_val);\n        result[i] = _mm_extract_epi32(current_max, 0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int simd_width = 4;\n    __m128i current_max = _mm_set1_epi32(numbers[0]);\n    __m128i temp_max;\n\n    for(size_t i = 0; i + simd_width <= numbers.size(); i += simd_width){\n        __m128i data = _mm_loadu_si128((__m128i*)&numbers[i]);\n        temp_max = _mm_max_epi32(current_max, data);\n        current_max = _mm_max_epi32(current_max, temp_max);\n        _mm_storeu_si128((__m128i*)&result[i], current_max);\n    }\n\n    // Handle remaining elements\n    for(size_t i = ((numbers.size() / simd_width) * simd_width); i < numbers.size(); ++i){\n        int max_val = (i == 0) ? numbers[0] : result[i-1];\n        result[i] = (numbers[i] > max_val) ? numbers[i] : max_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size(), 0);\n    if (numbers.empty()) return result;\n\n    const int N = 4; // Number of elements processed per SIMD operation\n    __m128i current_max = _mm_set1_epi32(numbers[0]);\n    __m128i temp_max;\n\n    int i = 0;\n    for (; i + N <= (int)numbers.size(); i += N) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(numbers.data() + i));\n        temp_max = _mm_max_epi32(vec, current_max);\n        current_max = temp_max;\n        _mm_storeu_si128((__m128i *)(result.data() + i), current_max);\n    }\n\n    for (; i < (int)numbers.size(); ++i) {\n        int max_val = (i == 0) ? numbers[0] : std::max(result[i-1], numbers[i]);\n        result[i] = max_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) return \"\";\n    \n    std::string result(len, '\\0');\n    size_t aligned_len = len - (len % 16);\n\n    const char* ap = a.c_str();\n    const char* bp = b.c_str();\n    char* res_p = &result[0];\n\n    for (size_t i = 0; i < aligned_len; i += 16) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&ap[i]));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&bp[i]));\n        __m128i vres = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&res_p[i]), vres);\n    }\n\n    for (size_t i = aligned_len; i < len; ++i) {\n        res_p[i] = ap[i] ^ bp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) return \"\";\n    \n    std::string result(len, '\\0');\n    const char* ap = a.c_str();\n    const char* bp = b.c_str();\n    char* res = &result[0];\n    \n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(ap + i));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(bp + i));\n        __m128i vr = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(res + i), vr);\n    }\n    \n    for (; i < len; ++i) {\n        res[i] = ap[i] ^ bp[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cstring>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) {\n        return \"\";\n    }\n\n    std::string result(len, '\\0');\n    const char* ap = a.c_str();\n    const char* bp = b.c_str();\n    char* res = &result[0];\n\n    size_t i = 0;\n    for (; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&ap[i]));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&bp[i]));\n        __m128i vr = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&res[i]), vr);\n    }\n\n    for (; i < len; ++i) {\n        res[i] = ap[i] ^ bp[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) return \"\";\n    \n    std::string result(len, '0');\n    size_t i = 0;\n    // Process 16 bytes at a time\n    for (; i + 15 < len; i += 16) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&a[i]));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&b[i]));\n        __m128i vres = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), vres);\n    }\n    // Process remaining characters\n    for (; i < len; ++i) {\n        result[i] = (a[i] == '0' ? (b[i] == '0' ? '0' : '1') : (b[i] == '0' ? '1' : '0'));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if (len != b.size()) return \"\";\n    \n    std::string result(len, '\\0');\n    size_t aligned_len = len - (len % 16);\n    \n    for (size_t i = 0; i < aligned_len; i += 16) {\n        __m128i va = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&a[i]));\n        __m128i vb = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&b[i]));\n        __m128i vres = _mm_xor_si128(va, vb);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), vres);\n    }\n    \n    for (size_t i = aligned_len; i < len; ++i) {\n        result[i] = a[i] ^ b[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int vector_size = 4;\n    size_t max_length = 0;\n    int max_index = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i current_lengths;\n\n    for(size_t i = 0; i + vector_size <= strings.size(); i += vector_size){\n        __m128i lengths = _mm_set_epi32(\n            static_cast<int>(strings[i+3].size()),\n            static_cast<int>(strings[i+2].size()),\n            static_cast<int>(strings[i+1].size()),\n            static_cast<int>(strings[i+0].size())\n        );\n\n        current_lengths = _mm_max_epi32(lengths, zero);\n        zero = current_lengths;\n\n        __m128i mask = _mm_cmpeq_epi32(current_lengths, lengths);\n        int index = _mm_movemask_epi8(mask);\n\n        if(index != 0){\n            int lane = __builtin_ctz(index) / 4;\n            if(i + lane == 0 || strings[i + lane].size() > max_length){\n                max_length = strings[i + lane].size();\n                max_index = i + lane;\n            }\n        }\n    }\n\n    for(size_t i = strings.size() - (strings.size() % vector_size); i < strings.size(); ++i){\n        if(strings[i].size() > max_length){\n            max_length = strings[i].size();\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int vector_size = 4;\n    size_t max_length = 0;\n    int max_index = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i current_lengths = _mm_setzero_si128();\n    __m128i max_lengths = _mm_setzero_si128();\n    __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n\n    for(size_t i = 0; i + vector_size <= strings.size(); i += vector_size){\n        __m128i lengths = _mm_loadu_si128((__m128i*)&strings[i].length());\n        __m128i cmp = _mm_cmpgt_epi32(lengths, max_lengths);\n        indices = _mm_add_epi32(indices, _mm_set_epi32(4*vector_size, 4*vector_size, 4*vector_size, 4*vector_size));\n        max_lengths = _mm_max_epi32(max_lengths, lengths);\n        current_lengths = lengths;\n    }\n\n    // Extract the maximum length and corresponding index\n    int lengths_array[4];\n    _mm_storeu_si128((__m128i*)lengths_array, max_lengths);\n\n    for(int i = 0; i < 4; ++i){\n        if(lengths_array[i] > max_length){\n            max_length = lengths_array[i];\n            max_index = i;\n        }\n    }\n\n    // Process remaining elements\n    for(size_t i = strings.size() - strings.size() % vector_size; i < strings.size(); ++i){\n        if(strings[i].length() > max_length){\n            max_length = strings[i].length();\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int n = strings.size();\n    int max_idx = 0;\n    int max_len = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i current_max = _mm_set1_epi32(0);\n    __m128i current_idx = _mm_set1_epi32(0);\n\n    for(int i = 0; i < n; i += 4){\n        __m128i lengths = _mm_loadu_si128((__m128i*)&strings[i].length());\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n\n        __m128i mask = _mm_cmpgt_epi32(lengths, current_max);\n        current_max = _mm_max_epi32(current_max, lengths);\n        current_idx = _mm_or_si128(_mm_and_si128(mask, indices), _mm_andnot_si128(mask, current_idx));\n\n        // Handle partial loads at the end of the vector\n        if(i + 4 > n){\n            int len0 = (i+0 < n) ? strings[i+0].length() : 0;\n            int len1 = (i+1 < n) ? strings[i+1].length() : 0;\n            int len2 = (i+2 < n) ? strings[i+2].length() : 0;\n            int len3 = (i+3 < n) ? strings[i+3].length() : 0;\n\n            int idx0 = (len0 > max_len) ? i+0 : max_idx;\n            int idx1 = (len1 > max_len) ? i+1 : max_idx;\n            int idx2 = (len2 > max_len) ? i+2 : max_idx;\n            int idx3 = (len3 > max_len) ? i+3 : max_idx;\n\n            max_len = std::max({len0, len1, len2, len3, max_len});\n            max_idx = (len0 == max_len) ? idx0 : \n                      (len1 == max_len) ? idx1 : \n                      (len2 == max_len) ? idx2 : \n                      (len3 == max_len) ? idx3 : max_idx;\n        }\n    }\n\n    int res[4];\n    _mm_storeu_si128((__m128i*)res, current_idx);\n    for(int i = 0; i < 4; ++i){\n        if(res[i] < n && strings[res[i]].length() > max_len){\n            max_len = strings[res[i]].length();\n            max_idx = res[i];\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <climits>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int n = strings.size();\n    int max_idx = 0;\n    int max_len = 0;\n\n    __m128i v_max_len = _mm_set1_epi32(0);\n    __m128i v_max_idx = _mm_set1_epi32(0);\n\n    for(int i = 0; i < n; i += 4){\n        __m128i v_lengths = _mm_loadu_si128((__m128i*)&strings[i].length());\n        __m128i v_cmp = _mm_cmplt_epi32(v_lengths, v_max_len);\n        v_max_len = _mm_max_epi32(v_lengths, v_max_len);\n        v_max_idx = _mm_or_si128(_mm_andnot_si128(v_cmp, _mm_set1_epi32(i)), _mm_and_si128(v_cmp, v_max_idx));\n    }\n\n    int *tmp_len = (int*)&v_max_len;\n    int *tmp_idx = (int*)&v_max_idx;\n\n    for(int i = 0; i < 4; ++i){\n        if(tmp_len[i] > max_len){\n            max_len = tmp_len[i];\n            max_idx = tmp_idx[i];\n        }\n    }\n\n    return strings[max_idx];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n#include <algorithm>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    const int vector_size = 4;\n    int num_strings = strings.size();\n    int aligned_num = num_strings - (num_strings % vector_size);\n\n    __m128i max_lengths = _mm_setzero_si128();\n    __m128i max_indices = _mm_setr_epi32(0, 1, 2, 3);\n    __m128i current_lengths;\n\n    for(int i = 0; i < aligned_num; i += vector_size){\n        __m128i indices = _mm_setr_epi32(i, i+1, i+2, i+3);\n        current_lengths = _mm_setr_epi32(strings[i].length(), strings[i+1].length(), strings[i+2].length(), strings[i+3].length());\n        \n        __m128i cmp_result = _mm_cmpgt_epi32(current_lengths, max_lengths);\n        max_lengths = _mm_max_epi32(max_lengths, current_lengths);\n        max_indices = _mm_or_si128(_mm_and_si128(cmp_result, indices), _mm_andnot_si128(cmp_result, max_indices));\n    }\n\n    int max_length = 0;\n    int max_index = 0;\n\n    // Extract the maximum length and index\n    int lengths_array[4];\n    int indices_array[4];\n    _mm_storeu_si128((__m128i*)lengths_array, max_lengths);\n    _mm_storeu_si128((__m128i*)indices_array, max_indices);\n\n    for(int i = 0; i < vector_size; ++i){\n        if(lengths_array[i] > max_length){\n            max_length = lengths_array[i];\n            max_index = indices_array[i];\n        }\n    }\n\n    // Process remaining elements\n    for(int i = aligned_num; i < num_strings; ++i){\n        if(strings[i].length() > max_length){\n            max_length = strings[i].length();\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <emmintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m128i zero = _mm_setzero_si128();\n    __m128i va = _mm_cvtsi64_si128(a);\n    __m128i vb = _mm_cvtsi64_si128(b);\n\n    while (!_mm_testz_si128(vb, vb)) {\n        __m128i vtemp = _mm_subs_epi64(va, vb);\n        va = vb;\n        vb = _mm_and_si128(vb, _mm_cmpeq_epi64(vtemp, zero));\n        vb = _mm_or_si128(vb, _mm_andnot_si128(_mm_cmpeq_epi64(vtemp, zero), vtemp));\n    }\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <emmintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i va = _mm_set_epi64x(0, a);\n    __m128i vb = _mm_set_epi64x(0, b);\n\n    while (1) {\n        __m128i cmp = _mm_cmpeq_epi64(va, vb);\n        if (_mm_movemask_epi8(cmp) == 0xFFFF) {\n            break;\n        }\n\n        __m128i sub = _mm_subs_epu64(va, vb);\n        __m128i mask = _mm_cmpgt_epi64(va, vb);\n\n        va = _mm_or_si128(_mm_and_si128(mask, sub), _mm_andnot_si128(mask, va));\n        vb = _mm_or_si128(_mm_and_si128(mask, vb), _mm_andnot_si128(mask, sub));\n\n        __m128i is_zero = _mm_cmpeq_epi64(vb, zero);\n        if (_mm_movemask_epi8(is_zero) == 0xFFFF) {\n            break;\n        }\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <emmintrin.h>\n#include <stdint.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i tmp_a = _mm_set_epi64x(0, a);\n    __m128i tmp_b = _mm_set_epi64x(0, b);\n\n    while (1) {\n        __m128i cmp = _mm_cmpeq_epi64(tmp_a, tmp_b);\n        if (_mm_movemask_epi8(cmp) == 0xFFFF) {\n            break;\n        }\n\n        __m128i tmp = _mm_subs_epu64(tmp_a, tmp_b);\n        __m128i mask = _mm_cmpgt_epi64(tmp_a, tmp_b);\n\n        tmp_a = _mm_andnot_si128(mask, tmp_a);\n        tmp_a = _mm_or_si128(tmp_a, _mm_and_si128(mask, tmp_b));\n\n        tmp_b = _mm_and_si128(tmp_b, _mm_cmpeq_epi64(tmp, zero));\n        tmp_b = _mm_or_si128(tmp_b, _mm_andnot_si128(_mm_cmpeq_epi64(tmp, zero), tmp));\n    }\n\n    return _mm_cvtsi128_si64(tmp_a);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\n#include <emmintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m128i zero = _mm_setzero_si128();\n    __m128i va = _mm_set_epi64x(0, a);\n    __m128i vb = _mm_set_epi64x(0, b);\n\n    while (1) {\n        __m128i cmp = _mm_cmpeq_epi64(va, vb);\n        if (_mm_movemask_epi8(cmp) == 0xFFFF) {\n            break;\n        }\n\n        __m128i sub = _mm_subs_epu64(va, vb);\n        __m128i mask = _mm_cmpgt_epi64(va, vb);\n\n        va = _mm_or_si128(_mm_and_si128(mask, sub), _mm_andnot_si128(mask, va));\n        vb = _mm_or_si128(_mm_and_si128(mask, vb), _mm_andnot_si128(mask, sub));\n\n        va = _mm_andnot_si128(_mm_cmpeq_epi64(va, zero), va);\n        vb = _mm_andnot_si128(_mm_cmpeq_epi64(vb, zero), vb);\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```cpp\n#include <emmintrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i va = _mm_cvtsi64_si128(a);\n    __m128i vb = _mm_cvtsi64_si128(b);\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi64(vb, zero)) != 0x3) {\n        __m128i temp = vb;\n        vb = _mm_sub_epi64(va, _mm_mullo_epi32(vb, _mm_cvtsi64_si128(_mm_cvtsi128_si64(_mm_div_epu32(va, vb)))));\n        va = temp;\n    }\n\n    return _mm_cvtsi128_si64(va);\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    __m128 min_pair = _mm_set_ps(0, 0, numbers[1], numbers[0]);\n    __m128 min_diff = _mm_set_ps(0, 0, std::abs(numbers[1] - numbers[0]), std::abs(numbers[1] - numbers[0]));\n\n    size_t n = numbers.size();\n    for(size_t i = 0; i + 3 < n; i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 diff = _mm_setzero_ps();\n        __m128 closest_pair = _mm_setzero_ps();\n\n        for(int j = 0; j < 4; ++j){\n            __m128 broadcast = _mm_shuffle_ps(vec, vec, _MM_SHUFFLE(j,j,j,j));\n            __m128 current_diff = _mm_sub_ps(vec, broadcast);\n            current_diff = _mm_andnot_ps(broadcast, _mm_or_ps(current_diff, _mm_move_ss(current_diff, broadcast)));\n\n            __m128 abs_diff = _mm_and_ps(current_diff, _mm_cmplt_ps(current_diff, _mm_setzero_ps()));\n            abs_diff = _mm_max_ps(abs_diff, _mm_sub_ps(_mm_setzero_ps(), abs_diff));\n\n            __m128 mask = _mm_cmplt_ps(abs_diff, min_diff);\n            min_diff = _mm_min_ps(abs_diff, min_diff);\n            closest_pair = _mm_blendv_ps(min_pair, vec, mask);\n        }\n\n        min_pair = closest_pair;\n    }\n\n    float results[4];\n    _mm_storeu_ps(results, min_pair);\n\n    std::vector<float> res({results[2], results[3]});\n    if(res[0] > res[1]) std::swap(res[0], res[1]);\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    int n = numbers.size();\n    if (n < 2) return {};\n\n    float min_diff = std::numeric_limits<float>::infinity();\n    float closest_pair[2] = {0.0f, 0.0f};\n\n    __m128 min_diff_vec = _mm_set1_ps(min_diff);\n    __m128 current_min_diff_vec = _mm_set1_ps(min_diff);\n\n    for(int i = 0; i < n - 1; ++i){\n        __m128 a = _mm_set1_ps(numbers[i]);\n        for(int j = i + 1; j < n; ++j){\n            __m128 b = _mm_set1_ps(numbers[j]);\n            __m128 diff = _mm_sub_ps(a, b);\n            __m128 abs_diff = _mm_andnot_ps(_mm_castsi128_ps(_mm_set1_epi32(0x80000000)), diff);\n            \n            current_min_diff_vec = _mm_min_ps(abs_diff, current_min_diff_vec);\n\n            __m128 mask = _mm_cmplt_ps(abs_diff, min_diff_vec);\n            if(_mm_movemask_ps(mask)){\n                min_diff = std::abs(numbers[i] - numbers[j]);\n                min_diff_vec = _mm_set1_ps(min_diff);\n                if(numbers[i] < numbers[j]){\n                    closest_pair[0] = numbers[i];\n                    closest_pair[1] = numbers[j];\n                }\n                else{\n                    closest_pair[0] = numbers[j];\n                    closest_pair[1] = numbers[i];\n                }\n            }\n        }\n    }\n\n    return {closest_pair[0], closest_pair[1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <float.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    float min_diff = FLT_MAX;\n    float closest_pair[2] = {0.0f, 0.0f};\n\n    size_t n = numbers.size();\n    __m128 min_diff_vec = _mm_set1_ps(min_diff);\n    __m128 current_min_diff_vec = _mm_set1_ps(min_diff);\n\n    for (size_t i = 0; i < n - 1; i += 4){\n        __m128 a = _mm_loadu_ps(&numbers[i]);\n        __m128 min_diff_current = _mm_set1_ps(FLT_MAX);\n\n        for (size_t j = i + 1; j < n; j += 4){\n            __m128 b = _mm_loadu_ps(&numbers[j]);\n            __m128 diff = _mm_sub_ps(a, b);\n            __m128 abs_diff = _mm_andnot_ps(_mm_castsi128_ps(_mm_set1_epi32(0x80000000)), diff);\n            __m128 mask = _mm_cmplt_ps(abs_diff, min_diff_current);\n            min_diff_current = _mm_min_ps(abs_diff, min_diff_current);\n\n            __m128 current_min_diff_mask = _mm_cmplt_ps(min_diff_current, current_min_diff_vec);\n            current_min_diff_vec = _mm_min_ps(min_diff_current, current_min_diff_vec);\n\n            // Check and update the closest pair\n            int index = _mm_movemask_ps(current_min_diff_mask);\n            if (index != 0){\n                size_t idx = i + (__builtin_ctz(index) % 4);\n                size_t jdx = j + (__builtin_ctz(index) / 4);\n                if (fabs(numbers[idx] - numbers[jdx]) < min_diff){\n                    min_diff = fabs(numbers[idx] - numbers[jdx]);\n                    if (numbers[idx] < numbers[jdx]){\n                        closest_pair[0] = numbers[idx];\n                        closest_pair[1] = numbers[jdx];\n                    }\n                    else{\n                        closest_pair[0] = numbers[jdx];\n                        closest_pair[1] = numbers[idx];\n                    }\n                }\n            }\n        }\n    }\n\n    return std::vector<float>{closest_pair[0], closest_pair[1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    float min_diff = std::numeric_limits<float>::max();\n    float closest1 = 0.0f, closest2 = 0.0f;\n\n    size_t n = numbers.size();\n    __m128 vmin_diff = _mm_set1_ps(min_diff);\n    __m128 vclosest1 = _mm_setzero_ps();\n    __m128 vclosest2 = _mm_setzero_ps();\n\n    for(size_t i = 0; i < n - 1; ++i){\n        __m128 vnum_i = _mm_set1_ps(numbers[i]);\n        for(size_t j = i + 1; j < n; ++j){\n            __m128 vnum_j = _mm_set1_ps(numbers[j]);\n            __m128 vdiff = _mm_sub_ps(vnum_i, vnum_j);\n            __m128 vabs_diff = _mm_andnot_ps(_mm_set1_ps(-0.0f), vdiff); // abs(diff)\n            \n            __m128 vmask = _mm_cmplt_ps(vabs_diff, vmin_diff);\n            vmin_diff = _mm_min_ps(vabs_diff, vmin_diff);\n\n            vclosest1 = _mm_or_ps(_mm_and_ps(vmask, vnum_i), _mm_andnot_ps(vmask, vclosest1));\n            vclosest2 = _mm_or_ps(_mm_and_ps(vmask, vnum_j), _mm_andnot_ps(vmask, vclosest2));\n        }\n    }\n\n    float data1[4], data2[4];\n    _mm_storeu_ps(data1, vclosest1);\n    _mm_storeu_ps(data2, vclosest2);\n\n    float res1 = data1[0], res2 = data2[0];\n    for(int k = 1; k < 4; ++k){\n        if(data1[k] != 0.0f) res1 = data1[k];\n        if(data2[k] != 0.0f) res2 = data2[k];\n    }\n\n    if(res1 > res2) std::swap(res1, res2);\n\n    return {res1, res2};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return {};\n\n    __m128 min_pair = _mm_set_ps(0, 0, numbers[1], numbers[0]);\n    __m128 min_diff = _mm_set_ps(0, 0, std::abs(numbers[1] - numbers[0]), std::abs(numbers[1] - numbers[0]));\n\n    size_t n = numbers.size();\n    for(size_t i = 0; i + 3 < n; i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 diff = _mm_setzero_ps();\n        __m128 current_min_diff = _mm_set1_ps(numbers[i+1] - numbers[i]);\n        __m128 current_min_pair = _mm_set_ps(0, 0, numbers[i+1], numbers[i]);\n\n        for(size_t j = i + 2; j < std::min(n, i + 4); ++j){\n            __m128 cmp_vec = _mm_set1_ps(numbers[j]);\n            __m128 abs_diff = _mm_sub_ps(vec, cmp_vec);\n            abs_diff = _mm_andnot_ps(_mm_castsi128_ps(_mm_cmplt_ps(abs_diff, _mm_setzero_ps())), abs_diff);\n\n            __m128 mask = _mm_cmplt_ps(abs_diff, current_min_diff);\n            current_min_diff = _mm_min_ps(abs_diff, current_min_diff);\n            current_min_pair = _mm_or_ps(_mm_and_ps(mask, _mm_set_ps(0, 0, numbers[j], numbers[i])), \n                                        _mm_andnot_ps(mask, current_min_pair));\n        }\n\n        __m128 global_mask = _mm_cmplt_ps(current_min_diff, min_diff);\n        min_diff = _mm_min_ps(current_min_diff, min_diff);\n        min_pair = _mm_or_ps(_mm_and_ps(global_mask, current_min_pair), \n                             _mm_andnot_ps(global_mask, min_pair));\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, min_pair);\n\n    return {std::min(result[2], result[3]), std::max(result[2], result[3])};\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    __m128 vrange = _mm_sub_ps(vmax, vmin);\n\n    std::vector<float> result(numbers.size());\n    size_t i = 0;\n\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 vnum = _mm_loadu_ps(&numbers[i]);\n        __m128 vsub = _mm_sub_ps(vnum, vmin);\n        __m128 vres = _mm_div_ps(vsub, vrange);\n        _mm_storeu_ps(&result[i], vres);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() <= 1) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    float scale = 1.0f / (max_val - min_val);\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vscale = _mm_set1_ps(scale);\n\n    size_t i = 0;\n    size_t vec_size = numbers.size() / 4 * 4;\n    std::vector<float> result(numbers.size());\n\n    for (; i < vec_size; i += 4) {\n        __m128 vnum = _mm_loadu_ps(&numbers[i]);\n        __m128 vsub = _mm_sub_ps(vnum, vmin);\n        __m128 vmul = _mm_mul_ps(vsub, vscale);\n        _mm_storeu_ps(&result[i], vmul);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    float scale = 1.0f / (max_val - min_val);\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vscale = _mm_set1_ps(scale);\n\n    size_t aligned_size = numbers.size() / 4 * 4;\n    std::vector<float> result(numbers.size());\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128 vnum = _mm_loadu_ps(&numbers[i]);\n        __m128 vsub = _mm_sub_ps(vnum, vmin);\n        __m128 vmul = _mm_mul_ps(vsub, vscale);\n        _mm_storeu_ps(&result[i], vmul);\n    }\n\n    for(size_t i = aligned_size; i < numbers.size(); ++i){\n        result[i] = (numbers[i] - min_val) * scale;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <stdexcept>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() < 2) {\n        throw std::invalid_argument(\"Vector must contain at least two elements.\");\n    }\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        return std::vector<float>(numbers.size(), 0.0f);\n    }\n\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    __m128 vrange = _mm_sub_ps(vmax, vmin);\n\n    std::vector<float> result(numbers.size());\n    size_t i = 0;\n\n    for (; i + 4 <= numbers.size(); i += 4) {\n        __m128 vnums = _mm_loadu_ps(&numbers[i]);\n        __m128 vsub = _mm_sub_ps(vnums, vmin);\n        __m128 vrescaled = _mm_div_ps(vsub, vrange);\n        _mm_storeu_ps(&result[i], vrescaled);\n    }\n\n    for (; i < numbers.size(); ++i) {\n        result[i] = (numbers[i] - min_val) / (max_val - min_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if (numbers.size() <= 1) return numbers;\n\n    float min_val = *std::min_element(numbers.begin(), numbers.end());\n    float max_val = *std::max_element(numbers.begin(), numbers.end());\n\n    if (min_val == max_val) {\n        std::vector<float> result(numbers.size(), 0.0f);\n        return result;\n    }\n\n    __m128 vmin = _mm_set1_ps(min_val);\n    __m128 vmax = _mm_set1_ps(max_val);\n    __m128 vrange = _mm_div_ps(_mm_sub_ps(vmax, vmin), _mm_set1_ps(1.0f));\n\n    size_t N = numbers.size();\n    std::vector<float> result(N);\n\n    for (size_t i = 0; i < N; i += 4) {\n        size_t chunk = std::min<size_t>(4, N - i);\n        __m128 vin = _mm_loadu_ps(&numbers[i]);\n        __m128 vnorm = _mm_div_ps(_mm_sub_ps(vin, vmin), vrange);\n        _mm_storeu_ps(&result[i], vnorm);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    uint64_t result = 0;\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_i = _mm_set_epi64x(0, n / 2);\n\n    for(uint64_t i = n / 2; i > 0; --i){\n        __m128i v_mod = _mm_cvtepi32_epi64(_mm_rem_epu32(_mm_cvtsi64_si128(n), _mm_cvtsi64_si128(i)));\n        __m128i is_zero = _mm_cmpeq_epi64(v_mod, v_zero);\n        __m128i mask = _mm_and_si128(is_zero, _mm_cmpgt_epi64(_mm_cvtsi64_si128(i), v_one));\n        if(_mm_movemask_epi8(mask)){\n            result = i;\n            break;\n        }\n        v_i = _mm_sub_epi64(v_i, v_one);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    uint64_t result = 0;\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_i = _mm_set_epi64x(0, n - 1);\n    for(uint64_t i = n / 2; i > 0; --i){\n        __m128i v_mod = _mm_cvtepi32_epi64(_mm_rem_epu32(_mm_cvtepi64_epi32(v_n), _mm_cvtepi64_epi32(v_i)));\n        __m128i v_cmp = _mm_cmpeq_epi64(v_mod, v_zero);\n        if(_mm_movemask_epi8(v_cmp) != 0){\n            result = i;\n            break;\n        }\n        v_i = _mm_sub_epi64(v_i, _mm_set_epi64x(0, 1));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    uint64_t result = 0;\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_i = _mm_set_epi64x(0, n / 2);\n\n    for(uint64_t i = n / 2; i > 0; i -= 2){\n        __m128i v_mod = _mm_cvtepi32_epi64(_mm_rem_epu32(_mm_cvtsi64_si128(i), _mm_cvtsi64_si128(n)));\n        __m128i v_cmp = _mm_cmpeq_epi64(v_mod, v_zero);\n        __m128i v_and = _mm_and_si128(v_cmp, _mm_set1_epi64x(i));\n        __m128i v_max = _mm_max_epi64(v_and, _mm_set1_epi64x(result));\n        result = _mm_cvtsi128_si64(_mm_shuffle_epi32(v_max, _MM_SHUFFLE(3, 3, 3, 3)));\n        if(result > 0) break;\n        v_i = _mm_sub_epi64(v_i, _mm_set_epi64x(2,2));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    uint64_t result = 0;\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_i = _mm_set_epi64x(0, n / 2);\n\n    for(uint64_t i = n / 2; i > 0; --i){\n        __m128i v_mod = _mm_cvtsi128_si64(_mm_rem_epu64(v_n, v_i));\n        if(_mm_movemask_epi8(_mm_cmpeq_epi64(v_mod, v_zero)) != 0){\n            result = i;\n            break;\n        }\n        v_i = _mm_sub_epi64(v_i, v_one);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    if (n <= 1) return 0;\n    uint64_t result = 0;\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi64x(1);\n    __m128i v_i = _mm_set_epi64x(0, n / 2);\n\n    for(uint64_t i = n / 2; i > 0; --i){\n        __m128i v_mod = _mm_cvtsi128_si64(_mm_rem_epu64(v_n, _mm_set1_epi64x(i)));\n        if(_mm_extract_epi64(v_mod, 0) == 0){\n            result = i;\n            break;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m128i vtwo = _mm_set1_epi32(2);\n    __m128i vthree = _mm_set1_epi32(3);\n    __m128i vfive = _mm_set1_epi32(5);\n\n    // Factor out 2s\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(_mm_set1_epi32(n % 2), _mm_setzero_si128()))) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Factor out 3s\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(_mm_set1_epi32(n % 3), _mm_setzero_si128()))) {\n        factors.push_back(3);\n        n /= 3;\n    }\n\n    // Factor out 5s\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(_mm_set1_epi32(n % 5), _mm_setzero_si128()))) {\n        factors.push_back(5);\n        n /= 5;\n    }\n\n    int divisor = 7;\n    while (divisor * divisor <= n) {\n        __m128i vdivisor = _mm_set1_epi32(divisor);\n        __m128i vremainder = _mm_set1_epi32(n % divisor);\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(vremainder, _mm_setzero_si128()))) {\n            factors.push_back(divisor);\n            n /= divisor;\n        } else {\n            divisor += 2;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n <= 1) return factors;\n\n    __m128i vtwo = _mm_set1_epi32(2);\n    __m128i vthree = _mm_set1_epi32(3);\n    __m128i vfive = _mm_set1_epi32(5);\n\n    // Handle factor of 2\n    while (n % 2 == 0) {\n        factors.push_back(2);\n        n /= 2;\n    }\n\n    // Handle factor of 3\n    while (n % 3 == 0) {\n        factors.push_back(3);\n        n /= 3;\n    }\n\n    // Handle factor of 5\n    while (n % 5 == 0) {\n        factors.push_back(5);\n        n /= 5;\n    }\n\n    int limit = (int)::sqrt((double)n);\n    int i = 7;\n\n    __m128i vi = _mm_set_epi32(i+24, i+12, i, i-12);\n    __m128i vn = _mm_set1_epi32(n);\n\n    for (; i <= limit; i += 25) {\n        __m128i vrem = _mm_rem_epu32(vn, vi); // Hypothetical SIMD remainder\n        __m128i vzero = _mm_setzero_si128();\n        __m128i veq = _mm_cmpeq_epi32(vrem, vzero);\n\n        int mask = _mm_movemask_epi8(veq);\n        if (mask) {\n            for (int j = 0; j < 4; ++j) {\n                if ((mask >> (j * 4)) & 0xF) {\n                    int factor = i + j * 12;\n                    if (n % factor == 0) {\n                        while (n % factor == 0) {\n                            factors.push_back(factor);\n                            n /= factor;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```\n\n**Note:** `_mm_rem_epu32` is not a real intrinsic and is used here hypothetically to illustrate the concept. You would need to implement or approximate this operation using available SSE/SSE2 instructions if required."}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n < 2) return factors;\n\n    __m128i vdiv = _mm_set_epi32(0, 0, 0, n);\n    __m128i vone = _mm_set1_epi32(1);\n\n    for (int i = 2; i * i <= n; ++i) {\n        __m128i vrem = _mm_set1_epi32(i);\n        __m128i vzero = _mm_setzero_si128();\n        while (_mm_cvtsi128_si32(_mm_andnot_si128(_mm_cmpeq_epi32(_mm_div_epi32(vdiv, vrem), vzero), vzero)) == 0) {\n            factors.push_back(i);\n            vdiv = _mm_sub_epi32(vdiv, _mm_mullo_epi32(vrem, vone));\n            n /= i;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n < 2) return factors;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_one = _mm_set1_epi32(1);\n\n    // Factor out powers of 2\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(_mm_and_si128(v_n, v_two), _mm_setzero_si128())) == 0xFFFF) {\n        factors.push_back(2);\n        v_n = _mm_sub_epi32(v_n, v_two);\n    }\n\n    __m128i v_divisor = _mm_set_epi32(31, 29, 27, 25);\n    __m128i v_step = _mm_set_epi32(6, 6, 6, 6);\n    __m128i v_limit = _mm_set1_epi32(static_cast<int>(sqrt(n)));\n\n    while (true) {\n        __m128i v_remainder = _mm_cvtepi32_epi64(_mm_and_si128(_mm_set_epi32(0, 0, 0, n % 25), v_one));\n        __m128i v_quotient = _mm_div_epi32(v_n, v_divisor); // Hypothetical intrinsic for division\n\n        __m128i v_check = _mm_cmpeq_epi32(_mm_mul_epi32(v_divisor, v_quotient), v_n);\n\n        int mask = _mm_movemask_epi8(v_check);\n        if (mask != 0) {\n            int index = __builtin_ctz(mask) / 4;\n            int divisor = _mm_extract_epi32(v_divisor, index);\n            factors.push_back(divisor);\n            v_n = _mm_div_epi32(v_n, _mm_set1_epi32(divisor)); // Update n\n            if (_mm_testz_si128(v_n, v_n)) break;\n        } else {\n            v_divisor = _mm_add_epi32(v_divisor, v_step);\n            if (_mm_cmpgt_epi32(v_divisor, v_limit).m128i_u32[0]) {\n                if (n > 1) factors.push_back(n);\n                break;\n            }\n        }\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    if (n < 2) return factors;\n\n    __m128i vtwo = _mm_set1_epi32(2);\n    __m128i vthree = _mm_set1_epi32(3);\n    __m128i vfive = _mm_set1_epi32(5);\n\n    // Factor out powers of 2\n    while (_mm_movemask_epi8(_mm_cmpeq_epi32(_mm_set1_epi32(n & 1), _mm_setzero_si128())) == 0) {\n        factors.push_back(2);\n        n >>= 1;\n    }\n\n    // Factor out odd numbers starting from 3\n    int limit = (int)::sqrt((double)n);\n    int divisor = 3;\n    while (divisor <= limit) {\n        __m128i vn = _mm_set1_epi32(n);\n        __m128i vdivisor = _mm_set1_epi32(divisor);\n        __m128i vremainder = _mm_set1_epi32(n % divisor);\n\n        if (_mm_movemask_epi8(_mm_cmpeq_epi32(vremainder, _mm_setzero_si128())) != 0) {\n            factors.push_back(divisor);\n            n /= divisor;\n            limit = (int)::sqrt((double)n);\n        } else {\n            divisor += 2;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    if(l.empty()) return result;\n\n    size_t size = l.size();\n    size_t aligned_size = size - (size % 4);\n\n    __m128 zero = _mm_setzero_ps();\n    __m128 mask;\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        mask = _mm_cmpgt_ps(vec, zero);\n        vec = _mm_and_ps(vec, mask);\n\n        float vals[4];\n        _mm_storeu_ps(vals, vec);\n\n        for(int j = 0; j < 4; ++j){\n            if(vals[j] > 0.0f) result.push_back(vals[j]);\n        }\n    }\n\n    for(size_t i = aligned_size; i < size; ++i){\n        if(l[i] > 0.0f) result.push_back(l[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    if(l.empty()) return result;\n\n    size_t size = l.size();\n    size_t aligned_size = size - (size % 4);\n\n    __m128 zero = _mm_setzero_ps();\n    __m128 mask;\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        mask = _mm_cmpgt_ps(vec, zero);\n        vec = _mm_and_ps(vec, mask);\n\n        float values[4];\n        _mm_storeu_ps(values, vec);\n\n        for(int j = 0; j < 4; ++j){\n            if(values[j] > 0.0f) result.push_back(values[j]);\n        }\n    }\n\n    for(size_t i = aligned_size; i < size; ++i){\n        if(l[i] > 0.0f) result.push_back(l[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    if(l.empty()) return result;\n\n    size_t size = l.size();\n    size_t aligned_size = size - (size % 4);\n\n    __m128 zero = _mm_setzero_ps();\n    __m128 mask;\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        mask = _mm_cmpgt_ps(vec, zero);\n        vec = _mm_and_ps(vec, mask);\n        \n        float temp[4] = {0};\n        _mm_store_ps(temp, vec);\n\n        for(int j = 0; j < 4; ++j){\n            if(temp[j] > 0) result.push_back(temp[j]);\n        }\n    }\n\n    for(size_t i = aligned_size; i < size; ++i){\n        if(l[i] > 0) result.push_back(l[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    if (l.empty()) return result;\n\n    size_t size = l.size();\n    size_t aligned_size = size - (size % 4);\n\n    __m128 zero = _mm_setzero_ps();\n    __m128 mask;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        mask = _mm_cmpgt_ps(vec, zero);\n        vec = _mm_and_ps(vec, mask);\n\n        float values[4];\n        _mm_storeu_ps(values, vec);\n\n        for(int j = 0; j < 4; ++j){\n            if(values[j] > 0.0f){\n                result.push_back(values[j]);\n            }\n        }\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if(l[i] > 0.0f){\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    std::vector<float> result;\n    if (l.empty()) return result;\n\n    size_t size = l.size();\n    size_t aligned_size = size - (size % 4);\n\n    float *data = const_cast<float*>(l.data());\n    __m128 zero = _mm_setzero_ps();\n    __m128 mask;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 vec = _mm_loadu_ps(data + i);\n        mask = _mm_cmpgt_ps(vec, zero);\n        vec = _mm_and_ps(vec, mask);\n\n        float vals[4];\n        _mm_storeu_ps(vals, vec);\n\n        for (int j = 0; j < 4; ++j) {\n            if (vals[j] > 0.0f) result.push_back(vals[j]);\n        }\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (data[i] > 0.0f) result.push_back(data[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n % 2) == 0 || (n % 3) == 0) return false;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n\n    int64_t sqrt_n = (int64_t)(sqrt((double)n));\n    for(int64_t i = 5; i <= sqrt_n; i += 6){\n        __m128i v_i = _mm_set_epi32(i+4, i+2, i, i-2);\n        __m128i rem1 = _mm_cvtepi32_epi64(_mm_rem_epu32(v_n, _mm_shuffle_epi32(v_i, _MM_SHUFFLE(0,0,0,0))));\n        __m128i rem2 = _mm_cvtepi32_epi64(_mm_rem_epu32(v_n, _mm_shuffle_epi32(v_i, _MM_SHUFFLE(1,1,1,1))));\n        __m128i rem3 = _mm_cvtepi32_epi64(_mm_rem_epu32(v_n, _mm_shuffle_epi32(v_i, _MM_SHUFFLE(2,2,2,2))));\n        __m128i rem4 = _mm_cvtepi32_epi64(_mm_rem_epu32(v_n, _mm_shuffle_epi32(v_i, _MM_SHUFFLE(3,3,3,3))));\n\n        __m128i cmp1 = _mm_cmpeq_epi64(rem1, zero);\n        __m128i cmp2 = _mm_cmpeq_epi64(rem2, zero);\n        __m128i cmp3 = _mm_cmpeq_epi64(rem3, zero);\n        __m128i cmp4 = _mm_cmpeq_epi64(rem4, zero);\n\n        __m128i result = _mm_or_si128(_mm_or_si128(cmp1, cmp2), _mm_or_si128(cmp3, cmp4));\n        if (!_mm_testz_si128(result, result)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n % 2 == 0) || (n % 3 == 0)) return false;\n\n    const int64_t limit = static_cast<int64_t>(std::sqrt(static_cast<double>(n))) + 1;\n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vzero = _mm_setzero_si128();\n\n    for (int64_t i = 5; i < limit; i += 6){\n        __m128i vi = _mm_set_epi64x(i+2, i);\n        __m128i vrem1 = _mm_rem_epu64(vn, vi); // Hypothetical intrinsic for remainder\n        __m128i vrem2 = _mm_rem_epu64(vn, _mm_add_epi64(vi, _mm_set_epi64x(2, 2))); \n\n        __m128i vtest1 = _mm_cmpeq_epi64(vrem1, vzero);\n        __m128i vtest2 = _mm_cmpeq_epi64(vrem2, vzero);\n\n        if (_mm_movemask_epi8(_mm_packs_epi16(vtest1, vtest2)) != 0)\n            return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n % 2) == 0 || (n % 3) == 0) return false;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_zero = _mm_setzero_si128();\n\n    int limit = (int)(sqrt((double)n)) + 1;\n    for(int i=5; i<limit; i+=4){\n        __m128i v_i = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i v_mod = _mm_and_si128(_mm_cmpeq_epi32(_mm_rem_epu32(v_n, v_i), v_zero), _mm_cmpgt_epi32(v_i, v_one));\n        __m128i mask = _mm_movemask_epi8(v_mod);\n        if(mask != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n & 1) == 0 || n % 3 == 0) return false;\n\n    const int64_t limit = static_cast<int64_t>(std::sqrt(static_cast<double>(n)));\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_two = _mm_set1_epi64x(2);\n\n    for (int64_t i = 5; i <= limit; i += 6){\n        __m128i v_i = _mm_set_epi64x(i+2, i);\n        __m128i v_mod = _mm_rem_epu64(v_n, v_i);\n        __m128i v_zero = _mm_setzero_si128();\n        __m128i v_cmp = _mm_cmpeq_epi64(v_mod, v_zero);\n        int mask = _mm_movemask_epi8(v_cmp);\n        if(mask != 0) return false;\n\n        v_i = _mm_add_epi64(v_i, v_two);\n        v_mod = _mm_rem_epu64(v_n, v_i);\n        v_cmp = _mm_cmpeq_epi64(v_mod, v_zero);\n        mask = _mm_movemask_epi8(v_cmp);\n        if(mask != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <immintrin.h>\n\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n % 2) == 0 || (n % 3) == 0) return false;\n\n    const int64_t limit = static_cast<int64_t>(std::sqrt(static_cast<double>(n)));\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i zero = _mm_setzero_si128();\n\n    for (int64_t i = 5; i <= limit; i += 6){\n        __m128i v_i = _mm_set_epi64x(i+2, i);\n        __m128i rem1 = _mm_rem_epu32(v_n, v_i); // Hypothetical intrinsic for remainder\n        __m128i cmp1 = _mm_cmpeq_epi32(rem1, zero);\n        int mask1 = _mm_movemask_epi8(cmp1);\n        if(mask1 != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <cctype>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return s;\n\n    const int len = s.length();\n    char result[len + 1];\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i zero = _mm_set1_epi8(0);\n    bool has_letter = false;\n    bool all_non_letters = true;\n\n    for(int i = 0; i < len; i += 16){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i cmp_lower = _mm_cmpgt_epi8(_mm_and_si128(vec, lower_mask), zero);\n        __m128i cmp_upper = _mm_cmpgt_epi8(vec, _mm_sub_epi8(lower_mask, zero));\n        __m128i is_letter = _mm_or_si128(cmp_lower, cmp_upper);\n\n        __m128i flipped_case = _mm_xor_si128(vec, _mm_and_si128(is_letter, lower_mask));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), flipped_case);\n\n        if (!_mm_testz_si128(is_letter, is_letter)){\n            has_letter = true;\n        }\n\n        if (_mm_movemask_epi8(is_letter) != 0xFFFF){\n            all_non_letters = false;\n        }\n    }\n\n    if (!has_letter && all_non_letters){\n        for(int i = 0; i < len; ++i){\n            result[i] = s[len - i - 1];\n        }\n    } else {\n        for(int i = len - (len % 16); i < len; ++i){\n            if(std::isalpha(s[i])){\n                result[i] = std::islower(s[i]) ? std::toupper(s[i]) : std::tolower(s[i]);\n                has_letter = true;\n            } else {\n                result[i] = s[i];\n            }\n        }\n    }\n\n    result[len] = '\\0';\n    return std::string(result);\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return \"\";\n\n    const int len = s.length();\n    std::string result(s);\n\n    __m128i lower_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0);\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i delta = _mm_set1_epi8(32);\n\n    bool has_letter = false;\n\n    for(int i = 0; i < len; i += 16){\n        int limit = std::min(i + 16, len);\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n\n        __m128i is_upper = _mm_and_si128(_mm_cmplt_epi8(vec, lower_a), _mm_cmpgt_epi8(vec, upper_a));\n        __m128i is_lower = _mm_and_si128(_mm_cmplt_epi8(vec, _mm_set1_epi8('z')), _mm_cmpgt_epi8(vec, lower_a));\n\n        __m128i toggle_case = _mm_xor_si128(vec, _mm_and_si128(_mm_or_si128(is_upper, is_lower), delta));\n\n        __m128i cmp_upper = _mm_cmpgt_epi8(vec, upper_a);\n        __m128i cmp_lower = _mm_cmplt_epi8(vec, lower_a);\n        __m128i any_letter = _mm_or_si128(_mm_and_si128(cmp_upper, cmp_lower), _mm_andnot_si128(lower_mask, vec));\n\n        has_letter |= (_mm_movemask_epi8(any_letter) != 0);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), toggle_case);\n    }\n\n    if (!has_letter){\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cstring>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return \"\";\n\n    const int N = 16;\n    size_t len = s.length();\n    char* result = new char[len + 1];\n    std::copy(s.begin(), s.end(), result);\n    result[len] = '\\0';\n\n    bool has_letter = false;\n\n    for(size_t i = 0; i < len; i += N){\n        __m128i chunk = _mm_loadu_si128((__m128i const*)(result + i));\n        __m128i lower_mask = _mm_set1_epi8(0x20);\n        __m128i is_alpha_mask = _mm_and_si128(_mm_cmplt_epi8(chunk, _mm_set1_epi8(0x7B)), \n                                             _mm_cmpgt_epi8(chunk, _mm_set1_epi8(0x40)));\n\n        __m128i toggle_case = _mm_xor_si128(chunk, _mm_and_si128(is_alpha_mask, lower_mask));\n        _mm_storeu_si128((__m128i *)(result + i), toggle_case);\n\n        __m128i zero = _mm_set1_epi8(0);\n        __m128i any_letter = _mm_or_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('A') - 1),\n                                          _mm_cmpeq_epi8(chunk, _mm_set1_epi8('A') - 1));\n        any_letter = _mm_and_si128(any_letter, _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z') + 1));\n\n        __m128i any_letter_lower = _mm_or_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('a') - 1),\n                                                _mm_cmpeq_epi8(chunk, _mm_set1_epi8('a') - 1));\n        any_letter_lower = _mm_and_si128(any_letter_lower, _mm_cmplt_epi8(chunk, _mm_set1_epi8('z') + 1));\n\n        __m128i combined = _mm_or_si128(any_letter, any_letter_lower);\n        has_letter |= (_mm_movemask_epi8(combined) != 0);\n    }\n\n    if (!has_letter){\n        for(int i = 0, j = len - 1; i < j; ++i, --j){\n            char temp = result[i];\n            result[i] = result[j];\n            result[j] = temp;\n        }\n    }\n\n    std::string res(result);\n    delete[] result;\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return s;\n\n    const int N = 16;\n    int len = s.length();\n    bool has_letter = false;\n    bool has_non_letter = false;\n\n    // Check for letters and non-letters\n    for(char c : s){\n        if (('a' <= c && c <= 'z') || ('A' <= c && c <= 'Z')){\n            has_letter = true;\n        }\n        else{\n            has_non_letter = true;\n        }\n        if (has_letter && has_non_letter) break;\n    }\n\n    if (!has_letter){\n        // Reverse the string if no letters are present\n        return std::string(s.rbegin(), s.rend());\n    }\n\n    std::string result(len, '\\0');\n    const __m128i lower_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 'a' - 1, 'z' + 1, 'a' - 1, 'z' + 1, 'a' - 1, 'z' + 1, 'a' - 1, 'z' + 1);\n    const __m128i upper_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 'A' - 1, 'Z' + 1, 'A' - 1, 'Z' + 1, 'A' - 1, 'Z' + 1, 'A' - 1, 'Z' + 1);\n    const __m128i diff_mask = _mm_set1_epi8('a' - 'A');\n\n    for(int i = 0; i < len; i += N){\n        int chunk_size = std::min(N, len - i);\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n\n        __m128i is_lower = _mm_cmpeq_epi8(_mm_max_epu8(data, lower_mask), lower_mask);\n        __m128i is_upper = _mm_cmpeq_epi8(_mm_max_epu8(data, upper_mask), upper_mask);\n\n        __m128i to_upper = _mm_and_si128(data, _mm_set1_epi8(~0x20));\n        __m128i to_lower = _mm_or_si128(data, _mm_set1_epi8(0x20));\n\n        __m128i swapped_case = _mm_blendv_epi8(to_lower, to_upper, is_lower);\n        __m128i unchanged = _mm_andnot_si128(_mm_or_si128(is_lower, is_upper), data);\n        __m128i final_data = _mm_or_si128(swapped_case, unchanged);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), final_data);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <algorithm>\n\nstd::string solve_simd(const std::string & s){\n    if (s.empty()) return s;\n\n    const int len = s.length();\n    std::string result(s);\n\n    __m128i lower_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20);\n    __m128i upper_a = _mm_set1_epi8('A');\n    __m128i lower_a = _mm_set1_epi8('a');\n    __m128i upper_z = _mm_set1_epi8('Z');\n    __m128i lower_z = _mm_set1_epi8('z');\n\n    bool has_letter = false;\n\n    for(int i = 0; i < len; i += 16){\n        int limit = std::min(i + 16, len);\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(data, upper_a), _mm_cmple_epi8(data, upper_z));\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(data, lower_a), _mm_cmple_epi8(data, lower_z));\n\n        __m128i is_letter = _mm_or_si128(is_upper, is_lower);\n\n        // Check if there are any letters in this block\n        int letter_check = _mm_movemask_epi8(is_letter);\n        if(letter_check != 0){\n            has_letter = true;\n        }\n\n        __m128i toggle_case = _mm_and_si128(is_letter, lower_mask);\n        __m128i res = _mm_xor_si128(data, toggle_case);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), res);\n    }\n\n    if(!has_letter){\n        std::reverse(result.begin(), result.end());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return l;\n\n    std::vector<int> third_elements;\n    third_elements.reserve((l.size() / 3) + 1);\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        if (i % 3 == 0) {\n            third_elements.push_back(l[i]);\n        }\n    }\n\n    std::sort(third_elements.begin(), third_elements.end());\n\n    size_t third_idx = 0;\n    for (size_t i = 0; i < l.size(); ++i) {\n        if (i % 3 == 0) {\n            l[i] = third_elements[third_idx++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    std::vector<int> third_elements;\n    for (size_t i = 0; i < l.size(); ++i) {\n        if (i % 3 == 0) {\n            third_elements.push_back(l[i]);\n        }\n    }\n    std::sort(third_elements.begin(), third_elements.end());\n\n    size_t third_index = 0;\n    for (size_t i = 0; i < l.size(); ++i) {\n        if (i % 3 == 0) {\n            l[i] = third_elements[third_index++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return l;\n\n    std::vector<int> third_elements;\n    third_elements.reserve((l.size() / 3) + 1);\n\n    for (size_t i = 0; i < l.size(); ++i) {\n        if ((i % 3) == 0) {\n            third_elements.push_back(l[i]);\n        }\n    }\n\n    std::sort(third_elements.begin(), third_elements.end());\n\n    size_t third_idx = 0;\n    for (size_t i = 0; i < l.size(); ++i) {\n        if ((i % 3) == 0) {\n            l[i] = third_elements[third_idx++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return {};\n\n    size_t n = l.size();\n    std::vector<int> result = l;\n    std::vector<int> thirds;\n\n    // Collect elements at indices divisible by 3\n    for(size_t i = 0; i < n; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&l[i]));\n        __m128i mask = _mm_set_epi32(0, -1, 0, 0); // Mask for indices divisible by 3\n        __m128i filtered = _mm_and_si128(vec, mask);\n        __m128i zero = _mm_setzero_si128();\n        int count = _mm_movemask_epi8(_mm_cmpeq_epi32(filtered, zero));\n        if(count & 1){\n            thirds.push_back(l[i]);\n        }\n        if(i+3 < n && (i+3)%3 == 0){\n            thirds.push_back(l[i+3]);\n        }\n    }\n\n    // Sort the collected elements\n    std::sort(thirds.begin(), thirds.end());\n\n    // Place sorted elements back into their respective positions\n    size_t third_idx = 0;\n    for(size_t i = 0; i < n && third_idx < thirds.size(); ++i){\n        if(i % 3 == 0){\n            result[i] = thirds[third_idx++];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    if (l.empty()) return {};\n\n    // Extract indices divisible by 3\n    std::vector<int> div_by_three;\n    for (size_t i = 0; i < l.size(); ++i) {\n        if (i % 3 == 0) {\n            div_by_three.push_back(l[i]);\n        }\n    }\n\n    // Sort the extracted values\n    std::sort(div_by_three.begin(), div_by_three.end());\n\n    // Reconstruct the vector with sorted values at indices divisible by 3\n    size_t div_idx = 0;\n    for (size_t i = 0; i < l.size(); ++i) {\n        if (i % 3 == 0) {\n            l[i] = div_by_three[div_idx++];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nfloat max_element_simd(std::vector<float> l){\n    float max_val = l[0];\n    size_t size = l.size();\n    __m128 current_max = _mm_set1_ps(l[0]);\n    __m128 vec;\n    size_t i = 0;\n\n    for (; i + 4 <= size; i += 4) {\n        vec = _mm_loadu_ps(&l[i]);\n        current_max = _mm_max_ps(current_max, vec);\n    }\n\n    float temp[4] __attribute__((aligned(16)));\n    _mm_store_ps(temp, current_max);\n\n    max_val = std::max({max_val, temp[0], temp[1], temp[2], temp[3]});\n\n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    float max_val = l[0];\n    size_t size = l.size();\n    __m128 current_max = _mm_set1_ps(max_val);\n    size_t i = 0;\n\n    for (; i + 4 <= size; i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 cmp = _mm_max_ps(vec, current_max);\n        current_max = cmp;\n    }\n\n    float temp[4] __attribute__((aligned(16)));\n    _mm_store_ps(temp, current_max);\n\n    max_val = temp[0];\n    for (int j = 1; j < 4; ++j) {\n        if (temp[j] > max_val) {\n            max_val = temp[j];\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > max_val) {\n            max_val = l[i];\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    __m128 max_val = _mm_set1_ps(l[0]);\n    size_t i = 0;\n    for(; i + 4 <= l.size(); i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n    float max_arr[4];\n    _mm_storeu_ps(max_arr, max_val);\n    float current_max = max_arr[0];\n    for(int j = 1; j < 4; ++j){\n        if(max_arr[j] > current_max){\n            current_max = max_arr[j];\n        }\n    }\n    for(; i < l.size(); ++i){\n        if(l[i] > current_max){\n            current_max = l[i];\n        }\n    }\n    return current_max;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) {\n        return NAN;\n    }\n\n    __m128 max_val = _mm_set1_ps(l[0]);\n    size_t i = 0;\n\n    for (; i + 4 <= l.size(); i += 4) {\n        __m128 current = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, current);\n    }\n\n    float max_result[4];\n    _mm_storeu_ps(max_result, max_val);\n\n    float max_single = max_result[0];\n    for (int j = 1; j < 4; ++j) {\n        if (max_result[j] > max_single) {\n            max_single = max_result[j];\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > max_single) {\n            max_single = l[i];\n        }\n    }\n\n    return max_single;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nfloat max_element_simd(std::vector<float> l){\n    if (l.empty()) return 0; // Assuming non-empty as per problem statement\n\n    __m128 max_val = _mm_set1_ps(l[0]);\n    size_t i = 0;\n\n    for (; i + 4 <= l.size(); i += 4) {\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n\n    float result[4];\n    _mm_storeu_ps(result, max_val);\n\n    float current_max = std::max({result[0], result[1], result[2], result[3]});\n\n    for (; i < l.size(); ++i) {\n        if (l[i] > current_max) current_max = l[i];\n    }\n\n    return current_max;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m128i v7 = _mm_set1_epi8('7');\n    __m128i zero = _mm_setzero_si128();\n    __m128i divisors[2] = { _mm_set1_epi32(11), _mm_set1_epi32(13) };\n\n    for(int i = 0; i < n; i++){\n        __m128i vi = _mm_set1_epi32(i);\n        __m128i rem1 = _mm_rem_epu32(vi, divisors[0]);\n        __m128i rem2 = _mm_rem_epu32(vi, divisors[1]);\n        if(_mm_testz_si128(_mm_or_si128(rem1, rem2), _mm_set1_epi32(-1))){\n            char buffer[16];\n            int len = sprintf(buffer, \"%d\", i);\n            __m128i vbuffer = _mm_loadu_si128((__m128i*)buffer);\n            __m128i eq = _mm_cmpeq_epi8(vbuffer, v7);\n            int mask = _mm_movemask_epi8(eq);\n            count += __builtin_popcount(mask & ((1 << len) - 1));\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    int count = 0;\n    const int step = 4;\n    __m128i divisor_11 = _mm_set1_epi32(11);\n    __m128i divisor_13 = _mm_set1_epi32(13);\n    __m128i digit_7 = _mm_set1_epi8('7');\n    __m128i zero = _mm_setzero_si128();\n\n    for(int i = 0; i + step <= n; i += step){\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i rem_11 = _mm_rem_epu32(nums, divisor_11);\n        __m128i rem_13 = _mm_rem_epu32(nums, divisor_13);\n        __m128i mask_11 = _mm_cmpeq_epi32(rem_11, zero);\n        __m128i mask_13 = _mm_cmpeq_epi32(rem_13, zero);\n        __m128i mask = _mm_or_si128(mask_11, mask_13);\n\n        __m128i selected = _mm_and_si128(nums, mask);\n        __m128i counts = _mm_setzero_si128();\n        \n        for(int j = 0; j < 4; ++j){\n            __m128i num = _mm_shuffle_epi32(selected, _MM_SHUFFLE(j,j,j,j));\n            __m128i digits = _mm_set1_epi32(num.m128i_i32[0]);\n            __m128i tens = _mm_set1_epi32(10);\n            \n            __m128i temp_counts = _mm_setzero_si128();\n            for(int k = 0; k < 9; ++k){\n                __m128i remainder = _mm_cvtepu8_epi16(_mm_rem_epu8(digits, tens));\n                __m128i cmp = _mm_cmpeq_epi8(remainder, digit_7);\n                temp_counts = _mm_add_epi8(temp_counts, cmp);\n                digits = _mm_div_epu8(digits, tens);\n            }\n            counts = _mm_add_epi8(counts, temp_counts);\n        }\n        count += _mm_extract_epi16(counts, 0);\n    }\n\n    for(int i = (n / step) * step; i < n; ++i){\n        if(i % 11 == 0 || i % 13 == 0){\n            int temp = i;\n            while(temp > 0){\n                if(temp % 10 == 7) count++;\n                temp /= 10;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nint fizz_buzz_simd(int n){\n    if (n <= 0) return 0;\n\n    const int step = 4;\n    int count = 0;\n    __m128i divisor_11 = _mm_set1_epi32(11);\n    __m128i divisor_13 = _mm_set1_epi32(13);\n    __m128i digit_7 = _mm_set1_epi32('7');\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set_epi32(0xFF, 0xFF, 0xFF, 0xFF);\n\n    for(int i = 0; i + step <= n; i += step){\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i rem_11 = _mm_rem_epu32(nums, divisor_11);\n        __m128i rem_13 = _mm_rem_epu32(nums, divisor_13);\n        __m128i is_divisible = _mm_or_si128(_mm_cmpeq_epi32(rem_11, zero), _mm_cmpeq_epi32(rem_13, zero));\n\n        __m128i temp = nums;\n        __m128i counts = _mm_setzero_si128();\n\n        for(int shift = 0; shift < 10; shift += 4){\n            __m128i digits = _mm_and_si128(_mm_srli_epi32(temp, shift), mask);\n            __m128i eq_7 = _mm_cmpeq_epi32(digits, digit_7);\n            counts = _mm_add_epi32(counts, _mm_and_si128(eq_7, is_divisible));\n        }\n\n        int arr[4];\n        _mm_storeu_si128((__m128i*)arr, counts);\n        count += arr[0] + arr[1] + arr[2] + arr[3];\n    }\n\n    for(int i = (n / step) * step; i < n; ++i){\n        if((i % 11 == 0 || i % 13 == 0)){\n            int num = i;\n            while(num > 0){\n                if(num % 10 == 7) count++;\n                num /= 10;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    if (n <= 0) return 0;\n\n    const int block_size = 4;\n    int count = 0;\n    __m128i seven = _mm_set1_epi8('7');\n    __m128i mask = _mm_set1_epi8(0xF);\n\n    for(int i = 0; i < n; i += block_size){\n        int block[4] = {i, i+1, i+2, i+3};\n        __m128i v = _mm_loadu_si128((__m128i*)block);\n        __m128i mod_11 = _mm_set1_epi32(11);\n        __m128i mod_13 = _mm_set1_epi32(13);\n        __m128i zero = _mm_setzero_si128();\n\n        __m128i rem_11 = _mm_and_si128(_mm_rem_epu32(v, mod_11), mask);\n        __m128i rem_13 = _mm_and_si128(_mm_rem_epu32(v, mod_13), mask);\n\n        __m128i cmp_11 = _mm_cmpeq_epi32(rem_11, zero);\n        __m128i cmp_13 = _mm_cmpeq_epi32(rem_13, zero);\n\n        __m128i or_res = _mm_or_si128(cmp_11, cmp_13);\n\n        int res = _mm_movemask_epi8(or_res);\n\n        for(int j = 0; j < block_size; ++j){\n            if(res & (1 << (j*4))){\n                int num = i + j;\n                if(num % 11 == 0 || num % 13 == 0){\n                    int temp = num;\n                    while(temp){\n                        int digit = temp % 10;\n                        if(digit == 7){\n                            count++;\n                        }\n                        temp /= 10;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nint fizz_buzz_simd(int n){\n    int count = 0;\n    const int step = 16; // Process 16 integers at a time\n    __m128i divisor_11 = _mm_set1_epi32(11);\n    __m128i divisor_13 = _mm_set1_epi32(13);\n    __m128i digit_7 = _mm_set1_epi8('7');\n    __m128i zero = _mm_setzero_si128();\n    \n    for(int i = 0; i + step <= n; i += step){\n        __m128i nums = _mm_set_epi32(i+15, i+14, i+13, i+12, i+11, i+10, i+9, i+8, i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i rem_11 = _mm_rem_epi32(nums, divisor_11);\n        __m128i rem_13 = _mm_rem_epi32(nums, divisor_13);\n        __m128i mask_11 = _mm_cmpeq_epi32(rem_11, zero);\n        __m128i mask_13 = _mm_cmpeq_epi32(rem_13, zero);\n        __m128i mask = _mm_or_si128(mask_11, mask_13);\n        \n        // Extract numbers divisible by 11 or 13\n        __m128i filtered = _mm_and_si128(nums, mask);\n        \n        // Convert to bytes and check for '7'\n        __m128i bytes = _mm_cvtepi32_epi8(filtered);\n        __m128i cmp = _mm_cmpeq_epi8(bytes, digit_7);\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i res = _mm_and_si128(cmp, ones);\n        \n        // Sum the results\n        int temp_count[16];\n        _mm_storeu_si128((__m128i*)temp_count, res);\n        for(int j = 0; j < 16; ++j){\n            count += temp_count[j];\n        }\n    }\n    \n    // Handle remaining elements\n    for(int i = ((n / step) * step); i < n; ++i){\n        if((i % 11 == 0 || i % 13 == 0)){\n            int num = i;\n            while(num > 0){\n                if(num % 10 == 7){\n                    count++;\n                }\n                num /= 10;\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> sorted_l = l;\n\n    if (n == 0) return sorted_l;\n\n    size_t num_elements = n / 2 * 2; // Process even indices in pairs\n\n    // Extract even-indexed elements\n    std::vector<float> even_elements;\n    for (size_t i = 0; i < num_elements; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n\n    // Sort the even-indexed elements\n    std::sort(even_elements.begin(), even_elements.end());\n\n    // Reconstruct the vector with sorted even indices\n    size_t even_idx = 0;\n    for (size_t i = 0; i < num_elements; i += 2) {\n        __m128 even_val = _mm_set_ps(0, 0, even_elements[even_idx], l[i + 1]);\n        _mm_storeu_ps(&sorted_l[i], even_val);\n        even_idx++;\n    }\n\n    // Handle remaining element if n is odd\n    if (n % 2 != 0) {\n        sorted_l[n - 1] = l[n - 1];\n    }\n\n    return sorted_l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    size_t n = l.size();\n    std::vector<float> even_elements;\n    \n    // Extract even-indexed elements\n    for(size_t i = 0; i < n; i += 2){\n        even_elements.push_back(l[i]);\n    }\n    \n    // Sort the even-indexed elements\n    std::sort(even_elements.begin(), even_elements.end());\n    \n    // Interleave sorted even elements back into the vector\n    size_t even_idx = 0;\n    for(size_t i = 0; i < n; i += 2){\n        l[i] = even_elements[even_idx++];\n    }\n    \n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    size_t n = l.size();\n    std::vector<float> even_elements;\n    \n    // Extract even-indexed elements\n    for(size_t i = 0; i < n; i += 2){\n        even_elements.push_back(l[i]);\n    }\n    \n    // Sort the even-indexed elements\n    std::sort(even_elements.begin(), even_elements.end());\n    \n    // Create a vector to hold the result\n    std::vector<float> result(n);\n    \n    // Load sorted even elements back into result\n    size_t even_idx = 0;\n    for(size_t i = 0; i < n; i += 2){\n        result[i] = even_elements[even_idx++];\n    }\n    \n    // Copy odd-indexed elements from original vector\n    for(size_t i = 1; i < n; i += 2){\n        result[i] = l[i];\n    }\n    \n    // SIMD optimization for copying sorted even elements\n    size_t aligned_size = n / 4 * 4;\n    even_idx = 0;\n    for(size_t i = 0; i < aligned_size; i += 4){\n        if(i % 2 == 0){\n            __m128 even_vec = _mm_loadu_ps(&even_elements[even_idx]);\n            _mm_storeu_ps(&result[i], even_vec);\n            even_idx += 4 / 2;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    size_t n = l.size();\n    if (n == 0) return l;\n\n    // Extract even indices elements\n    std::vector<float> evens;\n    for(size_t i = 0; i < n; i += 2){\n        evens.push_back(l[i]);\n    }\n\n    // Sort the even indices elements\n    std::sort(evens.begin(), evens.end());\n\n    // Reconstruct the vector with sorted even indices\n    size_t even_idx = 0;\n    for(size_t i = 0; i < n; i += 2){\n        l[i] = evens[even_idx++];\n    }\n\n    // SIMD optimization for copying back (if possible)\n    size_t aligned_size = n / 4 * 4;\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        _mm_storeu_ps(&l[i], vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l) {\n    size_t n = l.size();\n    std::vector<float> even_elements;\n    \n    // Extract even-indexed elements\n    for (size_t i = 0; i < n; i += 2) {\n        even_elements.push_back(l[i]);\n    }\n    \n    // Sort the even-indexed elements\n    std::sort(even_elements.begin(), even_elements.end());\n    \n    // Interleave sorted even elements back into the vector\n    size_t j = 0;\n    for (size_t i = 0; i < n; i += 2) {\n        if (j < even_elements.size()) {\n            l[i] = even_elements[j];\n            j++;\n        }\n    }\n    \n    return l;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    if(n < 3) return false;\n\n    __m128i zero = _mm_set1_epi32(0);\n\n    for(int i=0; i < n-2; ++i){\n        for(int j=i+1; j < n-1; ++j){\n            __m128i sum_ij = _mm_set1_epi32(l[i] + l[j]);\n            for(int k=j+1; k < n; k+=4){\n                if(k+3 < n){\n                    __m128i v_k = _mm_loadu_si128((__m128i*)&l[k]);\n                    __m128i sums = _mm_add_epi32(sum_ij, v_k);\n                    __m128i cmp = _mm_cmpeq_epi32(sums, zero);\n                    if(_mm_movemask_epi8(cmp)){\n                        return true;\n                    }\n                }\n                else{\n                    for(int m=k; m<n; ++m){\n                        if(l[i] + l[j] + l[m] == 0){\n                            return true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    size_t n = l.size();\n    if (n < 3) return false;\n\n    __m128i zero = _mm_setzero_si128();\n    for(size_t i = 0; i < n - 2; ++i){\n        __m128i a = _mm_set1_epi32(l[i]);\n        for(size_t j = i + 1; j < n - 1; ++j){\n            __m128i b = _mm_set1_epi32(l[j]);\n            __m128i sum_ab = _mm_add_epi32(a, b);\n            for(size_t k = j + 1; k < n; k += 4){\n                if(k + 3 < n){\n                    __m128i c = _mm_loadu_si128((__m128i*)&l[k]);\n                    __m128i sums = _mm_add_epi32(sum_ab, c);\n                    __m128i cmp = _mm_cmpeq_epi32(sums, zero);\n                    if(_mm_movemask_epi8(cmp) != 0){\n                        return true;\n                    }\n                }\n                else{\n                    for(size_t m = k; m < n; ++m){\n                        if(l[i] + l[j] + l[m] == 0){\n                            return true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    if(n < 3) return false;\n\n    std::vector<int> sorted_l = l;\n    std::sort(sorted_l.begin(), sorted_l.end());\n\n    for(int i = 0; i < n - 2; ++i){\n        if(i > 0 && sorted_l[i] == sorted_l[i-1]) continue;\n        int left = i + 1;\n        int right = n - 1;\n\n        while(left < right){\n            __m128i vleft = _mm_set1_epi32(sorted_l[left]);\n            __m128i vright = _mm_set1_epi32(sorted_l[right]);\n            __m128i vi = _mm_set1_epi32(sorted_l[i]);\n\n            __m128i sum = _mm_add_epi32(_mm_add_epi32(vi, vleft), vright);\n            __m128i zero = _mm_setzero_si128();\n\n            if(_mm_movemask_epi8(_mm_cmpeq_epi32(sum, zero)) != 0){\n                return true;\n            }\n\n            if(sorted_l[left] + sorted_l[right] < -sorted_l[i]){\n                ++left;\n            }\n            else{\n                --right;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    if(n < 3) return false;\n\n    std::vector<int> sorted_l = l;\n    std::sort(sorted_l.begin(), sorted_l.end());\n\n    for(int i = 0; i < n - 2; ++i){\n        if(i > 0 && sorted_l[i] == sorted_l[i-1]) continue;\n        int left = i + 1;\n        int right = n - 1;\n\n        while(left < right){\n            __m128i vleft = _mm_set1_epi32(sorted_l[left]);\n            __m128i vright = _mm_set1_epi32(sorted_l[right]);\n            __m128i vi = _mm_set1_epi32(sorted_l[i]);\n\n            __m128i sum = _mm_add_epi32(_mm_add_epi32(vi, vleft), vright);\n            __m128i zero = _mm_setzero_si128();\n\n            if(_mm_movemask_epi8(_mm_cmpeq_epi32(sum, zero))){\n                return true;\n            }\n\n            if(sorted_l[left] + sorted_l[right] < -sorted_l[i]){\n                ++left;\n            }\n            else{\n                --right;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    if(n < 3) return false;\n\n    __m128i zero = _mm_set1_epi32(0);\n\n    for(int i = 0; i < n - 2; ++i){\n        __m128i a = _mm_set1_epi32(l[i]);\n        for(int j = i + 1; j < n - 1; ++j){\n            __m128i b = _mm_set1_epi32(l[j]);\n            __m128i sum_ab = _mm_add_epi32(a, b);\n            for(int k = j + 1; k < n; k += 4){\n                if(k + 3 < n){\n                    __m128i c = _mm_loadu_si128((__m128i*)(&l[k]));\n                    __m128i sum_abc = _mm_add_epi32(sum_ab, c);\n                    __m128i cmp = _mm_cmpeq_epi32(sum_abc, zero);\n                    if(_mm_movemask_epi8(cmp)){\n                        return true;\n                    }\n                }\n                else{\n                    for(int m = k; m < n; ++m){\n                        if(l[i] + l[j] + l[m] == 0){\n                            return true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    \n    __m128i increment = _mm_set1_epi32(1);\n    size_t aligned_size = size - (size % 4);\n    \n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        vec = _mm_add_epi32(vec, increment);\n        _mm_storeu_si128((__m128i*)&result[i], vec);\n    }\n    \n    for(size_t i = aligned_size; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    __m128i increment = _mm_set1_epi32(1);\n    size_t aligned_size = size - (size % 4);\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i res = _mm_add_epi32(vec, increment);\n        _mm_storeu_si128((__m128i*)&result[i], res);\n    }\n\n    for(size_t i = aligned_size; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    const int increment = 1;\n    __m128i vincr = _mm_set1_epi32(increment);\n\n    for(size_t i = 0; i < size / 4 * 4; i += 4){\n        __m128i vdata = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i vres = _mm_add_epi32(vdata, vincr);\n        _mm_storeu_si128((__m128i*)&result[i], vres);\n    }\n\n    for(size_t i = size / 4 * 4; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    \n    const int increment = 1;\n    __m128i vincr = _mm_set1_epi32(increment);\n    \n    size_t i = 0;\n    for (; i + 3 < size; i += 4) {\n        __m128i vdata = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i vresult = _mm_add_epi32(vdata, vincr);\n        _mm_storeu_si128((__m128i*)&result[i], vresult);\n    }\n    \n    for (; i < size; ++i) {\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t size = l.size();\n    std::vector<int> result(size);\n    __m128i increment = _mm_set1_epi32(1);\n\n    for(size_t i = 0; i < size; i += 4){\n        if(i + 4 <= size){\n            __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n            __m128i res = _mm_add_epi32(vec, increment);\n            _mm_storeu_si128((__m128i*)&result[i], res);\n        }\n        else{\n            for(size_t j = i; j < size; ++j){\n                result[j] = l[j] + 1;\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n\n    __m128i zero = _mm_setzero_si128();\n    size_t n = l.size();\n    size_t aligned_size = n - (n % 4);\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128i v1 = _mm_loadu_si128((__m128i*)&l[i]);\n        for(size_t j = i + 4; j < aligned_size; j += 4){\n            __m128i v2 = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(v1, v2);\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            if(_mm_movemask_epi8(cmp) != 0){\n                return true;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = aligned_size; i < n; ++i){\n        for(size_t j = i + 1; j < n; ++j){\n            if(l[i] + l[j] == 0){\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n\n    size_t n = l.size();\n    __m128i zero = _mm_setzero_si128();\n\n    for(size_t i = 0; i < n - 1; ++i){\n        __m128i v_i = _mm_set1_epi32(l[i]);\n        \n        for(size_t j = i + 1; j < n; j += 4){\n            if(j + 3 < n){\n                __m128i v_j = _mm_loadu_si128((__m128i*)&l[j]);\n                __m128i sum = _mm_add_epi32(v_i, v_j);\n                __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n                int mask = _mm_movemask_epi8(cmp);\n                if(mask != 0) return true;\n            }\n            else{\n                for(size_t k = j; k < n; ++k){\n                    if(l[i] + l[k] == 0) return true;\n                }\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n\n    size_t n = l.size();\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < n - 1; ++i) {\n        __m128i v_i = _mm_set1_epi32(l[i]);\n        for (size_t j = i + 1; j < n; j += 4) {\n            size_t end_j = std::min(j + 4, n);\n            __m128i v_j = _mm_set_epi32(0, 0, 0, 0);\n            size_t k;\n            for (k = j; k < end_j; ++k) {\n                v_j = _mm_insert_epi32(v_j, l[k], k - j);\n            }\n            __m128i sum = _mm_add_epi32(v_i, v_j);\n            __m128i cmp = _mm_cmpeq_epi32(sum, zero);\n            int mask = _mm_movemask_epi8(cmp);\n            if (mask != 0) return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n\n    size_t n = l.size();\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < n; ++i) {\n        int x = l[i];\n        __m128i vx = _mm_set1_epi32(x);\n\n        for (size_t j = i + 1; j < n; j += 4) {\n            if (n - j < 4) {\n                for (size_t k = j; k < n; ++k) {\n                    if (l[k] + x == 0) return true;\n                }\n                break;\n            }\n\n            __m128i vj = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(vj, vx);\n            __m128i eq = _mm_cmpeq_epi32(sum, zero);\n            if (_mm_movemask_epi8(eq) != 0) return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    if (l.size() < 2) return false;\n\n    size_t n = l.size();\n    __m128i zero = _mm_setzero_si128();\n\n    for(size_t i = 0; i < n; ++i){\n        __m128i neg = _mm_set1_epi32(-l[i]);\n        for(size_t j = i + 1; j + 4 <= n; j += 4){\n            __m128i vec = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i sum = _mm_add_epi32(vec, neg);\n            __m128i mask = _mm_cmpeq_epi32(sum, zero);\n            if(_mm_movemask_epi8(mask) != 0){\n                return true;\n            }\n        }\n        // Handle remaining elements that don't fit into the last SIMD vector\n        for(size_t j = ((n / 4) * 4); j < n; ++j){\n            if(i != j && l[i] + l[j] == 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <array>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 9) return \"\";\n    if (x == 0) return \"0\";\n\n    const __m128i zero = _mm_setzero_si128();\n    __m128i digits = _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    __m128i mask = _mm_set1_epi8(0x0F);\n    __m128i rem = _mm_set1_epi8(base);\n\n    int64_t num = x;\n    int count = 0;\n\n    std::array<char, 32> result;\n    while (num > 0) {\n        __m128i n = _mm_set1_epi64x(num);\n        __m128i r = _mm_cvtepi8_epi16(_mm_and_si128(_mm_set_epi64x(0, num), mask));\n        __m128i quot = _mm_srli_epi16(_mm_div_epi16(n, rem), 4);\n        __m128i digit = _mm_sub_epi8(n, _mm_mullo_epi16(quot, rem));\n        digits = _mm_insert_epi8(digits, _mm_extract_epi8(digit, 0), count++);\n        num = _mm_extract_epi16(quot, 0);\n    }\n\n    for (int i = count - 1; i >= 0; --i) {\n        result[count - i - 1] = '0' + _mm_extract_epi8(digits, i);\n    }\n\n    return std::string(result.begin(), result.begin() + count);\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <array>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 9) return \"\";\n\n    std::string result;\n    bool is_negative = false;\n\n    if (x == 0) return \"0\";\n\n    if (x < 0){\n        is_negative = true;\n        x = -x;\n    }\n\n    __m128i vbase = _mm_set1_epi8(base);\n    __m128i vx = _mm_cvtsi64_si128(x);\n    __m128i zero = _mm_setzero_si128();\n    __m128i remainder_mask = _mm_set_epi8(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15);\n    std::array<int, 16> digits;\n\n    while(true){\n        __m128i vrem = _mm_cvtsi64_si128(x % base);\n        __m128i vquotient = _mm_cvtsi64_si128(x / base);\n        x /= base;\n\n        __m128i cmp_result = _mm_cmpeq_epi8(vrem, remainder_mask);\n        __m128i vzero = _mm_and_si128(cmp_result, vrem);\n\n        digits[digits.size() - 1] = _mm_extract_epi8(vrem, 0);\n        if(_mm_movemask_epi8(_mm_cmpeq_epi8(vx, zero))){\n            break;\n        }\n        vx = vquotient;\n    }\n\n    for(auto it = digits.rbegin(); it != digits.rend(); ++it){\n        if(*it >=0 && *it <10){\n            result += '0' + *it;\n        }\n    }\n\n    if(is_negative){\n        result = \"-\" + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <array>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if(base < 2 || base > 9) return \"\";\n\n    std::string result;\n    bool is_negative = false;\n\n    if(x == 0){\n        return \"0\";\n    }\n\n    if(x < 0){\n        is_negative = true;\n        x = -x;\n    }\n\n    __m128i vbase = _mm_set_epi8(base, base, base, base, base, base, base, base, base, base, base, base, base, base, base, base);\n    __m128i vx = _mm_set_epi64x(0, x);\n    __m128i zero = _mm_setzero_si128();\n    __m128i remainder_vec = _mm_setzero_si128();\n    std::array<int, 16> digits;\n\n    int count = 0;\n    while(true){\n        __m128i div_result = _mm_div_epu32(vx, vbase); // Hypothetical intrinsic for SIMD division\n        __m128i remainder = _mm_sub_epi64(vx, _mm_mullo_epi32(div_result, vbase));\n        if(_mm_cvtsi128_si32(vx) == 0 && count != 0){\n            break;\n        }\n        vx = div_result;\n        remainder_vec = _mm_or_si128(remainder_vec, _mm_slli_si128(remainder, count));\n        count++;\n    }\n\n    for(int i = count - 1; i >= 0; --i){\n        __m128i digit_val = _mm_shuffle_epi32(remainder_vec, _MM_SHUFFLE(i,i,i,i));\n        digits[i] = _mm_cvtsi128_si32(digit_val) + '0';\n        result += static_cast<char>(digits[i]);\n    }\n\n    if(is_negative){\n        result = \"-\" + result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <array>\n#include <algorithm>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if(base < 2 || base > 10) return \"\";\n    if(x == 0) return \"0\";\n\n    __m128i vbase = _mm_set1_epi8(base);\n    __m128i vx = _mm_cvtsi64_si128(x);\n    __m128i zero = _mm_setzero_si128();\n    __m128i remainder;\n    std::string result;\n\n    bool is_negative = false;\n    if(x < 0){\n        is_negative = true;\n        vx = _mm_sub_epi64(zero, vx);\n    }\n\n    while(true){\n        __m128i div_result = _mm_div_epu32(vx, vbase); // Simulated division\n        remainder = _mm_and_si128(_mm_sub_epi64(vx, _mm_mullo_epi32(div_result, vbase)), _mm_set_epi32(0, 0, 0, 0xFFFFFFFF));\n        char digit = static_cast<char>(_mm_extract_epi32(remainder, 0)) + '0';\n        result += digit;\n        vx = div_result;\n        if(_mm_cmpeq_epi32(vx, zero).m128i_u32[0]) break;\n    }\n\n    if(is_negative) result += '-';\n\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <array>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if (base < 2 || base > 9) return \"\";\n\n    std::string result;\n    bool is_negative = false;\n\n    if (x == 0) return \"0\";\n\n    if (x < 0) {\n        is_negative = true;\n        x = -x;\n    }\n\n    __m128i vbase = _mm_set1_epi8(base);\n    __m128i vx = _mm_cvtsi64_si128(x);\n    __m128i zero = _mm_setzero_si128();\n    __m128i ten = _mm_set1_epi8('0');\n    __m128i remainder;\n    std::array<int, 16> digits;\n\n    int count = 0;\n    while (!_mm_testz_si128(vx, vx)) {\n        __m128i div_result = _mm_div_epi32(vx, vbase); // Simulated division\n        remainder = _mm_sub_epi32(vx, _mm_mullo_epi32(div_result, vbase));\n        digits[count++] = _mm_extract_epi32(remainder, 0) + '0';\n        vx = div_result;\n    }\n\n    if (is_negative) {\n        result += '-';\n    }\n\n    for (int i = count - 1; i >= 0; --i) {\n        result += static_cast<char>(digits[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    const size_t simd_width = 16;\n    size_t i = 0;\n    size_t j = len - 1;\n\n    for (; j - i >= simd_width; i += simd_width, j -= simd_width) {\n        __m128i left = _mm_loadu_si128((__m128i*)&text[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&text[j - simd_width + 1]);\n        __m128i reversed_right = _mm_shuffle_epi8(right, _mm_set_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        __m128i cmp = _mm_cmpeq_epi8(left, reversed_right);\n        if (!_mm_movemask_epi8(cmp) == 0xFFFF) {\n            return false;\n        }\n    }\n\n    // Check remaining characters\n    for (; i < j; ++i, --j) {\n        if (text[i] != text[j]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    const size_t step = 16;\n    size_t i = 0;\n    __m128i cmpmask;\n\n    for (i = 0; i + step <= len / 2; i += step){\n        __m128i left = _mm_loadu_si128((__m128i*)&text[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&text[len - i - step]);\n\n        cmpmask = _mm_cmpeq_epi8(left, right);\n        if (_mm_movemask_epi8(cmpmask) != 0xFFFF)\n            return false;\n    }\n\n    for (; i < len / 2; ++i){\n        if (text[i] != text[len - i - 1])\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    const size_t simd_width = 16;\n    size_t i = 0;\n    size_t j = len - 1;\n\n    // Process in SIMD chunks\n    for (; i + simd_width <= j; i += simd_width, j -= simd_width) {\n        __m128i left = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&text[i]));\n        __m128i right = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&text[j]));\n\n        __m128i eq = _mm_cmpeq_epi8(left, _mm_reverse_epi8(right));\n        int mask = _mm_movemask_epi8(eq);\n        if (mask != 0xFFFF) return false;\n    }\n\n    // Process remaining scalar elements\n    for (; i < j; ++i, --j) {\n        if (text[i] != text[j]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nbool is_palindrome_simd(std::string text){\n    size_t len = text.length();\n    if (len == 0) return true;\n\n    size_t half_len = len / 2;\n    const char* data = text.data();\n\n    for(size_t i = 0; i + 15 < half_len; i += 16){\n        __m128i left = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i right = _mm_loadu_si128((__m128i*)(data + len - i - 16));\n\n        __m128i eq = _mm_cmpeq_epi8(left, right);\n        if(_mm_movemask_epi8(eq) != 0xFFFF){\n            return false;\n        }\n    }\n\n    for(size_t i = half_len - (half_len % 16); i < half_len; ++i){\n        if(data[i] != data[len - i - 1]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text) {\n    size_t len = text.length();\n    if (len <= 1) return true;\n\n    size_t half_len = len / 2;\n    const char* str = text.c_str();\n\n    size_t i = 0;\n    for (; i + 15 < half_len; i += 16) {\n        __m128i left = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i right = _mm_loadu_si128((__m128i*)(str + len - i - 16));\n\n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < half_len; ++i) {\n        if (str[i] != str[len - i - 1]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod = 26;\n    const int base = 'a';\n    const int num_elements = 16;\n\n    std::string out;\n    out.reserve(s.length());\n\n    __m128i v_shift = _mm_set1_epi8(shift);\n    __m128i v_mod = _mm_set1_epi8(mod);\n    __m128i v_base = _mm_set1_epi8(base);\n    __m128i v_zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < s.length(); i += num_elements) {\n        size_t chunk_size = std::min((size_t)num_elements, s.length() - i);\n\n        __m128i v_data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i v_sub = _mm_sub_epi8(v_data, v_base);\n        __m128i v_add = _mm_add_epi8(v_sub, v_shift);\n\n        // Compute ((x + shift) % mod + base)\n        __m128i v_cmp = _mm_cmplt_epi8(v_add, v_zero);\n        __m128i v_adj = _mm_add_epi8(v_add, _mm_and_si128(v_mod, v_cmp));\n        __m128i v_res = _mm_add_epi8(v_adj, v_base);\n\n        if (chunk_size == num_elements) {\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), v_res);\n        } else {\n            char temp[num_elements];\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), v_res);\n            for (size_t j = 0; j < chunk_size; ++j) {\n                out.push_back(temp[j]);\n            }\n        }\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod = 26;\n    const int base = 'a';\n    std::string out;\n    out.reserve(s.length());\n\n    __m128i v_shift = _mm_set1_epi8(shift);\n    __m128i v_mod = _mm_set1_epi8(mod);\n    __m128i v_base = _mm_set1_epi8(base);\n    __m128i v_a = _mm_set1_epi8('a');\n    __m128i v_z = _mm_set1_epi8('z');\n\n    for (size_t i = 0; i < s.length(); i += 16) {\n        size_t chunk_size = std::min((size_t)16, s.length() - i);\n        __m128i v_in = _mm_loadu_si128((__m128i const*)(s.data() + i));\n        \n        // Subtract 'a'\n        __m128i v_temp = _mm_sub_epi8(v_in, v_base);\n\n        // Add shift\n        v_temp = _mm_add_epi8(v_temp, v_shift);\n\n        // Perform modulo 26\n        __m128i v_cmp = _mm_cmplt_epi8(v_temp, _mm_setzero_si128());\n        __m128i v_mask = _mm_and_si128(v_cmp, v_mod);\n        v_temp = _mm_add_epi8(v_temp, v_mask);\n\n        // Add 'a' back\n        v_temp = _mm_add_epi8(v_temp, v_base);\n\n        // Store result\n        if (chunk_size == 16) {\n            char buffer[16];\n            _mm_storeu_si128((__m128i*)buffer, v_temp);\n            out.append(buffer, 16);\n        } else {\n            alignas(16) char buffer[16];\n            _mm_store_si128((__m128i*)buffer, v_temp);\n            out.append(buffer, chunk_size);\n        }\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const __m128i vshift = _mm_set1_epi8(shift);\n    const __m128i va = _mm_set1_epi8('a');\n    const __m128i v26 = _mm_set1_epi8(26);\n\n    std::string out;\n    out.reserve(s.size());\n\n    int i = 0;\n    for (; i + 15 < s.size(); i += 16) {\n        __m128i vchar = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i vtemp = _mm_sub_epi8(vchar, va);\n        __m128i vmod = _mm_add_epi8(vtemp, v26);\n        vmod = _mm_add_epi8(vmod, vshift);\n        vmod = _mm_and_si128(vmod, _mm_cmplt_epi8(vmod, v26));\n        vmod = _mm_add_epi8(vmod, va);\n        __m128i vresult = _mm_and_si128(vmod, _mm_cmpgt_epi8(vmod, _mm_setzero_si128()));\n\n        char result[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result), vresult);\n        out.append(result, 16);\n    }\n\n    for (; i < s.size(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out += static_cast<char>(w);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod_base = 26;\n    const char base_char = 'a';\n    const int vector_size = 16;\n    std::string out;\n\n    int len = s.length();\n    int aligned_len = len - (len % vector_size);\n\n    for(int i = 0; i < aligned_len; i += vector_size){\n        __m128i vec = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i base = _mm_set1_epi8(base_char);\n        __m128i shft = _mm_set1_epi8(shift);\n\n        __m128i sub = _mm_sub_epi8(vec, base);\n        __m128i add = _mm_add_epi8(sub, shft);\n\n        // Compute ((x + shift) % 26 + 'a') for each byte\n        __m128i cmp_ge_0 = _mm_cmpgt_epi8(add, _mm_setzero_si128());\n        __m128i mask_neg = _mm_cmplt_epi8(add, _mm_setzero_si128());\n        __m128i adj = _mm_and_si128(mask_neg, _mm_set1_epi8(mod_base));\n        __m128i mod = _mm_add_epi8(add, adj);\n\n        __m128i result = _mm_add_epi8(mod, base);\n\n        // Store the result\n        char temp[vector_size];\n        _mm_storeu_si128((__m128i*)temp, result);\n        out.append(temp, vector_size);\n    }\n\n    // Handle remaining characters\n    for(int i = aligned_len; i < len; ++i){\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out += (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string decode_shift_simd(std::string s){\n    const int shift = -5;\n    const int mod = 26;\n    const int base = 'a';\n    const int chars_per_reg = 16;\n\n    std::string out;\n    out.reserve(s.length());\n\n    for (size_t i = 0; i < s.length(); i += chars_per_reg) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i a = _mm_set1_epi8(base);\n        __m128i minus_a = _mm_sub_epi8(chunk, a);\n        __m128i shifted = _mm_add_epi8(minus_a, _mm_set1_epi8(shift));\n        \n        // Compute ((x + shift) % mod + mod) % mod to handle negative values\n        __m128i mod_val = _mm_set1_epi8(mod);\n        __m128i temp = _mm_add_epi8(shifted, mod_val);\n        __m128i modded = _mm_and_si128(_mm_cmplt_epi8(temp, mod_val), temp);\n        __m128i result = _mm_add_epi8(modded, a);\n\n        // Handle remaining characters if not aligned\n        size_t limit = std::min(i + chars_per_reg, s.length());\n        for (size_t j = i; j < limit; ++j) {\n            out += static_cast<char>(_mm_extract_epi8(result, j - i));\n        }\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <cstdint>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n\n    // Process 4 elements at a time using SIMD\n    for (; i + 4 <= l.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)&l[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, threshold); // Compare vec > threshold\n        if (_mm_movemask_epi8(cmp) != 0) return false; // If any element is greater, return false\n    }\n\n    // Handle remaining elements\n    for (; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n\n    for (; i + 3 < l.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        __m128i mask = _mm_cmpeq_epi32(cmp, _mm_set1_epi32(-1));\n        if (!_mm_movemask_epi8(mask)) {\n            return false;\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t vec_size = l.size();\n    size_t aligned_size = vec_size - (vec_size % 4);\n\n    bool result = true;\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128i data = _mm_loadu_si128((__m128i const*)(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(data, threshold);\n        __m128i mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            result = false;\n            break;\n        }\n    }\n\n    for (size_t i = aligned_size; i < vec_size; ++i) {\n        if (l[i] >= t) {\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t){\n    if (l.empty()) return true;\n\n    __m128i vt = _mm_set1_epi32(t);\n    size_t i = 0;\n    size_t len = l.size();\n\n    for (; i + 4 <= len; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i const*)(&l[i]));\n        __m128i cmp = _mm_cmpgt_epi32(v, vt);\n        if (_mm_movemask_epi8(cmp) != 0) {\n            return false;\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (l[i] >= t) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.empty()) return true;\n\n    __m128i threshold = _mm_set1_epi32(t);\n    size_t i = 0;\n\n    for (; i + 3 < l.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&l[i]));\n        __m128i cmp = _mm_cmplt_epi32(vec, threshold);\n        __m128i mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i < l.size(); ++i) {\n        if (l[i] >= t) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    const int n = brackets.length();\n    const int simd_width = 16; // Bytes per SIMD register\n    const int num_simd = n / simd_width;\n    const char* data = brackets.data();\n\n    for(int i = 0; i < num_simd * simd_width; i += simd_width){\n        __m128i chunk = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i less = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i greater = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n        \n        int open_count = _mm_popcnt_u32(_mm_movemask_epi8(less));\n        int close_count = _mm_popcnt_u32(_mm_movemask_epi8(greater));\n\n        balance += open_count - close_count;\n        if(balance < 0) return false;\n    }\n\n    // Handle remaining elements\n    for(int i = num_simd * simd_width; i < n; ++i){\n        if(data[i] == '<') balance++;\n        else if(data[i] == '>') balance--;\n        if(balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    const int n = brackets.size();\n    if (n == 0) return true;\n\n    const int simd_width = 16; // Bytes processed per SIMD operation\n    const char* data = brackets.c_str();\n\n    for(int i = 0; i < n; i += simd_width){\n        __m128i chunk = _mm_loadu_si128((__m128i*)&data[i]);\n        __m128i less = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i great = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n        \n        int open = _mm_popcnt_u32(_mm_movemask_epi8(less));\n        int close = _mm_popcnt_u32(_mm_movemask_epi8(great));\n\n        balance += open - close;\n        if(balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    const int simd_width = 16; // Bytes processed per SIMD operation\n    size_t n = brackets.size();\n    \n    for(size_t i = 0; i < n; i += simd_width){\n        __m128i chunk = _mm_loadu_si128((__m128i const*)&brackets[i]);\n        __m128i less = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i greater = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n        \n        int open_count = _mm_popcnt_u32(_mm_movemask_epi8(less));\n        int close_count = _mm_popcnt_u32(_mm_movemask_epi8(greater));\n        \n        balance += open_count - close_count;\n        if(balance < 0) return false;\n    }\n    \n    if(balance != 0) return false;\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    int balance = 0;\n    const int N = 16; // SSE register width in bytes\n    size_t length = brackets.length();\n    \n    for(size_t i = 0; i < length; i += N){\n        __m128i chunk = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i less = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('<'));\n        __m128i greater = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('>'));\n        \n        int open = _mm_popcnt_u32(_mm_movemask_epi8(less));\n        int close = _mm_popcnt_u32(_mm_movemask_epi8(greater));\n        \n        balance += open - close;\n        if(balance < 0) return false;\n    }\n    \n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int len = brackets.length();\n    if (len == 0) return true;\n\n    const int simd_width = 16;\n    __m128i zero = _mm_setzero_si128();\n    __m128i balance = _mm_setzero_si128();\n    __m128i open = _mm_set1_epi8('<');\n    __m128i close = _mm_set1_epi8('>');\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n\n    for(int i = 0; i < len; i += simd_width){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.c_str() + i));\n        __m128i cmp_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i cmp_close = _mm_cmpeq_epi8(chunk, close);\n        \n        __m128i inc = _mm_and_si128(cmp_open, one);\n        __m128i dec = _mm_andnot_si128(cmp_open, cmp_close);\n        dec = _mm_and_si128(dec, neg_one);\n\n        __m128i diff = _mm_sub_epi8(inc, dec);\n        balance = _mm_add_epi8(balance, diff);\n\n        __m128i negative_check = _mm_cmplt_epi8(balance, zero);\n        if (!_mm_testz_si128(negative_check, negative_check)) {\n            return false;\n        }\n    }\n\n    int sum = 0;\n    alignas(16) char buffer[16];\n    _mm_store_si128((__m128i*)buffer, balance);\n    for(int i = 0; i < simd_width; ++i){\n        sum += static_cast<unsigned char>(buffer[i]);\n    }\n\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cstring>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    if (brackets.empty()) return true;\n\n    const int n = brackets.size();\n    const int vec_size = 16; // SSE2 operates on 128-bit registers (16 bytes)\n    __m128i open = _mm_set1_epi8('(');\n    __m128i close = _mm_set1_epi8(')');\n    int balance = 0;\n\n    for (int i = 0; i < n; i += vec_size) {\n        int chunk_size = std::min(n - i, vec_size);\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i eq_open = _mm_cmpeq_epi8(data, open);\n        __m128i eq_close = _mm_cmpeq_epi8(data, close);\n\n        int count_open = _mm_popcnt_u32(_mm_movemask_epi8(eq_open));\n        int count_close = _mm_popcnt_u32(_mm_movemask_epi8(eq_close));\n\n        balance += count_open - count_close;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int simd_width = 16;\n    __m128i zero = _mm_setzero_si128();\n    __m128i balance = _mm_setzero_si128();\n    __m128i open = _mm_set1_epi8('(');\n    __m128i close = _mm_set1_epi8(')');\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i < brackets.size(); i += simd_width) {\n        size_t chunk_size = std::min(brackets.size() - i, static_cast<size_t>(simd_width));\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(brackets.data() + i));\n        __m128i is_open = _mm_cmpeq_epi8(data, open);\n        __m128i is_close = _mm_cmpeq_epi8(data, close);\n        __m128i inc = _mm_and_si128(is_open, one);\n        __m128i dec = _mm_and_si128(is_close, neg_one);\n        __m128i delta = _mm_add_epi8(inc, dec);\n        balance = _mm_add_epi8(balance, delta);\n\n        // Check if balance ever goes negative\n        __m128i negative_check = _mm_cmpgt_epi8(zero, balance);\n        if (_mm_movemask_epi8(negative_check)) {\n            return false;\n        }\n\n        if (chunk_size < simd_width) {\n            __m128i mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1);\n            mask = _mm_srli_si128(mask, simd_width - chunk_size);\n            balance = _mm_and_si128(balance, mask);\n        }\n    }\n\n    // Horizontal sum of balance\n    int sum = 0;\n    alignas(16) char temp[16];\n    _mm_store_si128((__m128i*)temp, balance);\n    for (int i = 0; i < 16; ++i) {\n        sum += static_cast<int8_t>(temp[i]);\n    }\n\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int size = brackets.size();\n    if (size == 0) return true;\n\n    const int simd_width = 16;\n    __m128i zero = _mm_setzero_si128();\n    __m128i balance = _mm_setzero_si128();\n\n    for (int i = 0; i < size; i += simd_width) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open = _mm_cmpeq_epi8(chunk, _mm_set1_epi8('('));\n        __m128i close = _mm_cmpeq_epi8(chunk, _mm_set1_epi8(')'));\n\n        __m128i open_count = _mm_sad_epu8(open, zero);\n        __m128i close_count = _mm_sad_epu8(close, zero);\n\n        open_count = _mm_shuffle_epi32(open_count, 0xFF);\n        close_count = _mm_shuffle_epi32(close_count, 0xFF);\n\n        balance = _mm_add_epi32(balance, _mm_sub_epi32(open_count, close_count));\n\n        __m128i negative_check = _mm_cmpgt_epi32(zero, balance);\n        if (!_mm_testz_si128(negative_check, negative_check)) {\n            return false;\n        }\n    }\n\n    __m128i final_balance = _mm_shuffle_epi32(balance, 0xFF);\n    int result = _mm_cvtsi128_si32(final_balance);\n    return result == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int len = brackets.length();\n    if (len == 0) return true;\n\n    const int simd_width = 16;\n    __m128i zero = _mm_setzero_si128();\n    __m128i balance = _mm_setzero_si128();\n    __m128i open = _mm_set1_epi8('(');\n    __m128i close = _mm_set1_epi8(')');\n    __m128i one = _mm_set1_epi8(1);\n    __m128i neg_one = _mm_set1_epi8(-1);\n\n    for (size_t i = 0; i < len; i += simd_width) {\n        size_t chunk_size = std::min<size_t>(simd_width, len - i);\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n\n        __m128i eq_open = _mm_cmpeq_epi8(data, open);\n        __m128i eq_close = _mm_cmpeq_epi8(data, close);\n\n        __m128i inc = _mm_and_si128(eq_open, one);\n        __m128i dec = _mm_and_si128(eq_close, neg_one);\n\n        __m128i delta = _mm_add_epi8(inc, dec);\n        balance = _mm_add_epi8(balance, delta);\n\n        __m128i negative_check = _mm_cmpgt_epi8(zero, balance);\n        int mask = _mm_movemask_epi8(negative_check);\n        if (mask != 0) return false;\n\n        if (chunk_size < simd_width) {\n            __m128i valid_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1);\n            valid_mask = _mm_srli_si128(valid_mask, simd_width - chunk_size);\n            balance = _mm_andnot_si128(valid_mask, balance);\n        }\n    }\n\n    int sum = 0;\n    alignas(16) char temp[16];\n    _mm_store_si128((__m128i*)temp, balance);\n    for (int i = 0; i < 16; ++i) {\n        sum += static_cast<int>(temp[i]);\n    }\n\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const int N = 16;\n    __m128i open = _mm_set1_epi8('(');\n    __m128i close = _mm_set1_epi8(')');\n    int balance = 0;\n\n    for (size_t i = 0; i + N <= brackets.size(); i += N) {\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i eq_open = _mm_cmpeq_epi8(chunk, open);\n        __m128i eq_close = _mm_cmpeq_epi8(chunk, close);\n\n        int count_open = _mm_popcnt_u32(_mm_movemask_epi8(eq_open));\n        int count_close = _mm_popcnt_u32(_mm_movemask_epi8(eq_close));\n\n        balance += count_open - count_close;\n        if (balance < 0) return false;\n    }\n\n    // Handle remaining characters\n    for (size_t i = (brackets.size() / N) * N; i < brackets.size(); ++i) {\n        if (brackets[i] == '(') balance++;\n        else if (brackets[i] == ')') balance--;\n        if (balance < 0) return false;\n    }\n\n    return balance == 0;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1, 0.0f);\n\n    __m128 multiplier = _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f);\n    size_t aligned_size = (n - 1) / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 indices = _mm_set_ps(i + 3, i + 2, i + 1, i);\n        __m128 prod = _mm_mul_ps(coeffs, indices);\n        _mm_storeu_ps(&result[i], prod);\n        multiplier = _mm_add_ps(multiplier, _mm_set1_ps(4.0f));\n    }\n\n    for (size_t i = aligned_size; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1, 0.0f);\n\n    __m128 multiplier = _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f);\n    size_t aligned_size = (n - 1) / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 indices = _mm_set_ps(i + 3, i + 2, i + 1, i);\n        __m128 prod = _mm_mul_ps(coeffs, indices);\n        _mm_storeu_ps(&result[i], prod);\n        multiplier = _mm_add_ps(multiplier, _mm_set1_ps(1.0f));\n    }\n\n    for (size_t i = aligned_size; i < n - 1; ++i) {\n        result[i] = static_cast<float>(i + 1) * xs[i + 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1, 0.0f);\n\n    // Create a vector of coefficients {1.0, 2.0, 3.0, 4.0} up to n-1\n    std::vector<float> coeff(n - 1);\n    for (size_t i = 0; i < n - 1; ++i) {\n        coeff[i] = static_cast<float>(i + 1);\n    }\n\n    size_t aligned_size = (n - 1) / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 xs_vec = _mm_loadu_ps(&xs[i + 1]);\n        __m128 coeff_vec = _mm_loadu_ps(&coeff[i]);\n        __m128 res_vec = _mm_mul_ps(xs_vec, coeff_vec);\n        _mm_storeu_ps(&result[i], res_vec);\n    }\n\n    for (size_t i = aligned_size; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * coeff[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1, 0.0f);\n\n    __m128 multiplier = _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f);\n    size_t aligned_size = (n - 1) / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i]);\n        __m128 indices = _mm_set_ps(i + 3, i + 2, i + 1, i);\n        __m128 product = _mm_mul_ps(coeffs, indices);\n        _mm_storeu_ps(&result[i], product);\n    }\n\n    for (size_t i = aligned_size; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    size_t n = xs.size();\n    if (n <= 1) return {};\n\n    std::vector<float> result(n - 1, 0.0f);\n\n    __m128 multiplier = _mm_set_ps(3.0f, 2.0f, 1.0f, 0.0f);\n    size_t aligned_size = (n - 1) / 4 * 4;\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 coeffs = _mm_loadu_ps(&xs[i + 1]);\n        __m128 indices = _mm_set_ps(i + 3, i + 2, i + 1, i);\n        __m128 derivs = _mm_mul_ps(coeffs, indices);\n        _mm_storeu_ps(&result[i], derivs);\n        multiplier = _mm_add_ps(multiplier, _mm_set1_ps(1.0f));\n    }\n\n    for (size_t i = aligned_size; i < n - 1; ++i) {\n        result[i] = xs[i + 1] * (i + 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <cctype>\n\nint vowels_count_simd(std::string s){\n    if (s.empty()) return 0;\n\n    const char* str = s.c_str();\n    int len = s.length();\n    int count = 0;\n\n    // Define the mask for vowels 'a', 'e', 'i', 'o', 'u' (both lowercase and uppercase)\n    __m128i vowels_mask = _mm_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1, // Unused bytes\n        'U', 'O', 'I', 'E', 'A',         // Uppercase vowels\n        'u', 'o', 'i', 'e', 'a'          // Lowercase vowels\n    );\n\n    for(int i = 0; i <= len - 16; i += 16){\n        __m128i data = _mm_loadu_si128((__m128i*)&str[i]);\n        __m128i cmp_res = _mm_cmpestrm(data, 16, vowels_mask, 10, _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK | _SIDD_UWORD_OPS);\n        int res = _mm_movemask_epi8(cmp_res);\n        count += __builtin_popcount(res);\n    }\n\n    // Handle remaining characters\n    for(int i = len - (len % 16); i < len; ++i){\n        char c = tolower(str[i]);\n        if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u'){\n            count++;\n        }\n        else if(c == 'y' && i == len - 1){\n            count++;\n        }\n    }\n\n    // Check if 'y' or 'Y' is at the end of the word\n    if(len > 0 && (str[len-1] == 'y' || str[len-1] == 'Y')){\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s) {\n    if (s.empty()) return 0;\n\n    const __m128i vowels = _mm_set_epi8('u', 'o', 'i', 'e', 'a', 'U', 'O', 'I', 'E', 'A', -1, -1, -1, -1, -1, -1);\n    const __m128i one = _mm_set1_epi8(1);\n    const int align = 16;\n    int count = 0;\n\n    int len = s.length();\n    int i = 0;\n\n    // Check if 'y' or 'Y' is at the end\n    bool y_at_end = false;\n    if (len > 0 && (s[len - 1] == 'y' || s[len - 1] == 'Y')) {\n        y_at_end = true;\n    }\n\n    // Process 16 characters at a time\n    for (; i <= len - 16; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i cmp = _mm_cmpestrm(data, 16, vowels, 10, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_MASK | _SIDD_NEGATIVE_POLARITY);\n        __m128i ones = _mm_and_si128(cmp, one);\n        __m128i sum = _mm_sad_epu8(ones, _mm_setzero_si128());\n        count += _mm_extract_epi16(sum, 0);\n    }\n\n    // Process remaining characters\n    for (; i < len; ++i) {\n        char c = std::tolower(s[i]);\n        if (c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u') {\n            count++;\n        }\n    }\n\n    // Add 'y' if it is at the end\n    if (y_at_end) {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s){\n    if (s.empty()) return 0;\n\n    const __m128i vowels = _mm_set_epi8('u','o','i','e','a','U','O','I','E','A',0,0,0,0,0,0);\n    const int align_len = s.size() / 16 * 16;\n    int count = 0;\n\n    for(int i = 0; i < align_len; i += 16){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower = _mm_and_si128(chunk, _mm_set1_epi8(0xDF)); // to uppercase\n        __m128i cmp_res = _mm_cmpestrm(vowels, 10, lower, 16, _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK | _SIDD_NEGATIVE_POLARITY);\n        int mask = _mm_movemask_epi8(cmp_res);\n        count += __builtin_popcount(mask);\n    }\n\n    for(int i = align_len; i < s.size(); ++i){\n        char c = std::toupper(s[i]);\n        if(c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U'){\n            count++;\n        }\n    }\n\n    if(std::toupper(s.back()) == 'Y') count++;\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <algorithm>\n\nint vowels_count_simd(std::string s){\n    if (s.empty()) return 0;\n\n    // Convert string to uppercase for uniformity\n    std::transform(s.begin(), s.end(), s.begin(), ::toupper);\n\n    const __m128i vowel_a = _mm_set1_epi8('A');\n    const __m128i vowel_e = _mm_set1_epi8('E');\n    const __m128i vowel_i = _mm_set1_epi8('I');\n    const __m128i vowel_o = _mm_set1_epi8('O');\n    const __m128i vowel_u = _mm_set1_epi8('U');\n    const __m128i vowel_y = _mm_set1_epi8('Y');\n\n    int count = 0;\n    size_t len = s.length();\n    size_t i = 0;\n\n    // Process 16 bytes at a time\n    for (; i + 15 < len; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i cmp_a = _mm_cmpeq_epi8(data, vowel_a);\n        __m128i cmp_e = _mm_cmpeq_epi8(data, vowel_e);\n        __m128i cmp_i = _mm_cmpeq_epi8(data, vowel_i);\n        __m128i cmp_o = _mm_cmpeq_epi8(data, vowel_o);\n        __m128i cmp_u = _mm_cmpeq_epi8(data, vowel_u);\n\n        __m128i mask = _mm_or_si128(cmp_a, cmp_e);\n        mask = _mm_or_si128(mask, cmp_i);\n        mask = _mm_or_si128(mask, cmp_o);\n        mask = _mm_or_si128(mask, cmp_u);\n\n        // Count the number of matches\n        int temp_count = _mm_movemask_epi8(mask);\n        count += __builtin_popcount(temp_count);\n    }\n\n    // Handle remaining characters\n    for (; i < len - 1; ++i) {\n        if (s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U') {\n            count++;\n        }\n    }\n\n    // Check the last character for 'Y'\n    if (s[len - 1] == 'Y') {\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cctype>\n\nint vowels_count_simd(std::string s){\n    if (s.empty()) return 0;\n\n    int count = 0;\n    __m128i vowels = _mm_set_epi8('u','o','i','e','a','U','O','I','E','A',0,0,0,0,0,0);\n    __m128i y_check = _mm_set1_epi8('Y');\n    __m128i y_lower_check = _mm_set1_epi8('y');\n\n    const unsigned char* str = reinterpret_cast<const unsigned char*>(s.c_str());\n    size_t len = s.length();\n\n    for(size_t i = 0; i <= len - 16; i += 16){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[i]));\n        __m128i cmp = _mm_cmpestrm(vowels, 10, chunk, 16, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_MASK | _SIDD_NEGATE zero);\n        int mask = _mm_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n\n        // Check for 'y' or 'Y' at the end of the word\n        if(i + 16 == len){\n            __m128i last_chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[len - 1]));\n            __m128i is_y = _mm_or_si128(_mm_cmpeq_epi8(last_chunk, y_check), _mm_cmpeq_epi8(last_chunk, y_lower_check));\n            int y_mask = _mm_movemask_epi8(is_y);\n            count += (y_mask != 0);\n        }\n    }\n\n    // Handle remaining characters\n    for(size_t i = len - (len % 16); i < len -1; ++i){\n        if(str[i] == 'a' || str[i] == 'e' || str[i] == 'i' || str[i] == 'o' || str[i] == 'u' ||\n           str[i] == 'A' || str[i] == 'E' || str[i] == 'I' || str[i] == 'O' || str[i] == 'U'){\n            count++;\n        }\n    }\n\n    // Check last character for 'y' or 'Y'\n    if(len > 0 && (std::tolower(str[len-1]) == 'y')){\n        count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n    __m128i v_upper = _mm_set1_epi8('A' - 1);\n    __m128i v_lower = _mm_set1_epi8('Z' + 1);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_sum = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 15 < len; i += 16) {\n        __m128i v_data = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i v_cmp1 = _mm_cmplt_epi8(v_data, v_upper);\n        __m128i v_cmp2 = _mm_cmpgt_epi8(v_data, v_lower);\n        __m128i v_mask = _mm_or_si128(v_cmp1, v_cmp2);\n        __m128i v_andnot = _mm_andnot_si128(v_mask, v_data);\n        v_sum = _mm_add_epi8(v_sum, v_andnot);\n    }\n\n    // Horizontal sum of v_sum\n    __m128i v_low = _mm_unpacklo_epi8(v_sum, v_zero);\n    __m128i v_high = _mm_unpackhi_epi8(v_sum, v_zero);\n    __m128i v_summed = _mm_add_epi16(v_low, v_high);\n    v_low = _mm_unpacklo_epi16(v_summed, v_zero);\n    v_high = _mm_unpackhi_epi16(v_summed, v_zero);\n    v_summed = _mm_add_epi32(v_low, v_high);\n    v_low = _mm_unpacklo_epi32(v_summed, v_zero);\n    v_high = _mm_unpackhi_epi32(v_summed, v_zero);\n    v_summed = _mm_add_epi64(v_low, v_high);\n    int temp_sum = _mm_cvtsi128_si32(v_summed);\n    sum += temp_sum;\n\n    // Handle remaining elements\n    for (size_t i = (len / 16) * 16; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const int length = s.length();\n    const int alignedLength = length - (length % 16);\n\n    __m128i upper_bound = _mm_set1_epi8('Z');\n    __m128i lower_bound = _mm_set1_epi8('A');\n    __m128i zero = _mm_setzero_si128();\n    __m128i total = _mm_setzero_si128();\n\n    for(int i = 0; i < alignedLength; i += 16){\n        __m128i data = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i ge_lower = _mm_cmpgt_epi8(data, lower_bound);\n        __m128i le_upper = _mm_cmple_epi8(data, upper_bound);\n        __m128i is_upper = _mm_and_si128(ge_lower, le_upper);\n        __m128i masked = _mm_and_si128(data, is_upper);\n        total = _mm_add_epi8(total, masked);\n    }\n\n    int temp[16] = {0};\n    _mm_storeu_si128((__m128i*)temp, total);\n\n    for(int i = 0; i < 16; ++i){\n        sum += temp[i];\n    }\n\n    for(int i = alignedLength; i < length; ++i){\n        if(s[i] >= 'A' && s[i] <= 'Z'){\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    const int length = s.length();\n    if (length == 0) return 0;\n\n    alignas(16) unsigned char buffer[16];\n    __m128i upper_mask = _mm_set1_epi8(0x40); // 'A' to 'Z' mask (0x41 to 0x5A)\n    __m128i lower_bound = _mm_set1_epi8('A');  // 'A'\n    __m128i upper_bound = _mm_set1_epi8('Z');  // 'Z'\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n\n    for (size_t i = 0; i < length; i += 16) {\n        size_t chunk_size = std::min((size_t)16, length - i);\n        memset(buffer, 0, 16);\n        memcpy(buffer, s.c_str() + i, chunk_size);\n\n        __m128i data = _mm_load_si128((__m128i*)buffer);\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(data, lower_bound), _mm_cmple_epi8(data, upper_bound));\n        __m128i masked_data = _mm_and_si128(data, is_upper);\n        sum = _mm_add_epi8(sum, masked_data);\n    }\n\n    int total_sum = 0;\n    __m128i temp = _mm_setzero_si128();\n    temp = _mm_sad_epu8(sum, temp);\n    __m128i high = _mm_shuffle_epi32(temp, _MM_SHUFFLE(3, 3, 2, 2));\n    __m128i low = _mm_shuffle_epi32(temp, _MM_SHUFFLE(1, 1, 0, 0));\n    temp = _mm_add_epi32(high, low);\n    high = _mm_shuffle_epi32(temp, _MM_SHUFFLE(3, 3, 1, 1));\n    low = _mm_shuffle_epi32(temp, _MM_SHUFFLE(2, 2, 0, 0));\n    temp = _mm_add_epi32(high, low);\n    _mm_store_si128((__m128i*)&total_sum, temp);\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n    \n    __m128i upper_bound = _mm_set1_epi8('Z');\n    __m128i lower_bound = _mm_set1_epi8('A');\n    __m128i zero = _mm_setzero_si128();\n    __m128i total = _mm_setzero_si128();\n\n    for(size_t i = 0; i + 15 < len; i += 16){\n        __m128i data = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(data, lower_bound), _mm_cmple_epi8(data, upper_bound));\n        __m128i masked = _mm_and_si128(data, is_upper);\n        total = _mm_add_epi8(total, masked);\n    }\n\n    // Horizontal sum of total\n    __m128i sum1 = _mm_sad_epu8(total, zero);\n    sum += _mm_extract_epi16(sum1, 0) + _mm_extract_epi16(sum1, 4);\n\n    // Handle remaining characters\n    for(size_t i = (len / 16) * 16; i < len; ++i){\n        if(str[i] >= 'A' && str[i] <= 'Z'){\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i upper_bound = _mm_set1_epi8('Z');\n    __m128i lower_bound = _mm_set1_epi8('A');\n    __m128i ascii_sum = _mm_setzero_si128();\n\n    for (; i + 15 < len; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(data, lower_bound), _mm_cmple_epi8(data, upper_bound));\n        __m128i masked_data = _mm_and_si128(data, is_upper);\n        ascii_sum = _mm_add_epi8(ascii_sum, masked_data);\n    }\n\n    int temp[16] = {0};\n    _mm_storeu_si128((__m128i*)temp, ascii_sum);\n\n    for (int j = 0; j < 16; ++j) {\n        sum += temp[j];\n    }\n\n    for (; i < len; ++i) {\n        if (str[i] >= 'A' && str[i] <= 'Z') {\n            sum += str[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    std::vector<int> result;\n    if (arr.empty()) return result;\n\n    const int vec_size = 4;\n    size_t n = arr.size();\n    size_t aligned_size = n - (n % vec_size);\n\n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i min_idx = _mm_set1_epi32(-1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i two = _mm_set1_epi32(2);\n\n    for (size_t i = 0; i < aligned_size; i += vec_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(data, two), zero);\n        __m128i candidates = _mm_and_si128(data, even_mask);\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i cmp_mask = _mm_cmplt_epi32(candidates, min_val);\n        min_val = _mm_min_epi32(min_val, candidates);\n        min_idx = _mm_blendv_epi8(min_idx, indices, cmp_mask);\n    }\n\n    int smallest_val = 0x7FFFFFFF;\n    int smallest_idx = -1;\n\n    for (size_t i = 0; i < vec_size; ++i) {\n        int val = _mm_extract_epi32(min_val, i);\n        if (val < smallest_val && val != 0x7FFFFFFF) {\n            smallest_val = val;\n            smallest_idx = _mm_extract_epi32(min_idx, i);\n        }\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        if (arr[i] >= 0 && arr[i] < smallest_val && (arr[i] % 2 == 0)) {\n            smallest_val = arr[i];\n            smallest_idx = i;\n        }\n    }\n\n    if (smallest_idx != -1) {\n        result = {smallest_val, smallest_idx};\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    if (arr.empty()) return {};\n\n    const int vec_size = 4;\n    int n = arr.size();\n    int remainder = n % vec_size;\n    int aligned_size = n - remainder;\n\n    __m128i min_val = _mm_set1_epi32(std::numeric_limits<int>::max());\n    __m128i min_idx = _mm_set1_epi32(-1);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(1);\n    __m128i idx_vec = _mm_setr_epi32(0, 1, 2, 3);\n\n    for (int i = 0; i < aligned_size; i += vec_size) {\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i even_mask = _mm_and_si128(_mm_srli_epi32(data, 31), mask_even);\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, zero);\n\n        __m128i current_min = _mm_min_epi32(_mm_and_si128(data, is_even), _mm_or_si128(min_val, _mm_andnot_si128(is_even, _mm_set1_epi32(std::numeric_limits<int>::max()))));\n\n        __m128i lt_mask = _mm_cmplt_epi32(current_min, min_val);\n        min_val = _mm_min_epi32(current_min, min_val);\n\n        __m128i current_idx = _mm_add_epi32(idx_vec, _mm_set1_epi32(i));\n        min_idx = _mm_blendv_epi8(min_idx, current_idx, lt_mask);\n    }\n\n    int scalar_min = std::numeric_limits<int>::max();\n    int scalar_idx = -1;\n\n    for (int i = 0; i < vec_size; ++i) {\n        if (_mm_extract_epi32(min_val, i) < scalar_min) {\n            scalar_min = _mm_extract_epi32(min_val, i);\n            scalar_idx = _mm_extract_epi32(min_idx, i);\n        }\n    }\n\n    for (int i = aligned_size; i < n; ++i) {\n        if ((arr[i] % 2 == 0) && (arr[i] < scalar_min || (arr[i] == scalar_min && i < scalar_idx))) {\n            scalar_min = arr[i];\n            scalar_idx = i;\n        }\n    }\n\n    if (scalar_idx != -1) {\n        return {scalar_min, scalar_idx};\n    } else {\n        return {};\n    }\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) return {};\n\n    const int simd_width = 4;\n    size_t size = arr.size();\n    size_t aligned_size = size / simd_width * simd_width;\n\n    __m128i min_val = _mm_set1_epi32(std::numeric_limits<int>::max());\n    __m128i min_idx = _mm_set_epi32(-1, -1, -1, -1);\n\n    __m128i mask_even = _mm_set1_epi32(0xAAAAAAAA); // bitmask for even numbers\n\n    for (size_t i = 0; i < aligned_size; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i is_even = _mm_andnot_si128(_mm_and_si128(vec, _mm_set1_epi32(1)), mask_even);\n        __m128i cmp_res = _mm_cmpeq_epi32(is_even, mask_even);\n\n        __m128i current_min_val = _mm_min_epi32(_mm_and_si128(cmp_res, vec), min_val);\n        __m128i current_min_idx = _mm_blendv_epi8(min_idx, _mm_set_epi32(i+3, i+2, i+1, i), _mm_cmpeq_epi32(current_min_val, min_val));\n\n        min_val = current_min_val;\n        min_idx = current_min_idx;\n    }\n\n    int scalar_min_val = std::numeric_limits<int>::max();\n    int scalar_min_idx = -1;\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if ((arr[i] & 1) == 0 && arr[i] < scalar_min_val) {\n            scalar_min_val = arr[i];\n            scalar_min_idx = i;\n        }\n    }\n\n    int vals[4];\n    int idxs[4];\n    _mm_storeu_si128((__m128i*)vals, min_val);\n    _mm_storeu_si128((__m128i*)idxs, min_idx);\n\n    for (int i = 0; i < simd_width; ++i) {\n        if (vals[i] < scalar_min_val && vals[i] != std::numeric_limits<int>::max()) {\n            scalar_min_val = vals[i];\n            scalar_min_idx = idxs[i];\n        }\n    }\n\n    if (scalar_min_idx == -1) return {};\n    return {scalar_min_val, scalar_min_idx};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if (arr.empty()) return {};\n\n    const int n = arr.size();\n    const int aligned_n = n - (n % 8);\n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i min_idx = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    for(int i = 0; i < aligned_n; i += 8){\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i even_mask = _mm_andnot_si128(_mm_srli_epi32(data, 31), mask_even);\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, mask_even);\n\n        __m128i current_min_val = _mm_min_epi32(data, min_val);\n        __m128i changed = _mm_cmplt_epi32(current_min_val, min_val);\n        __m128i changed_even = _mm_and_si128(changed, is_even);\n\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        min_val = _mm_blendv_epi8(min_val, data, changed_even);\n        min_idx = _mm_blendv_epi8(min_idx, indices, changed_even);\n    }\n\n    int scalar_min_val = 0x7FFFFFFF;\n    int scalar_min_idx = -1;\n\n    for(int i = 0; i < 4; ++i){\n        int val = _mm_extract_epi32(min_val, i);\n        int idx = _mm_extract_epi32(min_idx, i);\n        if(val < scalar_min_val && (val & 1) == 0){\n            scalar_min_val = val;\n            scalar_min_idx = idx;\n        }\n    }\n\n    for(int i = aligned_n; i < n; ++i){\n        if(arr[i] < scalar_min_val && (arr[i] & 1) == 0){\n            scalar_min_val = arr[i];\n            scalar_min_idx = i;\n        }\n    }\n\n    if(scalar_min_idx != -1){\n        return {scalar_min_val, scalar_min_idx};\n    }\n    return {};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> pluck_simd(std::vector<int> arr) {\n    std::vector<int> result;\n    if (arr.empty()) return result;\n\n    const int SIMD_WIDTH = 4;\n    int n = arr.size();\n    int remainder = n % SIMD_WIDTH;\n    int aligned_n = n - remainder;\n\n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i min_idx = _mm_set1_epi32(-1);\n\n    for (int i = 0; i < aligned_n; i += SIMD_WIDTH) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)&arr[i]);\n        __m128i even_mask = _mm_andnot_si128(_mm_srli_epi32(vec, 31), _mm_cmpeq_epi32(_mm_and_si128(vec, _mm_set1_epi32(1)), _mm_setzero_si128()));\n        __m128i valid_vals = _mm_and_si128(vec, even_mask);\n        __m128i valid_indices = _mm_and_si128(_mm_set_epi32(i+3, i+2, i+1, i), even_mask);\n\n        __m128i current_min_val = _mm_min_epi32(valid_vals, min_val);\n        __m128i current_min_idx = _mm_blendv_epi8(min_idx, valid_indices, _mm_cmpeq_epi32(current_min_val, valid_vals));\n\n        min_val = current_min_val;\n        min_idx = current_min_idx;\n    }\n\n    int scalar_min_val = 0x7FFFFFFF;\n    int scalar_min_idx = -1;\n\n    for (int i = aligned_n; i < n; ++i) {\n        if ((arr[i] & 1) == 0 && arr[i] < scalar_min_val) {\n            scalar_min_val = arr[i];\n            scalar_min_idx = i;\n        }\n    }\n\n    int final_min_val = 0x7FFFFFFF;\n    int final_min_idx = -1;\n\n    if (_mm_extract_epi32(min_idx, 0) != -1 && _mm_extract_epi32(min_val, 0) < scalar_min_val) {\n        final_min_val = _mm_extract_epi32(min_val, 0);\n        final_min_idx = _mm_extract_epi32(min_idx, 0);\n    } else if (scalar_min_idx != -1) {\n        final_min_val = scalar_min_val;\n        final_min_idx = scalar_min_idx;\n    }\n\n    if (final_min_idx != -1) {\n        result = {final_min_val, final_min_idx};\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 256) max_val = 256;\n    std::vector<int> freq(max_val + 1, 0);\n\n    // Count frequencies using SIMD\n    for(size_t i = 0; i < lst.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i mask = _mm_cmplt_epi32(vec, _mm_set1_epi32(max_val));\n        vec = _mm_and_si128(vec, mask);\n        \n        int vals[4];\n        _mm_storeu_si128((__m128i*)vals, vec);\n        for(int j = 0; j < 4; ++j){\n            if(vals[j] > 0 && vals[j] <= max_val){\n                freq[vals[j]]++;\n            }\n        }\n    }\n\n    int result = -1;\n    for(int i = 1; i <= max_val; ++i){\n        if(freq[i] >= i){\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 255) max_val = 255;\n    \n    __m128i freqs = _mm_setzero_si128();\n    for(size_t i = 0; i < lst.size(); i += 4){\n        __m128i vals = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i mask = _mm_cmplt_epi32(vals, _mm_set1_epi32(256));\n        vals = _mm_and_si128(vals, mask);\n        \n        __m128i zero = _mm_setzero_si128();\n        __m128i cur_freqs = _mm_setzero_si128();\n        __m128i ones = _mm_set1_epi32(1);\n        \n        for(int j = 0; j < 4; ++j){\n            __m128i val = _mm_shuffle_epi32(vals, _MM_SHUFFLE(j,j,j,j));\n            __m128i eq_mask = _mm_cmpeq_epi32(val, vals);\n            cur_freqs = _mm_add_epi32(cur_freqs, _mm_and_si128(eq_mask, ones));\n        }\n        freqs = _mm_add_epi32(freqs, cur_freqs);\n    }\n    \n    int result = -1;\n    for(int i = 1; i <= max_val; ++i){\n        __m128i idx = _mm_set1_epi32(i);\n        __m128i f = _mm_set1_epi32(((int*)&freqs)[i]);\n        __m128i cmp = _mm_cmpgt_epi32(f, idx);\n        if(_mm_movemask_epi8(cmp) != 0 && i >= ((int*)&freqs)[i]){\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 256) max_val = 256;\n    std::vector<int> freq(max_val + 1, 0);\n\n    // Count frequencies using SIMD\n    for(size_t i = 0; i < lst.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i mask = _mm_cmplt_epi32(vec, _mm_set1_epi32(max_val));\n        vec = _mm_and_si128(vec, mask);\n        \n        int a = _mm_extract_epi32(vec, 0);\n        int b = _mm_extract_epi32(vec, 1);\n        int c = _mm_extract_epi32(vec, 2);\n        int d = _mm_extract_epi32(vec, 3);\n\n        if(a >=0 && a <= max_val) freq[a]++;\n        if(b >=0 && b <= max_val) freq[b]++;\n        if(c >=0 && c <= max_val) freq[c]++;\n        if(d >=0 && d <= max_val) freq[d]++;\n    }\n\n    int result = -1;\n    for(int i = 1; i <= max_val; ++i){\n        if(freq[i] >= i){\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    const int MAX_VAL = 1024;\n    __m128i freqs = _mm_setzero_si128();\n    __m128i mask = _mm_set_epi32(3, 2, 1, 0);\n    __m128i one = _mm_set1_epi32(1);\n\n    for(size_t i = 0; i + 3 < lst.size(); i += 4){\n        __m128i vals = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i idxs = _mm_shuffle_epi32(vals, _MM_SHUFFLE(3, 2, 1, 0));\n        __m128i old_freqs = _mm_shuffle_epi32(freqs, _MM_SHUFFLE(3, 2, 1, 0));\n        __m128i inc = _mm_add_epi32(old_freqs, one);\n        freqs = _mm_shuffle_epi32(_mm_blendv_epi8(old_freqs, inc, _mm_cmpeq_epi32(idxs, mask)), _MM_SHUFFLE(3, 2, 1, 0));\n    }\n\n    for(size_t i = lst.size() - (lst.size() % 4); i < lst.size(); ++i){\n        int val = lst[i];\n        if(val >=1 && val < MAX_VAL){\n            int* fptr = reinterpret_cast<int*>(&freqs);\n            fptr[val]++;\n        }\n    }\n\n    int result = -1;\n    for(int i = 1; i < MAX_VAL; ++i){\n        int* fptr = reinterpret_cast<int*>(&freqs);\n        if(fptr[i] >= i){\n            result = i;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint search_simd(std::vector<int> lst){\n    int max_val = *std::max_element(lst.begin(), lst.end());\n    if (max_val > 255) max_val = 255;\n\n    __m128i freqs = _mm_setzero_si128();\n    size_t i = 0;\n    size_t len = lst.size();\n\n    // Count frequencies using SSE\n    for (; i + 4 <= len; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i indices = _mm_set_epi32(3, 2, 1, 0);\n        __m128i mask = _mm_cmplt_epi32(vec, _mm_set1_epi32(256));\n        vec = _mm_and_si128(vec, mask); // Ensure values are within 0-255\n\n        __m128i gather_indices = _mm_shuffle_epi32(vec, _MM_SHUFFLE(0, 0, 0, 0));\n        __m128i freq_add = _mm_set1_epi32(1);\n\n        for (int j = 0; j < 4; ++j) {\n            __m128i index = _mm_cvtepi32_epi64(_mm_shuffle_epi32(gather_indices, _MM_SHUFFLE(j, j, j, j)));\n            if (_mm_extract_epi32(mask, j)) {\n                freqs = _mm_add_epi32(freqs, _mm_cmpeq_epi32(index, _mm_set_epi32(j, j, j, j)));\n            }\n        }\n    }\n\n    // Scalar fallback for remaining elements\n    for (; i < len; ++i) {\n        int val = lst[i];\n        if (val >=0 && val < 256) {\n            freqs = _mm_add_epi32(freqs, _mm_cmpeq_epi32(_mm_set_epi32(val, val, val, val), _mm_set_epi32(val, val, val, val)));\n        }\n    }\n\n    // Extract frequency counts\n    int freq_arr[4];\n    _mm_storeu_si128((__m128i*)freq_arr, freqs);\n\n    int result = -1;\n    for (int k = 1; k <= max_val; ++k) {\n        if (freq_arr[k % 4] >= k) {\n            result = k;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is a palindrome\n    __m128i mask = _mm_set1_epi32(0xFFFFFFFF);\n    for (size_t i = 0; i < n / 2; ++i) {\n        __m128i left = _mm_set1_epi32(q[i]);\n        __m128i right = _mm_set1_epi32(q[n - i - 1]);\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        if (_mm_movemask_epi8(cmp) != mask.m128i_i32[0]) {\n            return false;\n        }\n    }\n\n    // Compute the sum of elements using SIMD\n    size_t aligned_size = n / 4 * 4;\n    __m128i sum_vec = _mm_setzero_si128();\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    int sum = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    for (int i = 0; i < 4; ++i) {\n        sum += temp[i];\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        sum += q[i];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool will_it_fly_simd(std::vector<int> q, int w){\n    size_t n = q.size();\n    if(n == 0) return false;\n\n    // Check if the vector is a palindrome\n    __m128i mask = _mm_set1_epi32(-1);\n    for(size_t i = 0; i < n / 2; ++i){\n        __m128i left = _mm_set1_epi32(q[i]);\n        __m128i right = _mm_set1_epi32(q[n - i - 1]);\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        if(_mm_movemask_epi8(cmp) != 0xFFFF){\n            return false;\n        }\n    }\n\n    // Calculate the sum of elements using SIMD\n    size_t aligned_size = n / 4 * 4;\n    __m128i sum_vec = _mm_setzero_si128();\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&q[i]));\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    int sum = 0;\n    __m128i sums = _mm_hadd_epi32(sum_vec, sum_vec);\n    sums = _mm_hadd_epi32(sums, sums);\n    _mm_store_si128(reinterpret_cast<__m128i*>(&sum), sums);\n\n    for(size_t i = aligned_size; i < n; ++i){\n        sum += q[i];\n    }\n\n    sum += sum;\n\n    if(n % 2 != 0){\n        sum += q[n / 2];\n    }\n\n    return sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w){\n    int n = q.size();\n    bool is_palindrome = true;\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    // Check palindrome and compute sum using SIMD\n    for(int i = 0; i < n / 2; i += 4){\n        __m128i left = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&q[n - i - 4]);\n        \n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        if(_mm_movemask_epi8(cmp) != 0xFFFF){\n            is_palindrome = false;\n        }\n        \n        sum_vec = _mm_add_epi32(sum_vec, left);\n        sum_vec = _mm_add_epi32(sum_vec, right);\n    }\n\n    // Handle middle element if odd length\n    if(n % 2 == 1){\n        int mid = q[n / 2];\n        sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(mid));\n    }\n\n    // Horizontal sum\n    int sum_array[4] = {0};\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    // Check remaining elements if size is not divisible by 4\n    for(int i = (n / 2) * 2; i < n; ++i){\n        total_sum += q[i];\n    }\n\n    return is_palindrome && (total_sum <= w);\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w){\n    int n = q.size();\n    bool is_palindrome = true;\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n\n    for(int i = 0; i < n / 2; ++i){\n        if(q[i] != q[n - i - 1]){\n            is_palindrome = false;\n        }\n    }\n\n    if(!is_palindrome) return false;\n\n    for(int i = 0; i + 3 < n; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum = _mm_add_epi32(sum, vec);\n    }\n\n    int remainder_start = (n / 4) * 4;\n    int remainder_sum = 0;\n    for(int i = remainder_start; i < n; ++i){\n        remainder_sum += q[i];\n    }\n\n    int total_sum = remainder_sum + \n                    _mm_cvtsi128_si32(sum) + \n                    _mm_extract_epi32(sum, 1) + \n                    _mm_extract_epi32(sum, 2) + \n                    _mm_extract_epi32(sum, 3);\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    size_t n = q.size();\n    if (n == 0) return false;\n\n    // Check if the vector is a palindrome\n    __m128i mask = _mm_set1_epi32(-1);\n    for (size_t i = 0; i < (n / 2) / 4 * 4; i += 4) {\n        __m128i left = _mm_loadu_si128((__m128i const*)(&q[i]));\n        __m128i right = _mm_loadu_si128((__m128i const*)(&q[n - i - 4]));\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        mask = _mm_and_si128(mask, cmp);\n    }\n    if (_mm_movemask_epi8(mask) != 0xFFFF) {\n        for (size_t i = (n / 2) / 4 * 4; i < n / 2; ++i) {\n            if (q[i] != q[n - i - 1]) return false;\n        }\n    }\n\n    // Calculate the sum of elements\n    __m128i sum = _mm_setzero_si128();\n    for (size_t i = 0; i < n / 4 * 4; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&q[i]));\n        sum = _mm_add_epi32(sum, vec);\n    }\n    int remainder_sum = 0;\n    for (size_t i = n / 4 * 4; i < n; ++i) {\n        remainder_sum += q[i];\n    }\n    sum = _mm_hadd_epi32(sum, sum);\n    sum = _mm_hadd_epi32(sum, sum);\n    int total_sum = _mm_cvtsi128_si32(sum) + remainder_sum;\n\n    return total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    // Process in chunks of 4 using SSE\n    for (; i + 3 < n / 2; i += 4) {\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 4]);\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        __m128i mask = _mm_cmpeq_epi32(cmp, _mm_setzero_si128());\n\n        int bitmask = _mm_movemask_epi8(mask);\n        changes += __builtin_popcount(bitmask);\n    }\n\n    // Handle remaining elements\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 7 < n / 2; i += 8) {\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 8]);\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        __m128i mask = _mm_xor_si128(cmp, _mm_set1_epi32(-1));\n\n        int count = _mm_movemask_ps(_mm_castsi128_ps(mask));\n        changes += __builtin_popcount(count);\n    }\n\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 7 < n / 2; i += 8) {\n        __m128i left = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i right = _mm_loadu_si128((__m128i*)&arr[n - i - 8]);\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        __m128i mask = _mm_movemask_epi8(cmp);\n\n        // Count the number of unequal elements\n        changes += __builtin_popcount(~mask);\n    }\n\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes / 4;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i + 7 < n / 2; i += 8) {\n        __m128i left = _mm_loadu_si128((__m128i const*)(&arr[i]));\n        __m128i right = _mm_loadu_si128((__m128i const*)(&arr[n - i - 8]));\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        __m128i mask = _mm_movemask_epi8(cmp);\n\n        // Count the number of non-equal elements\n        int count = _mm_popcnt_u32(~mask);\n        changes += count / 4;\n    }\n\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - i - 1]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    int changes = 0;\n    int i = 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n\n    for (; i + 7 < n / 2; i += 4) {\n        __m128i left = _mm_loadu_si128((__m128i const*)(&arr[i]));\n        __m128i right = _mm_loadu_si128((__m128i const*)(&arr[n - 1 - i]));\n\n        __m128i cmp = _mm_cmpeq_epi32(left, right);\n        __m128i mask = _mm_xor_si128(cmp, _mm_set1_epi32(-1));\n        __m128i diff = _mm_and_si128(mask, one);\n\n        int count = _mm_movemask_epi8(diff);\n        changes += __builtin_popcount(count);\n    }\n\n    for (; i < n / 2; ++i) {\n        if (arr[i] != arr[n - 1 - i]) {\n            ++changes;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <cstring>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for(const auto& str : lst1){\n        size_t len = str.length();\n        __m128i vlen = _mm_set_epi32(len, len, len, len);\n        sum1 = _mm_add_epi32(sum1, vlen);\n    }\n\n    for(const auto& str : lst2){\n        size_t len = str.length();\n        __m128i vlen = _mm_set_epi32(len, len, len, len);\n        sum2 = _mm_add_epi32(sum2, vlen);\n    }\n\n    __m128i mask = _mm_cmpgt_epi32(sum1, sum2);\n    int result = _mm_movemask_epi8(mask);\n\n    if(result != 0){\n        return lst1;\n    }\n    else{\n        mask = _mm_cmpgt_epi32(sum2, sum1);\n        result = _mm_movemask_epi8(mask);\n        if(result != 0){\n            return lst2;\n        }\n    }\n    return lst1;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for(const auto& str : lst1){\n        size_t len = str.length();\n        __m128i vlen = _mm_set1_epi32(len);\n        sum1 = _mm_add_epi32(sum1, vlen);\n    }\n\n    for(const auto& str : lst2){\n        size_t len = str.length();\n        __m128i vlen = _mm_set1_epi32(len);\n        sum2 = _mm_add_epi32(sum2, vlen);\n    }\n\n    int total1 = _mm_cvtsi128_si32(sum1);\n    int total2 = _mm_cvtsi128_si32(sum2);\n\n    if(total1 <= total2){\n        return lst1;\n    }\n    else{\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for(const auto& str : lst1){\n        __m128i len = _mm_set1_epi32(str.length());\n        sum1 = _mm_add_epi32(sum1, len);\n    }\n\n    for(const auto& str : lst2){\n        __m128i len = _mm_set1_epi32(str.length());\n        sum2 = _mm_add_epi32(sum2, len);\n    }\n\n    int total1[4];\n    int total2[4];\n\n    _mm_storeu_si128((__m128i*)total1, sum1);\n    _mm_storeu_si128((__m128i*)total2, sum2);\n\n    int res1 = total1[0] + total1[1] + total1[2] + total1[3];\n    int res2 = total2[0] + total2[1] + total2[2] + total2[3];\n\n    if(res1 <= res2){\n        return lst1;\n    }\n    else{\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for(const auto& str : lst1){\n        __m128i total = _mm_setzero_si128();\n        for(size_t i = 0; i + 15 <= str.size(); i += 16){\n            __m128i block = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[i]));\n            __m128i mask = _mm_cmpeq_epi8(block, zero);\n            __m128i count = _mm_sub_epi8(_mm_set1_epi8(1), mask);\n            total = _mm_add_epi32(_mm_sad_epu8(count, zero), total);\n        }\n        if(str.size() % 16){\n            __m128i block = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[str.size() - (str.size() % 16)]));\n            __m128i mask = _mm_cmpeq_epi8(block, zero);\n            __m128i count = _mm_sub_epi8(_mm_set1_epi8(1), mask);\n            total = _mm_add_epi32(_mm_sad_epu8(count, zero), total);\n        }\n        sum1 = _mm_add_epi32(sum1, total);\n    }\n\n    for(const auto& str : lst2){\n        __m128i total = _mm_setzero_si128();\n        for(size_t i = 0; i + 15 <= str.size(); i += 16){\n            __m128i block = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[i]));\n            __m128i mask = _mm_cmpeq_epi8(block, zero);\n            __m128i count = _mm_sub_epi8(_mm_set1_epi8(1), mask);\n            total = _mm_add_epi32(_mm_sad_epu8(count, zero), total);\n        }\n        if(str.size() % 16){\n            __m128i block = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[str.size() - (str.size() % 16)]));\n            __m128i mask = _mm_cmpeq_epi8(block, zero);\n            __m128i count = _mm_sub_epi8(_mm_set1_epi8(1), mask);\n            total = _mm_add_epi32(_mm_sad_epu8(count, zero), total);\n        }\n        sum2 = _mm_add_epi32(sum2, total);\n    }\n\n    int s1[4], s2[4];\n    _mm_storeu_si128((__m128i*)s1, sum1);\n    _mm_storeu_si128((__m128i*)s2, sum2);\n\n    int t1 = s1[0] + s1[1] + s1[2] + s1[3];\n    int t2 = s2[0] + s2[1] + s2[2] + s2[3];\n\n    if(t1 <= t2) return lst1;\n    return lst2;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    for (const auto& str : lst1) {\n        size_t len = str.length();\n        __m128i vlen = _mm_set_epi32(len, len, len, len);\n        sum1 = _mm_add_epi32(sum1, vlen);\n    }\n\n    for (const auto& str : lst2) {\n        size_t len = str.length();\n        __m128i vlen = _mm_set_epi32(len, len, len, len);\n        sum2 = _mm_add_epi32(sum2, vlen);\n    }\n\n    __m128i mask = _mm_cmpgt_epi32(sum1, sum2);\n    int result = _mm_movemask_epi8(mask);\n\n    if (result != 0) {\n        return lst1;\n    } else {\n        mask = _mm_cmpgt_epi32(sum2, sum1);\n        result = _mm_movemask_epi8(mask);\n        if (result != 0) {\n            return lst2;\n        }\n    }\n    return lst1;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n\n    __m128i vone = _mm_set1_epi64x(1);\n    __m128i vzero = _mm_set1_epi64x(0);\n\n    __m128i current = vn;\n    __m128i prev = vzero;\n\n    while (_mm_cmpgt_epi64(current, vzero)) {\n        prev = current;\n        current = _mm_mul_epu32(current, vn);\n        if (_mm_cmpeq_epi64(current, vx)) {\n            return true;\n        }\n        if (_mm_cmpgt_epi64(current, vx)) {\n            break;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n\n    __m128i vone = _mm_set1_epi64x(1);\n    __m128i vzero = _mm_set1_epi64x(0);\n\n    __m128i current = vn;\n    __m128i prev = vzero;\n\n    while (_mm_cmpgt_epi64(current, vzero)) {\n        prev = current;\n        current = _mm_mul_epu32(current, vn);\n        if (_mm_cmpeq_epi64(current, vx)) {\n            return true;\n        }\n        if (_mm_cmpgt_epi64(current, vx)) {\n            break;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <stdint.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n\n    __m128i vone = _mm_set1_epi64x(1);\n    __m128i vzero = _mm_set1_epi64x(0);\n\n    uint64_t power = 1;\n    while (power < x){\n        power *= n;\n        if (power == x) return true;\n        if (__builtin_expect((power > x || n == 0), 0)) break;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n\n    __m128i vone = _mm_set1_epi64x(1);\n    __m128i vzero = _mm_set1_epi64x(0);\n\n    __m128i current = vone;\n    bool result = false;\n\n    while (_mm_cmpgt_epi64(current, vx)._mm_extract_epi64(0) == 0 && !result) {\n        __m128i next = _mm_mul_epu32(current, vn);\n        result = _mm_cmpeq_epi64(next, vx)._mm_extract_epi64(0) != 0;\n        current = next;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if (n == 0) return x == 0;\n    if (n == 1) return x == 1;\n    if (x == 1) return true;\n\n    __m128i vx = _mm_set1_epi64x(x);\n    __m128i vn = _mm_set1_epi64x(n);\n\n    __m128i vone = _mm_set1_epi64x(1);\n    __m128i vzero = _mm_set1_epi64x(0);\n\n    __m128i current = vn;\n    __m128i prev = vzero;\n\n    while (_mm_cmpgt_epi64(current, vzero)) {\n        prev = current;\n        current = _mm_mul_epu32(current, vn);\n        if (_mm_cmpeq_epi64(current, vx)) {\n            return true;\n        }\n        if (_mm_cmpgt_epi64(current, vx)) {\n            break;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x80, 0x40 // F E D C B A 9 8 7 6 5 4 3 2 1 0\n    );\n    const __m128i cmp_mask = _mm_set_epi8(\n        0xF, 0xE, 0xD, 0xC, 0xB, 0xA, 0x9, 0x8, 0x7, 0x6, 0x5, 0x4, 0x3, 0x2, 0x1, 0x0\n    );\n\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.length();\n\n    for (; i <= len - 16; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        __m128i sub_res = _mm_sub_epi8(data, cmp_mask);\n        __m128i and_res = _mm_and_si128(sub_res, prime_mask);\n        __m128i cmp_res = _mm_cmpeq_epi8(and_res, _mm_setzero_si128());\n        count += 16 - _mm_movemask_epi8(cmp_res);\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m128i primes_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, // F E D C B A 9 8\n        0, 1, 1, 0, 1, 0, 1, 0   // 7 6 5 4 3 2 1 0\n    );\n\n    int count = 0;\n    size_t len = num.length();\n    size_t i = 0;\n\n    for (; i + 15 < len; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        __m128i cmp_res = _mm_cmpeq_epi8(data, primes_mask);\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i res = _mm_and_si128(cmp_res, ones);\n        __m128i sum = _mm_sad_epu8(res, _mm_setzero_si128());\n        count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4) + _mm_extract_epi16(sum, 2) + _mm_extract_epi16(sum, 6);\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' || c == 'B' || c == 'D') {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_set_epi8(\n        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0 // F E D C B A 9 8 7 6 5 4 3 2 1 0\n    );\n\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.length();\n\n    for (; i + 15 < len; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        data = _mm_and_si128(data, _mm_set1_epi8(0xF)); // Mask to handle digits and letters\n        __m128i cmp = _mm_cmpeq_epi8(data, _mm_set1_epi8(0));\n        __m128i masked = _mm_andnot_si128(cmp, prime_mask);\n        __m128i zeros = _mm_setzero_si128();\n        count += 16 - _mm_popcnt_u32(_mm_movemask_epi8(_mm_cmpeq_epi8(masked, zeros)));\n    }\n\n    for (; i < len; ++i) {\n        char c = num[i];\n        if (c >= '0' && c <= '9') {\n            unsigned char val = c - '0';\n            if (val == 2 || val == 3 || val == 5 || val == 7) {\n                count++;\n            }\n        } else if (c >= 'A' && c <= 'F') {\n            unsigned char val = 10 + (c - 'A');\n            if (val == 11 || val == 13) {\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m128i primes_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0x40, 0x20 // F E D C B A 9 8 7 6 5 4 3 2 1 0\n    );\n    const __m128i subtract_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -'A'+11, -'0' // F E D C B A 9 8 7 6 5 4 3 2 1 0\n    );\n    const __m128i cmp_mask = _mm_set1_epi8(0x80);\n\n    int count = 0;\n    size_t i = 0;\n    size_t length = num.length();\n\n    for (; i <= length - 16; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        __m128i adjusted = _mm_subs_epi8(data, subtract_mask);\n        __m128i masked = _mm_and_si128(adjusted, primes_mask);\n        __m128i compared = _mm_cmpeq_epi8(masked, adjusted);\n        __m128i sign_bits = _mm_sign_epi8(cmp_mask, compared);\n        uint64_t bits = _mm_movemask_epi8(sign_bits);\n        count += __builtin_popcount(bits);\n    }\n\n    for (; i < length; ++i) {\n        char c = num[i];\n        if ((c == '2') || (c == '3') || (c == '5') || (c == '7') ||\n            (c == 'B') || (c == 'D')) {\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    if (num.empty()) return 0;\n\n    const __m128i prime_mask = _mm_set_epi8(\n        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0 // F E D C B A 9 8 7 6 5 4 3 2 1 0\n    );\n\n    int count = 0;\n    size_t len = num.length();\n    const unsigned char *data = reinterpret_cast<const unsigned char *>(num.data());\n\n    for(size_t i = 0; i < len; i += 16){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        __m128i cmp_res = _mm_cmpeq_epi8(chunk, prime_mask);\n        __m128i zeros = _mm_setzero_si128();\n        int temp_count = _mm_movemask_epi8(cmp_res);\n        count += __builtin_popcount(temp_count);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if(len < 3) return false;\n\n    const char* data = s.c_str();\n    __m128i v0, v1, v2, cmp1, cmp2, cmp3;\n\n    for(size_t i = 0; i <= len - 3; i+=4){\n        v0 = _mm_loadu_si128((__m128i*)(data + i));\n        v1 = _mm_loadu_si128((__m128i*)(data + i + 1));\n        v2 = _mm_loadu_si128((__m128i*)(data + i + 2));\n\n        cmp1 = _mm_cmpeq_epi8(v0, v1);\n        cmp2 = _mm_cmpeq_epi8(v1, v2);\n        cmp3 = _mm_cmpeq_epi8(v0, v2);\n\n        __m128i result = _mm_or_si128(cmp1, cmp2);\n        result = _mm_or_si128(result, cmp3);\n\n        if(_mm_movemask_epi8(result) & 0x7F){\n            return false;\n        }\n    }\n\n    // Check remaining characters if any\n    for(size_t i = (len / 4) * 4; i <= len - 3; ++i){\n        if(data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    const char *data = s.c_str();\n    __m128i v0, v1, v2, mask, cmp1, cmp2, cmp3;\n\n    // Process in chunks of 16 bytes\n    size_t i = 0;\n    for (; i + 15 < len - 2; i += 16) {\n        v0 = _mm_loadu_si128((__m128i const*)(data + i));\n        v1 = _mm_loadu_si128((__m128i const*)(data + i + 1));\n        v2 = _mm_loadu_si128((__m128i const*)(data + i + 2));\n\n        // Compare v0 with v1 and v2\n        cmp1 = _mm_cmpeq_epi8(v0, v1);\n        cmp2 = _mm_cmpeq_epi8(v0, v2);\n\n        // Compare v1 with v2\n        cmp3 = _mm_cmpeq_epi8(v1, v2);\n\n        // Check if any comparison resulted in equality\n        mask = _mm_or_si128(cmp1, cmp2);\n        mask = _mm_or_si128(mask, cmp3);\n\n        if (_mm_movemask_epi8(mask)) {\n            return false;\n        }\n    }\n\n    // Handle remaining characters\n    for (; i <= len - 3; ++i) {\n        if (data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    const char* str = s.c_str();\n    size_t i = 0;\n\n    __m128i mask = _mm_set1_epi8(0xFF);\n    __m128i zero = _mm_setzero_si128();\n\n    for (; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i*)&str[i]);\n        __m128i shifted1 = _mm_alignr_epi8(chunk, chunk, 1);\n        __m128i shifted2 = _mm_alignr_epi8(chunk, chunk, 2);\n\n        __m128i eq1 = _mm_cmpeq_epi8(chunk, shifted1);\n        __m128i eq2 = _mm_cmpeq_epi8(chunk, shifted2);\n        __m128i eq3 = _mm_cmpeq_epi8(shifted1, shifted2);\n\n        __m128i any_eq = _mm_or_si128(_mm_or_si128(eq1, eq2), eq3);\n        any_eq = _mm_andnot_si128(mask, any_eq);\n\n        if (_mm_movemask_epi8(any_eq) != 0xFFFF) {\n            return false;\n        }\n    }\n\n    for (; i <= len - 3; ++i) {\n        if (str[i] == str[i + 1] || str[i] == str[i + 2] || str[i + 1] == str[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    const char *data = s.c_str();\n    size_t i = 0;\n\n    __m128i vzero = _mm_set1_epi8(0);\n    __m128i mask = _mm_set_epi8(0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n\n    for (; i <= len - 16; i += 16) {\n        __m128i chunk = _mm_loadu_si128((__m128i const*)(data + i));\n        __m128i shifted1 = _mm_alignr_epi8(chunk, chunk, 1);\n        __m128i shifted2 = _mm_alignr_epi8(chunk, chunk, 2);\n\n        __m128i cmp1 = _mm_cmpeq_epi8(chunk, shifted1);\n        __m128i cmp2 = _mm_cmpeq_epi8(chunk, shifted2);\n\n        cmp1 = _mm_or_si128(cmp1, cmp2);\n        cmp1 = _mm_andnot_si128(mask, cmp1);\n\n        if (!_mm_test_all_zeros(cmp1, vzero)) {\n            return false;\n        }\n    }\n\n    for (; i <= len - 3; ++i) {\n        if (data[i] == data[i+1] || data[i] == data[i+2] || data[i+1] == data[i+2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <cstring>\n#include <cstdint>\n\nbool is_happy_simd(const std::string & s){\n    size_t len = s.length();\n    if (len < 3) return false;\n\n    const char *data = s.c_str();\n    size_t i = 0;\n\n    // Handle the main loop with SSE2 for aligned data\n    for (; i <= len - 4; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i eq1 = _mm_cmpeq_epi8(v, _mm_srli_si128(v, 1));\n        __m128i eq2 = _mm_cmpeq_epi8(v, _mm_srli_si128(v, 2));\n\n        // Check if any of the comparisons are true\n        if (_mm_movemask_epi8(eq1) || _mm_movemask_epi8(eq2)) {\n            return false;\n        }\n    }\n\n    // Handle remaining characters with scalar checks\n    for (; i <= len - 3; ++i) {\n        if ((data[i] == data[i+1]) || (data[i] == data[i+2]) || (data[i+1] == data[i+2])) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cmath>\n\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if ((n & 1) == 0 || n % 3 == 0) return false;\n\n    __m128i vstart = _mm_set_epi32(5, 5, 5, 5);\n    __m128i vstep = _mm_set_epi32(6, 6, 6, 6);\n    __m128i vlimit = _mm_set_epi32(std::sqrt((double)n), std::sqrt((double)n), std::sqrt((double)n), std::sqrt((double)n));\n    __m128i vzero = _mm_setzero_si128();\n    __m128i vmod1 = _mm_set_epi32(1, 1, 1, 1);\n    __m128i vmod5 = _mm_set_epi32(5, 5, 5, 5);\n\n    int limit = static_cast<int>(_mm_cvtsi128_si32(vlimit));\n    for (int i = 5; i <= limit; i += 6) {\n        __m128i vi = _mm_set_epi32(i, i + 2, i, i + 2);\n        __m128i vrem1 = _mm_rem_epu32(_mm_mullo_epi32(vi, _mm_set1_epi32(n)), vi);\n        __m128i vrem2 = _mm_rem_epu32(_mm_mullo_epi32(_mm_add_epi32(vi, vstep), _mm_set1_epi32(n)), _mm_add_epi32(vi, vstep));\n        __m128i vcond1 = _mm_cmpeq_epi32(vrem1, vzero);\n        __m128i vcond2 = _mm_cmpeq_epi32(vrem2, vzero);\n        if (_mm_movemask_epi8(_mm_or_si128(vcond1, vcond2)) != 0) return false;\n    }\n    return true;\n}\n\nbool prime_length_simd(const std::string &str) {\n    int length = static_cast<int>(str.length());\n    return is_prime(length);\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n\nbool is_divisible(__m128i num, __m128i divisor) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i quot = _mm_cvtepi32_epi64(_mm_div_epi32(num, divisor));\n    __m128i prod = _mm_mullo_epi32(quot, divisor);\n    return _mm_movemask_epi8(_mm_cmpeq_epi32(prod, num)) != 0;\n}\n\nbool prime_length_simd(const std::string &str) {\n    int n = str.length();\n    if (n < 2) return false;\n    if (n == 2) return true;\n    if ((n & 1) == 0) return false;\n\n    int sqrt_n = static_cast<int>(std::sqrt(n));\n    __m128i num = _mm_set1_epi32(n);\n\n    for (int i = 3; i <= sqrt_n; i += 8) {\n        __m128i divisors = _mm_set_epi32(i+4, i+3, i+2, i+1);\n        if (is_divisible(num, divisors)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n\nbool is_divisible(__m128i num, __m128i divisor) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i quot = _mm_cvtepi32_epi64(_mm_div_epi32(num, divisor));\n    __m128i prod = _mm_mullo_epi32(quot, divisor);\n    return !_mm_movemask_epi8(_mm_cmpeq_epi32(prod, num));\n}\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2) return true;\n    if ((length & 1) == 0) return false;\n\n    int sqrt_len = static_cast<int>(std::sqrt(length));\n    __m128i len_vec = _mm_set1_epi32(length);\n\n    for(int i = 3; i <= sqrt_len; i += 2){\n        __m128i div_vec = _mm_set1_epi32(i);\n        if(is_divisible(len_vec, div_vec)) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cmath>\n\nbool is_divisible(__m128i num, __m128i divisor) {\n    __m128i quotient = _mm_div_epi32(num, divisor);\n    __m128i product = _mm_mullo_epi32(quotient, divisor);\n    __m128i cmp = _mm_cmpeq_epi32(product, num);\n    int mask = _mm_movemask_epi8(cmp);\n    return mask != 0;\n}\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2) return true;\n    if ((length & 1) == 0) return false;\n\n    int sqrt_len = static_cast<int>(std::sqrt(length));\n    __m128i num = _mm_set1_epi32(length);\n\n    for(int i = 3; i <= sqrt_len; i += 8){\n        __m128i divisors = _mm_set_epi32(i+7, i+5, i+3, i);\n        if(is_divisible(num, divisors)) return false;\n    }\n\n    for(int i = sqrt_len - (sqrt_len % 8) + 1; i <= sqrt_len; ++i){\n        if(length % i == 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <cmath>\n\nbool is_divisible(__m128i num, __m128i div) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i quot = _mm_div_epi32(num, div);\n    __m128i prod = _mm_mullo_epi32(quot, div);\n    return !_mm_movemask_epi8(_mm_cmpeq_epi32(prod, num));\n}\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    if (length < 2) return false;\n    if (length == 2) return true;\n    if ((length & 1) == 0) return false;\n\n    int sqrt_len = static_cast<int>(std::sqrt(length));\n    __m128i num = _mm_set1_epi32(length);\n\n    for(int i = 3; i <= sqrt_len; i += 2){\n        __m128i div = _mm_set1_epi32(i);\n        if(is_divisible(num, div)) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    \n    // Load N into a SIMD register\n    __m128i vN = _mm_set_epi64x(0, N);\n    __m128i vSum = _mm_setzero_si128();\n    __m128i vTen = _mm_set1_epi8(10);\n    __m128i vZero = _mm_setzero_si128();\n    \n    while(_mm_cvtsi128_si32(vN) != 0){\n        // Perform divmod operation using SIMD\n        __m128i vRem = _mm_cvtepi8_epi16(_mm_sub_epi8(_mm_div_epu8(vN, vTen), _mm_mullo_epi8(_mm_div_epu8(vN, vTen), vTen)));\n        vN = _mm_div_epu8(vN, vTen);\n        vSum = _mm_add_epi8(vSum, vRem);\n    }\n    \n    // Horizontal sum of the digits\n    uint64_t tempSum = 0;\n    __m128i vTempSum = _mm_hadd_epi16(vSum, vSum);\n    vTempSum = _mm_hadd_epi16(vTempSum, vTempSum);\n    vTempSum = _mm_hadd_epi16(vTempSum, vTempSum);\n    _mm_storel_pi((__m64*)&tempSum, vTempSum);\n    sum += tempSum;\n\n    // Convert sum to binary string\n    std::ostringstream oss;\n    for(int i = 63; i >= 0; --i){\n        if(sum & (1ULL << i)){\n            oss << '1';\n        }\n        else{\n            oss << '0';\n        }\n    }\n\n    return oss.str();\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    __m128i vN = _mm_cvtsi64_si128(N);\n    __m128i tens = _mm_set1_epi32(10);\n\n    while(_mm_extract_epi64(vN, 0) != 0){\n        __m128i remainder = _mm_cvtepi32_epi64(_mm_and_si128(_mm_cvtepi64_epi32(vN), _mm_set1_epi32(9)));\n        vsum = _mm_add_epi64(vsum, remainder);\n        vN = _mm_srli_epi64(vN, 1);\n        vN = _mm_subs_epu8(vN, tens);\n    }\n\n    sum = _mm_extract_epi64(vsum, 0) + _mm_extract_epi64(vsum, 1);\n\n    std::ostringstream oss;\n    for(int i = 63; i >= 0; --i){\n        if(sum & (UINT64_C(1) << i)){\n            oss << '1';\n        }\n        else{\n            oss << '0';\n        }\n    }\n    return oss.str();\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    __m128i constant_10 = _mm_set1_epi8(10);\n    \n    while(N > 0){\n        __m128i digit = _mm_set_epi64x(N % 10, N / 10);\n        vsum = _mm_add_epi8(vsum, _mm_cvtsi64_si128(N % 10));\n        N /= 10;\n    }\n    \n    uint64_t temp_sum[2] = {0};\n    _mm_storeu_si128((__m128i*)temp_sum, vsum);\n    sum = temp_sum[0] + temp_sum[1];\n    \n    std::ostringstream oss;\n    for(int i = 63; i >= 0; --i){\n        if(sum & (1ULL << i)){\n            oss << '1';\n        }\n        else{\n            oss << '0';\n        }\n    }\n    return oss.str();\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string solve_simd(uint64_t N){\n    uint64_t sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    __m128i constant_10 = _mm_set1_epi8(10);\n    \n    while(N > 0){\n        __m128i digit = _mm_set_epi64x(N % 10, N % 10);\n        vsum = _mm_add_epi8(vsum, digit);\n        N /= 10;\n    }\n    \n    uint8_t buffer[16];\n    _mm_storeu_si128((__m128i*)buffer, vsum);\n    \n    for(int i = 0; i < 16; ++i){\n        sum += buffer[i];\n    }\n    \n    std::string binary_sum = \"\";\n    if(sum == 0){\n        binary_sum = \"0\";\n    }\n    else{\n        while(sum > 0){\n            binary_sum = (sum % 2 == 0 ? \"0\" : \"1\") + binary_sum;\n            sum /= 2;\n        }\n    }\n    return binary_sum;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string solve_simd(uint64_t N){\n    __m128i vsum = _mm_setzero_si128();\n    __m128i digits = _mm_set_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);\n    uint64_t mask = 0xF;\n\n    while (N > 0) {\n        __m128i vn = _mm_set_epi64x(N >> 32, N & 0xFFFFFFFF);\n        __m128i lo = _mm_and_si128(vn, _mm_set1_epi32(mask));\n        __m128i hi = _mm_and_si128(_mm_srli_epi32(vn, 4), _mm_set1_epi32(mask));\n        \n        __m128i sum_lo = _mm_shuffle_epi8(digits, lo);\n        __m128i sum_hi = _mm_shuffle_epi8(digits, hi);\n\n        vsum = _mm_add_epi8(vsum, sum_lo);\n        vsum = _mm_add_epi8(vsum, sum_hi);\n\n        N >>= 8;\n    }\n\n    uint8_t result[16];\n    _mm_storeu_si128((__m128i*)result, vsum);\n\n    int total_sum = 0;\n    for(int i = 0; i < 16; ++i){\n        total_sum += result[i];\n    }\n\n    std::string binary;\n    std::vector<char> bits;\n    do {\n        bits.push_back('0' + (total_sum % 2));\n        total_sum /= 2;\n    } while(total_sum);\n\n    for(auto it = bits.rbegin(); it != bits.rend(); ++it){\n        binary += *it;\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    if(lst.size() <= 1) return sum;\n\n    __m128i vsum = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(0xAAAAAAAA);\n    __m128i mask_odd_idx = _mm_set_epi32(0, -1, 0, -1);\n\n    int n = lst.size();\n    int i = 1;\n    for(; i + 3 < n; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i even_masked = _mm_and_si128(vec, mask_even);\n        __m128i odd_idx_masked = _mm_and_si128(even_masked, mask_odd_idx);\n        vsum = _mm_add_epi32(vsum, odd_idx_masked);\n    }\n\n    int temp[4] = {0};\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for(; i < n; ++i){\n        if(i % 2 == 1 && lst[i] % 2 == 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    if (lst.size() <= 1) return sum;\n\n    size_t size = lst.size();\n    size_t aligned_size = size / 4 * 4;\n\n    __m128i mask_even = _mm_set1_epi32(0xAAAAAAAA); // Even numbers mask\n    __m128i mask_odd_index = _mm_set_epi32(0, -1, 0, -1); // Odd indices mask\n\n    __m128i total = _mm_setzero_si128();\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i and_even = _mm_and_si128(vec, mask_even);\n        __m128i cmp_even = _mm_cmpeq_epi32(and_even, _mm_setzero_si128());\n        __m128i odd_idx = _mm_and_si128(cmp_even, mask_odd_index);\n        __m128i sel = _mm_and_si128(vec, odd_idx);\n        total = _mm_add_epi32(total, sel);\n    }\n\n    __m128i shf = _mm_shuffle_epi32(total, _MM_SHUFFLE(0, 1, 2, 3));\n    total = _mm_add_epi32(total, shf);\n    shf = _mm_shuffle_epi32(total, _MM_SHUFFLE(1, 0, 3, 2));\n    total = _mm_add_epi32(total, shf);\n\n    sum += _mm_extract_epi32(total, 0);\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    if (lst.size() <= 1) return sum;\n\n    size_t size = lst.size();\n    size_t aligned_size = size / 4 * 4;\n\n    __m128i vsum = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(0xAAAAAAAA); // Mask for even numbers\n    __m128i mask_odd_idx = _mm_set_epi32(0, -1, 0, -1); // Mask for odd indices\n\n    const int* data = lst.data();\n    for (size_t i = 1; i < aligned_size; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&data[i]);\n        __m128i even_mask = _mm_and_si128(vec, mask_even);\n        __m128i is_even = _mm_cmpeq_epi32(even_mask, _mm_setzero_si128());\n        __m128i valid = _mm_and_si128(is_even, mask_odd_idx);\n        __m128i result = _mm_and_si128(valid, vec);\n        vsum = _mm_add_epi32(vsum, result);\n    }\n\n    // Horizontal sum\n    __m128i vsum_hi = _mm_unpackhi_epi32(vsum, _mm_setzero_si128());\n    __m128i vsum_all = _mm_add_epi32(vsum, vsum_hi);\n    __m128i vsum_final = _mm_hadd_epi32(vsum_all, vsum_all);\n    sum += _mm_extract_epi32(vsum_final, 0);\n\n    // Handle remaining elements\n    for (size_t i = aligned_size; i < size; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    if (lst.size() <= 1) return sum;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_odd_indices = _mm_set_epi32(0, -1, 0, -1); // Mask for odd indices\n    __m128i mask_even_values = _mm_set1_epi32(0xFFFFFFFE);   // Mask to check even numbers\n\n    size_t i = 0;\n    __m128i total = _mm_setzero_si128();\n\n    for (; i + 3 < lst.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        \n        // Check if index is odd\n        __m128i is_odd = _mm_and_si128(_mm_cmpgt_epi32(indices, zero), mask_odd_indices);\n\n        // Check if value is even\n        __m128i is_even = _mm_cmpeq_epi32(_mm_and_si128(vec, mask_even_values), zero);\n\n        // Combine conditions: odd index and even value\n        __m128i valid = _mm_and_si128(is_odd, is_even);\n\n        // Select valid values\n        __m128i selected = _mm_and_si128(valid, vec);\n\n        // Add to total\n        total = _mm_add_epi32(total, selected);\n    }\n\n    // Horizontal sum of total\n    __m128i sum1 = _mm_hadd_epi32(total, total);\n    __m128i sum2 = _mm_hadd_epi32(sum1, sum1);\n    _mm_store_si128((__m128i*)&sum, sum2);\n    sum = sum & 0xFFFFFFFF;\n\n    // Handle remaining elements\n    for (; i < lst.size(); ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst) {\n    int sum = 0;\n    if (lst.size() <= 1) return sum;\n\n    __m128i vsum = _mm_setzero_si128();\n    __m128i mask_even = _mm_set1_epi32(1); // To check if number is even\n    __m128i mask_one = _mm_set1_epi32(1);  // For odd index check\n\n    size_t i = 1;\n    for (; i + 3 < lst.size(); i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i odd_indices = _mm_and_si128(indices, mask_one);\n        __m128i cmp_odd = _mm_cmpeq_epi32(odd_indices, mask_one);\n\n        __m128i even_check = _mm_and_si128(vec, mask_even);\n        __m128i is_even = _mm_cmpeq_epi32(even_check, _mm_setzero_si128());\n\n        __m128i valid = _mm_and_si128(cmp_odd, is_even);\n        __m128i selected = _mm_and_si128(valid, vec);\n\n        vsum = _mm_add_epi32(vsum, selected);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    sum = temp[0] + temp[1] + temp[2] + temp[3];\n\n    for (; i < lst.size(); ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.length();\n    std::string result(length, ' ');\n\n    for(int i = 0; i < length; i += 16){\n        int chunk_size = std::min(16, length - i);\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_a = _mm_set1_epi8('a');\n        __m128i upper_z = _mm_set1_epi8('z');\n        __m128i lower_A = _mm_set1_epi8('A');\n        __m128i upper_Z = _mm_set1_epi8('Z');\n        __m128i shifter = _mm_set1_epi8(shift);\n\n        // Create mask for lowercase letters\n        __m128i is_lower = _mm_and_si128(_mm_cmplt_epi8(data, upper_z), _mm_cmpgt_epi8(data, lower_a));\n        __m128i shifted_lower = _mm_add_epi8(data, shifter);\n        __m128i wrap_lower = _mm_sub_epi8(shifted_lower, _mm_set1_epi8(26));\n        shifted_lower = _mm_blendv_epi8(shifted_lower, wrap_lower, _mm_cmplt_epi8(shifted_lower, lower_a));\n\n        // Create mask for uppercase letters\n        __m128i is_upper = _mm_and_si128(_mm_cmplt_epi8(data, upper_Z), _mm_cmpgt_epi8(data, lower_A));\n        __m128i shifted_upper = _mm_add_epi8(data, shifter);\n        __m128i wrap_upper = _mm_sub_epi8(shifted_upper, _mm_set1_epi8(26));\n        shifted_upper = _mm_blendv_epi8(shifted_upper, wrap_upper, _mm_cmplt_epi8(shifted_upper, lower_A));\n\n        // Combine results\n        __m128i encrypted = _mm_blendv_epi8(shifted_lower, shifted_upper, is_upper);\n        encrypted = _mm_blendv_epi8(data, encrypted, _mm_or_si128(is_lower, is_upper));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), encrypted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.length();\n    if (length == 0) return \"\";\n\n    char* result = new char[length + 1];\n    memset(result, 0, (length + 1) * sizeof(char));\n\n    for(int i = 0; i < length; i += 16){\n        __m128i input = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i lowerA = _mm_set1_epi8('a');\n        __m128i lowerZ = _mm_set1_epi8('z');\n        __m128i upperA = _mm_set1_epi8('A');\n        __m128i upperZ = _mm_set1_epi8('Z');\n        __m128i shiftVal = _mm_set1_epi8(shift);\n\n        __m128i isLower = _mm_and_si128(_mm_cmpgt_epi8(input, lowerA), _mm_cmple_epi8(input, lowerZ));\n        __m128i isUpper = _mm_and_si128(_mm_cmpgt_epi8(input, upperA), _mm_cmple_epi8(input, upperZ));\n\n        __m128i shifted = _mm_add_epi8(input, shiftVal);\n\n        __m128i wrapLower = _mm_sub_epi8(shifted, _mm_set1_epi8(26));\n        __m128i wrapUpper = _mm_sub_epi8(shifted, _mm_set1_epi8(26));\n\n        __m128i lowerResult = _mm_blendv_epi8(shifted, wrapLower, _mm_cmpgt_epi8(shifted, lowerZ));\n        __m128i finalResult = _mm_blendv_epi8(lowerResult, wrapUpper, _mm_and_si128(_mm_cmpgt_epi8(shifted, upperZ), isUpper));\n\n        __m128i unchanged = _mm_andnot_si128(_mm_or_si128(isLower, isUpper), input);\n        finalResult = _mm_or_si128(finalResult, unchanged);\n\n        _mm_storeu_si128((__m128i*)&result[i], finalResult);\n    }\n\n    result[length] = '\\0';\n    std::string res(result, length);\n    delete[] result;\n    return res;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.length();\n    std::string result(length, ' ');\n\n    const __m128i lower_a = _mm_set1_epi8('a');\n    const __m128i lower_z = _mm_set1_epi8('z');\n    const __m128i upper_a = _mm_set1_epi8('A');\n    const __m128i upper_z = _mm_set1_epi8('Z');\n    const __m128i shift_val = _mm_set1_epi8(shift);\n    const __m128i mod_val = _mm_set1_epi8(26);\n\n    for(int i = 0; i < length; i += 16){\n        int chunk_size = std::min(length - i, 16);\n        __m128i input = _mm_loadu_si128((__m128i*)&s[i]);\n        \n        __m128i encrypted = _mm_add_epi8(input, shift_val);\n\n        __m128i is_lower = _mm_and_si128(_mm_cmplt_epi8(input, lower_z), _mm_cmpgt_epi8(input, lower_a));\n        __m128i is_upper = _mm_and_si128(_mm_cmplt_epi8(input, upper_z), _mm_cmpgt_epi8(input, upper_a));\n\n        __m128i overflow_mask = _mm_or_si128(\n            _mm_and_si128(_mm_cmplt_epi8(encrypted, lower_a), is_lower),\n            _mm_and_si128(_mm_cmplt_epi8(encrypted, upper_a), is_upper)\n        );\n\n        __m128i adjust = _mm_sub_epi8(_mm_mullo_epi16(overflow_mask, mod_val), overflow_mask);\n        encrypted = _mm_add_epi8(encrypted, adjust);\n\n        _mm_storeu_si128((__m128i*)&result[i], encrypted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encrypt_simd(const std::string & s){\n    const int shift = 4; // 2 * 2\n    const int length = s.size();\n    std::string result(length, ' ');\n\n    const __m128i lower_a = _mm_set1_epi8('a');\n    const __m128i upper_a = _mm_set1_epi8('A');\n    const __m128i lower_z = _mm_set1_epi8('z');\n    const __m128i upper_z = _mm_set1_epi8('Z');\n    const __m128i shift_val = _mm_set1_epi8(shift);\n\n    for(int i = 0; i < length; i += 16){\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        \n        __m128i is_upper = _mm_and_si128(_mm_cmplt_epi8(data, lower_a), _mm_cmpgt_epi8(data, upper_z));\n        __m128i is_lower = _mm_and_si128(_mm_cmplt_epi8(data, lower_z), _mm_cmpgt_epi8(data, lower_a));\n\n        __m128i shifted_data = _mm_add_epi8(data, shift_val);\n\n        __m128i wrap_upper = _mm_and_si128(_mm_cmplt_epi8(shifted_data, upper_a), is_upper);\n        __m128i wrap_lower = _mm_and_si128(_mm_cmplt_epi8(shifted_data, lower_a), is_lower);\n\n        shifted_data = _mm_sub_epi8(shifted_data, _mm_or_si128(wrap_upper, wrap_lower));\n\n        shifted_data = _mm_blendv_epi8(shifted_data, _mm_add_epi8(data, _mm_set1_epi8(26)), _mm_or_si128(wrap_upper, wrap_lower));\n\n        int bytes_to_store = std::min(16, length - i);\n        __m128i mask = _mm_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1);\n        mask = _mm_slli_si128(mask, 16 - bytes_to_store);\n\n        __m128i masked_data = _mm_and_si128(shifted_data, mask);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), masked_data);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encrypt_simd(const std::string & s){\n    if (s.empty()) return \"\";\n    const int shift = 4; // 2 * 2\n    const int length = s.size();\n    std::string result(s);\n\n    for (size_t i = 0; i < length; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_a = _mm_set1_epi8('a');\n        __m128i lower_z = _mm_set1_epi8('z');\n        __m128i upper_a = _mm_set1_epi8('A');\n        __m128i upper_z = _mm_set1_epi8('Z');\n        __m128i shift_val = _mm_set1_epi8(shift);\n\n        // Create mask for lowercase letters\n        __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(data, lower_a), _mm_cmple_epi8(data, lower_z));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(data, upper_a), _mm_cmple_epi8(data, upper_z));\n\n        // Shift characters\n        __m128i shifted = _mm_add_epi8(data, shift_val);\n\n        // Handle wrap-around for lowercase\n        __m128i lower_wrap = _mm_sub_epi8(shifted, _mm_set1_epi8(26));\n        __m128i lower_result = _mm_blendv_epi8(shifted, lower_wrap, _mm_cmpgt_epi8(shifted, lower_z));\n\n        // Handle wrap-around for uppercase\n        __m128i upper_wrap = _mm_sub_epi8(lower_result, _mm_set1_epi8(26));\n        __m128i final_result = _mm_blendv_epi8(lower_result, upper_wrap, _mm_and_si128(_mm_cmpgt_epi8(lower_result, upper_z), is_upper));\n\n        // Apply mask to preserve non-alphabetic characters\n        __m128i is_alpha = _mm_or_si128(is_lower, is_upper);\n        __m128i encrypted = _mm_blendv_epi8(data, final_result, is_alpha);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), encrypted);\n    }\n\n    // Trim result to original size in case of padding\n    result.resize(length);\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encode_simd(const std::string & message){\n    const char vowels_lower[] = \"aeiou\";\n    const char vowels_upper[] = \"AEIOU\";\n    const char replace_lower[] = \"cgkmo\";\n    const char replace_upper[] = \"CGKMO\";\n\n    std::string result;\n    result.reserve(message.size());\n\n    const int chunk_size = 16;\n    size_t i = 0;\n    for (; i + chunk_size <= message.size(); i += chunk_size) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n\n        // Create masks for lowercase and uppercase letters\n        __m128i lower_mask = _mm_and_si128(_mm_cmpgt_epi8(vec, _mm_set1_epi8('a' - 1)), _mm_cmple_epi8(vec, _mm_set1_epi8('z')));\n        __m128i upper_mask = _mm_and_si128(_mm_cmpgt_epi8(vec, _mm_set1_epi8('A' - 1)), _mm_cmple_epi8(vec, _mm_set1_epi8('Z')));\n\n        // Swap case\n        __m128i swapped_case = _mm_or_si128(\n            _mm_and_si128(_mm_sub_epi8(_mm_add_epi8(vec, _mm_set1_epi8(32)), _mm_set1_epi8(64)), lower_mask),\n            _mm_and_si128(_mm_sub_epi8(_mm_add_epi8(vec, _mm_set1_epi8(-32)), _mm_set1_epi8(-64)), upper_mask)\n        );\n\n        // Replace vowels with corresponding letters\n        __m128i replaced_vec = vec;\n        for (int j = 0; j < 5; ++j) {\n            __m128i vowel_mask = _mm_or_si128(\n                _mm_cmpeq_epi8(vec, _mm_set1_epi8(vowels_lower[j])),\n                _mm_cmpeq_epi8(vec, _mm_set1_epi8(vowels_upper[j]))\n            );\n            replaced_vec = _mm_blendv_epi8(replaced_vec, \n                _mm_set1_epi8((vowels_lower[j] >= 'i') ? replace_lower[j] : replace_lower[j + 2]), \n                _mm_and_si128(vowel_mask, lower_mask)\n            );\n            replaced_vec = _mm_blendv_epi8(replaced_vec, \n                _mm_set1_epi8((vowels_upper[j] >= 'I') ? replace_upper[j] : replace_upper[j + 2]), \n                _mm_and_si128(vowel_mask, upper_mask)\n            );\n        }\n\n        // Combine swapped case and replaced vowels\n        __m128i final_vec = _mm_or_si128(swapped_case, replaced_vec);\n\n        // Store the result\n        char temp[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), final_vec);\n        result.append(temp, chunk_size);\n    }\n\n    // Process remaining characters\n    for (; i < message.size(); ++i) {\n        char c = message[i];\n        if (c >= 'a' && c <= 'z') {\n            c = (c == 'a') ? 'c' : (c == 'e') ? 'g' : (c == 'i') ? 'k' : (c == 'o') ? 'q' : (c == 'u') ? 'w' : (c - 32);\n        } else if (c >= 'A' && c <= 'Z') {\n            c = (c == 'A') ? 'C' : (c == 'E') ? 'G' : (c == 'I') ? 'K' : (c == 'O') ? 'Q' : (c == 'U') ? 'W' : (c + 32);\n        }\n        result += c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encode_simd(const std::string & message){\n    const unsigned char vowels_lower[] = { 'a', 'e', 'i', 'o', 'u' };\n    const unsigned char vowels_upper[] = { 'A', 'E', 'I', 'O', 'U' };\n    const unsigned char replace_lower[] = { 'c', 'g', 'k', 'q', 'w' };\n    const unsigned char replace_upper[] = { 'C', 'G', 'K', 'Q', 'W' };\n\n    std::string result;\n    result.reserve(message.size());\n\n    const int len = message.size();\n    for(int i = 0; i < len; i += 16){\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n\n        // Swap case\n        __m128i lower_mask = _mm_cmplt_epi8(data, _mm_set1_epi8('a'));\n        __m128i upper_mask = _mm_cmpgt_epi8(data, _mm_set1_epi8('Z'));\n        __m128i is_upper = _mm_andnot_si128(lower_mask, upper_mask);\n        __m128i swap_case = _mm_xor_si128(data, _mm_set1_epi8(' '));\n        data = _mm_blendv_epi8(swap_case, data, is_upper);\n\n        // Replace vowels\n        for(int j = 0; j < 5; ++j){\n            __m128i vowel_mask = _mm_or_si128(\n                _mm_cmpeq_epi8(data, _mm_set1_epi8(vowels_lower[j])),\n                _mm_cmpeq_epi8(data, _mm_set1_epi8(vowels_upper[j]))\n            );\n            __m128i replace = _mm_or_si128(\n                _mm_and_si128(_mm_cmpeq_epi8(data, _mm_set1_epi8(vowels_lower[j])), _mm_set1_epi8(replace_lower[j])),\n                _mm_and_si128(_mm_cmpeq_epi8(data, _mm_set1_epi8(vowels_upper[j])), _mm_set1_epi8(replace_upper[j]))\n            );\n            data = _mm_blendv_epi8(data, replace, vowel_mask);\n        }\n\n        // Store results\n        if(i + 16 <= len){\n            char temp[16];\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), data);\n            result.append(temp, 16);\n        }\n        else{\n            alignas(16) char temp[16];\n            _mm_store_si128(reinterpret_cast<__m128i*>(temp), data);\n            result.append(temp, len - i);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encode_simd(const std::string & message){\n    const unsigned char vowel_map[256] = {\n        ['a'] = 'c', ['A'] = 'C',\n        ['e'] = 'g', ['E'] = 'G',\n        ['i'] = 'k', ['I'] = 'K',\n        ['o'] = 'q', ['O'] = 'Q',\n        ['u'] = 'w', ['U'] = 'W'\n    };\n    \n    std::string result;\n    result.reserve(message.size());\n    \n    const int len = message.size();\n    const int aligned_len = len - (len % 16);\n    \n    for(int i = 0; i < aligned_len; i += 16){\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n        \n        // Swap case\n        __m128i lower_mask = _mm_cmplt_epi8(data, _mm_set1_epi8('a'));\n        __m128i upper_mask = _mm_cmpgt_epi8(data, _mm_set1_epi8('Z'));\n        __m128i is_upper = _mm_andnot_si128(lower_mask, upper_mask);\n        __m128i swap_mask = _mm_xor_si128(_mm_set1_epi8(0x20), is_upper);\n        __m128i swapped_case = _mm_xor_si128(data, swap_mask);\n        \n        // Replace vowels\n        __m128i replaced_vowels = _mm_shuffle_epi8(_mm_loadu_si128(reinterpret_cast<const __m128i*>(vowel_map)), swapped_case);\n        __m128i non_vowel_mask = _mm_cmpeq_epi8(_mm_setzero_si128(), _mm_shuffle_epi8(_mm_loadu_si128(reinterpret_cast<const __m128i*>(vowel_map)), data));\n        __m128i final_data = _mm_or_si128(_mm_andnot_si128(non_vowel_mask, replaced_vowels), _mm_and_si128(non_vowel_mask, swapped_case));\n        \n        // Store result\n        char temp[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), final_data);\n        result.append(temp, temp + 16);\n    }\n    \n    for(int i = aligned_len; i < len; ++i){\n        char c = message[i];\n        if(c >= 'a' && c <= 'z') c ^= 0x20;\n        else if(c >= 'A' && c <= 'Z') c ^= 0x20;\n        if(vowel_map[c]) c = vowel_map[c];\n        result += c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    const char vowel_map[256] = {\n        ['a'] = 'c', ['A'] = 'C',\n        ['e'] = 'g', ['E'] = 'G',\n        ['i'] = 'k', ['I'] = 'K',\n        ['o'] = 'q', ['O'] = 'Q',\n        ['u'] = 'w', ['U'] = 'W'\n    };\n    \n    std::string result;\n    result.reserve(message.size());\n    \n    const int len = message.size();\n    for(int i = 0; i < len; i += 16){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n        \n        // Create a mask for lowercase letters\n        __m128i lower_mask = _mm_and_si128(_mm_cmplt_epi8(chunk, _mm_set1_epi8('z')),\n                                           _mm_cmpgt_epi8(chunk, _mm_set1_epi8('a')));\n        \n        // Create a mask for uppercase letters\n        __m128i upper_mask = _mm_and_si128(_mm_cmplt_epi8(chunk, _mm_set1_epi8('Z')),\n                                           _mm_cmpgt_epi8(chunk, _mm_set1_epi8('A')));\n        \n        // Swap case\n        __m128i swapped_case = _mm_sub_epi8(_mm_xor_si128(chunk, _mm_set1_epi8(0x20)), _mm_set1_epi8(0x20));\n        \n        // Select swapped case based on masks\n        __m128i swapped = _mm_or_si128(\n            _mm_and_si128(lower_mask, swapped_case),\n            _mm_andnot_si128(lower_mask, chunk)\n        );\n        swapped = _mm_or_si128(\n            _mm_and_si128(upper_mask, swapped_case),\n            _mm_andnot_si128(upper_mask, swapped)\n        );\n        \n        // Replace vowels using a lookup table\n        __m128i replaced_vowels = _mm_shuffle_epi8(_mm_loadu_si128(reinterpret_cast<const __m128i*>(vowel_map)), chunk);\n        __m128i final_chunk = _mm_or_si128(\n            _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('a')), _mm_set1_epi8('c')),\n            _mm_or_si128(\n                _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('e')), _mm_set1_epi8('g')),\n                _mm_or_si128(\n                    _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('i')), _mm_set1_epi8('k')),\n                    _mm_or_si128(\n                        _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('o')), _mm_set1_epi8('q')),\n                        _mm_or_si128(\n                            _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('u')), _mm_set1_epi8('w')),\n                            _mm_or_si128(\n                                _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('A')), _mm_set1_epi8('C')),\n                                _mm_or_si128(\n                                    _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('E')), _mm_set1_epi8('G')),\n                                    _mm_or_si128(\n                                        _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('I')), _mm_set1_epi8('K')),\n                                        _mm_or_si128(\n                                            _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('O')), _mm_set1_epi8('Q')),\n                                            _mm_or_si128(\n                                                _mm_and_si128(_mm_cmpeq_epi8(chunk, _mm_set1_epi8('U')), _mm_set1_epi8('W')),\n                                                swapped\n                                            )\n                                        )\n                                    )\n                                )\n                            )\n                        )\n                    )\n                )\n            )\n        );\n        \n        // Store the result\n        if(i + 16 <= len){\n            char temp[16];\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), final_chunk);\n            result.append(temp, 16);\n        }\n        else{\n            alignas(16) char temp[16];\n            _mm_store_si128(reinterpret_cast<__m128i*>(temp), final_chunk);\n            result.append(temp, len - i);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <algorithm>\n\nstd::string encode_simd(const std::string & message){\n    const unsigned char vowel_map[256] = {\n        ['a'] = 'c', ['e'] = 'g', ['i'] = 'k', ['o'] = 'q', ['u'] = 'w',\n        ['A'] = 'C', ['E'] = 'G', ['I'] = 'K', ['O'] = 'Q', ['U'] = 'W'\n    };\n    \n    std::string result;\n    result.reserve(message.size());\n    \n    const int len = message.size();\n    const int aligned_len = len - (len % 16);\n    \n    for(int i = 0; i < aligned_len; i += 16){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n        \n        // Swap case\n        __m128i lower_mask = _mm_cmplt_epi8(vec, _mm_set1_epi8('a'));\n        __m128i diff = _mm_set1_epi8('a' - 'A');\n        __m128i swapped_case = _mm_sub_epi8(_mm_add_epi8(vec, diff), _mm_and_si128(lower_mask, diff));\n        \n        // Replace vowels\n        __m128i replaced_vowels = _mm_shuffle_epi8(_mm_loadu_si128(reinterpret_cast<const __m128i*>(vowel_map)), vec);\n        __m128i non_vowel_mask = _mm_cmpeq_epi8(vec, replaced_vowels);\n        swapped_case = _mm_blendv_epi8(swapped_case, replaced_vowels, non_vowel_mask);\n        \n        // Store result\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), swapped_case);\n    }\n    \n    for(int i = aligned_len; i < len; ++i){\n        char c = message[i];\n        if(c >= 'a' && c <= 'z'){\n            c = c - 'a' + 'A';\n        }\n        else if(c >= 'A' && c <= 'Z'){\n            c = c - 'A' + 'a';\n        }\n        if(vowel_map[c]){\n            c = vowel_map[c];\n        }\n        result += c;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first = true;\n    bool is_all_upper = false;\n    bool is_all_lower = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        if (key.empty()) return false;\n\n        bool current_is_upper = true;\n        bool current_is_lower = true;\n\n        size_t len = key.length();\n        size_t aligned_len = len - (len % 16);\n\n        const char* data = key.c_str();\n\n        __m128i lower_mask = _mm_set_epi8(\n            0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,0xFF,\n            'a'-'A','a'-'A','a'-'A','a'-'A','a'-'A','a'-'A','a'-'A','a'-'A'\n        );\n        __m128i upper_a = _mm_set1_epi8('A');\n        __m128i lower_a = _mm_set1_epi8('a');\n        __m128i upper_z = _mm_set1_epi8('Z');\n        __m128i lower_z = _mm_set1_epi8('z');\n\n        for (size_t i = 0; i < aligned_len; i += 16) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n            __m128i adjusted = _mm_subs_epi8(vec, lower_mask);\n            __m128i cmp_upper = _mm_cmpgt_epi8(_mm_cmpeq_epi8(vec, upper_a), _mm_cmpeq_epi8(vec, upper_z));\n            __m128i cmp_lower = _mm_cmpgt_epi8(_mm_cmpeq_epi8(adjusted, lower_a), _mm_cmpeq_epi8(adjusted, lower_z));\n\n            if (_mm_movemask_epi8(cmp_upper) != 0xFFFF) {\n                current_is_upper = false;\n            }\n            if (_mm_movemask_epi8(cmp_lower) != 0xFFFF) {\n                current_is_lower = false;\n            }\n        }\n\n        for (size_t i = aligned_len; i < len; ++i) {\n            if (data[i] >= 'A' && data[i] <= 'Z') {\n                current_is_lower = false;\n            } else if (data[i] >= 'a' && data[i] <= 'z') {\n                current_is_upper = false;\n            } else {\n                current_is_upper = false;\n                current_is_lower = false;\n            }\n        }\n\n        if (first) {\n            is_all_upper = current_is_upper;\n            is_all_lower = current_is_lower;\n            first = false;\n        } else {\n            if ((current_is_upper && !is_all_upper) || (current_is_lower && !is_all_lower)) {\n                return false;\n            }\n            is_all_upper &= current_is_upper;\n            is_all_lower &= current_is_lower;\n        }\n    }\n\n    return is_all_upper || is_all_lower;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first_key_processed = false;\n    int case_type = 0; // 1 for all lower, 2 for all upper\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n\n        if (key.empty()) return false;\n\n        __m128i all_lower_mask = _mm_set1_epi8('a' - 1);\n        __m128i all_upper_mask = _mm_set1_epi8('A' - 1);\n        __m128i lower_bound = _mm_set1_epi8('a');\n        __m128i upper_bound = _mm_set1_epi8('z');\n        __m128i upper_lower_bound = _mm_set1_epi8('A');\n        __m128i upper_upper_bound = _mm_set1_epi8('Z');\n\n        size_t len = key.length();\n        bool mixed = false;\n\n        for (size_t i = 0; i < len; i += 16) {\n            size_t chunk_size = std::min(len - i, size_t(16));\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.c_str() + i));\n\n            __m128i is_lower = _mm_and_si128(_mm_cmplt_epi8(data, _mm_set1_epi8('z' + 1)),\n                                            _mm_cmpgt_epi8(data, _mm_set1_epi8('a' - 1)));\n            __m128i is_upper = _mm_and_si128(_mm_cmplt_epi8(data, _mm_set1_epi8('Z' + 1)),\n                                            _mm_cmpgt_epi8(data, _mm_set1_epi8('A' - 1)));\n\n            __m128i lower_check = _mm_cmpeq_epi8(is_lower, _mm_set1_epi8(-1));\n            __m128i upper_check = _mm_cmpeq_epi8(is_upper, _mm_set1_epi8(-1));\n\n            int lower_result = _mm_movemask_epi8(lower_check);\n            int upper_result = _mm_movemask_epi8(upper_check);\n\n            if (chunk_size < 16) {\n                lower_result &= ((1 << chunk_size) - 1);\n                upper_result &= ((1 << chunk_size) - 1);\n            }\n\n            if (lower_result == ((1 << chunk_size) - 1)) {\n                if (case_type == 2) return false;\n                case_type = 1;\n            } else if (upper_result == ((1 << chunk_size) - 1)) {\n                if (case_type == 1) return false;\n                case_type = 2;\n            } else {\n                return false;\n            }\n        }\n\n        if (!first_key_processed) {\n            first_key_processed = true;\n        } else if (case_type == 0) {\n            return false;\n        }\n    }\n\n    return case_type != 0;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first = true;\n    bool is_all_lower = false;\n    bool is_all_upper = false;\n\n    __m128i lower_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0);\n    __m128i upper_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -32, 0);\n\n    for(const auto& [key, value] : dict){\n        const unsigned char* str = reinterpret_cast<const unsigned char*>(key.c_str());\n        size_t len = key.length();\n        bool mixed = false;\n\n        for(size_t i = 0; i < len; i += 16){\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&str[i]));\n            __m128i cmp_lower = _mm_cmpeq_epi8(_mm_and_si128(vec, lower_mask), vec);\n            __m128i cmp_upper = _mm_cmpeq_epi8(_mm_or_si128(vec, upper_mask), vec);\n\n            __m128i all_lower = _mm_movemask_epi8(cmp_lower);\n            __m128i all_upper = _mm_movemask_epi8(cmp_upper);\n\n            if(all_lower == 0xFFFF){\n                if(first){\n                    is_all_lower = true;\n                    is_all_upper = false;\n                    first = false;\n                }\n                else if(!is_all_lower){\n                    return false;\n                }\n            }\n            else if(all_upper == 0xFFFF){\n                if(first){\n                    is_all_upper = true;\n                    is_all_lower = false;\n                    first = false;\n                }\n                else if(!is_all_upper){\n                    return false;\n                }\n            }\n            else{\n                mixed = true;\n                break;\n            }\n        }\n\n        if(mixed){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first = true;\n    bool is_all_lower = false;\n    bool is_all_upper = false;\n\n    __m128i lower_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0);\n    __m128i upper_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -65, 0);\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        if (key.empty()) return false;\n\n        bool current_is_lower = true;\n        bool current_is_upper = true;\n\n        size_t i = 0;\n        for (; i + 15 < key.size(); i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&key[i]));\n            __m128i cmp_lower = _mm_cmplt_epi8(data, lower_mask);\n            __m128i cmp_upper = _mm_cmpgt_epi8(data, upper_mask);\n\n            if (_mm_movemask_epi8(cmp_lower) != 0xFFFF) current_is_lower = false;\n            if (_mm_movemask_epi8(cmp_upper) != 0xFFFF) current_is_upper = false;\n\n            if (!current_is_lower && !current_is_upper) break;\n        }\n\n        for (; i < key.size(); ++i) {\n            if (key[i] >= 'a' && key[i] <= 'z') {\n                current_is_upper = false;\n            } else if (key[i] >= 'A' && key[i] <= 'Z') {\n                current_is_lower = false;\n            } else {\n                current_is_lower = false;\n                current_is_upper = false;\n                break;\n            }\n        }\n\n        if (first) {\n            is_all_lower = current_is_lower;\n            is_all_upper = current_is_upper;\n            first = false;\n        } else {\n            if ((current_is_lower && !is_all_lower) || (current_is_upper && !is_all_upper)) {\n                return false;\n            }\n            is_all_lower &= current_is_lower;\n            is_all_upper &= current_is_upper;\n        }\n    }\n\n    return is_all_lower || is_all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <cctype>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if (dict.empty()) return false;\n\n    bool first = true;\n    bool is_all_upper = false;\n    bool is_all_lower = false;\n\n    for (const auto& pair : dict) {\n        const std::string& key = pair.first;\n        if (key.empty()) continue;\n\n        bool has_upper = false;\n        bool has_lower = false;\n\n        size_t len = key.length();\n        size_t aligned_len = len / 16 * 16;\n\n        __m128i lower_mask = _mm_set1_epi8('a');\n        __m128i upper_mask = _mm_set1_epi8('z');\n        __m128i Upper_mask = _mm_set1_epi8('A');\n        __m128i Lower_mask = _mm_set1_epi8('Z');\n\n        for (size_t i = 0; i < aligned_len; i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key.c_str() + i));\n            __m128i is_lower = _mm_and_si128(_mm_cmpgt_epi8(data, lower_mask), _mm_cmple_epi8(data, upper_mask));\n            __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(data, Upper_mask), _mm_cmple_epi8(data, Lower_mask));\n\n            if (!_mm_testz_si128(is_lower, is_lower)) {\n                has_lower = true;\n            }\n            if (!_mm_testz_si128(is_upper, is_upper)) {\n                has_upper = true;\n            }\n        }\n\n        for (size_t i = aligned_len; i < len; ++i) {\n            unsigned char c = key[i];\n            if (c >= 'a' && c <= 'z') {\n                has_lower = true;\n            } else if (c >= 'A' && c <= 'Z') {\n                has_upper = true;\n            }\n        }\n\n        if (first) {\n            is_all_upper = !has_lower;\n            is_all_lower = !has_upper;\n            first = false;\n        } else {\n            if (is_all_upper && has_lower) return false;\n            if (is_all_lower && has_upper) return false;\n        }\n    }\n\n    return is_all_upper || is_all_lower;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    int limit = n / 32 + 1;\n    __m128i* sieve = (__m128i*) _mm_malloc(limit * 32, 16);\n    _mm_setzero_si128(sieve);\n\n    // Mark non-primes\n    for(int i = 2; i*i < n; ++i){\n        if(!((unsigned int*)sieve)[i/32] & (1 << (i % 32))){\n            for(int j = i*i; j < n; j += i){\n                ((unsigned int*)sieve)[j/32] |= 1 << (j % 32);\n            }\n        }\n    }\n\n    // Collect primes\n    for(int i = 2; i < n; ++i){\n        if(!((unsigned int*)sieve)[i/32] & (1 << (i % 32))){\n            primes.push_back(i);\n        }\n    }\n\n    _mm_free(sieve);\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    const int limit = n;\n    int size = (limit + 7) / 8;\n    __m128i* sieve = (__m128i*) _mm_malloc(size * 8, 16);\n    _mm_setzero_si128(sieve);\n\n    // Mark non-primes\n    for (int i = 2; i * i < limit; ++i) {\n        if (!_mm_extract_epi8(_mm_load_si128((__m128i*)&sieve[i / 8]), i % 8)) {\n            for (int j = i * i; j < limit; j += i) {\n                int byteIndex = j / 8;\n                int bitIndex = j % 8;\n                __m128i mask = _mm_set1_epi8(1 << bitIndex);\n                __m128i val = _mm_or_si128(_mm_load_si128((__m128i*)&sieve[byteIndex]), mask);\n                _mm_store_si128((__m128i*)&sieve[byteIndex], val);\n            }\n        }\n    }\n\n    // Collect primes\n    for (int i = 2; i < limit; ++i) {\n        int byteIndex = i / 8;\n        int bitIndex = i % 8;\n        if (!(_mm_extract_epi8(_mm_load_si128((__m128i*)&sieve[byteIndex]), bitIndex))) {\n            primes.push_back(i);\n        }\n    }\n\n    _mm_free(sieve);\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n <= 1) return primes;\n\n    const int size = n;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    // Create a sieve array\n    std::vector<char> sieve(size, 1);\n    sieve[0] = sieve[1] = 0;\n\n    int limit = static_cast<int>(std::sqrt(size)) + 1;\n\n    for(int i = 2; i < limit; ++i){\n        if(sieve[i]){\n            // Mark multiples of i as non-prime\n            for(int j = i * i; j < size; j += i){\n                sieve[j] = 0;\n            }\n        }\n    }\n\n    // Collect primes using SIMD\n    for(int i = 0; i < size; i+=4){\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_loadu_si128((__m128i*)&sieve[i]);\n        __m128i is_prime = _mm_cmpeq_epi32(mask, one);\n        __m128i prime_indices = _mm_and_si128(is_prime, indices);\n\n        // Extract results\n        int res[4];\n        _mm_storeu_si128((__m128i*)res, prime_indices);\n\n        for(int j = 0; j < 4; ++j){\n            if(res[j] != 0 && res[j] < n){\n                primes.push_back(res[j]);\n            }\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 2) return primes;\n\n    int size = n / 32 + 1;\n    __m128i* sieve = (__m128i*)_mm_malloc(size * 32, 16);\n    _mm_store_si128((__m128i*)&sieve[0], _mm_setzero_si128());\n\n    // Mark non-primes\n    for(int i = 2; i*i < n; ++i){\n        if(!((sieve[i/32].m128i_u8[i%32] >> (i % 8)) & 1)){\n            for(int j = i * i; j < n; j += i){\n                sieve[j/32].m128i_u8[j%32] |= 1 << (j % 8);\n            }\n        }\n    }\n\n    // Collect primes\n    for(int i = 2; i < n; ++i){\n        if(!((sieve[i/32].m128i_u8[i%32] >> (i % 8)) & 1)){\n            primes.push_back(i);\n        }\n    }\n\n    _mm_free(sieve);\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n <= 1) return primes;\n\n    const int size = n;\n    __declspec(align(16)) char sieve[size];\n    for(int i=0;i<size;i++) sieve[i] = 1;\n    sieve[0] = sieve[1] = 0;\n\n    int limit = static_cast<int>(std::sqrt(size)) + 1;\n\n    for(int p=2; p<limit; p++){\n        if(sieve[p]){\n            int multiple = p*p;\n            if(multiple >= size) break;\n            for(int j=multiple; j<size; j+=p){\n                sieve[j] = 0;\n            }\n        }\n    }\n\n    __m128i one = _mm_set1_epi8(1);\n    for(int i=0;i<size;i+=16){\n        __m128i chunk = _mm_load_si128((__m128i*)&sieve[i]);\n        __m128i cmp = _mm_cmpeq_epi8(chunk, one);\n        int mask = _mm_movemask_epi8(cmp);\n        for(int j=0;j<16 && (i+j)<size;j++) {\n            if(mask & (1<<j)) primes.push_back(i+j);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    if (s.empty()) return 0;\n\n    const char* str = s.c_str();\n    int count = 0;\n    __m128i mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'U', 'O', 'I', 'E');\n    __m128i cmp_mask = _mm_cmpeq_epi8(_mm_set1_epi8('A'), _mm_set1_epi8('A'));\n\n    for (size_t i = 0; i < s.length(); i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i even_mask = _mm_set1_epi8(0x0F);\n        __m128i even_indices = _mm_and_si128(_mm_set_epi8(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0), even_mask);\n        __m128i filtered_data = _mm_and_si128(data, even_indices);\n\n        __m128i res = _mm_cmpeq_epi8(filtered_data, mask);\n        res = _mm_and_si128(res, cmp_mask);\n\n        __m128i ones = _mm_set1_epi8(1);\n        __m128i sum = _mm_sad_epu8(res, _mm_setzero_si128());\n        count += _mm_extract_epi16(sum, 0);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s) {\n    int count = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m128i mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'U', 'A'\n    ); // Uppercase vowels in reverse order for little-endian\n\n    for (size_t i = 0; i < len; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i even_mask = _mm_set1_epi8(0x0F); // Mask to check even indices\n        __m128i idx_mask = _mm_setr_epi8(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15);\n        __m128i and_mask = _mm_and_si128(idx_mask, even_mask);\n        __m128i cmp_res = _mm_cmpeq_epi8(and_mask, _mm_setzero_si128());\n        __m128i vowels = _mm_and_si128(data, cmp_res);\n        __m128i res = _mm_cmpestrm(vowels, 2, mask, 2, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_PRECISION | _SIDD_NEGATIVE_POLARITY);\n        int temp_count = _mm_popcnt_u32(_mm_movemask_epi8(res));\n        count += temp_count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    int count = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m128i vowels = _mm_set_epi8('O', 'I', 'E', 'A', 'U', 'O', 'I', 'E', 'A', 'U', 0, 0, 0, 0, 0, 0);\n    __m128i uppercase_mask = _mm_set1_epi8(0x7F); // To check if character is uppercase\n\n    for (size_t i = 0; i < len; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i even_indices = _mm_and_si128(data, _mm_set1_epi8(0x7F)); // Mask to keep only even indices\n        __m128i cmp_result = _mm_cmpestrm(even_indices, 8, vowels, 10, _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK | _SIDD_NEGATIVE_POLARITY);\n        __m128i bit_count = _mm_popcnt_u32(_mm_movemask_epi8(cmp_result));\n        count += _mm_extract_epi16(bit_count, 0);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m128i vowels = _mm_set_epi8('O','I','E','A','o','i','e','a',0,0,0,0,0,0,0,0);\n    __m128i uppercase_mask = _mm_set1_epi8(0x40); // 'A'-'Z' have bit 6 set\n\n    for(size_t i = 0; i < len; i += 16){\n        if(i % 2 != 0) continue; // Ensure even index alignment\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i is_upper = _mm_and_si128(_mm_cmpgt_epi8(chunk, _mm_set1_epi8('A'-1)), _mm_cmplt_epi8(chunk, _mm_set1_epi8('Z'+1)));\n        __m128i is_vowel = _mm_cmpestrm(vowels, 8, chunk, 16, _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK | _SIDD_NEGATIVE_POLARITY);\n        __m128i upper_vowels = _mm_and_si128(is_upper, is_vowel);\n        __m128i mask = _mm_setr_epi8(1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0);\n        __m128i even_indices = _mm_and_si128(upper_vowels, mask);\n        int temp_count = _mm_popcnt_u32(_mm_movemask_epi8(even_indices));\n        count += temp_count;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nint count_upper_simd(const std::string &s) {\n    int count = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    __m128i vowels = _mm_set_epi8('O', 'I', 'U', 'E', 'A', 'o', 'i', 'u', 'e', 'a', -1, -1, -1, -1, -1, -1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_even_indices = _mm_setr_epi8(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n    for (size_t i = 0; i + 15 < len; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i even_data = _mm_and_si128(data, mask_even_indices);\n        __m128i cmp = _mm_cmpestrm(even_data, 16, vowels, 10, _SIDD_UBYTE_OPS | _SIDD_CMP_EQUAL_ANY | _SIDD_FULL_PRECSIMD);\n        __m128i ones = _mm_cmpgt_epi8(zero, cmp); // Count non-zero bytes\n        __m128i sum = _mm_sad_epu8(ones, zero);   // Sum of all bits in the result\n        count += _mm_extract_epi16(sum, 0) + _mm_extract_epi16(sum, 4) + _mm_extract_epi16(sum, 2) + _mm_extract_epi16(sum, 6);\n    }\n\n    for (size_t i = (len / 16) * 16; i < len; ++i) {\n        if (i % 2 == 0 && strchr(\"AEIOU\", str[i])) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_zero = _mm_setzero_si128();\n\n    int remainder = n % 4;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4) {\n        __m128i v_idx = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i v_val = _mm_add_epi32(_mm_mullo_epi32(v_two, v_idx), v_n);\n        _mm_storeu_si128((__m128i*)&result[i], v_val);\n    }\n\n    for (; i < n; ++i) {\n        result[i] = n + 2 * i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_mod = _mm_and_si128(v_n, _mm_set1_epi32(1));\n    __m128i v_add = _mm_cmpeq_epi32(v_mod, _mm_set1_epi32(1)); // Check if n is odd\n    v_add = _mm_and_si128(v_add, _mm_set1_epi32(1)); // Set to 1 if odd, else 0\n\n    __m128i current = _mm_add_epi32(v_n, v_add);\n\n    for(int i = 0; i < n; i += 4){\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_cmplt_epi32(indices, _mm_set1_epi32(n));\n        mask = _mm_and_si128(mask, _mm_set1_epi32(-1));\n\n        __m128i values = _mm_add_epi32(_mm_mullo_epi32(indices, v_two), v_n);\n        values = _mm_and_si128(values, mask);\n\n        _mm_storeu_si128((__m128i*)&result[i], values);\n        current = _mm_add_epi32(current, _mm_set1_epi32(2));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_zero = _mm_setzero_si128();\n\n    int remainder = n % 4;\n    int i = 0;\n\n    for (; i <= n - 4; i += 4){\n        __m128i v_idx = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i v_val = _mm_add_epi32(_mm_mullo_epi32(v_two, v_idx), v_n);\n        _mm_storeu_si128((__m128i*)&result[i], v_val);\n    }\n\n    for (; i < n; ++i){\n        result[i] = n + 2 * i;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m128i v_n = _mm_set_epi32(0, 0, 0, n);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_inc = _mm_and_si128(_mm_set1_epi32(1), v_n); // Determine increment (1 if odd, 2 if even)\n    v_inc = _mm_or_si128(v_inc, v_two); // Increment is either 1 or 2\n\n    __m128i current = v_n;\n    for(int i = 0; i < n; i += 4){\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_cmplt_epi32(indices, _mm_set1_epi32(n));\n        mask = _mm_andnot_si128(_mm_cmpgt_epi32(indices, _mm_set1_epi32(n-4)), mask);\n\n        _mm_storeu_si128((__m128i*)&result[i], _mm_and_si128(mask, current));\n\n        current = _mm_add_epi32(current, _mm_shuffle_epi32(v_inc, _MM_SHUFFLE(0,0,0,0)));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    if (n <= 0) return result;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_inc = _mm_set_epi32(6, 4, 2, 0);\n\n    int remainder = n % 4;\n    int aligned_n = n - remainder;\n\n    for(int i=0; i<aligned_n; i+=4){\n        __m128i v_i = _mm_add_epi32(v_inc, _mm_set1_epi32(i));\n        __m128i v_cond = _mm_and_si128(_mm_cmpeq_epi32(_mm_and_si128(v_n, _mm_set1_epi32(1)), v_zero), v_two);\n        __m128i v_result = _mm_add_epi32(v_n, _mm_add_epi32(v_i, v_cond));\n        _mm_storeu_si128((__m128i*)&result[i], v_result);\n    }\n\n    for(int i=aligned_n; i<n; ++i){\n        int inc = (i != 0 && n % 2 == 0) ? 2 : 1;\n        result[i] = n + i * inc;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n\n    __m128i v_sum = _mm_sub_epi64(_mm_add_epi64(v_m, v_n), _mm_set1_epi64x(1));\n    __m128i v_count = _mm_sub_epi64(v_m, _mm_add_epi64(v_n, _mm_set1_epi64x(-1)));\n\n    __m128i v_avg = _mm_sra_epi64(_mm_add_epi64(v_sum, _mm_srli_epi64(v_count, 1)), _mm_set1_epi64x(1));\n\n    int64_t avg;\n    _mm_store_si128((__m128i*)&avg, v_avg);\n\n    std::ostringstream oss;\n    if (avg == 0) {\n        oss << \"0\";\n    } else {\n        int64_t num = avg;\n        if (num < 0) {\n            oss << \"-\";\n            num = -num;\n        }\n        std::string result;\n        while (num > 0) {\n            result = char('0' + (num & 1)) + result;\n            num >>= 1;\n        }\n        oss << result;\n    }\n    return oss.str();\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n\n    __m128i v_sum = _mm_add_epi64(v_n, v_m);\n    __m128i v_count = _mm_sub_epi64(v_m, v_n);\n    v_count = _mm_add_epi64(v_count, _mm_set1_epi64x(1));\n\n    __m128i v_avg = _mm_sra_epi64(_mm_add_epi64(v_sum, _mm_srli_epi64(v_count, 1)), _mm_set1_epi64x(1));\n    v_avg = _mm_div_epi64(v_avg, v_count); // This is a placeholder as there's no direct div instruction\n\n    int64_t avg;\n    _mm_store_si128((__m128i*)&avg, v_avg);\n\n    std::ostringstream oss;\n    if (avg == 0) {\n        oss << \"0\";\n    } else {\n        int64_t num = avg;\n        while (num > 0) {\n            oss << ((num & 1) ? '1' : '0');\n            num >>= 1;\n        }\n    }\n    std::string result = oss.str();\n    std::reverse(result.begin(), result.end());\n    return result;\n}\n``` \n\n**Note**: `_mm_div_epi64` does not exist in SSE/SSE2. You would need to implement the division using scalar operations or approximate it with shifts and multiplies depending on specific use cases. The above code assumes its existence for illustrative purposes."}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m){\n        return \"-1\";\n    }\n    \n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vm = _mm_set1_epi64x(m);\n    \n    __m128i vsum = _mm_add_epi64(_mm_mullo_epi32(vn, _mm_set1_epi64x(2)), vm);\n    __m128i vcount = _mm_sub_epi64(vm, vn);\n    vcount = _mm_add_epi64(vcount, _mm_set1_epi64x(1));\n    \n    __m128i vavg = _mm_sra_epi64(_mm_div_epi64(vsum, vcount), _mm_set1_epi64x(1));\n    \n    int64_t avg;\n    _mm_store_si128((__m128i*)&avg, vavg);\n    \n    std::ostringstream oss;\n    if (avg < 0){\n        oss << \"-1\";\n    }\n    else{\n        int remainder = avg % 2;\n        avg /= 2;\n        if (remainder || (avg & 1)){\n            avg += 1;\n        }\n        oss << std::hex << avg;\n    }\n    return oss.str();\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if (n > m) {\n        return \"-1\";\n    }\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_m = _mm_set1_epi64x(m);\n\n    __m128i v_sum = _mm_add_epi64(_mm_mullo_epi32(v_n, _mm_set1_epi64x(2)), _mm_mullo_epi32(v_m, _mm_set1_epi64x(2)));\n    __m128i v_count = _mm_sub_epi64(_mm_add_epi64(v_m, _mm_set1_epi64x(1)), v_n);\n    __m128i v_avg = _mm_sra_epi64(_mm_add_epi64(v_sum, _mm_slli_epi64(v_count, 1)), _mm_set1_epi64x(2));\n\n    int64_t avg = _mm_cvtsi128_si64(v_avg);\n\n    std::string binary;\n    uint64_t mask = 1ULL << 63;\n    for (uint64_t i = mask; i != 0; i >>= 1) {\n        binary += (avg & i) ? '1' : '0';\n    }\n\n    // Remove leading zeros\n    size_t start = binary.find_first_not_of('0');\n    if (start != std::string::npos) {\n        binary = binary.substr(start);\n    } else {\n        binary = \"0\";\n    }\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <sstream>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    \n    __m128i vn = _mm_set1_epi64x(n);\n    __m128i vm = _mm_set1_epi64x(m);\n    \n    __m128i vsum = _mm_sub_epi64(vm, vn);\n    __m128i vone = _mm_set1_epi64x(1);\n    __m128i vcount = _mm_add_epi64(vsum, vone);\n    __m128i vtotal = _mm_mullo_epi32(vsum, vone);\n    __m128i vavg = _mm_srai_epi32(_mm_add_epi32(vtotal, _mm_srli_epi64(vcount, 1)), 1);\n    \n    int64_t avg;\n    _mm_store_si128((__m128i*)&avg, vavg);\n    \n    std::ostringstream oss;\n    if(avg >=0 ){\n        oss << std::hex << (avg >> 3) ;\n    }\n    else{\n        return \"-1\";\n    }\n    \n    return oss.str();\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    const int MOD = 10000;\n    __m128i mod_vec = _mm_set1_epi32(MOD);\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 2 == 0){\n            // Compute factorial modulo 10000\n            int fact = 1;\n            for(int j = 1; j <= i; ++j){\n                fact = (fact * j) % MOD;\n            }\n            result[i-1] = fact;\n        }\n        else{\n            // Compute sum from 1 to i\n            int sum = (i * (i + 1)) / 2;\n            sum %= MOD;\n            result[i-1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    const int MOD = 10000;\n    __m128i mod_vec = _mm_set1_epi32(MOD);\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 2 == 0){\n            int factorial = 1;\n            for(int j = 1; j <= i; ++j){\n                factorial = (factorial * j) % MOD;\n            }\n            result[i-1] = factorial;\n        }\n        else{\n            int sum = (i * (i + 1)) / 2;\n            result[i-1] = sum;\n        }\n    }\n\n    // SIMD optimization for even indices\n    int aligned_size = (n / 4) * 4;\n    for(int i = 1; i <= aligned_size; i += 4){\n        __m128i idx_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i even_mask = _mm_and_si128(_mm_cmpeq_epi32(_mm_and_si128(idx_vec, _mm_set1_epi32(1)), _mm_setzero_si128()), _mm_set1_epi32(1));\n        \n        __m128i factorial_vec = _mm_set_epi32(1, 1, 1, 1);\n        __m128i sum_vec = _mm_set_epi32(0, 0, 0, 0);\n\n        for(int j = 1; j <= i+3; ++j){\n            __m128i j_vec = _mm_set1_epi32(j);\n            factorial_vec = _mm_mullo_epi32(factorial_vec, j_vec);\n            factorial_vec = _mm_and_si128(factorial_vec, _mm_cmplt_epi32(factorial_vec, mod_vec));\n            sum_vec = _mm_add_epi32(sum_vec, j_vec);\n        }\n\n        __m128i res_vec = _mm_or_si128(_mm_and_si128(even_mask, factorial_vec), _mm_andnot_si128(even_mask, sum_vec));\n        _mm_storeu_si128((__m128i*)&result[i-1], res_vec);\n    }\n\n    // Handle remaining elements\n    for(int i = aligned_size + 1; i <= n; ++i){\n        if(i % 2 == 0){\n            int factorial = 1;\n            for(int j = 1; j <= i; ++j){\n                factorial = (factorial * j) % MOD;\n            }\n            result[i-1] = factorial;\n        }\n        else{\n            int sum = (i * (i + 1)) / 2;\n            result[i-1] = sum;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    int factorial_mod = 1;\n    int sum = 0;\n    __m128i mod = _mm_set1_epi32(10000);\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 2 == 0){\n            factorial_mod = (factorial_mod * i) % 10000;\n            result[i-1] = factorial_mod;\n        }\n        else{\n            sum += i;\n            result[i-1] = sum;\n        }\n    }\n\n    // SIMD optimization for even indices factorial modulo\n    for(int i = 2; i < n; i += 4){\n        __m128i idx = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i fact = _mm_set_epi32(1, 1, 1, 1);\n        __m128i two = _mm_set1_epi32(2);\n\n        for(int j = 1; j <= i+3; ++j){\n            __m128i cmp = _mm_cmplt_epi32(_mm_set1_epi32(j), idx);\n            __m128i mask = _mm_and_si128(cmp, _mm_set1_epi32(j));\n            fact = _mm_mullo_epi32(fact, _mm_or_si128(mask, two));\n            fact = _mm_and_si128(fact, _mm_set1_epi32(9999));\n        }\n\n        fact = _mm_mullo_epi32(fact, _mm_set1_epi32(i%2 == 0 ? i : 1));\n        fact = _mm_and_si128(fact, mod);\n\n        _mm_storeu_si128((__m128i*)&result[i-1], fact);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 1);\n    if (n <= 0) return result;\n\n    const int MOD = 10000;\n    int factorial = 1;\n    int sum = 0;\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 2 == 0){\n            factorial = (factorial * i) % MOD;\n            result[i-1] = factorial;\n        }\n        else{\n            sum += i;\n            result[i-1] = sum;\n        }\n    }\n\n    // SIMD optimization for even indices (factorial modulo)\n    __m128i v_mod = _mm_set1_epi32(MOD);\n    for(int i = 2; i < n; i += 4){\n        __m128i v_idx = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i v_fact = _mm_set_epi32(1, 1, 1, 1);\n        __m128i v_res = _mm_set_epi32(1, 1, 1, 1);\n\n        for(int j = 1; j <= i+3; ++j){\n            __m128i v_j = _mm_set1_epi32(j);\n            v_fact = _mm_mullo_epi32(v_fact, v_j);\n            v_fact = _mm_and_si128(v_fact, _mm_cmplt_epi32(v_fact, v_mod)); // Avoid overflow\n            v_res = _mm_add_epi32(v_res, v_fact);\n        }\n\n        v_res = _mm_mullo_epi32(v_res, _mm_set_epi32(1, 1, 1, 1));\n        v_res = _mm_and_si128(v_res, _mm_cmplt_epi32(v_res, v_mod));\n\n        int res_arr[4];\n        _mm_storeu_si128((__m128i*)res_arr, v_res);\n\n        for(int k = 0; k < 4; ++k){\n            if((i+k) % 2 == 0 && i+k < n){\n                result[i+k-1] = res_arr[k] % MOD;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n, 0);\n    if (n <= 0) return result;\n\n    const int MOD = 10000;\n    int factorial = 1;\n    int sum = 0;\n\n    for(int i = 1; i <= n; ++i){\n        if(i % 2 == 0){\n            factorial = (factorial * i) % MOD;\n            result[i-1] = factorial;\n        }\n        else{\n            sum += i;\n            result[i-1] = sum;\n        }\n    }\n\n    // SIMD optimization for even indices (factorial modulo)\n    __m128i mod = _mm_set1_epi32(MOD);\n    for(int i = 2; i < n; i += 4){\n        __m128i vec_i = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i vec_fact = _mm_set_epi32(1, 1, 1, 1);\n\n        for(int j = 1; j <= i+3; ++j){\n            __m128i vec_j = _mm_set1_epi32(j);\n            vec_fact = _mm_mullo_epi32(vec_fact, vec_j);\n            vec_fact = _mm_and_si128(vec_fact, mod); // Modulo operation\n        }\n\n        _mm_storeu_si128((__m128i*)&result[i-1], vec_fact);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i cmp_mask = _mm_set1_epi32(1);\n    __m128i num = _mm_set_epi32(0, 0, 0, 0);\n    __m128i rev_num = _mm_set_epi32(0, 0, 0, 0);\n    __m128i ones = _mm_set1_epi32(1);\n\n    for(int i=1; i<=n; i+=4){\n        __m128i current = _mm_set_epi32(i+3 > n ? 0 : i+3, i+2 > n ? 0 : i+2, i+1 > n ? 0 : i+1, i);\n        num = current;\n\n        // Reverse digits (simplified for single-digit and two-digit numbers)\n        rev_num = _mm_shuffle_epi8(num, _mm_set_epi8(0xF, 0xF, 0xF, 0xF, 0xF, 0xF, 0xF, 0xF, 0xF, 0xF, 3, 2, 1, 0, 0xF, 0xF));\n\n        // Compare original and reversed numbers\n        __m128i eq = _mm_cmpeq_epi32(num, rev_num);\n\n        // Check if palindrome is even or odd\n        __m128i and_mask = _mm_and_si128(eq, current);\n        __m128i even_check = _mm_and_si128(and_mask, cmp_mask);\n        __m128i odd_check = _mm_andnot_si128(cmp_mask, and_mask);\n\n        // Count even and odd palindromes\n        even += _mm_popcnt_u32(_mm_movemask_epi8(_mm_cmpgt_epi32(even_check, zero)));\n        odd += _mm_popcnt_u32(_mm_movemask_epi8(_mm_cmpgt_epi32(odd_check, zero)));\n    }\n\n    return std::vector<int>{even, odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i mask = _mm_set1_epi32(0x0F);\n\n    for(int i=1; i<=n; i+=4){\n        __m128i v = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i rev_v = _mm_shuffle_epi8(v, _mm_set_epi8(15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0));\n        __m128i cmp_res = _mm_cmpeq_epi32(v, rev_v);\n        __m128i and_res = _mm_and_si128(cmp_res, v);\n        __m128i parity_check = _mm_and_si128(and_res, two);\n        __m128i is_even = _mm_cmpeq_epi32(parity_check, two);\n        __m128i is_odd = _mm_cmpeq_epi32(parity_check, zero);\n\n        int e = _mm_movemask_epi8(is_even) & 0xF;\n        int o = _mm_movemask_epi8(is_odd) & 0xF;\n\n        even += __builtin_popcount(e);\n        odd += __builtin_popcount(o);\n    }\n\n    return {even, odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i mask = _mm_set1_epi32(0x0F);\n\n    for(int i=1;i<=n;i+=4){\n        __m128i v = _mm_set_epi32(i+3,i+2,i+1,i);\n        __m128i rev_v = _mm_shuffle_epi8(v, mask);\n        __m128i cmp = _mm_cmpeq_epi32(v, rev_v);\n        __m128i sel = _mm_and_si128(cmp, one);\n        __m128i even_mask = _mm_and_si128(sel, _mm_srli_epi32(v, 1));\n        __m128i odd_mask = _mm_andnot_si128(even_mask, sel);\n        even += _mm_movemask_epi8(_mm_cmpeq_epi32(even_mask, one)) / 0xFF;\n        odd += _mm_movemask_epi8(_mm_cmpeq_epi32(odd_mask, one)) / 0xFF;\n    }\n    return std::vector<int>{even, odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    __m128i zero = _mm_setzero_si128();\n    __m128i count_even = _mm_setzero_si128();\n    __m128i count_odd = _mm_setzero_si128();\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for(int i=1;i<=n;i+=4){\n        __m128i v = _mm_set_epi32(i+3,i+2,i+1,i);\n        __m128i mask = _mm_cmplt_epi32(v, _mm_set1_epi32(n+1));\n        __m128i rev = _mm_shuffle_epi8(v, _mm_set_epi8(0xF,0xE,0xD,0xC,0xB,0xA,9,8,7,6,5,4,3,2,1,0));\n        __m128i cmp = _mm_cmpeq_epi32(v, rev);\n        __m128i is_palindrome = _mm_and_si128(mask, cmp);\n        __m128i even_mask = _mm_and_si128(is_palindrome, _mm_cmpestrm(v, two, 0x10));\n        __m128i odd_mask = _mm_andnot_si128(even_mask, is_palindrome);\n        count_even = _mm_add_epi32(count_even, _mm_countbits_epi128(even_mask));\n        count_odd = _mm_add_epi32(count_odd, _mm_countbits_epi128(odd_mask));\n    }\n\n    int total_even = 0;\n    int total_odd = 0;\n\n    __m128i sum_even = _mm_hadd_epi32(count_even, count_even);\n    sum_even = _mm_hadd_epi32(sum_even, sum_even);\n    _mm_storeu_si128((__m128i*)&total_even, sum_even);\n\n    __m128i sum_odd = _mm_hadd_epi32(count_odd, count_odd);\n    sum_odd = _mm_hadd_epi32(sum_odd, sum_odd);\n    _mm_storeu_si128((__m128i*)&total_odd, sum_odd);\n\n    return std::vector<int>{total_even, total_odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    int even = 0, odd = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set1_epi32(0xF);\n    __m128i cmp_mask = _mm_set1_epi32(1);\n\n    for(int i=1;i<=n;i+=4){\n        __m128i v = _mm_set_epi32(0,0,0,i);\n        __m128i rev_v = _mm_shuffle_epi8(v, mask);\n        __m128i eq = _mm_cmpeq_epi32(v, rev_v);\n        __m128i parity = _mm_and_si128(v, cmp_mask);\n        \n        int mask_cmp = _mm_movemask_epi8(eq);\n        if(mask_cmp){\n            int p = _mm_cvtsi128_si32(parity);\n            if((p & 1) == 0){\n                even++;\n            }\n            else{\n                odd++;\n            }\n        }\n    }\n\n    std::vector<int> result = {even, odd};\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    const int chunk_size = 4;\n    int count = 0;\n    int len = static_cast<int>(n.size());\n    int aligned_len = len & ~(chunk_size - 1);\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i sign_mask = _mm_set1_epi32(0x80808080);\n    __m128i abs_mask = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i ones = _mm_set1_epi32(1);\n\n    for(int i = 0; i < aligned_len; i += chunk_size){\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_vec = _mm_and_si128(vec, abs_mask);\n        __m128i signs = _mm_xor_si128(_mm_cmplt_epi32(vec, zero), sign_mask);\n        __m128i digits = _mm_add_epi32(abs_vec, signs);\n\n        __m128i sum_digits = _mm_setzero_si128();\n        sum_digits = _mm_add_epi32(sum_digits, _mm_shuffle_epi32(digits, _MM_SHUFFLE(0,0,0,0)));\n        sum_digits = _mm_add_epi32(sum_digits, _mm_shuffle_epi32(digits, _MM_SHUFFLE(1,1,1,1)));\n        sum_digits = _mm_add_epi32(sum_digits, _mm_shuffle_epi32(digits, _MM_SHUFFLE(2,2,2,2)));\n        sum_digits = _mm_add_epi32(sum_digits, _mm_shuffle_epi32(digits, _MM_SHUFFLE(3,3,3,3)));\n\n        __m128i cmp = _mm_cmpgt_epi32(sum_digits, zero);\n        __m128i mask = _mm_movemask_epi8(cmp);\n        count += _mm_popcnt_u32(mask);\n    }\n\n    for(int i = aligned_len; i < len; ++i){\n        int num = n[i];\n        int sum = 0;\n        if(num < 0){\n            sum -= (num / 1000) % 10;\n            sum += (num / 100) % 10;\n            sum += (num / 10) % 10;\n            sum += num % 10;\n        }\n        else{\n            sum += (num / 1000) % 10;\n            sum += (num / 100) % 10;\n            sum += (num / 10) % 10;\n            sum += num % 10;\n        }\n        if(sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if(n.empty()) return 0;\n\n    const int chunk_size = 4;\n    int count = 0;\n    int len = n.size();\n    int aligned_len = len - (len % chunk_size);\n\n    for(int i=0; i<aligned_len; i+=chunk_size){\n        __m128i v = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i sign_mask = _mm_set1_epi32(0x80000000);\n        __m128i abs_v = _mm_sub_epi32(_mm_xor_si128(v, sign_mask), sign_mask);\n\n        __m128i tens = _mm_set1_epi32(10);\n        __m128i digits_sum = _mm_setzero_si128();\n\n        __m128i temp = abs_v;\n        for(int j=0; j<10; ++j){\n            __m128i remainder = _mm_cvtepi32_epi64(_mm_and_si128(_mm_sub_epi32(temp, _mm_setzero_si128()), _mm_cmplt_epi32(temp, tens)));\n            digits_sum = _mm_add_epi32(digits_sum, remainder);\n            temp = _mm_srli_epi32(temp, 4);\n        }\n\n        __m128i signed_digits = _mm_sub_epi32(digits_sum, _mm_and_si128(_mm_cmpgt_epi32(_mm_setzero_si128(), v), _mm_set1_epi32(1)));\n\n        __m128i cmp_result = _mm_cmpgt_epi32(signed_digits, _mm_setzero_si128());\n        int mask = _mm_movemask_epi8(cmp_result) & 0xFFFF;\n        count += __builtin_popcount(mask);\n    }\n\n    for(int i=aligned_len; i<len; ++i){\n        int sum = 0;\n        int num = n[i];\n        if(num < 0){\n            sum -= (num / abs(num));\n            num = abs(num);\n        }\n        while(num > 0){\n            sum += num % 10;\n            num /= 10;\n        }\n        if(sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    if (n.empty()) return 0;\n\n    const int chunk_size = 4;\n    int count = 0;\n    int size = static_cast<int>(n.size());\n    int aligned_size = size - (size % chunk_size);\n\n    for(int i = 0; i < aligned_size; i += chunk_size){\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i sign_mask = _mm_set1_epi32(0x80000000);\n        __m128i abs_vec = _mm_sub_epi32(_mm_xor_si128(vec, sign_mask), sign_mask);\n\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i tens = _mm_set1_epi32(10);\n        __m128i hundreds = _mm_set1_epi32(100);\n        __m128i thousands = _mm_set1_epi32(1000);\n\n        __m128i div_tens = _mm_srli_epi32(abs_vec, 0);\n        __m128i mod_tens = _mm_and_si128(div_tens, _mm_set1_epi32(0xF));\n        __m128i div_hundreds = _mm_srli_epi32(abs_vec, 4);\n        __m128i mod_hundreds = _mm_and_si128(div_hundreds, _mm_set1_epi32(0xF));\n        __m128i div_thousands = _mm_srli_epi32(abs_vec, 8);\n        __m128i mod_thousands = _mm_and_si128(div_thousands, _mm_set1_epi32(0xF));\n\n        __m128i sum_digits = _mm_add_epi32(mod_tens, mod_hundreds);\n        sum_digits = _mm_add_epi32(sum_digits, mod_thousands);\n\n        __m128i sign_bits = _mm_srai_epi32(vec, 31);\n        __m128i first_digit = _mm_and_si128(_mm_srli_epi32(abs_vec, 12), _mm_set1_epi32(0xF));\n        first_digit = _mm_sub_epi32(first_digit, sign_bits);\n\n        sum_digits = _mm_add_epi32(sum_digits, first_digit);\n\n        __m128i cmp_result = _mm_cmpgt_epi32(sum_digits, _mm_setzero_si128());\n        count += _mm_popcnt_u32(_mm_movemask_epi8(cmp_result));\n    }\n\n    for(int i = aligned_size; i < size; ++i){\n        int num = n[i];\n        int sum = 0;\n        bool is_negative = false;\n        if(num < 0){\n            is_negative = true;\n            num = -num;\n        }\n        while(num > 0){\n            sum += num % 10;\n            num /= 10;\n        }\n        if(is_negative && std::abs(n[i]) >= 10){\n            sum -= 2 * (n[i] / std::pow(10, floor(log10(std::abs(n[i])))) % 10);\n        }\n        if(sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    const int chunk_size = 4;\n    int count = 0;\n    int len = n.size();\n    int aligned_len = len & ~(chunk_size - 1);\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i sign_mask = _mm_set1_epi32(0x80808080);\n    __m128i abs_mask = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i ten = _mm_set1_epi32(10);\n\n    for(int i = 0; i < aligned_len; i += chunk_size){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&n[i]));\n        __m128i abs_vec = _mm_and_si128(vec, abs_mask);\n        __m128i signs = _mm_srai_epi32(vec, 31);\n        __m128i digits = _mm_cvtepi32_epi8(abs_vec);\n        __m128i signed_digits = _mm_or_si128(digits, signs);\n\n        __m128i sum = _mm_setzero_si128();\n        for(int j = 0; j < 4; ++j){\n            __m128i rem = _mm_srl_epi32(abs_vec, _mm_set_epi32(0, 0, 0, j*8));\n            __m128i digit = _mm_and_si128(rem, ten);\n            __m128i signed_digit = _mm_or_si128(digit, _mm_slli_epi32(signs, j));\n            sum = _mm_add_epi32(sum, signed_digit);\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(sum, zero);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(cmp));\n    }\n\n    for(int i = aligned_len; i < len; ++i){\n        int num = n[i];\n        int sum = 0;\n        bool negative = num < 0;\n        if(negative) num = -num;\n        while(num > 0){\n            int digit = num % 10;\n            if(negative && num >= 10){\n                digit = -digit;\n            }\n            sum += digit;\n            num /= 10;\n        }\n        if(sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    if (n.empty()) return 0;\n\n    int count = 0;\n    const int size = n.size();\n    const int aligned_size = size - (size % 4);\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i tens = _mm_set1_epi32(10);\n    __m128i hundreds = _mm_set1_epi32(100);\n    __m128i thousands = _mm_set1_epi32(1000);\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n\n    for(int i = 0; i < aligned_size; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_vec = _mm_andnot_si128(sign_mask, vec);\n        __m128i signs = _mm_sign_epi32(vec, ones);\n\n        // Extract digits\n        __m128i div1 = _mm_srai_epi32(vec, 31); // Sign bit\n        __m128i mod1 = _mm_and_si128(abs_vec, _mm_set1_epi32(0xFFFFF));\n        __m128i digit1 = _mm_cvtepi32_epi8(_mm_div_epu32(mod1, thousands));\n\n        __m128i mod2 = _mm_sub_epi32(abs_vec, _mm_mullo_epi32(digit1, thousands));\n        __m128i digit2 = _mm_cvtepi32_epi8(_mm_div_epu32(mod2, hundreds));\n\n        __m128i mod3 = _mm_sub_epi32(abs_vec, _mm_mullo_epi32(digit2, hundreds));\n        __m128i digit3 = _mm_cvtepi32_epi8(_mm_div_epu32(mod3, tens));\n\n        __m128i mod4 = _mm_sub_epi32(abs_vec, _mm_mullo_epi32(digit3, tens));\n        __m128i digit4 = _mm_cvtepi32_epi8(mod4);\n\n        // Apply signs to first digits\n        digit1 = _mm_mullo_epi32(digit1, signs);\n\n        // Sum of digits\n        __m128i sum = _mm_add_epi32(digit1, digit2);\n        sum = _mm_add_epi32(sum, digit3);\n        sum = _mm_add_epi32(sum, digit4);\n\n        // Compare with zero\n        __m128i mask = _mm_cmpgt_epi32(sum, zero);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n\n    for(int i = aligned_size; i < size; ++i){\n        int num = n[i];\n        int sum = 0;\n        if(num < 0){\n            sum -= (num / 1000) % 10;\n            sum += (num / 100) % 10;\n            sum += (num / 10) % 10;\n            sum += num % 10;\n        }\n        else{\n            sum += (num / 1000) % 10;\n            sum += (num / 100) % 10;\n            sum += (num / 10) % 10;\n            sum += num % 10;\n        }\n        if(sum > 0) count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Find the index where the array breaks non-decreasing order\n    int break_point = -1;\n    for(int i = 0; i < N; ++i){\n        if(arr[i] > arr[(i+1)%N]){\n            break_point = i;\n            break;\n        }\n    }\n\n    if(break_point == -1) return true; // Already sorted\n\n    // Check if rotating at break_point results in a sorted array\n    __m128i prev = _mm_set1_epi32(arr[(break_point + 1) % N]);\n    int count = 0;\n\n    for(int i = 0; i < N; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i const*)(arr.data() + ((break_point + 1 + i) % N)));\n        __m128i cmp = _mm_cmplt_epi32(current, prev);\n        int mask = _mm_movemask_epi8(cmp);\n        if(mask != 0) return false;\n        prev = current;\n        count += 4;\n        if(count >= N) break;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Find the index where the array breaks non-decreasing order\n    int break_count = 0;\n    int break_index = -1;\n\n    for(int i = 0; i < N; ++i){\n        int current = arr[i];\n        int next = arr[(i + 1) % N];\n\n        __m128i vcurrent = _mm_set1_epi32(current);\n        __m128i vnext = _mm_set1_epi32(next);\n\n        __m128i cmp_result = _mm_cmpgt_epi32(vcurrent, vnext); // current > next\n\n        if(_mm_movemask_epi8(cmp_result) != 0){\n            break_count++;\n            break_index = i;\n            if(break_count > 1){\n                return false; // More than one break point, not possible to sort\n            }\n        }\n    }\n\n    return break_count <= 1;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Find the index where the array breaks the non-decreasing order\n    int break_point = -1;\n    for(int i = 0; i < N; ++i){\n        __m128i a = _mm_set_epi32(arr[(i+3)%N], arr[(i+2)%N], arr[(i+1)%N], arr[i]);\n        __m128i b = _mm_set_epi32(arr[(i+4)%N], arr[(i+3)%N], arr[(i+2)%N], arr[(i+1)%N]);\n        __m128i mask = _mm_cmplt_epi32(a, b);\n        int cmp = _mm_movemask_epi8(mask);\n        if((cmp & 0xF00F) != 0xF00F){\n            break_point = i;\n            break;\n        }\n    }\n\n    if(break_point == -1){\n        return std::is_sorted(arr.begin(), arr.end());\n    }\n\n    // Check if rotating at this point makes the array sorted\n    int first = arr[break_point];\n    int last = arr[N-1];\n    __m128i first_vec = _mm_set1_epi32(first);\n    __m128i last_vec = _mm_set1_epi32(last);\n\n    bool valid = true;\n    for(int i = break_point + 1; i < N; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp1 = _mm_cmplt_epi32(last_vec, current);\n        int mask1 = _mm_movemask_epi8(cmp1);\n        if((mask1 & 0xFFFF) != 0xFFFF){\n            valid = false;\n            break;\n        }\n    }\n\n    if(!valid) return false;\n\n    for(int i = 0; i < break_point; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp2 = _mm_cmplt_epi32(current, first_vec);\n        int mask2 = _mm_movemask_epi8(cmp2);\n        if((mask2 & 0xFFFF) != 0xFFFF){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int N = arr.size();\n    if (N == 0) return true;\n\n    // Find the number of decreasing points\n    int count = 0;\n    __m128i vcount = _mm_setzero_si128();\n\n    for(int i = 0; i < N; ++i){\n        int current = arr[i];\n        int next = arr[(i + 1) % N];\n        if(current > next){\n            count++;\n        }\n    }\n\n    // If there is more than one point where the sequence decreases, it's not possible\n    if(count > 1){\n        return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int N = arr.size();\n    if (N == 0) return true;\n\n    __m128i cmp_mask = _mm_set1_epi32(0); // To store comparison results\n    int drop_count = 0;\n\n    for(int i = 0; i < N; ++i){\n        int j = (i + 1) % N;\n        __m128i a = _mm_set1_epi32(arr[i]);\n        __m128i b = _mm_set1_epi32(arr[j]);\n        __m128i cmp = _mm_cmpgt_epi32(a, b); // Compare arr[i] > arr[j]\n        int mask = _mm_movemask_epi8(cmp);   // Get the comparison result as a bitmask\n        if(mask != 0){\n            drop_count++;\n            if(drop_count > 1){\n                return false;\n            }\n        }\n    }\n    return drop_count <= 1;\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    if (lst1.empty() || lst2.empty()) {\n        return \"NO\";\n    }\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i two = _mm_set1_epi32(2);\n\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    // Process lst1 to count odd numbers\n    for(size_t i = 0; i + 4 <= lst1.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)(&lst1[i]));\n        __m128i rem = _mm_and_si128(_mm_sub_epi32(vec, two), _mm_set1_epi32(1));\n        __m128i cmp = _mm_cmpeq_epi32(rem, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        odd_count_lst1 += __builtin_popcount(~mask & 0xFFFF);\n    }\n    for(size_t i = lst1.size() - (lst1.size() % 4); i < lst1.size(); ++i){\n        if(lst1[i] % 2 != 0){\n            odd_count_lst1 += 1;\n        }\n    }\n\n    // Process lst2 to count even numbers\n    for(size_t i = 0; i + 4 <= lst2.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)(&lst2[i]));\n        __m128i rem = _mm_and_si128(_mm_sub_epi32(vec, two), _mm_set1_epi32(1));\n        __m128i cmp = _mm_cmpeq_epi32(rem, zero);\n        int mask = _mm_movemask_epi8(cmp);\n        even_count_lst2 += __builtin_popcount(mask & 0xFFFF);\n    }\n    for(size_t i = lst2.size() - (lst2.size() % 4); i < lst2.size(); ++i){\n        if(lst2[i] % 2 == 0){\n            even_count_lst2 += 1;\n        }\n    }\n\n    if(even_count_lst2 >= odd_count_lst1){\n        return \"YES\";\n    }\n    else{\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cstdint>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2) {\n    int count_odd_lst1 = 0;\n    int count_even_lst2 = 0;\n\n    const int* ptr1 = lst1.data();\n    const int* ptr2 = lst2.data();\n    size_t size1 = lst1.size();\n    size_t size2 = lst2.size();\n\n    // Process lst1 in chunks of 4 using SSE\n    for (size_t i = 0; i + 4 <= size1; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&ptr1[i]));\n        __m128i mask = _mm_set1_epi32(1); // Mask for checking odd numbers (1)\n        __m128i and_result = _mm_and_si128(v, mask);\n        __m128i cmp_result = _mm_cmpeq_epi32(and_result, _mm_setzero_si128());\n        int num_even = _mm_movemask_epi8(cmp_result) & 0xF;\n        count_odd_lst1 += 4 - __builtin_popcount(num_even);\n    }\n    // Handle remaining elements in lst1\n    for (size_t i = (size1 / 4) * 4; i < size1; ++i) {\n        if (ptr1[i] % 2 != 0) count_odd_lst1++;\n    }\n\n    // Process lst2 in chunks of 4 using SSE\n    for (size_t i = 0; i + 4 <= size2; i += 4) {\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&ptr2[i]));\n        __m128i mask = _mm_set1_epi32(1); // Mask for checking even numbers (0)\n        __m128i and_result = _mm_and_si128(v, mask);\n        __m128i cmp_result = _mm_cmpeq_epi32(and_result, _mm_setzero_si128());\n        int num_even = _mm_movemask_epi8(cmp_result) & 0xF;\n        count_even_lst2 += __builtin_popcount(num_even);\n    }\n    // Handle remaining elements in lst2\n    for (size_t i = (size2 / 4) * 4; i < size2; ++i) {\n        if (ptr2[i] % 2 == 0) count_even_lst2++;\n    }\n\n    return (count_even_lst2 >= count_odd_lst1) ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n#include <vector>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int odd_count_lst1 = 0;\n    int even_count_lst2 = 0;\n\n    // Count odd numbers in lst1 using SIMD\n    for(size_t i = 0; i + 16 <= lst1.size(); i += 16){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst1[i]);\n        __m128i mask = _mm_set1_epi32(1);\n        __m128i and_res = _mm_and_si128(v, mask);\n        __m128i cmp_res = _mm_cmpeq_epi32(and_res, mask);\n        int count = _mm_movemask_epi8(cmp_res);\n        odd_count_lst1 += __builtin_popcount(count);\n    }\n    for(size_t i = lst1.size() - (lst1.size() % 16); i < lst1.size(); ++i){\n        if(lst1[i] % 2 != 0) odd_count_lst1++;\n    }\n\n    // Count even numbers in lst2 using SIMD\n    for(size_t i = 0; i + 16 <= lst2.size(); i += 16){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst2[i]);\n        __m128i mask = _mm_set1_epi32(1);\n        __m128i and_res = _mm_and_si128(v, mask);\n        __m128i cmp_res = _mm_cmpeq_epi32(and_res, _mm_setzero_si128());\n        int count = _mm_movemask_epi8(cmp_res);\n        even_count_lst2 += __builtin_popcount(count);\n    }\n    for(size_t i = lst2.size() - (lst2.size() % 16); i < lst2.size(); ++i){\n        if(lst2[i] % 2 == 0) even_count_lst2++;\n    }\n\n    if(even_count_lst2 >= odd_count_lst1){\n        return \"YES\";\n    } else {\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int count_odd_lst1 = 0;\n    int count_even_lst2 = 0;\n\n    // Count odd numbers in lst1 using SIMD\n    for(size_t i = 0; i + 3 < lst1.size(); i += 4){\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst1[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i and_res = _mm_and_si128(v, ones);\n        __m128i cmp_res = _mm_cmpeq_epi32(and_res, ones);\n        int mask = _mm_movemask_epi8(cmp_res);\n        count_odd_lst1 += __builtin_popcount(mask);\n    }\n    for(size_t i = lst1.size() - (lst1.size() % 4); i < lst1.size(); ++i){\n        if(lst1[i] % 2 != 0) count_odd_lst1++;\n    }\n\n    // Count even numbers in lst2 using SIMD\n    for(size_t i = 0; i + 3 < lst2.size(); i += 4){\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst2[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i and_res = _mm_and_si128(v, ones);\n        __m128i cmp_res = _mm_cmpeq_epi32(and_res, _mm_setzero_si128());\n        int mask = _mm_movemask_epi8(cmp_res);\n        count_even_lst2 += __builtin_popcount(mask);\n    }\n    for(size_t i = lst2.size() - (lst2.size() % 4); i < lst2.size(); ++i){\n        if(lst2[i] % 2 == 0) count_even_lst2++;\n    }\n\n    return count_even_lst2 >= count_odd_lst1 ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int count_odd_lst1 = 0;\n    int count_even_lst2 = 0;\n\n    // Count odd numbers in lst1 using SIMD\n    for(size_t i = 0; i + 4 <= lst1.size(); i += 4){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst1[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i mod = _mm_and_si128(vec, ones); // Get the last bit (mod 2)\n        __m128i cmp = _mm_cmpeq_epi32(mod, ones); // Compare with 1 (odd)\n        int mask = _mm_movemask_epi8(cmp); // Get bitmask of comparison\n        count_odd_lst1 += __builtin_popcount(mask); // Count number of set bits\n    }\n    for(size_t i = lst1.size() - (lst1.size() % 4); i < lst1.size(); ++i){\n        if(lst1[i] % 2 != 0) count_odd_lst1++;\n    }\n\n    // Count even numbers in lst2 using SIMD\n    for(size_t i = 0; i + 4 <= lst2.size(); i += 4){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst2[i]));\n        __m128i ones = _mm_set1_epi32(1);\n        __m128i mod = _mm_and_si128(vec, ones); // Get the last bit (mod 2)\n        __m128i cmp = _mm_cmpeq_epi32(mod, _mm_setzero_si128()); // Compare with 0 (even)\n        int mask = _mm_movemask_epi8(cmp); // Get bitmask of comparison\n        count_even_lst2 += __builtin_popcount(mask); // Count number of set bits\n    }\n    for(size_t i = lst2.size() - (lst2.size() % 4); i < lst2.size(); ++i){\n        if(lst2[i] % 2 == 0) count_even_lst2++;\n    }\n\n    if(count_odd_lst1 <= count_even_lst2){\n        return \"YES\";\n    }\n    else{\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    const char* format = \"the number of odd elements %dn the str%ding %d of the %dnput.\";\n    char buffer[256];\n    \n    for (const auto& s : lst) {\n        int count = 0;\n        int len = s.length();\n        \n        for(int i = 0; i + 15 < len; i += 16){\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n            __m128i ones = _mm_set1_epi8('1');\n            __m128i threes = _mm_set1_epi8('3');\n            __m128i fives = _mm_set1_epi8('5');\n            __m128i sevens = _mm_set1_epi8('7');\n            __m128i nines = _mm_set1_epi8('9');\n            \n            __m128i mask1 = _mm_cmpeq_epi8(data, ones);\n            __m128i mask2 = _mm_cmpeq_epi8(data, threes);\n            __m128i mask3 = _mm_cmpeq_epi8(data, fives);\n            __m128i mask4 = _mm_cmpeq_epi8(data, sevens);\n            __m128i mask5 = _mm_cmpeq_epi8(data, nines);\n            \n            __m128i mask_combined = _mm_or_si128(mask1, mask2);\n            mask_combined = _mm_or_si128(mask_combined, mask3);\n            mask_combined = _mm_or_si128(mask_combined, mask4);\n            mask_combined = _mm_or_si128(mask_combined, mask5);\n            \n            int temp_count = _mm_movemask_epi8(mask_combined);\n            count += __builtin_popcount(temp_count);\n        }\n        \n        for(int i = (len / 16) * 16; i < len; ++i){\n            if(s[i] == '1' || s[i] == '3' || s[i] == '5' || s[i] == '7' || s[i] == '9'){\n                count++;\n            }\n        }\n        \n        snprintf(buffer, sizeof(buffer), format, count, count, count, count);\n        result.push_back(std::string(buffer));\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    for (const auto& str : lst) {\n        int count = 0;\n        const unsigned char* data = reinterpret_cast<const unsigned char*>(str.data());\n        size_t len = str.size();\n        \n        // Process 16 bytes at a time\n        for (size_t i = 0; i + 15 <= len; i += 16) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n            __m128i ones = _mm_set1_epi8('1');\n            __m128i zeros = _mm_set1_epi8('0');\n            __m128i cmp = _mm_cmpeq_epi8(_mm_and_si128(vec, ones), ones);\n            __m128i mask = _mm_set1_epi8(0x01);\n            __m128i res = _mm_and_si128(cmp, mask);\n            int sum = _mm_cvtsi128_si32(_mm_sad_epu8(res, _mm_setzero_si128()));\n            count += sum;\n        }\n        \n        // Process remaining bytes\n        for (size_t i = (len / 16) * 16; i < len; ++i) {\n            if ((data[i] - '0') % 2 != 0) {\n                count++;\n            }\n        }\n        \n        // Construct the result string\n        std::string output = \"the number of odd elements \";\n        output += std::to_string(count);\n        output += \"n the str\";\n        output += std::to_string(count);\n        output += \"ng \";\n        output += std::to_string(count);\n        output += \" of the \";\n        output += std::to_string(count);\n        output += \"nput.\";\n        result.push_back(output);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    __m128i mask = _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, 1, 3, 5, 7, 9, -1, -1, -1);\n    for(const auto& s : lst){\n        int count = 0;\n        for(size_t i = 0; i + 15 < s.size(); i += 16){\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n            __m128i cmp = _mm_cmpeq_epi8(_mm_and_si128(data, mask), mask);\n            count += _mm_popcnt_u32(_mm_movemask_epi8(cmp));\n        }\n        for(size_t i = s.size() & ~size_t(15); i < s.size(); ++i){\n            if(s[i] == '1' || s[i] == '3' || s[i] == '5' || s[i] == '7' || s[i] == '9'){\n                count++;\n            }\n        }\n        result.push_back(\"the number of odd elements \" + std::to_string(count) + \n                         \"n the str\" + std::to_string(count) + \"ng \" + \n                         std::to_string(count) + \" of the \" + std::to_string(count) + \"nput.\");\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    const char* format = \"the number of odd elements %dn the str%ding %d of the %dnput.\";\n    char buffer[256];\n    \n    for (const auto& s : lst) {\n        int count = 0;\n        size_t len = s.size();\n        \n        // Handle unaligned data in chunks of 16 bytes\n        for (size_t i = 0; i + 15 < len; i += 16) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n            __m128i ones = _mm_set1_epi8('1');\n            __m128i threes = _mm_set1_epi8('3');\n            __m128i fives = _mm_set1_epi8('5');\n            __m128i sevens = _mm_set1_epi8('7');\n            __m128i nines = _mm_set1_epi8('9');\n            \n            __m128i mask_ones = _mm_cmpeq_epi8(data, ones);\n            __m128i mask_threes = _mm_cmpeq_epi8(data, threes);\n            __m128i mask_fives = _mm_cmpeq_epi8(data, fives);\n            __m128i mask_sevens = _mm_cmpeq_epi8(data, sevens);\n            __m128i mask_nines = _mm_cmpeq_epi8(data, nines);\n            \n            __m128i mask = _mm_or_si128(mask_ones, mask_threes);\n            mask = _mm_or_si128(mask, mask_fives);\n            mask = _mm_or_si128(mask, mask_sevens);\n            mask = _mm_or_si128(mask, mask_nines);\n            \n            count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n        }\n        \n        // Handle remaining characters\n        for (size_t i = len / 16 * 16; i < len; ++i) {\n            if (s[i] == '1' || s[i] == '3' || s[i] == '5' || s[i] == '7' || s[i] == '9') {\n                count++;\n            }\n        }\n        \n        // Construct the result string\n        snprintf(buffer, sizeof(buffer), format, count, count, count, count);\n        result.emplace_back(buffer);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n    \n    const char template_str[] = \"the number of odd elements Xn the strXng X of the Xnput.\";\n    const int template_len = sizeof(template_str) - 1;\n    \n    for (const auto& s : lst) {\n        int count = 0;\n        int len = s.length();\n        \n        // Handle strings in chunks of 16 bytes\n        for(int i = 0; i + 15 <= len; i += 16){\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n            __m128i ones = _mm_set1_epi8('1');\n            __m128i threes = _mm_set1_epi8('3');\n            __m128i fives = _mm_set1_epi8('5');\n            __m128i sevens = _mm_set1_epi8('7');\n            __m128i nines = _mm_set1_epi8('9');\n            \n            __m128i cmp1 = _mm_cmpeq_epi8(data, ones);\n            __m128i cmp2 = _mm_cmpeq_epi8(data, threes);\n            __m128i cmp3 = _mm_cmpeq_epi8(data, fives);\n            __m128i cmp4 = _mm_cmpeq_epi8(data, sevens);\n            __m128i cmp5 = _mm_cmpeq_epi8(data, nines);\n            \n            __m128i mask = _mm_or_si128(cmp1, cmp2);\n            mask = _mm_or_si128(mask, cmp3);\n            mask = _mm_or_si128(mask, cmp4);\n            mask = _mm_or_si128(mask, cmp5);\n            \n            int temp_count = _mm_popcnt_u32(_mm_movemask_epi8(mask));\n            count += temp_count;\n        }\n        \n        // Handle remaining characters\n        for(int i = (len / 16) * 16; i < len; ++i){\n            if(s[i] == '1' || s[i] == '3' || s[i] == '5' || s[i] == '7' || s[i] == '9'){\n                count++;\n            }\n        }\n        \n        // Construct result string\n        std::string res(template_str, template_len);\n        char count_char = '0' + count;\n        res[24] = count_char;\n        res[31] = count_char;\n        res[33] = count_char;\n        res[41] = count_char;\n        result.push_back(res);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int n = nums.size();\n    __m128i vmin = _mm_set1_epi64x(INT64_MAX);\n    __m128i current_min = _mm_loadu_si128((__m128i*)&nums[0]);\n    vmin = _mm_min_epi64(current_min, vmin);\n\n    for(int i = 1; i < n; ++i){\n        __m128i current = _mm_loadu_si128((__m128i*)&nums[i]);\n        current_min = _mm_min_epi64(current, _mm_add_epi64(current_min, current));\n        vmin = _mm_min_epi64(vmin, current_min);\n    }\n\n    alignas(16) int64_t result[2];\n    _mm_store_si128((__m128i*)result, vmin);\n\n    return std::min(result[0], result[1]);\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return INT64_MAX;\n\n    const int len = nums.size();\n    const int aligned_len = len - (len % 2);\n\n    __m128i min_curr = _mm_set_epi64x(nums[0], nums[0]);\n    __m128i min_global = min_curr;\n\n    for(int i = 1; i < aligned_len; i += 2){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&nums[i]));\n        \n        __m128i prev_min = _mm_srli_si128(min_curr, 8);\n        __m128i curr_min = _mm_min_epi64(_mm_add_epi64(vec, prev_min), vec);\n        \n        min_global = _mm_min_epi64(min_global, curr_min);\n        min_curr = curr_min;\n    }\n\n    int64_t min_val = INT64_MAX;\n    __m128i res = _mm_min_epi64(min_global, min_curr);\n    int64_t res_arr[2];\n    _mm_storeu_si128((__m128i*)res_arr, res);\n    min_val = std::min({min_val, res_arr[0], res_arr[1]});\n\n    for(int i = aligned_len; i < len; ++i){\n        int64_t curr = nums[i];\n        if(min_val > curr){\n            min_val = curr;\n        }\n        if(min_val > nums[i] + min_val){\n            min_val += nums[i];\n        }\n    }\n\n    return min_val;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <algorithm>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int simd_width = 2; // Number of elements processed per SIMD operation for __m128i\n    int64_t global_min = INT64_MAX;\n    int64_t current_min = 0;\n\n    size_t i = 0;\n    size_t n = nums.size();\n\n    // Handle the vectorized part\n    for (; i <= n - simd_width; i += simd_width) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i curr_min_vec = _mm_set_epi64x(current_min, current_min);\n\n        // Add current_min to each element in vec\n        vec = _mm_add_epi64(vec, curr_min_vec);\n\n        // Find minimum between current_min and each element in vec\n        __m128i new_curr_min_vec = _mm_min_epi64(vec, curr_min_vec);\n\n        // Update current_min with the minimum value from new_curr_min_vec\n        int64_t temp[2];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(temp), new_curr_min_vec);\n        current_min = std::min({temp[0], temp[1], current_min});\n\n        // Update global_min\n        global_min = std::min(global_min, current_min);\n    }\n\n    // Handle the remaining elements\n    for (; i < n; ++i) {\n        current_min = std::min((int64_t)nums[i], current_min + nums[i]);\n        global_min = std::min(global_min, current_min);\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n#include <climits>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int n = nums.size();\n    __m128i current_min = _mm_set_epi64x(nums[0], nums[0]);\n    __m128i global_min = _mm_set_epi64x(nums[0], nums[0]);\n\n    for(int i = 1; i < n; ++i){\n        __m128i val = _mm_set_epi64x(nums[i], nums[i]);\n        __m128i temp_sum = _mm_add_epi64(val, current_min);\n        current_min = _mm_min_epi64(temp_sum, val);\n        global_min = _mm_min_epi64(global_min, current_min);\n    }\n\n    int64_t result;\n    _mm_storeu_si128((__m128i*)&result, global_min);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <climits>\n#include <vector>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if (nums.empty()) return 0;\n\n    const int simd_width = 2; // Number of elements processed per SIMD operation for __m128i\n    int64_t min_sum = INT64_MAX;\n    int64_t current_min = 0;\n\n    size_t i = 0;\n    size_t n = nums.size();\n\n    // Handle the first element separately to initialize SIMD calculations\n    current_min = nums[0];\n    min_sum = nums[0];\n    i = 1;\n\n    // Process elements in batches of 2 using SIMD\n    for (; i <= n - simd_width; i += simd_width) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i curr_min_vec = _mm_set_epi64x(current_min, current_min);\n        \n        // Add current_min to each element in vec\n        vec = _mm_add_epi64(vec, curr_min_vec);\n\n        // Find minimum within the SIMD vector\n        __m128i min_vec = _mm_min_epi64(vec, curr_min_vec);\n\n        // Extract results and update current_min and min_sum\n        int64_t res0, res1;\n        _mm_storeu_si128((__m128i*)&res0, min_vec);\n        res1 = res0 + sizeof(int64_t);\n\n        current_min = std::min({current_min, res0, res1});\n        min_sum = std::min(min_sum, current_min);\n    }\n\n    // Process any remaining elements\n    for (; i < n; ++i) {\n        current_min = std::min((int64_t)nums[i], current_min + nums[i]);\n        min_sum = std::min(min_sum, current_min);\n    }\n\n    return min_sum;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int total_drops = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    for(const auto& row : grid){\n        int sum = 0;\n        size_t i = 0;\n        size_t len = row.size();\n        for(; i + 4 <= len; i += 4){\n            __m128i vec = _mm_loadu_si128((__m128i const*)(&row[i]));\n            __m128i mask = _mm_cmpeq_epi32(vec, ones);\n            int count = _mm_popcnt_u32(_mm_movemask_epi8(mask));\n            sum += count;\n        }\n        for(; i < len; ++i){\n            sum += row[i];\n        }\n        total_drops += (sum + capacity - 1) / capacity;\n    }\n    return total_drops;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int total_drops = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    for (const auto& row : grid) {\n        int sum = 0;\n        size_t i = 0;\n        for (; i + 3 < row.size(); i += 4) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[i]));\n            __m128i mask = _mm_cmpeq_epi32(vec, ones);\n            int count = _mm_popcnt_u32(_mm_movemask_epi8(mask));\n            sum += count;\n        }\n        for (; i < row.size(); ++i) {\n            sum += row[i];\n        }\n        total_drops += (sum + capacity - 1) / capacity;\n    }\n    return total_drops;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    if (grid.empty() || grid[0].empty()) return 0;\n\n    int rows = grid.size();\n    int cols = grid[0].size();\n    int total_water = 0;\n\n    const int simd_width = 16; // Process 16 bytes at a time using SSE2\n    __m128i ones = _mm_set1_epi8(-1); // All bits set to 1\n    __m128i zero = _mm_setzero_si128(); \n\n    for(int r = 0; r < rows; ++r){\n        int col = 0;\n        int row_sum = 0;\n\n        while(col <= cols - simd_width){\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&grid[r][col]));\n            __m128i cmp = _mm_cmpeq_epi8(data, ones);\n            int mask = _mm_movemask_epi8(cmp);\n            row_sum += __builtin_popcount(mask);\n            col += simd_width;\n        }\n\n        // Handle remaining elements\n        while(col < cols){\n            row_sum += grid[r][col];\n            col++;\n        }\n\n        total_water += row_sum;\n    }\n\n    return (total_water + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int totalDrops = 0;\n    const int simdWidth = 16; // Process 16 elements at a time using SSE2\n\n    for (const auto& row : grid) {\n        int sum = 0;\n        size_t i = 0;\n        size_t len = row.size();\n\n        // SIMD loop\n        for (; i + simdWidth <= len; i += simdWidth) {\n            __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[i]));\n            __m128i ones = _mm_set1_epi8(-1);\n            __m128i cmp = _mm_cmpeq_epi8(data, ones);\n            int mask = _mm_movemask_epi8(cmp);\n            sum += __builtin_popcount(mask);\n        }\n\n        // Tail handling\n        for (; i < len; ++i) {\n            sum += row[i];\n        }\n\n        if (capacity != 0) {\n            totalDrops += (sum + capacity - 1) / capacity;\n        }\n    }\n\n    return totalDrops;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int total_drops = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i cap = _mm_set1_epi32(capacity);\n\n    for (const auto& row : grid) {\n        int sum = 0;\n        for (size_t i = 0; i + 4 <= row.size(); i += 4) {\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&row[i]));\n            __m128i mask = _mm_cmpeq_epi32(vec, ones);\n            __m128i count = _mm_sub_epi32(_mm_setzero_si128(), mask);\n            int temp_sum = _mm_cvtsi128_si32(_mm_sad_epu8(count, _mm_setzero_si128())) / 4;\n            sum += temp_sum;\n        }\n        // Handle remaining elements\n        for (size_t i = row.size() - (row.size() % 4); i < row.size(); ++i) {\n            sum += row[i];\n        }\n        total_drops += (sum + capacity - 1) / capacity;\n    }\n    return total_drops;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    if(lst.size() == 0) return sum;\n\n    const int16_t* data = lst.data();\n    size_t size = lst.size();\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i mask_even_idx = _mm_set_epi8(0,-1,0,-1,0,-1,0,-1,0,-1,0,-1,0,-1,0,-1);\n\n    for(size_t i = 0; i + 7 < size; i += 8){\n        __m128i vec = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i idx_mask = _mm_and_si128(_mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i+0), mask_even_idx);\n        __m128i even_idx_vec = _mm_cmpeq_epi8(idx_mask, _mm_setzero_si128());\n        __m128i odd_check = _mm_and_si128(vec, mask_odd);\n        __m128i is_odd = _mm_cmpeq_epi16(odd_check, mask_odd);\n        __m128i filtered = _mm_and_si128(is_odd, even_idx_vec);\n        __m128i selected = _mm_and_si128(filtered, vec);\n        __m128i sums = _mm_sad_epu8(selected, zero);\n        uint32_t temp_sum = _mm_cvtsi128_si32(sums);\n        sum += (temp_sum & 0xFFFF) + ((temp_sum >> 16) & 0xFFFF);\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        if(i % 2 == 0 && (lst[i] & 1)){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t size = lst.size();\n    const int16_t* data = lst.data();\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i sum_vec = _mm_setzero_si128();\n\n    for (size_t i = 0; i < size; i += 8) {\n        if (i + 7 >= size || (i % 2) != 0) continue;\n\n        __m128i vec = _mm_loadu_si128((__m128i*)&data[i]);\n        __m128i and_mask = _mm_and_si128(vec, mask_odd);\n        __m128i cmp_result = _mm_cmpeq_epi16(and_mask, mask_odd);\n\n        __m128i even_pos_mask = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n        __m128i combined_mask = _mm_and_si128(cmp_result, even_pos_mask);\n\n        __m128i selected = _mm_and_si128(vec, combined_mask);\n        sum_vec = _mm_add_epi16(sum_vec, selected);\n    }\n\n    __m128i sum_high = _mm_unpackhi_epi64(sum_vec, zero);\n    __m128i total_sum = _mm_add_epi16(sum_vec, sum_high);\n\n    int16_t temp[8];\n    _mm_storeu_si128((__m128i*)temp, total_sum);\n\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    if(lst.size() == 0) return sum;\n\n    const int16_t* data = lst.data();\n    size_t size = lst.size();\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i ones = _mm_set1_epi16(1);\n    __m128i two = _mm_set1_epi16(2);\n    __m128i mask_odd = _mm_set1_epi16(1);\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&data[i]);\n        __m128i indices = _mm_set_epi16(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i even_mask = _mm_andnot_si128(_mm_rem_epu16(indices, two), ones);\n        __m128i odd_check = _mm_and_si128(vec, mask_odd);\n        __m128i is_odd = _mm_cmpeq_epi16(odd_check, mask_odd);\n        __m128i valid = _mm_and_si128(even_mask, is_odd);\n        __m128i selected = _mm_and_si128(valid, vec);\n        __m128i sums = _mm_sad_epu8(selected, zero);\n        uint64_t temp_sum = _mm_cvtsi128_si64(sums);\n        sum += (temp_sum & 0xFFFF) + ((temp_sum >> 16) & 0xFFFF);\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        if(i % 2 == 0 && (lst[i] & 1)){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return sum;\n\n    alignas(16) int16_t temp[8];\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i mask_even_idx = _mm_set_epi8(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n    __m128i vsum = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 7 < size; i += 8){\n        // Load 8 elements\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        // Create a mask for even indices\n        __m128i even_mask = _mm_and_si128(_mm_cmpeq_epi8(vec, vec), mask_even_idx);\n\n        // Check if the element is odd\n        __m128i is_odd = _mm_and_si128(_mm_cmpgt_epi16(vec, zero), mask_odd);\n\n        // Combine masks to get odd elements at even positions\n        __m128i combined_mask = _mm_and_si128(even_mask, is_odd);\n\n        // Select elements that satisfy the condition\n        __m128i selected = _mm_and_si128(vec, combined_mask);\n\n        // Add to running sum\n        vsum = _mm_add_epi16(vsum, selected);\n    }\n\n    // Horizontal sum of vsum\n    _mm_store_si128(reinterpret_cast<__m128i*>(temp), vsum);\n    for(int i = 0; i < 8; ++i){\n        sum += temp[i];\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size / 8) * 8; i < size; ++i){\n        if ((i % 2 == 0) && (lst[i] & 1)){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t size = lst.size();\n    if(size == 0) return 0;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set1_epi16(1);\n    __m128i mask_even_index = _mm_set_epi8(0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1);\n\n    for(size_t i = 0; i + 8 <= size; i += 8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i even_mask = _mm_and_si128(indices, mask_even_index);\n\n        __m128i is_even = _mm_cmpeq_epi8(even_mask, zero);\n        __m128i odd_check = _mm_and_si128(vec, mask_odd);\n        __m128i is_odd = _mm_cmpeq_epi16(odd_check, mask_odd);\n\n        __m128i valid = _mm_and_si128(is_even, is_odd);\n        __m128i selected = _mm_and_si128(valid, vec);\n\n        __m128i sums = _mm_sad_epu8(selected, zero);\n        uint64_t temp_sum = _mm_cvtsi128_si64(sums);\n        sum += temp_sum;\n    }\n\n    for(size_t i = (size / 8) * 8; i < size; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = 100;\n    int count = k / 4;\n    int remainder = k % 4;\n\n    __m128i vlimit = _mm_set1_epi32(limit);\n    __m128i vzero = _mm_setzero_si128();\n    __m128i vsum = _mm_setzero_si128();\n\n    for(int i = 0; i < count * 4; i += 4){\n        __m128i vdata = _mm_loadu_si128((__m128i const*)(&arr[i]));\n        __m128i vcond = _mm_cmpgt_epi32(vlimit, _mm_abs_epi32(vdata));\n        __m128i vresult = _mm_and_si128(vdata, vcond);\n        vsum = _mm_add_epi32(vsum, vresult);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for(int i = count * 4; i < count * 4 + remainder; ++i){\n        if(arr[i] >= -100 && arr[i] <= 100){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int count = 0;\n    const int threshold = 99;\n\n    __m128i v_threshold = _mm_set1_epi32(threshold);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_sum = _mm_setzero_si128();\n\n    for(int i = 0; i < k; i += 4){\n        __m128i v_data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i v_cmp = _mm_cmple_epi32(v_data, v_threshold);\n        __m128i v_masked = _mm_and_si128(v_data, v_cmp);\n        v_sum = _mm_add_epi32(v_sum, v_masked);\n    }\n\n    __m128i v_high = _mm_shuffle_epi32(v_sum, _MM_SHUFFLE(2, 3, 0, 1));\n    v_sum = _mm_add_epi32(v_sum, v_high);\n    __m128i v_low = _mm_shuffle_epi32(v_sum, _MM_SHUFFLE(0, 0, 0, 0));\n    v_sum = _mm_add_epi32(v_sum, v_low);\n\n    sum = _mm_cvtsi128_si32(v_sum);\n\n    // Handle remaining elements if any\n    for(int i = (k / 4) * 4; i < k; ++i){\n        if(arr[i] <= threshold){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    if(k <= 0) return sum;\n\n    __m128i vsum = _mm_setzero_si128();\n    __m128i lower_bound = _mm_set1_epi32(-99);\n    __m128i upper_bound = _mm_set1_epi32(99);\n\n    int i = 0;\n    for(; i + 3 < k; i += 4){\n        __m128i vdata = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i in_range_mask = _mm_and_si128(\n            _mm_cmpgt_epi32(vdata, lower_bound),\n            _mm_cmplt_epi32(vdata, upper_bound)\n        );\n        __m128i masked_data = _mm_and_si128(vdata, in_range_mask);\n        vsum = _mm_add_epi32(vsum, masked_data);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, vsum);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for(; i < k; ++i){\n        if(arr[i] > -100 && arr[i] < 100){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = k - (k % 4);\n    int mask_val = 99;\n\n    __m128i mask = _mm_set1_epi32(mask_val);\n    __m128i zero = _mm_setzero_si128();\n    __m128i total = _mm_setzero_si128();\n\n    for(int i = 0; i < limit; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp_res = _mm_cmpgt_epi32(vec, mask);\n        __m128i neg_cmp_res = _mm_xor_si128(cmp_res, _mm_cmpeq_epi32(zero, zero));\n        __m128i filtered = _mm_and_si128(vec, neg_cmp_res);\n        total = _mm_add_epi32(total, filtered);\n    }\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, total);\n    sum += temp[0] + temp[1] + temp[2] + temp[3];\n\n    for(int i = limit; i < k; ++i){\n        if(arr[i] <= mask_val){\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = 100; // Two digit limit\n\n    __m128i vlimit = _mm_set1_epi32(limit);\n    __m128i vzero = _mm_setzero_si128();\n    __m128i vsum = _mm_setzero_si128();\n\n    int i = 0;\n    for (; i + 3 < k; i += 4) {\n        __m128i vdata = _mm_loadu_si128((__m128i const*)(&arr[i]));\n        __m128i vabs_data = _mm_and_si128(vdata, _mm_set1_epi32(0x7FFFFFFF)); // Absolute value\n        __m128i cmp_result = _mm_cmpgt_epi32(vabs_data, vlimit); // Compare abs(data) > 100\n        __m128i mask = _mm_cmpeq_epi32(cmp_result, vzero); // Invert comparison (abs(data) <= 100)\n        __m128i filtered = _mm_and_si128(mask, vdata); // Zero out elements > 100\n        vsum = _mm_add_epi32(vsum, filtered); // Accumulate\n    }\n\n    // Horizontal sum of vsum\n    __m128i vsum_high = _mm_shuffle_epi32(vsum, _MM_SHUFFLE(3, 2, 3, 2));\n    vsum = _mm_add_epi32(vsum, vsum_high);\n    __m128i vsum_low = _mm_shuffle_epi32(vsum, _MM_SHUFFLE(1, 0, 1, 0));\n    vsum = _mm_add_epi32(vsum, vsum_low);\n    sum += _mm_cvtsi128_si32(vsum);\n\n    // Process remaining elements\n    for (; i < k; ++i) {\n        if (arr[i] >= -99 && arr[i] <= 99) {\n            sum += arr[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdint>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    size_t n = lst.size();\n    const int *data = lst.data();\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i prev = _mm_loadu_si128((__m128i const*)&data[0]);\n    __m128i count = _mm_set1_epi32(1);\n\n    size_t i = 4;\n    for (; i + 4 <= n; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i const*)&data[i]);\n        __m128i cmp = _mm_cmplt_epi32(prev, current);\n        __m128i eq = _mm_cmpeq_epi32(prev, current);\n\n        // Check if any element is not less than the previous\n        if (_mm_movemask_epi8(cmp) != 0xFFFF) {\n            return false;\n        }\n\n        // Count duplicates\n        __m128i new_count = _mm_add_epi32(count, eq);\n        if (_mm_movemask_epi8(_mm_cmpgt_epi32(new_count, _mm_set1_epi32(2))) != 0) {\n            return false;\n        }\n        count = new_count;\n\n        prev = current;\n    }\n\n    // Scalar check for remaining elements\n    for (; i < n; ++i) {\n        if (data[i] < data[i - 1] || (data[i] == data[i - 1] && __builtin_popcount(data[i]) > 2)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    size_t size = lst.size();\n    const int *data = lst.data();\n\n    __m128i prev = _mm_set_epi32(data[3], data[2], data[1], data[0]);\n    __m128i curr;\n    __m128i cmp_res;\n    __m128i dup_check;\n\n    for(size_t i = 4; i + 3 < size; i += 4){\n        curr = _mm_loadu_si128((__m128i*)(data + i));\n        cmp_res = _mm_cmplt_epi32(prev, curr);\n        dup_check = _mm_cmpeq_epi32(prev, curr);\n\n        if(_mm_movemask_epi8(cmp_res) != 0xFFFF || _mm_movemask_epi8(dup_check) > 0){\n            // Check individual elements in case of potential duplicates or unsorted\n            for(size_t j = i - 4; j < i + 4 && j + 1 < size; ++j){\n                if(data[j] >= data[j + 1]){\n                    return false;\n                }\n                if(j + 2 < size && data[j] == data[j + 1] && data[j + 1] == data[j + 2]){\n                    return false;\n                }\n            }\n        }\n        prev = curr;\n    }\n\n    // Check remaining elements\n    for(size_t i = ((size - 1) / 4) * 4; i + 1 < size; ++i){\n        if(data[i] >= data[i + 1]){\n            return false;\n        }\n        if(i + 2 < size && data[i] == data[i + 1] && data[i + 1] == data[i + 2]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    size_t size = lst.size();\n    const int *data = lst.data();\n    __m128i zero = _mm_setzero_si128();\n    __m128i prev = _mm_loadu_si128((__m128i const*)&data[0]);\n    __m128i count = _mm_set1_epi32(1);\n\n    for(size_t i = 4; i + 4 <= size; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i const*)&data[i]);\n        __m128i cmp = _mm_cmplt_epi32(prev, current);\n        __m128i eq = _mm_cmpeq_epi32(prev, current);\n        count = _mm_add_epi32(_mm_and_si128(eq, count), _mm_andnot_si128(eq, _mm_set1_epi32(1)));\n        __m128i mask = _mm_or_si128(_mm_cmpeq_epi32(cmp, zero), _mm_cmpgt_epi32(count, _mm_set1_epi32(1)));\n        if(_mm_movemask_epi8(mask) != 0) return false;\n        prev = current;\n    }\n\n    for(size_t i = ((size / 4) * 4); i < size -1; ++i){\n        if(lst[i] > lst[i+1]) return false;\n        if(lst[i] == lst[i+1]){\n            int cnt = 1;\n            for(size_t j = i; j < size -1; ++j){\n                if(lst[j] == lst[j+1]) cnt++;\n                else break;\n            }\n            if(cnt > 2) return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    size_t n = lst.size();\n    __m128i prev_vec, curr_vec, cmp_res, dup_check;\n    __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i + 4 <= n; i += 4) {\n        curr_vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        if (i == 0) {\n            prev_vec = curr_vec;\n            continue;\n        }\n\n        // Compare current vector with previous vector\n        cmp_res = _mm_cmplt_epi32(prev_vec, curr_vec);\n        if (_mm_movemask_epi8(cmp_res) != 0xFFFF) {\n            return false;\n        }\n\n        // Check for duplicates\n        dup_check = _mm_cmpeq_epi32(prev_vec, curr_vec);\n        if (_mm_movemask_epi8(dup_check) != 0x0000) {\n            // If duplicates exist, ensure they are consecutive and no more than two\n            __m128i diff = _mm_sub_epi32(curr_vec, prev_vec);\n            __m128i is_one = _mm_cmpeq_epi32(diff, one);\n            if (_mm_movemask_epi8(is_one) != _mm_movemask_epi8(dup_check)) {\n                return false;\n            }\n        }\n\n        prev_vec = curr_vec;\n    }\n\n    // Handle remaining elements\n    for (size_t i = ((n / 4) * 4); i < n - 1; ++i) {\n        if (lst[i] >= lst[i + 1]) return false;\n        if (lst[i] == lst[i + 1]) {\n            if (i > 0 && lst[i] == lst[i - 1]) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if (lst.size() <= 1) return true;\n\n    size_t size = lst.size();\n    const int *data = lst.data();\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i prev = _mm_loadu_si128((__m128i*)data);\n    __m128i count = _mm_set1_epi32(1);\n\n    size_t i = 0;\n    for (i = 0; i + 4 <= size; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i cmp = _mm_cmpeq_epi32(current, prev);\n        __m128i mask = _mm_and_si128(cmp, _mm_cmpgt_epi32(count, zero));\n        if (_mm_movemask_epi8(mask) != 0) return false;\n\n        __m128i diff = _mm_sub_epi32(current, prev);\n        __m128i check = _mm_cmplt_epi32(diff, zero);\n        if (_mm_movemask_epi8(check) != 0) return false;\n\n        prev = current;\n        count = _mm_add_epi32(count, _mm_set1_epi32(1));\n    }\n\n    for (; i < size - 1; ++i){\n        if (data[i] > data[i+1]) return false;\n        if (data[i] == data[i+1]){\n            int cnt = 0;\n            for (size_t j = i; j < size && data[j] == data[i]; ++j) ++cnt;\n            if (cnt > 2) return false;\n            i += cnt - 1;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i product_sign = _mm_set1_epi32(1);\n\n    int n = arr.size();\n    int i = 0;\n\n    for (; i + 3 < n; i += 4) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i signs = _mm_sign_epi32(data, _mm_set1_epi32(1));\n        __m128i abs_data = _mm_andnot_si128(_mm_cmpgt_epi32(_mm_setzero_si128(), data), data);\n        sum = _mm_add_epi32(sum, abs_data);\n        product_sign = _mm_mullo_epi32(product_sign, signs);\n    }\n\n    int total_sum = 0;\n    int total_product_sign = 1;\n\n    int intermediate[4];\n    _mm_storeu_si128((__m128i*)intermediate, sum);\n    for (int j = 0; j < 4; ++j) {\n        total_sum += intermediate[j];\n    }\n\n    _mm_storeu_si128((__m128i*)intermediate, product_sign);\n    for (int j = 0; j < 4; ++j) {\n        total_product_sign *= intermediate[j];\n    }\n\n    for (; i < n; ++i) {\n        int val = arr[i];\n        if (val == 0) {\n            total_product_sign = 0;\n        } else {\n            total_product_sign *= (val > 0) ? 1 : -1;\n            total_sum += std::abs(val);\n        }\n    }\n\n    return total_sum * total_product_sign;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstddef>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i neg_one = _mm_set1_epi32(-1);\n\n    int sum_magnitude = 0;\n    int product_sign = 1;\n\n    size_t i = 0;\n    size_t n = arr.size() / 4 * 4;\n\n    for (; i < n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&arr[i]));\n        __m128i cmp_zero = _mm_cmpeq_epi32(vec, zero);\n        __m128i signs = _mm_sign_epi32(one, vec);\n        signs = _mm_or_si128(signs, _mm_and_si128(_mm_cmplt_epi32(vec, zero), neg_one));\n        signs = _mm_andnot_si128(cmp_zero, signs);\n\n        __m128i abs_vec = _mm_sub_epi32(_mm_xor_si128(vec, sign_mask), sign_mask);\n        __m128i sum_abs = _mm_add_epi32(abs_vec, _mm_shuffle_epi32(abs_vec, 1));\n        sum_abs = _mm_add_epi32(sum_abs, _mm_shuffle_epi32(sum_abs, 2));\n\n        int tmp_sum[4];\n        _mm_storeu_si128((__m128i*)tmp_sum, sum_abs);\n        sum_magnitude += tmp_sum[0];\n\n        int tmp_signs[4];\n        _mm_storeu_si128((__m128i*)tmp_signs, signs);\n        product_sign *= tmp_signs[0] * tmp_signs[1] * tmp_signs[2] * tmp_signs[3];\n    }\n\n    for (; i < arr.size(); ++i) {\n        if (arr[i] == 0) {\n            product_sign = 0;\n            break;\n        }\n        sum_magnitude += std::abs(arr[i]);\n        product_sign *= (arr[i] > 0) - (arr[i] < 0);\n    }\n\n    return sum_magnitude * product_sign;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cstdlib>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    int total_sum = 0;\n    int product_of_signs = 1;\n\n    const int *data = arr.data();\n    size_t size = arr.size();\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i minus_one = _mm_set1_epi32(-1);\n    __m128i plus_one = _mm_set1_epi32(1);\n\n    for(size_t i = 0; i + 4 <= size; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i *)(data + i));\n        __m128i sign_vec = _mm_sign_epi32(_mm_set1_epi32(1), vec);\n        __m128i abs_vec = _mm_andnot_si128(_mm_cmpgt_epi32(zero, vec), vec);\n\n        // Calculate product of signs\n        __m128i cmp_zero = _mm_cmpeq_epi32(vec, zero);\n        __m128i mask = _mm_movemask_epi8(cmp_zero);\n        if(mask != 0){\n            product_of_signs = 0;\n            break;\n        }\n\n        __m128i prod_sign = _mm_cvtepi32_epi64(_mm_shuffle_epi32(sign_vec, _MM_SHUFFLE(0, 0, 0, 0)));\n        product_of_signs *= _mm_extract_epi32(sign_vec, 0) *\n                           _mm_extract_epi32(sign_vec, 1) *\n                           _mm_extract_epi32(sign_vec, 2) *\n                           _mm_extract_epi32(sign_vec, 3);\n\n        // Sum of magnitudes\n        __m128i sum_abs = _mm_add_epi32(abs_vec, _mm_srli_si128(abs_vec, 8));\n        sum_abs = _mm_add_epi32(sum_abs, _mm_srli_si128(sum_abs, 4));\n        total_sum += _mm_extract_epi32(sum_abs, 0);\n    }\n\n    for(size_t i = ((size / 4) * 4); i < size; ++i){\n        if(data[i] == 0){\n            product_of_signs = 0;\n            break;\n        }\n        total_sum += std::abs(data[i]);\n        product_of_signs *= (data[i] > 0) ? 1 : -1;\n    }\n\n    return product_of_signs * total_sum;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i neg_one = _mm_set1_epi32(-1);\n\n    int product_sign = 1;\n    int sum_magnitude = 0;\n\n    size_t i = 0;\n    size_t len = arr.size() / 4 * 4;\n\n    for (; i < len; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&arr[i]));\n        __m128i cmp = _mm_cmpeq_epi32(vec, zero);\n        __m128i signs = _mm_and_si128(vec, sign_mask);\n        signs = _mm_cmplt_epi32(signs, zero);\n        signs = _mm_blendv_epi8(one, neg_one, signs);\n\n        __m128i mask = _mm_movemask_epi8(cmp);\n        if (mask != 0xFFFFFFFF) {\n            product_sign *= _mm_extract_epi32(signs, 0) * !_mm_extract_epi32(cmp, 0);\n            product_sign *= _mm_extract_epi32(signs, 1) * !_mm_extract_epi32(cmp, 4);\n            product_sign *= _mm_extract_epi32(signs, 2) * !_mm_extract_epi32(cmp, 8);\n            product_sign *= _mm_extract_epi32(signs, 3) * !_mm_extract_epi32(cmp, 12);\n        } else {\n            return 0;\n        }\n\n        __m128i abs_vals = _mm_andnot_si128(sign_mask, vec);\n        sum_magnitude += _mm_extract_epi32(abs_vals, 0);\n        sum_magnitude += _mm_extract_epi32(abs_vals, 1);\n        sum_magnitude += _mm_extract_epi32(abs_vals, 2);\n        sum_magnitude += _mm_extract_epi32(abs_vals, 3);\n    }\n\n    for (; i < arr.size(); ++i) {\n        if (arr[i] == 0) return 0;\n        product_sign *= (arr[i] > 0) ? 1 : -1;\n        sum_magnitude += std::abs(arr[i]);\n    }\n\n    return product_sign * sum_magnitude;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <algorithm>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if (arr.empty()) return -32768;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i sign_mask = _mm_set1_epi32(0x80000000);\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i neg_ones = _mm_set1_epi32(-1);\n\n    __m128i total_sum = _mm_setzero_si128();\n    __m128i product_sign = _mm_set1_epi32(1);\n\n    for(size_t i = 0; i + 4 <= arr.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_andnot_si128(sign_mask, vec);\n        __m128i cmp_zero = _mm_cmpeq_epi32(vec, zero);\n        __m128i signs = _mm_sign_epi32(ones, vec);\n        signs = _mm_or_si128(_mm_andnot_si128(cmp_zero, signs), _mm_and_si128(cmp_zero, zero));\n        product_sign = _mm_mullo_epi32(product_sign, signs);\n        total_sum = _mm_add_epi32(total_sum, abs_vec);\n    }\n\n    int sum[4];\n    _mm_storeu_si128((__m128i*)sum, total_sum);\n    int total = sum[0] + sum[1] + sum[2] + sum[3];\n\n    int prod[4];\n    _mm_storeu_si128((__m128i*)prod, product_sign);\n    int product = prod[0] * prod[1] * prod[2] * prod[3];\n\n    for(size_t i = arr.size() & ~3; i < arr.size(); ++i){\n        if(arr[i] == 0){\n            product = 0;\n        }\n        else{\n            product *= (arr[i] > 0) ? 1 : -1;\n            total += std::abs(arr[i]);\n        }\n    }\n\n    return product * total;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> >& grid, int k) {\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    int min_val = INT32_MAX;\n    int start_x = 0, start_y = 0;\n\n    // Find the minimum value in the grid using SIMD\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; j += 4) {\n            __m128i vec = _mm_set_epi32(\n                (j + 3 < N) ? grid[i][j + 3] : INT32_MAX,\n                (j + 2 < N) ? grid[i][j + 2] : INT32_MAX,\n                (j + 1 < N) ? grid[i][j + 1] : INT32_MAX,\n                grid[i][j]\n            );\n            __m128i curr_min = _mm_min_epi32(vec, _mm_shuffle_epi32(vec, _MM_SHUFFLE(1, 0, 3, 2)));\n            curr_min = _mm_min_epi32(curr_min, _mm_shuffle_epi32(curr_min, _MM_SHUFFLE(2, 3, 0, 1)));\n            int candidate_min = _mm_cvtsi128_si32(curr_min);\n            if (candidate_min < min_val) {\n                min_val = candidate_min;\n                start_x = i;\n                start_y = j + (min_val == grid[i][j] ? 0 :\n                              min_val == ((j + 1 < N) ? grid[i][j + 1] : INT32_MAX) ? 1 :\n                              min_val == ((j + 2 < N) ? grid[i][j + 2] : INT32_MAX) ? 2 : 3);\n            }\n        }\n    }\n\n    // Perform DFS to find the lexicographically smallest path\n    std::vector<std::vector<int>> paths;\n    std::vector<int> current_path;\n    current_path.push_back(grid[start_x][start_y]);\n\n    std::function<void(int, int, std::vector<int>&)> dfs = [&](int x, int y, std::vector<int>& path) {\n        if (path.size() == k) {\n            paths.push_back(path);\n            return;\n        }\n        int directions[4][2] = { {-1, 0}, {1, 0}, {0, -1}, {0, 1} };\n        for (auto& dir : directions) {\n            int nx = x + dir[0];\n            int ny = y + dir[1];\n            if (nx >= 0 && nx < N && ny >= 0 && ny < N) {\n                path.push_back(grid[nx][ny]);\n                dfs(nx, ny, path);\n                path.pop_back();\n            }\n        }\n    };\n\n    dfs(start_x, start_y, current_path);\n\n    // Find the lexicographically smallest path\n    result = *std::min_element(paths.begin(), paths.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> & grid, int k){\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    int min_val = INT_MAX;\n    int start_x = 0, start_y = 0;\n\n    // Find the minimum starting value using SIMD\n    for(int i = 0; i < N; ++i){\n        for(int j = 0; j < N; j += 4){\n            __m128i values = _mm_set_epi32(\n                (j+3 < N) ? grid[i][j+3] : INT_MAX,\n                (j+2 < N) ? grid[i][j+2] : INT_MAX,\n                (j+1 < N) ? grid[i][j+1] : INT_MAX,\n                grid[i][j]\n            );\n            __m128i mins = _mm_min_epi32(values, _mm_shuffle_epi32(values, _MM_SHUFFLE(1, 0, 3, 2)));\n            mins = _mm_min_epi32(mins, _mm_shuffle_epi32(mins, _MM_SHUFFLE(2, 3, 0, 1)));\n            int candidate_min = _mm_cvtsi128_si32(mins);\n            if(candidate_min < min_val){\n                min_val = candidate_min;\n                start_x = i;\n                start_y = j + (candidate_min == grid[i][j+0] ? 0 :\n                              candidate_min == grid[i][j+1] ? 1 :\n                              candidate_min == grid[i][j+2] ? 2 :\n                              candidate_min == grid[i][j+3] ? 3 : 0);\n            }\n        }\n    }\n\n    result.push_back(grid[start_x][start_y]);\n    int current_x = start_x;\n    int current_y = start_y;\n\n    for(int step = 1; step < k; ++step){\n        int next_val = INT_MAX;\n        int next_x = current_x;\n        int next_y = current_y;\n\n        // Define neighboring directions\n        int dx[4] = {-1, 0, 1, 0};\n        int dy[4] = {0, -1, 0, 1};\n\n        for(int dir = 0; dir < 4; ++dir){\n            int nx = current_x + dx[dir];\n            int ny = current_y + dy[dir];\n\n            if(nx >= 0 && nx < N && ny >= 0 && ny < N){\n                int val = grid[nx][ny];\n                if(val < next_val){\n                    next_val = val;\n                    next_x = nx;\n                    next_y = ny;\n                }\n            }\n        }\n\n        result.push_back(next_val);\n        current_x = next_x;\n        current_y = next_y;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>> &grid, int k) {\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    int min_val = INT_MAX;\n    int start_x = 0, start_y = 0;\n\n    // Find the minimum value in the grid using SIMD\n    for (int i = 0; i < N; ++i) {\n        for (int j = 0; j < N; j += 4) {\n            __m128i values = _mm_set_epi32(\n                (j + 3 < N) ? grid[i][j + 3] : INT_MAX,\n                (j + 2 < N) ? grid[i][j + 2] : INT_MAX,\n                (j + 1 < N) ? grid[i][j + 1] : INT_MAX,\n                grid[i][j]\n            );\n            __m128i current_min = _mm_min_epi32(values, _mm_shuffle_epi32(values, _MM_SHUFFLE(1, 0, 3, 2)));\n            current_min = _mm_min_epi32(current_min, _mm_shuffle_epi32(current_min, _MM_SHUFFLE(0, 3, 2, 1)));\n            int min_value = _mm_cvtsi128_si32(current_min);\n            if (min_value < min_val) {\n                min_val = min_value;\n                start_x = i;\n                start_y = j + (min_val == grid[i][j + 0] ? 0 : \n                              min_val == grid[i][j + 1] ? 1 : \n                              min_val == grid[i][j + 2] ? 2 : 3);\n            }\n        }\n    }\n\n    // Perform DFS to find the lexicographically smallest path\n    std::vector<std::vector<bool>> visited(N, std::vector<bool>(N, false));\n    std::vector<int> path;\n    std::vector<int> best_path;\n    int best_score = INT_MAX;\n\n    std::function<void(int, int, int)> dfs = [&](int x, int y, int steps) {\n        if (steps == k) {\n            if (path < best_path || best_path.empty()) {\n                best_path = path;\n            }\n            return;\n        }\n        visited[x][y] = true;\n        path.push_back(grid[x][y]);\n        int dx[4] = {-1, 1, 0, 0};\n        int dy[4] = {0, 0, -1, 1};\n        for (int i = 0; i < 4; ++i) {\n            int nx = x + dx[i];\n            int ny = y + dy[i];\n            if (nx >= 0 && nx < N && ny >= 0 && ny < N && !visited[nx][ny]) {\n                dfs(nx, ny, steps + 1);\n            }\n        }\n        path.pop_back();\n        visited[x][y] = false;\n    };\n\n    dfs(start_x, start_y, 0);\n    return best_path;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int N = grid.size();\n    std::vector<int> result;\n    if (k <= 0 || N == 0) return result;\n\n    int min_val = INT_MAX;\n    int start_x = 0, start_y = 0;\n\n    // Find the minimum starting point using SIMD\n    for(int i = 0; i < N; ++i){\n        for(int j = 0; j + 3 < N; j += 4){\n            __m128i vals = _mm_loadu_si128((__m128i const*)(&grid[i][j]));\n            __m128i mins = _mm_min_epi32(vals, _mm_shuffle_epi32(vals, 1));\n            mins = _mm_min_epi32(mins, _mm_shuffle_epi32(mins, 2));\n            int min_val_local = _mm_cvtsi128_si32(mins);\n            if(min_val_local < min_val){\n                min_val = min_val_local;\n                start_x = i;\n                start_y = j + (_mm_extract_epi32(_mm_cmpeq_epi32(vals, mins), 0) * 0);\n            }\n        }\n        for(int j = N - (N % 4); j < N; ++j){\n            if(grid[i][j] < min_val){\n                min_val = grid[i][j];\n                start_x = i;\n                start_y = j;\n            }\n        }\n    }\n\n    result.push_back(min_val);\n    int current_x = start_x;\n    int current_y = start_y;\n    int prev_val = min_val;\n\n    for(int step = 1; step < k; ++step){\n        int next_val = INT_MAX;\n        int next_x = current_x, next_y = current_y;\n        for(int dx = -1; dx <= 1; ++dx){\n            for(int dy = -1; dy <= 1; ++dy){\n                int nx = current_x + dx;\n                int ny = current_y + dy;\n                if(nx >= 0 && nx < N && ny >= 0 && ny < N){\n                    if(grid[nx][ny] < next_val && grid[nx][ny] >= prev_val){\n                        next_val = grid[nx][ny];\n                        next_x = nx;\n                        next_y = ny;\n                    }\n                }\n            }\n        }\n        result.push_back(next_val);\n        current_x = next_x;\n        current_y = next_y;\n        prev_val = next_val;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <climits>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    int N = grid.size();\n    if (N == 0 || k <= 0) return {};\n    \n    std::vector<int> result;\n    int min_val = INT_MAX;\n    int start_x = 0, start_y = 0;\n\n    // Find the minimum starting value using SIMD\n    for(int i = 0; i < N; ++i){\n        for(int j = 0; j < N; ++j){\n            __m128i vec_min = _mm_set1_epi32(min_val);\n            __m128i vec_grid = _mm_set1_epi32(grid[i][j]);\n            __m128i cmp = _mm_cmplt_epi32(vec_grid, vec_min);\n            if(_mm_movemask_epi8(cmp)){\n                min_val = grid[i][j];\n                start_x = i;\n                start_y = j;\n            }\n        }\n    }\n\n    result.push_back(min_val);\n    int current_x = start_x;\n    int current_y = start_y;\n\n    for(int step = 1; step < k; ++step){\n        int next_val = INT_MAX;\n        int next_x = current_x;\n        int next_y = current_y;\n\n        // Define possible moves: up, down, left, right\n        int dx[4] = {-1, 1, 0, 0};\n        int dy[4] = {0, 0, -1, 1};\n\n        for(int dir = 0; dir < 4; ++dir){\n            int nx = current_x + dx[dir];\n            int ny = current_y + dy[dir];\n\n            if(nx >= 0 && nx < N && ny >= 0 && ny < N){\n                __m128i vec_next = _mm_set1_epi32(grid[nx][ny]);\n                __m128i vec_next_val = _mm_set1_epi32(next_val);\n                __m128i cmp = _mm_cmplt_epi32(vec_next, vec_next_val);\n\n                if(_mm_movemask_epi8(cmp)){\n                    next_val = grid[nx][ny];\n                    next_x = nx;\n                    next_y = ny;\n                }\n            }\n        }\n\n        result.push_back(next_val);\n        current_x = next_x;\n        current_y = next_y;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    const int odd_mask = 0xAAAAAAAA;\n    uint64_t product = 1;\n    int count = 0;\n\n    __m128i v_n = _mm_set_epi8(\n        (n >> 56) & 0xFF, (n >> 48) & 0xFF, (n >> 40) & 0xFF, (n >> 32) & 0xFF,\n        (n >> 24) & 0xFF, (n >> 16) & 0xFF, (n >> 8) & 0xFF, n & 0xFF,\n        (n >> 56) & 0xFF, (n >> 48) & 0xFF, (n >> 40) & 0xFF, (n >> 32) & 0xFF,\n        (n >> 24) & 0xFF, (n >> 16) & 0xFF, (n >> 8) & 0xFF, n & 0xFF\n    );\n    __m128i v_odd_mask = _mm_set1_epi8(odd_mask);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_one = _mm_set1_epi8(1);\n\n    __m128i is_odd = _mm_and_si128(_mm_cmpeq_epi8(_mm_and_si128(v_n, v_odd_mask), v_odd_mask), _mm_cmpgt_epi8(v_n, v_zero));\n    __m128i odd_digits = _mm_and_si128(v_n, is_odd);\n\n    __m128i prod_lo = _mm_shuffle_epi8(odd_digits, _mm_set_epi8(0,1,2,3,4,5,6,7,-1,-1,-1,-1,-1,-1,-1,-1));\n    __m128i prod_hi = _mm_shuffle_epi8(odd_digits, _mm_set_epi8(-1,-1,-1,-1,-1,-1,-1,-1,8,9,10,11,12,13,14,15));\n\n    __m128i prod = _mm_mullo_epi16(prod_lo, prod_hi);\n\n    uint16_t prod_arr[8];\n    _mm_storeu_si128((__m128i*)prod_arr, prod);\n\n    for(int i=0;i<8;i++) {\n        if(prod_arr[i] != 0){\n            product *= prod_arr[i];\n            count++;\n        }\n    }\n\n    if(count == 0) return 0;\n    return product;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    __m128i product = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i even_mask = _mm_set1_epi8(0xAA);\n    __m128i ones = _mm_set1_epi8(1);\n\n    while (n > 0) {\n        uint8_t digit = n % 10;\n        n /= 10;\n\n        __m128i d = _mm_set1_epi8(digit);\n        __m128i is_odd = _mm_andnot_si128(_mm_and_si128(d, even_mask), ones);\n\n        __m128i current_product = _mm_mullo_epi16(product, is_odd);\n        product = _mm_or_si128(product, current_product);\n\n        uint32_t mask = _mm_movemask_epi8(is_odd);\n        if (mask == 0 && n == 0) return 0;\n    }\n\n    __m128i res_low  = _mm_shuffle_epi32(product, _MM_SHUFFLE(0, 0, 0, 0));\n    __m128i res_high = _mm_shuffle_epi32(product, _MM_SHUFFLE(2, 2, 2, 2));\n    product = _mm_mullo_epi16(res_low, res_high);\n\n    res_low  = _mm_shuffle_epi32(product, _MM_SHUFFLE(1, 0, 1, 0));\n    res_high = _mm_shuffle_epi32(product, _MM_SHUFFLE(3, 2, 3, 2));\n    product = _mm_mullo_epi16(res_low, res_high);\n\n    uint32_t result = _mm_cvtsi128_si32(product);\n    return result == 1 ? 1 : result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    const uint8_t ones[16] = {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1};\n    const uint8_t mask_even[16] = {0x01, 0x03, 0x05, 0x07, 0x09, 0x01, 0x03, 0x05, 0x07, 0x09, 0x01, 0x03, 0x05, 0x07, 0x09, 0x01};\n    __m128i v_ones = _mm_loadu_si128((__m128i*)ones);\n    __m128i v_mask_even = _mm_loadu_si128((__m128i*)mask_even);\n    __m128i v_product = _mm_set_epi64x(1, 1);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_n = _mm_set_epi64x(0, n);\n    int has_odd = 0;\n\n    while(_mm_extract_epi64(v_n, 0) != 0){\n        __m128i v_digit = _mm_and_si128(_mm_set_epi64x(0, 0xf), v_n);\n        __m128i v_is_odd = _mm_cmpestrm(v_digit, _mm_set_epi64x(0, 1), v_mask_even, _mm_set_epi64x(0, 9), _SIDD_CMP_EQUAL_ANY | _SIDD_BIT_MASK);\n        __m128i v_odd_digit = _mm_and_si128(v_digit, v_is_odd);\n        __m128i v_temp_product = _mm_mullo_epi16(v_product, _mm_or_si128(v_odd_digit, v_ones));\n        v_product = _mm_and_si128(v_temp_product, _mm_cmpeq_epi8(v_temp_product, v_zero));\n        has_odd |= _mm_movemask_epi8(_mm_cmpgt_epi8(v_odd_digit, v_zero));\n        v_n = _mm_div_epi64(v_n, _mm_set_epi64x(0, 10));\n    }\n\n    if(!has_odd){\n        return 0;\n    }\n\n    uint64_t product_low, product_high;\n    _mm_storeu_si128((__m128i*)&product_low, v_product);\n    product_high = *((uint64_t*)((char*)&v_product + 8));\n    return product_low * product_high;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    __m128i ones = _mm_set1_epi8(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi16(1);\n    __m128i odd_mask = _mm_set1_epi8(0x01);\n\n    while (n > 0) {\n        __m128i digit = _mm_set1_epi8(n % 10);\n        n /= 10;\n\n        __m128i is_odd = _mm_and_si128(digit, odd_mask);\n        __m128i select_odd = _mm_cmpeq_epi8(is_odd, odd_mask);\n\n        __m128i current_product = _mm_shuffle_epi8(digit, select_odd);\n        product = _mm_mullo_epi16(product, _mm_cvtepi8_epi16(current_product));\n    }\n\n    uint16_t prod_arr[8];\n    _mm_storeu_si128((__m128i*)prod_arr, product);\n\n    uint64_t result = 1;\n    for(int i = 0; i < 8; ++i){\n        if(prod_arr[i] != 0){\n            result *= prod_arr[i];\n        }\n    }\n\n    return result == 1 ? 0 : result;\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    if (n == 0) return 0;\n\n    __m128i product = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n    __m128i odd_mask = _mm_set1_epi8(0x01);\n    __m128i even_flag = _mm_set1_epi32(1);\n\n    while (n > 0) {\n        __m128i digit = _mm_set_epi32(0, 0, 0, (int)(n % 10));\n        __m128i is_odd = _mm_and_si128(_mm_cmpeq_epi8(digit, _mm_and_si128(digit, odd_mask)), _mm_cmpgt_epi8(digit, zero));\n\n        __m128i current_product = _mm_shuffle_epi32(product, _MM_SHUFFLE(0, 0, 0, 0));\n        current_product = _mm_mullo_epi32(current_product, _mm_shuffle_epi32(digit, _MM_SHUFFLE(0, 0, 0, 0)));\n\n        even_flag = _mm_and_si128(even_flag, _mm_cmpeq_epi8(is_odd, zero));\n        product = _mm_or_si128(product, _mm_and_si128(is_odd, current_product));\n\n        n /= 10;\n    }\n\n    uint32_t result;\n    _mm_storeu_si128((__m128i*)&result, product);\n\n    if (!_mm_testz_si128(even_flag, even_flag)) return 0;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n#include <cstdint>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    __m128 zero = _mm_setzero_ps();\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i sums = _mm_setzero_si128();\n\n    for (size_t i = 0; i < size / 4 * 4; i += 4){\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 ceil_vec = _mm_ceil_ps(vec);\n        __m128 squared = _mm_mul_ps(ceil_vec, ceil_vec);\n        __m128i int_squared = _mm_cvtps_epi32(squared);\n        sums = _mm_add_epi32(sums, int_squared);\n    }\n\n    int intermediate[4];\n    _mm_storeu_si128((__m128i*)intermediate, sums);\n    total_sum += intermediate[0] + intermediate[1] + intermediate[2] + intermediate[3];\n\n    for (size_t i = size / 4 * 4; i < size; ++i){\n        float val = std::ceil(lst[i]);\n        total_sum += static_cast<int>(val * val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n#include <cstdint>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    alignas(16) float temp[4];\n    __m128 v_zero = _mm_setzero_ps();\n    __m128 v_one = _mm_set1_ps(1.0f);\n    __m128i v_ones = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i + 4 <= size; i += 4) {\n        __m128 v_data = _mm_loadu_ps(&lst[i]);\n        __m128 v_floor = _mm_floor_ps(v_data);\n        __m128 v_ceil_mask = _mm_cmpgt_ps(v_data, v_floor);\n        __m128 v_ceil = _mm_add_ps(v_floor, _mm_and_ps(v_ceil_mask, v_one));\n        __m128 v_sq = _mm_mul_ps(v_ceil, v_ceil);\n        \n        // Horizontal sum of the 4 floats\n        __m128 v_sum = _mm_add_ps(v_sq, _mm_movehl_ps(v_zero, v_sq));\n        v_sum = _mm_add_ss(v_sum, _mm_shuffle_ps(v_sum, v_sum, 1));\n        _mm_store_ss(temp, v_sum);\n        total_sum += static_cast<uint64_t>(temp[0]) * static_cast<uint64_t>(temp[0]);\n    }\n\n    // Handle remaining elements\n    for (size_t i = (size / 4) * 4; i < size; ++i) {\n        float val = std::ceil(lst[i]);\n        total_sum += static_cast<uint64_t>(val * val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n#include <cstdint>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return total_sum;\n\n    alignas(16) float temp[4];\n    __m128 v_one = _mm_set1_ps(1.0f);\n\n    for(size_t i = 0; i + 4 <= size; i += 4){\n        __m128 v_data = _mm_loadu_ps(&lst[i]);\n        __m128 v_ceiling = _mm_ceil_ps(v_data);\n        __m128 v_square = _mm_mul_ps(v_ceiling, v_ceiling);\n        _mm_store_ps(temp, v_square);\n        total_sum += static_cast<uint64_t>(temp[0]) + static_cast<uint64_t>(temp[1]) +\n                    static_cast<uint64_t>(temp[2]) + static_cast<uint64_t>(temp[3]);\n    }\n\n    for(size_t i = size - (size % 4); i < size; ++i){\n        float val = std::ceil(lst[i]);\n        total_sum += static_cast<uint64_t>(val * val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n#include <cstdint>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    alignas(16) float temp[4];\n    __m128 ones = _mm_set1_ps(1.0f);\n\n    for(size_t i = 0; i + 4 <= size; i += 4){\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        vec = _mm_add_ps(vec, ones); // Ceiling approximation by adding 1 and truncating\n        vec = _mm_floor_ps(vec);    // Floor the result to get ceiling effect\n        vec = _mm_mul_ps(vec, vec); // Square the numbers\n\n        _mm_store_ps(temp, vec);    // Store results to temp array\n        sum += static_cast<int32_t>(temp[0]) * static_cast<int32_t>(temp[0]);\n        sum += static_cast<int32_t>(temp[1]) * static_cast<int32_t>(temp[1]);\n        sum += static_cast<int32_t>(temp[2]) * static_cast<int32_t>(temp[2]);\n        sum += static_cast<int32_t>(temp[3]) * static_cast<int32_t>(temp[3]);\n    }\n\n    for(size_t i = size - (size % 4); i < size; ++i){\n        int32_t val = static_cast<int32_t>(std::ceil(lst[i]));\n        sum += val * val;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <cmath>\n#include <cstdint>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t total_sum = 0;\n    size_t size = lst.size();\n    if (size == 0) return 0;\n\n    __m128 zero = _mm_setzero_ps();\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i sums = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 4 <= size; i += 4){\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        // Ceiling operation\n        __m128 ceil_vec = _mm_ceil_ps(vec);\n        // Convert to integer\n        __m128i int_vec = _mm_cvtps_epi32(ceil_vec);\n        // Square the integers\n        __m128i squared_vec = _mm_mullo_epi32(int_vec, int_vec);\n        // Horizontal add within each 64-bit lane\n        __m128i lo = _mm_unpacklo_epi32(squared_vec, squared_vec);\n        __m128i hi = _mm_unpackhi_epi32(squared_vec, squared_vec);\n        __m128i sum_lo_hi = _mm_add_epi64(lo, hi);\n        // Accumulate the results\n        sums = _mm_add_epi64(sums, sum_lo_hi);\n    }\n\n    // Extract the accumulated sums\n    uint64_t sum_arr[2];\n    _mm_storeu_si128((__m128i*)sum_arr, sums);\n    total_sum += sum_arr[0] + sum_arr[1];\n\n    // Handle remaining elements\n    for (size_t i = (size / 4) * 4; i < size; ++i){\n        int ceil_val = static_cast<int>(std::ceil(lst[i]));\n        total_sum += static_cast<uint64_t>(ceil_val * ceil_val);\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int result = -1;\n    __m128i vresult = _mm_set1_epi32(-1);\n    __m128i vmax = _mm_set1_epi32(arr[0]);\n    size_t i = 1;\n\n    for (; i + 3 < arr.size(); i += 4){\n        __m128i current = _mm_loadu_si128((__m128i const*)(arr.data() + i));\n        __m128i prev = _mm_loadu_si128((__m128i const*)(arr.data() + i - 1));\n\n        __m128i cmp = _mm_cmplt_epi32(current, prev);\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mask = _mm_and_si128(cmp, indices);\n\n        __m128i max_index = _mm_max_epu32(mask, vresult);\n        vresult = max_index;\n\n        __m128i new_max = _mm_max_epi32(current, vmax);\n        vmax = new_max;\n    }\n\n    int temp_result = _mm_extract_epi32(vresult, 0);\n    result = std::max(result, temp_result);\n\n    for (; i < arr.size(); ++i){\n        if (arr[i] < arr[i-1]){\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int result = -1;\n    size_t n = arr.size();\n    size_t aligned_size = n - (n % 4);\n\n    __m128i cmp_result = _mm_setzero_si128();\n    __m128i mask = _mm_set_epi32(-1, -1, -1, -1);\n\n    for (size_t i = 0; i < aligned_size - 1; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i const*)(arr.data() + i));\n        __m128i next = _mm_loadu_si128((__m128i const*)(arr.data() + i + 1));\n\n        __m128i diff = _mm_cmpgt_epi32(next, current);\n        __m128i and_res = _mm_and_si128(diff, mask);\n\n        __m128i eq = _mm_cmpeq_epi32(and_res, mask);\n        cmp_result = _mm_or_si128(cmp_result, eq);\n\n        if (_mm_movemask_epi8(eq) != 0xFFFF){\n            int indices[4] = {i, i+1, i+2, i+3};\n            for(int j=0; j<4; ++j){\n                if(arr[indices[j]+1] < arr[indices[j]]){\n                    result = std::max(result, indices[j]+1);\n                }\n            }\n        }\n    }\n\n    for(size_t i = aligned_size; i < n-1; ++i){\n        if(arr[i+1] < arr[i]){\n            result = i+1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int result = -1;\n    size_t n = arr.size();\n    size_t aligned_size = n - (n % 4);\n\n    __m128i prev = _mm_set1_epi32(arr[0]);\n    __m128i indices = _mm_setr_epi32(0, 1, 2, 3);\n    __m128i max_index = _mm_set1_epi32(-1);\n\n    for(size_t i = 1; i < aligned_size; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i const*)(arr.data() + i));\n        __m128i cmp = _mm_cmplt_epi32(current, prev);\n\n        __m128i idx_cmp = _mm_and_si128(cmp, indices);\n        __m128i max_idx_cmp = _mm_max_epu32(idx_cmp, max_index);\n        max_index = max_idx_cmp;\n\n        prev = current;\n        indices = _mm_add_epi32(indices, _mm_set1_epi32(4));\n    }\n\n    int max_idx = -1;\n    __m128i zero = _mm_setzero_si128();\n    if(_mm_movemask_epi8(_mm_cmpgt_epi32(max_index, zero)) != 0){\n        int temp[4];\n        _mm_storeu_si128((__m128i*)temp, max_index);\n        for(int i = 0; i < 4; ++i){\n            if(temp[i] > max_idx){\n                max_idx = temp[i];\n            }\n        }\n    }\n\n    for(size_t i = aligned_size; i < n - 1; ++i){\n        if(arr[i+1] < arr[i]){\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    if (arr.size() < 2) return -1;\n\n    int n = arr.size();\n    __m128i vcmp, vprev, vcurr, vmask;\n    int result = -1;\n\n    for(int i = 0; i < n - 1; i += 4){\n        // Load current and previous elements\n        vprev = _mm_loadu_si128((__m128i const*)(arr.data() + i));\n        vcurr = _mm_loadu_si128((__m128i const*)(arr.data() + i + 1));\n\n        // Compare: curr >= prev\n        vcmp = _mm_cmpgt_epi32(vcurr, vprev);\n        vmask = _mm_cmpeq_epi32(vcmp, _mm_setzero_si128());\n\n        // Check if any comparison failed\n        if(_mm_movemask_epi8(vmask) != 0){\n            // Find the first failed index\n            __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n            __m128i masked_indices = _mm_and_si128(vmask, indices);\n            int idx = _mm_cvtsi128_si32(_mm_max_epi32(masked_indices, _mm_set1_epi32(-1)));\n            if(idx > result) result = idx;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if(n <= 1) return -1;\n\n    const int *data = arr.data();\n    __m128i indices = _mm_set_epi32(n-4, n-3, n-2, n-1);\n    __m128i max_index = _mm_set1_epi32(-1);\n\n    for(int i = 0; i <= n-5; i += 4){\n        __m128i v1 = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(data + i + 1));\n        __m128i cmp = _mm_cmplt_epi32(v2, v1);\n        __m128i mask = _mm_and_si128(cmp, indices);\n        __m128i current_max = _mm_max_epu32(mask, max_index);\n        max_index = current_max;\n        indices = _mm_add_epi32(indices, _mm_set1_epi32(4));\n    }\n\n    int result = -1;\n    __m128i res = _mm_max_epu32(max_index, _mm_set1_epi32(-1));\n    _mm_storeu_si128((__m128i*)&result, res);\n\n    // Handle remaining elements\n    for(int i = ((n-1)/4)*4; i < n-1; ++i){\n        if(data[i+1] < data[i]){\n            result = i;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n#include <climits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    __m128i max_neg = _mm_set1_epi32(INT_MIN);\n    __m128i min_pos = _mm_set1_epi32(INT_MAX);\n\n    const int zero = 0;\n    __m128i vzero = _mm_set1_epi32(zero);\n\n    for(size_t i = 0; i + 4 <= lst.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n\n        __m128i neg_mask = _mm_cmplt_epi32(vec, vzero);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, vzero);\n\n        __m128i current_max_neg = _mm_max_epi32(vec, max_neg);\n        __m128i current_min_pos = _mm_min_epi32(vec, min_pos);\n\n        max_neg = _mm_or_si128(_mm_and_si128(neg_mask, current_max_neg), _mm_andnot_si128(neg_mask, max_neg));\n        min_pos = _mm_or_si128(_mm_and_si128(pos_mask, current_min_pos), _mm_andnot_si128(pos_mask, min_pos));\n    }\n\n    int max_neg_val = INT_MIN;\n    int min_pos_val = INT_MAX;\n\n    // Horizontal max for max_neg\n    {\n        int vals[4];\n        _mm_storeu_si128((__m128i*)vals, max_neg);\n        for(int i = 0; i < 4; ++i){\n            if(vals[i] > max_neg_val) max_neg_val = vals[i];\n        }\n    }\n\n    // Horizontal min for min_pos\n    {\n        int vals[4];\n        _mm_storeu_si128((__m128i*)vals, min_pos);\n        for(int i = 0; i < 4; ++i){\n            if(vals[i] < min_pos_val) min_pos_val = vals[i];\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = lst.size() & ~size_t(3); i < lst.size(); ++i){\n        if(lst[i] < 0 && lst[i] > max_neg_val) max_neg_val = lst[i];\n        if(lst[i] > 0 && lst[i] < min_pos_val) min_pos_val = lst[i];\n    }\n\n    if(max_neg_val == INT_MIN) max_neg_val = 0;\n    if(min_pos_val == INT_MAX) min_pos_val = 0;\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <limits>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    __m128i max_neg = _mm_set1_epi32(std::numeric_limits<int>::min());\n    __m128i min_pos = _mm_set1_epi32(std::numeric_limits<int>::max());\n\n    const int size = lst.size();\n    const int aligned_size = size - (size % 4);\n\n    for(int i = 0; i < aligned_size; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        \n        // Create mask for negative numbers\n        __m128i neg_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), vec);\n        __m128i pos_mask = _mm_cmplt_epi32(vec, _mm_setzero_si128());\n\n        // Select negatives and positives\n        __m128i negs = _mm_and_si128(neg_mask, vec);\n        __m128i poss = _mm_and_si128(pos_mask, vec);\n\n        // Find max of negatives\n        max_neg = _mm_max_epi32(max_neg, negs);\n\n        // Find min of positives\n        min_pos = _mm_min_epi32(min_pos, poss);\n    }\n\n    // Horizontal max for negatives\n    int max_neg_h[4];\n    _mm_storeu_si128((__m128i*)max_neg_h, max_neg);\n    int max_neg_val = std::numeric_limits<int>::min();\n    for(int i = 0; i < 4; ++i){\n        if(max_neg_h[i] < 0 && max_neg_h[i] > max_neg_val){\n            max_neg_val = max_neg_h[i];\n        }\n    }\n\n    // Horizontal min for positives\n    int min_pos_h[4];\n    _mm_storeu_si128((__m128i*)min_pos_h, min_pos);\n    int min_pos_val = std::numeric_limits<int>::max();\n    for(int i = 0; i < 4; ++i){\n        if(min_pos_h[i] > 0 && min_pos_h[i] < min_pos_val){\n            min_pos_val = min_pos_h[i];\n        }\n    }\n\n    // Handle remaining elements\n    for(int i = aligned_size; i < size; ++i){\n        if(lst[i] < 0 && lst[i] > max_neg_val){\n            max_neg_val = lst[i];\n        }\n        if(lst[i] > 0 && lst[i] < min_pos_val){\n            min_pos_val = lst[i];\n        }\n    }\n\n    if(max_neg_val == std::numeric_limits<int>::min()) max_neg_val = 0;\n    if(min_pos_val == std::numeric_limits<int>::max()) min_pos_val = 0;\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    __m128i max_neg = _mm_set1_epi32(0x80000000);\n    __m128i min_pos = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i zero = _mm_setzero_si128();\n\n    for(size_t i = 0; i + 4 <= lst.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n\n        __m128i neg_vals = _mm_and_si128(vec, neg_mask);\n        __m128i pos_vals = _mm_and_si128(vec, pos_mask);\n\n        max_neg = _mm_max_epi32(max_neg, neg_vals);\n        min_pos = _mm_min_epi32(min_pos, pos_vals);\n    }\n\n    int max_neg_arr[4];\n    int min_pos_arr[4];\n\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos);\n\n    int a = 0;\n    int b = 0;\n\n    for(int i = 0; i < 4; ++i){\n        if(max_neg_arr[i] < 0 && (a == 0 || max_neg_arr[i] > a)) a = max_neg_arr[i];\n        if(min_pos_arr[i] > 0 && (b == 0 || min_pos_arr[i] < b)) b = min_pos_arr[i];\n    }\n\n    // Handle remaining elements\n    for(size_t i = lst.size() - (lst.size() % 4); i < lst.size(); ++i){\n        if(lst[i] < 0 && (a == 0 || lst[i] > a)) a = lst[i];\n        if(lst[i] > 0 && (b == 0 || lst[i] < b)) b = lst[i];\n    }\n\n    return {a, b};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    __m128i max_neg = _mm_set1_epi32(0x80000000);\n    __m128i min_pos = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i zero = _mm_setzero_si128();\n\n    int scalar_max_neg = 0x80000000;\n    int scalar_min_pos = 0x7FFFFFFF;\n\n    for(size_t i = 0; i + 4 <= lst.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        \n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n\n        __m128i curr_max_neg = _mm_max_epi32(vec, max_neg);\n        __m128i curr_min_pos = _mm_min_epi32(vec, min_pos);\n\n        curr_max_neg = _mm_and_si128(curr_max_neg, neg_mask);\n        curr_min_pos = _mm_and_si128(curr_min_pos, pos_mask);\n\n        max_neg = _mm_max_epi32(max_neg, curr_max_neg);\n        min_pos = _mm_min_epi32(min_pos, curr_min_pos);\n    }\n\n    int max_neg_arr[4];\n    int min_pos_arr[4];\n\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos);\n\n    for(int i = 0; i < 4; ++i){\n        if(max_neg_arr[i] > scalar_max_neg) scalar_max_neg = max_neg_arr[i];\n        if(min_pos_arr[i] < scalar_min_pos) scalar_min_pos = min_pos_arr[i];\n    }\n\n    for(size_t i = lst.size() - (lst.size() % 4); i < lst.size(); ++i){\n        if(lst[i] < 0 && lst[i] > scalar_max_neg) scalar_max_neg = lst[i];\n        if(lst[i] > 0 && lst[i] < scalar_min_pos) scalar_min_pos = lst[i];\n    }\n\n    if(scalar_max_neg == 0x80000000) scalar_max_neg = 0;\n    if(scalar_min_pos == 0x7FFFFFFF) scalar_min_pos = 0;\n\n    return {scalar_max_neg, scalar_min_pos};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    if (lst.empty()) return {0, 0};\n\n    __m128i max_neg = _mm_set1_epi32(0x80000000); // Initialize to smallest int\n    __m128i min_pos = _mm_set1_epi32(0x7FFFFFFF);  // Initialize to largest int\n    __m128i zero = _mm_set1_epi32(0);\n    __m128i has_neg = _mm_setzero_si128();\n    __m128i has_pos = _mm_setzero_si128();\n\n    for(size_t i = 0; i + 4 <= lst.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i neg_mask = _mm_cmplt_epi32(vec, zero);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, zero);\n\n        __m128i current_max_neg = _mm_max_epi32(vec, max_neg);\n        __m128i current_min_pos = _mm_min_epi32(vec, min_pos);\n\n        max_neg = _mm_or_si128(_mm_and_si128(neg_mask, current_max_neg), _mm_andnot_si128(neg_mask, max_neg));\n        min_pos = _mm_or_si128(_mm_and_si128(pos_mask, current_min_pos), _mm_andnot_si128(pos_mask, min_pos));\n\n        has_neg = _mm_or_si128(has_neg, neg_mask);\n        has_pos = _mm_or_si128(has_pos, pos_mask);\n    }\n\n    int max_neg_val = 0;\n    int min_pos_val = 0;\n\n    if (_mm_movemask_epi8(has_neg) != 0){\n        max_neg_val = std::max({_mm_extract_epi32(max_neg, 0), \n                               _mm_extract_epi32(max_neg, 1), \n                               _mm_extract_epi32(max_neg, 2), \n                               _mm_extract_epi32(max_neg, 3)});\n    }\n\n    if (_mm_movemask_epi8(has_pos) != 0){\n        min_pos_val = std::min({_mm_extract_epi32(min_pos, 0), \n                               _mm_extract_epi32(min_pos, 1), \n                               _mm_extract_epi32(min_pos, 2), \n                               _mm_extract_epi32(min_pos, 3)});\n    }\n\n    return {max_neg_val, min_pos_val};\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if(n == 0 || n == 1) return 1;\n\n    uint64_t result = 1;\n    __m128i vresult = _mm_set_epi64x(1, 1);\n    __m128i vone = _mm_set_epi64x(1, 1);\n\n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        __m128i vf = _mm_set_epi64x(1, 1);\n        for(uint64_t j = 2; j <= i; ++j){\n            vf = _mm_mul_epu32(vf, _mm_set_epi64x(j, j));\n            vf = _mm_shuffle_epi32(vf, 0x01); // Move to lower part\n        }\n        __m128i temp = _mm_mul_epu32(vresult, vf);\n        vresult = _mm_shuffle_epi32(temp, 0x01); // Move to lower part\n    }\n\n    result *= _mm_cvtsi128_si64(vresult);\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if(n == 0 || n == 1) return 1;\n\n    uint64_t result = 1;\n    __m128i vresult = _mm_set_epi64x(1, 1);\n    __m128i vprod = _mm_set_epi64x(1, 1);\n    __m128i vtemp;\n\n    for(uint64_t i = 2; i <= n; ++i){\n        vprod = _mm_set_epi64x(i, i);\n        for(uint64_t j = i - 1; j >= 1; --j){\n            vtemp = _mm_set_epi64x(j, j);\n            vprod = _mm_mul_epu32(vprod, vtemp);\n        }\n        vresult = _mm_mul_epu32(vresult, vprod);\n    }\n\n    uint64_t high, low;\n    _mm_storeu_si128((__m128i*)&low, vresult);\n    high = _mm_extract_epi64(vresult, 1);\n    result = high * low;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if(n == 0 || n == 1) return 1;\n    \n    uint64_t result = 1;\n    __m128i vresult = _mm_set_epi64x(1, 1);\n    __m128i vprod = _mm_set_epi64x(1, 1);\n    __m128i vcurr = _mm_set_epi64x(1, 1);\n    \n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            factorial *= j;\n        }\n        result *= factorial;\n        \n        // SIMD part for parallelism\n        vcurr = _mm_mul_epi64(vcurr, _mm_set_epi64x(i, i));\n        vprod = _mm_mul_epi64(vprod, vcurr);\n        vresult = _mm_mul_epi64(vresult, vprod);\n    }\n    \n    uint64_t high, low;\n    _mm_storeu_si128((__m128i*)&high, vresult);\n    low = high;\n    \n    return result * low;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if (n == 0 || n == 1) return 1;\n\n    __m128i vresult = _mm_set_epi64x(1, 1);\n    uint64_t result = 1;\n\n    for(uint64_t i = 2; i <= n; ++i){\n        uint64_t factorial = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            factorial *= j;\n        }\n        __m128i vf = _mm_set_epi64x(factorial, factorial);\n        vresult = _mm_mul_epu32(vresult, vf);\n        uint64_t high = _mm_cvtsi128_si64(_mm_srli_si128(vresult, 8));\n        uint64_t low = _mm_cvtsi128_si64(vresult);\n        result = low * high;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n){\n    if (n == 0) return 1;\n    uint64_t result = 1;\n    __m128i vresult = _mm_set_epi64x(1, 1);\n    __m128i vone = _mm_set_epi64x(1, 1);\n    for(uint64_t i = 1; i <= n; ++i){\n        uint64_t factorial = 1;\n        __m128i vf = _mm_set_epi64x(1, 1);\n        for(uint64_t j = 1; j <= i; ++j){\n            vf = _mm_mul_epu32(vf, _mm_set_epi64x(j, j));\n            vf = _mm_shuffle_epi32(vf, _MM_SHUFFLE(2, 3, 0, 1));\n        }\n        __m128i vt = _mm_mul_epu32(vresult, vf);\n        vresult = _mm_shuffle_epi32(vt, _MM_SHUFFLE(2, 3, 0, 1));\n    }\n    uint64_t high, low;\n    _mm_storeu_si128((__m128i*)&low, vresult);\n    high = *((uint64_t*)&low + 1);\n    result = high * low;\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if(lst.empty()) return 0;\n\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t i = 0;\n    size_t len = lst.size();\n\n    // Handle aligned data for better performance\n    for(; i + 3 < len; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i idx_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n\n        // Mask for multiple of 3\n        __m128i mod3_mask = _mm_cmpeq_epi32(_mm_and_si128(idx_vec, _mm_set1_epi32(2)), _mm_setzero_si128());\n        __m128i squared = _mm_mullo_epi32(vec, vec);\n        vec = _mm_blendv_epi8(vec, squared, mod3_mask);\n\n        // Mask for multiple of 4 and not multiple of 3\n        __m128i mod4_mask = _mm_cmpeq_epi32(_mm_and_si128(idx_vec, _mm_set1_epi32(3)), _mm_setzero_si128());\n        mod4_mask = _mm_andnot_si128(mod3_mask, mod4_mask);\n        __m128i cubed = _mm_mullo_epi32(_mm_mullo_epi32(vec, vec), vec);\n        vec = _mm_blendv_epi8(vec, cubed, mod4_mask);\n\n        vsum = _mm_add_epi32(vsum, vec);\n    }\n\n    // Horizontal sum\n    __m128i vsum1 = _mm_hadd_epi32(vsum, vsum);\n    __m128i vsum2 = _mm_hadd_epi32(vsum1, vsum1);\n    _mm_storeu_si128((__m128i*)&sum, vsum2);\n    sum = sum + ((int*)(&vsum2))[1] + ((int*)(&vsum2))[2] + ((int*)(&vsum2))[3];\n\n    // Handle remaining elements\n    for(; i < len; ++i){\n        if(i % 3 == 0){\n            sum += lst[i] * lst[i];\n        }\n        else if(i % 4 == 0){\n            sum += lst[i] * lst[i] * lst[i];\n        }\n        else{\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if (lst.empty()) return 0;\n\n    __m128i sum = _mm_setzero_si128();\n    __m128i v3 = _mm_set1_epi32(3);\n    __m128i v4 = _mm_set1_epi32(4);\n\n    size_t i = 0;\n    size_t len = lst.size() / 4 * 4;\n\n    for (; i < len; i += 4) {\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i mod3 = _mm_and_si128(_mm_cmpeq_epi32(_mm_rem_epu32(indices, v3), _mm_setzero_si128()), _mm_set1_epi32(1));\n        __m128i mod4_not_mod3 = _mm_andnot_si128(mod3, _mm_cmpeq_epi32(_mm_rem_epu32(indices, v4), _mm_setzero_si128()));\n\n        __m128i vals = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i squared = _mm_mullo_epi32(vals, vals);\n        __m128i cubed = _mm_mullo_epi32(squared, vals);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mod3, squared),\n            _mm_and_si128(mod4_not_mod3, cubed)\n        );\n        result = _mm_or_si128(result, _mm_andnot_si128(_mm_or_si128(mod3, mod4_not_mod3), vals));\n\n        sum = _mm_add_epi32(sum, result);\n    }\n\n    int total_sum = 0;\n    __m128i sum_h = _mm_hadd_epi32(sum, sum);\n    sum_h = _mm_hadd_epi32(sum_h, sum_h);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&total_sum), sum_h);\n\n    for (; i < lst.size(); ++i) {\n        if (i % 3 == 0) {\n            total_sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            total_sum += lst[i] * lst[i] * lst[i];\n        } else {\n            total_sum += lst[i];\n        }\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if(lst.empty()) return 0;\n\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t aligned_size = lst.size() / 4 * 4;\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i idx_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n\n        // Mask for multiple of 3\n        __m128i mod3_mask = _mm_and_si128(_mm_cmpeq_epi32(_mm_and_si128(idx_vec, _mm_set1_epi32(2)), _mm_setzero_si128()), _mm_set1_epi32(-1));\n        __m128i squared = _mm_mullo_epi32(vec, vec);\n        vec = _mm_or_si128(_mm_and_si128(mod3_mask, squared), _mm_andnot_si128(mod3_mask, vec));\n\n        // Mask for multiple of 4 and not multiple of 3\n        __m128i mod4_mask = _mm_andnot_si128(_mm_cmpeq_epi32(_mm_and_si128(idx_vec, _mm_set1_epi32(12)), _mm_setzero_si128()), _mm_set1_epi32(-1));\n        __m128i cubed = _mm_mullo_epi32(vec, _mm_mullo_epi32(vec, vec));\n        vec = _mm_or_si128(_mm_and_si128(mod4_mask, cubed), _mm_andnot_si128(mod4_mask, vec));\n\n        vsum = _mm_add_epi32(vsum, vec);\n    }\n\n    // Horizontal sum\n    __m128i vsum_h1 = _mm_hadd_epi32(vsum, vsum);\n    __m128i vsum_h2 = _mm_hadd_epi32(vsum_h1, vsum_h1);\n    _mm_store_si128((__m128i*)&sum, vsum_h2);\n    sum = sum & 0xFFFFFFFF;\n\n    // Process remaining elements\n    for(size_t i = aligned_size; i < lst.size(); ++i){\n        if(i % 3 == 0){\n            sum += lst[i] * lst[i];\n        }\n        else if(i % 4 == 0){\n            sum += lst[i] * lst[i] * lst[i];\n        }\n        else{\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if (lst.empty()) return 0;\n\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t aligned_size = lst.size() / 4 * 4;\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i index_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n\n        // Check multiples of 3\n        __m128i mod3 = _mm_and_si128(_mm_cmpeq_epi32(_mm_set1_epi32(0), _mm_and_si128(index_vec, _mm_set1_epi32(2))), _mm_set1_epi32(-1));\n        __m128i squared = _mm_mullo_epi32(vec, vec);\n        vec = _mm_or_si128(_mm_and_si128(mod3, squared), _mm_andnot_si128(mod3, vec));\n\n        // Check multiples of 4 and not multiples of 3\n        __m128i mod4_not3 = _mm_and_si128(_mm_cmpeq_epi32(_mm_set1_epi32(0), _mm_and_si128(index_vec, _mm_set1_epi32(3))), _mm_set1_epi32(-1));\n        mod4_not3 = _mm_andnot_si128(_mm_cmpeq_epi32(_mm_set1_epi32(0), _mm_and_si128(index_vec, _mm_set1_epi32(2))), mod4_not3);\n        __m128i cubed = _mm_mullo_epi32(_mm_mullo_epi32(vec, vec), vec);\n        vec = _mm_or_si128(_mm_and_si128(mod4_not3, cubed), _mm_andnot_si128(mod4_not3, vec));\n\n        vsum = _mm_add_epi32(vsum, vec);\n    }\n\n    // Horizontal sum\n    __m128i vsum1 = _mm_hadd_epi32(vsum, vsum);\n    __m128i vsum2 = _mm_hadd_epi32(vsum1, vsum1);\n    int arr[4] = {0};\n    _mm_storeu_si128((__m128i*)arr, vsum2);\n    sum += arr[0] + arr[1];\n\n    // Handle remaining elements\n    for(size_t i = aligned_size; i < lst.size(); ++i){\n        if(i % 3 == 0){\n            sum += lst[i] * lst[i];\n        }\n        else if(i % 4 == 0){\n            sum += lst[i] * lst[i] * lst[i];\n        }\n        else{\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    if(lst.empty()) return 0;\n\n    int sum = 0;\n    __m128i vsum = _mm_setzero_si128();\n    size_t i = 0;\n\n    for(; i + 3 < lst.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i const*)(&lst[i]));\n        __m128i idx_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n\n        // Check for multiple of 3\n        __m128i mod3_mask = _mm_cmpeq_epi32(_mm_and_si128(idx_vec, _mm_set1_epi32(3)), _mm_setzero_si128());\n        __m128i squared = _mm_mullo_epi32(vec, vec);\n\n        // Check for multiple of 4 and not multiple of 3\n        __m128i mod4_mask = _mm_cmpeq_epi32(_mm_and_si128(idx_vec, _mm_set1_epi32(4)), _mm_setzero_si128());\n        __m128i not_mod3_mask = _mm_xor_si128(mod3_mask, _mm_set1_epi32(-1));\n        __m128i mod4_not_mod3_mask = _mm_and_si128(mod4_mask, not_mod3_mask);\n        __m128i cubed = _mm_mullo_epi32(_mm_mullo_epi32(vec, vec), vec);\n\n        // Select values based on conditions\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(mod3_mask, squared),\n            _mm_and_si128(mod4_not_mod3_mask, cubed)\n        );\n        __m128i unchanged = _mm_andnot_si128(_mm_or_si128(mod3_mask, mod4_not_mod3_mask), vec);\n        result = _mm_add_epi32(result, unchanged);\n\n        // Accumulate the sum\n        vsum = _mm_add_epi32(vsum, result);\n    }\n\n    // Horizontal sum of vsum\n    __m128i vsum_hi = _mm_unpackhi_epi64(vsum, vsum);\n    vsum = _mm_add_epi32(vsum, vsum_hi);\n    vsum_hi = _mm_shuffle_epi32(vsum, _MM_SHUFFLE(1, 1, 3, 3));\n    vsum = _mm_add_epi32(vsum, vsum_hi);\n    sum += _mm_cvtsi128_si32(vsum);\n\n    // Handle remaining elements\n    for(; i < lst.size(); ++i){\n        if(i % 3 == 0){\n            sum += lst[i] * lst[i];\n        }\n        else if(i % 4 == 0){\n            sum += lst[i] * lst[i] * lst[i];\n        }\n        else{\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cctype>\n#include <climits>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int threshold = 10;\n    const int vectorSize = 4;\n    size_t i = 0;\n    size_t n = nums.size();\n\n    __m128i vthreshold = _mm_set1_epi32(threshold);\n    __m128i vcount = _mm_setzero_si128();\n\n    auto is_odd_digit = [](int num) -> int {\n        char first, last;\n        if (num == 0) return 0;\n        if (num < 0) num = -num;\n        last = '0' + num % 10;\n        first = '0';\n        while (num > 0){\n            first = '0' + num % 10;\n            num /= 10;\n        }\n        int odd_digits[] = {'1','3','5','7','9'};\n        for(int d : odd_digits){\n            if(d == first && d == last) return 1;\n        }\n        return 0;\n    };\n\n    for (; i <= n - vectorSize; i += vectorSize){\n        __m128i vnums = _mm_loadu_si128((__m128i const*)(&nums[i]));\n        __m128i vgreater = _mm_cmpgt_epi32(vnums, vthreshold);\n\n        int mask = _mm_movemask_epi8(vgreater);\n        int count_mask = 0;\n\n        for(int j = 0; j < vectorSize; ++j){\n            if(mask & (1 << (j*4))){\n                int num = nums[i + j];\n                count_mask += is_odd_digit(num);\n            }\n        }\n        vcount = _mm_add_epi32(vcount, _mm_set1_epi32(count_mask));\n    }\n\n    int temp_count[4] = {0};\n    _mm_storeu_si128((__m128i*)temp_count, vcount);\n    count += temp_count[0] + temp_count[1] + temp_count[2] + temp_count[3];\n\n    for(; i < n; ++i){\n        if(nums[i] > threshold && is_odd_digit(nums[i])){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cctype>\n#include <climits>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int n = nums.size();\n    if (n == 0) return 0;\n\n    __m128i vten = _mm_set1_epi32(10);\n    __m128i vone = _mm_set1_epi32(1);\n\n    for(int i = 0; i < n; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, vten); // vec > 10\n\n        int mask = _mm_movemask_epi8(cmp);\n\n        for(int j = 0; j < 4 && i + j < n; ++j){\n            if(mask & (1 << (j*4))){\n                int num = nums[i+j];\n                int abs_num = (num >=0) ? num : -num;\n                int last_digit = abs_num % 10;\n                int first_digit = abs_num;\n                while(first_digit >=10){\n                    first_digit /=10;\n                }\n                if( (last_digit & 1) && (first_digit & 1)){\n                    count++;\n                }\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cctype>\n#include <cmath>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    size_t size = nums.size();\n    size_t alignedSize = size - (size % 4);\n\n    for(size_t i = 0; i < alignedSize; i += 4){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i cmp = _mm_cmpgt_epi32(_mm_set1_epi32(10), vec); // Compare if nums > 10\n        cmp = _mm_andnot_si128(cmp, _mm_set1_epi32(-1)); // Invert comparison\n\n        int arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(arr), vec);\n\n        for(int j = 0; j < 4; ++j){\n            if(arr[j] > 10){\n                int num = std::abs(arr[j]);\n                int lastDigit = num % 10;\n                int firstDigit = num;\n                while(firstDigit >= 10){\n                    firstDigit /= 10;\n                }\n                if(lastDigit % 2 == 1 && firstDigit % 2 == 1){\n                    count++;\n                }\n            }\n        }\n    }\n\n    for(size_t i = alignedSize; i < size; ++i){\n        if(nums[i] > 10){\n            int num = std::abs(nums[i]);\n            int lastDigit = num % 10;\n            int firstDigit = num;\n            while(firstDigit >= 10){\n                firstDigit /= 10;\n            }\n            if(lastDigit % 2 == 1 && firstDigit % 2 == 1){\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <cctype>\n#include <climits>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int threshold = 10;\n    const int simd_width = 4;\n    size_t vec_size = nums.size();\n    size_t aligned_size = vec_size - (vec_size % simd_width);\n\n    __m128i v_threshold = _mm_set1_epi32(threshold);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_two = _mm_set1_epi32(2);\n    __m128i v_nine = _mm_set1_epi32(9);\n    __m128i v_mask_odd = _mm_set1_epi32(0x55555555);\n\n    for(size_t i = 0; i < aligned_size; i += simd_width){\n        __m128i v_nums = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i v_gt = _mm_cmpgt_epi32(v_nums, v_threshold);\n\n        // Extract absolute value\n        __m128i v_abs = _mm_andnot_si128(_mm_srai_epi32(v_nums, 31), v_nums);\n\n        // Extract last digit\n        __m128i v_last_digit = _mm_and_si128(v_abs, _mm_set1_epi32(9));\n        __m128i v_last_odd = _mm_and_si128(v_last_digit, v_mask_odd);\n        __m128i v_last_check = _mm_cmpeq_epi32(v_last_digit, v_last_odd);\n\n        // Extract first digit\n        __m128i v_first_digit = v_abs;\n        v_first_digit = _mm_subs_epu32(_mm_srli_epi32(v_first_digit, 27), _mm_setzero_si128());\n        v_first_digit = _mm_or_si128(_mm_and_si128(v_first_digit, _mm_set1_epi32(1)), \n                                     _mm_and_si128(_mm_srli_epi32(v_abs, 1), v_mask_odd));\n        __m128i v_first_odd = _mm_and_si128(v_first_digit, v_mask_odd);\n        __m128i v_first_check = _mm_cmpeq_epi32(v_first_digit, v_first_odd);\n\n        // Combine conditions\n        __m128i v_combined = _mm_and_si128(_mm_and_si128(v_gt, v_last_check), v_first_check);\n        __m128i v_ones = _mm_set1_epi32(1);\n        __m128i v_count = _mm_add_epi32(_mm_cmpeq_epi32(v_combined, _mm_setzero_si128()), v_ones);\n        count += _mm_extract_epi32(_mm_sad_epu8(v_count, _mm_setzero_si128()), 0);\n    }\n\n    // Handle remaining elements\n    for(size_t i = aligned_size; i < vec_size; ++i){\n        int num = nums[i];\n        if(num > 10){\n            int abs_num = std::abs(num);\n            int last_digit = abs_num % 10;\n            int first_digit = 0;\n            if(abs_num != 0){\n                first_digit = abs_num;\n                while(first_digit >= 10){\n                    first_digit /= 10;\n                }\n            }\n            if((last_digit & 1) && (first_digit & 1)){\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <cctype>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    int size = nums.size();\n    int i = 0;\n\n    for (; i + 3 < size; i += 4) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i cmp = _mm_cmpgt_epi32(_mm_set1_epi32(10), vec); // Check if each element is greater than 10\n        __m128i mask = _mm_cmpeq_epi32(cmp, _mm_setzero_si128()); // Invert the comparison (elements <= 10)\n\n        int arr[4];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(arr), vec);\n\n        for (int j = 0; j < 4; ++j) {\n            if (!_mm_extract_epi32(mask, j)) {\n                int num = arr[j];\n                int abs_num = (num >= 0) ? num : -num;\n                int last_digit = abs_num % 10;\n                int first_digit = abs_num;\n                while (first_digit >= 10) {\n                    first_digit /= 10;\n                }\n                if ((last_digit & 1) && (first_digit & 1)) {\n                    ++count;\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        int num = nums[i];\n        if (num > 10) {\n            int abs_num = (num >= 0) ? num : -num;\n            int last_digit = abs_num % 10;\n            int first_digit = abs_num;\n            while (first_digit >= 10) {\n                first_digit /= 10;\n            }\n            if ((last_digit & 1) && (first_digit & 1)) {\n                ++count;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const int MOD_COUNT = 3;\n    uint64_t counts[MOD_COUNT] = {0};\n\n    __m128i v3 = _mm_set1_epi32(3);\n    __m128i vminus1 = _mm_set1_epi32(-1);\n\n    uint64_t limit = n / 4 * 4;\n    for(uint64_t i = 1; i <= limit; i += 4){\n        __m128i vi = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i vi_sq = _mm_mullo_epi32(vi, vi);\n        __m128i val = _mm_sub_epi32(_mm_add_epi32(vi_sq, vminus1), vi);\n        __m128i mod = _mm_cvtepi32_epi64(_mm_and_si128(_mm_srai_epi32(_mm_sub_epi32(val, _mm_setzero_si128()), 31), v3));\n        \n        uint32_t m0 = (mod.m128i_u32[0]) % MOD_COUNT;\n        uint32_t m1 = (mod.m128i_u32[1]) % MOD_COUNT;\n        uint32_t m2 = (mod.m128i_u32[2]) % MOD_COUNT;\n        uint32_t m3 = (mod.m128i_u32[3]) % MOD_COUNT;\n\n        counts[m0]++;\n        counts[m1]++;\n        counts[m2]++;\n        counts[m3]++;\n    }\n\n    for(uint64_t i = limit + 1; i <= n; ++i){\n        uint64_t val = i * i - i + 1;\n        counts[val % MOD_COUNT]++;\n    }\n\n    uint64_t c0 = counts[0];\n    uint64_t c1 = counts[1];\n    uint64_t c2 = counts[2];\n\n    uint64_t result = c0*(c0-1)*(c0-2)/6 \n                    + c1*(c1-1)*(c1-2)/6 \n                    + c2*(c2-1)*(c2-2)/6 \n                    + c0*c1*c2;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    uint64_t count = 0;\n    __m128i vthree = _mm_set1_epi32(3);\n    __m128i vone = _mm_set1_epi32(1);\n    __m128i vzero = _mm_setzero_si128();\n\n    int *a = (int*) _mm_malloc(n * sizeof(int), 16);\n    for(uint64_t i = 1; i <= n; ++i){\n        a[i-1] = i * i - i + 1;\n    }\n\n    // Precompute modulo 3 results\n    int *mod3 = (int*) _mm_malloc(n * sizeof(int), 16);\n    for(uint64_t i = 0; i < n; ++i){\n        mod3[i] = a[i] % 3;\n    }\n\n    for(uint64_t i = 0; i < n - 2; ++i){\n        __m128i vi_mod3 = _mm_set1_epi32(mod3[i]);\n        for(uint64_t j = i + 1; j < n - 1; ++j){\n            __m128i vj_mod3 = _mm_set1_epi32(mod3[j]);\n            __m128i target_mod3 = _mm_sub_epi32(vthree, _mm_add_epi32(vi_mod3, vj_mod3));\n            target_mod3 = _mm_andnot_si128(_mm_cmplt_epi32(target_mod3, vzero), target_mod3);\n\n            for(uint64_t k = j + 1; k < n; k += 4){\n                if(k + 3 < n){\n                    __m128i vk_mod3 = _mm_loadu_si128((__m128i*)&mod3[k]);\n                    __m128i matches = _mm_cmpeq_epi32(vk_mod3, target_mod3);\n                    count += _mm_popcnt_u32(_mm_movemask_epi8(matches));\n                }\n                else{\n                    for(uint64_t l = k; l < n; ++l){\n                        if(mod3[l] == (3 - (mod3[i] + mod3[j]) % 3) % 3){\n                            count++;\n                        }\n                    }\n                    break;\n                }\n            }\n        }\n    }\n\n    _mm_free(a);\n    _mm_free(mod3);\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const uint64_t size = n;\n    uint32_t* a = (uint32_t*) _mm_malloc(sizeof(uint32_t) * size, 16);\n    __m128i ones = _mm_set_epi32(1, 1, 1, 1);\n    __m128i three = _mm_set_epi32(3, 3, 3, 3);\n\n    uint64_t count[3] = {0};\n    for(uint64_t i = 0; i < size; ++i){\n        a[i] = (i+1)*(i+1) - (i+1) + 1;\n        if(i % 4 == 0){\n            __m128i vec = _mm_load_si128((__m128i*)&a[i]);\n            __m128i mod = _mm_cvtepi32_epi64(_mm_and_si128(_mm_sub_epi32(vec, ones), three));\n            uint32_t m0 = (_mm_extract_epi32(mod, 0)) % 3;\n            uint32_t m1 = (_mm_extract_epi32(mod, 1)) % 3;\n            uint32_t m2 = (_mm_extract_epi32(mod, 2)) % 3;\n            uint32_t m3 = (_mm_extract_epi32(mod, 3)) % 3;\n            count[m0]++;\n            count[m1]++;\n            count[m2]++;\n            count[m3]++;\n        }\n    }\n\n    // Handle remaining elements\n    for(uint64_t i = ((size / 4) * 4); i < size; ++i){\n        uint32_t val = a[i];\n        count[val % 3]++;\n    }\n\n    uint64_t c0 = count[0];\n    uint64_t c1 = count[1];\n    uint64_t c2 = count[2];\n\n    uint64_t res = (c0 * (c0 - 1) * (c0 - 2)) / 6 +\n                   (c1 * (c1 - 1) * (c1 - 2)) / 6 +\n                   (c2 * (c2 - 1) * (c2 - 2)) / 6 +\n                   (c0 * (c0 - 1)) * c2 / 2 +\n                   (c1 * (c1 - 1)) * c0 / 2 +\n                   (c2 * (c2 - 1)) * c1 / 2 +\n                   c0 * c1 * c2;\n\n    _mm_free(a);\n    return res;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const int MOD_COUNT = 3;\n    uint64_t counts[MOD_COUNT] = {0};\n\n    __m128i vthree = _mm_set1_epi32(3);\n    __m128i vone = _mm_set1_epi32(1);\n\n    for(uint64_t i = 1; i <= n; ++i){\n        uint32_t val = (i * i - i + 1);\n        __m128i vval = _mm_set1_epi32(val);\n        __m128i vrem = _mm_cvtsi128_si32(_mm_and_si128(_mm_sub_epi32(vval, _mm_mullo_epi32(_mm_div_epi32(vval, vthree), vthree), vone));\n        counts[vrem]++;\n    }\n\n    uint64_t result = 0;\n    uint64_t c0 = counts[0];\n    uint64_t c1 = counts[1];\n    uint64_t c2 = counts[2];\n\n    result += (c0 * (c0 - 1) * (c0 - 2)) / 6;\n    result += (c1 * (c1 - 1) * (c1 - 2)) / 6;\n    result += (c2 * (c2 - 1) * (c2 - 2)) / 6;\n    result += (c0 * (c0 - 1)) / 2 * c1;\n    result += (c0 * (c0 - 1)) / 2 * c2;\n    result += (c1 * (c1 - 1)) / 2 * c0;\n    result += (c1 * (c1 - 1)) / 2 * c2;\n    result += (c2 * (c2 - 1)) / 2 * c0;\n    result += (c2 * (c2 - 1)) / 2 * c1;\n    result += c0 * c1 * c2;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n\n    const int MOD_COUNT = 3;\n    uint64_t counts[MOD_COUNT] = {0};\n\n    __m128i mod_vals = _mm_set_epi32(2, 1, 0, 3);\n    __m128i one = _mm_set1_epi32(1);\n\n    for(uint64_t i = 1; i <= n; ++i){\n        uint32_t val = (i * i - i + 1) % MOD_COUNT;\n        counts[val]++;\n    }\n\n    uint64_t result = 0;\n\n    // Count triples where all three numbers have the same remainder\n    for(int r = 0; r < MOD_COUNT; ++r){\n        uint64_t c = counts[r];\n        result += c * (c - 1) * (c - 2) / 6;\n    }\n\n    // Count triples where the remainders are distinct (0, 1, 2)\n    result += counts[0] * counts[1] * counts[2];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if (n < 2) return y;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i is_prime = _mm_set1_epi32(1);\n\n    int sqrt_n = (int)(sqrt((double)n));\n    for(int i=2; i<=sqrt_n; i++){\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i remainder = _mm_cvtepi32_epi64(_mm_and_si128(_mm_subs_epi32(_mm_mullo_epi32(v_n, v_i), _mm_mullo_epi32(v_i, v_i)), _mm_cmpeq_epi32(v_n, _mm_mullo_epi32(v_i, v_i))));\n        __m128i cmp_result = _mm_cmpeq_epi32(remainder, v_zero);\n        is_prime = _mm_and_si128(is_prime, cmp_result);\n    }\n\n    int mask = _mm_movemask_epi8(is_prime);\n    if(mask == 0xFFFF){\n        return x;\n    }\n    return y;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_zero = _mm_set1_epi32(0);\n    __m128i is_prime = _mm_set1_epi32(1);\n    int limit = (int)(_sqrt((double)n)) + 1;\n\n    for(int i=2; i < limit; i++){\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i remainder = _mm_cvtsi128_si32(_mm_rem_epu32(v_n, v_i));\n        __m128i cmp_result = _mm_cmpeq_epi32(_mm_set1_epi32(remainder), v_zero);\n        is_prime = _mm_andnot_si128(cmp_result, is_prime);\n        if(!_mm_test_all_ones(is_prime)){\n            return y;\n        }\n    }\n    return x;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n\n    __m128i vn = _mm_set1_epi32(n);\n    __m128i vone = _mm_set1_epi32(1);\n    __m128i vzero = _mm_setzero_si128();\n    __m128i is_prime = _mm_set1_epi32(1);\n\n    for(int i=2; i*i <=n; ++i){\n        __m128i vi = _mm_set1_epi32(i);\n        __m128i remainder = _mm_cvtepi32_epi64(_mm_and_si128(_mm_subs_epi32(vn, _mm_mullo_epi32(vi, _mm_div_epi32(vn, vi))), _mm_cmpeq_epi32(vi, vi)));\n        __m128i is_zero = _mm_cmpeq_epi64(remainder, vzero);\n        __m128i prime_mask = _mm_andnot_si128(is_zero, is_prime);\n        is_prime = prime_mask;\n    }\n\n    int mask = _mm_movemask_epi8(is_prime);\n    return (mask == 0xFFFF) ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if(n < 2) return y;\n    \n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_zero = _mm_setzero_si128();\n    __m128i v_x = _mm_set1_epi32(x);\n    __m128i v_y = _mm_set1_epi32(y);\n\n    int sqrt_n = (int)(sqrt((double)n));\n    int is_prime = 1;\n\n    for(int i=2; i<=sqrt_n; i++){\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i rem = _mm_cvtsi128_si32(_mm_subs_epu32(_mm_setzero_si128(), _mm_and_si128(_mm_cmplt_epi32(_mm_setzero_si128(), _mm_sub_epi32(v_n, _mm_mullo_epi32(v_i, _mm_div_epi32(v_n, v_i))), v_one)));\n        if(rem == 0){\n            is_prime = 0;\n            break;\n        }\n    }\n\n    return is_prime ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    if (n < 2) return y;\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_set1_epi32(0);\n\n    int sqrt_n = (int)(sqrt((double)n));\n    int is_prime = 1;\n\n    for(int i=2; i<=sqrt_n; i++){\n        __m128i v_i = _mm_set1_epi32(i);\n        __m128i remainder = _mm_cvtepi32_epi64(_mm_subs_epi32(v_n, _mm_mullo_epi32(v_i, _mm_div_epi32(v_n, v_i))));\n        __m128i cmp_result = _mm_cmpeq_epi32(remainder, zero);\n        int mask = _mm_movemask_epi8(cmp_result);\n        if(mask != 0){\n            is_prime = 0;\n            break;\n        }\n    }\n\n    __m128i v_x = _mm_set1_epi32(x);\n    __m128i v_y = _mm_set1_epi32(y);\n    __m128i selector = _mm_set1_epi32(is_prime);\n    __m128i result = _mm_or_si128(_mm_and_si128(selector, v_x), _mm_andnot_si128(selector, v_y));\n\n    return _mm_cvtsi128_si32(result);\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    if(lst.empty()) return 0;\n\n    const float* data = lst.data();\n    size_t size = lst.size();\n\n    __m128 zero = _mm_setzero_ps();\n    __m128i one = _mm_set1_epi32(1);\n    __m128 two = _mm_set1_ps(2.0f);\n\n    for(size_t i = 0; i + 4 <= size; i += 4){\n        __m128 vec = _mm_loadu_ps(data + i);\n        __m128 cmp = _mm_cmpge_ps(vec, zero); // Check >= 0\n        __m128i int_vec = _mm_cvtps_epi32(vec); // Convert to int\n        __m128i mod_mask = _mm_and_si128(_mm_sub_epi32(int_vec, one), _mm_set1_epi32(1)); // (val-1) & 1\n        __m128 mask = _mm_castsi128_ps(_mm_cmpeq_epi32(mod_mask, one)); // Check if odd\n        __m128 final_mask = _mm_and_ps(cmp, mask); // Combine masks\n        __m128 squared = _mm_mul_ps(vec, vec); // Square the values\n        __m128 result = _mm_and_ps(squared, final_mask); // Apply mask\n        __m128 doubled_diff = _mm_mul_ps(result, two); // Double the difference\n        sum += _mm_cvtss_f32(_mm_hadd_ps(_mm_hadd_ps(doubled_diff, doubled_diff), zero)); // Horizontal add and accumulate\n    }\n\n    for(size_t i = ((size / 4) * 4); i < size; ++i){\n        if(lst[i] >= 0 && static_cast<int>(lst[i]) % 2 == 1){\n            sum += 2 * lst[i] * lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    if(lst.empty()) return sum;\n\n    __m128i zero = _mm_setzero_si128();\n    __m128i two = _mm_set1_epi32(2);\n    __m128i one = _mm_set1_epi32(1);\n\n    size_t i = 0;\n    for(; i + 4 <= lst.size(); i += 4){\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        __m128i vi = _mm_cvtps_epi32(v); // Convert to integer\n        __m128i cmp = _mm_and_si128(_mm_cmpgt_epi32(vi, zero), _mm_cmpgt_epi32(vi, _mm_sub_epi32(vi, _mm_set1_epi32(1)))); // Check positive and integer\n        __m128i odd = _mm_and_si128(_mm_and_si128(cmp, _mm_srli_epi32(vi, 1)), _mm_set1_epi32(1)); // Check odd\n        __m128i squared = _mm_mullo_epi32(vi, vi); // Square the values\n        __m128i masked = _mm_and_si128(squared, _mm_castps_si128(_mm_castsi128_ps(odd))); // Mask out non-odd\n        sum += _mm_extract_epi32(masked, 0) + _mm_extract_epi32(masked, 1) + _mm_extract_epi32(masked, 2) + _mm_extract_epi32(masked, 3);\n    }\n\n    for(; i < lst.size(); ++i){\n        int val = static_cast<int>(lst[i]);\n        if(val > 0 && (val & 1)){\n            sum += val * val;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    if(lst.empty()) return 0;\n\n    __m128 zero = _mm_set1_ps(0.0f);\n    __m128 one = _mm_set1_ps(1.0f);\n    __m128 two = _mm_set1_ps(2.0f);\n\n    size_t i = 0;\n    for(; i + 3 < lst.size(); i += 4){\n        __m128 vec = _mm_loadu_ps(&lst[i]);\n        __m128 mask = _mm_cmpge_ps(vec, zero); // Check if >= 0\n        vec = _mm_and_ps(vec, mask); // Zero out negative numbers\n\n        // Compute floor and check if odd\n        __m128 floored = _mm_floor_ps(vec);\n        __m128 diff = _mm_sub_ps(floored, vec);\n        __m128 is_odd_mask = _mm_andnot_ps(_mm_cmpeq_ps(_mm_trunc_ps(diff), zero), mask);\n\n        // Square the values and sum if odd\n        __m128 squared = _mm_mul_ps(vec, vec);\n        __m128 squared_odd = _mm_and_ps(squared, is_odd_mask);\n\n        // Horizontal sum\n        float temp_sum[4];\n        _mm_storeu_ps(temp_sum, squared_odd);\n        for(int j = 0; j < 4; ++j){\n            if(temp_sum[j] > 0) sum += static_cast<uint64_t>(temp_sum[j]);\n        }\n    }\n\n    // Scalar processing for remaining elements\n    for(; i < lst.size(); ++i){\n        if(lst[i] >= 0 && static_cast<int>(lst[i]) % 2 == 1){\n            sum += static_cast<uint64_t>(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    if(lst.empty()) return 0;\n\n    size_t size = lst.size();\n    size_t aligned_size = size - (size % 4);\n\n    __m128 v_zero = _mm_set1_ps(0.0f);\n    __m128 v_half = _mm_set1_ps(0.5f);\n    __m128i v_one = _mm_set1_epi32(1);\n\n    __m128 sum_squares = _mm_setzero_ps();\n\n    for(size_t i = 0; i < aligned_size; i += 4){\n        __m128 v_data = _mm_loadu_ps(&lst[i]);\n        __m128 v_abs = _mm_andnot_ps(v_zero, v_data); // abs(x)\n        __m128 v_floor = _mm_cvtepi32_ps(_mm_cvttps_epi32(v_abs)); // floor(abs(x))\n        __m128 v_diff = _mm_sub_ps(v_abs, v_floor); // abs(x) - floor(abs(x))\n        __m128 v_cmp = _mm_cmple_ps(v_diff, v_half); // abs(x) - floor(abs(x)) <= 0.5\n        __m128i v_int = _mm_cvttps_epi32(v_abs); // int part of abs(x)\n        __m128i v_odd = _mm_and_si128(_mm_sub_epi32(v_int, v_one), _mm_set1_epi32(1)); // (int(abs(x)) - 1) & 1\n        __m128i v_mask = _mm_and_si128(_mm_castps_si128(v_cmp), v_odd); // mask for odd integers\n        __m128 v_sq = _mm_mul_ps(v_abs, v_abs); // abs(x)^2\n        __m128 v_masked_sq = _mm_and_ps(_mm_castsi128_ps(v_mask), v_sq); // mask squares\n        sum_squares = _mm_add_ps(sum_squares, v_masked_sq); // accumulate squares\n    }\n\n    float tmp[4] = {0};\n    _mm_storeu_ps(tmp, sum_squares);\n    sum += (uint64_t)(tmp[0] + tmp[1] + tmp[2] + tmp[3]);\n\n    for(size_t i = aligned_size; i < size; ++i){\n        float val = lst[i];\n        if(val > 0 && (int(val) & 1)){\n            sum += (uint64_t)(val * val);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if (lst.empty()) return 0;\n\n    uint64_t sum = 0;\n    size_t size = lst.size();\n    size_t aligned_size = size - (size % 4);\n\n    __m128 v_zero = _mm_set1_ps(0.0f);\n    __m128 v_two = _mm_set1_ps(2.0f);\n    __m128i v_one = _mm_set1_epi32(1);\n    __m128i v_mask = _mm_set1_epi32(0x7FFFFFFF); // Mask to ignore sign\n\n    __m128 sum_squares = _mm_setzero_ps();\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128 v_data = _mm_loadu_ps(&lst[i]);\n        __m128i v_data_int = _mm_cvtps_epi32(v_data); // Convert to int\n        __m128i v_abs_data = _mm_and_si128(v_data_int, v_mask); // Absolute value\n        __m128i v_odd_mask = _mm_and_si128(v_abs_data, v_one); // Check odd\n        __m128i v_odd = _mm_cmpeq_epi32(v_odd_mask, v_one); // Equal to 1\n        __m128 v_odd_float = _mm_castsi128_ps(_mm_and_si128(v_data_int, _mm_castps_si128(v_odd))); // Select odd\n        __m128 v_squared = _mm_mul_ps(v_odd_float, v_odd_float); // Square\n        sum_squares = _mm_add_ps(sum_squares, v_squared); // Accumulate squares\n    }\n\n    float sum_array[4];\n    _mm_storeu_ps(sum_array, sum_squares);\n    for (int i = 0; i < 4; ++i) {\n        sum += static_cast<uint64_t>(sum_array[i]);\n    }\n\n    for (size_t i = aligned_size; i < size; ++i) {\n        int val = static_cast<int>(lst[i]);\n        if (val >= 0 && (val & 1)) {\n            sum += static_cast<uint64_t>(val * val);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <immintrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t n = game.size();\n    std::vector<int> result(n, 0);\n\n    if (n == 0) return result;\n\n    size_t aligned_size = n - (n % 4);\n\n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128i v_game = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&game[i]));\n        __m128i v_guess = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&guess[i]));\n\n        __m128i diff = _mm_sub_epi32(v_game, v_guess);\n        __m128i abs_diff = _mm_or_si128(_mm_and_si128(diff, _mm_cmpgt_epi32(diff, _mm_setzero_si128())), \n                                        _mm_andnot_si128(_mm_cmpgt_epi32(diff, _mm_setzero_si128()), diff));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), abs_diff);\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = (diff > 0) ? diff : -diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t n = game.size();\n    std::vector<int> result(n, 0);\n\n    if (n == 0) return result;\n\n    size_t aligned_size = n - (n % 4);\n    \n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128i v_game = _mm_loadu_si128((__m128i*)&game[i]);\n        __m128i v_guess = _mm_loadu_si128((__m128i*)&guess[i]);\n\n        __m128i diff = _mm_sub_epi32(v_game, v_guess);\n        __m128i neg_diff = _mm_sub_epi32(v_guess, v_game);\n\n        __m128i mask = _mm_cmplt_epi32(diff, _mm_setzero_si128());\n        __m128i abs_diff = _mm_or_si128(_mm_and_si128(mask, neg_diff), _mm_andnot_si128(mask, diff));\n\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = (diff < 0) ? -diff : diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n\n    if (size == 0) return result;\n\n    alignas(16) int temp[4];\n    for (size_t i = 0; i < size; i += 4) {\n        __m128i g = _mm_loadu_si128((__m128i const*)&game[i]);\n        __m128i gu = _mm_loadu_si128((__m128i const*)&guess[i]);\n\n        __m128i diff = _mm_sub_epi32(g, gu);\n        __m128i neg_diff = _mm_sub_epi32(gu, g);\n\n        __m128i mask = _mm_cmplt_epi32(diff, _mm_setzero_si128());\n        __m128i abs_diff = _mm_or_si128(_mm_and_si128(mask, neg_diff), _mm_andnot_si128(mask, diff));\n\n        _mm_storeu_si128((__m128i*)&temp[0], abs_diff);\n\n        for (int j = 0; j < 4 && (i + j) < size; ++j) {\n            result[i + j] = temp[j];\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t size = game.size();\n    std::vector<int> result(size, 0);\n\n    if (size == 0) return result;\n\n    alignas(16) int temp[4];\n    __m128i zero = _mm_setzero_si128();\n\n    for (size_t i = 0; i < size; i += 4) {\n        size_t j = i + 4 <= size ? 4 : size - i;\n        \n        temp[0] = game[i];\n        temp[1] = (i+1 < size) ? game[i+1] : 0;\n        temp[2] = (i+2 < size) ? game[i+2] : 0;\n        temp[3] = (i+3 < size) ? game[i+3] : 0;\n\n        __m128i game_vec = _mm_load_si128((__m128i*)temp);\n\n        temp[0] = guess[i];\n        temp[1] = (i+1 < size) ? guess[i+1] : 0;\n        temp[2] = (i+2 < size) ? guess[i+2] : 0;\n        temp[3] = (i+3 < size) ? guess[i+3] : 0;\n\n        __m128i guess_vec = _mm_load_si128((__m128i*)temp);\n\n        __m128i diff = _mm_sub_epi32(game_vec, guess_vec);\n        __m128i abs_diff = _mm_or_si128(_mm_and_si128(diff, _mm_cmpgt_epi32(zero, diff)), \n                                        _mm_andnot_si128(_mm_cmpgt_epi32(zero, diff), diff));\n\n        _mm_store_si128((__m128i*)temp, abs_diff);\n\n        for (size_t k = 0; k < j; ++k) {\n            result[i + k] = temp[k];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t n = game.size();\n    std::vector<int> result(n, 0);\n\n    if (n == 0) return result;\n\n    size_t aligned_size = n - (n % 4);\n    \n    for (size_t i = 0; i < aligned_size; i += 4) {\n        __m128i v_game = _mm_loadu_si128((__m128i const*)(&game[i]));\n        __m128i v_guess = _mm_loadu_si128((__m128i const*)(&guess[i]));\n\n        __m128i diff = _mm_sub_epi32(v_game, v_guess);\n        __m128i abs_diff = _mm_or_si128(_mm_and_si128(diff, _mm_cmpgt_epi32(diff, _mm_setzero_si128())), \n                                        _mm_andnot_si128(_mm_cmpgt_epi32(diff, _mm_setzero_si128()), diff));\n\n        _mm_storeu_si128((__m128i*)&result[i], abs_diff);\n    }\n\n    for (size_t i = aligned_size; i < n; ++i) {\n        int diff = game[i] - guess[i];\n        result[i] = (diff > 0) ? diff : -diff;\n    }\n\n    return result;\n}\n```"}

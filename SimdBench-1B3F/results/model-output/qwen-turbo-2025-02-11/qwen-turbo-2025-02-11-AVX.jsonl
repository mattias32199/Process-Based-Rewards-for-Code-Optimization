{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_high = _mm_set1_epi32(0xFFFF0000);\n        __m128i mask_low = _mm_set1_epi32(0x0000FFFF);\n        __m128i mask_high_valid = _mm_andnot_si128(_mm_srli_epi32(mask_vec, 16), mask_high);\n        __m128i mask_low_valid = _mm_andnot_si128(_mm_slli_epi32(mask_vec, 16), mask_low);\n        __m128i combined_mask = _mm_or_si128(mask_high_valid, mask_low_valid);\n        __m128i dst_vec = _mm_or_si128(_mm_andnot_si128(combined_mask, _mm_loadu_si128((__m128i*)&dst[i])),\n                                        _mm_and_si128(combined_mask, src_vec));\n        _mm_storeu_si128((__m128i*)&dst[i], dst_vec);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_high = _mm_set1_epi32(0xFFFF0000);\n        __m128i mask_low = _mm_set1_epi32(0x0000FFFF);\n        __m128i mask_high_valid = _mm_andnot_si128(mask_high, mask_vec);\n        __m128i mask_low_valid = _mm_andnot_si128(mask_low, mask_vec);\n        __m128i dst_high = _mm_loadu_si128((__m128i*)&dst[i]);\n        __m128i dst_low = _mm_alignr_epi8(dst_high, dst_high, 8);\n        __m128i result_high = _mm_or_si128(_mm_andnot_si128(mask_high_valid, dst_high), _mm_and_si128(mask_high_valid, src_vec));\n        __m128i result_low = _mm_or_si128(_mm_andnot_si128(mask_low_valid, dst_low), _mm_and_si128(mask_low_valid, _mm_alignr_epi8(src_vec, src_vec, 8)));\n        _mm_storeu_si128((__m128i*)&dst[i], result_high);\n        _mm_storeu_si128((__m128i*)&dst[i + 2], result_low);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_high = _mm_set1_epi32(0xFFFF0000);\n        __m128i mask_low = _mm_set1_epi32(0x0000FFFF);\n\n        __m128i mask_high_valid = _mm_and_si128(mask_vec, _mm_castps_si128(_mm_castsi128_ps(mask_high)));\n        __m128i mask_low_valid = _mm_and_si128(mask_vec, _mm_castps_si128(_mm_castsi128_ps(mask_low)));\n\n        __m128i src_high = _mm_and_si128(src_vec, _mm_castps_si128(_mm_castsi128_ps(mask_high)));\n        __m128i src_low = _mm_and_si128(src_vec, _mm_castps_si128(_mm_castsi128_ps(mask_low)));\n\n        __m128i dst_high = _mm_or_si128(_mm_slli_epi32(src_high, 16), _mm_srli_epi32(dst[i + 1], 16));\n        __m128i dst_low = _mm_or_si128(_mm_slli_epi32(src_low, 16), _mm_srli_epi32(dst[i], 16));\n\n        _mm_storeu_si128((__m128i*)&dst[i], _mm_or_si128(dst_high, dst_low));\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_high = _mm_set1_epi32(0xFFFF0000);\n        __m128i mask_low = _mm_set1_epi32(0x0000FFFF);\n        __m128i mask_high_valid = _mm_and_si128(mask_vec, _mm_srli_epi32(mask_vec, 16));\n        __m128i mask_low_valid = _mm_and_si128(mask_vec, mask_low);\n        __m128i src_high = _mm_and_si128(src_vec, _mm_slli_epi32(mask_high_valid, 16));\n        __m128i src_low = _mm_and_si128(src_vec, _mm_slli_epi32(mask_low_valid, 16));\n        __m128i result = _mm_or_si128(src_high, src_low);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_high = _mm_set1_epi32(0xFFFF0000);\n        __m128i mask_low = _mm_set1_epi32(0x0000FFFF);\n        __m128i mask_high_valid = _mm_and_si128(mask_vec, _mm_srli_epi32(mask_vec, 16));\n        __m128i mask_low_valid = _mm_and_si128(mask_vec, mask_low);\n        __m128i dst_high = _mm_loadu_si128((__m128i*)&dst[i]);\n        __m128i dst_low = _mm_alignr_epi8(dst_high, dst_high, 8);\n        __m128i dst_high_updated = _mm_or_si128(_mm_andnot_si128(mask_high_valid, dst_high), _mm_and_si128(mask_high_valid, src_vec));\n        __m128i dst_low_updated = _mm_or_si128(_mm_andnot_si128(mask_low_valid, dst_low), _mm_and_si128(mask_low_valid, _mm_slli_epi32(src_vec, 16)));\n        _mm_storeu_si128((__m128i*)&dst[i], dst_high_updated);\n        _mm_storeu_si128((__m128i*)&dst[i + 2], dst_low_updated);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256i scale_shift_left = _mm256_slli_epi32(scale_vec, 3);\n    __m256i scale_shift_right = _mm256_srli_epi32(scale_vec, 3);\n    __m256i div_by_7 = _mm256_set1_epi32(7);\n\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mul_scale = _mm256_mullo_epi32(src_vec, scale_vec);\n        __m256i tmp1 = _mm256_srli_epi32(mul_scale, 3);\n        __m256i tmp2 = _mm256_slli_epi32(mul_scale, 3);\n        __m256i sum = _mm256_add_epi32(mul_scale, _mm256_mullo_epi32(tmp1, tmp2));\n        __m256i result = _mm256_div_epi32(sum, div_by_7);\n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; i++) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = ((element * scale) + (tmp1 * tmp2)) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    \n    // Process data in chunks of 8 integers\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Step 1: Calculate tmp1 = (element * scale) >> 3\n        __m256i tmp1_vec = _mm256_srai_epi32(_mm256_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 2: Calculate tmp2 = (element * scale) << 3\n        __m256i tmp2_vec = _mm256_slli_epi32(_mm256_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 3: Compute final result = (element * scale + tmp1 * tmp2) / 7\n        __m256i mul1_vec = _mm256_mul_epi32(src_vec, scale_vec);\n        __m256i mul2_vec = _mm256_mul_epi32(tmp1_vec, tmp2_vec);\n        __m256i add_vec = _mm256_add_epi32(mul1_vec, mul2_vec);\n        __m256i div_vec = _mm256_div_epi32(add_vec, _mm256_set1_epi32(7));\n        \n        // Store the result back into dst\n        _mm256_storeu_si256((__m256i*)&dst[i], div_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    __m256i scale_shift_left = _mm256_set1_epi32(scale << 3);\n    __m256i scale_shift_right = _mm256_set1_epi32(scale >> 3);\n    __m256i div_factor = _mm256_set1_epi32(7);\n\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i mul_scale = _mm256_mullo_epi32(src_vec, scale_vec);\n        __m256i tmp1 = _mm256_sra_epi32(mul_scale, _mm256_set1_epi32(3));\n        __m256i tmp2 = _mm256_sll_epi32(mul_scale, _mm256_set1_epi32(3));\n        __m256i mul_tmp1_tmp2 = _mm256_mullo_epi32(tmp1, tmp2);\n        __m256i sum = _mm256_add_epi32(mul_scale, mul_tmp1_tmp2);\n        __m256i result = _mm256_div_epu32(sum, div_factor);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        int res = ((element * scale) + (tmp1 * tmp2)) / 7;\n        dst[i] = res;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n    \n    // Process data in chunks of 8 integers (AVX2)\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Step 1: Calculate tmp1 = (src * scale) >> 3\n        __m256i tmp1_vec = _mm256_srai_epi32(_mm256_mullo_epi32(src_vec, scale_vec), 3);\n        \n        // Step 2: Calculate tmp2 = (src * scale) << 3\n        __m256i tmp2_vec = _mm256_slli_epi32(_mm256_mullo_epi32(src_vec, scale_vec), 3);\n        \n        // Step 3: Compute result = (src * scale + tmp1 * tmp2) / 7\n        __m256i mul1_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n        __m256i mul2_vec = _mm256_mullo_epi32(tmp1_vec, tmp2_vec);\n        __m256i add_vec = _mm256_add_epi32(mul1_vec, mul2_vec);\n        __m256i result_vec = _mm256_div_epi32(add_vec, _mm256_set1_epi32(7));\n        \n        // Store the result back into dst\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int src_val = src[i];\n        int tmp1 = (src_val * scale) >> 3;\n        int tmp2 = (src_val * scale) << 3;\n        dst[i] = (src_val * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    size_t n = length / 8 * 8; // Process 8 elements at a time\n\n    __m256i scale_vec = _mm256_set1_epi32(scale);\n\n    for (; i < n; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i scaled_vec = _mm256_mullo_epi32(src_vec, scale_vec);\n\n        __m256i tmp1_vec = _mm256_sra_epi32(scaled_vec, _mm256_set1_epi32(3));\n        __m256i tmp2_vec = _mm256_sll_epi32(scaled_vec, _mm256_set1_epi32(3));\n\n        __m256i mul_tmp_vec = _mm256_mullo_epi32(tmp1_vec, tmp2_vec);\n        __m256i add_vec = _mm256_add_epi32(scaled_vec, mul_tmp_vec);\n        __m256i div_vec = _mm256_div_epi32(add_vec, _mm256_set1_epi32(7));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), div_vec);\n    }\n\n    for (; i < length; i++) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    for(r = 0; r < rows; r += 4){\n        __m256d v1 = _mm256_setzero_pd();\n        __m256d v2 = _mm256_setzero_pd();\n        __m256d v3 = _mm256_setzero_pd();\n        __m256d v4 = _mm256_setzero_pd();\n\n        if(r + 0 < rows){\n            v1 = _mm256_loadu_pd(&src[(r + 0) * cols + stride]);\n        }\n        if(r + 1 < rows){\n            v2 = _mm256_loadu_pd(&src[(r + 1) * cols + stride]);\n        }\n        if(r + 2 < rows){\n            v3 = _mm256_loadu_pd(&src[(r + 2) * cols + stride]);\n        }\n        if(r + 3 < rows){\n            v4 = _mm256_loadu_pd(&src[(r + 3) * cols + stride]);\n        }\n\n        if(r + 0 < rows){\n            _mm256_storeu_pd(&dst[r + 0], v1);\n        }\n        if(r + 1 < rows){\n            _mm256_storeu_pd(&dst[r + 1], v2);\n        }\n        if(r + 2 < rows){\n            _mm256_storeu_pd(&dst[r + 2], v3);\n        }\n        if(r + 3 < rows){\n            _mm256_storeu_pd(&dst[r + 3], v4);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    for(r = 0; r < rows; r += 4){\n        __m256d v0 = _mm256_setzero_pd();\n        __m256d v1 = _mm256_setzero_pd();\n        if(r + 1 < rows){\n            v0 = _mm256_loadu_pd(&src[(r + 1) * cols + stride]);\n        }\n        if(r + 2 < rows){\n            v1 = _mm256_loadu_pd(&src[(r + 2) * cols + stride]);\n        }\n        if(r + 3 < rows){\n            __m256d v2 = _mm256_loadu_pd(&src[(r + 3) * cols + stride]);\n            _mm256_storeu_pd(&dst[r], v2);\n        }\n        if(r + 2 < rows){\n            _mm256_storeu_pd(&dst[r], v1);\n        }\n        if(r + 1 < rows){\n            _mm256_storeu_pd(&dst[r], v0);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    __m256d v_src, v_dst;\n\n    for(r = 0; r < rows; r += 4){\n        // Load 4 elements from src with stride\n        v_src = _mm256_set_pd(\n            *(src + (r+3)*cols + stride),\n            *(src + (r+2)*cols + stride),\n            *(src + (r+1)*cols + stride),\n            *(src + (r+0)*cols + stride)\n        );\n\n        // Store 4 elements into dst\n        _mm256_storeu_pd(dst + r, v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    for(r = 0; r < rows - 3; r += 4){\n        __m256d vec0 = _mm256_loadu_pd(&src[(r + 0) * cols + stride]);\n        __m256d vec1 = _mm256_loadu_pd(&src[(r + 1) * cols + stride]);\n        __m256d vec2 = _mm256_loadu_pd(&src[(r + 2) * cols + stride]);\n        __m256d vec3 = _mm256_loadu_pd(&src[(r + 3) * cols + stride]);\n\n        _mm256_storeu_pd(&dst[r + 0], vec0);\n        _mm256_storeu_pd(&dst[r + 1], vec1);\n        _mm256_storeu_pd(&dst[r + 2], vec2);\n        _mm256_storeu_pd(&dst[r + 3], vec3);\n    }\n    for(; r < rows; r++){\n        dst[r] = src[r * cols + stride];\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    __m256d v_src, v_dst;\n\n    for(r = 0; r < rows; r += 4){\n        // Load 4 elements from src with stride\n        v_src = _mm256_set_pd(\n            *(src + (r+3)*cols + stride),\n            *(src + (r+2)*cols + stride),\n            *(src + (r+1)*cols + stride),\n            *(src + (r+0)*cols + stride)\n        );\n\n        // Store 4 elements into dst\n        _mm256_storeu_pd(dst + r, v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 src_vec = _mm256_loadu_ps(&src[indices[i + 0]]);\n        src_vec = _mm256_insertf128_ps(src_vec, _mm256_loadu_ps(&src[indices[i + 4]]), 1);\n        \n        _mm256_storeu_ps(&dst[indices[i + 0]], src_vec);\n        _mm256_storeu_ps(&dst[indices[i + 4]], src_vec);\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 src_vec = _mm256_loadu_ps(&src[indices[i + 0]]);\n        src_vec = _mm256_insertf128_ps(src_vec, _mm256_loadu_ps(&src[indices[i + 4]]), 1);\n        \n        _mm256_storeu_ps(&dst[indices[i + 0]], src_vec);\n        _mm256_storeu_ps(&dst[indices[i + 4]], src_vec);\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 src_vec = _mm256_loadu_ps(&src[indices[i + 0]]);\n        src_vec = _mm256_insertf128_ps(src_vec, _mm256_loadu_ps(&src[indices[i + 4]]), 1);\n        \n        _mm256_storeu_ps(&dst[indices[i + 0]], src_vec);\n        _mm256_storeu_ps(&dst[indices[i + 4]], src_vec);\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 src_vec = _mm256_loadu_ps(&src[indices[i + 0]]);\n        src_vec = _mm256_insertf128_ps(src_vec, _mm256_loadu_ps(&src[indices[i + 4]]), 1);\n        \n        _mm256_storeu_ps(&dst[indices[i + 0]], src_vec);\n        _mm256_storeu_ps(&dst[indices[i + 4]], src_vec);\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256 src_vec = _mm256_loadu_ps(&src[indices[i + 0]]);\n        src_vec = _mm256_insertf128_ps(src_vec, _mm256_loadu_ps(&src[indices[i + 4]]), 1);\n        \n        _mm256_storeu_ps(&dst[indices[i + 0]], src_vec);\n        _mm256_storeu_ps(&dst[indices[i + 4]], src_vec);\n    }\n    for (; i < length; i++) {\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    // Align start index to 16 bytes boundary\n    int16_t *aligned_start = arr + (start & ~0xF);\n    int16_t *aligned_end = arr + (end & ~0xF);\n\n    // Reverse aligned blocks\n    while (aligned_start + 32 <= aligned_end) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(aligned_start));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(aligned_start + 16));\n        __m256i reversed_v1 = _mm256_permute4x64_epi64(v1, 0b10011000);\n        __m256i reversed_v2 = _mm256_permute4x64_epi64(v2, 0b10011000);\n        _mm256_storeu_si256((__m256i*)(aligned_start), reversed_v2);\n        _mm256_storeu_si256((__m256i*)(aligned_start + 16), reversed_v1);\n        aligned_start += 32;\n    }\n\n    // Handle remaining elements\n    while (aligned_start < aligned_end) {\n        int16_t temp = *aligned_start;\n        *aligned_start = *(aligned_end);\n        *(aligned_end) = temp;\n        aligned_start++;\n        aligned_end--;\n    }\n\n    // Reverse unaligned prefix\n    if ((end & 0xF) != 0) {\n        int16_t *unaligned_start = arr + start;\n        int16_t *unaligned_end = arr + end;\n        while (unaligned_start < unaligned_end) {\n            int16_t temp = *unaligned_start;\n            *unaligned_start = *unaligned_end;\n            *unaligned_end = temp;\n            unaligned_start++;\n            unaligned_end--;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    // Align start index to 16 bytes boundary\n    int16_t *aligned_start = arr + (start & ~0xF);\n    int16_t *aligned_end = arr + (end & ~0xF);\n\n    // Load initial misaligned data if needed\n    int16_t *current = aligned_start;\n    __m256i pre_data = _mm256_setzero_si256();\n    int pre_count = 0;\n    if (current > arr + start) {\n        pre_count = current - (arr + start);\n        pre_data = _mm256_loadu_si256((__m256i *)(arr + start));\n    }\n\n    // Load final misaligned data if needed\n    __m256i post_data = _mm256_setzero_si256();\n    int post_count = 0;\n    if (aligned_end < arr + end) {\n        post_count = (arr + end) - aligned_end;\n        post_data = _mm256_loadu_si256((__m256i *)aligned_end);\n    }\n\n    // Reverse aligned data\n    while (aligned_start <= aligned_end) {\n        __m256i data = _mm256_load_si256((__m256i *)aligned_start);\n        __m256i reversed = _mm256_permute4x64_epi64(data, 0b10010000);\n        _mm256_store_si256((__m256i *)aligned_start, reversed);\n        aligned_start += 8;\n    }\n\n    // Handle pre and post data\n    if (pre_count > 0) {\n        __m256i reversed_pre = _mm256_permute4x64_epi64(pre_data, 0b10010000);\n        _mm256_maskstore_epi16(arr + start, _mm256_set1_epi16(0xFFFF & ((1 << pre_count) - 1)), reversed_pre);\n    }\n    if (post_count > 0) {\n        __m256i reversed_post = _mm256_permute4x64_epi64(post_data, 0b10010000);\n        _mm256_maskstore_epi16(arr + end - post_count + 1, _mm256_set1_epi16(0xFFFF & ((1 << post_count) - 1)), reversed_post);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    // Align start index to 16 bytes boundary\n    int16_t *aligned_start = &arr[start];\n    int16_t *aligned_end = &arr[end];\n\n    // Load initial misaligned data if needed\n    int16_t *current = aligned_start;\n    __m256i vtemp = _mm256_setzero_si256();\n\n    if ((uintptr_t)current % 32 == 0) {\n        // Fully aligned case\n        while ((uintptr_t)current + 32 <= (uintptr_t)aligned_end - 32) {\n            __m256i v1 = _mm256_loadu_si256((__m256i *)(current));\n            __m256i v2 = _mm256_loadu_si256((__m256i *)(current + 16));\n            _mm256_storeu_si256((__m256i *)(current), _mm256_permute4x64_epi64(v2, 0x96));\n            _mm256_storeu_si256((__m256i *)(current + 16), _mm256_permute4x64_epi64(v1, 0x96));\n            current += 32;\n        }\n    } else {\n        // Handle misalignment at the start\n        size_t misalign = (uintptr_t)current % 32;\n        size_t load_bytes = 32 - misalign;\n        __m256i vstart = _mm256_maskload_epi16(current, _mm256_set1_epi32(0xFFFF & ~((1 << misalign) - 1)));\n        __m256i vend = _mm256_maskload_epi16(current + 16, _mm256_set1_epi32(0xFFFF & ~((1 << misalign) - 1)));\n\n        // Reverse the first vector\n        __m256i vrev_start = _mm256_permute4x64_epi64(vstart, 0x96);\n        __m256i vrev_end = _mm256_permute4x64_epi64(vend, 0x96);\n\n        // Store back reversed values\n        _mm256_maskstore_epi16(current, _mm256_set1_epi32(0xFFFF & ~((1 << misalign) - 1)), vrev_start);\n        _mm256_maskstore_epi16(current + 16, _mm256_set1_epi32(0xFFFF & ~((1 << misalign) - 1)), vrev_end);\n\n        current += 32 - misalign;\n    }\n\n    // Reverse remaining elements if any\n    while (current + 16 <= aligned_end) {\n        __m256i v1 = _mm256_loadu_si256((__m256i *)(current));\n        __m256i vrev = _mm256_permute4x64_epi64(v1, 0x96);\n        _mm256_storeu_si256((__m256i *)(current), vrev);\n        current += 16;\n    }\n\n    // Handle remaining single elements if any\n    while (current < aligned_end) {\n        int16_t temp = *current;\n        *(current++) = *(aligned_end);\n        *(aligned_end--) = temp;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    // Align start index to 16 bytes boundary\n    int16_t *aligned_start = arr + (start & ~0xF);\n    int16_t *aligned_end = arr + (end & ~0xF);\n\n    // Reverse aligned part using AVX2\n    __m256i *ptr = (__m256i*)aligned_start;\n    __m256i *end_ptr = (__m256i*)aligned_end;\n    while (ptr < end_ptr) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        __m256i reversed = _mm256_permute4x64_epi64(vec, 0b10011000);\n        _mm256_storeu_si256(ptr, reversed);\n        ptr++;\n    }\n\n    // Handle remaining elements that are not 32 items aligned\n    int16_t *last_ptr = arr + end;\n    while (last_ptr > aligned_end) {\n        last_ptr--;\n        *(last_ptr) = *(arr + start + (last_ptr - aligned_end));\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    if (start >= end) return;\n\n    // Align start index to 16 bytes boundary\n    int16_t *aligned_start = arr + (start & ~0xF);\n    int16_t *aligned_end = arr + (end & ~0xF);\n\n    // Reverse aligned part\n    __m256i *ptr = (__m256i *)aligned_start;\n    __m256i *end_ptr = (__m256i *)aligned_end;\n    while (ptr < end_ptr) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        __m256i reversed_vec = _mm256_permute4x64_epi64(vec, 0b10011000);\n        _mm256_storeu_si256(ptr, reversed_vec);\n        ptr++;\n    }\n\n    // Handle remaining elements\n    int16_t *last_ptr = arr + end;\n    int16_t *first_ptr = arr + start;\n    while (first_ptr < last_ptr) {\n        int16_t temp = *first_ptr;\n        *first_ptr = *last_ptr;\n        *last_ptr = temp;\n        first_ptr++;\n        last_ptr--;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi8(1);\n    \n    for (; i + 31 < dim1 * dim2; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(tensor + i * dim3 + slice_idx));\n        _mm256_storeu_si256((__m256i*)(slice + i), data);\n    }\n    \n    for (; i < dim1 * dim2; ++i) {\n        slice[i] = tensor[i * dim3 + slice_idx];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi8(1);\n    \n    for (; i + 31 < dim1 * dim2; i += 32) {\n        __m256i vec = _mm256_i32gather_epi32((const int*)tensor, (int)(i / dim3 + slice_idx * dim1), 1);\n        _mm256_maskstore_epi32((int*)slice + i, mask, vec);\n    }\n    \n    for (; i < dim1 * dim2; ++i) {\n        slice[i] = tensor[i / dim3 + slice_idx * dim1];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi8(1);\n    \n    for (; i + 31 < dim1 * dim2; i += 32) {\n        __m256i data = _mm256_loadu_si256((__m256i*)(tensor + i * dim3 + slice_idx));\n        _mm256_storeu_si256((__m256i*)(slice + i), data);\n    }\n    \n    for (; i < dim1 * dim2; i++) {\n        slice[i] = tensor[i * dim3 + slice_idx];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t vector_width = 32; // AVX width for uint8_t\n    size_t i = 0;\n\n    // Ensure alignment for AVX operations\n    __m256i zero_vector = _mm256_setzero_si256();\n\n    for (; i + vector_width <= dim1 * dim2; i += vector_width) {\n        __m256i indices = _mm256_set_epi32(\n            (i + 7) * dim3 + slice_idx,\n            (i + 6) * dim3 + slice_idx,\n            (i + 5) * dim3 + slice_idx,\n            (i + 4) * dim3 + slice_idx,\n            (i + 3) * dim3 + slice_idx,\n            (i + 2) * dim3 + slice_idx,\n            (i + 1) * dim3 + slice_idx,\n            i * dim3 + slice_idx\n        );\n\n        __m256i values = _mm256_i32gather_epi32((const int *)tensor, indices, 1);\n\n        _mm256_storeu_si256((__m256i *)(slice + i), values);\n    }\n\n    // Handle remaining elements\n    for (; i < dim1 * dim2; i++) {\n        slice[i] = tensor[i * dim3 + slice_idx];\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t vector_width = 32; // AVX: 256 bits = 32 bytes\n    size_t i = 0;\n\n    // Ensure alignment for AVX operations\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (; i + vector_width <= dim1 * dim2; i += vector_width) {\n        __m256i data_vec;\n        size_t offset = (i + slice_idx * dim3) * sizeof(uint8_t);\n\n        // Load data into AVX register\n        data_vec = _mm256_loadu_si256((__m256i*)(tensor + offset));\n\n        // Store data into the slice\n        _mm256_storeu_si256((__m256i*)(slice + i), data_vec);\n    }\n\n    // Handle remaining elements that don't fit into a full vector\n    for (; i < dim1 * dim2; ++i) {\n        size_t offset = (i + slice_idx * dim3) * sizeof(uint8_t);\n        slice[i] = tensor[offset];\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const size_t block_rows = block_size;\n    const size_t block_cols = block_size;\n\n    for (size_t br = 0; br < rows; br += block_rows) {\n        for (size_t bc = 0; bc < cols; bc += block_cols) {\n            for (size_t i = 0; i < block_rows; ++i) {\n                for (size_t j = 0; j < block_cols; j += 8) {\n                    __m256i row0 = _mm256_setzero_si256();\n                    __m256i row1 = _mm256_setzero_si256();\n                    __m256i row2 = _mm256_setzero_si256();\n                    __m256i row3 = _mm256_setzero_si256();\n\n                    for (size_t k = 0; k < 8; ++k) {\n                        if (bc + j + k < cols && br + i < rows) {\n                            int16_t val = src[(br + i) * cols + (bc + j + k)];\n                            row0 = _mm256_insert_epi16(row0, val, k);\n                        }\n                    }\n\n                    _mm256_storeu_si256((__m256i *)(dst + (bc + 0) * rows + (br + i)), row0);\n\n                    for (size_t k = 0; k < 8; ++k) {\n                        if (bc + j + k + 8 < cols && br + i < rows) {\n                            int16_t val = src[(br + i) * cols + (bc + j + k + 8)];\n                            row1 = _mm256_insert_epi16(row1, val, k);\n                        }\n                    }\n\n                    _mm256_storeu_si256((__m256i *)(dst + (bc + 8) * rows + (br + i)), row1);\n\n                    for (size_t k = 0; k < 8; ++k) {\n                        if (bc + j + k + 16 < cols && br + i < rows) {\n                            int16_t val = src[(br + i) * cols + (bc + j + k + 16)];\n                            row2 = _mm256_insert_epi16(row2, val, k);\n                        }\n                    }\n\n                    _mm256_storeu_si256((__m256i *)(dst + (bc + 16) * rows + (br + i)), row2);\n\n                    for (size_t k = 0; k < 8; ++k) {\n                        if (bc + j + k + 24 < cols && br + i < rows) {\n                            int16_t val = src[(br + i) * cols + (bc + j + k + 24)];\n                            row3 = _mm256_insert_epi16(row3, val, k);\n                        }\n                    }\n\n                    _mm256_storeu_si256((__m256i *)(dst + (bc + 24) * rows + (br + i)), row3);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k;\n    size_t blocks = rows / block_size;\n    __m256i row0, row1, row2, row3, row4, row5, row6, row7;\n\n    for (i = 0; i < blocks; ++i) {\n        for (j = 0; j < blocks; ++j) {\n            for (k = 0; k < block_size; k += 8) {\n                row0 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k)*cols + j*block_size]);\n                row1 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k + 1)*cols + j*block_size]);\n                row2 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k + 2)*cols + j*block_size]);\n                row3 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k + 3)*cols + j*block_size]);\n                row4 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k + 4)*cols + j*block_size]);\n                row5 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k + 5)*cols + j*block_size]);\n                row6 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k + 6)*cols + j*block_size]);\n                row7 = _mm256_loadu_si256((__m256i*)&src[(i*block_size + k + 7)*cols + j*block_size]);\n\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k)*rows + i*block_size], row0);\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k + 1)*rows + i*block_size], row1);\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k + 2)*rows + i*block_size], row2);\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k + 3)*rows + i*block_size], row3);\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k + 4)*rows + i*block_size], row4);\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k + 5)*rows + i*block_size], row5);\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k + 6)*rows + i*block_size], row6);\n                _mm256_storeu_si256((__m256i*)&dst[(j*block_size + k + 7)*rows + i*block_size], row7);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const size_t block_rows = block_size;\n    const size_t block_cols = block_size;\n\n    for (size_t br = 0; br < rows; br += block_rows) {\n        for (size_t bc = 0; bc < cols; bc += block_cols) {\n            for (size_t i = 0; i < block_rows; ++i) {\n                for (size_t j = 0; j < block_cols; j += 16) {\n                    __m256i row0 = _mm256_setzero_si256();\n                    __m256i row1 = _mm256_setzero_si256();\n                    __m256i row2 = _mm256_setzero_si256();\n                    __m256i row3 = _mm256_setzero_si256();\n                    __m256i row4 = _mm256_setzero_si256();\n                    __m256i row5 = _mm256_setzero_si256();\n                    __m256i row6 = _mm256_setzero_si256();\n                    __m256i row7 = _mm256_setzero_si256();\n\n                    size_t col_idx = (br + i) * cols + (bc + j);\n                    size_t src_idx = br * cols + (bc + j);\n\n                    if (j + 0 < block_cols) row0 = _mm256_loadu_si256((__m256i*)&src[src_idx + 0]);\n                    if (j + 1 < block_cols) row1 = _mm256_loadu_si256((__m256i*)&src[src_idx + 1]);\n                    if (j + 2 < block_cols) row2 = _mm256_loadu_si256((__m256i*)&src[src_idx + 2]);\n                    if (j + 3 < block_cols) row3 = _mm256_loadu_si256((__m256i*)&src[src_idx + 3]);\n                    if (j + 4 < block_cols) row4 = _mm256_loadu_si256((__m256i*)&src[src_idx + 4]);\n                    if (j + 5 < block_cols) row5 = _mm256_loadu_si256((__m256i*)&src[src_idx + 5]);\n                    if (j + 6 < block_cols) row6 = _mm256_loadu_si256((__m256i*)&src[src_idx + 6]);\n                    if (j + 7 < block_cols) row7 = _mm256_loadu_si256((__m256i*)&src[src_idx + 7]);\n\n                    size_t dst_idx = (bc + i) * cols + (br + j);\n\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 0], row0);\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 8], row1);\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 16], row2);\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 24], row3);\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 32], row4);\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 40], row5);\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 48], row6);\n                    _mm256_storeu_si256((__m256i*)&dst[dst_idx + 56], row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t br = 0; br < rows; br += block_size) {\n        for (size_t bc = 0; bc < cols; bc += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                if (br + i >= rows) continue;\n                for (size_t j = 0; j < block_size; j += 8) {\n                    if (bc + j + 7 >= cols) break;\n                    __m256i row0 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j)));\n                    __m256i row1 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j + 1)));\n                    __m256i row2 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j + 2)));\n                    __m256i row3 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j + 3)));\n                    __m256i row4 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j + 4)));\n                    __m256i row5 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j + 5)));\n                    __m256i row6 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j + 6)));\n                    __m256i row7 = _mm256_loadu_si256((__m256i*)(src + (br + i)*cols + (bc + j + 7)));\n\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j)*rows + (br + i)), row0);\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j + 1)*rows + (br + i)), row1);\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j + 2)*rows + (br + i)), row2);\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j + 3)*rows + (br + i)), row3);\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j + 4)*rows + (br + i)), row4);\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j + 5)*rows + (br + i)), row5);\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j + 6)*rows + (br + i)), row6);\n                    _mm256_storeu_si256((__m256i*)(dst + (bc + j + 7)*rows + (br + i)), row7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t r, c;\n    size_t block_rows = block_size;\n    size_t block_cols = block_size;\n\n    for (r = 0; r < rows; r += block_rows) {\n        for (c = 0; c < cols; c += block_cols) {\n            size_t br, bc;\n            size_t end_row = (r + block_rows > rows) ? rows : r + block_rows;\n            size_t end_col = (c + block_cols > cols) ? cols : c + block_cols;\n            size_t num_rows = end_row - r;\n            size_t num_cols = end_col - c;\n\n            __m256i row0, row1, row2, row3, row4, row5, row6, row7;\n            __m256i col0, col1, col2, col3, col4, col5, col6, col7;\n\n            for (br = r; br < end_row; br += 8) {\n                size_t current_col = c;\n                for (bc = c; current_col < end_col; current_col += 8) {\n                    row0 = _mm256_loadu_si256((__m256i*)&src[br * cols + current_col]);\n                    row1 = _mm256_loadu_si256((__m256i*)&src[(br + 1) * cols + current_col]);\n                    row2 = _mm256_loadu_si256((__m256i*)&src[(br + 2) * cols + current_col]);\n                    row3 = _mm256_loadu_si256((__m256i*)&src[(br + 3) * cols + current_col]);\n                    row4 = _mm256_loadu_si256((__m256i*)&src[(br + 4) * cols + current_col]);\n                    row5 = _mm256_loadu_si256((__m256i*)&src[(br + 5) * cols + current_col]);\n                    row6 = _mm256_loadu_si256((__m256i*)&src[(br + 6) * cols + current_col]);\n                    row7 = _mm256_loadu_si256((__m256i*)&src[(br + 7) * cols + current_col]);\n\n                    col0 = _mm256_unpacklo_epi16(row0, row1);\n                    col1 = _mm256_unpackhi_epi16(row0, row1);\n                    col2 = _mm256_unpacklo_epi16(row2, row3);\n                    col3 = _mm256_unpackhi_epi16(row2, row3);\n                    col4 = _mm256_unpacklo_epi16(row4, row5);\n                    col5 = _mm256_unpackhi_epi16(row4, row5);\n                    col6 = _mm256_unpacklo_epi16(row6, row7);\n                    col7 = _mm256_unpackhi_epi16(row6, row7);\n\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br], col0);\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br + 1], col1);\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br + 2], col2);\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br + 3], col3);\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br + 4], col4);\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br + 5], col5);\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br + 6], col6);\n                    _mm256_storeu_si256((__m256i*)&dst[current_col * rows + br + 7], col7);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    for (; i <= dim - 8; i += 8) {\n        __m256 v = _mm256_loadu_ps(&array[i * (dim * dim + dim + 1)]);\n        __m256 sum_vec = _mm256_set1_ps(0.0f);\n        sum_vec = _mm256_add_ps(sum_vec, v);\n        float temp[8];\n        _mm256_storeu_ps(temp, sum_vec);\n        for (size_t j = 0; j < 8; j++) {\n            sum += temp[j];\n        }\n    }\n\n    for (; i < dim; i++) {\n        sum += array[i * (dim * dim + dim + 1)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // Ensure alignment for AVX\n    if (((size_t)array) % 32 == 0 && dim >= 8) {\n        __m256 vsum = _mm256_setzero_ps();\n        for (; i + 7 < dim; i += 8) {\n            __m256 vdata = _mm256_load_ps(array + i * (dim + 1));\n            vsum = _mm256_add_ps(vsum, vdata);\n        }\n        __m256 temp = _mm256_hadd_ps(vsum, vsum);\n        temp = _mm256_hadd_ps(temp, temp);\n        sum += _mm256_cvtss_f32(temp);\n    }\n\n    // Handle remaining elements\n    for (; i < dim; ++i) {\n        sum += array[i * (dim + 1)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // Ensure alignment for AVX\n    if (((size_t)(array) % 32) == 0 && dim >= 8) {\n        __m256 vsum = _mm256_setzero_ps();\n        for (; i + 7 < dim; i += 8) {\n            __m256 vdata = _mm256_load_ps(array + i * (dim + 1));\n            vsum = _mm256_add_ps(vsum, vdata);\n        }\n        __m256 temp = _mm256_hadd_ps(vsum, vsum);\n        temp = _mm256_hadd_ps(temp, temp);\n        sum += _mm256_cvtss_f32(temp);\n    }\n\n    // Handle remaining elements\n    for (; i < dim; ++i) {\n        sum += array[i * (dim + 1)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // Ensure alignment for AVX loads\n    if (((size_t)array) % 32 == 0 && dim >= 8) {\n        __m256 vsum = _mm256_setzero_ps();\n        for (; i + 7 < dim; i += 8) {\n            __m256 vdata = _mm256_load_ps(array + i * (dim + 1));\n            vsum = _mm256_add_ps(vsum, vdata);\n        }\n        __m256 hsum = _mm256_hadd_ps(vsum, vsum);\n        __m128 hsum_low = _mm256_castps256_ps128(hsum);\n        __m128 hsum_high = _mm256_extractf128_ps(hsum, 1);\n        hsum_low = _mm_add_ps(hsum_low, hsum_high);\n        sum = _mm_cvtss_f32(hsum_low);\n    }\n\n    // Handle remaining elements\n    for (; i < dim; ++i) {\n        sum += array[i * (dim + 1)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    float sum = 0.0f;\n    size_t i = 0;\n\n    // Ensure alignment for AVX\n    if (((size_t)array) % 32 == 0 && dim >= 8) {\n        __m256 vsum = _mm256_setzero_ps();\n        for (; i + 7 < dim; i += 8) {\n            __m256 vdata = _mm256_load_ps(&array[i * (dim * 3 + 3)]);\n            vsum = _mm256_add_ps(vsum, vdata);\n        }\n        __m128 vsum_low = _mm256_castps256_ps128(vsum);\n        __m128 vsum_high = _mm256_extractf128_ps(vsum, 1);\n        vsum_low = _mm_add_ps(vsum_low, vsum_high);\n        __m128 vsum_final = _mm_hadd_ps(vsum_low, vsum_low);\n        __m128 vsum_final2 = _mm_hadd_ps(vsum_final, vsum_final);\n        sum += _mm_cvtss_f32(vsum_final2);\n    }\n\n    for (; i < dim; ++i) {\n        sum += array[i * (dim * 3 + 3)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    __m256d mask, val, scaled_val;\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            val = _mm256_loadu_pd(&src[i * cols + j]);\n            mask = _mm256_cmpgt_pd(val, threshold_vec);\n            scaled_val = _mm256_mul_pd(val, scale_vec);\n            val = _mm256_blendv_pd(scaled_val, val, mask);\n            _mm256_storeu_pd(&dst[i * cols + j], val);\n        }\n        for (; j < cols; ++j) {\n            dst[i * cols + j] = (src[i * cols + j] > threshold) ? (src[i * cols + j] * scale) : src[i * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    __m256d mask, val, scaled_val;\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            val = _mm256_loadu_pd(&src[i * cols + j]);\n            mask = _mm256_cmp_pd(val, threshold_vec, _CMP_GT_OS);\n            scaled_val = _mm256_blendv_pd(val, _mm256_mul_pd(val, scale_vec), mask);\n            _mm256_storeu_pd(&dst[i * cols + j], scaled_val);\n        }\n        for (; j < cols; ++j) {\n            dst[i * cols + j] = (src[i * cols + j] > threshold) ? (src[i * cols + j] * scale) : src[i * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    __m256d mask, scaled_value, original_value;\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[i * cols + j]);\n            mask = _mm256_cmpgt_pd(src_vec, threshold_vec);\n            scaled_value = _mm256_mul_pd(src_vec, scale_vec);\n            original_value = src_vec;\n            __m256d result = _mm256_blendv_pd(original_value, scaled_value, mask);\n            _mm256_storeu_pd(&dst[i * cols + j], result);\n        }\n        for (; j < cols; ++j) {\n            dst[i * cols + j] = (src[i * cols + j] > threshold) ? (src[i * cols + j] * scale) : src[i * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    __m256d mask, true_val, false_val;\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[i * cols + j]);\n            mask = _mm256_cmpgt_pd(src_vec, threshold_vec);\n            true_val = _mm256_mul_pd(src_vec, scale_vec);\n            false_val = src_vec;\n            __m256d result = _mm256_blendv_pd(false_val, true_val, mask);\n            _mm256_storeu_pd(&dst[i * cols + j], result);\n        }\n        for (; j < cols; j++) {\n            dst[i * cols + j] = (src[i * cols + j] > threshold) ? (src[i * cols + j] * scale) : src[i * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m256d threshold_vec = _mm256_set1_pd(threshold);\n    __m256d scale_vec = _mm256_set1_pd(scale);\n    __m256d mask, val, scaled_val;\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            val = _mm256_loadu_pd(src + i * cols + j);\n            mask = _mm256_cmpgt_pd(val, threshold_vec);\n            scaled_val = _mm256_mul_pd(val, scale_vec);\n            val = _mm256_blendv_pd(val, scaled_val, mask);\n            _mm256_storeu_pd(dst + i * cols + j, val);\n        }\n        for (; j < cols; ++j) {\n            dst[i * cols + j] = src[i * cols + j] > threshold ? src[i * cols + j] * scale : src[i * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    for (; i <= rows - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i*)&indices[i]);\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d src_vec = _mm256_setzero_pd();\n            if (j + 3 < cols) {\n                src_vec = _mm256_loadu_pd(&src[idx_vec[j / 4] * cols + j]);\n            }\n            if (j + 1 < cols) {\n                __m256d src_vec2 = _mm256_loadu_pd(&src[idx_vec[j / 4] * cols + j + 1]);\n                src_vec = _mm256_insertf128_pd(src_vec, src_vec2, 1);\n            }\n            _mm256_storeu_pd(&dst[i * cols + j], src_vec);\n        }\n    }\n    for (; i < rows; i++) {\n        size_t src_row = indices[i];\n        for (size_t j = 0; j < cols; j++) {\n            dst[i * cols + j] = src[src_row * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    for (; i <= rows - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i*)&indices[i]);\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[(i + idx_vec[0]) * cols + j]);\n            src_vec = _mm256_insertf128_pd(src_vec, _mm_load_sd(&src[(i + idx_vec[1]) * cols + j + 2]), 1);\n            src_vec = _mm256_insertf128_pd(src_vec, _mm256_broadcast_sd(&src[(i + idx_vec[2]) * cols + j + 4]), 2);\n            src_vec = _mm256_insertf128_pd(src_vec, _mm256_broadcast_sd(&src[(i + idx_vec[3]) * cols + j + 6]), 3);\n            _mm256_storeu_pd(&dst[i * cols + j], src_vec);\n        }\n    }\n    for (; i < rows; ++i) {\n        size_t idx = indices[i];\n        for (size_t j = 0; j < cols; ++j) {\n            dst[i * cols + j] = src[idx * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    for (; i <= rows - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i*)&indices[i]);\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d src_vec0 = _mm256_loadu_pd(&src[idx_vec[0] * cols + j]);\n            __m256d src_vec1 = _mm256_loadu_pd(&src[idx_vec[1] * cols + j]);\n            __m256d src_vec2 = _mm256_loadu_pd(&src[idx_vec[2] * cols + j]);\n            __m256d src_vec3 = _mm256_loadu_pd(&src[idx_vec[3] * cols + j]);\n            _mm256_storeu_pd(&dst[i * cols + j], src_vec0);\n            _mm256_storeu_pd(&dst[i * cols + j + 2], src_vec1);\n            _mm256_storeu_pd(&dst[i * cols + j + 4], src_vec2);\n            _mm256_storeu_pd(&dst[i * cols + j + 6], src_vec3);\n        }\n    }\n    for (; i < rows; ++i) {\n        size_t idx = indices[i];\n        for (size_t j = 0; j < cols; ++j) {\n            dst[i * cols + j] = src[idx * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    for (; i <= rows - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i*)&indices[i]);\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d src_vec = _mm256_loadu_pd(&src[(i + _mm256_extract_epi32(idx_vec, 0)) * cols + j]);\n            src_vec = _mm256_insertf128_pd(src_vec, _mm_load_sd(&src[(i + _mm256_extract_epi32(idx_vec, 1)) * cols + j]), 1);\n            src_vec = _mm256_insertf128_pd(src_vec, _mm256_castpd128_pd256(_mm_load_sd(&src[(i + _mm256_extract_epi32(idx_vec, 2)) * cols + j])), 2);\n            src_vec = _mm256_insertf128_pd(src_vec, _mm256_castpd128_pd256(_mm_load_sd(&src[(i + _mm256_extract_epi32(idx_vec, 3)) * cols + j])), 3);\n            _mm256_storeu_pd(&dst[i * cols + j], src_vec);\n        }\n    }\n    for (; i < rows; ++i) {\n        size_t idx = indices[i];\n        for (size_t j = 0; j < cols; ++j) {\n            dst[i * cols + j] = src[idx * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    for (; i <= rows - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i*)&indices[i]);\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d src_vec0 = _mm256_loadu_pd(&src[idx_vec[0] * cols + j]);\n            __m256d src_vec1 = _mm256_loadu_pd(&src[idx_vec[1] * cols + j]);\n            __m256d src_vec2 = _mm256_loadu_pd(&src[idx_vec[2] * cols + j]);\n            __m256d src_vec3 = _mm256_loadu_pd(&src[idx_vec[3] * cols + j]);\n            _mm256_storeu_pd(&dst[i * cols + j], src_vec0);\n            _mm256_storeu_pd(&dst[i * cols + j + 2], src_vec1);\n            _mm256_storeu_pd(&dst[i * cols + j + 4], src_vec2);\n            _mm256_storeu_pd(&dst[i * cols + j + 6], src_vec3);\n        }\n    }\n    for (; i < rows; ++i) {\n        size_t idx = indices[i];\n        for (size_t j = 0; j < cols; ++j) {\n            dst[i * cols + j] = src[idx * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n\n    for (; i <= length - 4; i += 4) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi64(v1, v2);\n        __m256i res = _mm256_add_epi64(sum, scalar_vec);\n        res = _mm256_sra_epi64(res, _mm256_set1_epi64x(1));\n        _mm256_storeu_si256((__m256i*)&dst[i], res);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i]) + scalar) >> 1;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n\n    // Process data in 4-element chunks (AVX2 supports 4 x 64-bit integers per register)\n    for (; i <= length - 4; i += 4) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi64(v1, v2);\n        __m256i res = _mm256_add_epi64(sum, scalar_vec);\n        res = _mm256_sra_epi64(res, _mm256_set1_epi64x(1)); // Divide by 2 (round-toward-zero)\n        _mm256_storeu_si256((__m256i*)&dst[i], res);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i] + scalar) >> 1);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i simd_scalar = _mm256_set1_epi64x(scalar);\n\n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i sum = _mm256_add_epi64(v1, v2);\n        __m256i res = _mm256_add_epi64(sum, simd_scalar);\n        res = _mm256_sra_epi64(res, _mm256_set1_epi64x(1)); // Divide by 2 (round-toward-zero)\n        _mm256_storeu_si256((__m256i*)(dst + i), res);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i] + scalar) >> 1);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i scalar_vec = _mm256_set1_epi64x(scalar);\n\n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi64(v1, v2);\n        __m256i res = _mm256_add_epi64(sum, scalar_vec);\n        res = _mm256_sra_epi64(res, _mm256_set1_epi64x(1));\n        _mm256_storeu_si256((__m256i*)&dst[i], res);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i] + scalar) >> 1);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m256i sc = _mm256_set1_epi64x(scalar);\n\n    for (; i <= length - 4; i += 4) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)&src1[i]);\n        __m256i v2 = _mm256_loadu_si256((__m256i*)&src2[i]);\n        __m256i sum = _mm256_add_epi64(v1, v2);\n        __m256i res = _mm256_add_epi64(sum, sc);\n        res = _mm256_sra_epi64(res, _mm256_set1_epi64x(1));\n        _mm256_storeu_si256((__m256i*)&dst[i], res);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i]) + scalar) >> 1;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    __m256i min_val = _mm256_set1_epi32(__INT32_MAX__);\n    size_t i = 0;\n    for(; i <= length - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i even_mask = _mm256_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i even_vals = _mm256_and_si256(vec, even_mask);\n        min_val = _mm256_min_epi32(min_val, even_vals);\n    }\n    int result = __INT32_MAX__;\n    for(size_t j = 0; j < 8; ++j){\n        if(i - 8 + j < length){\n            result = src[i - 8 + j] < result ? src[i - 8 + j] : result;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    \n    __m256i min_val = _mm256_set1_epi32(__INT32_MAX__);\n    size_t i = 0;\n    \n    for(; i <= length - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i even_mask = _mm256_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i even_vals = _mm256_and_si256(vec, even_mask);\n        \n        __m256i gt_mask = _mm256_cmpgt_epi32(even_vals, min_val);\n        __m256i mask = _mm256_cmpeq_epi32(gt_mask, _mm256_setzero_si256());\n        \n        min_val = _mm256_min_epu32(min_val, even_vals);\n    }\n    \n    int result = __INT32_MAX__;\n    for(; i < length; ++i){\n        if(src[i] % 2 == 0 && src[i] < result){\n            result = src[i];\n        }\n    }\n    \n    return result != __INT32_MAX__ ? result : -1;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    \n    const __m256i v_zero = _mm256_setzero_si256();\n    const __m256i v_odd_mask = _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1);\n    \n    size_t i = 0;\n    __m256i v_min = _mm256_set1_epi32(INT_MAX);\n    \n    for(i = 0; i + 8 <= length; i += 8){\n        __m256i v_src = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i v_even = _mm256_andnot_si256(v_odd_mask, v_src);\n        v_min = _mm256_min_epi32(v_min, v_even);\n    }\n    \n    int min_val = INT_MAX;\n    int lane_min[8];\n    _mm256_storeu_si256((__m256i*)lane_min, v_min);\n    \n    for(int j = 0; j < 8; j++) {\n        if(lane_min[j] < min_val) min_val = lane_min[j];\n    }\n    \n    for(; i < length; i++){\n        if(src[i] % 2 == 0 && src[i] < min_val){\n            min_val = src[i];\n        }\n    }\n    \n    return min_val == INT_MAX ? -1 : min_val;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    __m256i min_val = _mm256_set1_epi32(__INT32_MAX__);\n    size_t i = 0;\n    for(; i <= length - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m256i even_vec = _mm256_andnot_si256(mask, vec);\n        min_val = _mm256_min_epi32(min_val, even_vec);\n    }\n    __m256i final_min = min_val;\n    for(; i < length; ++i){\n        if(src[i] % 2 == 0 && src[i] < final_min.m256i_i32[0]){\n            final_min.m256i_i32[0] = src[i];\n        }\n    }\n    return final_min.m256i_i32[0];\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    __m256i min_val = _mm256_set1_epi32(__INT32_MAX__);\n    size_t i = 0;\n    for(; i <= length - 8; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n        __m256i even_vec = _mm256_andnot_si256(mask, vec);\n        min_val = _mm256_min_epi32(min_val, even_vec);\n    }\n    __m256i remaining = _mm256_loadu_si256((__m256i*)&src[i]);\n    __m256i rem_mask = _mm256_setr_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i rem_even = _mm256_andnot_si256(rem_mask, remaining);\n    min_val = _mm256_min_epi32(min_val, rem_even);\n    \n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, min_val);\n    int final_min = result[0];\n    for(int j = 1; j < 8; ++j){\n        if(result[j] < final_min){\n            final_min = result[j];\n        }\n    }\n    return final_min;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    size_t i = 0;\n    __m256i shift_mask_arith = _mm256_set1_epi32((1 << shift) - 1);\n    __m256i shift_mask_log = _mm256_set1_epi32(~(-1 << shift));\n\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m256i arith_shift = _mm256_sra_epi32(src_vec, _mm256_set1_epi32(shift));\n        \n        // Logical right shift\n        __m256i log_shift = _mm256_castps_si256(_mm256_and_ps(\n            _mm256_castsi256_ps(src_vec),\n            _mm256_castsi256_ps(shift_mask_log)\n        ));\n        \n        // Combine results\n        __m256i result = _mm256_add_epi32(arith_shift, log_shift);\n        \n        _mm256_storeu_si256((__m256i*)&dst[i], result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = (src[i] >> shift) + ((uint32_t)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i shift_mask = _mm256_set1_epi8(shift);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i src_vec, arith_shifted, logic_shifted, combined;\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n\n        // Arithmetic Right Shift\n        arith_shifted = _mm256_sra_epi32(src_vec, shift_mask);\n\n        // Logical Right Shift\n        src_vec = _mm256_castsi256_si128(src_vec);\n        __m128i upper_part = _mm256_extractf128_si256(src_vec, 1);\n        src_vec = _mm_or_si128(_mm256_castsi256_si128(src_vec), _mm_shuffle_epi32(upper_part, _MM_SHUFFLE(0, 0, 0, 0)));\n        logic_shifted = _mm256_cvtepu32_epi32(_mm256_srl_epi32(_mm256_castsi256_si128(src_vec), shift_mask));\n        logic_shifted = _mm256_insertf128_si256(logic_shifted, _mm_cvtepu32_epi32(_mm_srl_epi32(_mm_shuffle_epi32(upper_part, _MM_SHUFFLE(0, 0, 0, 0)), shift_mask)), 1);\n\n        // Combine Results\n        combined = _mm256_add_epi32(arith_shifted, logic_shifted);\n\n        _mm256_storeu_si256((__m256i*)&dst[i], combined);\n    }\n    for (; i < length; i++) {\n        int val = src[i];\n        dst[i] = (val >> shift) + ((unsigned int)val >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i shift_mask = _mm256_set1_epi32((1 << shift) - 1);\n    __m256i arithmetic_shift_mask = _mm256_set1_epi32(-1 << shift);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m256i arithmetic_shifted = _mm256_sra_epi32(src_vec, _mm256_set1_epi32(shift));\n        \n        // Logical right shift\n        __m256i logical_shifted = _mm256_srli_epi32(src_vec, shift);\n        \n        // Combine results\n        __m256i combined = _mm256_add_epi32(arithmetic_shifted, logical_shifted);\n        \n        _mm256_storeu_si256((__m256i*)&dst[i], combined);\n    }\n\n    for (size_t i = length - (length % 8); i < length; ++i) {\n        dst[i] = (src[i] >> shift) + (uint32_t(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i shift_mask = _mm256_set1_epi8(shift);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i src_vec, arith_shifted, logic_shifted, combined;\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n\n        // Arithmetic Right Shift\n        arith_shifted = _mm256_sra_epi32(src_vec, shift_mask);\n\n        // Logical Right Shift\n        src_vec = _mm256_castsi256_si128(src_vec);\n        __m128i upper_part = _mm256_extractf128_si256(src_vec, 1);\n        src_vec = _mm_or_si128(_mm256_castsi256_si128(src_vec), _mm_shuffle_epi32(upper_part, _MM_SHUFFLE(0, 0, 0, 0)));\n        logic_shifted = _mm256_cvtepu32_epi32(_mm256_srl_epi32(_mm256_castsi256_si128(src_vec), shift_mask));\n        logic_shifted = _mm256_insertf128_si256(logic_shifted, _mm_cvtepu32_epi32(_mm_srl_epi32(_mm_shuffle_epi32(upper_part, _MM_SHUFFLE(0, 0, 0, 0)), shift_mask)), 1);\n\n        // Combine Results\n        combined = _mm256_add_epi32(arith_shifted, logic_shifted);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), combined);\n    }\n    for (; i < length; i++) {\n        int val = src[i];\n        dst[i] = (val >> shift) + ((unsigned int)val >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i shift_mask = _mm256_set1_epi32((1 << shift) - 1);\n    __m256i arithmetic_shift_mask = _mm256_set1_epi32(-1 << shift);\n\n    for (size_t i = 0; i + 8 <= length; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m256i arith_shifted = _mm256_sra_epi32(src_vec, _mm256_set1_epi32(shift));\n        \n        // Logical right shift\n        __m256i logic_shifted = _mm256_srl_epi32(_mm256_castsi256_si128(src_vec), _mm_cvtsi32_si128(shift));\n        logic_shifted = _mm256_insertf128_si256(logic_shifted, _mm_srl_epi32(_mm256_extractf128_si256(src_vec, 1), _mm_cvtsi32_si128(shift)), 1);\n        \n        // Combine results\n        __m256i combined = _mm256_add_epi32(arith_shifted, logic_shifted);\n        \n        // Store the result\n        _mm256_storeu_si256((__m256i*)&dst[i], combined);\n    }\n\n    for (size_t i = length - (length % 8); i < length; ++i) {\n        dst[i] = (src[i] >> shift) + (uint32_t)(src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i, j;\n    size_t total_elements = rows * cols;\n\n    for (i = 0; i < total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    __m128i low_half = _mm256_castsi256_si128(sum_vec);\n    __m128i high_half = _mm256_extractf128_si256(sum_vec, 1);\n\n    __m128i sum_low = _mm_add_epi32(low_half, high_half);\n    __m128i sum_low_low = _mm_unpacklo_epi32(sum_low, sum_low);\n    __m128i sum_low_high = _mm_unpackhi_epi32(sum_low, sum_low);\n\n    __m128i final_sum = _mm_add_epi32(sum_low_low, sum_low_high);\n    return _mm_cvtsi128_si64(final_sum);\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const size_t total_elements = rows * cols;\n    const size_t vector_size = 8; // AVX: 8 x 32-bit integers\n    __m256i sum_vector = _mm256_setzero_si256();\n    \n    for (size_t i = 0; i < total_elements; i += vector_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vector = _mm256_add_epi32(sum_vector, vec);\n    }\n    \n    // Horizontal sum of the vector\n    __m256i shuffle1 = _mm256_shuffle_epi32(sum_vector, 0b00'01'10'11);\n    __m256i sum1 = _mm256_add_epi32(sum_vector, shuffle1);\n    __m256i shuffle2 = _mm256_shuffle_epi32(sum1, 0b00'00'11'11);\n    __m256i sum2 = _mm256_add_epi32(sum1, shuffle2);\n    __m256i shuffle3 = _mm256_shuffle_epi32(sum2, 0b00'00'00'11);\n    __m256i sum3 = _mm256_add_epi32(sum2, shuffle3);\n    \n    int64_t result;\n    _mm256_storel_epi64((__m128i*)&result, sum3);\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const size_t total_elements = rows * cols;\n    const size_t vector_size = 8; // AVX: 8 x 32-bit integers\n    __m256i sum_vector = _mm256_setzero_si256();\n    \n    for (size_t i = 0; i < total_elements; i += vector_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vector = _mm256_add_epi32(sum_vector, vec);\n    }\n    \n    // Horizontal sum of the vector\n    __m128i low_half = _mm256_castsi256_si128(sum_vector);\n    __m128i high_half = _mm256_extractf128_si256(sum_vector, 1);\n    __m128i summed_low = _mm_add_epi32(low_half, high_half);\n    __m128i summed_all = _mm_hadd_epi32(summed_low, summed_low);\n    __m128i summed_final = _mm_hadd_epi32(summed_all, summed_all);\n    \n    int64_t result;\n    _mm_storel_epi64((__m128i*)&result, summed_final);\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i, j;\n    size_t total_elements = rows * cols;\n\n    for (i = 0; i < total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    __m128i low_half = _mm256_castsi256_si128(sum_vec);\n    __m128i high_half = _mm256_extractf128_si256(sum_vec, 1);\n\n    __m128i sum_low = _mm_add_epi32(low_half, high_half);\n    __m128i sum_low_low = _mm_unpacklo_epi32(sum_low, sum_low);\n    __m128i sum_low_high = _mm_unpackhi_epi32(sum_low, sum_low);\n\n    __m128i final_sum = _mm_add_epi32(sum_low_low, sum_low_high);\n    return _mm_cvtsi128_si64(final_sum);\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m256i sum_vec = _mm256_setzero_si256();\n    size_t i, j;\n    size_t total_elements = rows * cols;\n\n    for (i = 0; i < total_elements; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&matrix[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n    }\n\n    __m128i low_sum = _mm256_extractf128_si256(sum_vec, 0);\n    __m128i high_sum = _mm256_extractf128_si256(sum_vec, 1);\n\n    low_sum = _mm_add_epi32(low_sum, high_sum);\n    __m128i temp_sum = _mm_hadd_epi32(low_sum, low_sum);\n    temp_sum = _mm_hadd_epi32(temp_sum, temp_sum);\n\n    int64_t result;\n    _mm_storel_epi64((__m128i*)&result, temp_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const size_t remainder = length & ~0xF;\n    __m256i max_vec = _mm256_setzero_si256();\n    __m256i index_vec = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    size_t global_max_index = 0;\n    size_t i = 0;\n\n    // Process full vectors\n    for (; i < remainder; i += 16) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i gt_mask = _mm256_cmpgt_epi8(vec, max_vec);\n        max_vec = _mm256_or_si256(_mm256_andnot_si256(gt_mask, max_vec),\n                                  _mm256_and_si256(gt_mask, vec));\n        index_vec = _mm256_add_epi32(index_vec, _mm256_set1_epi32(i));\n        __m256i new_indices = _mm256_add_epi32(index_vec, _mm256_and_si256(gt_mask, _mm256_set1_epi32(i)));\n        index_vec = _mm256_or_si256(_mm256_andnot_si256(gt_mask, index_vec),\n                                    _mm256_and_si256(gt_mask, new_indices));\n    }\n\n    // Find max index in the remaining elements\n    int8_t local_max = 0;\n    size_t local_max_index = 0;\n    for (; i < length; ++i) {\n        if (src[i] > local_max) {\n            local_max = src[i];\n            local_max_index = i;\n        }\n    }\n\n    // Compare remaining max with vector max\n    if (local_max > _mm256_castsi256_si128(max_vec)) {\n        global_max_index = local_max_index;\n    } else {\n        global_max_index = _mm256_extract_epi32(index_vec, 0);\n    }\n\n    return global_max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const size_t vector_size = 32; // AVX: 8 x 32-bit integers\n    __m256i max_vector = _mm256_setzero_si256();\n    __m256i index_vector = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    size_t global_max_index = 0;\n    size_t i = 0;\n\n    while (i + vector_size <= length) {\n        __m256i src_vector = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i gt_mask = _mm256_cmpgt_epi8(src_vector, max_vector);\n        max_vector = _mm256_or_si256(max_vector, _mm256_andnot_si256(gt_mask, src_vector));\n        __m256i new_indices = _mm256_add_epi32(index_vector, _mm256_set1_epi32((int)i));\n        __m256i max_indices = _mm256_blendv_epi8(_mm256_setzero_si256(), new_indices, gt_mask);\n        __m256i max_index_accum = _mm256_shuffle_epi32(max_indices, _MM_SHUFFLE(0, 0, 0, 0));\n        uint32_t max_index_u32;\n        _mm256_storeu_si256((__m256i*)&max_index_u32, max_index_accum);\n        global_max_index = (max_index_u32 > global_max_index) ? max_index_u32 : global_max_index;\n        i += vector_size;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if ((size_t)src[i] > src[global_max_index]) {\n            global_max_index = i;\n        }\n    }\n\n    return global_max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const size_t vector_size = 32; // AVX: 8 x 32-bit integers\n    __m256i max_vector = _mm256_setzero_si256();\n    __m256i index_vector = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    size_t global_max_index = 0;\n    size_t i = 0;\n\n    while (i + vector_size <= length) {\n        __m256i src_vector = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i gt_mask = _mm256_cmpgt_epi8(src_vector, max_vector);\n        max_vector = _mm256_or_si256(max_vector, _mm256_andnot_si256(gt_mask, src_vector));\n        __m256i new_indices = _mm256_add_epi32(index_vector, _mm256_set1_epi32((int)i));\n        __m256i updated_indices = _mm256_blendv_epi8(_mm256_setzero_si256(), new_indices, gt_mask);\n        global_max_index += _mm256_movemask_epi8(_mm256_cmpeq_epi8(updated_indices, _mm256_setzero_si256())) ^ 0xFF;\n        i += vector_size;\n    }\n\n    int8_t max_value = INT8_MIN;\n    for (; i < length; ++i) {\n        if (src[i] > max_value) {\n            max_value = src[i];\n            global_max_index = i;\n        }\n    }\n\n    return global_max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    const __m256i min_value = _mm256_set1_epi8(-128);\n    __m256i max_vec = min_value;\n    size_t max_idx = 0;\n\n    size_t i = 0;\n    while (i + 32 <= length) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i mask = _mm256_cmpgt_epi8(vec, max_vec);\n        max_vec = _mm256_or_si256(_mm256_and_si256(mask, vec), _mm256_andnot_si256(mask, max_vec));\n        uint32_t mask_val = _mm256_movemask_epi8(mask);\n        if (mask_val) {\n            for (int j = 0; j < 32; ++j) {\n                if ((mask_val >> j) & 1) {\n                    max_idx = i + j;\n                }\n            }\n        }\n        i += 32;\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m256i max_vec = _mm256_setzero_si256();\n    __m256i index_vec = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i max_index_vec = _mm256_setzero_si256();\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i gt_mask = _mm256_cmpgt_epi8(vec, max_vec);\n        max_vec = _mm256_or_si256(max_vec, _mm256_andnot_si256(gt_mask, vec));\n        max_index_vec = _mm256_add_epi32(max_index_vec, _mm256_and_si256(gt_mask, index_vec));\n        i += 32;\n        remaining -= 32;\n    }\n\n    // Handle remaining elements\n    int8_t max_val = -128;\n    size_t max_idx = 0;\n    for (; i < length; ++i) {\n        if (src[i] > max_val) {\n            max_val = src[i];\n            max_idx = i;\n        }\n    }\n\n    // Extract the result from max_index_vec\n    int32_t indices[8];\n    _mm256_storeu_si256((__m256i*)indices, max_index_vec);\n    for (int j = 0; j < 8; ++j) {\n        if (indices[j] >= (int32_t)length) continue;\n        if (src[indices[j]] > max_val) {\n            max_val = src[indices[j]];\n            max_idx = indices[j];\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 16; i += 16) {\n        __m256i idx = _mm256_set_epi32(15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0);\n        __m256i mask_vec = _mm256_set1_epi32(mask);\n        __m256i idx_and_mask = _mm256_and_si256(idx, mask_vec);\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i blend_mask = _mm256_cmpgt_epi32(idx_and_mask, _mm256_setzero_si256());\n        __m256i result = _mm256_blendv_epi8(src2_vec, src1_vec, blend_mask);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        dst[i] = ((uint32_t(i) & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi16((short)mask);\n    \n    // Process full vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i idx = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7,\n                                         i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15);\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i));\n        \n        __m256i cmp = _mm256_andnot_si256(_mm256_and_si256(idx, mask_vec), _mm256_set1_epi32(1));\n        __m256i src = _mm256_blendv_epi8(src2_vec, src1_vec, cmp);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), src);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint32_t)i & mask) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    \n    // Process full vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i idx = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15);\n        __m256i idx_and_mask = _mm256_and_si256(idx, mask_vec);\n        __m256i src1_mask = _mm256_cmpgt_epi32(idx_and_mask, _mm256_setzero_si256());\n        __m256i src1_data = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_data = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i result = _mm256_blendv_epi8(src2_data, src1_data, src1_mask);\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint32_t(i) & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    for (; i <= length - 16; i += 16) {\n        __m256i idx = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15);\n        __m256i idx_and_mask = _mm256_and_si256(idx, mask_vec);\n        __m256i src1_mask = _mm256_cmpgt_epi32(idx_and_mask, _mm256_setzero_si256());\n        __m256i src2_mask = _mm256_xor_si256(src1_mask, _mm256_set1_epi32(-1));\n        \n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i));\n        \n        __m256i res1 = _mm256_blendv_epi8(src1_vec, src2_vec, src1_mask);\n        __m256i res2 = _mm256_blendv_epi8(src2_vec, src1_vec, src2_mask);\n        \n        __m256i final_res = _mm256_or_si256(res1, res2);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), final_res);\n    }\n    for (; i < length; i++) {\n        dst[i] = ((uint32_t(i) & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i mask_vec = _mm256_set1_epi32(mask);\n    \n    // Process full vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i idx = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15);\n        __m256i idx_and_mask = _mm256_and_si256(idx, mask_vec);\n        __m256i src1_mask = _mm256_cmpgt_epi32(idx_and_mask, _mm256_setzero_si256());\n        __m256i src2_mask = _mm256_xor_si256(src1_mask, _mm256_set1_epi32(-1));\n\n        __m256i src1_vec = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i src2_vec = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i dst_vec = _mm256_or_si256(_mm256_and_si256(src1_vec, src1_mask), _mm256_and_si256(src2_vec, src2_mask));\n        _mm256_storeu_si256((__m256i*)(dst + i), dst_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i v_src, v_dst;\n    __m256i v_mask0 = _mm256_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F,\n                                        0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);\n    __m256i v_mask1 = _mm256_set_epi32(0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF,\n                                        0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF);\n    __m256i v_mask2 = _mm256_set_epi32(0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF,\n                                        0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF);\n\n    const int lanes = 8;\n    const int shift_amounts[4] = {0, 1, 2, 3};\n\n    while (i + lanes <= length) {\n        v_src = _mm256_loadu_si256((__m256i*)(src + i));\n\n        // Horizontal sum of 4 bytes in each lane\n        v_dst = _mm256_and_si256(v_src, v_mask0);\n        v_dst = _mm256_slli_epi32(v_dst, 4);\n        v_src = _mm256_sub_epi32(v_src, v_dst);\n\n        v_dst = _mm256_and_si256(v_src, v_mask1);\n        v_dst = _mm256_slli_epi32(v_dst, 8);\n        v_src = _mm256_sub_epi32(v_src, v_dst);\n\n        v_dst = _mm256_and_si256(v_src, v_mask2);\n        v_dst = _mm256_slli_epi32(v_dst, 16);\n        v_src = _mm256_sub_epi32(v_src, v_dst);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), v_src);\n\n        i += lanes;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i v_src, v_dst;\n    __m256i v_mask0 = _mm256_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F,\n                                        0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);\n    __m256i v_mask1 = _mm256_set_epi32(0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF,\n                                        0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF);\n    __m256i v_mask2 = _mm256_set_epi32(0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF,\n                                        0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF);\n\n    const __m256i v_shift1 = _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1);\n    const __m256i v_shift2 = _mm256_set_epi32(2, 2, 2, 2, 2, 2, 2, 2);\n    const __m256i v_shift3 = _mm256_set_epi32(3, 3, 3, 3, 3, 3, 3, 3);\n\n    while (i + 8 <= length) {\n        v_src = _mm256_loadu_si256((__m256i*)(src + i));\n        \n        // Count bits in groups of 4\n        v_dst = _mm256_and_si256(v_src, v_mask0);\n        v_dst = _mm256_add_epi32(v_dst, _mm256_srli_epi32(v_src, 4));\n\n        // Count bits in groups of 8\n        v_dst = _mm256_and_si256(v_dst, v_mask1);\n        v_dst = _mm256_add_epi32(v_dst, _mm256_srli_epi32(v_dst, 8));\n\n        // Count bits in groups of 16\n        v_dst = _mm256_and_si256(v_dst, v_mask2);\n        v_dst = _mm256_add_epi32(v_dst, _mm256_srli_epi32(v_dst, 16));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), v_dst);\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    __m256i pc_table = _mm256_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4,\n        1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5\n    );\n    \n    for (; i <= length - 8; i += 8) {\n        __m256i v_src = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i v_bytes0 = _mm256_shuffle_epi8(pc_table, v_src);\n        __m256i v_bytes1 = _mm256_shuffle_epi8(pc_table, _mm256_srli_epi32(v_src, 8));\n        __m256i v_bytes2 = _mm256_shuffle_epi8(pc_table, _mm256_srli_epi32(v_src, 16));\n        __m256i v_bytes3 = _mm256_shuffle_epi8(pc_table, _mm256_srli_epi32(v_src, 24));\n        \n        __m256i v_sum0 = _mm256_add_epi8(v_bytes0, v_bytes1);\n        __m256i v_sum1 = _mm256_add_epi8(v_bytes2, v_bytes3);\n        __m256i v_sum2 = _mm256_add_epi8(v_sum0, v_sum1);\n        \n        __m256i v_sum3 = _mm256_sad_epu8(zero, v_sum2);\n        uint32_t sum[8];\n        _mm256_storeu_si256((__m256i*)sum, v_sum3);\n        \n        dst[i + 0] += sum[0];\n        dst[i + 1] += sum[1];\n        dst[i + 2] += sum[2];\n        dst[i + 3] += sum[3];\n        dst[i + 4] += sum[4];\n        dst[i + 5] += sum[5];\n        dst[i + 6] += sum[6];\n        dst[i + 7] += sum[7];\n    }\n    \n    for (; i < length; i++) {\n        dst[i] += __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i v_src, v_dst;\n    __m256i v_mask0 = _mm256_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F,\n                                        0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);\n    __m256i v_mask1 = _mm256_set_epi32(0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF,\n                                        0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF);\n    __m256i v_mask2 = _mm256_set_epi32(0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF,\n                                        0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF);\n\n    while (i + 8 <= length) {\n        v_src = _mm256_loadu_si256((__m256i*)(src + i));\n        v_dst = _mm256_and_si256(v_src, v_mask0);\n        v_dst = _mm256_add_epi8(v_dst, _mm256_and_si256(_mm256_srli_epi32(v_src, 4), v_mask0));\n        v_dst = _mm256_add_epi8(v_dst, _mm256_and_si256(v_dst, v_mask1));\n        v_dst = _mm256_add_epi8(v_dst, _mm256_and_si256(_mm256_srli_epi16(v_dst, 8), v_mask1));\n        v_dst = _mm256_add_epi8(v_dst, _mm256_and_si256(v_dst, v_mask2));\n        v_dst = _mm256_permute4x64_epi64(v_dst, 78); // Shuffle to accumulate sums\n        _mm256_storeu_si256((__m256i*)(dst + i), v_dst);\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i v_src, v_dst;\n    __m256i v_mask0 = _mm256_set_epi32(0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F,\n                                        0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F, 0x0F0F0F0F);\n    __m256i v_mask1 = _mm256_set_epi32(0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF,\n                                        0x00FF00FF, 0x00FF00FF, 0x00FF00FF, 0x00FF00FF);\n    __m256i v_mask2 = _mm256_set_epi32(0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF,\n                                        0x0000FFFF, 0x0000FFFF, 0x0000FFFF, 0x0000FFFF);\n\n    __m256i v_shift0 = _mm256_set_epi32(4, 4, 4, 4, 4, 4, 4, 4);\n    __m256i v_shift1 = _mm256_set_epi32(8, 8, 8, 8, 8, 8, 8, 8);\n    __m256i v_shift2 = _mm256_set_epi32(16, 16, 16, 16, 16, 16, 16, 16);\n\n    while (i + 8 <= length) {\n        v_src = _mm256_loadu_si256((__m256i*)(src + i));\n        \n        // Count bits in groups of 4\n        v_dst = _mm256_and_si256(v_src, v_mask0);\n        v_dst = _mm256_shuffle_epi8(_mm256_set_epi32(0x03020100, 0x07060504, 0x0B0A0908, 0x0F0E0D0C,\n                                                     0x13121110, 0x17161514, 0x1B1A1918, 0x1F1E1D1C),\n                                    v_dst);\n        v_dst = _mm256_add_epi8(v_dst, _mm256_permutevar8x32_epi32(v_dst, v_shift0));\n\n        // Count bits in groups of 8\n        v_dst = _mm256_and_si256(v_dst, v_mask1);\n        v_dst = _mm256_shuffle_epi8(_mm256_set_epi32(0x07030302, 0x0F0B0706, 0x17130D09, 0x1F1B1713,\n                                                     0x2F2B2723, 0x37332F2B, 0x3F3B3733, 0x4F4B4743),\n                                    v_dst);\n        v_dst = _mm256_add_epi8(v_dst, _mm256_permutevar8x32_epi32(v_dst, v_shift1));\n\n        // Count bits in groups of 16\n        v_dst = _mm256_and_si256(v_dst, v_mask2);\n        v_dst = _mm256_shuffle_epi8(_mm256_set_epi32(0x0F0F0F07, 0x1F1F1F1B, 0x3F3F3F37, 0x7F7F7F73,\n                                                     0xFF7F7F7F, 0xFFBF7F7F, 0xFFDF7F7F, 0xFFEF7F7F),\n                                    v_dst);\n        v_dst = _mm256_add_epi8(v_dst, _mm256_permutevar8x32_epi32(v_dst, v_shift2));\n\n        _mm256_storeu_si256((__m256i*)(dst + i), v_dst);\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = __builtin_popcount(src[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    // Process data in 16-element chunks (8 AVX2 registers)\n    for (; i <= length - 32; i += 32) {\n        __m256i v1_0 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v1_1 = _mm256_loadu_si256((__m256i*)(src1 + i + 8));\n        __m256i v1_2 = _mm256_loadu_si256((__m256i*)(src1 + i + 16));\n        __m256i v1_3 = _mm256_loadu_si256((__m256i*)(src1 + i + 24));\n\n        __m256i v2_0 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i v2_1 = _mm256_loadu_si256((__m256i*)(src2 + i + 8));\n        __m256i v2_2 = _mm256_loadu_si256((__m256i*)(src2 + i + 16));\n        __m256i v2_3 = _mm256_loadu_si256((__m256i*)(src2 + i + 24));\n\n        __m256i sum0 = _mm256_adds_epu16(v1_0, v2_0);\n        __m256i sum1 = _mm256_adds_epu16(v1_1, v2_1);\n        __m256i sum2 = _mm256_adds_epu16(v1_2, v2_2);\n        __m256i sum3 = _mm256_adds_epu16(v1_3, v2_3);\n\n        sum0 = _mm256_min_epu16(sum0, max_val);\n        sum1 = _mm256_min_epu16(sum1, max_val);\n        sum2 = _mm256_min_epu16(sum2, max_val);\n        sum3 = _mm256_min_epu16(sum3, max_val);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), sum0);\n        _mm256_storeu_si256((__m256i*)(dst + i + 8), sum1);\n        _mm256_storeu_si256((__m256i*)(dst + i + 16), sum2);\n        _mm256_storeu_si256((__m256i*)(dst + i + 24), sum3);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 16) {\n        __m256i vsrc1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i vsrc2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n\n        // Perform saturating addition\n        __m256i vadd = _mm256_adds_epu16(vsrc1, vsrc2);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), vadd);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    while (remaining > 0) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    // Process data in 16-element chunks (8 AVX2 registers)\n    for (; i <= length - 32; i += 32) {\n        __m256i v1_0 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v1_1 = _mm256_loadu_si256((__m256i*)(src1 + i + 8));\n        __m256i v1_2 = _mm256_loadu_si256((__m256i*)(src1 + i + 16));\n        __m256i v1_3 = _mm256_loadu_si256((__m256i*)(src1 + i + 24));\n\n        __m256i v2_0 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i v2_1 = _mm256_loadu_si256((__m256i*)(src2 + i + 8));\n        __m256i v2_2 = _mm256_loadu_si256((__m256i*)(src2 + i + 16));\n        __m256i v2_3 = _mm256_loadu_si256((__m256i*)(src2 + i + 24));\n\n        __m256i sum0 = _mm256_adds_epu16(v1_0, v2_0);\n        __m256i sum1 = _mm256_adds_epu16(v1_1, v2_1);\n        __m256i sum2 = _mm256_adds_epu16(v1_2, v2_2);\n        __m256i sum3 = _mm256_adds_epu16(v1_3, v2_3);\n\n        sum0 = _mm256_min_epu16(sum0, max_val);\n        sum1 = _mm256_min_epu16(sum1, max_val);\n        sum2 = _mm256_min_epu16(sum2, max_val);\n        sum3 = _mm256_min_epu16(sum3, max_val);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), sum0);\n        _mm256_storeu_si256((__m256i*)(dst + i + 8), sum1);\n        _mm256_storeu_si256((__m256i*)(dst + i + 16), sum2);\n        _mm256_storeu_si256((__m256i*)(dst + i + 24), sum3);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    // Process data in 16-element chunks (8 AVX2 registers)\n    for (; i <= length - 32; i += 32) {\n        __m256i v1_0 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v1_1 = _mm256_loadu_si256((__m256i*)(src1 + i + 8));\n        __m256i v1_2 = _mm256_loadu_si256((__m256i*)(src1 + i + 16));\n        __m256i v1_3 = _mm256_loadu_si256((__m256i*)(src1 + i + 24));\n\n        __m256i v2_0 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i v2_1 = _mm256_loadu_si256((__m256i*)(src2 + i + 8));\n        __m256i v2_2 = _mm256_loadu_si256((__m256i*)(src2 + i + 16));\n        __m256i v2_3 = _mm256_loadu_si256((__m256i*)(src2 + i + 24));\n\n        __m256i sum0 = _mm256_adds_epu16(v1_0, v2_0);\n        __m256i sum1 = _mm256_adds_epu16(v1_1, v2_1);\n        __m256i sum2 = _mm256_adds_epu16(v1_2, v2_2);\n        __m256i sum3 = _mm256_adds_epu16(v1_3, v2_3);\n\n        sum0 = _mm256_min_epu16(sum0, max_val);\n        sum1 = _mm256_min_epu16(sum1, max_val);\n        sum2 = _mm256_min_epu16(sum2, max_val);\n        sum3 = _mm256_min_epu16(sum3, max_val);\n\n        _mm256_storeu_si256((__m256i*)(dst + i), sum0);\n        _mm256_storeu_si256((__m256i*)(dst + i + 8), sum1);\n        _mm256_storeu_si256((__m256i*)(dst + i + 16), sum2);\n        _mm256_storeu_si256((__m256i*)(dst + i + 24), sum3);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    // Process data in 16-element chunks (AVX2)\n    for (; i <= length - 16; i += 16) {\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(src1 + i));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(src2 + i));\n        __m256i sum = _mm256_add_epi16(v1, v2);\n        __m256i saturated = _mm256_min_epu16(sum, max_val);\n        _mm256_storeu_si256((__m256i*)(dst + i), saturated);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d mask = _mm256_cmpgt_pd(_mm256_set1_pd(100.0), a_vec);\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(a_vec, _mm256_set1_pd(-100.0)));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(_mm256_set1_pd(100.0), b_vec));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(b_vec, _mm256_set1_pd(-100.0)));\n                __m256d valid_a = _mm256_and_pd(a_vec, mask);\n                __m256d valid_b = _mm256_and_pd(b_vec, mask);\n                __m256d product = _mm256_mul_pd(valid_a, valid_b);\n                sum = _mm256_add_pd(sum, product);\n            }\n            __m256d partial_sum = sum;\n            for (size_t l = 0; l < 3; ++l) {\n                partial_sum = _mm256_hadd_pd(partial_sum, partial_sum);\n            }\n            double result = _mm_cvtsd_f64(partial_sum);\n            C[i * p + j] += result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d mask = _mm256_cmpgt_pd(_mm256_set1_pd(100.0), a_vec);\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(a_vec, _mm256_set1_pd(-100.0)));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(_mm256_set1_pd(100.0), b_vec));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(b_vec, _mm256_set1_pd(-100.0)));\n                __m256d valid_a = _mm256_and_pd(a_vec, mask);\n                __m256d valid_b = _mm256_and_pd(b_vec, mask);\n                __m256d product = _mm256_mul_pd(valid_a, valid_b);\n                sum = _mm256_add_pd(sum, product);\n            }\n            __m256d partial_sum = _mm256_hadd_pd(sum, sum);\n            partial_sum = _mm256_hadd_pd(partial_sum, partial_sum);\n            double result[4];\n            _mm256_storeu_pd(result, partial_sum);\n            double total = result[0] + result[1] + result[2] + result[3];\n            C[i * p + j] += total;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d mask = _mm256_cmpgt_pd(_mm256_set1_pd(100.0), a_vec);\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(a_vec, _mm256_set1_pd(-100.0)));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(_mm256_set1_pd(100.0), b_vec));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(b_vec, _mm256_set1_pd(-100.0)));\n                __m256d valid_a = _mm256_and_pd(a_vec, mask);\n                __m256d valid_b = _mm256_and_pd(b_vec, mask);\n                __m256d product = _mm256_mul_pd(valid_a, valid_b);\n                sum = _mm256_add_pd(sum, product);\n            }\n            __m256d sum_horizontal = _mm256_hadd_pd(sum, sum);\n            sum_horizontal = _mm256_hadd_pd(sum_horizontal, sum_horizontal);\n            double result = _mm256_cvtsd_f64(sum_horizontal);\n            C[i * p + j] += result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d mask = _mm256_cmpgt_pd(_mm256_set1_pd(100.0), a_vec);\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(a_vec, _mm256_set1_pd(-100.0)));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(_mm256_set1_pd(100.0), b_vec));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(b_vec, _mm256_set1_pd(-100.0)));\n                __m256d valid_a = _mm256_and_pd(a_vec, mask);\n                __m256d valid_b = _mm256_and_pd(b_vec, mask);\n                __m256d product = _mm256_mul_pd(valid_a, valid_b);\n                sum = _mm256_add_pd(sum, product);\n            }\n            __m256d sum_horizontal = _mm256_hadd_pd(sum, sum);\n            sum_horizontal = _mm256_hadd_pd(sum_horizontal, sum_horizontal);\n            double result = _mm256_cvtsd_f64(sum_horizontal);\n            C[i * p + j] += result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a_vec = _mm256_loadu_pd(&A[i * n + k]);\n                __m256d b_vec = _mm256_loadu_pd(&B[k * p + j]);\n                __m256d mask = _mm256_cmpgt_pd(_mm256_set1_pd(100.0), a_vec);\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(a_vec, _mm256_set1_pd(-100.0)));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(_mm256_set1_pd(100.0), b_vec));\n                mask = _mm256_and_pd(mask, _mm256_cmpgt_pd(b_vec, _mm256_set1_pd(-100.0)));\n                __m256d valid_a = _mm256_and_pd(a_vec, mask);\n                __m256d valid_b = _mm256_and_pd(b_vec, mask);\n                __m256d product = _mm256_mul_pd(valid_a, valid_b);\n                sum = _mm256_add_pd(sum, product);\n            }\n            double temp_sum[4];\n            _mm256_storeu_pd(temp_sum, sum);\n            double result = temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3];\n            C[i * p + j] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 8 <= total_elements) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n        i += 8;\n    }\n\n    while (i < total_elements) {\n        C[i] = A[i] + B[i];\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX2\n    for (; i <= total_elements - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX2\n    for (; i <= total_elements - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 4 using AVX2\n    for (; i <= total_elements - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_add_epi64(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 8 <= total_elements) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_B = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vec_C = _mm256_add_epi64(vec_A, vec_B);\n        _mm256_storeu_si256((__m256i*)&C[i], vec_C);\n        i += 8;\n    }\n\n    while (i < total_elements) {\n        C[i] = A[i] + B[i];\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 8) {\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n        __m256 v_sum = _mm256_add_ps(v_src1, v_src2);\n\n        // Add 0.5f before truncation to round to nearest integer\n        __m256 v_round = _mm256_add_ps(v_sum, _mm256_set1_ps(0.5f));\n        \n        // Truncate to integer by converting to integer and back to float\n        __m256i v_int = _mm256_cvttps_epi32(v_round);\n        __m256 v_result = _mm256_cvtepi32_ps(v_int);\n\n        _mm256_storeu_ps(dst + i, v_result);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle the remaining elements\n    while (remaining > 0) {\n        dst[i] = roundf(src1[i] + src2[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 8) {\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n        __m256 v_sum = _mm256_add_ps(v_src1, v_src2);\n\n        // Add 0.5f before converting to integer to round to nearest\n        __m256 v_round = _mm256_add_ps(v_sum, _mm256_set1_ps(0.5f));\n        \n        // Convert to integer by truncation (floor)\n        __m256i v_int = _mm256_cvttps_epi32(v_round);\n\n        // Store the result back to memory\n        _mm256_storeu_ps(dst + i, _mm256_cvtepi32_ps(v_int));\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle the remaining elements if length is not a multiple of 8\n    while (remaining > 0) {\n        dst[i] = roundf(src1[i] + src2[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 8) {\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n        __m256 v_sum = _mm256_add_ps(v_src1, v_src2);\n\n        // Add 0.5f before truncation to round to nearest integer\n        __m256 v_round = _mm256_add_ps(v_sum, _mm256_set1_ps(0.5f));\n        \n        // Truncate to integer by converting to integer and back to float\n        __m256i v_int = _mm256_cvttps_epi32(v_round);\n        __m256 v_result = _mm256_cvtepi32_ps(v_int);\n\n        _mm256_storeu_ps(dst + i, v_result);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements\n    while (remaining > 0) {\n        dst[i] = roundf(src1[i] + src2[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 8) {\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n        __m256 v_sum = _mm256_add_ps(v_src1, v_src2);\n\n        // Add 0.5f before conversion to simulate rounding\n        __m256 v_round = _mm256_add_ps(v_sum, _mm256_set1_ps(0.5f));\n        \n        // Convert to integer by truncation (floor)\n        __m256i v_int = _mm256_cvttps_epi32(v_round);\n\n        // Convert back to float to store in dst\n        __m256 v_result = _mm256_cvtepi32_ps(v_int);\n\n        _mm256_storeu_ps(dst + i, v_result);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements\n    while (remaining > 0) {\n        dst[i] = roundf(src1[i] + src2[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 8) {\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n        __m256 v_sum = _mm256_add_ps(v_src1, v_src2);\n\n        // Add 0.5f before conversion to simulate rounding\n        __m256 v_round = _mm256_add_ps(v_sum, _mm256_set1_ps(0.5f));\n        \n        // Convert to integer by truncation after adding 0.5\n        __m256i v_int = _mm256_cvttps_epi32(v_round);\n\n        // Store the result back to memory\n        _mm256_storeu_ps(dst + i, _mm256_cvtepi32_ps(v_int));\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle the remaining elements if length is not a multiple of 8\n    while (remaining > 0) {\n        dst[i] = roundf(src1[i] + src2[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    size_t remaining = length;\n\n    while(remaining >= 8){\n        __m256 v_src1 = _mm256_loadu_ps(src1 + i);\n        __m256 v_src2 = _mm256_loadu_ps(src2 + i);\n\n        // Check if index is a multiple of 2 (using bitwise AND)\n        __m256i index_mask = _mm256_set1_epi32(0x00000002);\n        __m256i v_index = _mm256_set_epi32(7 + i, 6 + i, 5 + i, 4 + i, 3 + i, 2 + i, 1 + i, i);\n        __m256i v_index_mod = _mm256_and_si256(v_index, index_mask);\n        __m256 mask_even_index = _mm256_cmpgt_ps(_mm256_setzero_ps(), _mm256_castsi256_ps(v_index_mod));\n\n        // Compute absolute difference\n        __m256 v_diff = _mm256_sub_ps(v_src1, v_src2);\n        __m256 v_abs_diff = _mm256_max_ps(v_diff, _mm256_sub_ps(_mm256_setzero_ps(), v_diff));\n        \n        // Compare absolute difference with threshold (50)\n        __m256 threshold = _mm256_set1_ps(50.0f);\n        __m256 mask_within_threshold = _mm256_cmp_ps(v_abs_diff, threshold, _CMP_LE_OS);\n\n        // Combine masks: even index AND within threshold\n        __m256 mask_valid = _mm256_and_ps(mask_even_index, mask_within_threshold);\n\n        // Element-wise multiplication\n        __m256 v_result = _mm256_mul_ps(v_src1, v_src2);\n\n        // Round up toward positive infinity\n        __m256 v_rounded = _mm256_ceil_ps(v_result);\n\n        // Apply mask: valid elements remain, others become -1\n        __m256 v_neg_one = _mm256_set1_ps(-1.0f);\n        __m256 v_final = _mm256_blendv_ps(v_rounded, v_neg_one, mask_valid);\n\n        // Store result\n        _mm256_storeu_ps(dst + i, v_final);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements\n    while(remaining--){\n        if((i % 2 == 0) && (fabs(src1[i] - src2[i]) <= 50.0f)){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m256 threshold = _mm256_set1_ps(50.0f);\n    __m256 minus_one_vec = _mm256_set1_ps(-1.0f);\n    __m256 two_vec = _mm256_set1_ps(2.0f);\n    \n    for(i = 0; i + 8 <= length; i += 8){\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n        __m256 diff_vec = _mm256_sub_ps(src1_vec, src2_vec);\n        __m256 abs_diff_vec = _mm256_abs_ps(diff_vec);\n        \n        // Check if index is a multiple of 2 (using bitwise AND)\n        __m256i index_mask = _mm256_set_epi32(7, 5, 3, 1, 7, 5, 3, 1);\n        __m256i index_check = _mm256_and_si256(_mm256_castps_si256(index_mask), _mm256_castps_si256(_mm256_set1_ps((float)i)));\n        __m256 index_check_mask = _mm256_castsi256_ps(index_check);\n        \n        // Combine conditions: abs_diff <= threshold AND index_check == 0\n        __m256 condition_vec = _mm256_cmp_ps(abs_diff_vec, threshold, _MM_CMPINT_LE);\n        condition_vec = _mm256_and_ps(condition_vec, index_check_mask);\n        \n        __m256 mul_result_vec = _mm256_div_ps(_mm256_add_ps(_mm256_mul_ps(src1_vec, src2_vec), two_vec), two_vec);\n        __m256 final_result_vec = _mm256_blendv_ps(minus_one_vec, mul_result_vec, condition_vec);\n        \n        _mm256_storeu_ps(&dst[i], final_result_vec);\n    }\n    \n    for(; i < length; i++){\n        if(abs(src1[i] - src2[i]) <= 50.0f && i % 2 == 0){\n            float result = ceilf(src1[i] * src2[i]);\n            dst[i] = result;\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m256 threshold = _mm256_set1_ps(50.0f);\n    __m256 minus_one_vec = _mm256_set1_ps(-1.0f);\n    __m256 two_vec = _mm256_set1_ps(2.0f);\n\n    for(i = 0; i + 8 <= length; i += 8){\n        __m256 src1_vec = _mm256_loadu_ps(src1 + i);\n        __m256 src2_vec = _mm256_loadu_ps(src2 + i);\n        __m256 diff = _mm256_sub_ps(src1_vec, src2_vec);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n        __m256 mask1 = _mm256_cmp_ps(abs_diff, threshold, _CMP_LE_OS);\n        __m256 index_mask = _mm256_set_m128(_mm_castsi128_ps(_mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i)), _mm_castsi128_ps(_mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i)));\n        __m256 mask2 = _mm256_cmp_ps(index_mask, two_vec, _CMP_EQ_OQ);\n        __m256 final_mask = _mm256_and_ps(mask1, mask2);\n        __m256 mul_result = _mm256_mul_ps(src1_vec, src2_vec);\n        __m256 rounded_result = _mm256_ceil_ps(mul_result);\n        __m256 masked_result = _mm256_blendv_ps(minus_one_vec, rounded_result, final_mask);\n        _mm256_storeu_ps(dst + i, masked_result);\n    }\n    for(; i < length; i++){\n        if((fabs(src1[i] - src2[i]) <= 50.0f) && (i % 2 == 0)){\n            dst[i] = ceil(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m256 threshold = _mm256_set1_ps(50.0f);\n    __m256 minus_one_vec = _mm256_set1_ps(-1.0f);\n    __m256 two_vec = _mm256_set1_ps(2.0f);\n    \n    for(i = 0; i + 8 <= length; i += 8){\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n        __m256 diff_vec = _mm256_sub_ps(src1_vec, src2_vec);\n        __m256 abs_diff_vec = _mm256_abs_ps(diff_vec);\n        \n        __m256 index_mask = _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0);\n        __m256 index_div_two = _mm256_cvtepi32_ps(_mm256_and_si256((__m256i)index_mask, (__m256i)_mm256_set1_epi32(1)));\n        __m256 index_check = _mm256_cmpeq_epi32(index_div_two, _mm256_setzero_si256());\n        \n        __m256 mask = _mm256_and_ps(_mm256_cmpgt_ps(abs_diff_vec, threshold), index_check);\n        __m256 mul_result = _mm256_round_ps(_mm256_mul_ps(src1_vec, src2_vec), _MM_FROUND_TO_POS_INF);\n        __m256 final_result = _mm256_blendv_ps(minus_one_vec, mul_result, mask);\n        \n        _mm256_storeu_ps(&dst[i], final_result);\n    }\n    \n    for(; i < length; i++){\n        if((fabs(src1[i] - src2[i]) <= 50.0f) && (i % 2 == 0)){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m256 threshold = _mm256_set1_ps(50.0f);\n    __m256 minus_one_vec = _mm256_set1_ps(-1.0f);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 two_vec = _mm256_set1_ps(2.0f);\n\n    for(i = 0; i + 8 <= length; i += 8){\n        __m256 src1_vec = _mm256_loadu_ps(&src1[i]);\n        __m256 src2_vec = _mm256_loadu_ps(&src2[i]);\n        __m256 diff_vec = _mm256_sub_ps(src1_vec, src2_vec);\n        __m256 abs_diff_vec = _mm256_abs_ps(diff_vec);\n        __m256 mask1 = _mm256_cmp_ps(abs_diff_vec, threshold, _CMP_LE_OS);\n        __m256 index_mask = _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0);\n        __m256 index_vec = _mm256_cvtepi32_ps(_mm256_castps_si256(index_mask));\n        __m256 div_vec = _mm256_div_ps(index_vec, two_vec);\n        __m256 int_check = _mm256_round_ps(div_vec, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n        __m256 mask2 = _mm256_cmpeq_ps(int_check, zero_vec);\n        __m256 final_mask = _mm256_and_ps(mask1, mask2);\n        \n        __m256 mul_vec = _mm256_mul_ps(src1_vec, src2_vec);\n        __m256 rounded_mul_vec = _mm256_ceil_ps(mul_vec);\n        __m256 result_vec = _mm256_blendv_ps(minus_one_vec, rounded_mul_vec, final_mask);\n        \n        _mm256_storeu_ps(&dst[i], result_vec);\n    }\n    \n    for(; i < length; i++){\n        if((fabs(src1[i] - src2[i]) <= 50.0f) && (i % 2 == 0)){\n            dst[i] = ceil(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t align = 4;\n    __m256d v_scalar = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += align) {\n            __m256d v_mat = _mm256_loadu_pd(mat + i * n + j);\n            __m256d v_result = _mm256_mul_pd(v_mat, v_scalar);\n            _mm256_storeu_pd(dst + i * n + j, v_result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t aligned_cols = n & ~3UL;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(mat + i * n + j);\n            __m256d res_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(dst + i * n + j, res_vec);\n        }\n        for (size_t j = aligned_cols; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t aligned_cols = n & ~3;\n    __m256d scalar_vec = _mm256_set1_pd(scalar);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < aligned_cols; j += 4) {\n            __m256d mat_vec = _mm256_loadu_pd(mat + i * n + j);\n            __m256d res_vec = _mm256_mul_pd(mat_vec, scalar_vec);\n            _mm256_storeu_pd(dst + i * n + j, res_vec);\n        }\n        for (size_t j = aligned_cols; j < n; ++j) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256d v_scalar = _mm256_set1_pd(scalar);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j + 4 <= n; j += 4) {\n            __m256d v_mat = _mm256_loadu_pd(mat + i * n + j);\n            __m256d v_result = _mm256_mul_pd(v_mat, v_scalar);\n            _mm256_storeu_pd(dst + i * n + j, v_result);\n        }\n        for (; j < n; j++) {\n            dst[i * n + j] = mat[i * n + j] * scalar;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const size_t align = 4;\n    const size_t vector_width = 4; // AVX: 4 doubles per register\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += vector_width) {\n            __m256d vec_mat = _mm256_loadu_pd(mat + i * n + j);\n            __m256d vec_scalar = _mm256_set1_pd(scalar);\n            __m256d vec_result = _mm256_mul_pd(vec_mat, vec_scalar);\n            _mm256_storeu_pd(dst + i * n + j, vec_result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 mul = _mm256_mul_ps(row, row);\n        __m256 sum = _mm256_hadd_ps(mul, mul);\n        sum = _mm256_hadd_ps(sum, sum);\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float norm = sqrtf(temp[0] + temp[4]);\n        if (norm > 0.0f) {\n            __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n            _mm256_storeu_ps(mat + i * n, _mm256_mul_ps(row, inv_norm));\n        } else {\n            _mm256_storeu_ps(mat + i * n, _mm256_setzero_ps());\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 mul = _mm256_mul_ps(row, row);\n        __m256 sum = _mm256_hadd_ps(mul, mul);\n        sum = _mm256_hadd_ps(sum, sum);\n        float result[8];\n        _mm256_storeu_ps(result, sum);\n        float norm = sqrtf(result[0] + result[4]);\n        if (norm > 0.0f) {\n            __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n            _mm256_storeu_ps(mat + i * n, _mm256_mul_ps(row, inv_norm));\n        } else {\n            for (size_t j = 0; j < n; j++) {\n                mat[i * n + j] = 0.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 mul = _mm256_mul_ps(row, row);\n        __m256 sum = _mm256_hadd_ps(mul, mul);\n        sum = _mm256_hadd_ps(sum, sum);\n        float result[8];\n        _mm256_storeu_ps(result, sum);\n        float norm = sqrtf(result[0] + result[4]);\n        if (norm > 0.0f) {\n            __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n            _mm256_storeu_ps(mat + i * n, _mm256_mul_ps(row, inv_norm));\n        } else {\n            // Handle zero norm case if necessary\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 mul = _mm256_mul_ps(row, row);\n        __m256 sum = _mm256_hadd_ps(mul, mul);\n        sum = _mm256_hadd_ps(sum, sum);\n        float temp[8];\n        _mm256_storeu_ps(temp, sum);\n        float norm = sqrtf(temp[0] + temp[4]);\n        if (norm > 0.0f) {\n            __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n            _mm256_storeu_ps(mat + i * n, _mm256_mul_ps(row, inv_norm));\n        } else {\n            // Handle division by zero if necessary\n            for (size_t j = 0; j < n; j++) {\n                mat[i * n + j] = 0.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 sum_squares = _mm256_mul_ps(row, row);\n\n        // Reduce sum_squares to compute the sum of squares\n        sum_squares = _mm256_hadd_ps(sum_squares, sum_squares);\n        sum_squares = _mm256_hadd_ps(sum_squares, sum_squares);\n        float sum_squares_array[8];\n        _mm256_storeu_ps(sum_squares_array, sum_squares);\n        float norm = sqrtf(sum_squares_array[0] + sum_squares_array[4]);\n\n        if (norm > 0.0f) {\n            __m256 inv_norm = _mm256_set1_ps(1.0f / norm);\n            for (size_t j = 0; j < n; j += 8) {\n                __m256 vec = _mm256_loadu_ps(mat + i * n + j);\n                vec = _mm256_mul_ps(vec, inv_norm);\n                _mm256_storeu_ps(mat + i * n + j, vec);\n            }\n        } else {\n            // Handle division by zero if necessary\n            for (size_t j = 0; j < n; j++) {\n                mat[i * n + j] = 0.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k + 4 <= n; k += 4) {\n                __m256d a0 = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b0 = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d a1 = _mm256_loadu_pd(mat1 + i * n + k + 4);\n                __m256d b1 = _mm256_loadu_pd(mat2 + (k + 4) * p + j);\n                sum = _mm256_fmadd_pd(a0, b0, sum);\n                sum = _mm256_fmadd_pd(a1, b1, sum);\n            }\n            // Handle remaining columns if n is not a multiple of 4\n            for (size_t k = n - (n % 4); k < n; ++k) {\n                sum[0] += mat1[i * n + k] * mat2[k * p + j];\n            }\n            // Round to nearest integer\n            __m256d rounded = _mm256_roundeven_pd(sum);\n            _mm256_storeu_pd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b0 = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d b1 = _mm256_loadu_pd(mat2 + (k + 1) * p + j);\n                __m256d b2 = _mm256_loadu_pd(mat2 + (k + 2) * p + j);\n                __m256d b3 = _mm256_loadu_pd(mat2 + (k + 3) * p + j);\n\n                __m256d prod0 = _mm256_mul_pd(a, b0);\n                __m256d prod1 = _mm256_mul_pd(a, b1);\n                __m256d prod2 = _mm256_mul_pd(a, b2);\n                __m256d prod3 = _mm256_mul_pd(a, b3);\n\n                sum = _mm256_add_pd(sum, prod0);\n                sum = _mm256_add_pd(sum, prod1);\n                sum = _mm256_add_pd(sum, prod2);\n                sum = _mm256_add_pd(sum, prod3);\n            }\n            // Horizontal sum reduction\n            __m256d s01 = _mm256_hadd_pd(sum, sum);\n            __m256d s23 = _mm256_hadd_pd(sum, sum);\n            __m256d s0123 = _mm256_add_pd(s01, s23);\n            __m256d rounded = _mm256_round_pd(s0123, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            _mm256_storeu_pd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b0 = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d b1 = _mm256_loadu_pd(mat2 + (k + 1) * p + j);\n                __m256d b2 = _mm256_loadu_pd(mat2 + (k + 2) * p + j);\n                __m256d b3 = _mm256_loadu_pd(mat2 + (k + 3) * p + j);\n\n                __m256d prod0 = _mm256_mul_pd(a, b0);\n                __m256d prod1 = _mm256_mul_pd(a, b1);\n                __m256d prod2 = _mm256_mul_pd(a, b2);\n                __m256d prod3 = _mm256_mul_pd(a, b3);\n\n                sum = _mm256_add_pd(sum, prod0);\n                sum = _mm256_add_pd(sum, prod1);\n                sum = _mm256_add_pd(sum, prod2);\n                sum = _mm256_add_pd(sum, prod3);\n            }\n            // Horizontal sum and round\n            __m128d low = _mm256_castpd256_pd128(sum);\n            __m128d high = _mm256_extractf128_pd(sum, 1);\n            low = _mm_add_pd(low, high);\n            __m128d temp = _mm_hadd_pd(low, low);\n            double rounded = _mm_cvtsd_f64(_mm_round_sd(temp, temp, _MM_FROUND_TO_NEAREST_INT));\n            dst[i * p + j] = rounded;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k + 4 <= n; k += 4) {\n                __m256d a0 = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b0 = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d a1 = _mm256_loadu_pd(mat1 + i * n + k + 4);\n                __m256d b1 = _mm256_loadu_pd(mat2 + (k + 4) * p + j);\n                sum = _mm256_fmadd_pd(a0, b0, sum);\n                sum = _mm256_fmadd_pd(a1, b1, sum);\n            }\n            // Handle remaining columns if n is not a multiple of 4\n            for (size_t k = n - (n % 4); k < n; ++k) {\n                sum[0] += mat1[i * n + k] * mat2[k * p + j];\n            }\n            // Round to nearest integer\n            __m256d rounded = _mm256_roundeven_pd(sum);\n            _mm256_storeu_pd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t k = 0; k < n; k += 4) {\n                __m256d a = _mm256_loadu_pd(mat1 + i * n + k);\n                __m256d b0 = _mm256_loadu_pd(mat2 + k * p + j);\n                __m256d b1 = _mm256_loadu_pd(mat2 + (k + 1) * p + j);\n                __m256d b2 = _mm256_loadu_pd(mat2 + (k + 2) * p + j);\n                __m256d b3 = _mm256_loadu_pd(mat2 + (k + 3) * p + j);\n\n                __m256d prod0 = _mm256_mul_pd(a, b0);\n                __m256d prod1 = _mm256_mul_pd(a, b1);\n                __m256d prod2 = _mm256_mul_pd(a, b2);\n                __m256d prod3 = _mm256_mul_pd(a, b3);\n\n                sum = _mm256_add_pd(sum, prod0);\n                sum = _mm256_add_pd(sum, prod1);\n                sum = _mm256_add_pd(sum, prod2);\n                sum = _mm256_add_pd(sum, prod3);\n            }\n            // Horizontal sum reduction\n            __m128d low = _mm256_castpd256_pd128(sum);\n            __m128d high = _mm256_extractf128_pd(sum, 1);\n            low = _mm_add_pd(low, high);\n            __m128d tmp = _mm_hadd_pd(low, low);\n            double rounded_val = _mm_cvtsd_f64(_mm_round_sd(tmp, tmp, _MM_FROUND_TO_NEAREST_INT));\n            dst[i * p + j] = rounded_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 four = _mm256_set1_ps(4.0f);\n    for (size_t i = 0; i < rows; i++) {\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256 row = _mm256_loadu_ps(src + i * cols + j);\n            // Transpose: Load column-wise\n            __m256 col0 = _mm256_broadcast_ss((const float*)(src + j * cols + i));\n            __m256 col1 = _mm256_broadcast_ss((const float*)(src + (j+1) * cols + i));\n            __m256 col2 = _mm256_broadcast_ss((const float*)(src + (j+2) * cols + i));\n            __m256 col3 = _mm256_broadcast_ss((const float*)(src + (j+3) * cols + i));\n            __m256 col4 = _mm256_broadcast_ss((const float*)(src + (j+4) * cols + i));\n            __m256 col5 = _mm256_broadcast_ss((const float*)(src + (j+5) * cols + i));\n            __m256 col6 = _mm256_broadcast_ss((const float*)(src + (j+6) * cols + i));\n            __m256 col7 = _mm256_broadcast_ss((const float*)(src + (j+7) * cols + i));\n            // Combine into a single vector\n            __m256 transposed = _mm256_permute2f128_ps(col0, col1, 0x20);\n            transposed = _mm256_insertf128_ps(transposed, _mm256_permute2f128_ps(col2, col3, 0x20), 1);\n            transposed = _mm256_insertf128_ps(transposed, _mm256_permute2f128_ps(col4, col5, 0x20), 2);\n            transposed = _mm256_insertf128_ps(transposed, _mm256_permute2f128_ps(col6, col7, 0x20), 3);\n            // Round to nearest quarter\n            __m256 rounded = _mm256_mul_ps(_mm256_round_ps(_mm256_mul_ps(transposed, four), _MM_FROUND_TO_NEAREST_INT), quarter);\n            // Store the result\n            _mm256_storeu_ps(dst + j * rows + i, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256 quarter = _mm256_set1_ps(0.25f);\n    const __m256 four = _mm256_set1_ps(4.0f);\n    const __m256i zero_vec = _mm256_setzero_si256();\n\n    size_t r, c;\n    for (r = 0; r < rows; r += 8) {\n        for (c = 0; c < cols; c += 8) {\n            __m256 row0 = _mm256_setzero_ps();\n            __m256 row1 = _mm256_setzero_ps();\n            __m256 row2 = _mm256_setzero_ps();\n            __m256 row3 = _mm256_setzero_ps();\n            __m256 row4 = _mm256_setzero_ps();\n            __m256 row5 = _mm256_setzero_ps();\n            __m256 row6 = _mm256_setzero_ps();\n            __m256 row7 = _mm256_setzero_ps();\n\n            size_t i, j;\n            for (j = 0; j < 8 && (r + j) < rows; j++) {\n                __m256 col_data = _mm256_loadu_ps(src + (r + j) * cols + c);\n\n                __m256 rounded = _mm256_add_ps(_mm256_mul_ps(col_data, four), quarter);\n                rounded = _mm256_floor_ps(rounded);\n                rounded = _mm256_div_ps(rounded, four);\n\n                if (j == 0) row0 = rounded;\n                else if (j == 1) row1 = rounded;\n                else if (j == 2) row2 = rounded;\n                else if (j == 3) row3 = rounded;\n                else if (j == 4) row4 = rounded;\n                else if (j == 5) row5 = rounded;\n                else if (j == 6) row6 = rounded;\n                else if (j == 7) row7 = rounded;\n            }\n\n            _mm256_storeu_ps(dst + c * rows + r, row0);\n            _mm256_storeu_ps(dst + c * rows + r + 8, row1);\n            _mm256_storeu_ps(dst + c * rows + r + 16, row2);\n            _mm256_storeu_ps(dst + c * rows + r + 24, row3);\n            _mm256_storeu_ps(dst + c * rows + r + 32, row4);\n            _mm256_storeu_ps(dst + c * rows + r + 40, row5);\n            _mm256_storeu_ps(dst + c * rows + r + 48, row6);\n            _mm256_storeu_ps(dst + c * rows + r + 56, row7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    __m256 quarter = _mm256_set1_ps(0.25f);\n    __m256 minus_half = _mm256_set1_ps(-0.5f);\n\n    for (r = 0; r + 7 < rows; r += 8) {\n        size_t c;\n        for (c = 0; c + 7 < cols; c += 8) {\n            __m256 row0 = _mm256_loadu_ps(src + r * cols + c + 0 * cols);\n            __m256 row1 = _mm256_loadu_ps(src + r * cols + c + 1 * cols);\n            __m256 row2 = _mm256_loadu_ps(src + r * cols + c + 2 * cols);\n            __m256 row3 = _mm256_loadu_ps(src + r * cols + c + 3 * cols);\n            __m256 row4 = _mm256_loadu_ps(src + r * cols + c + 4 * cols);\n            __m256 row5 = _mm256_loadu_ps(src + r * cols + c + 5 * cols);\n            __m256 row6 = _mm256_loadu_ps(src + r * cols + c + 6 * cols);\n            __m256 row7 = _mm256_loadu_ps(src + r * cols + c + 7 * cols);\n\n            __m256 t0 = _mm256_add_ps(row0, _mm256_set1_ps(cols));\n            __m256 t1 = _mm256_add_ps(row1, _mm256_set1_ps(cols));\n            __m256 t2 = _mm256_add_ps(row2, _mm256_set1_ps(cols));\n            __m256 t3 = _mm256_add_ps(row3, _mm256_set1_ps(cols));\n            __m256 t4 = _mm256_add_ps(row4, _mm256_set1_ps(cols));\n            __m256 t5 = _mm256_add_ps(row5, _mm256_set1_ps(cols));\n            __m256 t6 = _mm256_add_ps(row6, _mm256_set1_ps(cols));\n            __m256 t7 = _mm256_add_ps(row7, _mm256_set1_ps(cols));\n\n            __m256i i0 = _mm256_cvttps_epi32(_mm256_mul_ps(t0, quarter));\n            __m256i i1 = _mm256_cvttps_epi32(_mm256_mul_ps(t1, quarter));\n            __m256i i2 = _mm256_cvttps_epi32(_mm256_mul_ps(t2, quarter));\n            __m256i i3 = _mm256_cvttps_epi32(_mm256_mul_ps(t3, quarter));\n            __m256i i4 = _mm256_cvttps_epi32(_mm256_mul_ps(t4, quarter));\n            __m256i i5 = _mm256_cvttps_epi32(_mm256_mul_ps(t5, quarter));\n            __m256i i6 = _mm256_cvttps_epi32(_mm256_mul_ps(t6, quarter));\n            __m256i i7 = _mm256_cvttps_epi32(_mm256_mul_ps(t7, quarter));\n\n            __m256 r0 = _mm256_cvtepi32_ps(i0);\n            __m256 r1 = _mm256_cvtepi32_ps(i1);\n            __m256 r2 = _mm256_cvtepi32_ps(i2);\n            __m256 r3 = _mm256_cvtepi32_ps(i3);\n            __m256 r4 = _mm256_cvtepi32_ps(i4);\n            __m256 r5 = _mm256_cvtepi32_ps(i5);\n            __m256 r6 = _mm256_cvtepi32_ps(i6);\n            __m256 r7 = _mm256_cvtepi32_ps(i7);\n\n            _mm256_storeu_ps(dst + c * rows + r + 0 * cols, r0);\n            _mm256_storeu_ps(dst + c * rows + r + 1 * cols, r1);\n            _mm256_storeu_ps(dst + c * rows + r + 2 * cols, r2);\n            _mm256_storeu_ps(dst + c * rows + r + 3 * cols, r3);\n            _mm256_storeu_ps(dst + c * rows + r + 4 * cols, r4);\n            _mm256_storeu_ps(dst + c * rows + r + 5 * cols, r5);\n            _mm256_storeu_ps(dst + c * rows + r + 6 * cols, r6);\n            _mm256_storeu_ps(dst + c * rows + r + 7 * cols, r7);\n        }\n    }\n\n    // Handle remaining rows and columns\n    for (; r < rows; ++r) {\n        for (size_t c = 0; c < cols; ++c) {\n            float value = src[r * cols + c];\n            int rounded = (int)(value * 4 + 0.5f) / 4.0f;\n            dst[c * rows + r] = rounded;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    __m256 quarter = _mm256_set1_ps(0.25f);\n    __m256 minus_half = _mm256_set1_ps(-0.5f);\n\n    for (r = 0; r < rows - (rows % 8); r += 8) {\n        size_t c;\n        for (c = 0; c < cols - (cols % 8); c += 8) {\n            __m256 row0 = _mm256_loadu_ps(src + r * cols + c + 0 * cols);\n            __m256 row1 = _mm256_loadu_ps(src + r * cols + c + 1 * cols);\n            __m256 row2 = _mm256_loadu_ps(src + r * cols + c + 2 * cols);\n            __m256 row3 = _mm256_loadu_ps(src + r * cols + c + 3 * cols);\n            __m256 row4 = _mm256_loadu_ps(src + r * cols + c + 4 * cols);\n            __m256 row5 = _mm256_loadu_ps(src + r * cols + c + 5 * cols);\n            __m256 row6 = _mm256_loadu_ps(src + r * cols + c + 6 * cols);\n            __m256 row7 = _mm256_loadu_ps(src + r * cols + c + 7 * cols);\n\n            __m256 col0 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row0, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n            __m256 col1 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row1, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n            __m256 col2 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row2, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n            __m256 col3 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row3, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n            __m256 col4 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row4, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n            __m256 col5 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row5, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n            __m256 col6 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row6, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n            __m256 col7 = _mm256_add_ps(_mm256_mul_ps(_mm256_round_ps(_mm256_add_ps(row7, minus_half), _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC), quarter), quarter);\n\n            _mm256_storeu_ps(dst + c * rows + r + 0 * rows, col0);\n            _mm256_storeu_ps(dst + c * rows + r + 1 * rows, col1);\n            _mm256_storeu_ps(dst + c * rows + r + 2 * rows, col2);\n            _mm256_storeu_ps(dst + c * rows + r + 3 * rows, col3);\n            _mm256_storeu_ps(dst + c * rows + r + 4 * rows, col4);\n            _mm256_storeu_ps(dst + c * rows + r + 5 * rows, col5);\n            _mm256_storeu_ps(dst + c * rows + r + 6 * rows, col6);\n            _mm256_storeu_ps(dst + c * rows + r + 7 * rows, col7);\n        }\n    }\n\n    // Handle remaining rows and columns\n    for (size_t i = r; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            float value = src[i * cols + j];\n            float rounded_value = roundf(value * 4.0f) / 4.0f;\n            dst[j * rows + i] = rounded_value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    __m256 quarter = _mm256_set1_ps(0.25f);\n    __m256 round_mask = _mm256_set1_ps(0.5f);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for (r = 0; r < rows - (rows % 8); r += 8) {\n        size_t c;\n        for (c = 0; c < cols - (cols % 8); c += 8) {\n            __m256 row0 = _mm256_loadu_ps(src + r * cols + c + 0 * cols);\n            __m256 row1 = _mm256_loadu_ps(src + r * cols + c + 1 * cols);\n            __m256 row2 = _mm256_loadu_ps(src + r * cols + c + 2 * cols);\n            __m256 row3 = _mm256_loadu_ps(src + r * cols + c + 3 * cols);\n            __m256 row4 = _mm256_loadu_ps(src + r * cols + c + 4 * cols);\n            __m256 row5 = _mm256_loadu_ps(src + r * cols + c + 5 * cols);\n            __m256 row6 = _mm256_loadu_ps(src + r * cols + c + 6 * cols);\n            __m256 row7 = _mm256_loadu_ps(src + r * cols + c + 7 * cols);\n\n            __m256 col0 = _mm256_permute4x64_ps(row0, 0x0E);\n            __m256 col1 = _mm256_permute4x64_ps(row1, 0x0E);\n            __m256 col2 = _mm256_permute4x64_ps(row2, 0x0E);\n            __m256 col3 = _mm256_permute4x64_ps(row3, 0x0E);\n            __m256 col4 = _mm256_permute4x64_ps(row4, 0x0E);\n            __m256 col5 = _mm256_permute4x64_ps(row5, 0x0E);\n            __m256 col6 = _mm256_permute4x64_ps(row6, 0x0E);\n            __m256 col7 = _mm256_permute4x64_ps(row7, 0x0E);\n\n            __m256 sum0 = _mm256_add_ps(col0, col1);\n            __m256 sum1 = _mm256_add_ps(col2, col3);\n            __m256 sum2 = _mm256_add_ps(col4, col5);\n            __m256 sum3 = _mm256_add_ps(col6, col7);\n\n            __m256 sum0_rounded = _mm256_round_ps(_mm256_add_ps(sum0, round_mask), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 sum1_rounded = _mm256_round_ps(_mm256_add_ps(sum1, round_mask), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 sum2_rounded = _mm256_round_ps(_mm256_add_ps(sum2, round_mask), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            __m256 sum3_rounded = _mm256_round_ps(_mm256_add_ps(sum3, round_mask), _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            __m256 scaled0 = _mm256_sub_ps(sum0_rounded, _mm256_mul_ps(sum0_rounded, quarter));\n            __m256 scaled1 = _mm256_sub_ps(sum1_rounded, _mm256_mul_ps(sum1_rounded, quarter));\n            __m256 scaled2 = _mm256_sub_ps(sum2_rounded, _mm256_mul_ps(sum2_rounded, quarter));\n            __m256 scaled3 = _mm256_sub_ps(sum3_rounded, _mm256_mul_ps(sum3_rounded, quarter));\n\n            _mm256_storeu_ps(dst + c * rows + r + 0 * cols, scaled0);\n            _mm256_storeu_ps(dst + c * rows + r + 1 * cols, scaled1);\n            _mm256_storeu_ps(dst + c * rows + r + 2 * cols, scaled2);\n            _mm256_storeu_ps(dst + c * rows + r + 3 * cols, scaled3);\n        }\n    }\n\n    // Handle remaining rows and columns\n    for (size_t i = r; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            float value = src[i * cols + j];\n            float rounded_value = round(value * 4) / 4.0f;\n            dst[j * rows + i] = rounded_value;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX\n    for (; i <= total_elements - 8; i += 8) {\n        __m256 t1_vec = _mm256_loadu_ps(&tensor1[i]);\n        __m256 t2_vec = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub_vec = _mm256_sub_ps(t1_vec, t2_vec);\n\n        // Round towards zero using integer conversion\n        __m256i int_vec = _mm256_cvttps_epi32(sub_vec);\n        __m256 rounded_vec = _mm256_cvtepi32_ps(int_vec);\n\n        _mm256_storeu_ps(&dst[i], rounded_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        float result = (float)((int)(tensor1[i] - tensor2[i]));\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i, j, k;\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vector_size = 8; // AVX: 8 x 32-bit floats per register\n\n    for (i = 0; i < total_elements; i += vector_size) {\n        __m256 t1_vec = _mm256_loadu_ps(&tensor1[i]);\n        __m256 t2_vec = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub_vec = _mm256_sub_ps(t1_vec, t2_vec);\n\n        // Round towards zero using integer conversion\n        __m256i int_vec = _mm256_cvttps_epi32(sub_vec);\n        __m256 rounded_vec = _mm256_cvtepi32_ps(int_vec);\n\n        _mm256_storeu_ps(&dst[i], rounded_vec);\n    }\n\n    // Handle remaining elements if any\n    for (j = i; j < total_elements; ++j) {\n        dst[j] = roundf(tensor1[j] - tensor2[j]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX\n    for (; i <= total_elements - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_roundps(_mm256_sub_ps(vec1, vec2), _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        dst[i] = roundf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX\n    for (; i <= total_elements - 8; i += 8) {\n        __m256 t1_vec = _mm256_loadu_ps(&tensor1[i]);\n        __m256 t2_vec = _mm256_loadu_ps(&tensor2[i]);\n        __m256 sub_vec = _mm256_sub_ps(t1_vec, t2_vec);\n\n        // Round towards zero\n        __m256 sign_mask = _mm256_set1_ps(-0.0f);\n        __m256 abs_vec = _mm256_andnot_ps(sign_mask, sub_vec);\n        __m256 floor_vec = _mm256_floor_ps(abs_vec);\n        __m256 ceil_vec = _mm256_ceil_ps(abs_vec);\n        __m256 cmp_lt = _mm256_cmp_ps(abs_vec, floor_vec, _CMP_LT_OS);\n        __m256 rounded_vec = _mm256_blendv_ps(floor_vec, ceil_vec, cmp_lt);\n\n        // Apply sign back\n        __m256 sign_vec = _mm256_and_ps(sub_vec, sign_mask);\n        __m256 final_vec = _mm256_xor_ps(rounded_vec, sign_vec);\n\n        _mm256_storeu_ps(&dst[i], final_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        float t1_val = tensor1[i];\n        float t2_val = tensor2[i];\n        float result = t1_val - t2_val;\n        dst[i] = (result > 0.0f) ? floor(result) : ceil(result);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 8 using AVX\n    for (; i <= total_elements - 8; i += 8) {\n        __m256 vec1 = _mm256_loadu_ps(&tensor1[i]);\n        __m256 vec2 = _mm256_loadu_ps(&tensor2[i]);\n        __m256 result = _mm256_roundps(_mm256_sub_ps(vec1, vec2), _MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC);\n        _mm256_storeu_ps(&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        dst[i] = roundf(tensor1[i] - tensor2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 4); // Process full vectors of 4 elements\n\n    // Process complete rows of 4 elements\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < limit; j += 4) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n\n        // Handle remaining columns if n is not a multiple of 4\n        for (j = limit; j < n; j++) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 4); // Process full vectors of 4 elements\n\n    // Process complete rows of 4 elements\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < limit; j += 4) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n\n        // Handle remaining columns if n is not a multiple of 4\n        for (j = limit; j < n; j++) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 4);\n    \n    // Process full vector width\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j < limit; j += 4) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n        \n        // Handle remaining elements\n        for (; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 4); // Process full vectors of 4 elements\n\n    // Process complete rows of 4 elements\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < limit; j += 4) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2[i * n + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst[i * n + j], result);\n        }\n\n        // Handle remaining columns if n is not a multiple of 4\n        for (j = limit; j < n; j++) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 4); // Process full vectors of 4 elements\n\n    // Process complete rows\n    for (i = 0; i < m; i++) {\n        const double* row1 = mat1 + i * n;\n        const double* row2 = mat2 + i * n;\n        double* res_row = dst + i * n;\n\n        __m256d v1, v2;\n        for (j = 0; j < limit; j += 4) {\n            v1 = _mm256_loadu_pd(row1 + j);\n            v2 = _mm256_loadu_pd(row2 + j);\n            _mm256_storeu_pd(res_row + j, _mm256_mul_pd(v1, v2));\n        }\n\n        // Handle remaining elements\n        for (; j < n; j++) {\n            res_row[j] = row1[j] * row2[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 4; i += 4) {\n        __m256 row0 = _mm256_loadu_ps(mat + i * n);\n        __m256 row1 = _mm256_loadu_ps(mat + (i + 1) * n);\n        __m256 row2 = _mm256_loadu_ps(mat + (i + 2) * n);\n        __m256 row3 = _mm256_loadu_ps(mat + (i + 3) * n);\n\n        __m256 mul0 = _mm256_mul_ps(row0, _mm256_broadcast_ss(vec));\n        __m256 mul1 = _mm256_mul_ps(row1, _mm256_broadcast_ss(vec + 1));\n        __m256 mul2 = _mm256_mul_ps(row2, _mm256_broadcast_ss(vec + 2));\n        __m256 mul3 = _mm256_mul_ps(row3, _mm256_broadcast_ss(vec + 3));\n\n        __m256 sum0 = _mm256_hadd_ps(mul0, mul1);\n        __m256 sum1 = _mm256_hadd_ps(mul2, mul3);\n\n        __m256 final_sum = _mm256_add_ps(sum0, sum1);\n\n        _mm256_storeu_ps(dst + i, final_sum);\n    }\n    for (; i < m; i++) {\n        dst[i] = 0.0f;\n        for (size_t j = 0; j < n; j++) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 8; i += 8) {\n        __m256 row0 = _mm256_setzero_ps();\n        __m256 row1 = _mm256_setzero_ps();\n        __m256 row2 = _mm256_setzero_ps();\n        __m256 row3 = _mm256_setzero_ps();\n        __m256 row4 = _mm256_setzero_ps();\n        __m256 row5 = _mm256_setzero_ps();\n        __m256 row6 = _mm256_setzero_ps();\n        __m256 row7 = _mm256_setzero_ps();\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 v = _mm256_loadu_ps(&vec[j]);\n            __m256 m0 = _mm256_loadu_ps(&mat[i * n + j + 0]);\n            __m256 m1 = _mm256_loadu_ps(&mat[i * n + j + 8]);\n            __m256 m2 = _mm256_loadu_ps(&mat[i * n + j + 16]);\n            __m256 m3 = _mm256_loadu_ps(&mat[i * n + j + 24]);\n            __m256 m4 = _mm256_loadu_ps(&mat[i * n + j + 32]);\n            __m256 m5 = _mm256_loadu_ps(&mat[i * n + j + 40]);\n            __m256 m6 = _mm256_loadu_ps(&mat[i * n + j + 48]);\n            __m256 m7 = _mm256_loadu_ps(&mat[i * n + j + 56]);\n\n            row0 = _mm256_fmadd_ps(m0, v, row0);\n            row1 = _mm256_fmadd_ps(m1, v, row1);\n            row2 = _mm256_fmadd_ps(m2, v, row2);\n            row3 = _mm256_fmadd_ps(m3, v, row3);\n            row4 = _mm256_fmadd_ps(m4, v, row4);\n            row5 = _mm256_fmadd_ps(m5, v, row5);\n            row6 = _mm256_fmadd_ps(m6, v, row6);\n            row7 = _mm256_fmadd_ps(m7, v, row7);\n        }\n\n        _mm256_storeu_ps(&dst[i + 0], row0);\n        _mm256_storeu_ps(&dst[i + 8], row1);\n        _mm256_storeu_ps(&dst[i + 16], row2);\n        _mm256_storeu_ps(&dst[i + 24], row3);\n        _mm256_storeu_ps(&dst[i + 32], row4);\n        _mm256_storeu_ps(&dst[i + 40], row5);\n        _mm256_storeu_ps(&dst[i + 48], row6);\n        _mm256_storeu_ps(&dst[i + 56], row7);\n    }\n    for (; i < m; i++) {\n        dst[i] = 0.0f;\n        for (size_t j = 0; j < n; j++) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 8);\n    __m256 v_vec_chunk, v_sum, v_mat_chunk;\n\n    for (i = 0; i < m; i++) {\n        v_sum = _mm256_setzero_ps();\n\n        for (j = 0; j < limit; j += 8) {\n            v_vec_chunk = _mm256_loadu_ps(&vec[j]);\n            v_mat_chunk = _mm256_loadu_ps(&mat[i * n + j]);\n            v_sum = _mm256_fmadd_ps(v_vec_chunk, v_mat_chunk, v_sum);\n        }\n\n        // Handle remaining elements if n is not a multiple of 8\n        for (; j < n; j++) {\n            v_sum[0] += mat[i * n + j] * vec[j];\n        }\n\n        _mm256_storeu_ps(&dst[i], v_sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i = 0;\n    for (; i <= m - 8; i += 8) {\n        __m256 row0 = _mm256_setzero_ps();\n        __m256 row1 = _mm256_setzero_ps();\n        __m256 row2 = _mm256_setzero_ps();\n        __m256 row3 = _mm256_setzero_ps();\n        __m256 row4 = _mm256_setzero_ps();\n        __m256 row5 = _mm256_setzero_ps();\n        __m256 row6 = _mm256_setzero_ps();\n        __m256 row7 = _mm256_setzero_ps();\n\n        for (size_t j = 0; j < n; j += 8) {\n            __m256 v = _mm256_loadu_ps(&vec[j]);\n            __m256 m0 = _mm256_loadu_ps(&mat[i * n + j + 0]);\n            __m256 m1 = _mm256_loadu_ps(&mat[i * n + j + 8]);\n            __m256 m2 = _mm256_loadu_ps(&mat[i * n + j + 16]);\n            __m256 m3 = _mm256_loadu_ps(&mat[i * n + j + 24]);\n            __m256 m4 = _mm256_loadu_ps(&mat[i * n + j + 32]);\n            __m256 m5 = _mm256_loadu_ps(&mat[i * n + j + 40]);\n            __m256 m6 = _mm256_loadu_ps(&mat[i * n + j + 48]);\n            __m256 m7 = _mm256_loadu_ps(&mat[i * n + j + 56]);\n\n            row0 = _mm256_fmadd_ps(m0, v, row0);\n            row1 = _mm256_fmadd_ps(m1, v, row1);\n            row2 = _mm256_fmadd_ps(m2, v, row2);\n            row3 = _mm256_fmadd_ps(m3, v, row3);\n            row4 = _mm256_fmadd_ps(m4, v, row4);\n            row5 = _mm256_fmadd_ps(m5, v, row5);\n            row6 = _mm256_fmadd_ps(m6, v, row6);\n            row7 = _mm256_fmadd_ps(m7, v, row7);\n        }\n\n        _mm256_storeu_ps(&dst[i + 0], row0);\n        _mm256_storeu_ps(&dst[i + 8], row1);\n        _mm256_storeu_ps(&dst[i + 16], row2);\n        _mm256_storeu_ps(&dst[i + 24], row3);\n        _mm256_storeu_ps(&dst[i + 32], row4);\n        _mm256_storeu_ps(&dst[i + 40], row5);\n        _mm256_storeu_ps(&dst[i + 48], row6);\n        _mm256_storeu_ps(&dst[i + 56], row7);\n    }\n    for (; i < m; i++) {\n        dst[i] = 0.0f;\n        for (size_t j = 0; j < n; j++) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m256 v_vec = _mm256_loadu_ps(vec);\n    __m256 sum;\n\n    for (i = 0; i + 7 < m; i += 8) {\n        sum = _mm256_setzero_ps();\n        for (j = 0; j < n; j += 8) {\n            __m256 v_mat = _mm256_loadu_ps(mat + i * n + j);\n            __m256 v_mul = _mm256_mul_ps(v_mat, v_vec);\n            sum = _mm256_add_ps(sum, v_mul);\n        }\n        _mm256_storeu_ps(dst + i, sum);\n    }\n\n    // Handle remaining rows if m is not a multiple of 8\n    for (; i < m; i++) {\n        dst[i] = 0.0f;\n        for (j = 0; j < n; j++) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    for (i = 0; i < m; i++) {\n        for (j = 0; j + 4 <= n; j += 4) {\n            __m256d vec = _mm256_loadu_pd(mat + i * n + j);\n            sum_vec = _mm256_add_pd(sum_vec, vec);\n        }\n        for (; j < n; j++) {\n            sum_vec[0] += mat[i * n + j];\n        }\n    }\n    __m256d count_vec = _mm256_set1_pd((double)(m * n));\n    __m256d avg_vec = _mm256_div_pd(sum_vec, count_vec);\n    double avg[4];\n    _mm256_storeu_pd(avg, avg_vec);\n    double total_avg = avg[0] + avg[1] + avg[2] + avg[3];\n    return total_avg / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    size_t remaining_rows;\n\n    for (i = 0; i < m; i += 4) {\n        __m256d row_sum = _mm256_setzero_pd();\n        for (j = 0; j < n; j += 4) {\n            __m256d vec = _mm256_loadu_pd(mat + i * n + j);\n            row_sum = _mm256_add_pd(row_sum, vec);\n        }\n        sum_vec = _mm256_add_pd(sum_vec, row_sum);\n    }\n\n    double sum_array[4];\n    _mm256_storeu_pd(sum_array, sum_vec);\n\n    double total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    size_t total_elements = m * n;\n    return total_sum / total_elements;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    size_t remaining_rows;\n\n    for (i = 0; i < m; i += 4) {\n        __m256d row_sum = _mm256_setzero_pd();\n        size_t cols_remaining = n;\n\n        for (j = 0; j < n; j += 4) {\n            __m256d vec = _mm256_loadu_pd(mat + i * n + j);\n            row_sum = _mm256_add_pd(row_sum, vec);\n            cols_remaining -= 4;\n        }\n\n        if (cols_remaining > 0) {\n            __m128d tail_vec = _mm_loadu_pd(mat + i * n + j);\n            __m256d tail_vec_256 = _mm256_insertf128_pd(_mm256_setzero_pd(), tail_vec, 0);\n            row_sum = _mm256_add_pd(row_sum, tail_vec_256);\n        }\n\n        sum_vec = _mm256_add_pd(sum_vec, row_sum);\n    }\n\n    remaining_rows = m % 4;\n    if (remaining_rows > 0) {\n        __m128d sum_tail = _mm_setzero_pd();\n        for (i += 4; i < m; i++) {\n            __m128d row_vec = _mm_loadu_pd(mat + i * n);\n            sum_tail = _mm_add_pd(sum_tail, row_vec);\n        }\n        __m256d sum_tail_256 = _mm256_insertf128_pd(_mm256_setzero_pd(), sum_tail, 0);\n        sum_vec = _mm256_add_pd(sum_vec, sum_tail_256);\n    }\n\n    __m256d total_sum = sum_vec;\n    int remaining_elements = m * n - (m / 4) * 4 * n;\n    if (remaining_elements > 0) {\n        double scalar_sum = 0.0;\n        for (i = m - remaining_rows; i < m; i++) {\n            for (j = n - remaining_elements; j < n; j++) {\n                scalar_sum += mat[i * n + j];\n            }\n        }\n        total_sum = _mm256_add_pd(total_sum, _mm256_set1_pd(scalar_sum));\n    }\n\n    __m256d count_vec = _mm256_set1_pd((double)(m * n));\n    __m256d avg_vec = _mm256_div_pd(total_sum, count_vec);\n\n    double avg[4];\n    _mm256_storeu_pd(avg, avg_vec);\n\n    double total_avg = 0.0;\n    for (i = 0; i < 4; i++) {\n        total_avg += avg[i];\n    }\n\n    return total_avg / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    for (i = 0; i < m; ++i) {\n        for (j = 0; j + 4 <= n; j += 4) {\n            __m256d row_vec = _mm256_loadu_pd(mat + i * n + j);\n            sum_vec = _mm256_add_pd(sum_vec, row_vec);\n        }\n        for (; j < n; ++j) {\n            sum_vec[0] += mat[i * n + j];\n        }\n    }\n    __m256d count_vec = _mm256_set1_pd((double)(m * n));\n    __m256d avg_vec = _mm256_div_pd(sum_vec, count_vec);\n    double avg;\n    _mm256_storeu_sd(&avg, avg_vec);\n    return avg;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum_vec = _mm256_setzero_pd();\n    size_t i, j;\n    size_t remaining_rows;\n\n    for (i = 0; i < m; i += 4) {\n        __m256d row_sum = _mm256_setzero_pd();\n        size_t cols_remaining = n;\n\n        for (j = 0; j < n; j += 4) {\n            __m256d vec = _mm256_loadu_pd(mat + i * n + j);\n            row_sum = _mm256_add_pd(row_sum, vec);\n            cols_remaining -= 4;\n        }\n\n        if (cols_remaining > 0) {\n            __m128d tail_vec = _mm_loadu_pd(mat + i * n + j);\n            __m256d tail_vec_256 = _mm256_insertf128_pd(_mm256_setzero_pd(), tail_vec, 0);\n            row_sum = _mm256_add_pd(row_sum, tail_vec_256);\n        }\n\n        sum_vec = _mm256_add_pd(sum_vec, row_sum);\n    }\n\n    remaining_rows = m % 4;\n    if (remaining_rows > 0) {\n        __m128d sum_tail = _mm_setzero_pd();\n        for (i += 4; i < m; i++) {\n            sum_tail = _mm_add_pd(sum_tail, _mm_loadu_pd(mat + i * n));\n        }\n        __m256d sum_tail_256 = _mm256_insertf128_pd(_mm256_setzero_pd(), sum_tail, 0);\n        sum_vec = _mm256_add_pd(sum_vec, sum_tail_256);\n    }\n\n    __m256d total_sum_vec = sum_vec;\n    int32_t total_elements = m * n;\n\n    // Horizontal sum\n    __m128d low = _mm256_extractf128_pd(total_sum_vec, 0);\n    __m128d high = _mm256_extractf128_pd(total_sum_vec, 1);\n    __m128d sum = _mm_add_pd(low, high);\n    low = _mm_shuffle_pd(high, low, 0);\n    sum = _mm_add_pd(sum, low);\n\n    double total_sum[2];\n    _mm_storeu_pd(total_sum, sum);\n    double total_sum_value = total_sum[0] + total_sum[1];\n\n    return total_sum_value / total_elements;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256i prev_vec = _mm256_setzero_si256();\n        bool ascending = directions[r];\n        size_t c = 0;\n\n        while (c + 8 <= cols) {\n            __m256i curr_vec = _mm256_loadu_si256((__m256i*)(matrix + r * cols + c));\n            __m256i cmp_gt = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n            __m256i cmp_lt = _mm256_cmpgt_epi32(curr_vec, prev_vec);\n\n            if (ascending) {\n                if (_mm256_movemask_epi8(cmp_gt)) return false;\n            } else {\n                if (_mm256_movemask_epi8(cmp_lt)) return false;\n            }\n\n            prev_vec = curr_vec;\n            c += 8;\n        }\n\n        for (; c < cols; c++) {\n            if ((ascending && matrix[r * cols + c] < prev_vec.m256i_i32[0]) ||\n                (!ascending && matrix[r * cols + c] > prev_vec.m256i_i32[0])) {\n                return false;\n            }\n            prev_vec.m256i_i32[0] = matrix[r * cols + c];\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256i prev_vec = _mm256_setzero_si256();\n        bool ascending = directions[r];\n        size_t c = 0;\n\n        while (c + 8 <= cols) {\n            __m256i current_vec = _mm256_loadu_si256((__m256i*)(matrix + r * cols + c));\n            __m256i cmp_mask = _mm256_cmpgt_epi32(prev_vec, current_vec);\n\n            if (!ascending) {\n                cmp_mask = _mm256_xor_si256(cmp_mask, _mm256_set1_epi32(-1));\n            }\n\n            if (_mm256_movemask_epi8(cmp_mask) != 0) {\n                return false;\n            }\n\n            prev_vec = current_vec;\n            c += 8;\n        }\n\n        for (; c < cols; c++) {\n            if ((ascending && matrix[r * cols + c] < prev_vec.m256i_i32[0]) ||\n                (!ascending && matrix[r * cols + c] > prev_vec.m256i_i32[0])) {\n                return false;\n            }\n            prev_vec.m256i_i32[0] = matrix[r * cols + c];\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256i prev_vec = _mm256_setzero_si256();\n        bool is_sorted = true;\n        size_t c = 0;\n\n        // Process full vectors\n        for (; c + 8 <= cols; c += 8) {\n            __m256i curr_vec = _mm256_loadu_si256((__m256i*)&matrix[r * cols + c]);\n            __m256i cmp_mask = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n\n            if (directions[r]) {\n                // Ascending order: Check if all elements satisfy prev <= curr\n                is_sorted &= _mm256_movemask_epi8(_mm256_castsi256_ps(cmp_mask)) == 0;\n            } else {\n                // Descending order: Check if all elements satisfy prev >= curr\n                is_sorted &= _mm256_movemask_epi8(_mm256_castsi256_ps(_mm256_xor_si256(prev_vec, curr_vec))) == 0;\n            }\n\n            if (!is_sorted) break;\n            prev_vec = curr_vec;\n        }\n\n        // Handle remaining elements\n        for (; c < cols; c++) {\n            if (directions[r]) {\n                if (matrix[r * cols + c] < prev_vec.m256i_i32[0]) {\n                    is_sorted = false;\n                    break;\n                }\n            } else {\n                if (matrix[r * cols + c] > prev_vec.m256i_i32[0]) {\n                    is_sorted = false;\n                    break;\n                }\n            }\n            prev_vec.m256i_i32[0] = matrix[r * cols + c];\n        }\n\n        if (!is_sorted) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256i prev_vec = _mm256_setzero_si256();\n        bool sorted = true;\n        size_t c = 0;\n\n        // Process full vectors\n        for (; c + 8 <= cols; c += 8) {\n            __m256i curr_vec = _mm256_loadu_si256((__m256i*)&matrix[r * cols + c]);\n            __m256i cmp_mask = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n\n            if (directions[r]) {\n                // Ascending order: Check if all elements satisfy prev <= curr\n                sorted &= _mm256_movemask_epi8(_mm256_castsi256_ps(cmp_mask)) == 0;\n            } else {\n                // Descending order: Check if all elements satisfy prev >= curr\n                sorted &= _mm256_movemask_epi8(_mm256_castsi256_ps(_mm256_xor_si256(prev_vec, curr_vec))) == 0;\n            }\n\n            if (!sorted) break;\n            prev_vec = curr_vec;\n        }\n\n        // Handle remaining elements\n        for (; c < cols; c++) {\n            if (directions[r]) {\n                if (matrix[r * cols + c] < prev_vec.m256i_i32[0]) {\n                    sorted = false;\n                    break;\n                }\n            } else {\n                if (matrix[r * cols + c] > prev_vec.m256i_i32[0]) {\n                    sorted = false;\n                    break;\n                }\n            }\n            prev_vec.m256i_i32[0] = matrix[r * cols + c];\n        }\n\n        if (!sorted) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r++) {\n        __m256i prev_vec = _mm256_setzero_si256();\n        bool is_sorted = true;\n        size_t c = 0;\n\n        while (c + 8 <= cols) {\n            __m256i current_vec = _mm256_loadu_si256((__m256i*)(matrix + r * cols + c));\n            __m256i cmp_mask = _mm256_cmpgt_epi32(prev_vec, current_vec);\n\n            if (directions[r]) {\n                // Ascending order: Check if all elements are greater than or equal to previous\n                is_sorted &= _mm256_movemask_epi8(_mm256_castsi256_ps(cmp_mask)) == 0;\n            } else {\n                // Descending order: Check if all elements are less than or equal to previous\n                is_sorted &= _mm256_movemask_epi8(_mm256_castsi256_ps(_mm256_xor_si256(prev_vec, current_vec))) == 0;\n            }\n\n            if (!is_sorted) break;\n\n            prev_vec = current_vec;\n            c += 8;\n        }\n\n        // Handle remaining elements\n        for (; c < cols; c++) {\n            if (directions[r] ? matrix[r * cols + c] < prev_vec.m256i_i32[0] : matrix[r * cols + c] > prev_vec.m256i_i32[0]) {\n                is_sorted = false;\n                break;\n            }\n            prev_vec.m256i_i32[0] = matrix[r * cols + c];\n        }\n\n        if (!is_sorted) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 16 elements using AVX2\n    for (; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i div_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i floor_vec = _mm256_mullo_epi16(div_vec, base_vec);\n        \n        _mm256_storeu_si256((__m256i*)&dst[i], floor_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((int32_t)src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 16 elements using AVX2\n    for (; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i div_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i floor_vec = _mm256_mullo_epi16(div_vec, base_vec);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), floor_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((int32_t)src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 16 elements using AVX2\n    for (; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i div_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i floor_vec = _mm256_mullo_epi16(div_vec, base_vec);\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), floor_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((int32_t)src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16((int16_t)base);\n    \n    // Process 16 elements per loop\n    for (; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i div_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i floor_vec = _mm256_sub_epi16(div_vec, _mm256_cmpeq_epi16(div_vec, _mm256_andnot_si256(_mm256_cmpgt_epi16(src_vec, _mm256_mullo_epi16(base_vec, div_vec)), div_vec)));\n        __m256i result_vec = _mm256_mullo_epi16(floor_vec, base_vec);\n        _mm256_storeu_si256((__m256i*)&dst[i], result_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((src[i] / base) * base));\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m256i base_vec = _mm256_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 16 elements using AVX2\n    for (; i <= length - 16; i += 16) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i div_vec = _mm256_div_epi16(src_vec, base_vec);\n        __m256i floor_vec = _mm256_sub_epi16(div_vec, _mm256_cmpeq_epi16(div_vec, _mm256_andnot_si256(_mm256_set1_epi16(1), div_vec)));\n        __m256i result_vec = _mm256_mullo_epi16(floor_vec, base_vec);\n        _mm256_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((src[i] / base) * base));\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i = 0;\n    __m256d vec = _mm256_load_pd(vector);\n\n    // Determine how many full rows can be processed with AVX\n    size_t full_rows = rows / 4 * 4;\n\n    for (; i < full_rows; i += 4) {\n        __m256d row0 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d row1 = _mm256_loadu_pd(matrix + (i + 1) * cols);\n        __m256d row2 = _mm256_loadu_pd(matrix + (i + 2) * cols);\n        __m256d row3 = _mm256_loadu_pd(matrix + (i + 3) * cols);\n\n        __m256d eq0 = _mm256_cmpeq_pd(row0, vec);\n        __m256d eq1 = _mm256_cmpeq_pd(row1, vec);\n        __m256d eq2 = _mm256_cmpeq_pd(row2, vec);\n        __m256d eq3 = _mm256_cmpeq_pd(row3, vec);\n\n        __m256d all_eq0 = _mm256_and_pd(eq0, eq1);\n        __m256d all_eq1 = _mm256_and_pd(eq2, eq3);\n        __m256d final_eq = _mm256_and_pd(all_eq0, all_eq1);\n\n        int mask = _mm256_movemask_pd(final_eq);\n        if (mask == 0b1111) {\n            return true;\n        }\n    }\n\n    // Handle remaining rows that couldn't be processed in chunks of 4\n    for (; i < rows; i++) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j++) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i = 0;\n    __m256d vec = _mm256_load_pd(vector);\n\n    for (; i <= rows - 8; i += 8) {\n        __m256d row1 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d row2 = _mm256_loadu_pd(matrix + (i + 1) * cols);\n        __m256d row3 = _mm256_loadu_pd(matrix + (i + 2) * cols);\n        __m256d row4 = _mm256_loadu_pd(matrix + (i + 3) * cols);\n        __m256d row5 = _mm256_loadu_pd(matrix + (i + 4) * cols);\n        __m256d row6 = _mm256_loadu_pd(matrix + (i + 5) * cols);\n        __m256d row7 = _mm256_loadu_pd(matrix + (i + 6) * cols);\n        __m256d row8 = _mm256_loadu_pd(matrix + (i + 7) * cols);\n\n        __m256d eq1 = _mm256_cmpeq_pd(row1, vec);\n        __m256d eq2 = _mm256_cmpeq_pd(row2, vec);\n        __m256d eq3 = _mm256_cmpeq_pd(row3, vec);\n        __m256d eq4 = _mm256_cmpeq_pd(row4, vec);\n        __m256d eq5 = _mm256_cmpeq_pd(row5, vec);\n        __m256d eq6 = _mm256_cmpeq_pd(row6, vec);\n        __m256d eq7 = _mm256_cmpeq_pd(row7, vec);\n        __m256d eq8 = _mm256_cmpeq_pd(row8, vec);\n\n        __m256d all_eq1 = _mm256_and_pd(eq1, eq2);\n        __m256d all_eq2 = _mm256_and_pd(eq3, eq4);\n        __m256d all_eq3 = _mm256_and_pd(eq5, eq6);\n        __m256d all_eq4 = _mm256_and_pd(eq7, eq8);\n\n        __m256d final_eq1 = _mm256_and_pd(all_eq1, all_eq2);\n        __m256d final_eq2 = _mm256_and_pd(all_eq3, all_eq4);\n\n        __m256d final_result = _mm256_or_pd(final_eq1, final_eq2);\n\n        int mask = _mm256_movemask_pd(final_result);\n        if (mask != 0) return true;\n    }\n\n    for (; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i = 0;\n    __m256d vec = _mm256_load_pd(vector);\n\n    for (; i <= rows - 8; i += 8) {\n        __m256d row0 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d row1 = _mm256_loadu_pd(matrix + (i + 1) * cols);\n        __m256d row2 = _mm256_loadu_pd(matrix + (i + 2) * cols);\n        __m256d row3 = _mm256_loadu_pd(matrix + (i + 3) * cols);\n        __m256d row4 = _mm256_loadu_pd(matrix + (i + 4) * cols);\n        __m256d row5 = _mm256_loadu_pd(matrix + (i + 5) * cols);\n        __m256d row6 = _mm256_loadu_pd(matrix + (i + 6) * cols);\n        __m256d row7 = _mm256_loadu_pd(matrix + (i + 7) * cols);\n\n        __m256d eq0 = _mm256_cmpeq_pd(row0, vec);\n        __m256d eq1 = _mm256_cmpeq_pd(row1, vec);\n        __m256d eq2 = _mm256_cmpeq_pd(row2, vec);\n        __m256d eq3 = _mm256_cmpeq_pd(row3, vec);\n        __m256d eq4 = _mm256_cmpeq_pd(row4, vec);\n        __m256d eq5 = _mm256_cmpeq_pd(row5, vec);\n        __m256d eq6 = _mm256_cmpeq_pd(row6, vec);\n        __m256d eq7 = _mm256_cmpeq_pd(row7, vec);\n\n        __m256d all_eq0 = _mm256_and_pd(eq0, eq1);\n        __m256d all_eq1 = _mm256_and_pd(eq2, eq3);\n        __m256d all_eq2 = _mm256_and_pd(eq4, eq5);\n        __m256d all_eq3 = _mm256_and_pd(eq6, eq7);\n\n        __m256d final_eq0 = _mm256_and_pd(all_eq0, all_eq1);\n        __m256d final_eq1 = _mm256_and_pd(all_eq2, all_eq3);\n\n        __m256d final_check = _mm256_or_pd(final_eq0, final_eq1);\n\n        int mask = _mm256_movemask_pd(final_check);\n        if (mask == 0x0F) {\n            return true;\n        }\n    }\n\n    for (; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i = 0;\n    __m256d vec = _mm256_load_pd(vector);\n\n    for (; i <= rows - 8; i += 8) {\n        __m256d row0 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d row1 = _mm256_loadu_pd(matrix + (i + 1) * cols);\n        __m256d row2 = _mm256_loadu_pd(matrix + (i + 2) * cols);\n        __m256d row3 = _mm256_loadu_pd(matrix + (i + 3) * cols);\n        __m256d row4 = _mm256_loadu_pd(matrix + (i + 4) * cols);\n        __m256d row5 = _mm256_loadu_pd(matrix + (i + 5) * cols);\n        __m256d row6 = _mm256_loadu_pd(matrix + (i + 6) * cols);\n        __m256d row7 = _mm256_loadu_pd(matrix + (i + 7) * cols);\n\n        __m256d eq0 = _mm256_cmpeq_pd(row0, vec);\n        __m256d eq1 = _mm256_cmpeq_pd(row1, vec);\n        __m256d eq2 = _mm256_cmpeq_pd(row2, vec);\n        __m256d eq3 = _mm256_cmpeq_pd(row3, vec);\n        __m256d eq4 = _mm256_cmpeq_pd(row4, vec);\n        __m256d eq5 = _mm256_cmpeq_pd(row5, vec);\n        __m256d eq6 = _mm256_cmpeq_pd(row6, vec);\n        __m256d eq7 = _mm256_cmpeq_pd(row7, vec);\n\n        __m256d all_eq0 = _mm256_and_pd(eq0, eq1);\n        __m256d all_eq1 = _mm256_and_pd(eq2, eq3);\n        __m256d all_eq2 = _mm256_and_pd(eq4, eq5);\n        __m256d all_eq3 = _mm256_and_pd(eq6, eq7);\n\n        __m256d final_eq0 = _mm256_and_pd(all_eq0, all_eq1);\n        __m256d final_eq1 = _mm256_and_pd(all_eq2, all_eq3);\n\n        __m256d final_check = _mm256_or_pd(final_eq0, final_eq1);\n\n        int mask = _mm256_movemask_pd(final_check);\n        if (mask == 0x0F) {\n            return true;\n        }\n    }\n\n    for (; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    size_t i = 0;\n    __m256d vec = _mm256_load_pd(vector);\n\n    for (; i <= rows - 8; i += 8) {\n        __m256d row1 = _mm256_loadu_pd(matrix + i * cols);\n        __m256d row2 = _mm256_loadu_pd(matrix + (i + 1) * cols);\n        __m256d row3 = _mm256_loadu_pd(matrix + (i + 2) * cols);\n        __m256d row4 = _mm256_loadu_pd(matrix + (i + 3) * cols);\n        __m256d row5 = _mm256_loadu_pd(matrix + (i + 4) * cols);\n        __m256d row6 = _mm256_loadu_pd(matrix + (i + 5) * cols);\n        __m256d row7 = _mm256_loadu_pd(matrix + (i + 6) * cols);\n        __m256d row8 = _mm256_loadu_pd(matrix + (i + 7) * cols);\n\n        __m256d eq1 = _mm256_cmpeq_pd(row1, vec);\n        __m256d eq2 = _mm256_cmpeq_pd(row2, vec);\n        __m256d eq3 = _mm256_cmpeq_pd(row3, vec);\n        __m256d eq4 = _mm256_cmpeq_pd(row4, vec);\n        __m256d eq5 = _mm256_cmpeq_pd(row5, vec);\n        __m256d eq6 = _mm256_cmpeq_pd(row6, vec);\n        __m256d eq7 = _mm256_cmpeq_pd(row7, vec);\n        __m256d eq8 = _mm256_cmpeq_pd(row8, vec);\n\n        __m256d all_eq1 = _mm256_and_pd(eq1, eq2);\n        __m256d all_eq2 = _mm256_and_pd(eq3, eq4);\n        __m256d all_eq3 = _mm256_and_pd(eq5, eq6);\n        __m256d all_eq4 = _mm256_and_pd(eq7, eq8);\n\n        __m256d final_eq1 = _mm256_and_pd(all_eq1, all_eq2);\n        __m256d final_eq2 = _mm256_and_pd(all_eq3, all_eq4);\n\n        __m256d final_check = _mm256_or_pd(final_eq1, final_eq2);\n\n        int mask = _mm256_movemask_pd(final_check);\n        if (mask != 0) {\n            return true;\n        }\n    }\n\n    for (; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; ++j) {\n            if (matrix[i * cols + j] != vector[j]) {\n                match = false;\n                break;\n            }\n        }\n        if (match) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Compute strides for each dimension\n    size_t stride_w = 1;\n    size_t stride_h = dims[3] * stride_w;\n    size_t stride_d = dims[2] * stride_h;\n    size_t stride_c = dims[1] * stride_d;\n\n    // Total number of elements\n    size_t total_elements = dims[0] * stride_c;\n\n    __m256i eq_mask = _mm256_setzero_si256();\n    __m256i zero_mask = _mm256_set1_epi8(0);\n\n    while (i + 8 <= total_elements) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n\n        eq_mask = _mm256_cmpeq_epi8(a_vec, b_vec);\n        size_t mask = _mm256_movemask_epi8(eq_mask);\n        total_count += __builtin_popcount(mask);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Compute the total number of elements\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n\n    // Process elements in chunks of 32 using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i eq_mask = _mm256_cmpeq_epi8(vec_A, vec_B);\n        size_t mask = _mm256_movemask_epi8(eq_mask);\n        total_count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Compute strides for each dimension\n    size_t stride_w = 1;\n    size_t stride_h = dims[3] * stride_w;\n    size_t stride_d = dims[2] * stride_h;\n    size_t stride_c = dims[1] * stride_d;\n\n    // Total elements in the tensor\n    size_t total_elements = dims[0] * stride_c;\n\n    while (i + 32 <= total_elements) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i eq_mask = _mm256_cmpeq_epi8(vec_A, vec_B);\n        size_t mask = _mm256_movemask_epi8(eq_mask);\n\n        total_count += __builtin_popcount(mask);\n        i += 32;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Compute the total number of elements\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n\n    // Process elements in chunks of 32 using AVX2\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i eq_mask = _mm256_cmpeq_epi8(vec_A, vec_B);\n        size_t mask = _mm256_movemask_epi8(eq_mask);\n        total_count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t count = 0;\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n    \n    size_t i = 0;\n    for (; i + 31 < total_elements; i += 32) {\n        __m256i vec_A = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_B = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i eq_mask = _mm256_cmpeq_epi8(vec_A, vec_B);\n        count += _mm256_popcnt_epi8(eq_mask).m128i_u64[0];\n    }\n    \n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    __m256i a_vec = _mm256_set1_epi64x(a);\n\n    for (; i <= length - 4; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i*)(x + i));\n        __m256i mul_vec = _mm256_mul_epi64(a_vec, x_vec);\n        __m256i abs_vec = _mm256_abs_epi64(mul_vec);\n        __m256i sub_vec = _mm256_sub_epi64(abs_vec, x_vec);\n        _mm256_storeu_si256((__m256i*)(y + i), sub_vec);\n    }\n\n    for (; i < length; i++) {\n        y[i] = abs(x[i] * a) - x[i];\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    __m256i a_vec = _mm256_set1_epi64x(a);\n\n    for (; i <= length - 4; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i*)&x[i]);\n        __m256i mul_vec = _mm256_mul_epi64(a_vec, x_vec);\n        __m256i abs_vec = _mm256_abs_epi64(mul_vec);\n        __m256i sub_vec = _mm256_sub_epi64(abs_vec, x_vec);\n        _mm256_storeu_si256((__m256i*)&y[i], sub_vec);\n    }\n\n    for (; i < length; i++) {\n        y[i] = abs(x[i] * a) - x[i];\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    __m256i a_vec = _mm256_set1_epi64x(a);\n\n    for (; i <= length - 4; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i*)(x + i));\n        __m256i mul_vec = _mm256_mul_epi64(a_vec, x_vec);\n        __m256i abs_vec = _mm256_abs_epi64(mul_vec);\n        __m256i sub_vec = _mm256_sub_epi64(abs_vec, x_vec);\n        _mm256_storeu_si256((__m256i*)(y + i), sub_vec);\n    }\n\n    for (; i < length; i++) {\n        y[i] = abs(x[i] * a) - x[i];\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m256i vec_x = _mm256_loadu_si256((__m256i*)(x + i));\n        __m256i vec_a = _mm256_set1_epi64x(a);\n        __m256i vec_ax = _mm256_mullo_epi64(vec_x, vec_a);\n        __m256i vec_abs = _mm256_abs_epi64(vec_ax);\n        __m256i vec_sub = _mm256_sub_epi64(vec_abs, vec_ax);\n        _mm256_storeu_si256((__m256i*)(y + i), vec_sub);\n    }\n    for (; i < length; i++) {\n        int64_t xi = x[i];\n        int64_t axi = xi * a;\n        int64_t abs_axi = abs(axi);\n        y[i] = abs_axi - axi;\n    }\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    __m256i a_vec = _mm256_set1_epi64x(a);\n\n    for (; i <= length - 4; i += 4) {\n        __m256i x_vec = _mm256_loadu_si256((__m256i*)(x + i));\n        __m256i ax_vec = _mm256_mullo_epi64(x_vec, a_vec);\n        __m256i abs_ax_vec = _mm256_abs_epi64(ax_vec);\n        _mm256_storeu_si256((__m256i*)(y + i), abs_ax_vec);\n    }\n\n    for (; i < length; i++) {\n        y[i] = abs((int64_t)(x[i] * a));\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = { .min_num = INT16_MAX, .max_num = INT16_MIN };\n    size_t i = 0;\n\n    // Process full AVX2 vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        __m256i vmin, vmax;\n        vmin = vmax = v;\n\n        // Horizontal reduction to find min and max\n        vmin = _mm256_min_epi16(vmin, _mm256_permute2x128_si256(vmin, vmin, 1));\n        vmax = _mm256_max_epi16(vmax, _mm256_permute2x128_si256(vmax, vmax, 1));\n        vmin = _mm256_min_epi16(vmin, _mm256_permute2x128_si256(vmin, vmin, 2));\n        vmax = _mm256_max_epi16(vmax, _mm256_permute2x128_si256(vmax, vmax, 2));\n        vmin = _mm256_min_epi16(vmin, _mm256_permute2x128_si256(vmin, vmin, 3));\n        vmax = _mm256_max_epi16(vmax, _mm256_permute2x128_si256(vmax, vmax, 3));\n\n        result.min_num = _mm256_extract_epi16(vmin, 0) < result.min_num ? _mm256_extract_epi16(vmin, 0) : result.min_num;\n        result.max_num = _mm256_extract_epi16(vmax, 0) > result.max_num ? _mm256_extract_epi16(vmax, 0) : result.max_num;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (vec[i] < result.min_num) result.min_num = vec[i];\n        if (vec[i] > result.max_num) result.max_num = vec[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = { .min_num = INT16_MAX, .max_num = INT16_MIN };\n    size_t i = 0;\n\n    // Process full AVX2 vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        __m256i vmin = _mm256_min_epi16(v, v);\n        __m256i vmax = _mm256_max_epi16(v, v);\n\n        for (int j = 0; j < 4; ++j) {\n            __m256i lane = _mm256_permutevar8x32_epi32(vmin, _mm256_set_epi32(j, j, j, j, j, j, j, j));\n            result.min_num = _mm_cvtsi128_si32(_mm_minpos_epu16(_mm_cvtepi16_epi32(lane)));\n\n            lane = _mm256_permutevar8x32_epi32(vmax, _mm256_set_epi32(j, j, j, j, j, j, j, j));\n            result.max_num = _mm_cvtsi128_si32(_mm_maxpos_epu16(_mm_cvtepi16_epi32(lane)));\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < result.min_num) result.min_num = vec[i];\n        if (vec[i] > result.max_num) result.max_num = vec[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = { .min_num = INT16_MAX, .max_num = INT16_MIN };\n    size_t i = 0;\n\n    // Process full AVX2 vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        __m256i vmin, vmax;\n        vmin = vmax = v;\n\n        // Horizontal reduction to find min and max\n        vmin = _mm256_min_epi16(vmin, _mm256_permute2x128_si256(vmin, vmin, 1));\n        vmax = _mm256_max_epi16(vmax, _mm256_permute2x128_si256(vmax, vmax, 1));\n\n        vmin = _mm256_min_epi16(vmin, _mm256_shuffle_epi32(vmin, 1));\n        vmax = _mm256_max_epi16(vmax, _mm256_shuffle_epi32(vmax, 1));\n\n        vmin = _mm256_min_epi16(vmin, _mm256_shufflelo_epi16(vmin, 7));\n        vmax = _mm256_max_epi16(vmax, _mm256_shufflelo_epi16(vmax, 7));\n\n        vmin = _mm256_min_epi16(vmin, _mm256_shufflehi_epi16(vmin, 7));\n        vmax = _mm256_max_epi16(vmax, _mm256_shufflehi_epi16(vmax, 7));\n\n        result.min_num = _mm256_extract_epi16(vmin, 0) < result.min_num ? _mm256_extract_epi16(vmin, 0) : result.min_num;\n        result.max_num = _mm256_extract_epi16(vmax, 0) > result.max_num ? _mm256_extract_epi16(vmax, 0) : result.max_num;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (vec[i] < result.min_num) result.min_num = vec[i];\n        if (vec[i] > result.max_num) result.max_num = vec[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = { INT16_MAX, INT16_MIN };\n    size_t i = 0;\n\n    // Process full AVX2 vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        __m256i vmin = _mm256_min_epi16(v, v);\n        __m256i vmax = _mm256_max_epi16(v, v);\n\n        vmin = _mm256_min_epi16(vmin, _mm256_permute2x128_si256(vmin, vmin, 1));\n        vmax = _mm256_max_epi16(vmax, _mm256_permute2x128_si256(vmax, vmax, 1));\n\n        vmin = _mm256_min_epi16(vmin, _mm256_shuffle_epi32(vmin, 1));\n        vmax = _mm256_max_epi16(vmax, _mm256_shuffle_epi32(vmax, 1));\n\n        vmin = _mm256_min_epi16(vmin, _mm256_shufflelo_epi16(vmin, 7));\n        vmax = _mm256_max_epi16(vmax, _mm256_shufflelo_epi16(vmax, 7));\n\n        int16_t min_vals[16];\n        int16_t max_vals[16];\n        _mm256_storeu_si256((__m256i*)min_vals, vmin);\n        _mm256_storeu_si256((__m256i*)max_vals, vmax);\n\n        for (size_t j = 0; j < 16; ++j) {\n            if (min_vals[j] < result.min_num) result.min_num = min_vals[j];\n            if (max_vals[j] > result.max_num) result.max_num = max_vals[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (vec[i] < result.min_num) result.min_num = vec[i];\n        if (vec[i] > result.max_num) result.max_num = vec[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    MinMaxPair result = { .min_num = INT16_MAX, .max_num = INT16_MIN };\n    size_t i = 0;\n\n    // Process full AVX2 vectors\n    for (; i <= length - 16; i += 16) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&vec[i]);\n        __m256i vmin, vmax;\n        vmin = vmax = v;\n\n        // Horizontal reduction to find min and max\n        vmin = _mm256_min_epi16(vmin, _mm256_permute2x128_si256(vmin, vmin, 1));\n        vmax = _mm256_max_epi16(vmax, _mm256_permute2x128_si256(vmax, vmax, 1));\n        vmin = _mm256_min_epi16(vmin, _mm256_srli_si256(vmin, 8));\n        vmax = _mm256_max_epi16(vmax, _mm256_srli_si256(vmax, 8));\n        vmin = _mm256_min_epi16(vmin, _mm256_permute4x64_epi64(vmin, 1));\n        vmax = _mm256_max_epi16(vmax, _mm256_permute4x64_epi64(vmax, 1));\n\n        result.min_num = _mm256_extract_epi16(vmin, 0) < result.min_num ? _mm256_extract_epi16(vmin, 0) : result.min_num;\n        result.max_num = _mm256_extract_epi16(vmax, 0) > result.max_num ? _mm256_extract_epi16(vmax, 0) : result.max_num;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        if (vec[i] < result.min_num) result.min_num = vec[i];\n        if (vec[i] > result.max_num) result.max_num = vec[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0)\n        return false;\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m256i prev_vec = _mm256_loadu_si256((__m256i*)(row_ptr));\n        bool valid = true;\n\n        for (size_t j = 1; j < cols; j += 8) {\n            __m256i curr_vec = _mm256_loadu_si256((__m256i*)(row_ptr + j));\n            __m256i cmp_gt = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n            if (!_mm256_testz_si256(cmp_gt, cmp_gt)) {\n                valid = false;\n                break;\n            }\n            prev_vec = curr_vec;\n        }\n\n        if (!valid)\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n    size_t i = 0;\n    for (; i + 8 <= rows; i += 8) {\n        __m256i prev_row = _mm256_setzero_si256();\n        for (size_t j = 0; j < cols; j += 8) {\n            __m256i current_row = _mm256_loadu_si256((__m256i*)&matrix[i * cols + j]);\n            __m256i gt_mask = _mm256_cmpgt_epi32(prev_row, current_row);\n            __m256i eq_mask = _mm256_cmpeq_epi32(prev_row, current_row);\n            if (_mm256_movemask_epi8(_mm256_or_si256(gt_mask, eq_mask))) {\n                return false;\n            }\n            prev_row = current_row;\n        }\n    }\n    for (; i < rows; ++i) {\n        for (size_t j = 1; j < cols; ++j) {\n            if (matrix[i * cols + j] <= matrix[i * cols + j - 1]) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m256i prev_vec = _mm256_loadu_si256((__m256i*)(row_ptr));\n        bool valid = true;\n\n        for (size_t j = 1; j < cols; j += 8) {\n            __m256i curr_vec = _mm256_loadu_si256((__m256i*)(row_ptr + j));\n            __m256i cmp_gt = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n            if (!_mm256_testz_si256(cmp_gt, cmp_gt)) {\n                valid = false;\n                break;\n            }\n            prev_vec = curr_vec;\n        }\n\n        if (!valid) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m256i prev_vec = _mm256_loadu_si256((__m256i*)(row_ptr));\n        bool valid = true;\n\n        for (size_t j = 1; j < cols; j += 8) {\n            __m256i curr_vec = _mm256_loadu_si256((__m256i*)(row_ptr + j));\n            __m256i cmp_gt = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n            if (!_mm256_testz_si256(cmp_gt, cmp_gt)) {\n                valid = false;\n                break;\n            }\n            prev_vec = curr_vec;\n        }\n\n        if (!valid) continue;\n\n        // Handle remaining elements that don't fit into vector\n        for (size_t j = cols - (cols % 8); j < cols - 1; ++j) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                valid = false;\n                break;\n            }\n        }\n\n        if (valid) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m256i prev_vec = _mm256_loadu_si256((__m256i*)(row_ptr));\n        bool valid = true;\n\n        for (size_t j = 1; j < cols; j += 8) {\n            __m256i curr_vec = _mm256_loadu_si256((__m256i*)(row_ptr + j));\n            __m256i cmp_gt = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n            if (!_mm256_testz_si256(cmp_gt, cmp_gt)) {\n                valid = false;\n                break;\n            }\n            prev_vec = curr_vec;\n        }\n\n        if (!valid) continue;\n\n        // Handle remaining elements that don't fit into vector\n        for (size_t j = cols - (cols % 8); j < cols - 1; ++j) {\n            if (row_ptr[j] >= row_ptr[j + 1]) {\n                valid = false;\n                break;\n            }\n        }\n\n        if (valid) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_rows = rows / block_size;\n    size_t num_cols = cols / block_size;\n    __m256i first_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                _mm256_setzero_si256(), _mm256_setzero_si256() };\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; ++j) {\n            size_t idx = i * cols + j;\n            first_block[i] = _mm256_insert_epi32(first_block[i], matrix[idx], j);\n        }\n    }\n    for (size_t r = 1; r < num_rows; ++r) {\n        for (size_t c = 1; c < num_cols; ++c) {\n            __m256i current_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                          _mm256_setzero_si256(), _mm256_setzero_si256() };\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    size_t idx = (r * block_size + i) * cols + (c * block_size + j);\n                    current_block[i] = _mm256_insert_epi32(current_block[i], matrix[idx], j);\n                }\n            }\n            for (size_t k = 0; k < block_size; ++k) {\n                if (!_mm256_cmpeq_epi32_mask(first_block[k], current_block[k])) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_rows = rows / block_size;\n    size_t num_cols = cols / block_size;\n    __m256i first_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                _mm256_setzero_si256(), _mm256_setzero_si256() };\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; ++j) {\n            size_t idx = i * cols + j;\n            first_block[i] = _mm256_insert_epi32(first_block[i], matrix[idx], j);\n        }\n    }\n    for (size_t r = 1; r < num_rows; ++r) {\n        for (size_t c = 1; c < num_cols; ++c) {\n            __m256i current_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                          _mm256_setzero_si256(), _mm256_setzero_si256() };\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    size_t idx = (r * block_size + i) * cols + (c * block_size + j);\n                    current_block[i] = _mm256_insert_epi32(current_block[i], matrix[idx], j);\n                }\n            }\n            for (size_t k = 0; k < block_size; ++k) {\n                if (!_mm256_cmpeq_epi32_mask(first_block[k], current_block[k])) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_rows = rows / block_size;\n    size_t num_cols = cols / block_size;\n    __m256i first_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                _mm256_setzero_si256(), _mm256_setzero_si256() };\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; ++j) {\n            size_t idx = i * cols + j;\n            first_block[i] = _mm256_insert_epi32(first_block[i], matrix[idx], j);\n        }\n    }\n    for (size_t r = 1; r < num_rows; ++r) {\n        for (size_t c = 1; c < num_cols; ++c) {\n            __m256i current_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                          _mm256_setzero_si256(), _mm256_setzero_si256() };\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    size_t idx = (r * block_size + i) * cols + (c * block_size + j);\n                    current_block[i] = _mm256_insert_epi32(current_block[i], matrix[idx], j);\n                }\n            }\n            for (size_t k = 0; k < block_size; ++k) {\n                if (!_mm256_cmpeq_epi32_mask(first_block[k], current_block[k])) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    __m256i first_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                _mm256_setzero_si256(), _mm256_setzero_si256() };\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; j += 8) {\n            size_t idx = i * cols + j;\n            __m256i current = _mm256_loadu_si256((__m256i*)(matrix + idx));\n            first_block[i] = _mm256_or_si256(first_block[i], current);\n        }\n    }\n    for (size_t r = 1; r < num_blocks_row; ++r) {\n        for (size_t c = 0; c < num_blocks_col; ++c) {\n            __m256i block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                  _mm256_setzero_si256(), _mm256_setzero_si256() };\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; j += 8) {\n                    size_t idx = (r * block_size + i) * cols + c * block_size + j;\n                    __m256i current = _mm256_loadu_si256((__m256i*)(matrix + idx));\n                    block[i] = _mm256_or_si256(block[i], current);\n                }\n            }\n            for (size_t k = 0; k < block_size; ++k) {\n                if (!_mm256_movemask_epi8(_mm256_cmpeq_epi32(first_block[k], block[k]))) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_rows = rows / block_size;\n    size_t num_cols = cols / block_size;\n    __m256i first_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                _mm256_setzero_si256(), _mm256_setzero_si256() };\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; ++j) {\n            size_t idx = i * cols + j;\n            first_block[i] = _mm256_insert_epi32(first_block[i], matrix[idx], j);\n        }\n    }\n    for (size_t r = 1; r < num_rows; ++r) {\n        for (size_t c = 1; c < num_cols; ++c) {\n            __m256i current_block[4] = { _mm256_setzero_si256(), _mm256_setzero_si256(),\n                                          _mm256_setzero_si256(), _mm256_setzero_si256() };\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    size_t idx = (r * block_size + i) * cols + (c * block_size + j);\n                    current_block[i] = _mm256_insert_epi32(current_block[i], matrix[idx], j);\n                }\n            }\n            for (size_t k = 0; k < block_size; ++k) {\n                if (!_mm256_cmpeq_epi32_mask(first_block[k], current_block[k])) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 4 != 0) return false;\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t b = 1; b < num_blocks; b++) {\n        size_t offset = b * block_size;\n        __m256d block = _mm256_loadu_pd(vec + offset);\n        __m256d first_block = _mm256_loadu_pd(vec + 0);\n\n        __m256d diff = _mm256_sub_pd(block, first_block);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        __m256d compare = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n\n        if (!_mm256_movemask_pd(compare)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 4 != 0) return false;\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    size_t i = 0;\n\n    while (i + block_size <= length) {\n        __m256d block0 = _mm256_loadu_pd(vec + i);\n        __m256d block1 = _mm256_loadu_pd(vec + i + block_size);\n        __m256d diff = _mm256_sub_pd(block0, block1);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        __m256d compare = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n\n        int mask = _mm256_movemask_pd(compare);\n        if (mask != 0b1111) return false;\n\n        i += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 4 != 0) return false;\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t b = 1; b < num_blocks; b++) {\n        size_t offset = b * block_size;\n        __m256d block = _mm256_loadu_pd(vec + offset);\n        __m256d ref_block = _mm256_loadu_pd(vec + 0);\n        __m256d diff = _mm256_sub_pd(block, ref_block);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        __m256d cmp_result = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n        if (!_mm256_movemask_pd(cmp_result)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 4 != 0) return false;\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 0; i < num_blocks - 1; ++i) {\n        size_t offset = i * block_size;\n        __m256d ref_block = _mm256_loadu_pd(vec + offset);\n        \n        for (size_t j = i + 1; j < num_blocks; ++j) {\n            size_t block_offset = j * block_size;\n            __m256d current_block = _mm256_loadu_pd(vec + block_offset);\n\n            __m256d diff = _mm256_sub_pd(current_block, ref_block);\n            __m256d abs_diff = _mm256_abs_pd(diff);\n            __m256d compare = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n\n            unsigned int mask = _mm256_movemask_pd(compare);\n            if (mask != 0b1111) return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 4 != 0) return false;\n\n    __m256d tol_vec = _mm256_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 0; i < num_blocks - 1; ++i) {\n        size_t offset = i * block_size;\n        __m256d block_ref = _mm256_loadu_pd(vec + offset);\n        \n        for (size_t j = i + 1; j < num_blocks; ++j) {\n            size_t block_offset = j * block_size;\n            __m256d block_curr = _mm256_loadu_pd(vec + block_offset);\n\n            __m256d diff = _mm256_sub_pd(block_ref, block_curr);\n            __m256d abs_diff = _mm256_abs_pd(diff);\n            __m256d cmp_result = _mm256_cmp_pd(abs_diff, tol_vec, _CMP_LE_OS);\n\n            if (!_mm256_movemask_pd(cmp_result)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(matrix + i * cols + j);\n            sum = _mm256_add_pd(sum, _mm256_mul_pd(vec, vec));\n        }\n        __m256d threshold = _mm256_set1_pd(thresholds[i]);\n        __m256d cmp_result = _mm256_cmp_pd(sum, threshold, _CMP_LT_OS);\n        bool any_gt = !_mm256_movemask_pd(cmp_result);\n        result[i] = any_gt;\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t r;\n    for(r = 0; r < rows; r += 4){\n        __m256d sum0 = _mm256_setzero_pd();\n        __m256d sum1 = _mm256_setzero_pd();\n        __m256d sum2 = _mm256_setzero_pd();\n        __m256d sum3 = _mm256_setzero_pd();\n        size_t c;\n        for(c = 0; c < cols; c += 4){\n            __m256d data0 = _mm256_loadu_pd(matrix + (r + 0)*cols + c);\n            __m256d data1 = _mm256_loadu_pd(matrix + (r + 1)*cols + c);\n            __m256d data2 = _mm256_loadu_pd(matrix + (r + 2)*cols + c);\n            __m256d data3 = _mm256_loadu_pd(matrix + (r + 3)*cols + c);\n\n            __m256d square0 = _mm256_mul_pd(data0, data0);\n            __m256d square1 = _mm256_mul_pd(data1, data1);\n            __m256d square2 = _mm256_mul_pd(data2, data2);\n            __m256d square3 = _mm256_mul_pd(data3, data3);\n\n            sum0 = _mm256_add_pd(sum0, square0);\n            sum1 = _mm256_add_pd(sum1, square1);\n            sum2 = _mm256_add_pd(sum2, square2);\n            sum3 = _mm256_add_pd(sum3, square3);\n        }\n        __m256d threshold = _mm256_load1_pd(thresholds + r);\n        __m256d norm0 = _mm256_sqrt_pd(sum0);\n        __m256d norm1 = _mm256_sqrt_pd(sum1);\n        __m256d norm2 = _mm256_sqrt_pd(sum2);\n        __m256d norm3 = _mm256_sqrt_pd(sum3);\n\n        __m256d compare0 = _mm256_cmp_pd(norm0, threshold, _CMP_LT_OS);\n        __m256d compare1 = _mm256_cmp_pd(norm1, threshold, _CMP_LT_OS);\n        __m256d compare2 = _mm256_cmp_pd(norm2, threshold, _CMP_LT_OS);\n        __m256d compare3 = _mm256_cmp_pd(norm3, threshold, _CMP_LT_OS);\n\n        _mm256_storeu_si256((__m256i*)(result + (r + 0)), _mm256_castpd_si256(compare0));\n        _mm256_storeu_si256((__m256i*)(result + (r + 1)), _mm256_castpd_si256(compare1));\n        _mm256_storeu_si256((__m256i*)(result + (r + 2)), _mm256_castpd_si256(compare2));\n        _mm256_storeu_si256((__m256i*)(result + (r + 3)), _mm256_castpd_si256(compare3));\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    size_t r;\n    for(r = 0; r < rows; r += 4){\n        __m256d sum0 = _mm256_setzero_pd();\n        __m256d sum1 = _mm256_setzero_pd();\n        __m256d sum2 = _mm256_setzero_pd();\n        __m256d sum3 = _mm256_setzero_pd();\n        size_t c;\n        for(c = 0; c < cols; c += 4){\n            __m256d data0 = _mm256_loadu_pd(matrix + (r + 0)*cols + c);\n            __m256d data1 = _mm256_loadu_pd(matrix + (r + 1)*cols + c);\n            __m256d data2 = _mm256_loadu_pd(matrix + (r + 2)*cols + c);\n            __m256d data3 = _mm256_loadu_pd(matrix + (r + 3)*cols + c);\n            \n            sum0 = _mm256_add_pd(sum0, _mm256_mul_pd(data0, data0));\n            sum1 = _mm256_add_pd(sum1, _mm256_mul_pd(data1, data1));\n            sum2 = _mm256_add_pd(sum2, _mm256_mul_pd(data2, data2));\n            sum3 = _mm256_add_pd(sum3, _mm256_mul_pd(data3, data3));\n        }\n        sum0 = _mm256_sqrt_pd(sum0);\n        sum1 = _mm256_sqrt_pd(sum1);\n        sum2 = _mm256_sqrt_pd(sum2);\n        sum3 = _mm256_sqrt_pd(sum3);\n\n        __m256d thr0 = _mm256_loadu_pd(thresholds + r + 0);\n        __m256d thr1 = _mm256_loadu_pd(thresholds + r + 1);\n        __m256d thr2 = _mm256_loadu_pd(thresholds + r + 2);\n        __m256d thr3 = _mm256_loadu_pd(thresholds + r + 3);\n\n        __m256d cmp0 = _mm256_cmp_pd(sum0, thr0, _CMP_GT_OS);\n        __m256d cmp1 = _mm256_cmp_pd(sum1, thr1, _CMP_GT_OS);\n        __m256d cmp2 = _mm256_cmp_pd(sum2, thr2, _CMP_GT_OS);\n        __m256d cmp3 = _mm256_cmp_pd(sum3, thr3, _CMP_GT_OS);\n\n        _mm256_storeu_si256((__m256i*)(result + (r + 0)), _mm256_castpd_si256(cmp0));\n        _mm256_storeu_si256((__m256i*)(result + (r + 1)), _mm256_castpd_si256(cmp1));\n        _mm256_storeu_si256((__m256i*)(result + (r + 2)), _mm256_castpd_si256(cmp2));\n        _mm256_storeu_si256((__m256i*)(result + (r + 3)), _mm256_castpd_si256(cmp3));\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(matrix + i * cols + j);\n            vec = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, vec);\n        }\n        __m256d threshold = _mm256_set1_pd(thresholds[i]);\n        __m256d cmp_mask = _mm256_cmp_pd(sum, threshold, _CMP_LT_OS);\n        int mask = _mm256_movemask_pd(cmp_mask);\n        result[i] = !(mask != 0);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    for (size_t i = 0; i < rows; i++) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < cols; j += 4) {\n            __m256d vec = _mm256_loadu_pd(matrix + i * cols + j);\n            vec = _mm256_mul_pd(vec, vec);\n            sum = _mm256_add_pd(sum, vec);\n        }\n        // Reduction\n        __m256d tmp = _mm256_hadd_pd(sum, sum);\n        __m128d low = _mm256_castpd256_pd128(tmp);\n        __m128d high = _mm256_extractf128_pd(tmp, 1);\n        low = _mm_add_sd(low, high);\n        double partial_sum[2];\n        _mm_store_sd(partial_sum, low);\n        double norm_squared = partial_sum[0];\n        result[i] = (norm_squared > thresholds[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    // Process data in 8-element chunks using AVX2\n    for (; i <= length - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    // Process data in 8-element chunks using AVX2\n    for (; i <= length - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i result_vec = _mm256_or_si256(a_vec, b_vec);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result_vec);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle the remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    // Process data in 8-element chunks using AVX2\n    for (; i <= length - 8; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vecB = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i vecC = _mm256_or_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)&C[i], vecC);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_mullo_epi16(a_vec, a_vec);\n        __m256i b_sq = _mm256_mullo_epi16(b_vec, b_vec);\n\n        __m256i ab = _mm256_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_permute4x64_epi64(_mm256_add_epi32(a_sq, b_sq), 78));\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_permute4x64_epi64(ab, 78));\n    }\n\n    // Handle remaining elements\n    int64_t scalar_sum_of_squares = 0;\n    int64_t scalar_sum_of_products = 0;\n    for (; i < length; ++i) {\n        int8_t a_val = A[i];\n        int8_t b_val = B[i];\n        scalar_sum_of_squares += a_val * a_val + b_val * b_val;\n        scalar_sum_of_products += a_val * b_val;\n    }\n\n    __m256i result_vec = _mm256_sub_epi64(sum_of_squares, sum_of_products);\n    int64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, result_vec);\n\n    return result[0] + result[1] + result[2] + result[3] + scalar_sum_of_squares - scalar_sum_of_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_mullo_epi16(a_vec, a_vec);\n        __m256i b_sq = _mm256_mullo_epi16(b_vec, b_vec);\n        __m256i ab = _mm256_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_permute4x64_epi64(_mm256_add_epi32(a_sq, b_sq), 78));\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_permute4x64_epi64(ab, 78));\n    }\n\n    // Handle remaining elements\n    int64_t scalar_sum_of_squares = 0;\n    int64_t scalar_sum_of_products = 0;\n    for (; i < length; ++i) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n        scalar_sum_of_squares += a * a + b * b;\n        scalar_sum_of_products += a * b;\n    }\n\n    // Combine results\n    __m256i result_vec = _mm256_sub_epi64(_mm256_set_epi64x(0, scalar_sum_of_squares, 0, scalar_sum_of_products), _mm256_cvtepi32_epi64(sum_of_squares));\n    int64_t result = _mm256_extract_epi64(result_vec, 0) + _mm256_extract_epi64(result_vec, 1);\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_mullo_epi16(a_vec, a_vec);\n        __m256i b_sq = _mm256_mullo_epi16(b_vec, b_vec);\n\n        __m256i ab = _mm256_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_permute4x64_epi64(_mm256_add_epi32(a_sq, b_sq), 78));\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_permute4x64_epi64(ab, 78));\n    }\n\n    // Handle remaining elements\n    int64_t scalar_sum_of_squares = 0;\n    int64_t scalar_sum_of_products = 0;\n    for (; i < length; i++) {\n        int8_t a_val = A[i];\n        int8_t b_val = B[i];\n        scalar_sum_of_squares += a_val * a_val + b_val * b_val;\n        scalar_sum_of_products += a_val * b_val;\n    }\n\n    __m256i result_vec = _mm256_sub_epi64(sum_of_squares, sum_of_products);\n    int64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, result_vec);\n\n    return result[0] + result[1] + result[2] + result[3] + scalar_sum_of_squares - scalar_sum_of_products;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_squared = _mm256_mullo_epi16(a_vec, a_vec);\n        __m256i b_squared = _mm256_mullo_epi16(b_vec, b_vec);\n\n        __m256i ab_product = _mm256_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_permute4x64_epi64(a_squared, 0x0E));\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_permute4x64_epi64(b_squared, 0x0E));\n\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_permute4x64_epi64(ab_product, 0x0E));\n    }\n\n    // Handle remaining elements\n    int64_t scalar_sum_of_squares = 0;\n    int64_t scalar_sum_of_products = 0;\n    for (; i < length; ++i) {\n        int8_t a_val = A[i];\n        int8_t b_val = B[i];\n        scalar_sum_of_squares += a_val * a_val + b_val * b_val;\n        scalar_sum_of_products += a_val * b_val;\n    }\n\n    sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_set_epi32(\n        (length > 7 ? scalar_sum_of_squares : 0),\n        (length > 6 ? scalar_sum_of_squares : 0),\n        (length > 5 ? scalar_sum_of_squares : 0),\n        (length > 4 ? scalar_sum_of_squares : 0),\n        (length > 3 ? scalar_sum_of_squares : 0),\n        (length > 2 ? scalar_sum_of_squares : 0),\n        (length > 1 ? scalar_sum_of_squares : 0),\n        (length > 0 ? scalar_sum_of_squares : 0)\n    ));\n\n    sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_set_epi32(\n        (length > 7 ? scalar_sum_of_products : 0),\n        (length > 6 ? scalar_sum_of_products : 0),\n        (length > 5 ? scalar_sum_of_products : 0),\n        (length > 4 ? scalar_sum_of_products : 0),\n        (length > 3 ? scalar_sum_of_products : 0),\n        (length > 2 ? scalar_sum_of_products : 0),\n        (length > 1 ? scalar_sum_of_products : 0),\n        (length > 0 ? scalar_sum_of_products : 0)\n    ));\n\n    __m256i diff = _mm256_sub_epi64(sum_of_squares, sum_of_products);\n    int64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, diff);\n\n    return result[0] + result[1] + result[2] + result[3];\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_of_squares = _mm256_setzero_si256();\n    __m256i sum_of_products = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i a_sq = _mm256_mullo_epi16(a_vec, a_vec);\n        __m256i b_sq = _mm256_mullo_epi16(b_vec, b_vec);\n        __m256i ab = _mm256_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_permute4x64_epi64(a_sq, 0x78));\n        sum_of_squares = _mm256_add_epi32(sum_of_squares, _mm256_permute4x64_epi64(b_sq, 0x78));\n\n        sum_of_products = _mm256_add_epi32(sum_of_products, _mm256_permute4x64_epi64(ab, 0x78));\n    }\n\n    // Handle remaining elements\n    int64_t scalar_sum_of_squares = 0;\n    int64_t scalar_sum_of_products = 0;\n    for (; i < length; ++i) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n        scalar_sum_of_squares += a * a + b * b;\n        scalar_sum_of_products += a * b;\n    }\n\n    // Combine results\n    __m256i result_vec = _mm256_sub_epi64(sum_of_squares, sum_of_products);\n    int64_t result = _mm256_extract_epi64(result_vec, 0) + _mm256_extract_epi64(result_vec, 1);\n    result += scalar_sum_of_squares - scalar_sum_of_products;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    // Process data in 4-vector chunks (256 bits)\n    for (; i + 4 <= length; i += 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)&B[i]);\n        __m256i not_b_vec = _mm256_xor_si256(_mm256_set1_epi64x(~0ULL), b_vec);\n        __m256i result_vec = _mm256_and_si256(a_vec, not_b_vec);\n\n        _mm256_storeu_si256((__m256i*)&C[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i b_neg_vec = _mm256_xor_si256(_mm256_set1_epi64x(~0ULL), b_vec);\n        __m256i result_vec = _mm256_and_si256(a_vec, b_neg_vec);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result_vec);\n\n        i += 4;\n        remaining -= 4;\n    }\n\n    while (remaining > 0) {\n        C[i] = A[i] & (~B[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i not_b_vec = _mm256_xor_si256(_mm256_set1_epi64x(~0ULL), b_vec);\n        __m256i result_vec = _mm256_and_si256(a_vec, not_b_vec);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result_vec);\n\n        i += 4;\n        remaining -= 4;\n    }\n\n    while (remaining > 0) {\n        C[i] = A[i] & (~B[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 4) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b_vec = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i b_neg_vec = _mm256_xor_si256(_mm256_set1_epi64x(~0ULL), b_vec);\n        __m256i result_vec = _mm256_and_si256(a_vec, b_neg_vec);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result_vec);\n\n        i += 4;\n        remaining -= 4;\n    }\n\n    while (remaining > 0) {\n        C[i] = A[i] & (~B[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    __m256i* A_vec = (__m256i*)A;\n    __m256i* B_vec = (__m256i*)B;\n    __m256i* C_vec = (__m256i*)C;\n\n    while (remaining >= 4) {\n        __m256i a = _mm256_loadu_si256(A_vec + i);\n        __m256i b = _mm256_loadu_si256(B_vec + i);\n\n        __m256i neg_b = _mm256_xor_si256(_mm256_set1_epi64x(~0ULL), b);\n        __m256i result = _mm256_and_si256(a, neg_b);\n\n        _mm256_storeu_si256(C_vec + i, result);\n\n        i += 4;\n        remaining -= 4;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        \n        // Compare A > 0 and B > 0\n        __m256i maskABPos = _mm256_and_si256(_mm256_cmpgt_epi8(vecA, _mm256_setzero_si256()),\n                                             _mm256_cmpgt_epi8(vecB, _mm256_setzero_si256()));\n        // Compare A < 0 and B < 0\n        __m256i maskABNeg = _mm256_and_si256(_mm256_cmpgt_epi8(_mm256_setzero_si256(), vecA),\n                                             _mm256_cmpgt_epi8(_mm256_setzero_si256(), vecB));\n        \n        // Set 1 where both are positive\n        __m256i resultPos = _mm256_set1_epi8(1);\n        resultPos = _mm256_blendv_epi8(_mm256_setzero_si256(), resultPos, maskABPos);\n        \n        // Set -1 where both are negative\n        __m256i resultNeg = _mm256_set1_epi8(-1);\n        resultNeg = _mm256_blendv_epi8(_mm256_setzero_si256(), resultNeg, maskABNeg);\n        \n        // Combine results\n        __m256i result = _mm256_or_si256(resultPos, resultNeg);\n        \n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; i++) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        \n        // Compare A > 0 and B > 0\n        __m256i maskABPos = _mm256_and_si256(_mm256_cmpgt_epi8(vecA, _mm256_setzero_si256()),\n                                             _mm256_cmpgt_epi8(vecB, _mm256_setzero_si256()));\n        // Compare A < 0 and B < 0\n        __m256i maskABNeg = _mm256_and_si256(_mm256_cmpgt_epi8(_mm256_setzero_si256(), vecA),\n                                             _mm256_cmpgt_epi8(_mm256_setzero_si256(), vecB));\n        \n        // Set result to 1 where both A and B are positive\n        __m256i resultPos = _mm256_set1_epi8(1);\n        resultPos = _mm256_blendv_epi8(_mm256_setzero_si256(), resultPos, maskABPos);\n        \n        // Set result to -1 where both A and B are negative\n        __m256i resultNeg = _mm256_set1_epi8(-1);\n        resultNeg = _mm256_blendv_epi8(_mm256_setzero_si256(), resultNeg, maskABNeg);\n        \n        // Combine results\n        __m256i result = _mm256_or_si256(resultPos, resultNeg);\n        \n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; i++) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        \n        // Compare A > 0 and B > 0\n        __m256i maskPos = _mm256_and_si256(_mm256_cmpgt_epi8(vecA, _mm256_setzero_si256()),\n                                            _mm256_cmpgt_epi8(vecB, _mm256_setzero_si256()));\n        __m256i resultPos = _mm256_set1_epi8(1);\n        \n        // Compare A < 0 and B < 0\n        __m256i maskNeg = _mm256_and_si256(_mm256_cmpgt_epi8(_mm256_setzero_si256(), vecA),\n                                            _mm256_cmpgt_epi8(_mm256_setzero_si256(), vecB));\n        __m256i resultNeg = _mm256_set1_epi8(-1);\n        \n        // Combine results\n        __m256i result = _mm256_or_si256(_mm256_blendv_epi8(_mm256_setzero_si256(), resultPos, maskPos),\n                                         _mm256_blendv_epi8(_mm256_setzero_si256(), resultNeg, maskNeg));\n        \n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; i++) {\n        if (A[i] > 0 && B[i] > 0) {\n            C[i] = 1;\n        } else if (A[i] < 0 && B[i] < 0) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        \n        // Compare A > 0 and B > 0\n        __m256i maskPos = _mm256_and_si256(_mm256_cmpgt_epi8(vecA, _mm256_setzero_si256()),\n                                            _mm256_cmpgt_epi8(vecB, _mm256_setzero_si256()));\n        // Compare A < 0 and B < 0\n        __m256i maskNeg = _mm256_and_si256(_mm256_cmpgt_epi8(_mm256_setzero_si256(), vecA),\n                                            _mm256_cmpgt_epi8(_mm256_setzero_si256(), vecB));\n        \n        // Set result to 1 where both are positive\n        __m256i resultPos = _mm256_set1_epi8(1);\n        resultPos = _mm256_blendv_epi8(_mm256_setzero_si256(), resultPos, maskPos);\n        \n        // Set result to -1 where both are negative\n        __m256i resultNeg = _mm256_set1_epi8(-1);\n        resultNeg = _mm256_blendv_epi8(_mm256_setzero_si256(), resultNeg, maskNeg);\n        \n        // Combine results\n        __m256i result = _mm256_or_si256(resultPos, resultNeg);\n        \n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; i++) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 32; i += 32) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        \n        // Compare A > 0 and B > 0\n        __m256i maskABPos = _mm256_and_si256(_mm256_cmpgt_epi8(vecA, _mm256_setzero_si256()),\n                                             _mm256_cmpgt_epi8(vecB, _mm256_setzero_si256()));\n        // Compare A < 0 and B < 0\n        __m256i maskABNeg = _mm256_and_si256(_mm256_cmpgt_epi8(_mm256_setzero_si256(), vecA),\n                                             _mm256_cmpgt_epi8(_mm256_setzero_si256(), vecB));\n        \n        // Set 1 where both are positive\n        __m256i resultPos = _mm256_set1_epi8(1);\n        resultPos = _mm256_blendv_epi8(_mm256_setzero_si256(), resultPos, maskABPos);\n        \n        // Set -1 where both are negative\n        __m256i resultNeg = _mm256_set1_epi8(-1);\n        resultNeg = _mm256_blendv_epi8(_mm256_setzero_si256(), resultNeg, maskABNeg);\n        \n        // Combine results\n        __m256i result = _mm256_or_si256(resultPos, resultNeg);\n        \n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n    for (; i < length; i++) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Handle first half (before midpoint)\n    for (; i < mid; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle exact midpoint\n    if (mid > 0 && (length % 2 == 1)) {\n        C[mid] = A[mid];\n    }\n\n    // Handle second half (after midpoint)\n    for (; i < length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Handle first half (before midpoint)\n    for (; i < mid; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle exact midpoint\n    if (mid > 0 && (length % 2 == 1)) {\n        C[mid] = A[mid];\n    }\n\n    // Handle second half (after midpoint)\n    for (; i < length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Handle first half (before midpoint)\n    for (; i < mid; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC_before = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC_before);\n    }\n\n    // Handle exact midpoint\n    if (length % 2 == 1) {\n        C[mid] = A[mid];\n    }\n\n    // Handle second half (after midpoint)\n    for (; i < length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC_after = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC_after);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Handle first half (before midpoint)\n    for (; i < mid; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_and_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle exact midpoint\n    if (mid > 0 && (length % 2 == 1)) {\n        C[mid] = A[mid];\n    }\n\n    // Handle second half (after midpoint)\n    for (; i < length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC = _mm256_xor_si256(vecA, vecB);\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Handle first half (before midpoint)\n    for (; i < mid; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC_xor = _mm256_xor_epi16(vecA, vecB);\n        __m256i vecC_and = _mm256_and_epi16(vecA, vecB);\n        __m256i vecC = _mm256_blendv_epi8(vecC_and, vecC_xor, _mm256_set1_epi16(i >= mid ? 0 : -1));\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n\n    // Handle midpoint element\n    if (length % 2 == 1) {\n        C[mid] = A[mid];\n    }\n\n    // Handle second half (after midpoint)\n    for (; i < length; i += 8) {\n        __m256i vecA = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vecB = _mm256_loadu_si256((__m256i*)(B + i));\n        __m256i vecC_xor = _mm256_xor_epi16(vecA, vecB);\n        __m256i vecC_and = _mm256_and_epi16(vecA, vecB);\n        __m256i vecC = _mm256_blendv_epi8(vecC_and, vecC_xor, _mm256_set1_epi16(i >= mid ? 0 : -1));\n        _mm256_storeu_si256((__m256i*)(C + i), vecC);\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 8 <= total_elements) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256 mask1 = _mm256_and_ps(cond1_vec, cond2_vec); // cond1 && cond2\n        __m256 mask2 = _mm256_andnot_ps(cond1_vec, cond2_vec); // !cond1 && cond2\n        __m256 mask3 = _mm256_andnot_ps(cond2_vec, cond1_vec); // cond1 && !cond2\n        __m256 mask4 = _mm256_andnot_ps(_mm256_or_ps(cond1_vec, cond2_vec), _mm256_set1_ps(1.0f)); // !cond1 && !cond2\n\n        __m256 result = _mm256_add_ps(\n            _mm256_mul_ps(a_vec, mask1),\n            _mm256_add_ps(\n                _mm256_mul_ps(b_vec, mask3),\n                _mm256_add_ps(\n                    _mm256_mul_ps(c_vec, mask2),\n                    _mm256_mul_ps(d_vec, mask4)\n                )\n            )\n        );\n\n        _mm256_storeu_ps(&out[i], result);\n\n        i += 8;\n    }\n\n    for (; i < total_elements; i++) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 8 <= total_elements) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256 mask1 = _mm256_and_ps(cond1_vec, cond2_vec); // cond1 && cond2\n        __m256 mask2 = _mm256_andnot_ps(cond1_vec, cond2_vec); // !cond1 && cond2\n        __m256 mask3 = _mm256_andnot_ps(cond2_vec, cond1_vec); // cond1 && !cond2\n        __m256 mask4 = _mm256_andnot_ps(_mm256_or_ps(cond1_vec, cond2_vec), _mm256_set1_ps(1.0f)); // !cond1 && !cond2\n\n        __m256 result = _mm256_add_ps(\n            _mm256_mul_ps(a_vec, mask1),\n            _mm256_add_ps(\n                _mm256_mul_ps(b_vec, mask3),\n                _mm256_add_ps(\n                    _mm256_mul_ps(c_vec, mask2),\n                    _mm256_mul_ps(d_vec, mask4)\n                )\n            )\n        );\n\n        _mm256_storeu_ps(&out[i], result);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 8 <= total_elements) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256 mask1 = _mm256_and_ps(cond1_vec, cond2_vec); // cond1 && cond2\n        __m256 mask2 = _mm256_andnot_ps(cond1_vec, cond2_vec); // !cond1 && cond2\n        __m256 mask3 = _mm256_andnot_ps(cond2_vec, cond1_vec); // cond1 && !cond2\n        __m256 mask4 = _mm256_andnot_ps(_mm256_or_ps(cond1_vec, cond2_vec), _mm256_set1_ps(1.0f)); // !cond1 && !cond2\n\n        __m256 result = _mm256_add_ps(\n            _mm256_mul_ps(a_vec, mask1),\n            _mm256_add_ps(\n                _mm256_mul_ps(b_vec, mask2),\n                _mm256_add_ps(\n                    _mm256_mul_ps(c_vec, mask3),\n                    _mm256_mul_ps(d_vec, mask4)\n                )\n            )\n        );\n\n        _mm256_storeu_ps(&out[i], result);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 8 <= total_elements) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256 mask1 = _mm256_and_ps(cond1_vec, cond2_vec);\n        __m256 mask2 = _mm256_andnot_ps(cond1_vec, cond2_vec);\n        __m256 mask3 = _mm256_andnot_ps(_mm256_or_ps(cond1_vec, cond2_vec), _mm256_set1_ps(1.0f));\n\n        __m256 result = _mm256_add_ps(\n            _mm256_mul_ps(a_vec, mask1),\n            _mm256_add_ps(\n                _mm256_mul_ps(b_vec, mask2),\n                _mm256_mul_ps(c_vec, mask3)\n            )\n        );\n\n        _mm256_storeu_ps(&out[i], result);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 8 <= total_elements) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 c_vec = _mm256_loadu_ps(&C[i]);\n        __m256 d_vec = _mm256_loadu_ps(&D[i]);\n\n        __m256 cond1_vec = _mm256_loadu_si256((__m256i*)&cond1[i]);\n        __m256 cond2_vec = _mm256_loadu_si256((__m256i*)&cond2[i]);\n\n        __m256 mask1 = _mm256_and_ps(cond1_vec, cond2_vec);\n        __m256 mask2 = _mm256_andnot_ps(cond1_vec, cond2_vec);\n        __m256 mask3 = _mm256_andnot_ps(_mm256_or_ps(cond1_vec, cond2_vec), _mm256_set1_ps(1.0f));\n\n        __m256 result_vec = _mm256_add_ps(\n            _mm256_mul_ps(a_vec, mask1),\n            _mm256_add_ps(\n                _mm256_mul_ps(b_vec, mask2),\n                _mm256_mul_ps(c_vec, mask3)\n            )\n        );\n\n        _mm256_storeu_ps(&out[i], result_vec);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    const size_t vector_size = 8; // AVX: 8 x 32-bit integers\n    const size_t num_vectors = total_elements / vector_size;\n\n    __m256i bit_count_mask = _mm256_set_epi8(\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n    );\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        __m256i vec_in = _mm256_loadu_si256((__m256i*)(A + i * vector_size));\n        __m256i vec_popcnt = _mm256_popcnt_epi32(vec_in);\n        __m256i vec_bit_counts = _mm256_maddubs_epi16(vec_popcnt, bit_count_mask);\n        _mm256_storeu_si256((__m256i*)(out + i * vector_size), vec_bit_counts);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_vectors * vector_size; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    const size_t vector_size = 8; // AVX: 8 x 32-bit integers\n    const size_t num_vectors = total_elements / vector_size;\n\n    __m256i bit_count_mask = _mm256_set_epi8(\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n    );\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        __m256i vec_in = _mm256_loadu_si256((__m256i*)(A + i * vector_size));\n        __m256i vec_popcnt = _mm256_popcnt_epi32(vec_in);\n        __m256i vec_bit_counts = _mm256_maddubs_epi16(vec_popcnt, bit_count_mask);\n        _mm256_storeu_si256((__m256i*)(out + i * vector_size), vec_bit_counts);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_vectors * vector_size; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    const size_t vector_size = 8; // AVX: 8 x 32-bit integers\n    const size_t num_vectors = total_elements / vector_size;\n\n    __m256i bit_count_mask = _mm256_set_epi8(\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n    );\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        __m256i vec_in = _mm256_loadu_si256((__m256i*)(A + i * vector_size));\n        __m256i vec_popcnt = _mm256_popcnt_epi32(vec_in);\n        __m256i vec_bit_counts = _mm256_maddubs_epi16(vec_popcnt, bit_count_mask);\n        _mm256_storeu_si256((__m256i*)(out + i * vector_size), vec_bit_counts);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_vectors * vector_size; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    const size_t vector_size = 8; // AVX: 8 x 32-bit integers\n    const size_t num_vectors = total_elements / vector_size;\n\n    __m256i bit_count_mask = _mm256_set_epi8(\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n    );\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        __m256i vec_in = _mm256_loadu_si256((__m256i*)(A + i * vector_size));\n        __m256i vec_popcnt = _mm256_popcnt_epi32(vec_in);\n        __m256i vec_bit_counts = _mm256_maddubs_epi16(vec_popcnt, bit_count_mask);\n        _mm256_storeu_si256((__m256i*)(out + i * vector_size), vec_bit_counts);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_vectors * vector_size; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    const size_t vector_size = 8; // AVX: 8 x 32-bit integers\n    const size_t num_vectors = total_elements / vector_size;\n\n    __m256i bit_count_mask = _mm256_set_epi8(\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n    );\n\n    for (size_t i = 0; i < num_vectors; ++i) {\n        __m256i vec_in = _mm256_loadu_si256((__m256i*)(A + i * vector_size));\n        __m256i vec_popcnt = _mm256_popcnt_epi32(vec_in);\n        __m256i vec_bit_counts = _mm256_maddubs_epi16(vec_popcnt, bit_count_mask);\n        _mm256_storeu_si256((__m256i*)(out + i * vector_size), vec_bit_counts);\n    }\n\n    // Handle remaining elements\n    for (size_t i = num_vectors * vector_size; i < total_elements; ++i) {\n        out[i] = __builtin_popcount(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n    __m256i mask, value, abs_value;\n\n    for (i = 0; i < limit; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        \n        // Check divisibility by 3\n        mask = _mm256_set1_epi32(3);\n        __m256i mod_result = _mm256_and_si256(a_vec, mask);\n        __m256i cmp_result = _mm256_cmpeq_epi32(mod_result, _mm256_setzero_si256());\n\n        // Compute absolute values\n        value = a_vec;\n        abs_value = _mm256_sub_epi32(_mm256_setzero_si256(), a_vec);\n        __m256i result = _mm256_blendv_epi8(value, abs_value, cmp_result);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n\n    __m256i mask = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (i = 0; i < limit; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_b = vec_a;\n        __m256i remainder = _mm256_and_si256(vec_a, mask);\n\n        // Check if remainder is zero (divisible by 3)\n        __m256i is_divisible = _mm256_cmpeq_epi32(remainder, zero);\n\n        // Conditional abs: if divisible by 3, take abs value\n        __m256i abs_value = _mm256_sub_epi32(vec_b, _mm256_add_epi32(vec_b, zero)); // Zero out negative values\n\n        // Blend based on condition\n        __m256i result = _mm256_blendv_epi8(vec_a, abs_value, is_divisible);\n\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n    __m256i mask, val_a, val_b;\n\n    for (i = 0; i < limit; i += 8) {\n        // Load 8 elements from A\n        val_a = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Check divisibility by 3 for each element\n        mask = _mm256_set1_epi32(3);\n        val_b = _mm256_mullo_epi32(val_a, _mm256_setzero_si256());\n        val_b = _mm256_and_si256(_mm256_cmp_epi32(val_b, mask, _MM_CMPINT_EQ), mask);\n\n        // Conditional abs operation\n        val_a = _mm256_sub_epi32(val_a, val_b);\n        val_b = _mm256_xor_si256(val_a, val_b);\n        val_a = _mm256_add_epi32(val_a, val_b);\n\n        // Store the result back to B\n        _mm256_storeu_si256((__m256i*)&B[i], val_a);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n\n    __m256i mask = _mm256_set1_epi32(3);\n    __m256i zero = _mm256_setzero_si256();\n\n    for (i = 0; i < limit; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i vec_b = vec_a;\n        __m256i remainder = _mm256_and_si256(vec_a, mask);\n\n        // Check if remainder is zero (divisible by 3)\n        __m256i is_divisible = _mm256_cmpeq_epi32(remainder, zero);\n\n        // Conditional abs: if divisible by 3, take abs value\n        __m256i abs_value = _mm256_sub_epi32(vec_a, _mm256_add_epi32(vec_a, zero));\n        __m256i result = _mm256_blendv_epi8(vec_a, abs_value, is_divisible);\n\n        _mm256_storeu_si256((__m256i*)&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n    __m256i mask, val_a, val_b;\n\n    for (i = 0; i < limit; i += 8) {\n        // Load 8 integers from A\n        val_a = _mm256_loadu_si256((__m256i*)&A[i]);\n\n        // Check divisibility by 3: (a % 3 == 0)\n        // Compute remainder: a - ((a / 3) * 3)\n        __m256i div_3 = _mm256_mullo_epi32(_mm256_set1_epi32(3), _mm256_cvtepi32_epi32(val_a));\n        mask = _mm256_cmpeq_epi32(val_a, div_3);\n\n        // Compute abs(a): (a >= 0) ? a : -a\n        __m256i neg_a = _mm256_sub_epi32(_mm256_setzero_si256(), val_a);\n        val_b = _mm256_blendv_epi8(val_a, neg_a, mask);\n\n        // Store the result back to B\n        _mm256_storeu_si256((__m256i*)&B[i], val_b);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    const __m256i mask = _mm256_set1_epi32(0xF); // Mask for the low 4 bits\n    const __m256i rotate_mask = _mm256_set1_epi32((1 << rotate_bits) - 1); // Mask for rotated bits\n    const __m256i negate_mask = _mm256_set1_epi32(0); // Initialize to 0 for negation\n\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low4bits = _mm256_and_si256(src_vec, mask); // Extract low 4 bits\n        __m256i rotate_condition = _mm256_cmpeq_epi32(low4bits, mask); // Compare with 0xF\n        __m256i rotated = _mm256_or_si256(_mm256_slli_epi32(src_vec, rotate_bits), _mm256_srli_epi32(src_vec, (32 - rotate_bits))); // Rotate left\n        __m256i negated = _mm256_xor_si256(src_vec, rotate_mask); // Bitwise negation\n        __m256i result = _mm256_blendv_epi8(negated, rotated, rotate_condition); // Conditional blend\n        _mm256_storeu_si256((__m256i*)&dst[i], result); // Store result\n    }\n    for (; i < length; i++) {\n        uint32_t src_val = src[i];\n        uint32_t low4bits = src_val & 0xF;\n        if (low4bits == 0xF) {\n            dst[i] = (src_val << rotate_bits) | (src_val >> (32 - rotate_bits));\n        } else {\n            dst[i] = ~src_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xF); // Mask for the lower 4 bits\n    __m256i rot_mask = _mm256_set1_epi32((1 << rotate_bits) - 1); // Mask for rotated bits\n    __m256i ones = _mm256_set1_epi32(-1); // All ones\n\n    for(i = 0; i + 8 <= length; i += 8){\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low4bits = _mm256_and_si256(src_vec, mask); // Extract low 4 bits\n        __m256i rotate_condition = _mm256_cmpeq_epi32(low4bits, ones); // Compare with all ones\n        __m256i rotated = _mm256_or_si256(_mm256_slli_epi32(src_vec, rotate_bits), _mm256_srli_epi32(src_vec, (32 - rotate_bits))); // Rotate left\n        __m256i negated = _mm256_xor_si256(src_vec, ones); // Bitwise negation\n        __m256i result = _mm256_blendv_epi8(negated, rotated, rotate_condition); // Conditional blend\n        _mm256_storeu_si256((__m256i*)&dst[i], result); // Store result\n    }\n\n    for(; i < length; i++){\n        uint32_t src_val = src[i];\n        uint32_t low4bits = src_val & 0xF;\n        if(low4bits == 0xF){\n            uint32_t rotated = ((src_val << rotate_bits) | (src_val >> (32 - rotate_bits)));\n            dst[i] = rotated;\n        }\n        else{\n            dst[i] = ~src_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xF); // Mask for the lower 4 bits\n    __m256i rot_mask = _mm256_set1_epi32((1 << rotate_bits) - 1); // Mask for rotated bits\n    __m256i ones = _mm256_set1_epi32(-1); // All ones\n\n    for(i = 0; i + 8 <= length; i += 8){\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i low4bits = _mm256_and_si256(src_vec, mask); // Extract low 4 bits\n        __m256i rotate_condition = _mm256_cmpeq_epi32(low4bits, ones); // Compare with all ones\n        __m256i rotated = _mm256_or_si256(_mm256_slli_epi32(src_vec, rotate_bits), _mm256_srli_epi32(src_vec, (32 - rotate_bits))); // Rotate left\n        __m256i negated = _mm256_xor_si256(src_vec, ones); // Bitwise negation\n        __m256i result = _mm256_blendv_epi8(negated, rotated, rotate_condition); // Conditional blend\n        _mm256_storeu_si256((__m256i*)&dst[i], result); // Store result\n    }\n\n    for(; i < length; i++){\n        uint32_t src_val = src[i];\n        if((src_val & 0xF) == 0xF){\n            dst[i] = (src_val << rotate_bits) | (src_val >> (32 - rotate_bits));\n        }\n        else{\n            dst[i] = ~src_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xF); // Mask for the lower 4 bits\n    __m256i rot_mask = _mm256_set1_epi32((1 << rotate_bits) - 1); // Mask for rotated bits\n    __m256i ones = _mm256_set1_epi32(-1); // All ones\n\n    for(i = 0; i + 8 <= length; i += 8){\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i src_low4 = _mm256_and_si256(src_vec, mask); // Extract low 4 bits\n        __m256i rot_condition = _mm256_cmpeq_epi32(src_low4, ones); // Compare with all ones\n        __m256i src_rotated = _mm256_or_si256(_mm256_slli_epi32(src_vec, rotate_bits), _mm256_srli_epi32(src_vec, (32 - rotate_bits))); // Rotate left\n        __m256i src_negated = _mm256_xor_si256(src_vec, ones); // Bitwise negation\n        __m256i dst_vec = _mm256_blendv_epi8(src_negated, src_rotated, _mm256_castps2 fiftyfifty\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m256i mask = _mm256_set1_epi32(0xF); // Mask for the lower 4 bits\n    __m256i rot_mask = _mm256_set1_epi32((1 << rotate_bits) - 1); // Mask for rotated bits\n    __m256i ones = _mm256_set1_epi32(-1); // All ones\n\n    for(i = 0; i + 8 <= length; i += 8){\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)&src[i]);\n        __m256i src_low4 = _mm256_and_si256(src_vec, mask); // Extract low 4 bits\n        __m256i rotate_condition = _mm256_cmpeq_epi32(src_low4, ones); // Compare with all ones\n        __m256i rotated = _mm256_or_si256(_mm256_slli_epi32(src_vec, rotate_bits), _mm256_srli_epi32(src_vec, (32 - rotate_bits))); // Rotate left\n        __m256i negated = _mm256_xor_si256(src_vec, ones); // Bitwise negation\n        __m256i result = _mm256_blendv_epi8(negated, rotated, rotate_condition); // Conditional blend\n        _mm256_storeu_si256((__m256i*)&dst[i], result); // Store result\n    }\n\n    for(; i < length; i++){\n        uint32_t src_val = src[i];\n        if((src_val & 0xF) == 0xF){\n            dst[i] = (src_val << rotate_bits) | (src_val >> (32 - rotate_bits));\n        }\n        else{\n            dst[i] = ~src_val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps_vec = _mm256_set1_ps(epsilon);\n    __m256 pos_mask = _mm256_set1_ps(1.0f);\n    __m256 neg_mask = _mm256_set1_ps(-1.0f);\n    __m256 zero_mask = _mm256_setzero_ps();\n\n    for (; i <= length - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_max_ps(_mm256_sub_ps(zero_mask, diff_vec), _mm256_sub_ps(diff_vec, zero_mask));\n        \n        __m256 cmp_gt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_LT_OS);\n\n        __m256 result_vec = _mm256_blendv_ps(_mm256_blendv_ps(neg_mask, pos_mask, cmp_gt), zero_mask, cmp_lt);\n\n        _mm256_storeu_si256((__m256i*) &out[i], _mm256_castps_si256(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps_vec = _mm256_set1_ps(epsilon);\n    __m256 pos_mask = _mm256_set1_ps(1.0f);\n    __m256 neg_mask = _mm256_set1_ps(-1.0f);\n    __m256 zero_mask = _mm256_setzero_ps();\n\n    for (; i <= length - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_abs_ps(diff_vec);\n\n        __m256 cmp_gt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_LT_OS);\n\n        __m256 result_pos = _mm256_blendv_ps(zero_mask, pos_mask, cmp_gt);\n        __m256 result_neg = _mm256_blendv_ps(result_pos, neg_mask, cmp_lt);\n\n        _mm256_storeu_si256((__m256i*)(&out[i]), _mm256_castps_si256(result_neg));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps_vec = _mm256_set1_ps(epsilon);\n    __m256 pos_mask = _mm256_set1_ps(1.0f);\n    __m256 neg_mask = _mm256_set1_ps(-1.0f);\n    \n    for (; i <= length - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_abs_ps(diff_vec);\n        \n        __m256 cmp_gt = _mm256_cmpgt_ps(abs_diff_vec, eps_vec);\n        __m256 cmp_lt = _mm256_cmpgt_ps(eps_vec, abs_diff_vec);\n        \n        __m256 result_pos = _mm256_blendv_ps(neg_mask, pos_mask, cmp_gt);\n        __m256 result_final = _mm256_blendv_ps(result_pos, _mm256_setzero_ps(), cmp_lt);\n        \n        _mm256_storeu_si256((__m256i*) &out[i], _mm256_cvtps_epi32(result_final));\n    }\n    \n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps_vec = _mm256_set1_ps(epsilon);\n    __m256 pos_mask = _mm256_set1_ps(1.0f);\n    __m256 neg_mask = _mm256_set1_ps(-1.0f);\n    __m256 zero_mask = _mm256_setzero_ps();\n\n    for (; i <= length - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_max_ps(_mm256_sub_ps(zero_mask, diff_vec), _mm256_sub_ps(diff_vec, zero_mask));\n        \n        __m256 cmp_gt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_LT_OS);\n\n        __m256 result_vec = _mm256_blendv_ps(_mm256_blendv_ps(neg_mask, pos_mask, cmp_gt), zero_mask, cmp_lt);\n\n        _mm256_storeu_si256((__m256i*)(&out[i]), _mm256_castps_si256(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m256 eps_vec = _mm256_set1_ps(epsilon);\n    __m256 pos_mask = _mm256_set1_ps(1.0f);\n    __m256 neg_mask = _mm256_set1_ps(-1.0f);\n    __m256 zero_mask = _mm256_setzero_ps();\n\n    for (; i <= length - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 diff_vec = _mm256_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm256_max_ps(_mm256_sub_ps(zero_mask, diff_vec), _mm256_sub_ps(diff_vec, zero_mask));\n        \n        __m256 cmp_gt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_GT_OS);\n        __m256 cmp_lt = _mm256_cmp_ps(abs_diff_vec, eps_vec, _CMP_LT_OS);\n\n        __m256 result_vec = _mm256_blendv_ps(_mm256_blendv_ps(neg_mask, pos_mask, cmp_gt), zero_mask, cmp_lt);\n\n        _mm256_storeu_si256((__m256i*)(&out[i]), _mm256_castps_si256(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 min_mask = _mm256_cmpgt_ps(a_vec, max_val);\n        __m256 max_mask = _mm256_cmpgt_ps(min_val, a_vec);\n        __m256 saturated = _mm256_add_ps(_mm256_andnot_ps(min_mask, a_vec), _mm256_and_ps(min_mask, max_val));\n        saturated = _mm256_sub_ps(saturated, _mm256_and_ps(max_mask, min_val));\n        _mm256_storeu_si256((__m256i*)(&B[i]), _mm256_cvtps_epi32(saturated));\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        B[i] = (val > INT32_MAX) ? INT32_MAX : ((val < INT32_MIN) ? INT32_MIN : (int32_t)val);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m256 int32_max_vec = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 int32_min_vec = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 min_vec = _mm256_max_ps(a_vec, int32_min_vec);\n        __m256 max_vec = _mm256_min_ps(min_vec, int32_max_vec);\n        __m256i b_vec = _mm256_cvttps_epi32(max_vec);\n        _mm256_storeu_si256((__m256i*)&B[i], b_vec);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m256 int32_max_vec = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 int32_min_vec = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 min_vec = _mm256_max_ps(a_vec, int32_min_vec);\n        __m256 max_vec = _mm256_min_ps(min_vec, int32_max_vec);\n        __m256i b_vec = _mm256_cvttps_epi32(max_vec);\n        _mm256_storeu_si256((__m256i*)&B[i], b_vec);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m256 max_val = _mm256_set1_ps((float)INT32_MAX);\n    __m256 min_val = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 min_mask = _mm256_cmpgt_ps(a_vec, max_val);\n        __m256 max_mask = _mm256_cmpgt_ps(min_val, a_vec);\n        __m256 saturated = _mm256_add_ps(_mm256_sub_ps(a_vec, min_mask), max_mask);\n        __m256i saturated_i = _mm256_cvttps_epi32(saturated);\n        _mm256_storeu_si256((__m256i*)&B[i], saturated_i);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        B[i] = (val > INT32_MAX) ? INT32_MAX : ((val < INT32_MIN) ? INT32_MIN : (int32_t)val);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    const __m256 int32_max_vec = _mm256_set1_ps((float)INT32_MAX);\n    const __m256 int32_min_vec = _mm256_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 min_vec = _mm256_min_ps(a_vec, int32_max_vec);\n        __m256 max_vec = _mm256_max_ps(min_vec, int32_min_vec);\n        __m256i b_vec = _mm256_cvttps_epi32(max_vec);\n        _mm256_storeu_si256((__m256i*)&B[i], b_vec);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps(scale);\n    \n    // Process 8 elements per iteration\n    for (; i <= size - 8; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_b = _mm256_cvtepi32_ps(vec_a);\n        vec_b = _mm256_mul_ps(vec_b, scale_vec);\n        _mm256_storeu_ps(B + i, vec_b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps(scale);\n    \n    // Process 8 elements per iteration\n    for (; i <= size - 8; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_b = _mm256_cvtepi32_ps(vec_a);\n        vec_b = _mm256_mul_ps(vec_b, scale_vec);\n        _mm256_storeu_ps(B + i, vec_b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps(scale);\n    \n    // Process data in chunks of 8 elements (AVX)\n    for (; i <= size - 8; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_b = _mm256_cvtepi32_ps(vec_a);\n        vec_b = _mm256_mul_ps(vec_b, scale_vec);\n        _mm256_storeu_ps(B + i, vec_b);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps(scale);\n\n    // Process data in chunks of 8 integers at a time\n    for (; i <= size - 8; i += 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256 vec_b = _mm256_cvtepi32_ps(vec_a);\n        vec_b = _mm256_mul_ps(vec_b, scale_vec);\n        _mm256_storeu_ps(&B[i], vec_b);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i = 0;\n    __m256 scale_vec = _mm256_set1_ps(scale);\n    \n    // Process 8 elements per iteration\n    for (; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 a_float_vec = _mm256_cvtepi32_ps(a_vec);\n        __m256 scaled_vec = _mm256_mul_ps(a_float_vec, scale_vec);\n        _mm256_storeu_ps(B + i, scaled_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 div_vec = _mm256_div_ps(a_vec, b_vec);\n\n        // Convert to integers using rounding mode\n        __m256i rounded_vec = _mm256_cvttps_epi32(div_vec);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i*)&C[i], rounded_vec);\n    }\n    for (; i < size; i++) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 div_vec = _mm256_div_ps(a_vec, b_vec);\n\n        // Convert to integers using rounding mode\n        __m256i rounded_vec = _mm256_cvttps_epi32(div_vec);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i*)&C[i], rounded_vec);\n    }\n    for (; i < size; i++) {\n        C[i] = (int32_t)(A[i] / B[i] + (A[i] >= 0 ? 0.5f : -0.5f));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 div_vec = _mm256_div_ps(a_vec, b_vec);\n\n        // Convert to integer and round\n        __m256i rounded_vec = _mm256_cvttps_epi32(div_vec);\n\n        // Adjust rounding if necessary\n        __m256 sign_mask = _mm256_cmp_ps(div_vec, _mm256_setzero_ps(), _CMP_LT_OQ);\n        __m256 fraction_part = _mm256_sub_ps(div_vec, _mm256_floor_ps(div_vec));\n        __m256 threshold = _mm256_set1_ps(0.5f);\n        __m256 needs_adjustment = _mm256_andnot_ps(sign_mask, _mm256_cmp_ps(fraction_part, threshold, _CMP_GE_OS));\n\n        __m256 adjustment = _mm256_blendv_ps(_mm256_setzero_ps(), _mm256_set1_ps(1.0f), needs_adjustment);\n        __m256 adjusted_div = _mm256_add_ps(div_vec, adjustment);\n\n        __m256i adjusted_rounded_vec = _mm256_cvttps_epi32(adjusted_div);\n\n        _mm256_storeu_si256((__m256i*)&C[i], adjusted_rounded_vec);\n    }\n    for (; i < size; i++) {\n        C[i] = static_cast<int32_t>(round(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 div_vec = _mm256_div_ps(a_vec, b_vec);\n\n        // Convert to integer and round\n        __m256i round_vec = _mm256_cvttps_epi32(div_vec);\n\n        // Check for rounding errors and adjust if necessary\n        __m256 sign_mask = _mm256_set1_ps(-0.0f);\n        __m256 sign_a = _mm256_and_ps(a_vec, sign_mask);\n        __m256 sign_b = _mm256_and_ps(b_vec, sign_mask);\n        __m256 sign_check = _mm256_cmp_ps(sign_a, sign_b, _CMP_EQ_OQ);\n        __m256 remainder = _mm256_sub_ps(div_vec, _mm256_cvtepi32_ps(round_vec));\n        __m256 remainder_check = _mm256_cmp_ps(remainder, _mm256_set1_ps(0.5f), _CMP_LT_OQ);\n        __m256 adjustment = _mm256_blendv_ps(_mm256_setzero_ps(), _mm256_set1_ps(1.0f), remainder_check);\n\n        // Adjust rounded values based on remainder\n        __m256i adjusted_round_vec = _mm256_add_epi32(round_vec, _mm256_cvtps_epi32(adjustment));\n\n        // Store the results\n        _mm256_storeu_si256((__m256i*)&C[i], adjusted_round_vec);\n    }\n    for (; i < size; i++) {\n        C[i] = static_cast<int32_t>(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 b_vec = _mm256_loadu_ps(&B[i]);\n        __m256 div_vec = _mm256_div_ps(a_vec, b_vec);\n\n        // Convert to integers using rounding mode\n        __m256i rounded_vec = _mm256_cvttps_epi32(div_vec);\n\n        // Store the result\n        _mm256_storeu_si256((__m256i*)&C[i], rounded_vec);\n    }\n    for (; i < size; i++) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256d max_val_d = _mm256_set1_pd(FLT_MAX);\n    __m256 min_val_f = _mm256_set1_ps(-FLT_MAX);\n    __m256 max_val_f = _mm256_set1_ps(FLT_MAX);\n\n    for (; i <= size - 8; i += 8) {\n        __m256d A_vec = _mm256_loadu_pd(&A[i]);\n        __m256 A_vec_f = _mm256_cvtpd_ps(A_vec);\n\n        __m256 cmp_max = _mm256_cmpgt_ps(A_vec_f, max_val_f);\n        __m256 cmp_min = _mm256_cmplt_ps(A_vec_f, min_val_f);\n\n        A_vec_f = _mm256_blendv_ps(A_vec_f, max_val_f, cmp_max);\n        A_vec_f = _mm256_blendv_ps(A_vec_f, min_val_f, cmp_min);\n\n        _mm256_storeu_ps(&B[i], A_vec_f);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        float fval = (val > FLT_MAX) ? FLT_MAX : ((val < -FLT_MAX) ? -FLT_MAX : (float)val);\n        B[i] = fval;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    const __m256d max_val_d = _mm256_set1_pd((double)FLT_MAX);\n    const __m256 min_val_f = _mm256_set1_ps(FLT_MIN);\n    const __m256 max_val_f = _mm256_set1_ps(FLT_MAX);\n\n    for (; i <= size - 8; i += 8) {\n        __m256d A_vec = _mm256_loadu_pd(&A[i]);\n        __m256 A_vec_f = _mm256_cvtpd_ps(A_vec);\n\n        // Saturate to positive range\n        __m256 cmp_pos = _mm256_cmpgt_ps(A_vec_f, max_val_f);\n        __m256 saturated_pos = _mm256_andnot_ps(cmp_pos, A_vec_f);\n        __m256 saturated_neg = _mm256_andnot_ps(_mm256_cmpgt_ps(min_val_f, A_vec_f), A_vec_f);\n        __m256 result = _mm256_add_ps(saturated_pos, saturated_neg);\n\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        float fval = (val > (double)FLT_MAX) ? FLT_MAX : ((val < -(double)FLT_MAX) ? -FLT_MAX : (float)val);\n        B[i] = fval;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    const __m256d max_val_d = _mm256_set1_pd(FLT_MAX);\n    const __m256 min_val_f = _mm256_set1_ps(-FLT_MAX);\n    const __m256 max_val_f = _mm256_set1_ps(FLT_MAX);\n\n    for (; i <= size - 8; i += 8) {\n        __m256d A_vec = _mm256_loadu_pd(&A[i]);\n        __m256 A_vec_f = _mm256_cvtpd_ps(A_vec);\n\n        __m256 A_clamped = _mm256_max_ps(A_vec_f, min_val_f);\n        A_clamped = _mm256_min_ps(A_clamped, max_val_f);\n\n        _mm256_storeu_ps(&B[i], A_clamped);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        float fval = (float)(val > FLT_MAX ? FLT_MAX : (val < -FLT_MAX ? -FLT_MAX : val));\n        B[i] = fval;\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256d max_val_d = _mm256_set1_pd(FLT_MAX);\n    __m256 min_val_f = _mm256_set1_ps(-FLT_MAX);\n    __m256 max_val_f = _mm256_set1_ps(FLT_MAX);\n\n    for (; i <= size - 8; i += 8) {\n        __m256d A_vec = _mm256_loadu_pd(&A[i]);\n        __m256 A_vec_f = _mm256_cvtpd_ps(A_vec);\n\n        __m256 cmp_max = _mm256_cmpgt_ps(A_vec_f, max_val_f);\n        __m256 cmp_min = _mm256_cmplt_ps(A_vec_f, min_val_f);\n\n        A_vec_f = _mm256_blendv_ps(A_vec_f, max_val_f, cmp_max);\n        A_vec_f = _mm256_blendv_ps(A_vec_f, min_val_f, cmp_min);\n\n        _mm256_storeu_ps(&B[i], A_vec_f);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = static_cast<float>(val);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256d max_val_d = _mm256_set1_pd(FLT_MAX);\n    __m256 min_val_f = _mm256_set1_ps(-FLT_MAX);\n    __m256 max_val_f = _mm256_set1_ps(FLT_MAX);\n\n    for (; i <= size - 8; i += 8) {\n        __m256d A_vec = _mm256_loadu_pd(&A[i]);\n        __m256 A_vec_f = _mm256_cvtpd_ps(A_vec);\n\n        __m256 cmp_max = _mm256_cmpgt_ps(A_vec_f, max_val_f);\n        __m256 cmp_min = _mm256_cmplt_ps(A_vec_f, min_val_f);\n\n        A_vec_f = _mm256_blendv_ps(A_vec_f, max_val_f, cmp_max);\n        A_vec_f = _mm256_blendv_ps(A_vec_f, min_val_f, cmp_min);\n\n        _mm256_storeu_ps(&B[i], A_vec_f);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        float result = (val > FLT_MAX) ? FLT_MAX : ((val < -FLT_MAX) ? -FLT_MAX : (float)val);\n        B[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 16) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_b = _mm256_cvtepi16_epi32(vec_a);\n        _mm256_storeu_si256((__m256i*)(B + i), vec_b);\n        i += 16;\n        remaining -= 16;\n    }\n\n    if (remaining > 0) {\n        __m128i tail_vec_a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i tail_vec_b = _mm_cvtepi16_epi32(tail_vec_a);\n        _mm_storeu_si128((__m128i*)(B + i), tail_vec_b);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 16) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_b = _mm256_cvtepi16_epi32(vec_a);\n        _mm256_storeu_si256((__m256i*)(B + i), vec_b);\n        i += 16;\n        remaining -= 16;\n    }\n\n    if (remaining > 0) {\n        __m128i tail_vec_a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i tail_vec_b = _mm_cvtepi16_epi32(tail_vec_a);\n        _mm_storeu_si128((__m128i*)(B + i), tail_vec_b);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 16) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_b = _mm256_cvtepi16_epi32(vec_a);\n        _mm256_storeu_si256((__m256i*)(B + i), vec_b);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    while (remaining > 0) {\n        B[i] = static_cast<int32_t>(A[i]);\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 16) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_b = _mm256_cvtepi16_epi32(vec_a);\n        _mm256_storeu_si256((__m256i*)(B + i), vec_b);\n        i += 16;\n        remaining -= 16;\n    }\n\n    if (remaining > 0) {\n        __m128i tail_vec_a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i tail_vec_b = _mm_cvtepi16_epi32(tail_vec_a);\n        _mm_storeu_si128((__m128i*)(B + i), tail_vec_b);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 16) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i vec_b = _mm256_cvtepi16_epi32(vec_a);\n        _mm256_storeu_si256((__m256i*)(B + i), vec_b);\n        i += 16;\n        remaining -= 16;\n    }\n\n    if (remaining > 0) {\n        __m128i tail_vec_a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i tail_vec_b = _mm_cvtepi16_epi32(tail_vec_a);\n        _mm_storeu_si128((__m128i*)(B + i), tail_vec_b);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 c_vec = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i*)&control[i]));\n\n        // Compute (A[i] - min_val)\n        __m256 diff_vec = _mm256_sub_ps(a_vec, min_val_vec);\n\n        // Compute (max_val - min_val)\n        __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n\n        // Compute (A[i] - min_val) / (max_val - min_val)\n        __m256 norm_vec = _mm256_div_ps(diff_vec, range_vec);\n\n        // Clamp to [0, 1]\n        __m256 clamp_vec = _mm256_max_ps(zero_vec, _mm256_min_ps(norm_vec, one_vec));\n\n        // Conditional copy: If control > 0, use normalized value; else, copy A[i]\n        __m256i mask_vec = _mm256_cmpgt_epi32(_mm256_castps_si256(c_vec), _mm256_setzero_si256());\n        __m256 result_vec = _mm256_blendv_ps(a_vec, clamp_vec, _mm256_castsi256_ps(mask_vec));\n\n        // Store the result\n        _mm256_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (control[i] > 0) ? fmaxf(0.0f, fminf((A[i] - min_val) / (max_val - min_val), 1.0f)) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 c_vec = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i*)&control[i]));\n        __m256 mask = _mm256_cmpgt_epi32(_mm256_castps_si256(c_vec), _mm256_setzero_si256());\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), _mm256_sub_ps(max_val_vec, min_val_vec));\n        normalized = _mm256_max_ps(zero_vec, _mm256_min_ps(normalized, one_vec));\n\n        __m256 a_copy = _mm256_loadu_ps(&A[i]);\n        __m256 result = _mm256_blendv_ps(a_copy, normalized, mask);\n\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; i++) {\n        B[i] = (control[i] > 0) ? fmaxf(0.0f, fminf((A[i] - min_val) / (max_val - min_val), 1.0f)) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 c_vec = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i*)&control[i]));\n        __m256 mask = _mm256_cmpgt_epi32(_mm256_castps_si256(c_vec), _mm256_setzero_si256());\n\n        __m256 diff_vec = _mm256_sub_ps(a_vec, min_val_vec);\n        __m256 range_vec = _mm256_sub_ps(max_val_vec, min_val_vec);\n        __m256 norm_vec = _mm256_div_ps(diff_vec, range_vec);\n\n        // Clamp to [0, 1]\n        norm_vec = _mm256_max_ps(norm_vec, zero_vec);\n        norm_vec = _mm256_min_ps(norm_vec, one_vec);\n\n        // Apply mask: use norm_vec where mask is true, else copy A values\n        __m256 a_masked = _mm256_blendv_ps(_mm256_setzero_ps(), a_vec, mask);\n        __m256 b_vec = _mm256_blendv_ps(a_masked, norm_vec, mask);\n\n        _mm256_storeu_ps(&B[i], b_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 c_vec = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i*)&control[i]));\n        __m256 mask = _mm256_cmpgt_epi32(_mm256_castps_si256(c_vec), _mm256_setzero_si256());\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), _mm256_sub_ps(max_val_vec, min_val_vec));\n        normalized = _mm256_max_ps(zero_vec, _mm256_min_ps(normalized, one_vec));\n\n        __m256 a_copy = _mm256_loadu_ps(&A[i]);\n        __m256 result = _mm256_blendv_ps(a_copy, normalized, mask);\n\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m256 min_val_vec = _mm256_set1_ps(min_val);\n    __m256 max_val_vec = _mm256_set1_ps(max_val);\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 one_vec = _mm256_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m256 a_vec = _mm256_loadu_ps(&A[i]);\n        __m256 c_vec = _mm256_castsi256_ps(_mm256_loadu_si256((__m256i*)&control[i]));\n        __m256 mask = _mm256_cmpgt_epi32(_mm256_castps_si256(c_vec), _mm256_setzero_si256());\n\n        __m256 normalized = _mm256_div_ps(_mm256_sub_ps(a_vec, min_val_vec), _mm256_sub_ps(max_val_vec, min_val_vec));\n        normalized = _mm256_max_ps(zero_vec, _mm256_min_ps(normalized, one_vec));\n\n        __m256 a_copy = _mm256_loadu_ps(&A[i]);\n        __m256 result = _mm256_blendv_ps(a_copy, normalized, mask);\n\n        _mm256_storeu_ps(&B[i], result);\n    }\n\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    // Process data in chunks of 8 elements using AVX\n    while (remaining >= 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_high_bits = _mm256_cvtepi32_ps(_mm256_srli_epi32(vec_a, 16));\n        __m256 vec_sqrt = _mm256_sqrt_ps(vec_high_bits);\n        _mm256_storeu_ps(B + i, vec_sqrt);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements if size is not a multiple of 8\n    while (remaining > 0) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    // Process data in chunks of 8 elements using AVX\n    while (remaining >= 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_high_bits = _mm256_cvtepi32_ps(_mm256_srli_epi32(vec_a, 16));\n        __m256 vec_sqrt = _mm256_sqrt_ps(vec_high_bits);\n        _mm256_storeu_ps(B + i, vec_sqrt);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements if size is not a multiple of 8\n    while (remaining > 0) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    // Process data in chunks of 8 elements using AVX\n    while (remaining >= 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_high_bits = _mm256_cvtepi32_ps(_mm256_srli_epi32(vec_a, 16));\n        __m256 vec_sqrt = _mm256_sqrt_ps(vec_high_bits);\n        _mm256_storeu_ps(B + i, vec_sqrt);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements if size is not a multiple of 8\n    while (remaining > 0) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    // Process data in 8-lane AVX vectors\n    while (remaining >= 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_shifted = _mm256_srli_epi32(vec_a, 16);\n        __m256 vec_converted = _mm256_cvtepi32_ps(vec_shifted);\n        __m256 vec_sqrt = _mm256_sqrt_ps(vec_converted);\n\n        _mm256_storeu_ps(B + i, vec_sqrt);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements if size is not a multiple of 8\n    while (remaining > 0) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    // Process data in chunks of 8 elements using AVX\n    while (remaining >= 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_high_bits = _mm256_cvtepi32_ps(_mm256_srli_epi32(vec_a, 16));\n        __m256 vec_sqrt = _mm256_sqrt_ps(vec_high_bits);\n        _mm256_storeu_ps(B + i, vec_sqrt);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    // Handle remaining elements if size is not a multiple of 8\n    while (remaining > 0) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n        i++;\n        remaining--;\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 8) {\n        __m256i vec_a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_b = _mm256_castsi256_ps(vec_a);\n        _mm256_storeu_ps(B + i, vec_b);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    if (remaining > 0) {\n        __m256i vec_a = _mm256_maskz_loadu_epi32((__mmask8)(1 << remaining) - 1, A + i);\n        __m256 vec_b = _mm256_castsi256_ps(vec_a);\n        _mm256_maskstore_ps(B + i, (__mmask8)(1 << remaining) - 1, vec_b);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 8) {\n        __m256i vec_int = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_float = _mm256_castsi256_ps(vec_int);\n        _mm256_storeu_ps(B + i, vec_float);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    if (remaining > 0) {\n        __m256i vec_int = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_float = _mm256_castsi256_ps(vec_int);\n        _mm256_maskstore_ps(B + i, _mm256_set1_epi32(-1), vec_float);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 8) {\n        __m256i vec_int = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_float = _mm256_castsi256_ps(vec_int);\n        _mm256_storeu_ps(B + i, vec_float);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    if (remaining > 0) {\n        __m256i vec_int = _mm256_maskz_loadu_epi32((__mmask8)(1u << remaining) - 1, A + i);\n        __m256 vec_float = _mm256_castsi256_ps(vec_int);\n        _mm256_maskstore_ps(B + i, (__mmask8)(1u << remaining) - 1, vec_float);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    size_t remaining = size;\n\n    while (remaining >= 8) {\n        __m256i vec_int = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256 vec_float = _mm256_castsi256_ps(vec_int);\n        _mm256_storeu_ps(B + i, vec_float);\n\n        i += 8;\n        remaining -= 8;\n    }\n\n    if (remaining > 0) {\n        __m256i vec_int = _mm256_maskz_loadu_epi32((__mmask8)(1 << remaining), (A + i));\n        __m256 vec_float = _mm256_castsi256_ps(vec_int);\n        _mm256_maskstore_ps(B + i, (__mmask8)(1 << remaining), vec_float);\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m256i vec_a;\n    __m256 vec_b;\n\n    // Process full vector loads\n    for (; i <= size - 8; i += 8) {\n        vec_a = _mm256_loadu_si128((__m128i*)(A + i));\n        vec_a = _mm256_inserti128_epi32(vec_a, _mm_loadu_si128((__m128i*)(A + i + 4)), 1);\n        vec_b = _mm256_castsi256_ps(vec_a);\n        _mm256_storeu_ps(B + i, vec_b);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = *reinterpret_cast<float*>(&A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 prev_vec = zero_vec;\n\n    // Handle initial value\n    if (size > 0) {\n        diff[0] = 0.0f;\n        i += 8; // Skip first element for vectorization\n    }\n\n    // Process data in chunks of 8\n    for (; i + 8 <= size; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i prev_a_vec = _mm256_loadu_si256((__m256i*)&A[i - 8]);\n        __m256 cond_vec = _mm256_loadu_ps((float*)&cond[i]);\n\n        __m256 diff_vec = _mm256_sub_epi32(a_vec, prev_a_vec);\n        diff_vec = _mm256_and_ps(_mm256_castsi256_ps(cond_vec), diff_vec);\n        diff_vec = _mm256_add_ps(diff_vec, _mm256_sub_ps(zero_vec, _mm256_andnot_ps(cond_vec, zero_vec)));\n\n        _mm256_storeu_ps(&diff[i], diff_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 prev_vec = zero_vec;\n\n    // Handle initial value\n    diff[0] = 0.0f;\n\n    // Process data in chunks of 8 elements\n    for (; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i prev_a_vec = _mm256_loadu_si256((__m256i*)&A[i - 8]);\n        __m256 cond_vec = _mm256_loadu_ps((float*)&cond[i]);\n\n        __m256 diff_vec = _mm256_sub_ps(_mm256_cvtepi32_ps(a_vec), _mm256_cvtepi32_ps(prev_a_vec));\n        __m256 masked_diff_vec = _mm256_blendv_ps(zero_vec, diff_vec, cond_vec);\n\n        _mm256_storeu_ps(&diff[i], masked_diff_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 prev_val_vec = zero_vec;\n\n    // Handle initial value\n    diff[0] = 0.0f;\n\n    // Process data in chunks of 8 (AVX)\n    for (; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i prev_a_vec = _mm256_loadu_si256((__m256i*)&A[i - 1]);\n        __m256 cond_vec = _mm256_loadu_ps((float*)&cond[i]);\n\n        __m256 diff_vec = _mm256_sub_ps(_mm256_cvtepi32_ps(a_vec), _mm256_cvtepi32_ps(prev_a_vec));\n        __m256 masked_diff_vec = _mm256_blendv_ps(zero_vec, diff_vec, cond_vec);\n\n        _mm256_storeu_ps(&diff[i], masked_diff_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 prev_vec = zero_vec;\n\n    // Handle initial value\n    diff[0] = 0.0f;\n\n    // Process data in chunks of 8 elements\n    for (; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i prev_a_vec = _mm256_loadu_si256((__m256i*)&A[i - 8]);\n        __m256 cond_vec = _mm256_loadu_ps((float*)&cond[i]);\n\n        __m256 diff_vec = _mm256_sub_ps(_mm256_cvtepi32_ps(a_vec), _mm256_cvtepi32_ps(prev_a_vec));\n        __m256 masked_diff_vec = _mm256_blendv_ps(zero_vec, diff_vec, cond_vec);\n\n        _mm256_storeu_ps(&diff[i], masked_diff_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    size_t i = 0;\n    __m256 zero_vec = _mm256_setzero_ps();\n    __m256 prev_vec = zero_vec;\n\n    // Handle initial value\n    diff[0] = 0.0f;\n\n    // Process data in chunks of 8 elements\n    for (; i <= size - 8; i += 8) {\n        __m256i a_vec = _mm256_loadu_si256((__m256i*)&A[i]);\n        __m256i prev_a_vec = _mm256_loadu_si256((__m256i*)&A[i - 8]);\n        __m256 cond_vec = _mm256_loadu_ps((float*)&cond[i]);\n\n        __m256 diff_vec = _mm256_sub_ps(_mm256_cvtepi32_ps(a_vec), _mm256_cvtepi32_ps(prev_a_vec));\n        __m256 masked_diff_vec = _mm256_blendv_ps(zero_vec, diff_vec, cond_vec);\n\n        _mm256_storeu_ps(&diff[i], masked_diff_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n        __m256i result = _mm256_or_si256(src_vec, _mm256_castps_si256(_mm256_broadcast_ss((float*)&high_bits)));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n        __m256i result = _mm256_or_si256(src_vec, _mm256_slli_epi64(high_bits, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n        __m256i result = _mm256_or_si256(src_vec, _mm256_slli_epi64(high_bits, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n        __m256i result = _mm256_or_si256(src_vec, _mm256_slli_epi64(high_bits, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 8; i += 8) {\n        __m256i src_vec = _mm256_loadu_si256((__m256i*)(src + i));\n        __m256i high_bits = _mm256_set1_epi32(0xFFFFFFFF);\n        __m256i result = _mm256_or_si256(src_vec, _mm256_slli_epi64(high_bits, 32));\n        _mm256_storeu_si256((__m256i*)(dst + i), result);\n    }\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process 4 elements per iteration using AVX\n    for (; i <= length - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d vec0 = _mm256_i32gather_pd(&vec[0], idx_vec, sizeof(double));\n        __m256d vec1 = _mm256_i32gather_pd(&vec[0], _mm256_add_epi32(idx_vec, _mm256_set1_epi32(4)), sizeof(double));\n        sum = _mm256_add_pd(sum, _mm256_add_pd(vec0, vec1));\n    }\n\n    // Handle remaining elements\n    double temp[4];\n    double partial_sum = 0.0;\n    for (; i < length; i++) {\n        partial_sum += vec[index[i]];\n    }\n\n    _mm256_storeu_pd(temp, sum);\n    double final_sum = temp[0] + temp[1] + temp[2] + temp[3] + partial_sum;\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 4\n    for (; i <= length - 8; i += 8) {\n        __m256i idx_vec1 = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256i idx_vec2 = _mm256_loadu_si256((__m256i*)&index[i + 4]);\n\n        __m256d vec1 = _mm256_i32gather_pd(&vec[0], idx_vec1, sizeof(double));\n        __m256d vec2 = _mm256_i32gather_pd(&vec[0], idx_vec2, sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec1);\n        sum = _mm256_add_pd(sum, vec2);\n    }\n\n    // Handle remaining elements\n    double temp[4] = {0.0};\n    for (; i < length; ++i) {\n        temp[i % 4] += vec[index[i]];\n    }\n\n    __m256d remainder = _mm256_loadu_pd(temp);\n    sum = _mm256_add_pd(sum, remainder);\n\n    // Reduce sum to a single value\n    __m128d sum_low = _mm256_castpd256_pd128(sum);\n    __m128d sum_high = _mm256_extractf128_pd(sum, 1);\n    sum_low = _mm_add_pd(sum_low, sum_high);\n    __m128d sum_final = _mm_hadd_pd(sum_low, sum_low);\n    return _mm_cvtsd_f64(sum_final);\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 4 elements using AVX\n    for (; i <= length - 8; i += 8) {\n        __m256i idx_vec1 = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256i idx_vec2 = _mm256_loadu_si256((__m256i*)&index[i + 4]);\n\n        __m256d vec1 = _mm256_i32gather_pd(&vec[0], idx_vec1, sizeof(double));\n        __m256d vec2 = _mm256_i32gather_pd(&vec[0], idx_vec2, sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec1);\n        sum = _mm256_add_pd(sum, vec2);\n    }\n\n    // Handle remaining elements\n    double scalar_sum[4] = {0.0};\n    _mm256_storeu_pd(scalar_sum, sum);\n\n    for (; i < length; ++i) {\n        scalar_sum[i % 4] += vec[index[i]];\n    }\n\n    double total = 0.0;\n    for (i = 0; i < 4; ++i) {\n        total += scalar_sum[i];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process full vectors\n    for (; i <= length - 8; i += 8) {\n        __m256i idx_vec = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256d vec0 = _mm256_i32gather_pd(&vec[0], idx_vec, sizeof(double));\n        __m256d vec1 = _mm256_i32gather_pd(&vec[0], _mm256_add_epi32(idx_vec, _mm256_set1_epi32(4)), sizeof(double));\n        sum = _mm256_add_pd(sum, vec0);\n        sum = _mm256_add_pd(sum, vec1);\n    }\n\n    // Handle remaining elements\n    double scalar_sum[4] = {0.0};\n    for (; i < length; ++i) {\n        scalar_sum[i % 4] += vec[index[i]];\n    }\n\n    // Reduce vector sum\n    double temp_sum[4];\n    _mm256_storeu_pd(temp_sum, sum);\n\n    // Add remaining scalar elements\n    for (size_t j = 0; j < length % 8; ++j) {\n        scalar_sum[j] += temp_sum[j];\n    }\n\n    double total = 0.0;\n    for (size_t j = 0; j < 4; ++j) {\n        total += scalar_sum[j];\n    }\n\n    return total;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 4 elements using AVX\n    for (; i <= length - 8; i += 8) {\n        __m256i idx_vec1 = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256i idx_vec2 = _mm256_loadu_si256((__m256i*)&index[i + 4]);\n\n        __m256d vec1 = _mm256_i32gather_pd(&vec[0], idx_vec1, sizeof(double));\n        __m256d vec2 = _mm256_i32gather_pd(&vec[0], idx_vec2, sizeof(double));\n\n        sum = _mm256_add_pd(sum, vec1);\n        sum = _mm256_add_pd(sum, vec2);\n    }\n\n    // Handle remaining elements\n    double temp[4] = {0.0};\n    for (; i < length; ++i) {\n        temp[i % 4] += vec[index[i]];\n    }\n\n    __m256d remainder = _mm256_loadu_pd(temp);\n    sum = _mm256_add_pd(sum, remainder);\n\n    // Reduce sum to a single value\n    __m128d sum_low = _mm256_castpd256_pd128(sum);\n    __m128d sum_high = _mm256_extractf128_pd(sum, 1);\n    sum_low = _mm_add_pd(sum_low, sum_high);\n    __m128d sum_final = _mm_hadd_pd(sum_low, sum_low);\n    return _mm_cvtsd_f64(sum_final);\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const int kernel_half = kernel_size / 2;\n    const int output_size = input_size - kernel_size + 1;\n\n    for (size_t i = kernel_half; i < input_size - kernel_half; ++i) {\n        for (size_t j = kernel_half; j < input_size - kernel_half; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    double input_val = input[(i + m) * input_size + (j + n)];\n                    double kernel_val = kernel[m * kernel_size + n];\n                    __m256d input_vec = _mm256_broadcast_sd(&input_val);\n                    __m256d kernel_vec = _mm256_broadcast_sd(&kernel_val);\n                    __m256d prod = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum = _mm256_add_pd(sum, prod);\n                }\n            }\n            __m256d relu_mask = _mm256_cmp_pd(sum, _mm256_setzero_pd(), _CMP_LT_OS);\n            sum = _mm256_sub_pd(_mm256_setzero_pd(), _mm256_andnot_pd(relu_mask, sum));\n            _mm256_storeu_pd(&output[(i - kernel_half) * output_size + (j - kernel_half)], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const int kernel_half = kernel_size / 2;\n    const int output_size = input_size - kernel_size + 1;\n\n    for (size_t i = kernel_half; i < input_size - kernel_half; ++i) {\n        for (size_t j = kernel_half; j < input_size - kernel_half; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    double input_val = input[(i + m) * input_size + (j + n)];\n                    double kernel_val = kernel[m * kernel_size + n];\n                    __m256d input_vec = _mm256_broadcast_sd(&input_val);\n                    __m256d kernel_vec = _mm256_broadcast_sd(&kernel_val);\n                    __m256d prod = _mm256_mul_pd(input_vec, kernel_vec);\n                    sum = _mm256_add_pd(sum, prod);\n                }\n            }\n            __m256d relu_mask = _mm256_cmp_pd(sum, _mm256_setzero_pd(), _CMP_LT_OS);\n            sum = _mm256_sub_pd(_mm256_setzero_pd(), _mm256_andnot_pd(relu_mask, sum));\n            _mm256_storeu_pd(&output[(i - kernel_half) * output_size + (j - kernel_half)], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const int block_size = 4;\n    const int output_size = input_size - kernel_size + 1;\n    __m256d zero = _mm256_setzero_pd();\n    __m256d relu_mask = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < output_size; i += block_size) {\n        for (size_t j = 0; j < output_size; j += block_size) {\n            __m256d sum0 = zero, sum1 = zero, sum2 = zero, sum3 = zero;\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    double k = kernel[m * kernel_size + n];\n                    __m256d kern = _mm256_set1_pd(k);\n                    for (int l = 0; l < block_size; ++l) {\n                        double val = input[(i + m) * input_size + j + n + l * output_size];\n                        __m256d in_val = _mm256_set1_pd(val);\n                        sum0 = _mm256_fmadd_pd(in_val, kern, sum0);\n                        sum1 = _mm256_fmadd_pd(_mm256_permute2f128_pd(in_val, in_val, 1), kern, sum1);\n                        sum2 = _mm256_fmadd_pd(_mm256_permute2f128_pd(in_val, in_val, 2), kern, sum2);\n                        sum3 = _mm256_fmadd_pd(_mm256_permute2f128_pd(in_val, in_val, 3), kern, sum3);\n                    }\n                }\n            }\n            __m256d out0 = _mm256_max_pd(sum0, zero);\n            __m256d out1 = _mm256_max_pd(sum1, zero);\n            __m256d out2 = _mm256_max_pd(sum2, zero);\n            __m256d out3 = _mm256_max_pd(sum3, zero);\n            _mm256_storeu_pd(output + i * output_size + j, out0);\n            _mm256_storeu_pd(output + i * output_size + j + output_size, out1);\n            _mm256_storeu_pd(output + (i + output_size) * output_size + j, out2);\n            _mm256_storeu_pd(output + (i + output_size) * output_size + j + output_size, out3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const int block_size = 4;\n    const int kernel_half = kernel_size / 2;\n    const int output_size = input_size - kernel_size + 1;\n\n    for (size_t i = kernel_half; i < input_size - kernel_half; ++i) {\n        for (size_t j = kernel_half; j < input_size - kernel_half; ++j) {\n            __m256d sum = _mm256_setzero_pd();\n            for (int m = -kernel_half; m <= kernel_half; ++m) {\n                for (int n = -kernel_half; n <= kernel_half; ++n) {\n                    double in_val = input[(i + m) * input_size + (j + n)];\n                    double ker_val = kernel[(m + kernel_half) * kernel_size + (n + kernel_half)];\n                    __m256d in_vec = _mm256_broadcast_sd(&in_val);\n                    __m256d ker_vec = _mm256_broadcast_sd(&ker_val);\n                    sum = _mm256_fmadd_pd(in_vec, ker_vec, sum);\n                }\n            }\n            __m256d relu_mask = _mm256_cmp_pd(sum, _mm256_setzero_pd(), _CMP_LT_OS);\n            sum = _mm256_sub_pd(_mm256_setzero_pd(), _mm256_andnot_pd(relu_mask, sum));\n            _mm256_storeu_pd(output + (i - kernel_half) * output_size + (j - kernel_half), sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const size_t output_size = input_size - kernel_size + 1;\n    __m256d zero = _mm256_setzero_pd();\n    __m256d relu_mask = _mm256_set1_pd(0.0);\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m256d acc = zero;\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    double input_val = input[(i + m) * input_size + (j + n)];\n                    double kernel_val = kernel[m * kernel_size + n];\n                    __m256d input_vec = _mm256_set1_pd(input_val);\n                    __m256d kernel_vec = _mm256_set1_pd(kernel_val);\n                    acc = _mm256_fmadd_pd(input_vec, kernel_vec, acc);\n                }\n            }\n            __m256d result = _mm256_max_pd(acc, zero); // Apply ReLU\n            _mm256_storeu_pd(output + i * output_size + j, result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    const int vec_size = 4; // AVX uses 4 doubles per register\n    const int size = numbers.size();\n    \n    if (size < 2) return false;\n\n    __m256d thr = _mm256_set1_pd(threshold);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i + vec_size <= size; i += vec_size) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(v, _mm256_permute2f128_pd(v, v, 1));\n        diff = _mm256_andnot_pd(_mm256_set_epi32(0, -1, 0, -1, 0, -1, 0, -1), diff);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        __m256d mask = _mm256_cmp_pd(abs_diff, thr, _CMP_LT_OS);\n        if (_mm256_movemask_pd(mask) != 0) {\n            return true;\n        }\n    }\n\n    for (size_t i = size - (size % vec_size); i < size - 1; ++i) {\n        for (size_t j = i + 1; j < size; ++j) {\n            if (std::fabs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vec_size = 4; // AVX: 4 doubles per register\n    const int size = numbers.size();\n    \n    if(size < 2)\n        return false;\n    \n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    \n    for(int i = 0; i <= size - vec_size; i += vec_size){\n        __m256d v1 = _mm256_loadu_pd(&numbers[i]);\n        __m256d v2 = _mm256_permute2f128_pd(v1, v1, 1); // Load next 4 elements\n        \n        __m256d diff = _mm256_sub_pd(v1, v2);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        \n        __m256d mask = _mm256_cmpgt_pd(abs_diff, thresh_vec);\n        if(!_mm256_movemask_pd(mask))\n            return true;\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> &numbers, double threshold) {\n    const int vec_size = 4; // AVX uses 4 double precision elements\n    const int size = numbers.size();\n    \n    if (size < 2) return false;\n\n    __m256d thr = _mm256_set1_pd(threshold);\n    __m256d zero = _mm256_setzero_pd();\n\n    for (size_t i = 0; i + vec_size <= size; i += vec_size) {\n        __m256d v = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(v, _mm256_permute2f128_pd(v, v, 1));\n        diff = _mm256_andnot_pd(_mm256_set_epi64x(0x8000000000000000), diff); // Absolute values\n        __m256d mask = _mm256_cmp_pd(diff, thr, _CMP_LT_OS);\n        if (_mm256_movemask_pd(mask) != 0) {\n            return true;\n        }\n    }\n\n    for (size_t i = size - (size % vec_size); i < size; ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            if (std::fabs(numbers[i] - numbers[j]) < threshold) {\n                return true;\n            }\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vec_size = 4; // AVX: 4 doubles per register\n    const int size = numbers.size();\n    \n    if(size < 2) return false;\n\n    __m256d thresh_vec = _mm256_set1_pd(threshold);\n    __m256d min_diff = _mm256_set1_pd(-threshold);\n\n    for(int i=0; i<size-1; i+=vec_size){\n        __m256d v1 = _mm256_loadu_pd(&numbers[i]);\n        __m256d v2 = _mm256_permute2f128_pd(v1, v1, 1); // Load next vector\n\n        __m256d diff = _mm256_sub_pd(v1, v2);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n\n        __m256d mask = _mm256_cmpgt_pd(abs_diff, min_diff);\n        if(!_mm256_movemask_pd(mask)) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vec_size = 4; // AVX uses 4 double precision elements\n    const __m256d thresh_vec = _mm256_set1_pd(threshold);\n    \n    if(numbers.size() < vec_size + 1) return false;\n    \n    for(size_t i = 0; i <= numbers.size() - vec_size; i += vec_size){\n        __m256d nums = _mm256_loadu_pd(&numbers[i]);\n        __m256d diffs = _mm256_sub_pd(nums, _mm256_permute2f128_pd(nums, nums, 1));\n        __m256d abs_diffs = _mm256_andnot_pd(_mm256_set1_pd(-0.0), diffs);\n        \n        __m256d mask = _mm256_cmp_pd(abs_diffs, thresh_vec, _CMP_LT_OS);\n        if(_mm256_movemask_pd(mask) != 0){\n            return true;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int n = operations.size();\n    if (n == 0) return false;\n\n    int i = 0;\n    __m256i current_balance = _mm256_setzero_si256();\n\n    // Process 8 elements per iteration\n    for (; i <= n - 8; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        current_balance = _mm256_add_epi32(current_balance, ops);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), current_balance);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        current_balance.m256i_i32[0] += operations[i];\n        if (current_balance.m256i_i32[0] < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int n = operations.size();\n    if (n == 0) return false;\n\n    int i = 0;\n    __m256i current_balance = _mm256_setzero_si256();\n\n    // Process 8 elements per iteration\n    for (; i <= n - 8; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        current_balance = _mm256_add_epi32(current_balance, ops);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), current_balance);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        current_balance = _mm256_extract_epi32(current_balance, 0) + operations[i];\n        if (current_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int n = operations.size();\n    if (n == 0) return false;\n\n    int i = 0;\n    __m256i current_balance = _mm256_setzero_si256();\n\n    // Process 8 elements per iteration\n    for (; i <= n - 8; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        current_balance = _mm256_add_epi32(current_balance, ops);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), current_balance);\n        if (_mm256_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        current_balance.m256i_i32[0] += operations[i];\n        if (current_balance.m256i_i32[0] < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int n = operations.size();\n    if (n == 0) return false;\n\n    __m256i current_balance = _mm256_setzero_si256();\n    __m256i threshold = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        current_balance = _mm256_add_epi32(current_balance, ops);\n        __m256i below_zero = _mm256_cmpgt_epi32(threshold, current_balance);\n        if (_mm256_movemask_epi8(below_zero) != 0) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        current_balance = _mm256_extract_epi32(current_balance, 0) + operations[i];\n        if (current_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int n = operations.size();\n    if (n == 0) return false;\n\n    int i = 0;\n    __m256i current_balance = _mm256_setzero_si256();\n\n    // Process 8 elements per iteration\n    for (; i <= n - 8; i += 8) {\n        __m256i ops = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        current_balance = _mm256_add_epi32(current_balance, ops);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), current_balance);\n        if (_mm256_movemask_epi8(mask)) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        current_balance = _mm256_extract_epi32(current_balance, 0) + operations[i];\n        if (current_balance < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int n = numbers.size();\n    if(n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d count_vec = _mm256_setzero_pd();\n    __m256d mean_vec = _mm256_setzero_pd();\n\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n        count_vec = _mm256_add_pd(count_vec, _mm256_set1_pd(1.0));\n    }\n\n    __m256d total_sum = _mm256_hadd_pd(sum_vec, sum_vec);\n    total_sum = _mm256_hadd_pd(total_sum, total_sum);\n    double total_sum_scalar = _mm_cvtsd_f64(total_sum);\n\n    __m256d total_count = _mm256_hadd_pd(count_vec, count_vec);\n    total_count = _mm256_hadd_pd(total_count, total_count);\n    double total_count_scalar = _mm_cvtsd_f64(total_count);\n\n    double mean = total_sum_scalar / total_count_scalar;\n\n    __m256d deviations = _mm256_setzero_pd();\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d deviation = _mm256_sub_pd(data, _mm256_set1_pd(mean));\n        deviations = _mm256_add_pd(deviations, _mm256_abs_pd(deviation));\n    }\n\n    __m256d mad_sum = _mm256_hadd_pd(deviations, deviations);\n    mad_sum = _mm256_hadd_pd(mad_sum, mad_sum);\n    double mad_sum_scalar = _mm_cvtsd_f64(mad_sum);\n\n    double mad = mad_sum_scalar / total_count_scalar;\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int n = numbers.size();\n    if(n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d count_vec = _mm256_setzero_pd();\n    __m256d mean_vec = _mm256_setzero_pd();\n\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n        count_vec = _mm256_add_pd(count_vec, _mm256_set1_pd(1.0));\n    }\n\n    mean_vec = _mm256_div_pd(sum_vec, count_vec);\n\n    __m256d mad_sum = _mm256_setzero_pd();\n    __m256d count_mad = _mm256_setzero_pd();\n\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        mad_sum = _mm256_add_pd(mad_sum, abs_diff);\n        count_mad = _mm256_add_pd(count_mad, _mm256_set1_pd(1.0));\n    }\n\n    __m256d mad_avg = _mm256_div_pd(mad_sum, count_mad);\n    double result[4];\n    _mm256_storeu_pd(result, mad_avg);\n\n    double total_mad = 0.0;\n    for(int i=0; i<4; ++i){\n        total_mad += result[i];\n    }\n\n    return total_mad / ((n + 3) / 4);\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int n = numbers.size();\n    if(n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d count_vec = _mm256_setzero_pd();\n    __m256d mean_vec = _mm256_setzero_pd();\n\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n        count_vec = _mm256_add_pd(count_vec, _mm256_set1_pd(1.0));\n    }\n\n    __m256d total_sum = _mm256_hadd_pd(sum_vec, sum_vec);\n    total_sum = _mm256_hadd_pd(total_sum, total_sum);\n    double total_sum_scalar = _mm_cvtsd_f64(total_sum);\n\n    __m256d total_count = _mm256_hadd_pd(count_vec, count_vec);\n    total_count = _mm256_hadd_pd(total_count, total_count);\n    double total_count_scalar = _mm_cvtsd_f64(total_count);\n\n    double mean = total_sum_scalar / total_count_scalar;\n\n    __m256d deviations = _mm256_setzero_pd();\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d deviation = _mm256_sub_pd(data, _mm256_set1_pd(mean));\n        deviations = _mm256_add_pd(deviations, _mm256_abs_pd(deviation));\n    }\n\n    __m256d mad_sum = _mm256_hadd_pd(deviations, deviations);\n    mad_sum = _mm256_hadd_pd(mad_sum, mad_sum);\n    double mad_sum_scalar = _mm_cvtsd_f64(mad_sum);\n\n    double mad = mad_sum_scalar / total_count_scalar;\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int n = numbers.size();\n    if(n == 0) return 0.0;\n\n    __m256d sum_vec = _mm256_setzero_pd();\n    __m256d count_vec = _mm256_setzero_pd();\n\n    int i = 0;\n    for(; i + 4 <= n; i += 4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum_vec = _mm256_add_pd(sum_vec, data);\n        count_vec = _mm256_add_pd(count_vec, _mm256_set1_pd(1.0));\n    }\n\n    __m256d total_sum = _mm256_hadd_pd(sum_vec, sum_vec);\n    total_sum = _mm256_permute2f128_pd(total_sum, total_sum, 1);\n    total_sum = _mm256_add_pd(total_sum, sum_vec);\n\n    double total_sum_arr[4];\n    _mm256_storeu_pd(total_sum_arr, total_sum);\n    double total_sum_val = total_sum_arr[0] + total_sum_arr[1];\n\n    __m256d total_count = _mm256_hadd_pd(count_vec, count_vec);\n    total_count = _mm256_permute2f128_pd(total_count, total_count, 1);\n    total_count = _mm256_add_pd(total_count, count_vec);\n\n    double total_count_arr[4];\n    _mm256_storeu_pd(total_count_arr, total_count);\n    double mean = total_sum_val / (total_count_arr[0] + total_count_arr[1]);\n\n    __m256d deviations = _mm256_setzero_pd();\n    for(int j = 0; j + 4 <= n; j += 4){\n        __m256d data = _mm256_loadu_pd(&numbers[j]);\n        __m256d abs_dev = _mm256_abs_pd(_mm256_sub_pd(data, _mm256_set1_pd(mean)));\n        deviations = _mm256_add_pd(deviations, abs_dev);\n    }\n\n    __m256d mad_sum = _mm256_hadd_pd(deviations, deviations);\n    mad_sum = _mm256_permute2f128_pd(mad_sum, mad_sum, 1);\n    mad_sum = _mm256_add_pd(mad_sum, deviations);\n\n    double mad_sum_arr[4];\n    _mm256_storeu_pd(mad_sum_arr, mad_sum);\n    double mad_val = (mad_sum_arr[0] + mad_sum_arr[1]) / n;\n\n    return mad_val;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```cpp\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    __m256d sum = _mm256_setzero_pd();\n    __m256d count = _mm256_setzero_pd();\n    __m256d mean_vec = _mm256_setzero_pd();\n\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        sum = _mm256_add_pd(sum, data);\n        count = _mm256_add_pd(count, _mm256_set1_pd(1.0));\n    }\n\n    mean_vec = _mm256_div_pd(sum, count);\n\n    __m256d mad_sum = _mm256_setzero_pd();\n    for(int i=0; i<n; i+=4){\n        __m256d data = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(data, mean_vec);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        mad_sum = _mm256_add_pd(mad_sum, abs_diff);\n    }\n\n    double mad_result[4];\n    _mm256_storeu_pd(mad_result, mad_sum);\n    double total_mad = 0.0;\n    for(int i=0; i<4; ++i){\n        total_mad += mad_result[i];\n    }\n    return total_mad / n;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    int n = numbers.size();\n    std::vector<int> result;\n    result.reserve(2 * n -1);\n    \n    const __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    \n    int i = 0;\n    for (; i + 8 <= n; i += 8){\n        __m256i nums_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i interleaved = _mm256_insert_epi32(nums_vec, delimeter, 1);\n        interleaved = _mm256_insert_epi32(interleaved, delimeter, 3);\n        interleaved = _mm256_insert_epi32(interleaved, delimeter, 5);\n        interleaved = _mm256_insert_epi32(interleaved, delimeter, 7);\n        \n        _mm256_storeu_si256((__m256i*)&result[2*i], interleaved);\n    }\n    \n    for (; i < n; ++i){\n        result.push_back(numbers[i]);\n        if (i != n-1){\n            result.push_back(delimeter);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    size_t n = numbers.size();\n    std::vector<int> result(n * 2 - 1);\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    \n    size_t i = 0;\n    for (; i <= n - 8; i += 8){\n        __m256i nums = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i nums_interleaved = _mm256_insert_epi32(nums, _mm256_extract_epi32(delim_vec, 0), 1);\n        nums_interleaved = _mm256_insert_epi32(nums_interleaved, _mm256_extract_epi32(delim_vec, 0), 3);\n        nums_interleaved = _mm256_insert_epi32(nums_interleaved, _mm256_extract_epi32(delim_vec, 0), 5);\n        nums_interleaved = _mm256_insert_epi32(nums_interleaved, _mm256_extract_epi32(delim_vec, 0), 7);\n        _mm256_storeu_si256((__m256i*)&result[i * 2], nums_interleaved);\n    }\n    \n    for (; i < n; ++i){\n        result[i * 2] = numbers[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    int n = numbers.size();\n    std::vector<int> result(n * 2 - 1);\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    \n    int i = 0;\n    for (; i <= n - 4; i += 4){\n        __m256i nums = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i nums_with_delim = _mm256_insert_epi32(nums, _mm256_extract_epi32(delim_vec, 0), 1);\n        nums_with_delim = _mm256_insert_epi32(nums_with_delim, _mm256_extract_epi32(delim_vec, 0), 3);\n        nums_with_delim = _mm256_insert_epi32(nums_with_delim, _mm256_extract_epi32(delim_vec, 0), 5);\n        nums_with_delim = _mm256_insert_epi32(nums_with_delim, _mm256_extract_epi32(delim_vec, 0), 7);\n        _mm256_storeu_si256((__m256i*)&result[i * 2], nums_with_delim);\n    }\n    \n    for (; i < n; ++i){\n        result[i * 2] = numbers[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    size_t n = numbers.size();\n    std::vector<int> result(2*n -1, 0);\n    __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    \n    size_t i = 0;\n    for(; i <= n - 8; i += 8){\n        __m256i nums = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i nums_with_delim = _mm256_insert_epi32(nums, _mm256_extract_epi32(delim_vec, 0), 1);\n        nums_with_delim = _mm256_insert_epi32(nums_with_delim, _mm256_extract_epi32(delim_vec, 0), 3);\n        nums_with_delim = _mm256_insert_epi32(nums_with_delim, _mm256_extract_epi32(delim_vec, 0), 5);\n        nums_with_delim = _mm256_insert_epi32(nums_with_delim, _mm256_extract_epi32(delim_vec, 0), 7);\n        _mm256_storeu_si256((__m256i*)&result[2*i], nums_with_delim);\n    }\n    \n    for(; i < n; ++i){\n        result[2*i] = numbers[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    int n = numbers.size();\n    std::vector<int> result;\n    result.reserve(2 * n -1);\n    \n    const __m256i delim_vec = _mm256_set1_epi32(delimeter);\n    \n    int i = 0;\n    for(; i + 8 <= n; i += 8){\n        __m256i nums_vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i interleaved = _mm256_insert_epi32(nums_vec, delimeter, 1);\n        interleaved = _mm256_insert_epi32(interleaved, delimeter, 3);\n        interleaved = _mm256_insert_epi32(interleaved, delimeter, 5);\n        interleaved = _mm256_insert_epi32(interleaved, delimeter, 7);\n        \n        _mm256_storeu_si256((__m256i*)&result[i*2], interleaved);\n    }\n    \n    for(int j = i; j < n; ++j){\n        result.push_back(numbers[j]);\n        if(j != n-1){\n            result.push_back(delimeter);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // AVX can process 4 int16 values at once\n    int size = numbers.size();\n    std::vector<int64_t> result(2, 0); // [sum, product]\n\n    if(size == 0){\n        return result;\n    }\n\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i prod_vec = _mm_set1_epi16(1);\n\n    for(int i = 0; i < size; i += vec_size){\n        __m128i vec = _mm_loadu_si128((__m128i*)&numbers[i]);\n        sum_vec = _mm_add_epi16(sum_vec, vec);\n        prod_vec = _mm_mullo_epi16(prod_vec, vec);\n    }\n\n    __m128i sum_acc = _mm_hadd_epi16(sum_vec, sum_vec);\n    __m128i prod_acc = _mm_hadd_epi16(prod_vec, prod_vec);\n\n    result[0] = _mm_cvtsi128_si32(sum_acc);\n    result[1] = _mm_cvtsi128_si32(prod_acc);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // AVX2 supports 8 x int16_t\n    int size = numbers.size();\n    std::vector<int64_t> result(2, 0);\n\n    if(size == 0){\n        return result;\n    }\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n\n    for(int i = 0; i < size; i += vec_size){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        sum_vec = _mm256_add_epi16(sum_vec, vec);\n        prod_vec = _mm256_mullo_epi16(prod_vec, vec);\n    }\n\n    __m128i low_sum = _mm256_castsi256_si128(sum_vec);\n    __m128i high_sum = _mm256_extracti128_si256(sum_vec, 1);\n    __m128i low_prod = _mm256_castsi256_si128(prod_vec);\n    __m128i high_prod = _mm256_extracti128_si256(prod_vec, 1);\n\n    low_sum = _mm_add_epi16(low_sum, high_sum);\n    low_prod = _mm_add_epi16(low_prod, high_prod);\n\n    __m128i sum_result = _mm_hadd_epi16(low_sum, low_sum);\n    __m128i prod_result = _mm_hadd_epi16(low_prod, low_prod);\n\n    result[0] = _mm_cvtsi128_si32(sum_result);\n    result[1] = _mm_cvtsi128_si32(prod_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // AVX2 supports 8 x int16_t\n    std::vector<int64_t> result(2, 0);\n    \n    if(numbers.empty()){\n        return result;\n    }\n\n    size_t i = 0;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n\n    for(; i + vec_size <= numbers.size(); i += vec_size){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        \n        __m256i vec_sum = _mm256_add_epi16(vec, sum_vec);\n        sum_vec = _mm256_add_epi16(vec_sum, sum_vec);\n        \n        __m256i vec_prod = _mm256_mullo_epi16(vec, prod_vec);\n        prod_vec = _mm256_mullo_epi16(vec_prod, prod_vec);\n    }\n\n    // Horizontal sum and product\n    __m128i low_sum = _mm256_castsi256_si128(sum_vec);\n    __m128i high_sum = _mm256_extractf128_si256(sum_vec, 1);\n    low_sum = _mm_add_epi16(low_sum, high_sum);\n    low_sum = _mm_hadd_epi16(low_sum, low_sum);\n    low_sum = _mm_hadd_epi16(low_sum, low_sum);\n    int64_t total_sum = _mm_cvtsi128_si64(low_sum);\n\n    __m128i low_prod = _mm256_castsi256_si128(prod_vec);\n    __m128i high_prod = _mm256_extractf128_si256(prod_vec, 1);\n    low_prod = _mm_mullo_epi16(low_prod, high_prod);\n    low_prod = _mm_mullo_epi16(low_prod, low_prod);\n    low_prod = _mm_mullo_epi16(low_prod, low_prod);\n    int64_t total_prod = _mm_cvtsi128_si64(low_prod);\n\n    result[0] = total_sum;\n    result[1] = total_prod;\n\n    // Handle remaining elements\n    for(; i < numbers.size(); ++i){\n        result[0] += numbers[i];\n        result[1] *= numbers[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // AVX: 8 x int16_t\n    std::vector<int64_t> result(2, 0);\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n\n    if(numbers.empty()) {\n        return result;\n    }\n\n    int i = 0;\n    for(; i + vec_size <= numbers.size(); i += vec_size){\n        __m256i nums = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i prod_mask = _mm256_set_epi16(1, 1, 1, 1, 1, 1, 1, 1);\n        \n        sum_vec = _mm256_add_epi16(sum_vec, nums);\n        prod_vec = _mm256_mullo_epi16(prod_vec, nums);\n    }\n\n    // Horizontal sum and product\n    __m128i low_sum = _mm256_castsi256_si128(sum_vec);\n    __m128i high_sum = _mm256_extracti128_si256(sum_vec, 1);\n    low_sum = _mm_add_epi16(low_sum, high_sum);\n    low_sum = _mm_hadd_epi16(low_sum, low_sum);\n    low_sum = _mm_hadd_epi16(low_sum, low_sum);\n    int64_t total_sum = _mm_cvtsi128_si64(low_sum);\n\n    __m128i low_prod = _mm256_castsi256_si128(prod_vec);\n    __m128i high_prod = _mm256_extracti128_si256(prod_vec, 1);\n    low_prod = _mm_mullo_epi16(low_prod, high_prod);\n    low_prod = _mm_mulhi_epi16(low_prod, low_prod);\n    low_prod = _mm_mulhi_epi16(low_prod, low_prod);\n    int64_t total_prod = _mm_cvtsi128_si64(low_prod);\n\n    result[0] = total_sum;\n    result[1] = total_prod;\n\n    // Handle remaining elements\n    for(; i < numbers.size(); ++i){\n        result[0] += numbers[i];\n        result[1] *= numbers[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // AVX2 supports 8 x int16_t\n    int size = numbers.size();\n    std::vector<int64_t> result(2, 0);\n    \n    if(size == 0){\n        return result;\n    }\n    \n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i prod_vec = _mm256_set1_epi16(1);\n    \n    for(int i = 0; i < size; i += vec_size){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        sum_vec = _mm256_add_epi16(sum_vec, vec);\n        prod_vec = _mm256_mullo_epi16(prod_vec, vec);\n    }\n    \n    __m128i low_sum = _mm256_castsi256_si128(sum_vec);\n    __m128i high_sum = _mm256_extractf128_si256(sum_vec, 1);\n    __m128i sum_result = _mm_add_epi16(low_sum, high_sum);\n    \n    __m128i low_prod = _mm256_castsi256_si128(prod_vec);\n    __m128i high_prod = _mm256_extractf128_si256(prod_vec, 1);\n    __m128i prod_result = _mm_add_epi16(low_prod, high_prod);\n    \n    int64_t sum_acc = _mm_cvtsi128_si64(sum_result);\n    int64_t prod_acc = _mm_cvtsi128_si64(prod_result);\n    \n    result[0] = sum_acc;\n    result[1] = prod_acc;\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    const __m256i zero = _mm256_setzero_si256();\n    const int vec_size = 8; // AVX size: 8 integers per register\n\n    for(int i=0; i<n; i+=vec_size){\n        __m256i current_max = zero;\n        __m256i input_vec = zero;\n\n        for(int j=0; j<vec_size && (i+j)<n; j++){\n            input_vec = _mm256_insert_epi32(input_vec, numbers[i+j], j);\n        }\n\n        current_max = _mm256_max_epi32(current_max, input_vec);\n\n        for(int j=0; j<vec_size && (i+j)<n; j++){\n            result[i+j] = _mm256_extract_epi32(current_max, j);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    const __m256i zero = _mm256_setzero_si256();\n    const int vec_size = 8; // AVX size: 8 integers per register\n\n    for(int i = 0; i < n; i += vec_size){\n        int limit = (i + vec_size <= n) ? i + vec_size : n;\n        __m256i current_max = zero;\n        for(int j = i; j < limit; j += vec_size){\n            __m256i nums = _mm256_loadu_si256((__m256i*)&numbers[j]);\n            __m256i max_mask = _mm256_cmpgt_epi32(nums, current_max);\n            current_max = _mm256_or_si256(_mm256_andnot_si256(max_mask, current_max), \n                                          _mm256_and_si256(max_mask, nums));\n        }\n        for(int j = i; j < limit; ++j){\n            result[j] = current_max.m256i_i32[0];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    const __m256i zero = _mm256_setzero_si256();\n    const int vec_size = 8; // AVX size: 8 integers per register\n\n    for(int i=0; i<n; i+=vec_size){\n        int limit = std::min(i + vec_size, n);\n        __m256i current_max = zero;\n        for(int j=i; j<limit; j++){\n            __m256i value = _mm256_set1_epi32(numbers[j]);\n            current_max = _mm256_max_epi32(current_max, value);\n            result[j] = _mm256_extract_epi32(current_max, j-i);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i max_mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for(int i = 0; i + 8 <= n; i += 8){\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i prev_max = _mm256_loadu_si256((__m256i*)&result[i]);\n\n        // Compute max for each lane\n        __m256i max_values = _mm256_max_epi32(current, prev_max);\n\n        // Store the result back\n        _mm256_storeu_si256((__m256i*)&result[i], max_values);\n    }\n\n    // Handle remaining elements\n    for(int i = n - (n % 8); i < n; ++i){\n        result[i] = std::max(numbers[i], result[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i max_mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for(int i = 0; i + 8 <= n; i += 8){\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i prev_max = _mm256_loadu_si256((__m256i*)&result[i]);\n\n        // Compute max for each lane\n        __m256i max_values = _mm256_max_epi32(current, prev_max);\n\n        // Store the result back\n        _mm256_storeu_si256((__m256i*)&result[i], max_values);\n    }\n\n    // Handle remaining elements\n    for(int i = n - (n % 8); i < n; ++i){\n        result[i] = std::max(numbers[i], result[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <iostream>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        return \"\"; // Handle error: strings must be of equal length\n    }\n\n    size_t i = 0;\n    __m256i result = _mm256_setzero_si256();\n\n    // Process in 32-character chunks (256 bits)\n    for (; i <= len - 32; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        __m256i xor_result = _mm256_xor_epi8(vec_a, vec_b);\n        result = _mm256_or_si256(result, xor_result);\n    }\n\n    // Handle remaining characters less than 32 bytes\n    std::string temp;\n    temp.resize(32);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(temp.data()), result);\n\n    std::string final_result;\n    for (size_t j = 0; j < temp.size(); ++j) {\n        if (i + j < len && temp[j] == 1) {\n            final_result += '1';\n        } else if (i + j < len) {\n            final_result += '0';\n        }\n    }\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    if(b.size() != len) {\n        return \"\";\n    }\n    \n    size_t i = 0;\n    __m256i result = _mm256_setzero_si256();\n\n    // Process in 32-character chunks\n    for (; i <= len - 32; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        __m256i xor_vec = _mm256_xor_epi8(vec_a, vec_b);\n        result = _mm256_or_si256(result, xor_vec);\n    }\n\n    // Handle remaining characters\n    uint8_t remainder[32] = {0};\n    if(i < len){\n        memset(remainder, '0', sizeof(remainder));\n        memcpy(remainder, a.data() + i, len - i);\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(remainder));\n        memset(remainder, '0', sizeof(remainder));\n        memcpy(remainder, b.data() + i, len - i);\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(remainder));\n        __m256i xor_vec = _mm256_xor_epi8(vec_a, vec_b);\n        result = _mm256_or_si256(result, xor_vec);\n    }\n\n    // Store the result back into a string\n    uint8_t output[32] = {0};\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(output), result);\n\n    std::string res(output, 32);\n    res.erase(res.find_last_not_of('0') + 1, std::string::npos); // Remove leading zeros\n    return res;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must have equal length\");\n    }\n\n    size_t i = 0;\n    __m256i zero = _mm256_setzero_si256();\n    std::vector<char> result(len + 1, '\\0');\n\n    // Process in 32-bit chunks\n    for (; i <= len - 32; i += 32) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(a.data() + i));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(b.data() + i));\n        __m256i xor_result = _mm256_xor_epi8(vec_a, vec_b);\n\n        // Convert to char array and store\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), xor_result);\n    }\n\n    // Handle remaining bytes\n    for (; i < len; ++i) {\n        result[i] = (a[i] ^ b[i]);\n    }\n\n    return std::string(result.data(), len);\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (b.size() != len) {\n        throw std::invalid_argument(\"Strings must have equal length\");\n    }\n\n    std::string result(len, '0');\n    size_t i = 0;\n\n    while (i + 32 <= len) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n        __m256i xor_vec = _mm256_xor_si256(vec_a, vec_b);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), xor_vec);\n        i += 32;\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    size_t len = a.size();\n    if (b.size() != len) {\n        throw std::invalid_argument(\"Strings must be of equal length\");\n    }\n\n    std::string result(len, '0');\n    size_t i = 0;\n\n    while (i + 32 <= len) {\n        __m256i vec_a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&a[i]));\n        __m256i vec_b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&b[i]));\n        __m256i xor_vec = _mm256_xor_si256(vec_a, vec_b);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), xor_vec);\n        i += 32;\n    }\n\n    for (; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n    \n    __m256i max_len = _mm256_setzero_si256();\n    __m256i max_idx = _mm256_setzero_si256();\n    int idx = 0;\n    \n    for(int i=0; i<strings.size(); i+=8){\n        __m256i len_vec = _mm256_setzero_si256();\n        for(int j=0; j<8 && i+j < strings.size(); ++j){\n            const std::string& s = strings[i+j];\n            len_vec = _mm256_insert_epi32(len_vec, static_cast<int>(s.length()), j);\n        }\n        \n        __m256i cmp_mask = _mm256_cmpgt_epi32(len_vec, max_len);\n        max_len = _mm256_max_epi32(len_vec, max_len);\n        max_idx = _mm256_blendv_epi8(max_idx, _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i), cmp_mask);\n    }\n    \n    int final_max_len = 0;\n    int final_max_idx = 0;\n    _mm256_storeu_si256((__m256i*)&final_max_len, max_len);\n    _mm256_storeu_si256((__m256i*)&final_max_idx, max_idx);\n    \n    for(int i=0; i<strings.size(); ++i){\n        if(strings[i].length() == final_max_len){\n            return strings[i];\n        }\n    }\n    \n    return \"\";\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n\n    __m256i max_len = _mm256_setzero_si256();\n    __m256i max_idx = _mm256_setzero_si256();\n    int max_len_scalar = 0;\n    int max_idx_scalar = 0;\n\n    for(int i=0; i<strings.size(); i+=8){\n        __m256i lengths = _mm256_setzero_si256();\n        for(int j=0; j<8 && i+j < strings.size(); ++j){\n            const std::string& s = strings[i+j];\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(s.length()), j);\n        }\n\n        __m256i gt_mask = _mm256_cmpgt_epi32(lengths, max_len);\n        max_len = _mm256_max_epi32(lengths, max_len);\n        max_idx = _mm256_blendv_epi8(max_idx, _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i), gt_mask);\n\n        // Handle remaining elements\n        for(int j=0; j<8 && i+j < strings.size(); ++j){\n            if(static_cast<int>(lengths.m256i_i32[j]) > max_len_scalar){\n                max_len_scalar = lengths.m256i_i32[j];\n                max_idx_scalar = i+j;\n            }\n        }\n    }\n\n    return strings[max_idx_scalar];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if (strings.empty()) return \"\";\n\n    __m256i max_len = _mm256_setzero_si256();\n    __m256i max_idx = _mm256_setzero_si256();\n    int max_len_scalar = 0;\n    size_t idx = 0;\n\n    for (size_t i = 0; i < strings.size(); i += 8) {\n        __m256i lengths = _mm256_setzero_si256();\n        for (int j = 0; j < 8 && (i + j) < strings.size(); ++j) {\n            const std::string &s = strings[i + j];\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(s.length()), j);\n        }\n\n        __m256i gt_mask = _mm256_cmpgt_epi32(lengths, max_len);\n        max_len = _mm256_max_epi32(lengths, max_len);\n        max_idx = _mm256_blendv_epi8(max_idx, _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i), gt_mask);\n\n        // Handle remaining elements\n        for (int j = 0; j < 8 && (i + j) < strings.size(); ++j) {\n            if (static_cast<int>(strings[i + j].length()) > max_len_scalar) {\n                max_len_scalar = strings[i + j].length();\n                idx = i + j;\n            }\n        }\n    }\n\n    return strings[idx];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n    \n    __m256i max_len_vec = _mm256_setzero_si256();\n    __m256i max_idx_vec = _mm256_setzero_si256();\n    int max_len = 0;\n    int max_idx = 0;\n\n    for(size_t i = 0; i < strings.size(); i += 8){\n        __m256i len_vec = _mm256_setzero_si256();\n        __m256i idx_vec = _mm256_set_epi32(7+i, 6+i, 5+i, 4+i, 3+i, 2+i, 1+i, 0+i);\n\n        for(int j = 0; j < 8 && (i+j) < strings.size(); ++j){\n            int len = strings[i+j].length();\n            __m256i temp = _mm256_set1_epi32(len);\n            len_vec = _mm256_max_epu32(len_vec, temp);\n        }\n\n        __m256i cmp_mask = _mm256_cmpgt_epi32(len_vec, max_len_vec);\n        max_len_vec = _mm256_max_epu32(max_len_vec, len_vec);\n        max_idx_vec = _mm256_blendv_epi8(max_idx_vec, idx_vec, cmp_mask);\n    }\n\n    _mm256_storeu_si256((__m256i *)&max_idx, max_idx_vec);\n    max_len = _mm256_extract_epi32(max_len_vec, 0);\n    max_idx = _mm256_extract_epi32(max_idx_vec, 0);\n\n    for(int i = 0; i < strings.size(); ++i){\n        if(strings[i].length() == max_len){\n            return strings[i];\n        }\n    }\n\n    return \"\";\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n\n    __m256i max_len = _mm256_setzero_si256();\n    __m256i max_idx = _mm256_setzero_si256();\n    int max_len_scalar = 0;\n    int max_idx_scalar = 0;\n\n    for(int i=0; i<strings.size(); i+=8){\n        __m256i lengths = _mm256_setzero_si256();\n        for(int j=0; j<8 && i+j < strings.size(); ++j){\n            const std::string& s = strings[i+j];\n            lengths = _mm256_insert_epi32(lengths, static_cast<int>(s.length()), j);\n        }\n\n        __m256i mask = _mm256_cmpgt_epi32(lengths, max_len);\n        max_len = _mm256_max_epi32(lengths, max_len);\n        max_idx = _mm256_blendv_epi8(max_idx, _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i), mask);\n\n        // Handle remaining elements\n        for(int j=0; j<8 && i+j < strings.size(); ++j){\n            if(static_cast<int>(lengths[j]) > max_len_scalar){\n                max_len_scalar = lengths[j];\n                max_idx_scalar = i+j;\n            }\n        }\n    }\n\n    // Final comparison for remaining elements\n    for(int i=strings.size() - (strings.size() % 8); i<strings.size(); ++i){\n        if(strings[i].length() > max_len_scalar){\n            max_len_scalar = strings[i].length();\n            max_idx_scalar = i;\n        }\n    }\n\n    return strings[max_idx_scalar];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    // AVX2 implementation of gcd using vectorized operations is non-trivial due to\n    // the nature of the Euclidean algorithm requiring sequential dependency.\n    // However, for demonstration purposes, we can process multiple gcd pairs in parallel.\n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i b_vec = _mm256_set_epi64x(b, b, b, b);\n\n    while (!_mm256_testz_epi64(_mm256_and_si256(a_vec, b_vec), b_vec)) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i a_gt_b = _mm256_cmpgt_epi64(a_vec, b_vec);\n        __m256i a_lt_b = _mm256_cmpgt_epi64(b_vec, a_vec);\n\n        a_vec = _mm256_sub_epi64(a_vec, _mm256_and_si256(a_gt_b, b_vec));\n        b_vec = _mm256_sub_epi64(b_vec, _mm256_and_si256(a_lt_b, a_vec));\n    }\n\n    return _mm256_extract_epi64(a_vec, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    // AVX2 implementation of gcd using vectorized operations is non-trivial due to\n    // the nature of the Euclidean algorithm requiring sequential dependency.\n    // This function cannot be directly vectorized while preserving correctness.\n    // Hence, returning a placeholder value for demonstration purposes.\n    return __builtin_ia32_vpandq((__m128i)_mm_set_epi64x(a, b), (__m128i)_mm_set_epi64x(a, b)).m128i_i64[0];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    // AVX2 implementation of gcd using vectorized operations is non-trivial due to\n    // the nature of the Euclidean algorithm requiring sequential dependency.\n    // However, for demonstration purposes, we can process multiple gcd pairs in parallel.\n    \n    // Example: Process 4 gcd computations in parallel using AVX2\n    \n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i b_vec = _mm256_set_epi64x(b, b, b, b);\n    \n    while (!_mm256_testz_si256(a_vec, b_vec)) {\n        __m256i gt_mask = _mm256_cmpgt_epi64(b_vec, a_vec);\n        a_vec = _mm256_sub_epi64(_mm256_andnot_si256(gt_mask, b_vec), \n                                 _mm256_and_si256(gt_mask, a_vec));\n        b_vec = _mm256_sub_epi64(_mm256_and_si256(gt_mask, b_vec), \n                                 _mm256_andnot_si256(gt_mask, a_vec));\n    }\n    \n    uint64_t result[4];\n    _mm256_storeu_si256((__m256i*)result, a_vec);\n    \n    // Assuming all results are identical, return the first one\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    // AVX2 implementation of gcd using vectorized operations is non-trivial due to\n    // the sequential nature of the Euclidean algorithm. However, for demonstration,\n    // we can process multiple gcd pairs in parallel using vectors.\n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i b_vec = _mm256_set_epi64x(b, b, b, b);\n\n    while (!_mm256_testz_epi64(_mm256_and_si256(a_vec, b_vec), b_vec)) {\n        __m256i zero = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpgt_epi64(b_vec, zero);\n        __m256i temp = _mm256_blendv_epi8(zero, a_vec, mask);\n        a_vec = _mm256_blendv_epi8(a_vec, _mm256_sub_epi64(a_vec, b_vec), mask);\n        b_vec = _mm256_blendv_epi8(temp, b_vec, mask);\n    }\n\n    return _mm256_extract_epi64(a_vec, 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    // SIMD implementation is not directly applicable to GCD computation as it is inherently scalar.\n    // However, if you want to process multiple GCDs in parallel, you can use vectorized inputs.\n    // Here's an example of processing 4 GCDs in parallel using AVX2:\n    \n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i b_vec = _mm256_set_epi64x(b, b, b, b);\n    \n    while (!_mm256_testz_si256(a_vec, b_vec)) {\n        __m256i mask = _mm256_cmpgt_epi64(b_vec, a_vec);\n        __m256i temp = _mm256_andnot_si256(mask, _mm256_sub_epi64(b_vec, a_vec));\n        a_vec = _mm256_or_si256(_mm256_and_si256(mask, a_vec), temp);\n        temp = _mm256_andnot_si256(mask, _mm256_sub_epi64(a_vec, b_vec));\n        b_vec = _mm256_or_si256(_mm256_and_si256(mask, b_vec), temp);\n    }\n    \n    __m256i result = _mm256_andnot_si256(_mm256_cmpgt_epi64(a_vec, b_vec), a_vec);\n    result = _mm256_or_si256(result, _mm256_andnot_si256(_mm256_cmpgt_epi64(b_vec, a_vec), b_vec));\n    \n    uint64_t res0, res1, res2, res3;\n    _mm256_storeu_si256((__m256i*)&res0, result);\n    _mm256_storeu_si256((__m256i*)&res1, result + 1);\n    _mm256_storeu_si256((__m256i*)&res2, result + 2);\n    _mm256_storeu_si256((__m256i*)&res3, result + 3);\n    \n    return res0; // Assuming you only care about the first result\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vec_size = 4; // AVX size\n    int n = numbers.size();\n    __m256 min_diff = _mm256_set1_ps(__FLT_MAX__);\n    __m256 smallest_pair = _mm256_setzero_ps();\n\n    for(int i=0; i<n-1; i+=vec_size){\n        __m256 v1 = _mm256_loadu_ps(&numbers[i]);\n        __m256 v2 = _mm256_permute2f128_ps(v1, v1, 1); // Duplicate the vector\n\n        __m256 diff = _mm256_sub_ps(v1, v2);\n        __m256 abs_diff = _mm256_abs_ps(diff);\n\n        __m256 mask = _mm256_cmp_ps(abs_diff, min_diff, _CMP_LT_OQ);\n        min_diff = _mm256_min_ps(min_diff, abs_diff);\n        smallest_pair = _mm256_blendv_ps(smallest_pair, _mm256_add_ps(v1, v2), mask);\n    }\n\n    float result[4];\n    _mm256_storeu_ps(result, smallest_pair);\n    std::vector<float> closest_elements;\n    for(int i=0;i<4;i++) {\n        if(result[i] != 0.0f){\n            closest_elements.push_back(result[i]);\n        }\n    }\n    return closest_elements;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vec_size = 4; // AVX size\n    int n = numbers.size();\n    if(n < 2) return {};\n\n    std::vector<float> min_pair;\n    float min_diff = std::numeric_limits<float>::max();\n\n    for(int i=0; i<n-1; i+=vec_size){\n        __m256 v_numbers = _mm256_loadu_ps(&numbers[i]);\n        for(int j=i+1; j<n; j+=vec_size){\n            __m256 v_other = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_sub_ps(v_numbers, v_other);\n            __m256 abs_diff = _mm256_abs_ps(diff);\n\n            int mask = _mm256_movemask_ps(abs_diff);\n            if(mask != 0){\n                for(int k=0; k<vec_size; ++k){\n                    if((mask & (1 << k)) != 0){\n                        float a = v_numbers[k];\n                        float b = v_other[k];\n                        float current_diff = std::abs(a - b);\n                        if(current_diff < min_diff){\n                            min_diff = current_diff;\n                            if(a < b){\n                                min_pair = {a, b};\n                            }\n                            else{\n                                min_pair = {b, a};\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return min_pair;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vec_size = 4; // AVX size\n    int n = numbers.size();\n    __m256 min_diff = _mm256_set1_ps(__FLT_MAX__);\n    __m256 smallest_pair = _mm256_setzero_ps();\n\n    for(int i=0; i<n-1; i+=vec_size){\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n        for(int j=i+1; j<n-1; j+=vec_size){\n            __m256 w = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_abs_ps(_mm256_sub_ps(v, w));\n            __m256 mask = _mm256_cmp_ps(diff, min_diff, _CMP_LT_OQ);\n            min_diff = _mm256_min_ps(min_diff, diff);\n            smallest_pair = _mm256_blendv_ps(smallest_pair, _mm256_add_ps(v, w), mask);\n        }\n    }\n\n    float result[4];\n    _mm256_storeu_ps(result, smallest_pair);\n    std::vector<float> closest(result, result + 2);\n    return closest;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vec_size = 4; // AVX size\n    int n = numbers.size();\n    __m256 min_diff = _mm256_set1_ps(__FLT_MAX__);\n    __m256 smallest_pair = _mm256_setzero_ps();\n\n    for(int i=0; i<n-1; i+=vec_size){\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n        for(int j=i+1; j<n; j+=vec_size){\n            __m256 w = _mm256_loadu_ps(&numbers[j]);\n            __m256 diff = _mm256_abs_ps(_mm256_sub_ps(v, w));\n            __m256 mask = _mm256_cmp_ps(diff, min_diff, _CMP_LT_OQ);\n            min_diff = _mm256_blendv_ps(min_diff, diff, mask);\n            smallest_pair = _mm256_blendv_ps(smallest_pair, _mm256_add_ps(v, w), mask);\n        }\n    }\n\n    float result[4];\n    _mm256_storeu_ps(result, smallest_pair);\n    std::vector<float> closest(result, result + 2);\n    return closest;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    const int vec_size = 4; // AVX size\n    int n = numbers.size();\n    __m256 min_diff = _mm256_set1_ps(__FLT_MAX__);\n    __m256 smallest_pair = _mm256_setzero_ps();\n\n    for(int i=0; i<n-1; i+=vec_size){\n        __m256 v0 = _mm256_loadu_ps(&numbers[i]);\n        if(i+vec_size < n){\n            __m256 v1 = _mm256_loadu_ps(&numbers[i+vec_size]);\n            __m256 diff = _mm256_sub_ps(v1, v0);\n            __m256 abs_diff = _mm256_abs_ps(diff);\n            __m256 mask = _mm256_cmp_ps(abs_diff, min_diff, _CMP_LT_OQ);\n            smallest_pair = _mm256_blendv_ps(smallest_pair, _mm256_minpos_ps(_mm256_permute2f128_ps(v0, v1, 1)), mask);\n            min_diff = _mm256_min_ps(min_diff, abs_diff);\n        }\n        else{\n            break;\n        }\n    }\n\n    float result[4];\n    _mm256_storeu_ps(result, smallest_pair);\n    std::vector<float> closest_elements(result, result + 2);\n    return closest_elements;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    int size = numbers.size();\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for(int i = 0; i < size; i += 8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    float minf[8];\n    float maxf[8];\n    _mm256_storeu_ps(minf, min_val);\n    _mm256_storeu_ps(maxf, max_val);\n\n    float global_min = minf[0];\n    float global_max = maxf[0];\n\n    for(int i = 1; i < 8; ++i){\n        if(minf[i] < global_min) global_min = minf[i];\n        if(maxf[i] > global_max) global_max = maxf[i];\n    }\n\n    std::vector<float> result(size);\n    for(int i = 0; i < size; i += 8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 scaled = _mm256_sub_ps(vec, _mm256_set1_ps(global_min));\n        scaled = _mm256_div_ps(scaled, _mm256_set1_ps(global_max - global_min));\n        _mm256_storeu_ps(&result[i], scaled);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    int size = numbers.size();\n    std::vector<float> result(size);\n    \n    __m256 min_val_vec = _mm256_set1_ps(numbers[0]);\n    __m256 max_val_vec = _mm256_set1_ps(numbers[0]);\n    \n    for(int i=0; i<size; i+=8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        \n        // Update min and max values\n        __m256 min_mask = _mm256_cmp_ps(vec, min_val_vec, _CMP_LT_OS);\n        min_val_vec = _mm256_blendv_ps(min_val_vec, vec, min_mask);\n        \n        __m256 max_mask = _mm256_cmp_ps(vec, max_val_vec, _CMP_GT_OS);\n        max_val_vec = _mm256_blendv_ps(max_val_vec, vec, max_mask);\n    }\n    \n    float min_val = _mm256_cvtss_f32(min_val_vec);\n    float max_val = _mm256_cvtss_f32(max_val_vec);\n    \n    __m256 range_vec = _mm256_set1_ps(max_val - min_val);\n    \n    for(int i=0; i<size; i+=8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 normalized = _mm256_sub_ps(vec, min_val_vec);\n        normalized = _mm256_div_ps(normalized, range_vec);\n        _mm256_storeu_ps(&result[i], normalized);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    int size = numbers.size();\n    std::vector<float> result(size);\n    \n    __m256 min_val_vec = _mm256_set1_ps(numbers[0]);\n    __m256 max_val_vec = _mm256_set1_ps(numbers[0]);\n    \n    for(int i=0; i<size; i+=8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        \n        // Update min and max values\n        __m256 min_mask = _mm256_cmp_ps(vec, min_val_vec, _CMP_LT_OS);\n        min_val_vec = _mm256_blendv_ps(min_val_vec, vec, min_mask);\n        \n        __m256 max_mask = _mm256_cmp_ps(vec, max_val_vec, _CMP_GT_OS);\n        max_val_vec = _mm256_blendv_ps(max_val_vec, vec, max_mask);\n    }\n    \n    float min_val = _mm256_cvtss_f32(min_val_vec);\n    float max_val = _mm256_cvtss_f32(max_val_vec);\n    \n    for(int i=0; i<size; i+=8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 scaled_vec = _mm256_sub_ps(vec, _mm256_set1_ps(min_val));\n        __m256 range_vec = _mm256_set1_ps(max_val - min_val);\n        scaled_vec = _mm256_div_ps(scaled_vec, range_vec);\n        \n        _mm256_storeu_ps(&result[i], scaled_vec);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    size_t n = numbers.size();\n    std::vector<float> result(n);\n    \n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n    \n    for(size_t i=0; i<n; i+=8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        \n        // Update min and max values\n        __m256 min_mask = _mm256_cmp_ps(vec, min_val, _CMP_LT_OS);\n        min_val = _mm256_min_ps(min_val, vec);\n        max_val = _mm256_max_ps(max_val, vec);\n        \n        // Compute rescaled vector\n        __m256 range = _mm256_sub_ps(max_val, min_val);\n        __m256 zero_check = _mm256_cmp_ps(range, _mm256_setzero_ps(), _CMP_EQ_OQ);\n        __m256 adjusted_vec = _mm256_sub_ps(vec, min_val);\n        __m256 rescaled = _mm256_div_ps(adjusted_vec, _mm256_blendv_ps(range, _mm256_set1_ps(1.0f), zero_check));\n        \n        _mm256_storeu_ps(&result[i], rescaled);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    size_t n = numbers.size();\n    std::vector<float> result(n);\n    \n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n    \n    for(size_t i=0; i<n; i+=8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        \n        // Update min and max values\n        __m256 min_update = _mm256_min_ps(vec, min_val);\n        __m256 max_update = _mm256_max_ps(vec, max_val);\n        _mm256_storeu_ps(&min_val, min_update);\n        _mm256_storeu_ps(&max_val, max_update);\n    }\n    \n    float global_min = _mm256_cvtss_f32(min_val);\n    float global_max = _mm256_cvtss_f32(max_val);\n    \n    for(size_t i=0; i<n; i+=8){\n        __m256 vec = _mm256_loadu_ps(&numbers[i]);\n        __m256 min_vec = _mm256_set1_ps(global_min);\n        __m256 max_vec = _mm256_set1_ps(global_max);\n        \n        // Rescale vector\n        __m256 rescaled = _mm256_sub_ps(vec, min_vec);\n        __m256 range = _mm256_sub_ps(max_vec, min_vec);\n        rescaled = _mm256_div_ps(rescaled, range);\n        \n        _mm256_storeu_ps(&result[i], rescaled);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_setzero_si256();\n    \n    for(int i=1; i<=16; i++){\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_div = _mm256_div_epu64(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mul_epu64(vec_div, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_mul, vec_n);\n        int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(vec_cmp));\n        \n        if(mask != 0){\n            __m256i vec_mask = _mm256_set1_epi32(mask);\n            vec_result = _mm256_blendv_epi8(vec_result, vec_i, _mm256_castsi128_si256(vec_mask));\n        }\n    }\n    \n    return _mm256_extract_epi64(vec_result, 0);\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_setzero_si256();\n    \n    for(int i=1; i<=32; i++){\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_div = _mm256_div_epu64(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mul_epu64(vec_div, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_mul, vec_n);\n        \n        int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(vec_cmp));\n        if(mask != 0){\n            vec_result = vec_i;\n            break;\n        }\n    }\n    \n    return _mm256_extract_epi64(vec_result, 0);\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_setzero_si256();\n    \n    for(int i=1; i<=32; i++){\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_div = _mm256_div_epu64(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mul_epu64(vec_div, vec_i);\n        __m256i vec_check = _mm256_cmpeq_epi64(vec_n, vec_mul);\n        vec_result = _mm256_or_si256(vec_result, vec_check);\n    }\n    \n    uint64_t result_mask = _mm256_movemask_epi8(_mm256_castsi256_si128(vec_result));\n    return __builtin_ctz(result_mask);\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_setzero_si256();\n    \n    for(int i=1; i<=16; i++){\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_div = _mm256_div_epu64(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mul_epu64(vec_div, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_mul, vec_n);\n        vec_result = _mm256_blendv_epi8(vec_result, vec_i, vec_cmp);\n    }\n    \n    return _mm256_extract_epi64(vec_result, 0);\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    __m256i vec_n = _mm256_set1_epi64x(n);\n    __m256i vec_result = _mm256_setzero_si256();\n    \n    for(int i=1; i<=32; i++){\n        __m256i vec_i = _mm256_set1_epi64x(i);\n        __m256i vec_div = _mm256_div_epu64(vec_n, vec_i);\n        __m256i vec_mul = _mm256_mul_epu64(vec_div, vec_i);\n        __m256i vec_cmp = _mm256_cmpeq_epi64(vec_mul, vec_n);\n        \n        int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(vec_cmp));\n        if(mask != 0){\n            vec_result = vec_i;\n            break;\n        }\n    }\n    \n    return _mm256_extract_epi64(vec_result, 0);\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    __m256i number = _mm256_set1_epi32(n);\n    __m256i limit = _mm256_sqrt_epi32(_mm256_set1_epi32(n));\n    __m256i i = _mm256_set_epi32(7, 5, 3, 2, 7, 5, 3, 2);\n    \n    while (_mm256_cmpgt_epi32(limit, _mm256_setzero_epi32()).m256i_i32[0]) {\n        __m256i remainder = _mm256_setzero_si256();\n        __m256i div_result = _mm256_div_epu32(number, i, &remainder);\n        \n        if (!_mm256_movemask_epi8(_mm256_cmpeq_epi32(remainder, _mm256_setzero_si256()))) {\n            continue;\n        }\n        \n        int count = 0;\n        while (!_mm256_movemask_epi8(_mm256_cmpeq_epi32(remainder, _mm256_setzero_si256()))) {\n            factors.push_back(i.m256i_i32[0]);\n            number = div_result;\n            remainder = _mm256_setzero_si256();\n            div_result = _mm256_div_epu32(number, i, &remainder);\n            count++;\n        }\n        \n        __m256i new_limit = _mm256_sqrt_epi32(number);\n        if (_mm256_cmpgt_epi32(new_limit, _mm256_setzero_epi32()).m256i_i32[0]) {\n            limit = new_limit;\n        }\n        \n        i = _mm256_add_epi32(i, _mm256_set1_epi32(2));\n    }\n    \n    if (!_mm256_movemask_epi8(_mm256_cmpeq_epi32(number, _mm256_set1_epi32(1)))) {\n        factors.push_back(number.m256i_i32[0]);\n    }\n    \n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    __m256i number = _mm256_set1_epi32(n);\n    __m256i limit = _mm256_sqrt_epi32(_mm256_set1_epi32(n));\n    __m256i i = _mm256_set_epi32(7, 5, 3, 2, 7, 5, 3, 2);\n    \n    while (_mm256_cmpgt_epi32(limit, _mm256_setzero_epi32()).m256i_i32[0]) {\n        __m256i remainder = _mm256_setzero_si256();\n        __m256i div_result = _mm256_div_epu32(number, i, &remainder);\n        \n        if (!_mm256_movemask_epi8(_mm256_cmpeq_epi32(remainder, _mm256_setzero_si256()))) {\n            continue;\n        }\n        \n        int count = 0;\n        while (!_mm256_movemask_epi8(_mm256_cmpeq_epi32(remainder, _mm256_setzero_si256()))) {\n            factors.push_back(i.m256i_i32[0]);\n            number = div_result;\n            remainder = _mm256_setzero_si256();\n            div_result = _mm256_div_epu32(number, i, &remainder);\n            count++;\n        }\n        \n        __m256i sqrt_check = _mm256_sqrt_epi32(i);\n        if (_mm256_cmpgt_epi32(sqrt_check, _mm256_setzero_epi32()).m256i_i32[0]) {\n            factors.push_back(i.m256i_i32[0]);\n        }\n        \n        i = _mm256_add_epi32(i, _mm256_set_epi32(8, 8, 8, 8, 8, 8, 8, 8));\n    }\n    \n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    __m256i num = _mm256_set1_epi32(n);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i current = _mm256_setzero_si256();\n    __m256i limit = _mm256_sqrt_epi32(_mm256_set1_epi32(n));\n    \n    while (_mm256_cmpgt_epi32(num, _mm256_set1_epi32(1))) {\n        current = two;\n        while (_mm256_extract_epi32(_mm256_mul_epu32(current, current), 0) <= n) {\n            __m256i rem = _mm256_mod_epi32(num, current);\n            if (!_mm256_movemask_epi8(_mm256_cmpeq_epi32(rem, _mm256_setzero_si256()))) {\n                break;\n            }\n            factors.push_back(_mm256_extract_epi32(current, 0));\n            num = _mm256_div_epi32(num, current);\n        }\n        current = _mm256_add_epi32(current, _mm256_set1_epi32(1));\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    __m256i num = _mm256_set1_epi32(n);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i current = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i limit = _mm256_sqrt_epi32(_mm256_mul_epi32(num, num));\n    \n    while (_mm256_cmpgt_epi32(current, _mm256_setzero_si256())) {\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_mod_epi32(num, current), _mm256_setzero_si256());\n        if (_mm256_movemask_epi8(mask) != 0) {\n            factors.push_back(__builtin_ctz(_mm256_movemask_epi8(mask)));\n            num = _mm256_div_epi32(num, current);\n        } else {\n            current = _mm256_sub_epi32(current, _mm256_set1_epi32(2));\n        }\n    }\n    \n    if (!_mm256_cmpgt_epi32(num, _mm256_setzero_si256())) {\n        factors.push_back(num[0]);\n    }\n    \n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    __m256i num = _mm256_set1_epi32(n);\n    __m256i two = _mm256_set1_epi32(2);\n    __m256i limit = _mm256_sqrt_epi32(_mm256_set1_epi32(n));\n    \n    while (_mm256_cmpgt_epi32(num, _mm256_setzero_epi32())) {\n        __m256i mask = _mm256_cmpeq_epi32(_mm256_mod_epi32(num, two), _mm256_setzero_epi32());\n        if (!_mm256_movemask_epi8(mask)) {\n            factors.push_back(n);\n            break;\n        }\n        __m256i factor = two;\n        factors.push_back(2);\n        num = _mm256_div_epi32(num, two);\n    }\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    int size = l.size();\n    std::vector<float> result;\n    result.reserve(size);\n\n    const __m256 zero = _mm256_setzero_ps();\n    int i = 0;\n\n    for (; i + 8 <= size; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 pos_mask = _mm256_cmp_ps(vec, zero, _MM_CMP_GT_OS);\n        __m256 positive = _mm256_and_ps(vec, pos_mask);\n        __m256i pos_mask_int = _mm256_castps_si256(pos_mask);\n        int mask = _mm256_movemask_ps(pos_mask);\n        if (mask != 0) {\n            float data[8];\n            _mm256_storeu_ps(data, positive);\n            for (int j = 0; j < 8; ++j) {\n                if ((mask & (1 << j)) != 0) {\n                    result.push_back(data[j]);\n                }\n            }\n        }\n    }\n\n    for (; i < size; ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int n = l.size();\n    __m256 pos_mask = _mm256_set1_ps(0.0f);\n    std::vector<float> result;\n    result.reserve(n); // Preallocate memory for efficiency\n\n    for(int i = 0; i + 8 <= n; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 pos_vec = _mm256_cmpgt_ps(vec, pos_mask); // Compare each element with 0.0f\n        __m256 filtered_vec = _mm256_and_ps(vec, pos_vec); // Keep only positive values\n        float temp[8];\n        _mm256_storeu_ps(temp, filtered_vec); // Store the result back to memory\n        for(int j = 0; j < 8; ++j){\n            if(temp[j] > 0.0f){\n                result.push_back(temp[j]);\n            }\n        }\n    }\n\n    // Handle remaining elements that don't fit into a full vector\n    for(int i = (n / 8) * 8; i < n; ++i){\n        if(l[i] > 0.0f){\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> &l) {\n    std::vector<float> result;\n    size_t n = l.size();\n    size_t i = 0;\n\n    // Process data in chunks of 8 using AVX\n    for (; i + 8 <= n; i += 8) {\n        __m256 input = _mm256_loadu_ps(&l[i]);\n        __m256 positive_mask = _mm256_cmpgt_ps(input, _mm256_setzero_ps());\n        __m256 positive_values = _mm256_and_ps(input, positive_mask);\n        float values[8];\n        _mm256_storeu_ps(values, positive_values);\n\n        for (int j = 0; j < 8; ++j) {\n            if (values[j] > 0.0f) {\n                result.push_back(values[j]);\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (l[i] > 0.0f) {\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int n = l.size();\n    __m256 pos_mask = _mm256_set1_ps(0.0f);\n    std::vector<float> result;\n    result.reserve(n); // Reserve space to avoid reallocations\n\n    for(int i = 0; i + 8 <= n; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 positive_mask = _mm256_cmpgt_ps(vec, pos_mask);\n        __m256 filtered_vec = _mm256_and_ps(vec, positive_mask);\n        float* filtered_data = (float*) &filtered_vec;\n        for(int j = 0; j < 8; ++j){\n            if(filtered_data[j] > 0.0f){\n                result.push_back(filtered_data[j]);\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for(int i = (n / 8) * 8; i < n; ++i){\n        if(l[i] > 0.0f){\n            result.push_back(l[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int n = l.size();\n    __m256 pos_mask = _mm256_set1_ps(0.0f);\n    std::vector<float> result;\n    result.reserve(n); // Preallocate memory for efficiency\n\n    for(int i = 0; i + 8 <= n; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        __m256 pos_vec = _mm256_cmpgt_ps(vec, pos_mask); // Compare each element with 0.0f\n        __m256 filtered_vec = _mm256_and_ps(vec, pos_vec); // Keep only positive values\n        _mm256_storeu_ps(&result[i], filtered_vec); // Store back to result\n    }\n\n    // Handle remaining elements that don't fit into a full vector\n    for(int i = n - (n % 8); i < n; ++i){\n        if(l[i] > 0.0f){\n            result.push_back(l[i]);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n){\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi64x(1);\n    const __m256i two = _mm256_set1_epi64x(2);\n    const __m256i three = _mm256_set1_epi64x(3);\n\n    // Handle edge cases\n    if(n <= 1) return false;\n    if(n == 2 || n == 3) return true;\n\n    // Check divisibility by 2 or 3\n    if(n % 2 == 0 || n % 3 == 0) return false;\n\n    // Generate potential factors up to sqrt(n)\n    int64_t limit = (int64_t)sqrt((double)n);\n    __m256i factors = _mm256_set_epi64x(31, 29, 23, 19, 17, 13, 11, 7);\n    while(_mm256_movemask_epi8(_mm256_cmpeq_epi64(factors, _mm256_set1_epi64x(limit)))) {\n        __m256i remainder = _mm256_mod_epi64(_mm256_set1_epi64x(n), factors);\n        if(!_mm256_testz_si256(remainder, remainder)) return false;\n        factors = _mm256_sub_epi64(factors, two);\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n){\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi64x(1);\n    const __m256i two = _mm256_set1_epi64x(2);\n\n    // Handle edge cases\n    if(n <= 1) return false;\n    if(n == 2 || n == 3) return true;\n    if(n % 2 == 0 || n % 3 == 0) return false;\n\n    // Create a vector of numbers from n-256 to n-1\n    __m256i v_numbers = _mm256_set_epi64x(n-8, n-7, n-6, n-5, n-4, n-3, n-2, n-1);\n\n    // Create a mask for numbers divisible by 2 or 3\n    __m256i v_mask_div_2_or_3 = _mm256_or_si256(\n        _mm256_and_si256(_mm256_and_si256(_mm256_set1_epi64x(1), _mm256_srli_epi64(v_numbers, 1)), \n                         _mm256_set1_epi64x(-1)),\n        _mm256_and_si256(_mm256_and_si256(_mm256_set1_epi64x(1), _mm256_srli_epi64(v_numbers, 2)), \n                         _mm256_set1_epi64x(-1))\n    );\n\n    // Check divisibility up to sqrt(n)\n    for(__m256i i = _mm256_add_epi64(two, two); _mm256_movemask_epi8(_mm256_cmpgt_epi64(i, _mm256_sqrt_epi64(_mm256_set1_epi64x(n)))); i = _mm256_add_epi64(i, two)){\n        __m256i div = _mm256_mullo_epi64(i, i);\n        __m256i mod = _mm256_rem_epi64(v_numbers, i);\n        v_mask_div_2_or_3 = _mm256_or_si256(v_mask_div_2_or_3, mod);\n    }\n\n    // Check if any number in the vector is divisible by any prime up to sqrt(n)\n    int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi64(zero, v_mask_div_2_or_3));\n    return __builtin_popcount(mask) == 0;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n){\n    __m256i num = _mm256_set1_epi64x(n);\n    __m256i mask = _mm256_setr_epi64x(2, 3, 5, 7);\n    \n    // Check divisibility by 2, 3, 5, 7\n    __m256i div = _mm256_div_epi64(num, mask);\n    __m256i rem = _mm256_and_si256(_mm256_sub_epi64(num, _mm256_mul_epu64(div, mask)), mask);\n    __m256i is_divisible = _mm256_cmpeq_epi64(rem, _mm256_setzero_si256());\n    \n    int mask_bits = _mm256_movemask_epi8(_mm256_castsi256_si128(is_divisible)) |\n                    _mm256_movemask_epi8(_mm256_extracti128_si256(is_divisible));\n    if (mask_bits != 0) return false;\n    \n    // Check divisibility by other primes up to sqrt(n)\n    int limit = (int)sqrt(n);\n    for(int i=11; i<=limit; i+=2){\n        mask = _mm256_set1_epi64x(i);\n        div = _mm256_div_epi64(num, mask);\n        rem = _mm256_and_si256(_mm256_sub_epi64(num, _mm256_mul_epu64(div, mask)), mask);\n        is_divisible = _mm256_cmpeq_epi64(rem, _mm256_setzero_si256());\n        mask_bits = _mm256_movemask_epi8(_mm256_castsi256_si128(is_divisible)) |\n                    _mm256_movemask_epi8(_mm256_extracti128_si256(is_divisible));\n        if (mask_bits != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n){\n    __m256i mask = _mm256_set1_epi64x(n);\n    __m256i primes = _mm256_setr_epi64x(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i divisors = _mm256_setr_epi64x(2, 2, 2, 2, 2, 2, 2, 2);\n\n    __m256i remainder = _mm256_div_epi64(mask, divisors);\n    __m256i is_divisible = _mm256_cmpeq_epi64(_mm256_mul_epu32(remainder, divisors), mask);\n\n    int mask_result = _mm256_movemask_epi8(_mm256_castsi256_si128(is_divisible));\n    return (mask_result == 0);\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c\nbool is_prime_simd(int64_t n){\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i one = _mm256_set1_epi64x(1);\n    const __m256i two = _mm256_set1_epi64x(2);\n    const __m256i three = _mm256_set1_epi64x(3);\n\n    // Handle edge cases\n    if(n <= 1) return false;\n    if(n == 2 || n == 3) return true;\n\n    // Check divisibility by 2 or 3\n    if(n % 2 == 0 || n % 3 == 0) return false;\n\n    // Generate potential factors up to sqrt(n)\n    int limit = (int)sqrt((double)n);\n    __m256i factors = _mm256_set_epi64x(limit-8, limit-7, limit-6, limit-5, limit-4, limit-3, limit-2, limit-1);\n\n    // Mask for valid primes\n    __m256i mask = _mm256_cmpgt_epi64(factors, one);\n\n    // SIMD loop\n    do {\n        __m256i divisible = _mm256_setzero_si256();\n\n        // Check divisibility for each factor\n        __m256i mod_result = _mm256_mul_epu32(_mm256_div_epu32(_mm256_set1_epi64x(n), factors), factors);\n        divisible = _mm256_cmpeq_epi64(mod_result, zero);\n\n        // Update mask based on divisibility\n        mask = _mm256_andnot_si256(divisible, mask);\n\n        // Increment factors\n        factors = _mm256_add_epi64(factors, two);\n    } while(_mm256_movemask_epi8(mask) != 0);\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    size_t len = s.size();\n    if(len == 0) return s;\n\n    __m256i mask_letters = _mm256_set1_epi8(0x20);\n    __m256i mask_alpha = _mm256_setr_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 32, 32, 32, 32, 32, 32, 32, 32\n    );\n    \n    size_t i = 0;\n    __m256i vresult;\n    __m256i vchar;\n    __m256i vshifted;\n    __m256i vcmp_result;\n    __m256i vcmp_alpha;\n    __m256i vcmp_not_alpha;\n    __m256i vcmp_mask;\n\n    while(i + 8 <= len){\n        vchar = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        vshifted = _mm256_or_si256(vchar, mask_letters);\n        vcmp_alpha = _mm256_cmpgtb_epi8(vchar, mask_alpha);\n        vcmp_not_alpha = _mm256_andnot_si256(vcmp_alpha, _mm256_set1_epi8(0xFF));\n        vcmp_mask = _mm256_or_si256(vcmp_alpha, _mm256_set1_epi8(0x20));\n        vresult = _mm256_xor_si256(vchar, vcmp_mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(const_cast<char*>(s.data() + i)), vresult);\n        i += 8;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = s[i];\n        if(c >= 'A' && c <= 'Z') s[i] = c + ('a' - 'A');\n        else if(c >= 'a' && c <= 'z') s[i] = c - ('a' - 'A');\n    }\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    size_t len = s.size();\n    __m256i mask_letters = _mm256_set1_epi8(0x20);\n    __m256i mask_alpha = _mm256_setr_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n    __m256i mask_shift = _mm256_setr_epi8(\n        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31\n    );\n\n    if(len == 0) return \"\";\n\n    size_t i = 0;\n    std::string result = s;\n    __m256i v_input, v_masked, v_shifted, v_reversed;\n\n    while(i + 8 <= len){\n        v_input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        v_masked = _mm256_and_si256(v_input, mask_letters);\n        v_shifted = _mm256_xor_si256(v_input, mask_masked);\n        v_reversed = _mm256_permutevar8x32_epi32(v_shifted, mask_shift);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), v_reversed);\n        i += 8;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = s[i];\n        if(c >= 'a' && c <= 'z') result[i] = c - 32;\n        else if(c >= 'A' && c <= 'Z') result[i] = c + 32;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    size_t len = s.size();\n    if(len == 0) return s;\n\n    // Determine if there's at least one alphabetic character\n    bool has_alpha = false;\n    for(char c : s) {\n        if( (c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') ){\n            has_alpha = true;\n            break;\n        }\n    }\n\n    if(!has_alpha){\n        std::string reversed(s.rbegin(), s.rend());\n        return reversed;\n    }\n\n    size_t i = 0;\n    __m256i mask_upper = _mm256_set1_epi8('_'); // Mask for uppercase check\n    __m256i mask_lower = _mm256_set1_epi8('_'); // Mask for lowercase check\n    __m256i zero = _mm256_setzero_si256();\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i toggle_case;\n    __m256i input, output;\n    std::string result = s;\n\n    while(i + 32 <= len){\n        input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n\n        // Create masks for uppercase and lowercase\n        mask_upper = _mm256_or_si256(_mm256_cmpgt_epi8(input, _mm256_set1_epi8('A')), \n                                     _mm256_cmpgt_epi8(_mm256_set1_epi8('Z'), input));\n        mask_lower = _mm256_or_si256(_mm256_cmpgt_epi8(input, _mm256_set1_epi8('a')), \n                                     _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), input));\n\n        // Combine masks to identify alphabetic characters\n        __m256i is_alpha = _mm256_or_si256(mask_upper, mask_lower);\n\n        // Toggle case for alphabetic characters\n        toggle_case = _mm256_xor_si256(input, _mm256_andnot_si256(is_alpha, _mm256_set1_epi8(0x20)));\n\n        // Store the result back into the vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), toggle_case);\n\n        i += 32;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = s[i];\n        if(c >= 'A' && c <= 'Z') result[i] = c + ('a' - 'A');\n        else if(c >= 'a' && c <= 'z') result[i] = c - ('a' - 'A');\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    size_t len = s.size();\n    __m256i mask_letters = _mm256_set_epi8(\n        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n    );\n    __m256i mask_upper = _mm256_set_epi8(\n        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n    );\n    __m256i mask_lower = _mm256_set_epi8(\n        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n    );\n\n    for(int i=0;i<16;i++) {\n        mask_letters = _mm256_insert_epi8(mask_letters, 0x20^(s[i]&0xFF), i);\n        mask_upper = _mm256_insert_epi8(mask_upper, (s[i]-'A') <= 25 ? 1 : 0, i);\n        mask_lower = _mm256_insert_epi8(mask_lower, (s[i]-'a') <= 25 ? 1 : 0, i);\n    }\n\n    __m256i v_input = _mm256_loadu_si256((__m256i const*)(s.data()));\n    __m256i v_mask_letters = _mm256_loadu_si256((__m256i const*)(mask_letters));\n    __m256i v_mask_upper = _mm256_loadu_si256((__m256i const*)(mask_upper));\n    __m256i v_mask_lower = _mm256_loadu_si256((__m256i const*)(mask_lower));\n\n    __m256i v_result = _mm256_or_si256(\n        _mm256_andnot_si256(v_mask_letters, v_input),\n        _mm256_xor_si256(\n            _mm256_and_si256(v_mask_upper, v_input),\n            _mm256_and_si256(v_mask_upper, mask_letters)\n        ),\n        _mm256_xor_si256(\n            _mm256_and_si256(v_mask_lower, v_input),\n            _mm256_and_si256(v_mask_lower, mask_letters)\n        )\n    );\n\n    // Handle remaining characters if len % 32 != 0\n    // Implementation details omitted for brevity\n\n    return std::string((char*)&v_result, len);\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    size_t len = s.size();\n    if(len == 0) return s;\n\n    // Determine if there's at least one alphabetic character\n    bool has_alpha = false;\n    for(char c : s) {\n        if( (c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') ){\n            has_alpha = true;\n            break;\n        }\n    }\n\n    // If no alphabetic characters, reverse the string\n    if(!has_alpha){\n        std::string reversed = s;\n        std::reverse(reversed.begin(), reversed.end());\n        return reversed;\n    }\n\n    // Prepare output buffer\n    std::string result = s;\n    size_t i = 0;\n\n    while(i + 32 <= len){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Check for lowercase letters\n        __m256i lower_mask = _mm256_cmpgt_epi8(_mm256_and_si256(vec, _mm256_set1_epi8(0x20)), _mm256_setzero_si256());\n        __m256i lower_to_upper = _mm256_andnot_si256(lower_mask, _mm256_xor_si256(vec, _mm256_set1_epi8(0x20)));\n\n        // Check for uppercase letters\n        __m256i upper_mask = _mm256_cmpgt_epi8(_mm256_and_si256(vec, _mm256_set1_epi8(0x20)), _mm256_set1_epi8(0x40));\n        __m256i upper_to_lower = _mm256_and_si256(upper_mask, _mm256_xor_si256(vec, _mm256_set1_epi8(0x20)));\n\n        // Combine transformations\n        __m256i transformed = _mm256_or_si256(lower_to_upper, upper_to_lower);\n\n        // Load mask for non-alphabetic characters\n        __m256i char_mask = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8(0x20));\n        __m256i char_mask_neg = _mm256_cmpeq_epi8(char_mask, _mm256_setzero_si256());\n\n        // Apply mask to keep non-alphabetic characters unchanged\n        __m256i final_vec = _mm256_or_si256(_mm256_and_si256(transformed, char_mask), _mm256_andnot_si256(char_mask, vec));\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final_vec);\n        i += 32;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = s[i];\n        if(c >= 'A' && c <= 'Z'){\n            result[i] = c + ('a' - 'A');\n        }\n        else if(c >= 'a' && c <= 'z'){\n            result[i] = c - ('a' - 'A');\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n    __m256i v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n    int i = 0;\n\n    while(i + 16 <= n){\n        // Load 16 elements into vectors\n        v0 = _mm256_loadu_si256((__m256i*)&l[i]);\n        v1 = _mm256_permutevar8x32_epi32(v0, _mm256_setr_epi32(0, 3, 6, 9, 12, 15, 1, 4));\n        v2 = _mm256_permutevar8x32_epi32(v0, _mm256_setr_epi32(0, 3, 6, 9, 12, 15, 2, 5));\n        v3 = _mm256_permutevar8x32_epi32(v0, _mm256_setr_epi32(1, 4, 7, 10, 13, 16, 2, 5));\n        v4 = _mm256_permutevar8x32_epi32(v0, _mm256_setr_epi32(1, 4, 7, 10, 13, 16, 3, 6));\n        v5 = _mm256_permutevar8x32_epi32(v0, _mm256_setr_epi32(2, 5, 8, 11, 14, 17, 3, 6));\n        v6 = _mm256_permutevar8x32_epi32(v0, _mm256_setr_epi32(2, 5, 8, 11, 14, 17, 4, 7));\n        \n        // Sort each group of 3 elements\n        v7 = _mm256_min_epu32(v1, v2);\n        v8 = _mm256_max_epu32(v1, v2);\n        v9 = _mm256_min_epu32(v7, v3);\n        va = _mm256_max_epu32(v7, v3);\n        vb = _mm256_min_epu32(v9, v4);\n        vc = _mm256_max_epu32(v9, v4);\n        vd = _mm256_min_epu32(vb, v5);\n        ve = _mm256_max_epu32(vb, v5);\n        vf = _mm256_min_epu32(vd, v6);\n\n        // Store sorted elements back\n        _mm256_storeu_si256((__m256i*)&result[i], vf);\n        i += 16;\n    }\n\n    // Handle remaining elements if any\n    for(; i < n; ++i){\n        result[i] = l[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    const int n = l.size();\n    std::vector<int> result = l;\n    if(n < 3) return result;\n\n    __m256i* vec_ptr = (__m256i*)&result[0];\n    int remaining = n & ~7;\n\n    for(int i=0; i<remaining; i+=8){\n        __m256i v0 = vec_ptr[i+0];\n        __m256i v1 = vec_ptr[i+1];\n        __m256i v2 = vec_ptr[i+2];\n\n        __m256i sorted = _mm256_permutevar8x32_epi32(\n            _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),\n            _mm256_cmpgt_epi32(v0, v1)\n        );\n\n        __m256i temp = v0;\n        v0 = _mm256_blendv_epi8(v0, v1, sorted);\n        v1 = _mm256_blendv_epi8(v1, temp, sorted);\n\n        sorted = _mm256_permutevar8x32_epi32(\n            _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),\n            _mm256_cmpgt_epi32(v0, v2)\n        );\n\n        temp = v0;\n        v0 = _mm256_blendv_epi8(v0, v2, sorted);\n        v2 = _mm256_blendv_epi8(v2, temp, sorted);\n\n        sorted = _mm256_permutevar8x32_epi32(\n            _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),\n            _mm256_cmpgt_epi32(v1, v2)\n        );\n\n        temp = v1;\n        v1 = _mm256_blendv_epi8(v1, v2, sorted);\n        v2 = _mm256_blendv_epi8(v2, temp, sorted);\n\n        vec_ptr[i+0] = v0;\n        vec_ptr[i+1] = v1;\n        vec_ptr[i+2] = v2;\n    }\n\n    // Handle remaining elements\n    for(int i=remaining; i<n; ++i){\n        if(i % 3 == 0){\n            // Sort the last three elements\n            if(i+1 < n && l[i] > l[i+1]){\n                std::swap(l[i], l[i+1]);\n            }\n            if(i+2 < n && l[i] > l[i+2]){\n                std::swap(l[i], l[i+2]);\n            }\n            if(i+1 < n && i+2 < n && l[i+1] > l[i+2]){\n                std::swap(l[i+1], l[i+2]);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    const int n = l.size();\n    std::vector<int> result = l;\n    if(n < 3) return result;\n\n    __m256i* vec_ptr = (__m256i*)&result[0];\n    __m256i* input_ptr = (__m256i*)&l[0];\n\n    int remaining = n & ~7; // Align to 8 elements\n\n    for(int i=0; i<remaining; i+=8){\n        __m256i v0 = _mm256_permutevar8x32_epi32(input_ptr[i+0], _mm256_set_epi32(6, 3, 0, 7, 4, 1, 2, 5));\n        __m256i v1 = _mm256_permutevar8x32_epi32(input_ptr[i+1], _mm256_set_epi32(6, 3, 0, 7, 4, 1, 2, 5));\n        __m256i v2 = _mm256_permutevar8x32_epi32(input_ptr[i+2], _mm256_set_epi32(6, 3, 0, 7, 4, 1, 2, 5));\n\n        __m256i sorted0 = _mm256_shuffle_epi32(v0, 0b00110011);\n        __m256i sorted1 = _mm256_shuffle_epi32(v1, 0b00110011);\n        __m256i sorted2 = _mm256_shuffle_epi32(v2, 0b00110011);\n\n        _mm256_storeu_si256(&vec_ptr[i+0], sorted0);\n        _mm256_storeu_si256(&vec_ptr[i+1], sorted1);\n        _mm256_storeu_si256(&vec_ptr[i+2], sorted2);\n    }\n\n    // Handle remaining elements\n    for(int i=remaining; i<n; ++i){\n        if(i % 3 == 0){\n            // Sort the group of three\n            int group[3] = {l[i], l[i+1], l[i+2]};\n            std::sort(group, group + 3);\n            result[i] = group[0];\n            result[i+1] = group[1];\n            result[i+2] = group[2];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n    __m256i v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n    int i = 0;\n\n    while(i + 16 <= n){\n        // Load 16 elements into vectors\n        v0 = _mm256_loadu_si256((__m256i*)&l[i]);\n        v1 = _mm256_permutevar8x32_epi32(v0, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        v2 = _mm256_permutevar8x32_epi32(v0, _mm256_set_epi32(8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Sort each group of 8 elements\n        v3 = _mm256_shuffle_epi32(v1, 0b01000100);\n        v4 = _mm256_min_epi32(v1, v3);\n        v5 = _mm256_max_epi32(v1, v3);\n        v6 = _mm256_shuffle_epi32(v5, 0b10101010);\n        v7 = _mm256_min_epi32(v5, v6);\n        v8 = _mm256_max_epi32(v5, v6);\n\n        v9 = _mm256_shuffle_epi32(v2, 0b01000100);\n        va = _mm256_min_epi32(v2, v9);\n        vb = _mm256_max_epi32(v2, v9);\n        vc = _mm256_shuffle_epi32(vb, 0b10101010);\n        vd = _mm256_min_epi32(vb, vc);\n        ve = _mm256_max_epi32(vb, vc);\n        vf = _mm256_permute4x64_epi64(_mm256_set_m128i(vd, ve), 78);\n\n        // Interleave sorted groups\n        v0 = _mm256_permute2x128_si256(v7, vf, 0x20);\n        v1 = _mm256_permute2x128_si256(v7, vf, 0x31);\n        v2 = _mm256_permute2x128_si256(v8, vf, 0x20);\n        v3 = _mm256_permute2x128_si256(v8, vf, 0x31);\n\n        // Store back to result\n        _mm256_storeu_si256((__m256i*)&result[i], v0);\n        _mm256_storeu_si256((__m256i*)&result[i+8], v2);\n\n        i += 16;\n    }\n\n    // Handle remaining elements if any\n    for(; i < n; ++i){\n        if(i % 3 == 0){\n            // Find the minimum among the next three elements\n            int min_val = l[i];\n            if(i+1 < n && l[i+1] < min_val) min_val = l[i+1];\n            if(i+2 < n && l[i+2] < min_val) min_val = l[i+2];\n            result[i] = min_val;\n            result[i+1] = l[i+1] == min_val ? l[i] : l[i+1];\n            result[i+2] = l[i+2] == min_val ? l[i] : l[i+2];\n            i += 2;\n        }\n        else{\n            result[i] = l[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n    __m256i v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n    int i = 0;\n\n    while(i + 16 <= n){\n        // Load 16 elements into vectors\n        v0 = _mm256_loadu_si256((__m256i*)&l[i]);\n        v1 = _mm256_permutevar8x32_epi32(v0, _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7));\n        v2 = _mm256_permutevar8x32_epi32(v0, _mm256_set_epi32(8, 9, 10, 11, 12, 13, 14, 15));\n\n        // Sort each group of 8 elements\n        v3 = _mm256_shuffle_epi32(v1, 0b01000100);\n        v4 = _mm256_min_epi32(v1, v3);\n        v5 = _mm256_max_epi32(v1, v3);\n        v6 = _mm256_shuffle_epi32(v5, 0b10101010);\n        v7 = _mm256_min_epi32(v5, v6);\n        v8 = _mm256_max_epi32(v5, v6);\n\n        v9 = _mm256_shuffle_epi32(v2, 0b01000100);\n        va = _mm256_min_epi32(v2, v9);\n        vb = _mm256_max_epi32(v2, v9);\n        vc = _mm256_shuffle_epi32(vb, 0b10101010);\n        vd = _mm256_min_epi32(vb, vc);\n        ve = _mm256_max_epi32(vb, vc);\n        vf = _mm256_permute2x128_si256(v8, ve, 0x20);\n\n        // Store sorted elements back into result\n        _mm256_storeu_si256((__m256i*)&result[i], vf);\n        i += 16;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    int n = l.size();\n    if (n == 0) return 0.0f;\n\n    __m256 max_val = _mm256_setzero_ps();\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    __m256 horizontal_max = max_val;\n    horizontal_max = _mm256_hadd_ps(horizontal_max, horizontal_max);\n    horizontal_max = _mm256_hadd_ps(horizontal_max, horizontal_max);\n    float result[8];\n    _mm256_storeu_ps(result, horizontal_max);\n    float current_max = result[0];\n    for (; i < n; ++i) {\n        if (l[i] > current_max) {\n            current_max = l[i];\n        }\n    }\n    return current_max;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    int n = l.size();\n    if (n == 0) return 0.0f;\n\n    __m256 max_val = _mm256_setzero_ps();\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    __m256 mask = _mm256_cmpgt_ps(_mm256_permutevar8x32_ps(max_val, _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7)),\n                                  _mm256_permutevar8x32_ps(max_val, _mm256_setr_epi32(4, 5, 6, 7, 0, 1, 2, 3)));\n    __m256 max1 = _mm256_blendv_ps(_mm256_permutevar8x32_ps(max_val, _mm256_setr_epi32(0, 1, 2, 3, 0, 1, 2, 3)),\n                                   _mm256_permutevar8x32_ps(max_val, _mm256_setr_epi32(4, 5, 6, 7, 4, 5, 6, 7)), mask);\n    __m256 max2 = _mm256_blendv_ps(max1, _mm256_permutevar8x32_ps(max1, _mm256_setr_epi32(0, 0, 0, 0, 1, 1, 1, 1)), \n                                   _mm256_cmpgt_ps(max1, _mm256_permutevar8x32_ps(max1, _mm256_setr_epi32(1, 1, 1, 1, 0, 0, 0, 0))));\n\n    float result[8];\n    _mm256_storeu_ps(result, max2);\n    float max_float = result[0];\n    for (int j = 1; j < 8; ++j) {\n        if (result[j] > max_float) {\n            max_float = result[j];\n        }\n    }\n\n    for (; i < n; ++i) {\n        if (l[i] > max_float) {\n            max_float = l[i];\n        }\n    }\n\n    return max_float;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l){\n    int n = l.size();\n    __m256 max_val = _mm256_setzero_ps();\n    int i = 0;\n    \n    // Process 8 elements at a time\n    for (; i <= n - 8; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n    \n    // Horizontal max reduction\n    __m256 shuffle1 = _mm256_permute2f128_ps(max_val, max_val, 0b00);\n    __m256 shuffle2 = _mm256_permute2f128_ps(max_val, max_val, 0b01);\n    __m256 max_temp = _mm256_max_ps(shuffle1, shuffle2);\n    __m128 high_word = _mm256_extractf128_ps(max_temp, 1);\n    __m128 low_word = _mm256_extractf128_ps(max_temp, 0);\n    __m128 final_max = _mm_max_ps(high_word, low_word);\n    \n    return _mm_cvtss_f32(final_max);\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l){\n    const int n = l.size();\n    __m256 max_val = _mm256_setzero_ps();\n    int i = 0;\n    \n    // Process 8 elements at a time\n    for (; i <= n - 8; i += 8){\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n    \n    // Horizontal max reduction\n    __m256 shuffle1 = _mm256_permute2f128_ps(max_val, max_val, 0b00);\n    __m256 shuffle2 = _mm256_permute2f128_ps(max_val, max_val, 0b01);\n    __m256 max_temp = _mm256_max_ps(shuffle1, shuffle2);\n    \n    __m128 high_word = _mm256_extractf128_ps(max_temp, 1);\n    __m128 low_word = _mm256_extractf128_ps(max_temp, 0);\n    __m128 final_max = _mm_max_ps(high_word, low_word);\n    \n    return _mm_cvtss_f32(final_max);\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(const std::vector<float>& l) {\n    int n = l.size();\n    if (n == 0) return 0.0f;\n\n    __m256 max_val = _mm256_setzero_ps();\n    int i = 0;\n    \n    // Process 8 elements at a time\n    for (; i + 8 <= n; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&l[i]);\n        max_val = _mm256_max_ps(max_val, vec);\n    }\n\n    // Horizontal max reduction\n    __m256 shuffle1 = _mm256_permute2f128_ps(max_val, max_val, 0b00);\n    __m256 shuffle2 = _mm256_permute2f128_ps(max_val, max_val, 0b01);\n    __m256 max_temp = _mm256_max_ps(shuffle1, shuffle2);\n\n    __m128 high_word = _mm256_extractf128_ps(max_temp, 1);\n    __m128 low_word = _mm256_extractf128_ps(max_temp, 0);\n    __m128 final_max = _mm_max_ps(high_word, low_word);\n\n    return _mm_cvtss_f32(final_max);\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m256i mask_7 = _mm256_set1_epi8(7);\n    __m256i divisors = _mm256_setr_epi32(11, 13, 11, 13, 11, 13, 11, 13);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i seven_count = _mm256_setzero_si256();\n\n    for(int i=0; i<n; i+=8){\n        __m256i num = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod_divisors = _mm256_setzero_si256();\n        \n        for(int j=0; j<2; ++j){\n            __m256i temp = _mm256_div_epu32(num, _mm256_permutevar8x32_epi32(divisors, _mm256_setr_epi32(j*4, j*4+1, j*4+2, j*4+3, j*4+4, j*4+5, j*4+6, j*4+7)));\n            mod_divisors = _mm256_or_si256(mod_divisors, temp);\n        }\n\n        __m256i is_divisible = _mm256_cmpeq_epi32(mod_divisors, zero);\n        __m256i digits = _mm256_and_si256(_mm256_set1_epi32(0xFF), num);\n        __m256i digit_is_7 = _mm256_cmpeq_epi8(digits, mask_7);\n        seven_count = _mm256_add_epi8(seven_count, digit_is_7);\n        \n        count += _mm256_movemask_epi8(is_divisible);\n    }\n\n    // Horizontal sum for remaining elements if n is not a multiple of 8\n    for(int i=n - (n%8); i<n; ++i){\n        if(i % 11 == 0 || i % 13 == 0){\n            char digits[4];\n            digits[0] = (char)(i / 1000);\n            digits[1] = (char)((i / 100) % 10);\n            digits[2] = (char)((i / 10) % 10);\n            digits[3] = (char)(i % 10);\n            for(int d=0; d<4; ++d){\n                if(digits[d] == '7') count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m256i mask_7 = _mm256_set1_epi8(7);\n    __m256i divisors = _mm256_setr_epi32(11, 13, 11, 13, 11, 13, 11, 13);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i seven_count = _mm256_setzero_si256();\n\n    for(int i=0; i<n; i+=8){\n        __m256i num = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod_divisors = _mm256_mul_epu32(num, _mm256_set1_epi32(1));\n        __m256i divisible = _mm256_cmpeq_epi32(_mm256_and_si256(mod_divisors, divisors), zero);\n\n        __m256i digits = _mm256_and_si256(_mm256_set1_epi8(0xF), num);\n        __m256i eq_7 = _mm256_cmpeq_epi8(digits, mask_7);\n        seven_count = _mm256_add_epi8(seven_count, eq_7);\n        \n        __m256i total_divisible = _mm256_blendv_epi8(zero, seven_count, divisible);\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi8(total_divisible, zero));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m256i mask_7 = _mm256_set1_epi8(7);\n    __m256i divisors = _mm256_setr_epi32(11, 13, 11, 13, 11, 13, 11, 13);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i seven_count = _mm256_setzero_si256();\n\n    for(int i=0; i<n; i+=8){\n        __m256i num = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod_divisors = _mm256_setzero_si256();\n        \n        for(int j=0; j<2; ++j){\n            __m256i temp = _mm256_div_epu32(num, _mm256_permutevar8x32_epi32(divisors, _mm256_setr_epi32(j*4, j*4+1, j*4+2, j*4+3, j*4+4, j*4+5, j*4+6, j*4+7)));\n            mod_divisors = _mm256_or_si256(mod_divisors, temp);\n        }\n\n        __m256i is_divisible = _mm256_cmpeq_epi32(mod_divisors, zero);\n        __m256i digits = _mm256_and_si256(_mm256_set1_epi32(0xFF), num);\n        __m256i digit_matches = _mm256_cmpeq_epi8(digits, mask_7);\n        seven_count = _mm256_add_epi8(seven_count, digit_matches);\n        \n        __m256i count_mask = _mm256_cvtepi8_epi32(digit_matches);\n        count += _mm256_movemask_epi8(_mm256_cmpgt_epi8(count_mask, zero));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m256i mask_7 = _mm256_set1_epi8(7);\n    __m256i divisors = _mm256_setr_epi32(11, 13, 11, 13, 11, 13, 11, 13);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i seven_count = _mm256_setzero_si256();\n\n    for(int i=0; i<n; i+=8){\n        __m256i num = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod_divisors = _mm256_mul_epu32(num, _mm256_set1_epi32(1));\n        __m256i divisible = _mm256_cmpeq_epi32(_mm256_and_si256(mod_divisors, divisors), zero);\n\n        __m256i digits = _mm256_and_si256(_mm256_set1_epi8(0xF), num);\n        __m256i eq_7 = _mm256_cmpeq_epi8(digits, mask_7);\n        seven_count = _mm256_add_epi8(seven_count, eq_7);\n        \n        __m256i total = _mm256_blendv_epi8(zero, seven_count, divisible);\n        count += _mm256_movemask_epi8(total);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m256i mask_7 = _mm256_set1_epi8(7);\n    __m256i divisors = _mm256_setr_epi32(11, 13, 11, 13, 11, 13, 11, 13);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i seven_count = _mm256_setzero_si256();\n\n    for(int i=0; i<n; i+=8){\n        __m256i num = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i mod_divisors = _mm256_setzero_si256();\n        \n        for(int j=0; j<2; ++j){\n            __m256i temp = _mm256_div_epu32(num, _mm256_permutevar8x32_epi32(divisors, _mm256_setr_epi32(j*4, j*4+1, j*4+2, j*4+3, j*4+4, j*4+5, j*4+6, j*4+7)));\n            mod_divisors = _mm256_or_si256(mod_divisors, temp);\n        }\n\n        __m256i is_divisible = _mm256_cmpeq_epi32(mod_divisors, zero);\n        __m256i digits = _mm256_and_si256(_mm256_set1_epi32(0xFF), num);\n        \n        __m256i eq_7 = _mm256_cmpeq_epi8(digits, mask_7);\n        seven_count = _mm256_add_epi8(seven_count, eq_7);\n        \n        count += _mm256_movemask_epi8(is_divisible);\n    }\n    \n    // Horizontal sum for seven_count\n    __m256i shuffle1 = _mm256_shuffle_epi32(seven_count, _MM_SHUFFLE(2,3,0,1));\n    __m256i sum1 = _mm256_add_epi8(seven_count, shuffle1);\n    __m256i shuffle2 = _mm256_shuffle_epi32(sum1, _MM_SHUFFLE(1,0,3,2));\n    __m256i sum2 = _mm256_add_epi8(sum1, shuffle2);\n    __m256i shuffle3 = _mm256_shuffle_epi32(sum2, _MM_SHUFFLE(0,1,2,3));\n    __m256i sum3 = _mm256_add_epi8(sum2, shuffle3);\n    \n    count += _mm256_movemask_epi8(_mm256_castsi256_si128(sum3)) + \n             _mm256_extract_epi16(_mm256_castsi256_si128(sum3), 4);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    std::vector<float> result(n);\n    \n    if(n == 0) return result;\n    \n    // Load data into SIMD registers\n    __m256 v0 = _mm256_loadu_ps(&l[0]);\n    __m256 v1 = _mm256_loadu_ps(&l[4]);\n    __m256 v2 = _mm256_loadu_ps(&l[8]);\n    __m256 v3 = _mm256_loadu_ps(&l[12]);\n\n    // Extract even indices\n    __m256 even0 = _mm256_permute2f128_ps(v0, v1, 0b01000000);\n    __m256 even1 = _mm256_permute2f128_ps(v2, v3, 0b01000000);\n\n    // Sort even indices\n    __m256 sorted_even0 = _mm256_permute4x64_ps(even0, 0b01100001);\n    __m256 sorted_even1 = _mm256_permute4x64_ps(even1, 0b01100001);\n\n    // Interleave sorted evens with odds\n    __m256 interleaved0 = _mm256_permute2f128_ps(sorted_even0, v0, 0b00010000);\n    __m256 interleaved1 = _mm256_permute2f128_ps(sorted_even1, v2, 0b00010000);\n\n    // Store results\n    _mm256_storeu_ps(&result[0], interleaved0);\n    _mm256_storeu_ps(&result[8], interleaved1);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    std::vector<float> result(n);\n    \n    if(n == 0) return result;\n    \n    // Load data into SIMD registers\n    __m256 v0 = _mm256_loadu_ps(&l[0]);\n    __m256 v1 = _mm256_loadu_ps(&l[4]);\n    __m256 v2 = _mm256_loadu_ps(&l[8]);\n    __m256 v3 = _mm256_loadu_ps(&l[12]);\n\n    // Extract even indices\n    __m256 even0 = _mm256_permute2f128_ps(v0, v1, 0b01000000);\n    __m256 even1 = _mm256_permute2f128_ps(v2, v3, 0b01000000);\n\n    // Sort even indices\n    __m256 sorted0 = _mm256_permute4x64_ps(even0, 0b01100110);\n    __m256 sorted1 = _mm256_permute4x64_ps(even1, 0b01100110);\n\n    // Blend sorted evens with original odds\n    __m256 res0 = _mm256_blend_ps(sorted0, v0, 0b10101010);\n    __m256 res1 = _mm256_blend_ps(sorted1, v2, 0b10101010);\n\n    // Store results\n    _mm256_storeu_ps(&result[0], res0);\n    _mm256_storeu_ps(&result[8], res1);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    std::vector<float> result(n);\n    \n    if(n == 0) return result;\n    \n    // Load data into SIMD registers\n    __m256 v0 = _mm256_loadu_ps(&l[0]);\n    __m256 v1 = _mm256_loadu_ps(&l[4]);\n    __m256 v2 = _mm256_loadu_ps(&l[8]);\n    __m256 v3 = _mm256_loadu_ps(&l[12]);\n\n    // Extract even indices\n    __m256 even0 = _mm256_permute2f128_ps(v0, v1, 0b01000000);\n    __m256 even1 = _mm256_permute2f128_ps(v2, v3, 0b01000000);\n\n    // Sort each set of even indices\n    __m256 sorted0 = _mm256_permute4x64_ps(even0, 0b01100110);\n    __m256 sorted1 = _mm256_permute4x64_ps(even1, 0b01100110);\n\n    // Interleave sorted evens with original odds\n    __m256 interleaved0 = _mm256_permute2f128_ps(sorted0, v0, 0b00010000);\n    __m256 interleaved1 = _mm256_permute2f128_ps(sorted1, v2, 0b00010000);\n\n    // Store results\n    _mm256_storeu_ps(&result[0], interleaved0);\n    _mm256_storeu_ps(&result[8], interleaved1);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    std::vector<float> result(n);\n    \n    if(n == 0) return result;\n    \n    // Load data into SIMD registers\n    __m256 v0 = _mm256_loadu_ps(&l[0]);\n    __m256 v1 = _mm256_loadu_ps(&l[4]);\n    \n    // Extract even indices\n    __m256 even0 = _mm256_permutevar8x32_ps(v0, _mm256_setr_epi32(0, -1, 2, -1, 4, -1, 6, -1));\n    __m256 even1 = _mm256_permutevar8x32_ps(v1, _mm256_setr_epi32(0, -1, 2, -1, 4, -1, 6, -1));\n    \n    // Sort even indices\n    __m256 sorted_even0 = _mm256_shuffle_ps(even0, even0, _MM_SHUFFLE(2, 3, 0, 1));\n    __m256 mask = _mm256_cmp_ps(sorted_even0, even0, _CMP_LT_OS);\n    __m256 sorted_even1 = _mm256_blendv_ps(sorted_even0, even0, mask);\n    \n    // Interleave sorted evens with odds\n    __m256 interleaved0 = _mm256_permute2f128_ps(sorted_even0, even0, 0x20);\n    __m256 interleaved1 = _mm256_permute2f128_ps(sorted_even1, even1, 0x20);\n    \n    // Store results\n    _mm256_storeu_ps(&result[0], interleaved0);\n    _mm256_storeu_ps(&result[8], interleaved1);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    std::vector<float> result(n);\n    \n    if(n == 0) return result;\n    \n    __m256 in_even[2], in_odd[2];\n    __m256 out_even[2], out_odd[2];\n    __m256 temp_even, temp_odd;\n    __m256i indices = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    \n    int i = 0;\n    for(; i <= n - 8; i += 8){\n        // Load data\n        in_even[0] = _mm256_loadu_ps(&l[i + 0]);\n        in_even[1] = _mm256_loadu_ps(&l[i + 2]);\n        in_even[2] = _mm256_loadu_ps(&l[i + 4]);\n        in_even[3] = _mm256_loadu_ps(&l[i + 6]);\n        \n        // Sort each group of 4 even elements\n        temp_even = _mm256_permutevar8x32_ps(in_even[0], indices);\n        out_even[0] = _mm256_min_ps(temp_even, in_even[0]);\n        out_even[1] = _mm256_max_ps(temp_even, in_even[0]);\n        \n        temp_even = _mm256_permutevar8x32_ps(in_even[1], indices);\n        out_even[2] = _mm256_min_ps(temp_even, in_even[1]);\n        out_even[3] = _mm256_max_ps(temp_even, in_even[1]);\n        \n        // Store sorted even elements back\n        _mm256_storeu_ps(&result[i + 0], out_even[0]);\n        _mm256_storeu_ps(&result[i + 2], out_even[1]);\n        _mm256_storeu_ps(&result[i + 4], out_even[2]);\n        _mm256_storeu_ps(&result[i + 6], out_even[3]);\n        \n        // Odd elements remain unchanged\n        _mm256_storeu_ps(&result[i + 1], _mm256_loadu_ps(&l[i + 1]));\n        _mm256_storeu_ps(&result[i + 3], _mm256_loadu_ps(&l[i + 3]));\n        _mm256_storeu_ps(&result[i + 5], _mm256_loadu_ps(&l[i + 5]));\n        _mm256_storeu_ps(&result[i + 7], _mm256_loadu_ps(&l[i + 7]));\n    }\n    \n    // Handle remaining elements\n    for(; i < n; ++i){\n        result[i] = l[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int n = l.size();\n    if(n < 3) return false;\n\n    __m256i zero_vec = _mm256_setzero_si256();\n    for(int i=0; i <= n-vec_size; i+=vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i v_sq = _mm256_mullo_epi32(v, v);\n        __m256i v_cubed = _mm256_mullo_epi32(v_sq, v);\n\n        if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(v, zero_vec)) != 0){\n            return true;\n        }\n        if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(v_sq, zero_vec)) != 0){\n            return true;\n        }\n        if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(v_cubed, zero_vec)) != 0){\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int n = l.size();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    if(n < 3)\n        return false;\n\n    for(int i=0; i <= n-vec_size; i+=vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i v_sq = _mm256_mullo_epi32(v, v);\n        __m256i v_cubed = _mm256_mullo_epi32(v_sq, v);\n\n        if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(v, zero_vec)) != 0 ||\n           _mm256_movemask_epi8(_mm256_cmpeq_epi32(v_sq, zero_vec)) != 0 ||\n           _mm256_movemask_epi8(_mm256_cmpeq_epi32(v_cubed, zero_vec)) != 0){\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int n = l.size();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    if(n < 3)\n        return false;\n\n    for(int i=0; i <= n-vec_size; i+=vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i v_sq = _mm256_mullo_epi32(v, v);\n        __m256i v_cubed = _mm256_mullo_epi32(v_sq, v);\n\n        if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(v, zero_vec)) != 0 ||\n           _mm256_movemask_epi8(_mm256_cmpeq_epi32(v_sq, zero_vec)) != 0 ||\n           _mm256_movemask_epi8(_mm256_cmpeq_epi32(v_cubed, zero_vec)) != 0){\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int n = l.size();\n    if(n < 3) return false;\n\n    __m256i zero_vec = _mm256_setzero_si256();\n    for(int i=0; i <= n-vec_size; i+=vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i v_sq = _mm256_mullo_epi32(v, v);\n        __m256i sum = _mm256_add_epi32(v, _mm256_permute2x128_si256(v, v, 1));\n        __m256i cmp = _mm256_cmpeq_epi32(sum, zero_vec);\n        if(_mm256_movemask_epi8(cmp) != 0){\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int n = l.size();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    if(n < 3)\n        return false;\n\n    for(int i=0; i <= n-vec_size; i+=vec_size){\n        __m256i a = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&l[i+1]);\n        __m256i c = _mm256_loadu_si256((__m256i*)&l[i+2]);\n\n        __m256i sum_abc = _mm256_add_epi32(a, _mm256_add_epi32(b, c));\n        \n        if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(sum_abc, zero_vec)) != 0)\n            return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t n = l.size();\n    std::vector<int> result(n);\n    \n    __m256i* in = (__m256i*)l.data();\n    __m256i* out = (__m256i*)result.data();\n    size_t i;\n    \n    for(i = 0; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_load_si256(in + i);\n        __m256i inc = _mm256_add_epi32(vec, _mm256_set1_epi32(1));\n        _mm256_store_si256(out + i, inc);\n    }\n    \n    for(; i < n; ++i){\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t n = l.size();\n    std::vector<int> result(n);\n    \n    __m256i* in = (__m256i*)l.data();\n    __m256i* out = (__m256i*)result.data();\n    size_t i;\n    \n    for(i = 0; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_load_si256(in + i);\n        __m256i inc = _mm256_add_epi32(vec, _mm256_set1_epi32(1));\n        _mm256_store_si256(out + i, inc);\n    }\n    \n    for(size_t j = i; j < n; ++j){\n        result[j] = l[j] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t n = l.size();\n    std::vector<int> result(n);\n    \n    __m256i* in = (__m256i*)l.data();\n    __m256i* out = (__m256i*)result.data();\n    size_t num_full_vectors = n / 8;\n\n    for(size_t i = 0; i < num_full_vectors; ++i){\n        __m256i vec = _mm256_load_si256(in + i);\n        __m256i incremented = _mm256_add_epi32(vec, _mm256_set1_epi32(1));\n        _mm256_store_si256(out + i, incremented);\n    }\n\n    // Handle remaining elements if any\n    size_t remainder = n % 8;\n    if(remainder > 0){\n        __m256i tail_vec = _mm256_loadu_si256(in + num_full_vectors);\n        __m256i mask = _mm256_setr_epi32(-1, -1, -1, -1, -1, -1, -1, 0);\n        __m256i masked_vec = _mm256_and_si256(tail_vec, mask);\n        __m256i incremented_tail = _mm256_add_epi32(masked_vec, _mm256_set1_epi32(1));\n        _mm256_maskstore_epi32((int*)(out + num_full_vectors), mask, incremented_tail);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t n = l.size();\n    std::vector<int> result(n);\n    \n    __m256i* in = (__m256i*)l.data();\n    __m256i* out = (__m256i*)result.data();\n    size_t num_full_vectors = n / 8;\n\n    for(size_t i = 0; i < num_full_vectors; ++i){\n        __m256i vec = _mm256_load_si256(in + i);\n        __m256i incremented = _mm256_add_epi32(vec, _mm256_set1_epi32(1));\n        _mm256_store_si256(out + i, incremented);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    size_t n = l.size();\n    std::vector<int> result(n);\n    \n    __m256i* in = (__m256i*)l.data();\n    __m256i* out = (__m256i*)result.data();\n    size_t i;\n    \n    for(i = 0; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_load_si256(in + i);\n        __m256i inc = _mm256_add_epi32(vec, _mm256_set1_epi32(1));\n        _mm256_store_si256(out + i, inc);\n    }\n    \n    for(size_t j = i; j < n; ++j){\n        result[j] = l[j] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int size = l.size();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for(int i=0; i < size; i += vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i neg_v = _mm256_sub_epi32(zero_vec, v);\n\n        for(int j=i+vec_size; j < size; j += vec_size){\n            __m256i w = _mm256_loadu_si256((__m256i*)&l[j]);\n            __m256i match = _mm256_cmpeq_epi32(neg_v, w);\n            \n            if(_mm256_movemask_epi8(match) != 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int size = l.size();\n    \n    if(size < 2)\n        return false;\n    \n    __m256i zero_vec = _mm256_setzero_si256();\n    std::vector<int> padded_list(l.begin(), l.end());\n    \n    // Pad the list to make its length a multiple of vec_size\n    while(padded_list.size() % vec_size != 0)\n        padded_list.push_back(0);\n    \n    int padded_size = padded_list.size();\n    for(int i=0; i<padded_size-vec_size+1; i+=vec_size){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&padded_list[i]);\n        __m256i neg_current_vec = _mm256_sub_epi32(zero_vec, current_vec);\n        \n        for(int j=i+vec_size; j<padded_size-vec_size+1; j+=vec_size){\n            __m256i compare_vec = _mm256_loadu_si256((__m256i*)&padded_list[j]);\n            __m256i mask = _mm256_cmpeq_epi32(compare_vec, neg_current_vec);\n            \n            if(_mm256_movemask_epi8(mask) != 0){\n                return true;\n            }\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int size = l.size();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for(int i=0; i<size; i+=vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i neg_v = _mm256_sub_epi32(zero_vec, v);\n        __m256i mask = _mm256_cmpgt_epi32(v, neg_v);\n        if(_mm256_movemask_epi8(mask) != 0){\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int size = l.size();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for(int i=0; i<size; i+=vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i neg_v = _mm256_sub_epi32(zero_vec, v);\n        __m256i mask = _mm256_cmpgt_epi32(v, neg_v);\n\n        int mask_int;\n        _mm256_storeu_si256((__m256i*)&mask_int, mask);\n        if(mask_int != 0){\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 8; // AVX size for int is 8\n    const int size = l.size();\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    for(int i=0; i<size; i+=vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i neg_v = _mm256_sub_epi32(zero_vec, v);\n        __m256i mask = _mm256_cmpgt_epi32(v, neg_v);\n\n        if(_mm256_movemask_epi8(mask) != 0){\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n\n    std::vector<char> digits;\n    while (x > 0) {\n        __m256i v_input = _mm256_set1_epi64x(x);\n        __m256i v_divisor = _mm256_set1_epi64x(base);\n        __m256i v_div = _mm256_div_epu64(v_input, v_divisor);\n        __m256i v_mod = _mm256_and_si256(_mm256_sub_epi64(v_input, _mm256_mul_epu64(v_div, v_divisor)), v_divisor);\n\n        int remainder[4];\n        _mm256_storeu_si256((__m256i*)remainder, v_mod);\n\n        for (int i = 0; i < 4; ++i) {\n            digits.push_back('0' + remainder[i]);\n            x = _mm256_extract_epi64(v_div, i);\n        }\n    }\n\n    std::reverse(digits.begin(), digits.end());\n    return std::string(digits.begin(), digits.end());\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <sstream>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    // AVX2 operates on 8 elements at once, so we need to handle 8 numbers in parallel\n    const int vec_size = 8;\n    __m256i dividend = _mm256_set1_epi64x(x);\n    __m256i divisor = _mm256_set1_epi8(base);\n    std::ostringstream oss[vec_size];\n    \n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi64(dividend, _mm256_setzero_si256())) != 0){\n        __m256i div_result = _mm256_div_epu32(dividend, divisor); // Integer division\n        __m256i mod_result = _mm256_and_si256(dividend, _mm256_sub_epi64(divisor, _mm256_setzero_si256()));\n        \n        // Convert remainders to characters and store in oss\n        for(int i=0; i<vec_size; ++i){\n            int remainder = mod_result.m256i_i8[i] + '0';\n            oss[i] << static_cast<char>(remainder);\n        }\n        \n        dividend = div_result;\n    }\n    \n    // Collect results from each vector element\n    std::string result;\n    for(int i=0; i<vec_size; ++i){\n        std::string temp = oss[i].str();\n        std::reverse(temp.begin(), temp.end());\n        result += temp;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (x == 0) return \"0\";\n    \n    std::vector<int> digits;\n    while (x > 0) {\n        __m256i dividend = _mm256_set1_epi64x(x);\n        __m256i divisor = _mm256_set1_epi64x(base);\n        __m256i div_result = _mm256_div_epu64(dividend, divisor);\n        __m256i mod_result = _mm256_and_si256(dividend, _mm256_sub_epi64(divisor, _mm256_set1_epi64x(1)));\n\n        int remainder[4];\n        _mm256_storeu_si256((__m256i*)remainder, mod_result);\n\n        for (int i = 0; i < 4; ++i) {\n            digits.push_back(remainder[i]);\n            x = _mm256_extract_epi64(div_result, i);\n        }\n    }\n\n    std::reverse(digits.begin(), digits.end());\n    std::string result;\n    for (auto digit : digits) {\n        result += std::to_string(digit);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text){\n    const int len = text.length();\n    if(len <= 1) return true;\n\n    const char* str = text.c_str();\n    int pos = 0;\n\n    while(pos + 32 <= len){\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + pos));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + len - pos - 32));\n\n        __m256i eq = _mm256_cmpeq_epi8(left, right);\n        if(!_mm256_movemask_epi8(eq)) {\n            return false;\n        }\n        pos += 32;\n    }\n\n    while(pos < len){\n        if(str[pos] != str[len - pos -1]){\n            return false;\n        }\n        pos++;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text){\n    const int len = text.length();\n    if(len <= 1) return true;\n\n    const char* str = text.c_str();\n    int pos = 0;\n\n    while(pos + 32 <= len){\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + pos));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + len - pos - 32));\n\n        __m256i eq = _mm256_cmpeq_epi8(left, right);\n        if(!_mm256_movemask_epi8(eq)) {\n            return false;\n        }\n        pos += 32;\n    }\n\n    while(pos < len){\n        if(str[pos] != str[len - pos - 1]){\n            return false;\n        }\n        pos++;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\nbool is_palindrome_simd(std::string text){\n    const char* str = text.c_str();\n    size_t len = text.length();\n    \n    if(len == 0)\n        return true;\n        \n    size_t i = 0;\n    __m256i loop_condition = _mm256_set1_epi32(0);\n    \n    while(i + 8 <= len){\n        __m256i left = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i right = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + len - i - 8));\n        __m256i cmp_result = _mm256_cmpeq_epi8(left, right);\n        loop_condition = _mm256_and_si256(loop_condition, cmp_result);\n        i += 8;\n    }\n    \n    // Handle remaining characters\n    while(i < len){\n        if(str[i] != str[len - i - 1])\n            return false;\n        i++;\n    }\n    \n    return _mm256_movemask_epi8(loop_condition) == 0xFFFFFFFF;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text){\n    const int len = text.length();\n    if(len <= 1) return true;\n\n    const char* str = text.c_str();\n    int pos = 0;\n\n    while(pos + 32 <= len){\n        __m256i v1 = _mm256_loadu_si256((__m256i*)(str + pos));\n        __m256i v2 = _mm256_loadu_si256((__m256i*)(str + len - pos - 32));\n        __m256i eq = _mm256_cmpeq_epi8(v1, v2);\n        if(!_mm256_movemask_epi8(eq)) return false;\n        pos += 32;\n    }\n\n    while(pos < len){\n        if(str[pos] != str[len - pos - 1]) return false;\n        pos++;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```cpp\nbool is_palindrome_simd(std::string text){\n    const char* str = text.c_str();\n    size_t len = text.length();\n    \n    if(len == 0)\n        return true;\n    \n    size_t i = 0;\n    __m256i loop_condition = _mm256_set1_epi32(0);\n    \n    // Load the first half of the string into a vector\n    __m256i forward = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n    __m256i backward = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + len - 8 - i));\n    \n    // Compare characters from both ends\n    __m256i cmp_result = _mm256_cmpeq_epi8(forward, backward);\n    \n    // Check if all characters match\n    __m256i mask = _mm256_set1_epi32(0xFF);\n    int mask_result = _mm256_movemask_epi8(cmp_result);\n    \n    if(mask_result != 0xFFFF){\n        return false;\n    }\n    \n    i += 8;\n    \n    while(i < len / 2){\n        forward = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        backward = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + len - 8 - i));\n        \n        cmp_result = _mm256_cmpeq_epi8(forward, backward);\n        mask_result = _mm256_movemask_epi8(cmp_result);\n        \n        if(mask_result != 0xFFFF){\n            return false;\n        }\n        \n        i += 8;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m256i shift = _mm256_set1_epi8(5);\n    const __m256i base = _mm256_set1_epi8('a');\n    const __m256i limit = _mm256_set1_epi8('z');\n\n    size_t len = s.size();\n    std::string out;\n    out.reserve(len);\n\n    for (size_t i = 0; i + 32 <= len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i decoded = _mm256_sub_epi8(vec, shift);\n\n        // Handle wrap-around for lowercase letters\n        __m256i mask = _mm256_cmpgt_epi8(base, decoded);\n        decoded = _mm256_sub_epi8(decoded, _mm256_and_si256(mask, _mm256_set1_epi8(26)));\n\n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(out.resize(out.size() + 32).data()), decoded);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - (len % 32); i < len; ++i) {\n        char c = s[i];\n        int w = ((c - 'a' - 5 + 26) % 26) + 'a';\n        out += static_cast<char>(w);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\nstd::string decode_shift_simd(std::string s){\n    std::string out;\n    int len = s.length();\n    __m256i key = _mm256_set1_epi8((char)(-5));\n    __m256i base = _mm256_set1_epi8((char)'a');\n    __m256i mask = _mm256_set1_epi8(0x1f);\n    int i = 0;\n\n    while(i + 8 <= len){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i res = _mm256_add_epi8(vec, key);\n        __m256i mod = _mm256_and_si256(res, mask);\n        __m256i cmp = _mm256_cmpgt_epi8(base, mod);\n        __m256i adj = _mm256_sub_epi8(cmp, _mm256_set1_epi8((char)26));\n        __m256i final = _mm256_add_epi8(mod, adj);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), final);\n        i += 32;\n    }\n\n    for(; i < len; ++i){\n        char c = s[i];\n        int w = ((c - 'a' - 5 + 26) % 26) + 'a';\n        out += (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m256i shift = _mm256_set1_epi8(5);\n    const __m256i base = _mm256_set1_epi8('a');\n    const __m256i limit = _mm256_set1_epi8('z');\n\n    size_t len = s.size();\n    std::string out;\n    out.reserve(len);\n\n    for (size_t i = 0; i + 32 <= len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i decoded = _mm256_sub_epi8(vec, shift);\n\n        // Handle wrap-around for lowercase letters\n        __m256i mask = _mm256_cmpgt_epi8(base, decoded);\n        decoded = _mm256_sub_epi8(decoded, _mm256_andnot_si256(mask, _mm256_set1_epi8(26)));\n\n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), decoded);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - (len % 32); i < len; ++i) {\n        char c = s[i] - 5;\n        if (c < 'a') c += 26;\n        out += c;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m256i shift = _mm256_set1_epi8(5);\n    const __m256i base = _mm256_set1_epi8('a');\n    const __m256i limit = _mm256_set1_epi8('z');\n\n    size_t len = s.size();\n    std::string out;\n    out.reserve(len);\n\n    for (size_t i = 0; i + 32 <= len; i += 32) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i decoded = _mm256_sub_epi8(vec, shift);\n\n        // Handle wrap-around for lowercase letters\n        __m256i mask = _mm256_cmpgt_epi8(base, decoded);\n        decoded = _mm256_sub_epi8(decoded, _mm256_andnot_si256(mask, _mm256_set1_epi8(26)));\n\n        // Store the result\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(out.data() + i), decoded);\n    }\n\n    // Handle remaining characters\n    for (size_t i = len - (len % 32); i < len; ++i) {\n        char c = s[i];\n        int w = ((c - 'a' - 5 + 26) % 26) + 'a';\n        out += static_cast<char>(w);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```cpp\nstd::string decode_shift_simd(std::string s){\n    std::string out;\n    if(s.empty()) return out;\n    size_t len = s.length();\n    size_t i = 0;\n\n    __m256i key = _mm256_set1_epi8((char)(-5));\n    __m256i base = _mm256_set1_epi8((char)'a');\n    __m256i mod = _mm256_set1_epi8((char)26);\n\n    while(i + 32 <= len){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        __m256i res = _mm256_adds_epu8(vec, key);\n        res = _mm256_sub_epi8(res, base);\n        res = _mm256_or_si256(res, base);\n        res = _mm256_sub_epi8(res, mod);\n        res = _mm256_or_si256(res, base);\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&out[i]), res);\n        i += 32;\n    }\n\n    // Handle remaining characters\n    while(i < len){\n        char c = s[i];\n        char w = ((c - 'a' - 5 + 26) % 26) + 'a';\n        out += w;\n        i++;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.size() < 8) return true;\n\n    const int n = l.size();\n    const int* data = l.data();\n\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i mask_all = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        mask_all = _mm256_or_si256(mask_all, cmp);\n    }\n\n    int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(mask_all));\n    mask |= _mm256_movemask_epi8(_mm256_extracti128_si256(mask_all, 1));\n\n    return mask == 0;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    const int vec_size = 8; // AVX size for int is 8\n    const int size = l.size();\n    __m256i threshold = _mm256_set1_epi32(t);\n\n    for (int i = 0; i < size; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        unsigned int mask = _mm256_movemask_epi8(cmp);\n        if (mask != 0) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.size() < 8) return true;\n\n    const __m256i threshold = _mm256_set1_epi32(t);\n    const int size = l.size();\n    const int remainder = size % 8;\n    const int limit = size - remainder;\n\n    for (int i = 0; i < limit; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        if (!_mm256_testz_si256(cmp, cmp)) {\n            return false;\n        }\n    }\n\n    // Handle remaining elements\n    for (int i = limit; i < size; ++i) {\n        if (l[i] >= t) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    const int vec_size = 8; // AVX2: 8 integers per register\n    const int size = l.size();\n    \n    if(size == 0) return true;\n    \n    int i = 0;\n    __m256i threshold_vec = _mm256_set1_epi32(t);\n    \n    // Process full vectors\n    for(; i <= size - vec_size; i += vec_size) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i cmp_result = _mm256_cmpgt_epi32(vec, threshold_vec);\n        if(!_mm256_testz_si256(cmp_result, cmp_result)) {\n            return false;\n        }\n    }\n    \n    // Handle remaining elements\n    for(; i < size; ++i) {\n        if(l[i] >= t) {\n            return false;\n        }\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool below_threshold_simd(const std::vector<int>& l, int t) {\n    if (l.size() < 8) return true;\n\n    const int n = l.size();\n    const int* data = l.data();\n\n    __m256i threshold = _mm256_set1_epi32(t);\n    __m256i mask_all = _mm256_setzero_si256();\n\n    for (int i = 0; i <= n - 8; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&data[i]));\n        __m256i cmp = _mm256_cmpgt_epi32(vec, threshold);\n        mask_all = _mm256_or_si256(mask_all, cmp);\n    }\n\n    int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(mask_all));\n    mask |= _mm256_movemask_epi8(_mm256_extracti128_si256(mask_all, 1));\n\n    return mask == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const char* str = brackets.c_str();\n    size_t len = brackets.length();\n    __m256i open_mask = _mm256_set1_epi8('<');\n    __m256i close_mask = _mm256_set1_epi8('>');\n    __m256i open_count = _mm256_setzero_si256();\n    __m256i close_count = _mm256_setzero_si256();\n\n    for(size_t i=0; i<len; i+=8){\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i open = _mm256_cmpeq_epi8(chars, open_mask);\n        __m256i close = _mm256_cmpeq_epi8(chars, close_mask);\n\n        open_count = _mm256_add_epi8(open_count, open);\n        close_count = _mm256_add_epi8(close_count, close);\n    }\n\n    __m256i total_open = _mm256_setzero_si256();\n    __m256i total_close = _mm256_setzero_si256();\n\n    for(int j=0; j<4; ++j){\n        __m128i low = _mm256_extractf128_si256(open_count, j);\n        __m128i high = _mm256_extractf128_si256(open_count, j+1);\n        open_count = _mm256_insertf128_si256(open_count, _mm_add_epi8(low, high), 1);\n\n        low = _mm256_extractf128_si256(close_count, j);\n        high = _mm256_extractf128_si256(close_count, j+1);\n        close_count = _mm256_insertf128_si256(close_count, _mm_add_epi8(low, high), 1);\n\n        total_open = _mm256_add_epi8(total_open, open_count);\n        total_close = _mm256_add_epi8(total_close, close_count);\n    }\n\n    int final_open = _mm_cvtsi128_si32(_mm256_castsi256_si128(total_open));\n    int final_close = _mm_cvtsi128_si32(_mm256_castsi256_si128(total_close));\n\n    return final_open == final_close;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_len = 32; // AVX: 8 x 4-byte integers\n    const __m256i open_mask = _mm256_set1_epi32(0x3C3C3C3C); // \"<\" repeated 8 times\n    const __m256i close_mask = _mm256_set1_epi32(0x3E3E3E3E); // \">\" repeated 8 times\n    const __m256i zero_vec = _mm256_setzero_si256();\n    \n    int len = brackets.size();\n    if(len == 0) return true;\n    \n    int i = 0;\n    __m256i accum = zero_vec;\n    \n    while(i + vec_len <= len){\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i opens = _mm256_cmpeq_epi32(chunk, open_mask);\n        __m256i closes = _mm256_cmpeq_epi32(chunk, close_mask);\n        \n        accum = _mm256_add_epi32(accum, _mm256_sub_epi32(opens, closes));\n        i += vec_len;\n    }\n    \n    // Handle remaining characters\n    while(i < len){\n        if(brackets[i] == '<') accum = _mm256_add_epi32(accum, _mm256_set1_epi32(1));\n        else if(brackets[i] == '>') accum = _mm256_add_epi32(accum, _mm256_set1_epi32(-1));\n        i++;\n    }\n    \n    __m256i mask = _mm256_cmpgt_epi32(zero_vec, accum);\n    int sum = _mm256_movemask_epi8(mask);\n    return sum == 0;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_len = 32; // AVX: 8 x 4-byte integers\n    const __m256i open_mask = _mm256_set1_epi8('<');\n    const __m256i close_mask = _mm256_set1_epi8('>');\n    const __m256i zero_vec = _mm256_setzero_si256();\n    const __m256i one_vec = _mm256_set1_epi8(1);\n    \n    int len = brackets.size();\n    if(len == 0) return true;\n    \n    int pos = 0;\n    __m256i accum = zero_vec;\n    \n    while(pos + vec_len <= len){\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[pos]));\n        \n        __m256i opens = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i closes = _mm256_cmpeq_epi8(chunk, close_mask);\n        \n        accum = _mm256_add_epi8(accum, _mm256_sub_epi8(opens, closes));\n        \n        __m256i check_zero = _mm256_abs_epi8(_mm256_sub_epi8(accum, zero_vec));\n        if(!_mm256_testz_si256(check_zero, check_zero)){\n            return false;\n        }\n        \n        pos += vec_len;\n    }\n    \n    // Handle remaining characters\n    while(pos < len){\n        char c = brackets[pos];\n        if(c == '<') accum = _mm256_add_epi8(accum, one_vec);\n        else if(c == '>') accum = _mm256_sub_epi8(accum, one_vec);\n        pos++;\n    }\n    \n    __m256i check_final = _mm256_abs_epi8(_mm256_sub_epi8(accum, zero_vec));\n    return _mm256_testz_si256(check_final, check_final);\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_len = 32; // AVX: 8 x 4-byte integers\n    const __m256i open_mask = _mm256_set1_epi8('<');\n    const __m256i close_mask = _mm256_set1_epi8('>');\n    const __m256i zero_vec = _mm256_setzero_si256();\n    const __m256i one_vec = _mm256_set1_epi8(1);\n    \n    int len = brackets.size();\n    if(len == 0) return true;\n    \n    int pos = 0;\n    __m256i accum = zero_vec;\n    \n    while(pos + vec_len <= len){\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[pos]));\n        \n        __m256i opens = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i closes = _mm256_cmpeq_epi8(chunk, close_mask);\n        \n        accum = _mm256_add_epi8(accum, _mm256_sub_epi8(opens, closes));\n        \n        __m256i check_zero = _mm256_abs_epi8(_mm256_sub_epi8(accum, zero_vec));\n        if(!_mm256_testz_si256(check_zero, check_zero)) {\n            return false;\n        }\n        \n        pos += vec_len;\n    }\n    \n    // Handle remaining characters\n    while(pos < len){\n        char c = brackets[pos];\n        if(c == '<') accum += 1;\n        else if(c == '>') accum -= 1;\n        if(accum > vec_len || accum < -vec_len){\n            return false;\n        }\n        pos++;\n    }\n    \n    return accum == zero_vec;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const char *s = brackets.c_str();\n    size_t len = brackets.length();\n    __m256i open = _mm256_setzero_si256();\n    size_t i = 0;\n\n    while (i + 8 <= len) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s + i));\n        __m256i cmp_lt = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('<'));\n        __m256i cmp_gt = _mm256_cmpgt_epi8(_mm256_set1_epi8('>'), chars);\n        __m256i mask = _mm256_andnot_si256(cmp_lt, cmp_gt);\n\n        open = _mm256_sub_epi8(open, _mm256_and_si256(mask, _mm256_set1_epi8(1)));\n        open = _mm256_add_epi8(open, _mm256_andnot_si256(mask, _mm256_set1_epi8(-1)));\n\n        i += 32;\n    }\n\n    int final_open = _mm256_extract_epi32(open, 0);\n    final_open += _mm256_extract_epi32(open, 1);\n    final_open += _mm256_extract_epi32(open, 2);\n    final_open += _mm256_extract_epi32(open, 3);\n\n    return final_open == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const char *str = brackets.c_str();\n    size_t len = brackets.length();\n    \n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i zero = _mm256_setzero_si256();\n    __m256i open_count = zero;\n    __m256i close_count = zero;\n\n    for(size_t i = 0; i < len; i += 32){\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i open_cmp = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i close_cmp = _mm256_cmpeq_epi8(chunk, close_mask);\n\n        open_count = _mm256_add_epi8(open_count, open_cmp);\n        close_count = _mm256_add_epi8(close_count, close_cmp);\n    }\n\n    __m256i total_open = _mm256_hadd_epi8(open_count, open_count);\n    __m256i total_close = _mm256_hadd_epi8(close_count, close_count);\n\n    int sum_open[8];\n    int sum_close[8];\n    _mm256_storeu_si256((__m256i*)sum_open, total_open);\n    _mm256_storeu_si256((__m256i*)sum_close, total_close);\n\n    int open_total = sum_open[0] + sum_open[1] + sum_open[2] + sum_open[3] + sum_open[4] + sum_open[5] + sum_open[6] + sum_open[7];\n    int close_total = sum_close[0] + sum_close[1] + sum_close[2] + sum_close[3] + sum_close[4] + sum_close[5] + sum_close[6] + sum_close[7];\n\n    return open_total == close_total;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const char* str = brackets.c_str();\n    size_t len = brackets.length();\n\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i open_count = _mm256_setzero_si256();\n    __m256i close_count = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < len; i += 32) {\n        __m256i chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i open_match = _mm256_cmpeq_epi8(chunk, open_mask);\n        __m256i close_match = _mm256_cmpeq_epi8(chunk, close_mask);\n\n        open_count = _mm256_add_epi8(open_count, open_match);\n        close_count = _mm256_add_epi8(close_count, close_match);\n    }\n\n    __m256i total_open = _mm256_setzero_si256();\n    __m256i total_close = _mm256_setzero_si256();\n\n    for (int j = 0; j < 4; ++j) {\n        __m64 low = _mm256_extractf128_si256(open_count, j);\n        __m64 high = _mm256_extractf128_si256(close_count, j);\n        total_open = _mm_add_pi8(total_open, _mm_cvtsi64_si128(_mm_cvtsi128_si64(low)));\n        total_close = _mm_add_pi8(total_close, _mm_cvtsi64_si128(_mm_cvtsi128_si64(high)));\n    }\n\n    int count_open = _mm_cvtsi128_si32(total_open);\n    int count_close = _mm_cvtsi128_si32(total_close);\n\n    return count_open == count_close;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.size();\n    if (len == 0) return true;\n\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i balance = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.c_str() + i));\n        __m256i open = _mm256_cmpeq_epi8(chars, open_mask);\n        __m256i close = _mm256_cmpeq_epi8(chars, close_mask);\n\n        balance = _mm256_add_epi8(balance, _mm256_sub_epi8(open, close));\n    }\n\n    int final_balance[32];\n    _mm256_storeu_si256((__m256i*)final_balance, balance);\n\n    int total_balance = 0;\n    for (int j = 0; j < 32; ++j) {\n        total_balance += final_balance[j];\n    }\n\n    return total_balance == 0 && (len % 32 == 0);\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    size_t len = brackets.size();\n    if (len == 0) return true;\n\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i balance = _mm256_setzero_si256();\n\n    for (size_t i = 0; i + 32 <= len; i += 32) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(brackets.data() + i));\n        __m256i open = _mm256_cmpeq_epi8(chars, open_mask);\n        __m256i close = _mm256_cmpeq_epi8(chars, close_mask);\n\n        balance = _mm256_add_epi8(balance, _mm256_sub_epi8(open, close));\n    }\n\n    balance = _mm256_hadd_epi16(balance, balance);\n    balance = _mm256_hadd_epi32(balance, balance);\n    int result = _mm256_extract_epi32(balance, 0);\n\n    return result == 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(const std::string& brackets) {\n    const char* str = brackets.c_str();\n    size_t len = brackets.length();\n\n    // Handle cases where length is less than 32 (AVX2 vector width in bytes)\n    if (len < 32) {\n        int balance = 0;\n        for (size_t i = 0; i < len; ++i) {\n            if (str[i] == '(') balance++;\n            else if (str[i] == ')') balance--;\n            if (balance < 0) return false;\n        }\n        return balance == 0;\n    }\n\n    __m256i open_mask = _mm256_set1_epi8('(');\n    __m256i close_mask = _mm256_set1_epi8(')');\n    __m256i balance = _mm256_setzero_si256();\n    __m256i one = _mm256_set1_epi8(1);\n\n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i open_cmp = _mm256_cmpeq_epi8(chars, open_mask);\n        __m256i close_cmp = _mm256_cmpeq_epi8(chars, close_mask);\n\n        __m256i open_bits = _mm256_cvtepi8_epi32(open_cmp);\n        __m256i close_bits = _mm256_cvtepi8_epi32(close_cmp);\n\n        __m256i diff = _mm256_sub_epi32(open_bits, close_bits);\n        balance = _mm256_add_epi32(balance, diff);\n\n        // Check for negative balance\n        __m256i neg_check = _mm256_cmpgt_epi32(_mm256_setzero_si256(), balance);\n        if (_mm256_movemask_epi8(neg_check)) return false;\n    }\n\n    // Handle remaining characters\n    int final_balance = 0;\n    for (; i < len; ++i) {\n        if (str[i] == '(') final_balance++;\n        else if (str[i] == ')') final_balance--;\n        if (final_balance < 0) return false;\n    }\n\n    return final_balance == _mm256_extract_epi32(balance, 0);\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    std::vector<float> result;\n    result.reserve(n - 1);\n    \n    const __m256 zero = _mm256_setzero_ps();\n    const __m256 factor = _mm256_set1_ps(1.0f);\n    \n    for(int i = 0; i < n - 1; i += 8){\n        __m256 coeffs = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_setr_ps(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256 multiplied = _mm256_mul_ps(coeffs, indices);\n        __m256 shifted = _mm256_permutevar8x32_ps(multiplied, _mm256_setr_epi32(1,2,3,4,5,6,7,0));\n        __m256 summed = _mm256_add_ps(multiplied, shifted);\n        __m256 final = _mm256_fmadd_ps(factor, summed, zero);\n        \n        float res[8];\n        _mm256_storeu_ps(res, final);\n        for(int j = 0; j < 8 && i+j < n-1; ++j){\n            result.push_back(res[j]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    std::vector<float> result;\n    result.reserve(n - 1);\n\n    for(int i = 0; i < n - 4; i += 4){\n        __m256 coeffs = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f);\n        __m256 deriv = _mm256_mul_ps(coeffs, indices);\n        float temp[8] __attribute__((aligned(32)));\n        _mm256_store_ps(temp, deriv);\n        result.insert(result.end(), temp, temp + 4);\n    }\n\n    for(int i = n - (n % 4); i < n - 1; ++i){\n        result.push_back(xs[i] * static_cast<float>(i + 1));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    std::vector<float> result;\n    result.reserve(n - 1);\n\n    __m256 coeff_vec;\n    __m256 mul_vec = _mm256_set1_ps(1.0f);\n    int i = 0;\n\n    // Process full chunks of 8 coefficients\n    for (; i <= n - 8; i += 8) {\n        coeff_vec = _mm256_loadu_ps(&xs[i]);\n        coeff_vec = _mm256_mul_ps(coeff_vec, mul_vec);\n        mul_vec = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);\n        coeff_vec = _mm256_mul_ps(coeff_vec, mul_vec);\n        _mm256_storeu_ps(&result[i], coeff_vec);\n    }\n\n    // Handle remaining coefficients\n    for (; i < n; ++i) {\n        result.push_back(xs[i] * static_cast<float>(i));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    std::vector<float> result;\n    result.reserve(n - 1);\n\n    for(int i = 0; i < n - 4; i += 4){\n        __m256 vec = _mm256_loadu_ps(&xs[i]);\n        __m256 indices = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f);\n        __m256 deriv = _mm256_mul_ps(vec, indices);\n        float res[4];\n        _mm256_storeu_ps(res, deriv);\n        result.insert(result.end(), res, res + 4);\n    }\n\n    for(int i = n - (n % 4); i < n - 1; ++i){\n        result.push_back(xs[i] * static_cast<float>(i + 1));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    std::vector<float> result;\n    result.reserve(n - 1);\n\n    __m256 coeff_vec;\n    __m256 mul_vec = _mm256_set1_ps(1.0f);\n    int i = 0;\n\n    // Process coefficients in chunks of 8 using AVX\n    for (; i <= n - 8; i += 8) {\n        coeff_vec = _mm256_loadu_ps(&xs[i]);\n        __m256 index_vec = _mm256_setr_ps(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f, 7.0f, 8.0f);\n        __m256 mul_result = _mm256_mul_ps(coeff_vec, index_vec);\n        _mm256_storeu_ps(&result[i], mul_result);\n    }\n\n    // Handle remaining coefficients if any\n    for (; i < n; ++i) {\n        result.push_back(xs[i] * static_cast<float>(i));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c\nint vowels_count_simd(std::string s){\n    const int vec_len = 32; // AVX2 operates on 256-bit vectors, 32 bytes\n    const char *data = reinterpret_cast<const char*>(s.c_str());\n    int count = 0;\n    __m256i mask_a = _mm256_set1_epi8('a');\n    __m256i mask_e = _mm256_set1_epi8('e');\n    __m256i mask_i = _mm256_set1_epi8('i');\n    __m256i mask_o = _mm256_set1_epi8('o');\n    __m256i mask_u = _mm256_set1_epi8('u');\n    __m256i mask_y = _mm256_set1_epi8('y');\n    \n    while (*data) {\n        __m256i str_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data));\n        __m256i lower_vec = _mm256_or_si256(_mm256_or_si256(_mm256_or_si256(\n            _mm256_or_si256(_mm256_cmpeq_epi8(_mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF)), mask_a),\n                           _mm256_cmpeq_epi8(_mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF)), mask_e)),\n                           _mm256_cmpeq_epi8(_mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF)), mask_i)),\n                           _mm256_cmpeq_epi8(_mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF)), mask_o)),\n                           _mm256_cmpeq_epi8(_mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF)), mask_u));\n        \n        __m256i y_vec = _mm256_cmpeq_epi8(_mm256_and_si256(str_vec, _mm256_set1_epi8(0xDF)), mask_y);\n        __m256i y_end_check = _mm256_cmpgt_epi8(_mm256_setzero_si256(), _mm256_sub_epi8(_mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + 31)), mask_y));\n        \n        __m256i total_mask = _mm256_or_si256(lower_vec, _mm256_and_si256(y_vec, y_end_check));\n        count += _mm256_movemask_epi8(total_mask);\n        \n        data += vec_len;\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c\nint vowels_count_simd(std::string s){\n    const __m256i v_vowels = _mm256_setr_epi8(\n        1,0,0,0, 1,0,0,0, 1,0,0,0, 1,0,0,0, // a,e,i,o\n        1,0,0,0, 1,0,0,0, 1,0,0,0, 0,0,0,0   // u,y\n    );\n    int count = 0;\n    const char *data = s.c_str();\n    size_t len = s.length();\n\n    for(size_t i=0; i<len; i+=32){\n        __m256i chunk;\n        if(i + 32 <= len){\n            chunk = _mm256_loadu_si256((__m256i const*)(data + i));\n        }\n        else{\n            chunk = _mm256_setzero_si256();\n            for(size_t j=0; j<32; ++j){\n                if(i+j >= len) break;\n                chunk = _mm256_insert_epi8(chunk, data[i+j], j);\n            }\n        }\n\n        __m256i mask_a = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('a'));\n        __m256i mask_e = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('e'));\n        __m256i mask_i = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('i'));\n        __m256i mask_o = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('o'));\n        __m256i mask_u = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('u'));\n        __m256i mask_y = _mm256_cmpeq_epi8(chunk, _mm256_set1_epi8('y'));\n\n        __m256i mask_end = _mm256_cmpgt_epi8(_mm256_set1_epi8(len - i), _mm256_setr_epi8(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31));\n\n        __m256i total_mask = _mm256_or_si256(mask_a, mask_e);\n        total_mask = _mm256_or_si256(total_mask, mask_i);\n        total_mask = _mm256_or_si256(total_mask, mask_o);\n        total_mask = _mm256_or_si256(total_mask, mask_u);\n        total_mask = _mm256_or_si256(total_mask, mask_y);\n\n        total_mask = _mm256_and_si256(total_mask, mask_end);\n\n        count += _mm256_popcnt_epi8(total_mask);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c\nint vowels_count_simd(std::string s){\n    const __m256i mask_a = _mm256_set1_epi8('a');\n    const __m256i mask_e = _mm256_set1_epi8('e');\n    const __m256i mask_i = _mm256_set1_epi8('i');\n    const __m256i mask_o = _mm256_set1_epi8('o');\n    const __m256i mask_u = _mm256_set1_epi8('u');\n    const __m256i mask_y = _mm256_set1_epi8('y');\n    const __m256i mask_A = _mm256_set1_epi8('A');\n    const __m256i mask_E = _mm256_set1_epi8('E');\n    const __m256i mask_I = _mm256_set1_epi8('I');\n    const __m256i mask_O = _mm256_set1_epi8('O');\n    const __m256i mask_U = _mm256_set1_epi8('U');\n    const __m256i mask_Y = _mm256_set1_epi8('Y');\n    int count = 0;\n    int i = 0;\n    while(i + 8 <= s.size()){\n        __m256i chars = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i lower_chars = _mm256_or_si256(_mm256_andnot_si256(_mm256_set1_epi8(0x20), chars), _mm256_set1_epi8(0x20));\n        __m256i vowels_mask = _mm256_or_si256(\n            _mm256_cmpgt_epi8(lower_chars, mask_a),\n            _mm256_or_si256(\n                _mm256_cmpgt_epi8(lower_chars, mask_e),\n                _mm256_or_si256(\n                    _mm256_cmpgt_epi8(lower_chars, mask_i),\n                    _mm256_or_si256(\n                        _mm256_cmpgt_epi8(lower_chars, mask_o),\n                        _mm256_cmpgt_epi8(lower_chars, mask_u)\n                    )\n                )\n            )\n        );\n        __m256i y_mask = _mm256_cmpeq_epi8(lower_chars, _mm256_setr_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'y'));\n        __m256i y_end_mask = _mm256_cmpeq_epi8(_mm256_sub_epi8(_mm256_set1_epi8(s.size()), _mm256_add_epi8(_mm256_set1_epi8(i), _mm256_set1_epi8(8))), _mm256_set1_epi8(0));\n        y_mask = _mm256_and_si256(y_mask, y_end_mask);\n        vowels_mask = _mm256_or_si256(vowels_mask, y_mask);\n        count += _mm256_movemask_epi8(vowels_mask);\n        i += 8;\n    }\n    for(; i < s.size(); ++i){\n        char c = s[i];\n        if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n           c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U'){\n            count++;\n        }\n        else if(c == 'y' && i == s.size() - 1){\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c\nint vowels_count_simd(std::string s){\n    const __m256i mask_a = _mm256_set1_epi8('a');\n    const __m256i mask_e = _mm256_set1_epi8('e');\n    const __m256i mask_i = _mm256_set1_epi8('i');\n    const __m256i mask_o = _mm256_set1_epi8('o');\n    const __m256i mask_u = _mm256_set1_epi8('u');\n    const __m256i mask_y = _mm256_set1_epi8('y');\n    const __m256i mask_A = _mm256_set1_epi8('A');\n    const __m256i mask_E = _mm256_set1_epi8('E');\n    const __m256i mask_I = _mm256_set1_epi8('I');\n    const __m256i mask_O = _mm256_set1_epi8('O');\n    const __m256i mask_U = _mm256_set1_epi8('U');\n    const __m256i mask_Y = _mm256_set1_epi8('Y');\n    int count = 0;\n    int i = 0;\n    while(i + 8 <= s.size()){\n        __m256i chars = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i lower_chars = _mm256_or_si256(_mm256_andnot_si256(_mm256_set1_epi8(0x20), chars), _mm256_set1_epi8(0x20));\n        __m256i vowels_mask = _mm256_or_si256(_mm256_or_si256(_mm256_cmpeq_epi8(lower_chars, mask_a),\n                                                             _mm256_cmpeq_epi8(lower_chars, mask_e)),\n                                               _mm256_or_si256(_mm256_cmpeq_epi8(lower_chars, mask_i),\n                                                               _mm256_cmpeq_epi8(lower_chars, mask_o)));\n        __m256i y_mask = _mm256_cmpeq_epi8(lower_chars, mask_y);\n        __m256i last_char_mask = _mm256_cmpgt_epi8(_mm256_setzero_si256(), _mm256_sub_epi8(_mm256_set1_epi8(s.size()), _mm256_setr_epi8(i, i, i, i, i, i, i, i)));\n        y_mask = _mm256_and_si256(y_mask, last_char_mask);\n        vowels_mask = _mm256_or_si256(vowels_mask, y_mask);\n        count += _mm256_movemask_epi8(vowels_mask);\n        i += 8;\n    }\n    for(; i < s.size(); ++i){\n        if(s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u' ||\n           s[i] == 'A' || s[i] == 'E' || s[i] == 'I' || s[i] == 'O' || s[i] == 'U'){\n            count++;\n        }\n        else if(s[i] == 'y' || s[i] == 'Y') {\n            if(i == s.size() - 1){\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c\nint vowels_count_simd(std::string s){\n    const __m256i mask_a = _mm256_set1_epi8('a');\n    const __m256i mask_e = _mm256_set1_epi8('e');\n    const __m256i mask_i = _mm256_set1_epi8('i');\n    const __m256i mask_o = _mm256_set1_epi8('o');\n    const __m256i mask_u = _mm256_set1_epi8('u');\n    const __m256i mask_y = _mm256_set1_epi8('y');\n    const __m256i mask_A = _mm256_set1_epi8('A');\n    const __m256i mask_E = _mm256_set1_epi8('E');\n    const __m256i mask_I = _mm256_set1_epi8('I');\n    const __m256i mask_O = _mm256_set1_epi8('O');\n    const __m256i mask_U = _mm256_set1_epi8('U');\n    const __m256i mask_Y = _mm256_set1_epi8('Y');\n    int count = 0;\n    int i = 0;\n    while(i + 8 <= s.size()){\n        __m256i chars = _mm256_loadu_si256((__m256i*)(s.data() + i));\n        __m256i lower_chars = _mm256_or_si256(_mm256_andnot_si256(_mm256_set1_epi8(0x20), chars), _mm256_set1_epi8(0x20));\n        __m256i vowels_mask = _mm256_or_si256(\n            _mm256_cmpgt_epi8(lower_chars, mask_a),\n            _mm256_or_si256(\n                _mm256_cmpgt_epi8(lower_chars, mask_e),\n                _mm256_or_si256(\n                    _mm256_cmpgt_epi8(lower_chars, mask_i),\n                    _mm256_or_si256(\n                        _mm256_cmpgt_epi8(lower_chars, mask_o),\n                        _mm256_cmpgt_epi8(lower_chars, mask_u)\n                    )\n                )\n            )\n        );\n        __m256i y_mask = _mm256_cmpeq_epi8(lower_chars, _mm256_setr_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 'y'));\n        __m256i y_end_mask = _mm256_and_si256(y_mask, _mm256_set1_epi8(0x01));\n        count += _mm256_movemask_epi8(vowels_mask);\n        count += _mm256_movemask_epi8(y_end_mask);\n        i += 8;\n    }\n    for(; i < s.size(); ++i){\n        char c = s[i];\n        if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u' ||\n           c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U'){\n            count++;\n        }\n        else if(c == 'y' || c == 'Y') {\n            if(i == s.size() - 1){\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\nint digitSum_simd(std::string s){\n    const int vec_len = 32; // AVX2 vector length in bytes\n    int sum = 0;\n    __m256i upper_mask = _mm256_set1_epi8(0x20); // Mask to check for uppercase letters\n    __m256i zero_check = _mm256_setzero_si256(); // Zero vector for comparison\n    __m256i ascii_upper_a = _mm256_set1_epi8('A'); // 'A' ASCII value\n    __m256i ascii_upper_z = _mm256_set1_epi8('Z'); // 'Z' ASCII value\n\n    int i = 0;\n    while(i + vec_len <= s.size()){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&s[i]));\n        \n        // Apply mask to convert to lowercase equivalent\n        __m256i lower_vec = _mm256_xor_si256(vec, upper_mask);\n        \n        // Create a mask for uppercase letters\n        __m256i upper_mask_vec = _mm256_cmpgt_epi8(_mm256_sub_epi8(lower_vec, ascii_upper_a), zero_check);\n        upper_mask_vec = _mm256_and_si256(upper_mask_vec, _mm256_cmpgt_epi8(ascii_upper_z, lower_vec));\n        \n        // Extract ASCII values of uppercase letters\n        __m256i ascii_values = _mm256_add_epi8(lower_vec, upper_mask);\n        \n        // Multiply by the mask to zero out non-uppercase letters\n        ascii_values = _mm256_mullo_epi16(ascii_values, upper_mask_vec);\n        \n        // Horizontal sum of ASCII values\n        __m256i sum_vec = _mm256_setzero_si256();\n        for(int j = 0; j < vec_len; j += 4){\n            __m128i low = _mm256_extractf128_si256(ascii_values, j / 4);\n            __m128i high = _mm_unpackhi_epi64(low, low);\n            __m128i sum_low = _mm_add_epi32(_mm_cvtsi128_si32(low), _mm_extract_epi32(low, 1));\n            __m128i sum_high = _mm_add_epi32(_mm_cvtsi128_si32(high), _mm_extract_epi32(high, 1));\n            sum_vec = _mm256_add_epi32(sum_vec, _mm256_inserti128_si256(_mm256_setzero_si256(), sum_low, 0));\n            sum_vec = _mm256_add_epi32(sum_vec, _mm256_inserti128_si256(_mm256_setzero_si256(), sum_high, 1));\n        }\n        sum += _mm256_cvtsi256_si32(sum_vec);\n        \n        i += vec_len;\n    }\n\n    // Handle remaining characters\n    for(; i < s.size(); ++i){\n        if(s[i] >= 'A' && s[i] <= 'Z'){\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c\nint digitSum_simd(std::string s){\n    const int vec_len = 32;\n    __m256i sum_vec = _mm256_setzero_si256();\n    int i = 0;\n\n    while(i + vec_len <= s.size()){\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i upper_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_chars = _mm256_andnot_si256(upper_mask, chars);\n        __m256i ascii_mask = _mm256_set1_epi8(0x7F);\n        __m256i valid_chars = _mm256_and_si256(upper_chars, ascii_mask);\n        __m256i ascii_vals = _mm256_sub_epi8(valid_chars, _mm256_set1_epi8('A' - 1));\n        sum_vec = _mm256_add_epi8(sum_vec, ascii_vals);\n        i += vec_len;\n    }\n\n    __m256i horizontal_sum = _mm256_setzero_si256();\n    for(int j = 0; j < vec_len; j += 4){\n        __m128i low = _mm256_castsi256_si128(sum_vec);\n        __m128i high = _mm256_extractf128_si256(sum_vec, 1);\n        low = _mm_add_epi8(low, high);\n        horizontal_sum = _mm_add_epi8(horizontal_sum, low);\n        sum_vec = _mm256_permute2x128_si256(sum_vec, sum_vec, 1);\n    }\n\n    int final_sum = _mm_cvtsi128_si32(horizontal_sum);\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c\nint digitSum_simd(std::string s){\n    const int vec_len = 32;\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i upper_mask = _mm256_set1_epi8(0x20);\n    __m256i upper_check = _mm256_setr_epi8(\n        'A','A','A','A','A','A','A','A',\n        'A','A','A','A','A','A','A','A',\n        'A','A','A','A','A','A','A','A',\n        'A','A','A','A','A','A','A','A'\n    );\n    __m256i zero_check = _mm256_setr_epi8(\n        'Z','Z','Z','Z','Z','Z','Z','Z',\n        'Z','Z','Z','Z','Z','Z','Z','Z',\n        'Z','Z','Z','Z','Z','Z','Z','Z',\n        'Z','Z','Z','Z','Z','Z','Z','Z'\n    );\n\n    int i = 0;\n    while(i + vec_len <= s.size()){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i upper_masked = _mm256_xor_si256(vec, upper_mask);\n        __m256i upper_test = _mm256_cmpgt_epi8(_mm256_sub_epi8(upper_masked, upper_check), zero_check);\n        __m256i ascii_sum = _mm256_add_epi8(vec, _mm256_set1_epi8(-'A'));\n        __m256i filtered_ascii = _mm256_and_si256(ascii_sum, upper_test);\n        sum_vec = _mm256_add_epi8(sum_vec, filtered_ascii);\n        i += vec_len;\n    }\n\n    __m256i sum_horizontal = _mm256_setzero_si256();\n    while(i < s.size()){\n        if(s[i] >= 'A' && s[i] <= 'Z'){\n            sum_horizontal = _mm256_add_epi8(sum_horizontal, _mm256_set1_epi8(s[i]));\n        }\n        i++;\n    }\n\n    __m256i sum_low = _mm256_extracti128_si256(sum_vec, 0);\n    __m256i sum_high = _mm256_extracti128_si256(sum_vec, 1);\n    sum_low = _mm256_add_epi8(sum_low, sum_high);\n    sum_low = _mm256_add_epi8(sum_low, sum_horizontal);\n\n    int sum_result[8];\n    _mm256_storeu_si256((__m256i*)sum_result, sum_low);\n    int total = 0;\n    for(int j = 0; j < 8; j++){\n        total += sum_result[j];\n    }\n    return total;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c\nint digitSum_simd(std::string s){\n    const int vec_size = 32; // AVX size in bytes\n    int sum = 0;\n    __m256i mask = _mm256_set1_epi8(1 << 5); // Mask for checking uppercase letters\n    __m256i ascii_mask = _mm256_set1_epi8(0xFF); // Mask to extract ASCII values\n    int i = 0;\n\n    while(i + vec_size <= s.size()){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i upper = _mm256_and_si256(vec, mask);\n        __m256i ascii = _mm256_and_si256(vec, ascii_mask);\n\n        // Compare if the character is uppercase\n        __m256i cmp = _mm256_cmpeq_epi8(_mm256_srli_epi16(upper, 5), _mm256_set1_epi8(1));\n        __m256i valid_chars = _mm256_and_si256(cmp, _mm256_cmpgt_epi8(ascii, _mm256_setzero_si256()));\n\n        // Sum the valid uppercase characters\n        __m256i sums = _mm256_maddubs_epi16(valid_chars, ascii);\n        sums = _mm256_sad_epu8(sums, _mm256_setzero_si256());\n        sum += _mm256_cvtsi256_si32(sums);\n\n        i += vec_size;\n    }\n\n    // Handle remaining characters\n    for(; i < s.size(); ++i){\n        if(s[i] >= 'A' && s[i] <= 'Z'){\n            sum += s[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```cpp\nint digitSum_simd(std::string s){\n    int sum = 0;\n    const char* str = s.c_str();\n    size_t len = s.length();\n\n    while(len >= 32){\n        __m256i vec = _mm256_loadu_si256((__m256i*)str);\n        __m256i upper_mask = _mm256_set1_epi8(0x20);\n        __m256i upper_check = _mm256_or_si256(vec, upper_mask);\n        __m256i upper_mask_final = _mm256_cmpgtb_epi8(upper_check, _mm256_set1_epi8('A'-1));\n        __m256i upper_mask_final2 = _mm256_cmpgtb_epi8(_mm256_set1_epi8('Z'+1), upper_check);\n        __m256i upper_mask_final3 = _mm256_and_si256(upper_mask_final, upper_mask_final2);\n        __m256i ascii_vec = _mm256_add_epi8(vec, _mm256_set1_epi8(-'A' + 'a'));\n        __m256i ascii_upper = _mm256_and_si256(ascii_vec, upper_mask_final3);\n        __m256i ascii_sum = _mm256_sub_epi8(ascii_upper, _mm256_set1_epi8('a' - 1));\n        __m256i sum_vec = _mm256_cvtepu8_epi32(ascii_sum);\n        sum = _mm256_extract_epi32(sum_vec, 0) + _mm256_extract_epi32(sum_vec, 1) +\n              _mm256_extract_epi32(sum_vec, 2) + _mm256_extract_epi32(sum_vec, 3);\n        str += 32;\n        len -= 32;\n    }\n\n    while(len--){\n        char c = *str++;\n        if(c >= 'A' && c <= 'Z'){\n            sum += c;\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    const int vec_size = 8; // AVX size for int is 8\n    int min_even_val = INT32_MAX;\n    int min_index = -1;\n\n    for(int i = 0; i < arr.size(); i += vec_size){\n        __m256i v_arr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i v_mask = _mm256_set1_epi32(1);\n        __m256i v_even = _mm256_and_si256(v_arr, v_mask);\n\n        // Check if any element is even\n        __m256i v_zero = _mm256_setzero_si256();\n        int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi32(v_even, v_zero));\n\n        if(mask != 0){\n            // Find the minimum even value and its index\n            __m256i v_min_val = _mm256_set1_epi32(min_even_val);\n            __m256i v_min_idx = _mm256_set1_epi32(min_index);\n\n            __m256i v_cmp_lt = _mm256_cmpgt_epi32(v_arr, v_min_val);\n            __m256i v_cmp_eq = _mm256_cmpeq_epi32(v_arr, v_min_val);\n\n            int cmp_mask = _mm256_movemask_epi8(v_cmp_lt);\n            int eq_mask = _mm256_movemask_epi8(v_cmp_eq);\n\n            if(cmp_mask == 0 && eq_mask != 0){\n                // Update min_even_val and min_index\n                int local_min_index = i + __builtin_ctz(eq_mask);\n                if(arr[local_min_index] < min_even_val){\n                    min_even_val = arr[local_min_index];\n                    min_index = local_min_index;\n                }\n            }\n        }\n    }\n\n    if(min_index != -1){\n        return {min_even_val, min_index};\n    }\n\n    return {};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    const int vec_size = 8; // AVX size for int is 8\n    int min_even_val = INT32_MAX;\n    int min_index = -1;\n\n    for(int i = 0; i < arr.size(); i += vec_size){\n        __m256i v_arr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i v_mask = _mm256_set1_epi32(1);\n        __m256i v_is_even = _mm256_and_si256(v_arr, v_mask);\n\n        // Check if all elements are odd\n        __m256i v_all_odd = _mm256_cmpgt_epi32(v_is_even, _mm256_setzero_si256());\n        int mask_all_odd = _mm256_movemask_epi8(v_all_odd);\n        if(mask_all_odd == 0xFF){\n            continue;\n        }\n\n        // Find the minimum even value and its index\n        __m256i v_min_val = _mm256_set1_epi32(min_even_val);\n        __m256i v_min_idx = _mm256_set1_epi32(min_index);\n        __m256i v_min_mask = _mm256_cmpeq_epi32(v_arr, v_min_val);\n\n        // Update min_even_val and min_index\n        for(int j = 0; j < vec_size && (i + j) < arr.size(); ++j){\n            if((v_is_even[j] == 0) && (arr[i + j] < min_even_val)){\n                min_even_val = arr[i + j];\n                min_index = i + j;\n            }\n        }\n    }\n\n    return {min_even_val, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    const int vec_size = 8; // AVX size for 32-bit integers\n    const int size = arr.size();\n    __m256i min_val_vec = _mm256_set1_epi32(__INT32_MAX__);\n    __m256i min_idx_vec = _mm256_set1_epi32(-1);\n    int global_min_val = __INT32_MAX__;\n    int global_min_idx = -1;\n\n    for(int i=0; i<size; i+=vec_size){\n        __m256i vals = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i evens = _mm256_and_si256(vals, _mm256_set1_epi32(1));\n        __m256i valid_mask = _mm256_cmpeq_epi32(evens, _mm256_setzero_si256());\n        \n        if(!_mm256_movemask_epi8(valid_mask)) continue;\n\n        __m256i vals_filtered = _mm256_blendv_epi8(_mm256_setzero_si256(), vals, valid_mask);\n        __m256i idxs = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n\n        __m256i lt_mask = _mm256_cmpgt_epi32(min_val_vec, vals_filtered);\n        min_val_vec = _mm256_blendv_epi8(min_val_vec, vals_filtered, lt_mask);\n        min_idx_vec = _mm256_blendv_epi8(min_idx_vec, idxs, lt_mask);\n\n        // Update global min if needed\n        int local_min_val[vec_size];\n        int local_min_idx[vec_size];\n        _mm256_storeu_si256((__m256i*)local_min_val, min_val_vec);\n        _mm256_storeu_si256((__m256i*)local_min_idx, min_idx_vec);\n\n        for(int j=0; j<vec_size; ++j){\n            if(local_min_val[j] < global_min_val || \n               (local_min_val[j] == global_min_val && local_min_idx[j] < global_min_idx)){\n                global_min_val = local_min_val[j];\n                global_min_idx = local_min_idx[j];\n            }\n        }\n    }\n\n    if(global_min_val == __INT32_MAX__) return {};\n    return {global_min_val, global_min_idx};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    const int vec_size = 8; // AVX size for int is 8\n    int min_even_val = INT32_MAX;\n    int min_index = -1;\n\n    for(int i = 0; i < arr.size(); i += vec_size){\n        __m256i v_arr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i v_mask = _mm256_set1_epi32(1);\n        __m256i v_even = _mm256_and_si256(v_arr, v_mask);\n\n        // Check if any element is even\n        __m256i v_zero = _mm256_setzero_si256();\n        int mask = _mm256_movemask_epi8(_mm256_cmpeq_epi32(v_even, v_zero));\n        if(mask == 0) continue;\n\n        // Find the minimum even value and its index\n        __m256i v_min_val = _mm256_set1_epi32(min_even_val);\n        __m256i v_min_idx = _mm256_set1_epi32(min_index);\n        __m256i v_cmp = _mm256_cmpgt_epi32(v_arr, v_min_val);\n        __m256i v_new_min_val = _mm256_blendv_epi8(v_min_val, v_arr, v_cmp);\n        __m256i v_new_min_idx = _mm256_blendv_epi8(v_min_idx, _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i), v_cmp);\n\n        // Update global min values\n        int new_min_val = _mm256_extract_epi32(v_new_min_val, 0);\n        int new_min_idx = _mm256_extract_epi32(v_new_min_idx, 0);\n        if(new_min_val < min_even_val){\n            min_even_val = new_min_val;\n            min_index = new_min_idx;\n        }\n    }\n\n    return {min_even_val, min_index};\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    const int vec_size = 8; // AVX size for int is 8\n    int min_even_val = INT32_MAX;\n    int min_index = -1;\n\n    for(int i = 0; i < arr.size(); i += vec_size){\n        __m256i v_arr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i v_mask = _mm256_set1_epi32(1);\n        __m256i v_is_even = _mm256_and_si256(v_arr, v_mask);\n\n        // Check if all elements are odd\n        __m256i v_all_odd = _mm256_cmpgt_epi32(v_is_even, _mm256_setzero_si256());\n        int mask_all_odd = _mm256_movemask_epi8(v_all_odd);\n        if(mask_all_odd == 0xFF){\n            continue;\n        }\n\n        // Find the minimum even value and its index\n        __m256i v_min_val = _mm256_set1_epi32(min_even_val);\n        __m256i v_min_idx = _mm256_set1_epi32(min_index);\n        __m256i v_min_mask = _mm256_cmpeq_epi32(v_arr, v_min_val);\n\n        // Update min_even_val and min_index if necessary\n        for(int j = 0; j < vec_size && (i + j) < arr.size(); ++j){\n            if((v_is_even[j] == 0) || (arr[i + j] >= min_even_val)) continue;\n            min_even_val = arr[i + j];\n            min_index = i + j;\n        }\n    }\n\n    return {min_even_val, min_index};\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_val = -1;\n    size_t n = lst.size();\n    __m256i threshold = _mm256_set1_epi32(0);\n    __m256i mask = _mm256_set1_epi32(0);\n    __m256i count = _mm256_setzero_si256();\n\n    for(size_t i = 0; i < n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i eq = _mm256_cmpeq_epi32(vec, threshold);\n        __m256i ge = _mm256_cmpgt_epi32(vec, threshold);\n        \n        count = _mm256_add_epi32(count, _mm256_and_si256(eq, _mm256_set1_epi32(1)));\n        mask = _mm256_or_si256(mask, _mm256_and_si256(ge, _mm256_set1_epi32(1)));\n    }\n\n    int remaining[8];\n    _mm256_storeu_si256((__m256i*)remaining, count);\n\n    for(int i = 0; i < 8; ++i){\n        if(mask & (1 << i)){\n            if(remaining[i] >= lst[i] && lst[i] > max_val){\n                max_val = lst[i];\n            }\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_val = -1;\n    size_t n = lst.size();\n    __m256i threshold = _mm256_set1_epi32(0);\n    __m256i mask = _mm256_set1_epi32(0);\n    __m256i count = _mm256_setzero_si256();\n    __m256i zero = _mm256_setzero_si256();\n\n    if(n < 8) {\n        // Handle small vectors manually\n        for(auto num : lst){\n            if(num > 0 && std::count(lst.begin(), lst.end(), num) >= num){\n                if(num > max_val){\n                    max_val = num;\n                }\n            }\n        }\n        return max_val;\n    }\n\n    for(size_t i=0; i<n; i+=8){\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i cmp_gt_zero = _mm256_cmpgt_epi32(data, zero);\n        __m256i valid = _mm256_and_si256(cmp_gt_zero, threshold);\n\n        __m256i eq_mask = _mm256_cmpeq_epi32(data, data);\n        __m256i freq = _mm256_setzero_si256();\n\n        for(int j=0; j<8; ++j){\n            if(i+j < n){\n                freq = _mm256_add_epi32(freq, _mm256_set1_epi32((lst[i+j] == data[j]) ? 1 : 0));\n            }\n        }\n\n        __m256i valid_freq = _mm256_cmpgt_epi32(freq, data);\n        mask = _mm256_and_si256(valid, valid_freq);\n        count = _mm256_add_epi32(count, mask);\n    }\n\n    int cnt_arr[8];\n    _mm256_storeu_si256((__m256i*)cnt_arr, count);\n\n    for(int k=0; k<8; ++k){\n        if(cnt_arr[k] > 0 && cnt_arr[k] >= (k+1)){\n            if((k+1) > max_val){\n                max_val = k+1;\n            }\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_val = -1;\n    if(lst.empty()) return max_val;\n\n    size_t n = lst.size();\n    __m256i threshold = _mm256_setzero_si256();\n    __m256i count = _mm256_setzero_si256();\n    __m256i mask = _mm256_setzero_si256();\n\n    for(size_t i=0; i<n; i+=8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        \n        // Count occurrences\n        __m256i eq = _mm256_cmpeq_epi32(vec, threshold);\n        __m256i ones = _mm256_set1_epi32(1);\n        __m256i cnt = _mm256_add_epi32(count, _mm256_and_si256(eq, ones));\n        \n        // Compare frequency with value\n        mask = _mm256_cmpgt_epi32(cnt, vec);\n        __m256i valid = _mm256_and_si256(mask, vec);\n        \n        // Update max_val\n        max_val = std::max(max_val, _mm256_movemask_epi8(_mm256_castsi256_si128(valid)) & 0xFF);\n        max_val = std::max(max_val, _mm256_extract_epi32(valid, 4) & 0xFF);\n\n        count = cnt;\n    }\n\n    // Handle remaining elements\n    for(size_t i=n&~7; i<n; ++i){\n        if(lst[i] > 0 && lst[i] <= n && count[lst[i]] >= lst[i]){\n            max_val = std::max(max_val, lst[i]);\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_val = -1;\n    size_t n = lst.size();\n    __m256i threshold = _mm256_set1_epi32(0);\n    __m256i count_vec = _mm256_setzero_si256();\n    __m256i mask;\n\n    for(size_t i = 0; i < n; i += 8){\n        __m256i values = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i cmp_gt_zero = _mm256_cmpgt_epi32(values, threshold);\n        __m256i valid = _mm256_andnot_si256(cmp_gt_zero, count_vec);\n\n        mask = _mm256_cmpgt_epi32(count_vec, values);\n        count_vec = _mm256_add_epi32(count_vec, _mm256_and_si256(mask, cmp_gt_zero));\n        \n        mask = _mm256_and_si256(cmp_gt_zero, _mm256_cmpge_epi32(count_vec, values));\n        __m256i candidate = _mm256_blendv_epi8(_mm256_setzero_si256(), values, mask);\n        max_val = std::max(max_val, _mm256_movemask_epi8(_mm256_cmpeq_epi32(candidate, _mm256_set1_epi32(max_val))));\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_val = -1;\n    size_t n = lst.size();\n    __m256i threshold = _mm256_set1_epi32(0);\n    __m256i mask = _mm256_set1_epi32(0);\n    __m256i count = _mm256_setzero_si256();\n\n    if(n < 8) {\n        // Handle small vectors manually\n        for(auto num : lst){\n            if(num > 0 && num <= n){\n                int freq = std::count(lst.begin(), lst.end(), num);\n                if(freq >= num && num > max_val){\n                    max_val = num;\n                }\n            }\n        }\n        return max_val;\n    }\n\n    for(size_t i=0; i<n; i+=8){\n        __m256i data = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i gt_zero = _mm256_cmpgt_epi32(data, threshold);\n        __m256i le_n = _mm256_cmpgt_epi32(_mm256_set1_epi32((int)n), data);\n\n        mask = _mm256_and_si256(gt_zero, le_n);\n        __m256i valid = _mm256_andnot_si256(mask, data);\n\n        __m256i eq = _mm256_cmpeq_epi32(valid, data);\n        count = _mm256_add_epi32(count, _mm256_cvtepu8_epi32(eq));\n    }\n\n    int* cnt_arr = (int*)&count;\n    for(int j=0; j<8; ++j){\n        if(cnt_arr[j] >= j+1 && j+1 > max_val){\n            max_val = j+1;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    const int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        __m256i shuffled = _mm256_permute4x64_epi64(vec, 0x4E);\n        __m256i diff = _mm256_sub_epi32(vec, shuffled);\n\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        balance_vec = _mm256_add_epi32(balance_vec, diff);\n    }\n\n    // Horizontal sum for remaining elements\n    __m256i temp_sum = _mm256_setzero_si256();\n    __m256i temp_balance = _mm256_setzero_si256();\n    for (; i < n; ++i) {\n        temp_sum = _mm256_add_epi32(temp_sum, _mm256_set1_epi32(q[i]));\n        temp_balance = _mm256_add_epi32(temp_balance, _mm256_set1_epi32(q[i] - q[n - i - 1]));\n    }\n\n    __m256i total_sum = _mm256_add_epi32(sum_vec, temp_sum);\n    __m256i total_balance = _mm256_add_epi32(balance_vec, temp_balance);\n\n    // Horizontal sum for total_sum and total_balance\n    int sum_result[8];\n    int balance_result[8];\n    _mm256_storeu_si256((__m256i*)sum_result, total_sum);\n    _mm256_storeu_si256((__m256i*)balance_result, total_balance);\n\n    int total_sum_value = 0;\n    int total_balance_value = 0;\n    for (int j = 0; j < 8; ++j) {\n        total_sum_value += sum_result[j];\n        total_balance_value += balance_result[j];\n    }\n\n    return total_sum_value <= w && total_balance_value == 0;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    const int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        __m256i shuffled = _mm256_permute4x64_epi64(vec, 0x4E);\n        __m256i diff = _mm256_sub_epi32(vec, shuffled);\n\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        balance_vec = _mm256_add_epi32(balance_vec, diff);\n    }\n\n    // Horizontal sum for remaining elements\n    __m256i temp_sum = _mm256_setzero_si256();\n    __m256i temp_balance = _mm256_setzero_si256();\n    for (; i < n; ++i) {\n        temp_sum = _mm256_add_epi32(temp_sum, _mm256_set1_epi32(q[i]));\n        temp_balance = _mm256_add_epi32(temp_balance, _mm256_set1_epi32(q[i] - q[n - 1 - i]));\n    }\n\n    __m256i total_sum = _mm256_add_epi32(sum_vec, temp_sum);\n    __m256i total_balance = _mm256_add_epi32(balance_vec, temp_balance);\n\n    // Horizontal sum for total_sum and total_balance\n    int sum_result[8];\n    int balance_result[8];\n    _mm256_storeu_si256((__m256i*)sum_result, total_sum);\n    _mm256_storeu_si256((__m256i*)balance_result, total_balance);\n\n    int sum = 0;\n    int balance = 0;\n    for (int j = 0; j < 8; ++j) {\n        sum += sum_result[j];\n        balance += balance_result[j];\n    }\n\n    return (sum <= w) && (balance == 0);\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    const int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        balance_vec = _mm256_add_epi32(balance_vec, _mm256_sub_epi32(vec, _mm256_permutevar8x32_epi32(vec, _mm256_setr_epi32(7, 6, 5, 4, 3, 2, 1, 0))));\n    }\n\n    __m128i sum_low = _mm256_extractf128_si256(sum_vec, 0);\n    __m128i sum_high = _mm256_extractf128_si256(sum_vec, 1);\n    sum_low = _mm_add_epi32(sum_low, sum_high);\n    __m128i balance_low = _mm256_extractf128_si256(balance_vec, 0);\n    __m128i balance_high = _mm256_extractf128_si256(balance_vec, 1);\n    balance_low = _mm_add_epi32(balance_low, balance_high);\n\n    int sum_result[4];\n    int balance_result[4];\n    _mm_storeu_si128((__m128i*)sum_result, sum_low);\n    _mm_storeu_si128((__m128i*)balance_result, balance_low);\n\n    int total_sum = sum_result[0] + sum_result[1] + sum_result[2] + sum_result[3];\n    int total_balance = balance_result[0] + balance_result[1] + balance_result[2] + balance_result[3];\n\n    for (; i < n; ++i) {\n        total_sum += q[i];\n        total_balance += q[i] - q[n - i - 1];\n    }\n\n    return total_balance == 0 && total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    const int n = q.size();\n    if (n == 0) return false;\n\n    // Load data into SIMD registers\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&q[i]);\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        __m256i reverse_vec = _mm256_permute4x64_epi64(vec, 0x55); // Reverse order\n        balance_vec = _mm256_add_epi32(balance_vec, _mm256_cmpeq_epi32(vec, reverse_vec));\n    }\n\n    // Horizontal sum for remaining elements\n    __m256i temp_sum = _mm256_setzero_si256();\n    for (; i < n; ++i) {\n        temp_sum = _mm256_add_epi32(temp_sum, _mm256_set1_epi32(q[i]));\n    }\n    sum_vec = _mm256_add_epi32(sum_vec, temp_sum);\n\n    // Horizontal sum for balance check\n    __m128i balance_low = _mm256_castsi256_si128(balance_vec);\n    __m128i balance_high = _mm256_extractf128_si256(balance_vec, 1);\n    balance_low = _mm_add_epi32(balance_low, balance_high);\n    balance_high = _mm256_castsi256_si128(_mm256_permute4x64_epi64(balance_vec, 0b10110001));\n    balance_low = _mm_add_epi32(balance_low, balance_high);\n    int balance_check = _mm_cvtsi128_si32(balance_low);\n\n    // Horizontal sum for total sum\n    __m128i sum_low = _mm256_castsi256_si128(sum_vec);\n    __m128i sum_high = _mm256_extractf128_si256(sum_vec, 1);\n    sum_low = _mm_add_epi32(sum_low, sum_high);\n    sum_high = _mm256_castsi256_si128(_mm256_permute4x64_epi64(sum_vec, 0b10110001));\n    sum_low = _mm_add_epi32(sum_low, sum_high);\n    int total_sum = _mm_cvtsi128_si32(sum_low);\n\n    return (total_sum <= w) && (balance_check == n);\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(const std::vector<int>& q, int w) {\n    const int n = q.size();\n    if (n == 0) return false;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i balance_vec = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[i]));\n        __m256i shuffled = _mm256_permute4x64_epi64(vec, 0x4E);\n        __m256i diff = _mm256_sub_epi32(vec, shuffled);\n\n        sum_vec = _mm256_add_epi32(sum_vec, vec);\n        balance_vec = _mm256_add_epi32(balance_vec, diff);\n    }\n\n    // Horizontal sum for remaining elements\n    __m256i temp_sum = _mm256_setzero_si256();\n    __m256i temp_balance = _mm256_setzero_si256();\n    for (; i < n; ++i) {\n        temp_sum = _mm256_add_epi32(temp_sum, _mm256_set1_epi32(q[i]));\n        temp_balance = _mm256_add_epi32(temp_balance, _mm256_set1_epi32(q[i] - q[n - i - 1]));\n    }\n\n    __m256i total_sum = _mm256_add_epi32(sum_vec, temp_sum);\n    __m256i total_balance = _mm256_add_epi32(balance_vec, temp_balance);\n\n    // Horizontal sum for total_sum and total_balance\n    int sum_result[8];\n    int balance_result[8];\n    _mm256_storeu_si256((__m256i*)sum_result, total_sum);\n    _mm256_storeu_si256((__m256i*)balance_result, total_balance);\n\n    int sum = 0;\n    int balance = 0;\n    for (int j = 0; j < 8; ++j) {\n        sum += sum_result[j];\n        balance += balance_result[j];\n    }\n\n    return (sum <= w) && (balance == 0);\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    size_t n = arr.size();\n    size_t i = 0;\n\n    while(i + 8 <= n){\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - 1 - i]);\n\n        __m256i diff = _mm256_sub_epi32(left, right);\n        diff = _mm256_abs_epi32(diff); // Absolute difference\n\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), diff);\n        changes += _mm256_movemask_epi8(mask);\n\n        i += 8;\n    }\n\n    while(i < n){\n        if(arr[i] != arr[n - 1 - i]){\n            changes++;\n        }\n        i++;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    size_t n = arr.size();\n    size_t i = 0;\n\n    while(i + 8 <= n){\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - 1 - i]);\n\n        __m256i diff = _mm256_cmpeq_epi32(left, right);\n        int mask = _mm256_movemask_epi8(diff);\n        changes += __builtin_popcount(mask ^ 0xFFFFFFFF);\n\n        i += 8;\n    }\n\n    while(i < n){\n        if(arr[i] != arr[n - 1 - i]){\n            changes++;\n        }\n        i++;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    size_t n = arr.size();\n    size_t i = 0;\n\n    while(i + 8 <= n){\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - 1 - i]);\n\n        __m256i diff = _mm256_sub_epi32(left, right);\n        diff = _mm256_abs_epi32(diff); // Absolute difference\n\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), diff);\n        changes += _mm256_movemask_epi8(mask);\n\n        i += 8;\n    }\n\n    while(i < n){\n        if(arr[i] != arr[n - 1 - i]){\n            changes++;\n        }\n        i++;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(const std::vector<int>& arr) {\n    int n = arr.size();\n    if (n <= 1) return 0;\n\n    int changes = 0;\n    int i = 0;\n\n    while (i + 8 <= n) {\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - i - 1]);\n\n        __m256i cmp = _mm256_cmpeq_epi32(left, right);\n        int mask = _mm256_movemask_epi8(cmp);\n\n        while (mask != 0) {\n            changes++;\n            mask &= (mask - 1);\n        }\n\n        i += 8;\n    }\n\n    while (i < n) {\n        if (arr[i] != arr[n - i - 1]) {\n            changes++;\n        }\n        i++;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    size_t n = arr.size();\n    size_t i = 0;\n\n    while(i + 8 <= n){\n        __m256i left = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i right = _mm256_loadu_si256((__m256i*)&arr[n - 1 - i]);\n\n        __m256i diff = _mm256_cmpeq_epi32(left, right);\n        int mask = _mm256_movemask_epi8(diff);\n        changes += __builtin_popcount(mask ^ 0xFFFFFFFF);\n\n        i += 8;\n    }\n\n    while(i < n){\n        if(arr[i] != arr[n - 1 - i]){\n            changes++;\n        }\n        i++;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```c\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= lst1.size() && i + 8 <= lst2.size(); i += 8) {\n        __m256i len1 = _mm256_set_epi32(lst1[i+7].length(), lst1[i+6].length(), lst1[i+5].length(), lst1[i+4].length(),\n                                        lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i+0].length());\n        __m256i len2 = _mm256_set_epi32(lst2[i+7].length(), lst2[i+6].length(), lst2[i+5].length(), lst2[i+4].length(),\n                                        lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i+0].length());\n\n        sum1 = _mm256_add_epi32(sum1, len1);\n        sum2 = _mm256_add_epi32(sum2, len2);\n    }\n\n    // Handle remaining elements\n    for (; i < lst1.size(); ++i) {\n        sum1 = _mm256_add_epi32(sum1, _mm256_set1_epi32(lst1[i].length()));\n    }\n    for (; i < lst2.size(); ++i) {\n        sum2 = _mm256_add_epi32(sum2, _mm256_set1_epi32(lst2[i].length()));\n    }\n\n    // Compare sums\n    __m256i cmp_result = _mm256_cmpgt_epi32(sum1, sum2);\n    int result = _mm256_movemask_epi8(cmp_result);\n\n    if (result != 0) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= lst1.size() && i + 8 <= lst2.size(); i += 8) {\n        __m256i len1 = _mm256_set_epi32(lst1[i+7].length(), lst1[i+6].length(), lst1[i+5].length(), lst1[i+4].length(),\n                                        lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i].length());\n        __m256i len2 = _mm256_set_epi32(lst2[i+7].length(), lst2[i+6].length(), lst2[i+5].length(), lst2[i+4].length(),\n                                        lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i].length());\n\n        sum1 = _mm256_add_epi32(sum1, len1);\n        sum2 = _mm256_add_epi32(sum2, len2);\n    }\n\n    // Handle remaining elements\n    for (; i < lst1.size(); ++i) {\n        sum1 = _mm256_add_epi32(sum1, _mm256_set1_epi32(lst1[i].length()));\n    }\n    for (; i < lst2.size(); ++i) {\n        sum2 = _mm256_add_epi32(sum2, _mm256_set1_epi32(lst2[i].length()));\n    }\n\n    // Combine and compare sums\n    __m256i cmp_result = _mm256_cmpgt_epi32(sum2, sum1);\n    int result_mask = _mm256_movemask_epi8(cmp_result);\n\n    if (result_mask != 0) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```c\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= lst1.size() && i + 8 <= lst2.size(); i += 8) {\n        __m256i len1 = _mm256_set_epi32(lst1[i+7].length(), lst1[i+6].length(), lst1[i+5].length(), lst1[i+4].length(),\n                                        lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i+0].length());\n        __m256i len2 = _mm256_set_epi32(lst2[i+7].length(), lst2[i+6].length(), lst2[i+5].length(), lst2[i+4].length(),\n                                        lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i+0].length());\n\n        sum1 = _mm256_add_epi32(sum1, len1);\n        sum2 = _mm256_add_epi32(sum2, len2);\n    }\n\n    // Handle remaining elements\n    for (; i < lst1.size(); ++i) {\n        sum1 = _mm256_add_epi32(sum1, _mm256_set1_epi32(lst1[i].length()));\n    }\n    for (; i < lst2.size(); ++i) {\n        sum2 = _mm256_add_epi32(sum2, _mm256_set1_epi32(lst2[i].length()));\n    }\n\n    // Combine and compare sums\n    __m256i cmp_result = _mm256_cmpgt_epi32(sum1, sum2);\n    int result = _mm256_movemask_epi8(cmp_result);\n\n    if (result != 0) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```c\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= lst1.size() && i + 8 <= lst2.size(); i += 8) {\n        __m256i len1 = _mm256_set_epi32(lst1[i+7].length(), lst1[i+6].length(), lst1[i+5].length(), lst1[i+4].length(),\n                                        lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i+0].length());\n        __m256i len2 = _mm256_set_epi32(lst2[i+7].length(), lst2[i+6].length(), lst2[i+5].length(), lst2[i+4].length(),\n                                        lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i+0].length());\n\n        sum1 = _mm256_add_epi32(sum1, len1);\n        sum2 = _mm256_add_epi32(sum2, len2);\n    }\n\n    // Handle remaining elements\n    for (; i < lst1.size(); ++i) {\n        sum1 = _mm256_add_epi32(sum1, _mm256_set1_epi32(lst1[i].length()));\n    }\n    for (; i < lst2.size(); ++i) {\n        sum2 = _mm256_add_epi32(sum2, _mm256_set1_epi32(lst2[i].length()));\n    }\n\n    // Combine and reduce sums\n    __m256i sum1_low = _mm256_extractf128_si256(sum1, 0);\n    __m256i sum1_high = _mm256_extractf128_si256(sum1, 1);\n    sum1_low = _mm256_add_epi32(sum1_low, sum1_high);\n    sum1_high = _mm256_permute2x128_si256(sum1_low, sum1_low, 1);\n    sum1_low = _mm256_add_epi32(sum1_low, sum1_high);\n\n    __m256i sum2_low = _mm256_extractf128_si256(sum2, 0);\n    __m256i sum2_high = _mm256_extractf128_si256(sum2, 1);\n    sum2_low = _mm256_add_epi32(sum2_low, sum2_high);\n    sum2_high = _mm256_permute2x128_si256(sum2_low, sum2_low, 1);\n    sum2_low = _mm256_add_epi32(sum2_low, sum2_high);\n\n    int total1 = _mm256_cvtsi256_si32(sum1_low);\n    int total2 = _mm256_cvtsi256_si32(sum2_low);\n\n    if (total1 < total2) {\n        return lst1;\n    } else if (total2 < total1) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    int i = 0;\n    for (; i + 8 <= lst1.size() && i + 8 <= lst2.size(); i += 8) {\n        __m256i len1 = _mm256_set_epi32(lst1[i+7].length(), lst1[i+6].length(), lst1[i+5].length(), lst1[i+4].length(),\n                                        lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i+0].length());\n        __m256i len2 = _mm256_set_epi32(lst2[i+7].length(), lst2[i+6].length(), lst2[i+5].length(), lst2[i+4].length(),\n                                        lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i+0].length());\n\n        sum1 = _mm256_add_epi32(sum1, len1);\n        sum2 = _mm256_add_epi32(sum2, len2);\n    }\n\n    // Handle remaining elements\n    for (; i < lst1.size(); ++i) {\n        sum1 = _mm256_add_epi32(sum1, _mm256_set1_epi32(lst1[i].length()));\n    }\n    for (; i < lst2.size(); ++i) {\n        sum2 = _mm256_add_epi32(sum2, _mm256_set1_epi32(lst2[i].length()));\n    }\n\n    // Compare sums\n    __m256i cmp_result = _mm256_cmpgt_epi32(sum1, sum2);\n    int result = _mm256_movemask_epi8(cmp_result);\n\n    if (result != 0) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    __m256i base = _mm256_set1_epi64x(n);\n    __m256i current = _mm256_set1_epi64x(1);\n    __m256i limit = _mm256_set1_epi64x(x);\n    int mask = 0;\n\n    while(_mm256_movemask_epi8(_mm256_cmpgt_epi64(limit, current)) == 0b1111){\n        __m256i next = _mm256_mullo_epi64(current, base);\n        current = next;\n        mask |= _mm256_movemask_epi8(_mm256_cmpeq_epi64(next, limit));\n    }\n\n    return mask != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    __m256i base = _mm256_set1_epi64x(n);\n    __m256i current = _mm256_set1_epi64x(1);\n    __m256i limit = _mm256_set1_epi64x(x);\n    __m256i mask = _mm256_setzero_si256();\n\n    while(!_mm256_movemask_epi8(_mm256_cmpeq_epi64(current, limit))){\n        current = _mm256_mullo_epi64(current, base);\n        mask = _mm256_or_si256(mask, _mm256_cmpgt_epi64(current, limit));\n        if(_mm256_movemask_epi8(mask)) break;\n    }\n\n    return _mm256_movemask_epi8(_mm256_cmpeq_epi64(current, limit)) != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if(n == 1){\n        return x == 1;\n    }\n    __m256i base = _mm256_set1_epi64x(n);\n    __m256i current = _mm256_set1_epi64x(1);\n    __m256i limit = _mm256_set1_epi64x(x);\n    int mask = 0;\n\n    while(_mm256_movemask_epi8(_mm256_cmpgt_epi64(limit, current)) != 0){\n        __m256i cmp = _mm256_cmpeq_epi64(current, limit);\n        mask |= _mm256_movemask_epi8(cmp);\n\n        current = _mm256_mullo_epi64(current, base);\n    }\n\n    return mask != 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    __m256i base = _mm256_set1_epi64x(n);\n    __m256i current = _mm256_set1_epi64x(1);\n    __m256i limit = _mm256_set1_epi64x(x);\n    int mask = 0;\n\n    for(int i=0; i<64; i++){\n        __m256i cmp = _mm256_cmpgt_epi64(limit, current);\n        mask |= _mm256_movemask_epi8(cmp);\n        if(mask == 0) break;\n        current = _mm256_mullo_epi64(current, base);\n    }\n\n    return __builtin_popcount(mask) > 0;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    __m256i base = _mm256_set1_epi64x(n);\n    __m256i current = _mm256_set1_epi64x(1);\n    __m256i limit = _mm256_set1_epi64x(x);\n    int mask = 0;\n\n    for(int i=0; i<64; i++){\n        __m256i cmp = _mm256_cmpgt_epi64(limit, current);\n        mask |= _mm256_movemask_epi8(cmp);\n        if(mask == 0) break;\n        current = _mm256_mullo_epi64(current, base);\n    }\n\n    return __builtin_popcount(mask) > 0;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m256i primes_mask = _mm256_setr_epi8(\n        /* 0 */ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 1 */ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 2 */ 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n        /* 3 */ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n    int count = 0;\n    size_t i = 0;\n    for (; i + 8 <= num.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(num.data() + i));\n        __m256i masked = _mm256_and_si256(vec, primes_mask);\n        int mask = _mm256_movemask_epi8(masked);\n        count += __builtin_popcount(mask);\n    }\n    for (; i < num.size(); ++i){\n        char c = num[i];\n        if (c == '2' || c == '3' || c == '5' || c == '7' ||\n            c == 'B' || c == 'D'){\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m256i primes_mask = _mm256_setr_epi8(\n        /* 0 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 2 */ 0, 0, 1, 0, 0, 1, 0, 0,\n        /* 4 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 6 */ 0, 0, 1, 0, 0, 1, 0, 0\n    );\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n    \n    while(i + 8 <= len){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(num.data() + i));\n        __m256i masked = _mm256_and_si256(vec, primes_mask);\n        int mask = _mm256_movemask_epi8(masked);\n        count += __builtin_popcount(mask);\n        i += 8;\n    }\n    \n    for(; i < len; ++i){\n        char c = num[i];\n        if(c == '2' || c == '3' || c == '5' || c == '7' ||\n           c == 'B' || c == 'D'){\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m256i primes_mask = _mm256_setr_epi8(\n        /* 0 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 8 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* F */ 0, 0, 0, 0, 1, 0, 1, 0,\n        /* 17 */ 1, 0, 1, 0, 0, 0, 0, 0\n    );\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n    \n    __m256i v_hex;\n    __m256i v_mask;\n    __m256i v_cmp;\n\n    while(i + 32 <= len){\n        v_hex = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(num.data() + i));\n        v_mask = _mm256_and_si256(v_hex, _mm256_set1_epi8(0xF));\n        v_cmp = _mm256_cmpeq_epi8(v_mask, primes_mask);\n        count += _mm256_popcnt_epi8(v_cmp);\n        i += 32;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = num[i];\n        if(c == '2' || c == '3' || c == '5' || c == '7' ||\n           c == 'B' || c == 'D'){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m256i primes_mask = _mm256_setr_epi8(\n        /* 0 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 8 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* F */ 0, 0, 0, 0, 1, 1, 1, 0,\n        /* 17 */ 1, 0, 1, 0, 0, 0, 0, 0\n    );\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n    \n    __m256i v_hex;\n    __m256i v_mask;\n    __m256i v_cmp;\n\n    while(i + 32 <= len){\n        v_hex = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(num.data() + i));\n        v_mask = _mm256_and_si256(v_hex, _mm256_set1_epi8(0xF));\n        v_cmp = _mm256_cmpeq_epi8(v_mask, primes_mask);\n        count += _mm256_popcnt_epi8(v_cmp);\n        i += 32;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = num[i];\n        if(c == '2' || c == '3' || c == '5' || c == '7' ||\n           c == 'B' || c == 'D'){\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m256i primes_mask = _mm256_setr_epi8(\n        /* 0 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 2 */ 0, 0, 1, 0, 0, 1, 0, 0,\n        /* 4 */ 0, 0, 0, 0, 0, 0, 0, 0,\n        /* 6 */ 0, 0, 1, 0, 0, 1, 0, 0\n    );\n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n    \n    while(i + 8 <= len){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(num.data() + i));\n        __m256i masked = _mm256_and_si256(vec, primes_mask);\n        int mask = _mm256_movemask_epi8(masked);\n        count += __builtin_popcount(mask);\n        i += 8;\n    }\n    \n    for(; i < len; ++i){\n        char c = num[i];\n        if(c == '2' || c == '3' || c == '5' || c == '7' ||\n           c == 'B' || c == 'D'){\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char * str = s.c_str();\n    size_t len = s.length();\n\n    if(len < 3) return false;\n\n    __m256i prev = _mm256_setzero_si256();\n    __m256i curr;\n    __m256i mask1 = _mm256_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n    __m256i mask2 = _mm256_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n\n    for(size_t i = 0; i + 8 <= len; i += 8){\n        curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i eq1 = _mm256_cmpeq_epi8(prev, curr);\n        __m256i eq2 = _mm256_cmpeq_epi8(_mm256_permute2x128_si256(curr, curr, 1), mask1);\n        __m256i eq3 = _mm256_cmpeq_epi8(_mm256_alignr_epi8(curr, prev, 1), mask2);\n        __m256i any_eq = _mm256_or_si256(eq1, eq2);\n        any_eq = _mm256_or_si256(any_eq, eq3);\n        int mask = _mm256_movemask_epi8(any_eq);\n        if(mask != 0) return false;\n        prev = curr;\n    }\n\n    for(size_t i = len / 8 * 8; i < len - 2; ++i){\n        if(str[i] == str[i+1] || str[i] == str[i+2] || str[i+1] == str[i+2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char *str = s.c_str();\n    size_t len = s.length();\n\n    // AVX processes 4 characters at a time\n    size_t i = 0;\n    __m256i prev_group = _mm256_setzero_si256();\n    __m256i curr_group;\n\n    if(len < 3) return false;\n\n    // Load first group of 4 characters\n    curr_group = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n    i += 8; // 4 chars per iteration\n\n    // Check first 3 characters\n    __m256i mask1 = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1);\n    __m256i mask2 = _mm256_set_epi8(-1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i mask3 = _mm256_set_epi8(0, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    __m256i v1 = _mm256_and_si256(curr_group, mask1);\n    __m256i v2 = _mm256_and_si256(curr_group, mask2);\n    __m256i v3 = _mm256_and_si256(curr_group, mask3);\n\n    if(!_mm256_cmpistreq_epi8(_MM_CMPINT_EQ, v1, v2) ||\n       !_mm256_cmpistreq_epi8(_MM_CMPINT_EQ, v2, v3)) {\n        return false;\n    }\n\n    prev_group = curr_group;\n\n    // Process remaining groups\n    while(i + 8 <= len){\n        curr_group = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        i += 8;\n\n        __m256i v1 = _mm256_and_si256(curr_group, mask1);\n        __m256i v2 = _mm256_and_si256(curr_group, mask2);\n        __m256i v3 = _mm256_and_si256(curr_group, mask3);\n\n        if(!_mm256_cmpistreq_epi8(_MM_CMPINT_EQ, v1, v2) ||\n           !_mm256_cmpistreq_epi8(_MM_CMPINT_EQ, v2, v3) ||\n           !_mm256_cmpistreq_epi8(_MM_CMPINT_EQ, v3, _mm256_permutevar8x32_epi32(prev_group, _mm256_set_epi32(3, 2, 1, 0, 7, 6, 5, 4)))) {\n            return false;\n        }\n\n        prev_group = curr_group;\n    }\n\n    // Handle remaining characters less than 4\n    for(; i < len; ++i){\n        if(str[i-2] == str[i-1] || str[i-1] == str[i] || str[i-2] == str[i]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char * str = s.c_str();\n    size_t len = s.length();\n\n    if(len < 3) return false;\n\n    __m256i prev = _mm256_setzero_si256();\n    __m256i curr;\n    __m256i mask = _mm256_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n\n    for(size_t i = 0; i + 32 <= len; i += 32){\n        curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i eq1 = _mm256_cmpeq_epi8(curr, prev);\n        __m256i eq2 = _mm256_cmpeq_epi8(_mm256_permute2x128_si256(curr, prev, 0x20), prev);\n        __m256i eq3 = _mm256_cmpeq_epi8(_mm256_alignr_epi8(curr, prev, 1), prev);\n\n        mask = _mm256_or_si256(mask, eq1);\n        mask = _mm256_or_si256(mask, eq2);\n        mask = _mm256_or_si256(mask, eq3);\n\n        prev = curr;\n    }\n\n    // Handle remaining characters\n    for(size_t i = len - (len % 3); i < len; ++i){\n        if(str[i] == str[i-1] || str[i] == str[i-2]){\n            return false;\n        }\n    }\n\n    return !_mm256_movemask_epi8(mask);\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char * str = s.c_str();\n    size_t len = s.length();\n\n    if(len < 3) return false;\n\n    __m256i prev = _mm256_setzero_si256();\n    __m256i curr;\n    __m256i mask = _mm256_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n\n    for(size_t i = 0; i + 32 <= len; i += 32){\n        curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        __m256i eq1 = _mm256_cmpeq_epi8(curr, prev);\n        __m256i eq2 = _mm256_cmpeq_epi8(_mm256_permute2x128_si256(curr, prev, 0x21), mask);\n        __m256i any_eq = _mm256_or_si256(eq1, eq2);\n        if(_mm256_movemask_epi8(any_eq) != 0) return false;\n        prev = curr;\n    }\n\n    for(size_t i = len - (len % 4); i < len; ++i){\n        if(str[i] == str[i-1] || str[i] == str[i-2] || str[i-1] == str[i-2]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```cpp\nbool is_happy_simd(const std::string &s) {\n    const char *str = s.c_str();\n    size_t len = s.length();\n\n    if (len < 3) return false;\n\n    __m256i prev = _mm256_setzero_si256();\n    __m256i curr = _mm256_setzero_si256();\n    __m256i next = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i <= len - 16; i += 16) {\n        prev = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n        curr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i + 4));\n        next = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i + 8));\n\n        __m256i mask1 = _mm256_cmpgt_epi8(prev, _mm256_setzero_si256());\n        __m256i mask2 = _mm256_cmpgt_epi8(curr, _mm256_setzero_si256());\n        __m256i mask3 = _mm256_cmpgt_epi8(next, _mm256_setzero_si256());\n\n        __m256i eq1 = _mm256_cmpeq_epi8(prev, curr);\n        __m256i eq2 = _mm256_cmpeq_epi8(curr, next);\n\n        __m256i invalid = _mm256_or_si256(eq1, eq2);\n        invalid = _mm256_andnot_si256(mask1, invalid);\n        invalid = _mm256_andnot_si256(mask2, invalid);\n        invalid = _mm256_andnot_si256(mask3, invalid);\n\n        if (_mm256_movemask_epi8(invalid)) {\n            return false;\n        }\n    }\n\n    for (; i < len; ++i) {\n        if (str[i] == str[i + 1] || str[i] == str[i + 2] || str[i + 1] == str[i + 2]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool prime_length_simd(const std::string & str){\n    const int length = str.length();\n    \n    // Handle small primes directly\n    if(length <= 1) return false;\n    if(length == 2 || length == 3 || length == 5 || length == 7) return true;\n    if(length % 2 == 0 || length % 3 == 0 || length % 5 == 0 || length % 7 == 0) return false;\n\n    // Use AVX2 to check divisibility by primes up to sqrt(length)\n    __m256i primes = _mm256_set_epi32(31, 29, 23, 19, 17, 13, 11, 7);\n    __m256i limit = _mm256_set1_epi32((int)sqrt(length));\n    __m256i mask = _mm256_setzero_si256();\n\n    for(int i = 0; i < 8; ++i){\n        __m256i prime = _mm256_set1_epi32(primes[i]);\n        __m256i div = _mm256_div_epi32(_mm256_set1_epi32(length), prime);\n        __m256i rem = _mm256_sub_epi32(div, _mm256_mullo_epi32(div, prime));\n        mask = _mm256_or_si256(mask, rem);\n    }\n\n    int mask_sum = _mm256_movemask_epi8(_mm256_castsi256_ps(mask));\n    return mask_sum == 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int len = str.length();\n    if(len < 2) return false;\n    __m256i primes = _mm256_set_epi32(7,5,7,5,3,2,2,0);\n    __m256i mask = _mm256_set1_epi32(len);\n    __m256i div = _mm256_div_epi32(mask, primes);\n    __m256i rem = _mm256_sub_epi32(mask, _mm256_mul_epu32(div, primes));\n    __m256i zero = _mm256_setzero_si256();\n    int remainder[8];\n    _mm256_storeu_si256((__m256i*)remainder, rem);\n    for(int i=0;i<8;i++) {\n        if(remainder[i]==0 && primes[i] != len) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int length = str.size();\n    if(length < 2) return false;\n\n    __m256i primes = _mm256_set_epi32(7,5,7,5,3,2,2,0);\n    __m256i mask = _mm256_set1_epi32(1);\n\n    for(int i=0; i<4; ++i){\n        __m256i div = _mm256_set1_epi32(i+2);\n        __m256i cmp = _mm256_div_epi32(_mm256_loadu_si256((__m256i*)&primes), div);\n        __m256i eq = _mm256_cmpeq_epi32(cmp, mask);\n        int res;\n        _mm256_storeu_si256((__m256i*)&res, eq);\n        if(res) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int len = str.length();\n    if(len < 2) return false;\n    __m256i primes = _mm256_set_epi32(7,5,7,5,3,2,2,0);\n    __m256i mask = _mm256_set1_epi32(len);\n    __m256i div = _mm256_div_epi32(mask, primes);\n    __m256i rem = _mm256_sub_epi32(mask, _mm256_mul_epu32(div, primes));\n    __m256i zero = _mm256_setzero_si256();\n    int remainder[8];\n    _mm256_storeu_si256((__m256i*)remainder, rem);\n    for(int i=0;i<8;i++) {\n        if(remainder[i]==0 && primes[i]!=len) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int len = str.length();\n    if(len < 2) return false;\n    __m256i primes = _mm256_set_epi32(7,5,7,5,3,2,2,0);\n    __m256i mask = _mm256_set1_epi32(len);\n    __m256i div = _mm256_div_epi32(mask, primes);\n    __m256i rem = _mm256_sub_epi32(mask, _mm256_mul_epu32(div, primes));\n    __m256i zero = _mm256_setzero_si256();\n    int remainder[8];\n    _mm256_storeu_si256((__m256i*)remainder, rem);\n    for(int i=0;i<8;i++) {\n        if(remainder[i]==0 && primes[i]!=len) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int vec_size = 4; // AVX processes 4 double values at once\n    __m256d input = _mm256_set1_pd(N); // Broadcast N into all lanes\n    __m256d powers_of_ten = _mm256_setr_pd(1e15, 1e14, 1e13, 1e12);\n    __m256d digits = _mm256_setzero_pd();\n    \n    for(int i = 0; i < 16; ++i){\n        __m256d divisors = _mm256_set1_pd(powers_of_ten[i]);\n        __m256d quotients = _mm256_div_pd(input, divisors);\n        __m256d floored = _mm256_floor_pd(quotients);\n        __m256d digit = _mm256_sub_pd(floored, _mm256_mul_pd(_mm256_sub_pd(floored, floored), _mm256_set1_pd(1)));\n        digits = _mm256_add_pd(digits, digit);\n        input = _mm256_sub_pd(input, _mm256_mul_pd(floored, divisors));\n    }\n    \n    double sum_digits[4];\n    _mm256_storeu_pd(sum_digits, digits);\n    uint64_t total_sum = static_cast<uint64_t>(sum_digits[0] + sum_digits[1] + sum_digits[2] + sum_digits[3]);\n    \n    std::string binary_result;\n    while(total_sum > 0){\n        binary_result += (total_sum % 2 == 1) ? '1' : '0';\n        total_sum /= 2;\n    }\n    std::reverse(binary_result.begin(), binary_result.end());\n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int vec_size = 4; // AVX operates on 4 double values\n    __m256d input = _mm256_set1_pd(N); // Broadcast N into all lanes\n    __m256d powers_of_ten = _mm256_setr_pd(1e15, 1e14, 1e13, 1e12);\n    __m256d digits = _mm256_setzero_pd();\n    \n    for(int i = 0; i < 16; ++i){\n        __m256d divisors = _mm256_set1_pd(powers_of_ten[i]);\n        __m256d quotients = _mm256_div_pd(input, divisors);\n        __m256d floored = _mm256_floor_pd(quotients);\n        __m256d subtracted = _mm256_sub_pd(quotients, floored);\n        digits = _mm256_add_pd(digits, floored);\n        input = _mm256_sub_pd(input, _mm256_mul_pd(floored, divisors));\n    }\n    \n    // Convert to integers and sum\n    __m256i digit_ints = _mm256_cvtpd_epi32(digits);\n    __m256i sum = _mm256_setzero_si256();\n    for(int i = 0; i < 4; ++i){\n        __m256i lane = _mm256_permutevar8x32_epi32(digit_ints, _mm256_set_epi32(0, 1, 2, 3, 0, 0, 0, 0));\n        sum = _mm256_add_epi32(sum, lane);\n    }\n    \n    // Horizontal sum\n    __m128i low = _mm256_castsi256_si128(sum);\n    __m128i high = _mm256_extractf128_si256(sum, 1);\n    __m128i total = _mm_add_epi32(low, high);\n    total = _mm_add_epi32(total, _mm_shuffle_epi32(total, _MM_SHUFFLE(2, 3, 0, 1)));\n    total = _mm_add_epi32(total, _mm_shuffle_epi32(total, _MM_SHUFFLE(0, 1, 2, 3)));\n    \n    int final_sum = _mm_cvtsi128_si32(total);\n    \n    // Convert to binary string\n    std::string binary = \"\";\n    while(final_sum > 0){\n        binary = std::to_string(final_sum % 2) + binary;\n        final_sum /= 2;\n    }\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int vec_size = 4; // AVX operates on 4 double values\n    __m256d input = _mm256_set1_pd(N); // Broadcast N into all lanes\n    __m256d powers_of_ten = _mm256_setr_pd(1e15, 1e14, 1e13, 1e12); // Precomputed powers of ten\n    __m256d digits = _mm256_setzero_pd(); // Initialize digit sums\n\n    for(int i = 0; i < 16; ++i){\n        __m256d current_digit = _mm256_div_pd(input, powers_of_ten);\n        current_digit = _mm256_round_pd(current_digit, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n        digits = _mm256_add_pd(digits, current_digit);\n\n        input = _mm256_sub_pd(input, _mm256_mul_pd(current_digit, powers_of_ten));\n        powers_of_ten = _mm256_div_pd(powers_of_ten, 10.0);\n    }\n\n    double sum_digits[4];\n    _mm256_storeu_pd(sum_digits, digits);\n\n    uint64_t total_sum = static_cast<uint64_t>(sum_digits[0] + sum_digits[1] + sum_digits[2] + sum_digits[3]);\n    \n    std::string binary_result;\n    while(total_sum > 0){\n        binary_result += (total_sum % 2 == 0) ? '0' : '1';\n        total_sum /= 2;\n    }\n    if(binary_result.empty()) binary_result = \"0\";\n    std::reverse(binary_result.begin(), binary_result.end());\n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int vec_size = 4; // AVX processes 4 double values at once\n    __m256d input = _mm256_set1_pd(N); // Broadcast N into all lanes\n    __m256d powers_of_ten = _mm256_setr_pd(1e15, 1e14, 1e13, 1e12);\n    __m256d tens = _mm256_setr_pd(10, 10, 10, 10);\n    __m256d ones = _mm256_set1_pd(1);\n    __m256d digits_sum = _mm256_setzero_pd();\n\n    for(int i = 0; i < 16; ++i){\n        __m256d div = _mm256_div_pd(input, powers_of_ten);\n        __m256d floor_val = _mm256_floor_pd(div);\n        digits_sum = _mm256_add_pd(digits_sum, floor_val);\n        input = _mm256_sub_pd(input, _mm256_mul_pd(floor_val, powers_of_ten));\n        powers_of_ten = _mm256_div_pd(powers_of_ten, tens);\n    }\n\n    double sum_arr[4];\n    _mm256_storeu_pd(sum_arr, digits_sum);\n\n    uint64_t total_sum = static_cast<uint64_t>(sum_arr[0] + sum_arr[1] + sum_arr[2] + sum_arr[3]);\n    \n    std::string binary_result;\n    while(total_sum > 0){\n        binary_result += (total_sum % 2 == 0) ? '0' : '1';\n        total_sum /= 2;\n    }\n    std::reverse(binary_result.begin(), binary_result.end());\n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int vec_size = 4; // AVX operates on 4 double values\n    __m256d input = _mm256_set1_pd(N); // Broadcast N into all elements of the vector\n    __m256d powers_of_ten = _mm256_setr_pd(1e15, 1e14, 1e13, 1e12); // Precompute powers of ten\n    __m256d digits = _mm256_setzero_pd(); // Initialize digit sums to zero\n\n    for(int i = 0; i < 16; i += vec_size){\n        __m256d current_chunk = _mm256_div_pd(input, powers_of_ten);\n        __m256d truncated = _mm256_floor_pd(current_chunk);\n        digits = _mm256_add_pd(digits, truncated);\n\n        input = _mm256_sub_pd(input, _mm256_mul_pd(truncated, powers_of_ten));\n        powers_of_ten = _mm256_div_pd(powers_of_ten, _mm256_set1_pd(10));\n    }\n\n    double sum[4];\n    _mm256_storeu_pd(sum, digits);\n\n    uint64_t total_sum = static_cast<uint64_t>(sum[0] + sum[1] + sum[2] + sum[3]);\n    \n    char buffer[64];\n    snprintf(buffer, sizeof(buffer), \"%llu\", total_sum);\n    return std::string(buffer);\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```cpp\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n    int i = 0;\n\n    __m256i evens_mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i odd_indices_mask = _mm256_set_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n\n    while(i + 8 <= n){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_check = _mm256_and_si256(vec, evens_mask);\n        __m256i odd_check = _mm256_and_si256(vec, odd_indices_mask);\n        \n        __m256i even_at_odd = _mm256_and_si256(even_check, odd_check);\n        sum += _mm256_cvtsi256_si32(even_at_odd);\n        sum += _mm256_extract_epi32(even_at_odd, 1);\n        sum += _mm256_extract_epi32(even_at_odd, 2);\n        sum += _mm256_extract_epi32(even_at_odd, 3);\n        sum += _mm256_extract_epi32(even_at_odd, 4);\n        sum += _mm256_extract_epi32(even_at_odd, 5);\n        sum += _mm256_extract_epi32(even_at_odd, 6);\n        sum += _mm256_extract_epi32(even_at_odd, 7);\n\n        i += 8;\n    }\n\n    for(; i < n; ++i){\n        if(i % 2 == 1 && lst[i] % 2 == 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    const int n = lst.size();\n    int i = 0;\n\n    // Ensure alignment for AVX\n    if (((size_t)&lst[0] % 32) == 0 && n >= 8) {\n        __m256i vsum = _mm256_setzero_si256();\n        __m256i mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n        for (; i + 8 <= n; i += 8) {\n            __m256i vdata = _mm256_loadu_si256((__m256i*)&lst[i]);\n            __m256i veven = _mm256_and_si256(vdata, mask);\n            __m256i visodd = _mm256_cmpgt_epi32(_mm256_set1_epi32(i), _mm256_setzero_si256());\n            __m256i vfiltered = _mm256_and_si256(veven, visodd);\n            vsum = _mm256_add_epi32(vsum, vfiltered);\n        }\n\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, vsum);\n        for (int j = 0; j < 8; ++j) {\n            sum += temp[j];\n        }\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    const int n = lst.size();\n    int i = 0;\n\n    // Ensure alignment for AVX\n    if (((size_t)&lst[0] % 32) == 0 && n >= 8) {\n        __m256i vsum = _mm256_setzero_si256();\n        __m256i vodd_mask = _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 0);\n\n        for (; i + 8 <= n; i += 8) {\n            __m256i vdata = _mm256_loadu_si256((__m256i*)&lst[i]);\n            __m256i veven_mask = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n            __m256i veven = _mm256_and_si256(vdata, veven_mask);\n            __m256i vodd = _mm256_and_si256(vdata, vodd_mask);\n            __m256i veven_at_odd = _mm256_and_si256(veven, vodd);\n            vsum = _mm256_add_epi32(vsum, veven_at_odd);\n        }\n\n        int temp_sum[8];\n        _mm256_storeu_si256((__m256i*)temp_sum, vsum);\n        sum += temp_sum[0] + temp_sum[1] + temp_sum[2] + temp_sum[3] +\n               temp_sum[4] + temp_sum[5] + temp_sum[6] + temp_sum[7];\n    }\n\n    // Handle remaining elements\n    for (; i < n; i++) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n    int i = 0;\n\n    __m256i zero = _mm256_setzero_si256();\n    __m256i mask_odd = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n    while(i + 8 <= n){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_mask = _mm256_and_si256(_mm256_set1_epi32(1), _mm256_srli_epi32(vec, 1));\n        __m256i odd_indices = _mm256_andnot_si256(mask_odd, vec);\n        __m256i even_at_odd = _mm256_and_si256(even_mask, odd_indices);\n        __m256i sum_vec = _mm256_add_epi32(_mm256_mullo_epi32(even_at_odd, mask_odd), zero);\n        int temp[8];\n        _mm256_storeu_si256((__m256i*)temp, sum_vec);\n        for(int j = 0; j < 8; j++) {\n            sum += temp[j];\n        }\n        i += 8;\n    }\n\n    for(; i < n; i++){\n        if(i % 2 == 1 && lst[i] % 2 == 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    const int n = lst.size();\n    int i = 0;\n\n    // Ensure alignment for AVX\n    if (((size_t)&lst[0] % 32) == 0 && n >= 8) {\n        __m256i vsum = _mm256_setzero_si256();\n        __m256i vodd = _mm256_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n\n        for (; i + 8 <= n; i += 8) {\n            __m256i vdata = _mm256_loadu_si256((__m256i*)&lst[i]);\n            __m256i veven = _mm256_andnot_si256(vodd, vdata);\n            __m256i veven_at_odd = _mm256_permutevar8x32_epi32(veven, _mm256_setr_epi32(1, 0, 3, 2, 5, 4, 7, 6));\n            vsum = _mm256_add_epi32(vsum, veven_at_odd);\n        }\n\n        int* sum_array = (int*)_mm256_malloc(8 * sizeof(int), 32);\n        _mm256_store_si256((__m256i*)sum_array, vsum);\n        for (int j = 0; j < 8; ++j) {\n            sum += sum_array[j];\n        }\n        _mm256_free(sum_array);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (i % 2 == 1 && lst[i] % 2 == 0) {\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    const int vec_len = 32; // AVX2 vector size in bytes\n    const int shift = 4;    // Two rotations * two places\n    const char *alphabet = \"abcdefghijklmnopqrstuvwxyz\";\n    \n    int len = s.size();\n    std::string result = s;\n    \n    for(int i = 0; i < len; i += vec_len){\n        int chunk_size = std::min(vec_len, len - i);\n        \n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i alphabet_base = _mm256_set1_epi8(alphabet[0]);\n        __m256i shift_mask = _mm256_set1_epi8(shift);\n        \n        // Compute indices with wrap-around\n        __m256i indices = _mm256_add_epi8(input, shift_mask);\n        __m256i wrapped_indices = _mm256_sub_epi8(indices, _mm256_set1_epi8(26));\n        __m256i mask = _mm256_cmpgt_epi8(_mm256_set1_epi8(25), indices);\n        __m256i final_indices = _mm256_blendv_epi8(wrapped_indices, indices, mask);\n        \n        // Lookup encrypted characters\n        __m256i encrypted = _mm256_shuffle_epi8(_mm256_loadu_si256(reinterpret_cast<const __m256i*>(alphabet)), final_indices);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), encrypted);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    size_t len = s.size();\n    __m256i shift = _mm256_set1_epi8(2);\n    __m256i base = _mm256_setr_epi8(\n        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n        'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n        'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n        'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f'\n    );\n    std::string result;\n    result.reserve(len);\n\n    for(size_t i = 0; i + 32 <= len; i += 32){\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        __m256i mask = _mm256_cmpgt_epi8(_mm256_set1_epi8('z'), input);\n        __m256i wrapped = _mm256_sub_epi8(encrypted, _mm256_set1_epi8(26));\n        __m256i final = _mm256_or_si256(_mm256_andnot_si256(mask, encrypted), _mm256_and_si256(mask, wrapped));\n        __m256i mod = _mm256_or_si256(_mm256_and_si256(final, _mm256_cmpgt_epi8(final, base)), \n                                      _mm256_andnot_si256(_mm256_cmpgt_epi8(final, base), \n                                                          _mm256_sub_epi8(final, base)));\n        char output[32];\n        _mm256_storeu_si256((__m256i*)output, mod);\n        result.append(output, 32);\n    }\n\n    for(size_t i = len - (len % 32); i < len; ++i){\n        char c = s[i];\n        if(c >= 'a' && c <= 'z'){\n            c = (c - 'a' + 2) % 26 + 'a';\n        }\n        result += c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    const int vec_len = 32;\n    const char shift = 4; // 2 * 2\n    const char base = 'a';\n    const char mask = 0b11011011; // To wrap around within 'a'-'z'\n\n    size_t len = s.size();\n    std::string result(len, ' ');\n\n    __m256i shift_vec = _mm256_set1_epi8(shift);\n    __m256i base_vec = _mm256_set1_epi8(base);\n    __m256i mask_vec = _mm256_set1_epi8(mask);\n\n    for (size_t i = 0; i < len; i += vec_len) {\n        size_t chunk_size = (len - i >= vec_len) ? vec_len : len % vec_len;\n\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i shifted = _mm256_add_epi8(input, shift_vec);\n        __m256i wrapped = _mm256_and_si256(shifted, mask_vec);\n        __m256i encrypted = _mm256_add_epi8(wrapped, base_vec);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), encrypted);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    size_t len = s.size();\n    __m256i shift = _mm256_set1_epi8(2);\n    __m256i base = _mm256_setr_epi8(\n        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n        'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n        'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n        'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f'\n    );\n    std::string result;\n    result.reserve(len);\n\n    for(size_t i = 0; i + 32 <= len; i += 32){\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        __m256i mask = _mm256_cmpgt_epi8(_mm256_add_epi8(input, shift), _mm256_set1_epi8('z'));\n        __m256i wrapped = _mm256_andnot_si256(mask, _mm256_sub_epi8(encrypted, _mm256_set1_epi8('z' - 'a')));\n        __m256i unwrapped = _mm256_or_si256(wrapped, _mm256_and_si256(mask, _mm256_sub_epi8(base, _mm256_set1_epi8('a'))));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), unwrapped);\n    }\n\n    for(size_t i = len - (len % 32); i < len; ++i){\n        char c = s[i];\n        if(c >= 'a' && c <= 'z'){\n            char enc = c + 2;\n            if(enc > 'z') enc -= 26;\n            result += enc;\n        }\n        else{\n            result += c;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    size_t len = s.size();\n    __m256i shift = _mm256_set1_epi8(2);\n    __m256i base = _mm256_setr_epi8(\n        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n        'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n        'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n        'y', 'z', 'a', 'b', 'c', 'd', 'e', 'f'\n    );\n    std::string result;\n    result.reserve(len);\n\n    for(size_t i = 0; i + 32 <= len; i += 32){\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.data() + i));\n        __m256i encrypted = _mm256_add_epi8(input, shift);\n        __m256i mask = _mm256_cmpgt_epi8(_mm256_add_epi8(base, shift), input);\n        __m256i adjusted = _mm256_sub_epi8(encrypted, _mm256_andnot_si256(mask, _mm256_set1_epi8(26)));\n        __m256i final = _mm256_or_si256(_mm256_andnot_si256(mask, input), _mm256_and_si256(mask, adjusted));\n        result.append(reinterpret_cast<char*>(&final), 32);\n    }\n\n    for(size_t i = len - (len % 32); i < len; ++i){\n        char c = s[i];\n        if(c >= 'a' && c <= 'z'){\n            c = ((c - 'a' + 2) % 26) + 'a';\n        }\n        result += c;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string &message) {\n    const int vec_size = 32; // AVX size in bytes\n    const char vowels[] = \"AEIOUaeiou\";\n    const char vowel_shifts[] = \"CGQWcgqy\";\n    __m256i v_vowels = _mm256_loadu_si256((__m256i*)vowels);\n    __m256i v_vowel_shifts = _mm256_loadu_si256((__m256i*)vowel_shifts);\n\n    std::string encoded;\n    encoded.reserve(message.size());\n\n    for (size_t i = 0; i < message.size(); i += vec_size / 2) {\n        __m256i v_input = _mm256_setzero_si256();\n        if (i + vec_size / 2 <= message.size()) {\n            v_input = _mm256_loadu_si256((__m256i*)(message.data() + i));\n        } else {\n            char padding[vec_size / 2];\n            for (size_t j = 0; j < vec_size / 2; ++j) {\n                padding[j] = i + j < message.size() ? message[i + j] : '\\0';\n            }\n            v_input = _mm256_loadu_si256((__m256i*)padding);\n        }\n\n        __m256i v_mask_upper = _mm256_cmpgt_epi8(v_input, _mm256_set1_epi8('A' - 1));\n        __m256i v_mask_lower = _mm256_cmpgt_epi8(_mm256_set1_epi8('z' + 1), v_input);\n        __m256i v_is_alpha = _mm256_and_si256(v_mask_upper, v_mask_lower);\n\n        __m256i v_vowel_mask = _mm256_cmpeq_epi8(v_input, v_vowels);\n        __m256i v_shifted = _mm256_or_si256(_mm256_andnot_si256(v_vowel_mask, v_input), _mm256_and_si256(v_vowel_mask, v_vowel_shifts));\n\n        __m256i v_case_swapped = _mm256_xor_si256(v_shifted, _mm256_set1_epi8(0x20));\n        __m256i v_final = _mm256_blendv_epi8(v_shifted, v_case_swapped, _mm256_castps256_ps128(_mm256_andnot_ps(_mm256_castsi256_ps(v_is_alpha), _mm256_castsi256_ps(_mm256_set1_epi32(-1)))));\n\n        char output[vec_size / 2];\n        _mm256_storeu_si256((__m256i*)output, v_final);\n        for (int j = 0; j < vec_size / 2; ++j) {\n            if (output[j] != '\\0') {\n                encoded.push_back(output[j]);\n            }\n        }\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string &message) {\n    const int vec_size = 32; // AVX size in bytes\n    const char vowels[] = \"AEIOUaeiou\";\n    const char vowel_shifts[] = \"CGQWcgqw\";\n    __m256i v_vowels = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(vowels));\n    __m256i v_vowel_shifts = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(vowel_shifts));\n\n    std::string encoded;\n    encoded.reserve(message.size());\n\n    for (size_t i = 0; i + vec_size <= message.size(); i += vec_size) {\n        __m256i v_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n        __m256i v_lower = _mm256_andnot_si256(_mm256_set1_epi8(0x20), v_chars);\n        __m256i v_vowel_mask = _mm256_cmpeq_epi8(v_lower, _mm256_permutevar8x32_epi32(v_vowels, _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7)));\n        __m256i v_encoded = _mm256_blendv_epi8(v_chars, v_vowel_shifts, v_vowel_mask);\n\n        for (int j = 0; j < 32; ++j) {\n            if (_mm256_test_bit_epi8(j, v_vowel_mask)) {\n                v_encoded = _mm256_set_m128i(\n                    _mm_or_si128(_mm256_extractf128_si256(v_encoded, 1), _mm_andnot_si128(_mm_set1_epi8(0x20), _mm256_extractf128_si256(v_vowel_shifts, 1))),\n                    _mm_or_si128(_mm256_extractf128_si256(v_encoded, 0), _mm_andnot_si128(_mm_set1_epi8(0x20), _mm256_extractf128_si256(v_vowel_shifts, 0)))\n                );\n            }\n        }\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(encoded.data()) + (i / vec_size), v_encoded);\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\nstd::string encode_simd(const std::string & message){\n    const int vec_len = 32; // AVX2 vector length in bytes\n    const char vowels[] = \"AEIOUaeiou\";\n    const char vowel_shift[] = \"CGTUcg tu\";\n    \n    std::string result = message;\n    size_t i = 0;\n    \n    while(i + vec_len <= message.size()){\n        __m256i msg_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[i]));\n        __m256i swapped_case_vec = _mm256_or_si256(\n            _mm256_andnot_si256(_mm256_set1_epi8(0x20), msg_vec), // Uppercase\n            _mm256_set1_epi8(0x20)                               // Toggle bit for lowercase\n        );\n        \n        __m256i vowel_mask = _mm256_cmpistrm(\n            _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&vowels[0])),\n            swapped_case_vec,\n            _SIDD_EQ_OPS | _SIDD_LEAST_SIGNIFICANT | _SIDD_NEGATIVE_POLARITY\n        );\n        \n        __m256i encoded_vec = _mm256_blendv_epi8(\n            swapped_case_vec,\n            _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&vowel_shift[0])),\n            vowel_mask\n        );\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), encoded_vec);\n        i += vec_len;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string &message) {\n    const int vec_len = 32;\n    const char vowels[] = \"AEIOUaeiou\";\n    const char vowel_shifts[] = \"CGKQSUcgksu\";\n    __m256i v_vowels = _mm256_loadu_si256((__m256i*)vowels);\n    __m256i v_vowel_shifts = _mm256_loadu_si256((__m256i*)vowel_shifts);\n\n    std::string encoded;\n    encoded.reserve(message.size());\n\n    for (size_t i = 0; i + vec_len <= message.size(); i += vec_len) {\n        __m256i v_input = _mm256_loadu_si256((__m256i*)(message.data() + i));\n        __m256i v_lower = _mm256_andnot_si256(_mm256_set1_epi8(0x20), v_input);\n        __m256i v_upper_mask = _mm256_cmpgt_epi8(v_lower, _mm256_set1_epi8('A' - 1));\n        __m256i v_lower_mask = _mm256_andnot_si256(v_upper_mask, _mm256_set1_epi8(-1));\n\n        __m256i v_vowel_test = _mm256_cmpeq_epi8(v_lower, v_vowels);\n        __m256i v_shifted = _mm256_or_si256(_mm256_and_si256(v_vowel_test, v_vowel_shifts), \n                                            _mm256_andnot_si256(v_vowel_test, v_lower));\n\n        __m256i v_result = _mm256_or_si256(_mm256_and_si256(v_upper_mask, v_shifted),\n                                           _mm256_andnot_si256(v_upper_mask, v_input));\n        _mm256_storeu_si256((__m256i*)(encoded.data() + i), v_result);\n    }\n\n    // Handle remaining characters\n    for (size_t i = message.size() - (message.size() % vec_len); i < message.size(); ++i) {\n        char c = message[i];\n        if (c >= 'A' && c <= 'Z') {\n            c = c + ('a' - 'A');\n        }\n        if (strchr(vowels, c)) {\n            int idx = strchr(vowels, c) - vowels;\n            c = vowel_shifts[idx];\n        }\n        encoded += c;\n    }\n\n    return encoded;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\nstd::string encode_simd(const std::string & message){\n    const int vec_size = 32; // AVX2 operates on 256-bit vectors, which can hold 32 characters assuming 8-bit chars\n    const char vowels[] = \"AEIOUaeiou\";\n    const char vowel_shifts[] = \"CGQWcgqy\";\n    \n    size_t len = message.size();\n    std::string result = message;\n    \n    for(size_t i = 0; i < len; i += vec_size){\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(message.data() + i));\n        __m256i lower_mask = _mm256_set1_epi8(0x20);\n        __m256i lower_case = _mm256_and_si256(input, lower_mask); // Convert to lowercase\n        \n        // Check if character is a vowel\n        __m256i is_vowel = _mm256_setzero_si256();\n        for(int j = 0; j < 10; ++j){\n            __m256i vowel = _mm256_set1_epi8(vowels[j]);\n            __m256i eq = _mm256_cmpeq_epi8(lower_case, vowel);\n            is_vowel = _mm256_or_si256(is_vowel, eq);\n        }\n        \n        // Shift vowels by 2 positions\n        __m256i shifted_vowels = _mm256_setzero_si256();\n        for(int j = 0; j < 10; ++j){\n            __m256i vowel = _mm256_set1_epi8(vowels[j]);\n            __m256i shift = _mm256_set1_epi8(vowel_shifts[j] - vowels[j]);\n            __m256i shifted = _mm256_add_epi8(vowel, shift);\n            shifted_vowels = _mm256_or_si256(shifted_vowels, shifted);\n        }\n        \n        // Replace vowels in input with shifted vowels\n        __m256i output = _mm256_blendv_epi8(input, shifted_vowels, is_vowel);\n        \n        // Swap case\n        __m256i swapped_case = _mm256_xor_si256(output, lower_mask);\n        \n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(result.data() + i), swapped_case);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    const int vec_size = 32; // AVX register size\n    bool all_upper = true;\n    bool all_lower = true;\n\n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n        if(key.size() == 0) continue;\n\n        const char *str = key.c_str();\n        __m256i upper_mask = _mm256_set1_epi8('_'); // Mask for uppercase letters\n        __m256i lower_mask = _mm256_set1_epi8('_'); // Mask for lowercase letters\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str));\n        \n        for(int i=0; i<key.size(); i+=vec_size){\n            __m256i chars_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(chars_vec, _mm256_set1_epi8('A')), _mm256_setzero_si256());\n            __m256i lower_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(chars_vec, _mm256_set1_epi8('a')), _mm256_setzero_si256());\n\n            upper_mask = _mm256_and_si256(upper_mask, upper_check);\n            lower_mask = _mm256_and_si256(lower_mask, lower_check);\n        }\n\n        all_upper &= (_mm256_movemask_epi8(upper_mask) == 0);\n        all_lower &= (_mm256_movemask_epi8(lower_mask) == 0);\n\n        if(!all_upper && !all_lower) return false;\n    }\n\n    return all_upper || all_lower;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    const int vec_size = 32; // AVX register size\n    bool all_upper = true;\n    bool all_lower = true;\n\n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n        if(key.size() == 0) continue;\n\n        const char *str = key.c_str();\n        __m256i upper_mask = _mm256_set1_epi8('_'); // Mask for uppercase check\n        __m256i lower_mask = _mm256_set1_epi8('_'); // Mask for lowercase check\n        int i = 0;\n\n        while(i + vec_size <= key.size()){\n            __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n            \n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(vec, _mm256_set1_epi8('A')), _mm256_set1_epi8(0));\n            __m256i lower_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(vec, _mm256_set1_epi8('a')), _mm256_set1_epi8(0));\n\n            upper_mask = _mm256_andnot_si256(upper_check, upper_mask);\n            lower_mask = _mm256_andnot_si256(lower_check, lower_mask);\n\n            if(!_mm256_movemask_epi8(upper_mask)) all_upper = false;\n            if(!_mm256_movemask_epi8(lower_mask)) all_lower = false;\n\n            if(!all_upper && !all_lower) return false;\n            i += vec_size;\n        }\n    }\n\n    return all_upper || all_lower;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    const int vec_size = 32; // AVX register size\n    bool all_upper = true;\n    bool all_lower = true;\n\n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n        if(key.size() == 0) continue;\n\n        const char *str = key.c_str();\n        __m256i upper_mask = _mm256_set1_epi8('_'); // Mask for uppercase letters\n        __m256i lower_mask = _mm256_set1_epi8('_'); // Mask for lowercase letters\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str));\n        \n        for(int i=0; i<key.size(); i+=vec_size){\n            __m256i current_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(current_chars, _mm256_set1_epi8('A')), _mm256_setzero_si256());\n            __m256i lower_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(_mm256_set1_epi8('z'), current_chars), _mm256_setzero_si256());\n\n            upper_mask = _mm256_and_si256(upper_mask, upper_check);\n            lower_mask = _mm256_and_si256(lower_mask, lower_check);\n        }\n\n        all_upper &= (_mm256_movemask_epi8(upper_mask) == 0);\n        all_lower &= (_mm256_movemask_epi8(lower_mask) == 0);\n\n        if(!all_upper && !all_lower) return false;\n    }\n\n    return all_upper || all_lower;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    const int vec_size = 32; // AVX: 256 bits\n    const __m256i lower_mask = _mm256_set1_epi8(0x20);\n    const __m256i upper_mask = _mm256_setzero_si256();\n    \n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n        if(key.size() == 0) continue;\n\n        const char *key_data = key.data();\n        size_t i = 0;\n        size_t remaining = key.size();\n\n        while(remaining >= vec_size){\n            __m256i key_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(key_data + i));\n            __m256i key_lower = _mm256_and_si256(key_vec, lower_mask);\n            __m256i key_upper = _mm256_andnot_si256(lower_mask, key_vec);\n\n            bool all_lower = _mm256_testz_si256(_mm256_xor_si256(key_lower, key_vec), lower_mask);\n            bool all_upper = _mm256_testz_si256(_mm256_xor_si256(key_upper, key_vec), upper_mask);\n\n            if(!all_lower && !all_upper){\n                return false;\n            }\n\n            i += vec_size;\n            remaining -= vec_size;\n        }\n\n        // Handle remaining characters\n        while(i < key.size()){\n            char c = key[i];\n            if(c >= 'A' && c <= 'Z') {\n                if(remaining > 1){\n                    char next_c = key[i+1];\n                    if(next_c >= 'a' && next_c <= 'z'){\n                        return false;\n                    }\n                }\n            }\n            else if(c >= 'a' && c <= 'z'){\n                if(remaining > 1){\n                    char next_c = key[i+1];\n                    if(next_c >= 'A' && next_c <= 'Z'){\n                        return false;\n                    }\n                }\n            }\n            else{\n                return false;\n            }\n            i++;\n            remaining--;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    const int vec_size = 32; // AVX register size\n    bool all_upper = true;\n    bool all_lower = true;\n\n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n        if(key.size() == 0) continue;\n\n        const char *str = key.c_str();\n        __m256i upper_mask = _mm256_set1_epi8('_'); // Mask for uppercase letters\n        __m256i lower_mask = _mm256_set1_epi8('_'); // Mask for lowercase letters\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str));\n        \n        for(int i=0; i<key.size(); i+=vec_size){\n            __m256i current_chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(str + i));\n            __m256i upper_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(current_chars, _mm256_set1_epi8('A')), _mm256_setzero_si256());\n            __m256i lower_check = _mm256_cmpgt_epi8(_mm256_sub_epi8(_mm256_set1_epi8('z'), current_chars), _mm256_setzero_si256());\n\n            upper_mask = _mm256_and_si256(upper_mask, upper_check);\n            lower_mask = _mm256_and_si256(lower_mask, lower_check);\n        }\n\n        if(!_mm256_testz_si256(upper_mask, upper_mask)) all_lower = false;\n        if(!_mm256_testz_si256(lower_mask, lower_mask)) all_upper = false;\n\n        if(!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // AVX2 registers can hold 8 integers each\n    __m256i sieve = _mm256_set_epi32(7,5,3,2,0,0,0,0);\n    __m256i mask = _mm256_set1_epi32(1);\n\n    int index = 8;\n    while(index < n){\n        __m256i multiples = _mm256_mullo_epi32(sieve, mask);\n        __m256i cmp = _mm256_cmpgt_epi32(_mm256_set1_epi32(n), multiples);\n        int mask_result = _mm256_movemask_epi8(cmp);\n\n        if(mask_result != 0){\n            primes.push_back(index - 7);\n            primes.push_back(index - 6);\n            primes.push_back(index - 5);\n            primes.push_back(index - 4);\n            primes.push_back(index - 3);\n            primes.push_back(index - 2);\n            primes.push_back(index - 1);\n            primes.push_back(index);\n        }\n\n        sieve = _mm256_add_epi32(sieve, mask);\n        index += 8;\n    }\n\n    // Handle remaining numbers less than n\n    for(int i = index - 8; i < n; ++i){\n        bool is_prime = true;\n        for(auto p : primes){\n            if(i % p == 0){\n                is_prime = false;\n                break;\n            }\n        }\n        if(is_prime){\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // AVX2 registers can hold 8 integers each\n    __m256i sieve = _mm256_set_epi32(7,5,3,2,0,0,0,0);\n    __m256i limit = _mm256_set1_epi32(n);\n    __m256i mask = _mm256_set_epi32(0,0,0,0,0xFF,0xFF,0xFF,0xFF);\n\n    while(_mm256_movemask_epi8(_mm256_cmpeq_epi32(sieve, _mm256_setzero_epi32())) != 0xFF){\n        int p = _mm256_movemask_epi8(_mm256_cmpgt_epi32(sieve, _mm256_setzero_epi32()));\n        p = __builtin_ctz(p);\n        int prime = _mm256_extract_epi32(sieve, p);\n        primes.push_back(prime);\n\n        __m256i step = _mm256_set1_epi32(prime);\n        __m256i start = _mm256_add_epi32(sieve, step);\n        __m256i end = _mm256_min_epi32(start, limit);\n        __m256i mask = _mm256_set_epi32(0,0,0,0,0xFF,0xFF,0xFF,0xFF);\n        __m256i cmp = _mm256_and_si256(_mm256_cmpgt_epi32(end, start), mask);\n        sieve = _mm256_maskload_epi32((const int*)&start, cmp);\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // AVX2 registers can hold 8 integers each\n    __m256i sieve = _mm256_set_epi32(7, 5, 3, 2, 19, 17, 13, 11);\n    __m256i mask = _mm256_set1_epi32(1);\n\n    while(_mm256_movemask_epi8(_mm256_cmpeq_epi32(sieve, mask)) != 0xFF){\n        int p = _mm256_movemask_epi8(_mm256_cmpeq_epi32(sieve, mask));\n        for(int i=0;i<8;i++) {\n            if(p & (1<<i)){\n                primes.push_back(_mm256_extract_epi32(sieve, i));\n                __m256i step = _mm256_set1_epi32(_mm256_extract_epi32(sieve, i));\n                __m256i range = _mm256_add_epi32(step, _mm256_setr_epi32(0,1,2,3,4,5,6,7));\n                sieve = _mm256_sub_epi32(sieve, _mm256_and_si256(range, _mm256_cmpgt_epi32(sieve, _mm256_setzero_si256())));\n            }\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // AVX2 registers can hold 8 integers each\n    __m256i sieve = _mm256_set_epi32(7, 5, 3, 2, 0, 0, 0, 0);\n    __m256i mask = _mm256_set1_epi32(1);\n\n    while(_mm256_extract_epi32(sieve, 0) < n){\n        bool is_prime = true;\n        for(int i = 0; i < 4; ++i){\n            int val = _mm256_extract_epi32(sieve, i);\n            if(val >= n) continue;\n            if(val < 2){\n                is_prime = false;\n                break;\n            }\n            __m256i temp = _mm256_set1_epi32(val);\n            __m256i mul = _mm256_mullo_epi32(temp, sieve);\n            if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(mul, sieve)) != 0){\n                is_prime = false;\n                break;\n            }\n        }\n        if(is_prime){\n            primes.push_back(_mm256_extract_epi32(sieve, 0));\n            __m256i increment = _mm256_set1_epi32(1);\n            sieve = _mm256_add_epi32(sieve, increment);\n        }\n        else{\n            sieve = _mm256_add_epi32(sieve, mask);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // AVX2 register to hold sieve mask\n    __m256i sieve_mask = _mm256_set1_epi8(-1);\n\n    // Initialize sieve array\n    int sieve_size = (n + 31) / 32 * 4; // Each int covers 32 bits\n    std::vector<int> sieve(sieve_size, -1);\n\n    // Set 0 and 1 as non-prime\n    sieve[0] &= ~((1 << 0) | (1 << 1));\n\n    // SIMD sieve of Eratosthenes\n    for(int i = 2; i*i < n; i += 8){\n        __m256i current = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i mask = _mm256_loadu_si256((__m256i*)&sieve[current]);\n        \n        if(_mm256_movemask_epi8(_mm256_andnot_si256(mask, sieve_mask)) != 0){\n            continue;\n        }\n\n        // Mark multiples as non-prime\n        __m256i step = _mm256_set1_epi32(i);\n        __m256i multiple = _mm256_mullo_epi32(current, step);\n        while(multiple.m256i_i32[0] < n){\n            _mm256_storeu_si256((__m256i*)&sieve[multiple], _mm256_andnot_si256(current, sieve_mask));\n            multiple = _mm256_add_epi32(multiple, step);\n        }\n    }\n\n    // Collect primes\n    for(int i = 2; i < n; ++i){\n        if(sieve[i >> 5] & (1 << (i & 31))){\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    size_t len = s.length();\n    \n    __m256i upper_vowels = _mm256_setr_epi8(\n        'A', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'E', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'I', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'O', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n    \n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i const*)(data + i));\n        __m256i mask_even = _mm256_setr_epi8(\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1,\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1\n        );\n        __m256i even_chars = _mm256_and_si256(chars, mask_even);\n        __m256i cmp_upper = _mm256_cmpgt_epi8(even_chars, _mm256_sub_epi8(upper_vowels, upper_vowels));\n        __m256i cmp_vowel = _mm256_cmpgt_epi8(_mm256_sub_epi8(even_chars, upper_vowels), _mm256_setzero_si256());\n        __m256i is_upper_vowel = _mm256_and_si256(cmp_upper, cmp_vowel);\n        count += _mm256_popcnt_epi8(is_upper_vowel);\n    }\n    \n    for (; i < len; ++i) {\n        if (i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U')) {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    size_t len = s.length();\n\n    __m256i upper_vowels = _mm256_setr_epi8(\n        'A', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'E', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'I', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'O', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n\n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i mask0 = _mm256_cmpgt_epi8(chars, _mm256_set1_epi8('A' - 1));\n        __m256i mask1 = _mm256_cmpgt_epi8(_mm256_set1_epi8('Z' + 1), chars);\n        __m256i upper_mask = _mm256_and_si256(mask0, mask1);\n\n        __m256i vowel_mask = _mm256_cmpeq_epi8(chars, upper_vowels);\n        vowel_mask = _mm256_permute4x64_epi64(vowel_mask, 0x85); // Combine results from different lanes\n\n        __m256i total_mask = _mm256_and_si256(upper_mask, vowel_mask);\n        total_mask = _mm256_and_si256(total_mask, _mm256_set1_epi8(0x01));\n\n        count += _mm256_popcnt_epi64(_mm256_castsi256_si128(total_mask)) +\n                 _mm_popcnt_u64(_mm_extract_epi64(total_mask, 1));\n    }\n\n    for (; i < len; ++i) {\n        if (i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' &&\n            (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' ||\n             data[i] == 'O' || data[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    size_t len = s.length();\n    \n    __m256i upper_vowels = _mm256_setr_epi8(\n        'A','A','A','A','A','A','A','A',\n        'A','A','A','A','A','A','A','A',\n        'E','E','E','E','E','E','E','E',\n        'I','I','I','I','O','O','U','Y'\n    );\n    __m256i mask_even = _mm256_setr_epi8(\n        1,0,1,0,1,0,1,0,\n        1,0,1,0,1,0,1,0,\n        1,0,1,0,1,0,1,0,\n        1,0,1,0,1,0,1,0\n    );\n\n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i even_chars = _mm256_and_si256(chars, mask_even);\n        __m256i cmp_res = _mm256_cmpeq_epi8(even_chars, upper_vowels);\n        __m256i mask = _mm256_movemask_epi8(cmp_res);\n        count += __builtin_popcount(mask);\n    }\n\n    for (; i < len; ++i) {\n        if (i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && \n            (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || \n             data[i] == 'O' || data[i] == 'U' || data[i] == 'Y')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    size_t len = s.length();\n    \n    __m256i upper_vowels = _mm256_setr_epi8(\n        'A', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'E', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'I', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'O', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n    \n    size_t i = 0;\n    for (; i <= len - 32; i += 32) {\n        __m256i chars = _mm256_loadu_si256((__m256i*)(data + i));\n        __m256i mask_even = _mm256_setr_epi8(\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1,\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1\n        );\n        __m256i even_chars = _mm256_and_si256(chars, mask_even);\n        \n        __m256i cmp_res = _mm256_cmpgt_epi8(even_chars, upper_vowels);\n        int mask = _mm256_movemask_epi8(cmp_res);\n        count += __builtin_popcount(mask);\n    }\n    \n    for (; i < len; ++i) {\n        if (i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && \n            (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || \n             data[i] == 'O' || data[i] == 'U')) {\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    size_t len = s.length();\n\n    __m256i upper_vowels = _mm256_setr_epi8(\n        'A', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'E', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'I', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'O', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n        'U', -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1\n    );\n    __m256i zero = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for (; i <= len - 16; i += 16) {\n        __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + i));\n        __m256i mask_even = _mm256_setr_epi8(\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1,\n            0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1\n        );\n        __m256i even_chars = _mm256_and_si256(chars, mask_even);\n        __m256i matches = _mm256_cmpeq_epi8(even_chars, upper_vowels);\n        matches = _mm256_and_si256(matches, mask_even);\n        int vcnt = _mm256_movemask_epi8(matches);\n        count += __builtin_popcount(vcnt);\n    }\n\n    for (; i < len; ++i) {\n        if (i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && \n            (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || \n             data[i] == 'O' || data[i] == 'U')) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(8);\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_odd = _mm256_set_epi32(7, 5, 3, 1, 7, 5, 3, 1);\n    __m256i v_even = _mm256_set_epi32(8, 6, 4, 2, 8, 6, 4, 2);\n\n    __m256i v_is_odd = _mm256_and_si256(v_n, _mm256_set1_epi32(1));\n    __m256i v_next_odd = _mm256_add_epi32(v_n, _mm256_add_epi32(v_n, v_odd));\n    __m256i v_next_even = _mm256_add_epi32(v_n, _mm256_add_epi32(v_n, v_even));\n\n    __m256i v_result = _mm256_blendv_epi8(v_next_odd, v_next_even, v_is_odd);\n\n    _mm256_storeu_si256((__m256i*)result.data(), v_result);\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(8);\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_odd = _mm256_set_epi32(7, 5, 3, 1, 7, 5, 3, 1);\n    __m256i v_even = _mm256_set_epi32(8, 6, 4, 2, 8, 6, 4, 2);\n\n    __m256i v_is_odd = _mm256_and_si256(v_n, _mm256_set1_epi32(1));\n    __m256i v_next_odd = _mm256_add_epi32(v_n, _mm256_add_epi32(v_n, v_odd));\n    __m256i v_next_even = _mm256_add_epi32(v_n, _mm256_add_epi32(v_n, v_even));\n\n    __m256i v_result = _mm256_blendv_epi8(v_next_even, v_next_odd, v_is_odd);\n\n    _mm256_storeu_si256((__m256i*)result.data(), v_result);\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(8);\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_odd = _mm256_set_epi32(7, 5, 3, 1, 7, 5, 3, 1);\n    __m256i v_even = _mm256_set_epi32(8, 6, 4, 2, 8, 6, 4, 2);\n\n    __m256i v_is_odd = _mm256_and_si256(v_n, _mm256_set1_epi32(1));\n    __m256i v_next_odd = _mm256_add_epi32(v_n, _mm256_set1_epi32(2));\n    __m256i v_next_even = _mm256_add_epi32(v_n, _mm256_set1_epi32(1));\n\n    __m256i v_result = _mm256_blendv_epi8(v_next_odd, v_next_even, v_is_odd);\n    \n    _mm256_storeu_si256((__m256i*)result.data(), v_result);\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(8, 0);\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_index = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i v_even_mask = _mm256_set1_epi32(1);\n\n    // Load initial values\n    __m256i v_current = _mm256_add_epi32(v_n, _mm256_sub_epi32(_mm256_mullo_epi32(v_index, v_index), v_index));\n\n    // Store the results\n    _mm256_storeu_si256((__m256i*)&result[0], v_current);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(8);\n    __m256i v_n = _mm256_set1_epi32(n);\n    __m256i v_index = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i v_even_mask = _mm256_set1_epi32(1);\n\n    // Load initial values\n    __m256i v_current = v_n;\n\n    for(int i=0; i<8; i+=4){\n        // Check if current value is odd or even\n        __m256i v_is_odd = _mm256_and_si256(v_current, _mm256_set1_epi32(1));\n        \n        // Compute next odd/even numbers\n        __m256i v_next_odd = _mm256_add_epi32(v_current, _mm256_set1_epi32(2));\n        __m256i v_next_even = _mm256_add_epi32(v_current, _mm256_set1_epi32(1));\n\n        // Select next odd or even based on current parity\n        __m256i v_next = _mm256_blendv_epi8(v_next_odd, v_next_even, _mm256_castps_si256(_mm256_cvtepi32_ps(v_is_odd)));\n\n        // Store results\n        _mm256_storeu_si256((__m256i*)(result.data()+i), v_current);\n\n        // Update current values\n        v_current = v_next;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    __m256i sum_vec = _mm256_setzero_si256();\n    int count = 0;\n    for(int i = n; i <= m; i += 8){\n        __m256i vec = _mm256_set_epi64x(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        sum_vec = _mm256_add_epi64(sum_vec, vec);\n        count += 8;\n    }\n    __m256i sum_scalar;\n    if(count < 8){\n        sum_scalar = _mm256_set_epi64x(0,0,0,0,0,0,0,0);\n        for(int i = m - (count % 8); i <= m; ++i){\n            sum_scalar = _mm256_add_epi64(sum_scalar, _mm256_set1_epi64x(i));\n        }\n    }\n    else{\n        sum_scalar = sum_vec;\n    }\n    __m256i sum_high = _mm256_extractf128_si256(sum_scalar, 1);\n    __m128i sum_low = _mm256_castsi256_si128(sum_scalar);\n    sum_low = _mm_add_epi64(sum_low, sum_high);\n    while(_mm_movemask_epi8(_mm_cmpgt_epi64(sum_low, _mm_set1_epi64x(0))) != 0){\n        __m128i sum_high_half = _mm_unpackhi_epi64(sum_low, sum_low);\n        sum_low = _mm_add_epi64(sum_low, sum_high_half);\n    }\n    int64_t total_sum = _mm_cvtsi128_si64(sum_low);\n    int64_t avg = round((double)total_sum / (m - n + 1));\n    return std::bitset<64>(avg).to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    __m256i sum_vec = _mm256_setzero_si256();\n    int count = 0;\n    for(int i = n; i <= m; i += 8){\n        __m256i vec = _mm256_set_epi64x(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        sum_vec = _mm256_add_epi64(sum_vec, vec);\n        count += 8;\n    }\n    __m256i sum_scalar;\n    if(count < 8){\n        sum_scalar = _mm256_set_epi64x(0,0,0,0,0,0,0,0);\n        for(int i = m - (count % 8); i <= m; ++i){\n            sum_scalar = _mm256_add_epi64(sum_scalar, _mm256_set1_epi64x(i));\n        }\n    }\n    else{\n        sum_scalar = sum_vec;\n    }\n    __m256i sum_high = _mm256_extractf128_si256(sum_scalar, 1);\n    __m128i sum_low = _mm256_castsi256_si128(sum_scalar);\n    sum_low = _mm_add_epi64(sum_low, sum_high);\n    while(_mm_movemask_epi8(_mm_cmpgt_epi64(sum_low, _mm_set1_epi64x(0))) != 0){\n        __m128i sum_high_half = _mm_unpackhi_epi64(sum_low, sum_low);\n        sum_low = _mm_add_epi64(sum_low, sum_high_half);\n    }\n    int64_t total_sum = _mm_cvtsi128_si64(sum_low);\n    int64_t avg = round((double)total_sum / (double)(m - n + 1));\n    return std::bitset<64>(avg).to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    __m256i sum_vec = _mm256_setzero_si256();\n    int count = 0;\n    for(int i = n; i <= m; i += 8){\n        __m256i vec = _mm256_set_epi64x(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        sum_vec = _mm256_add_epi64(sum_vec, vec);\n        count += 8;\n    }\n    __m256i sum_scalar;\n    if(count < 8){\n        sum_scalar = _mm256_set_epi64x(0,0,0,0,0,0,0,0);\n        for(int i = m - (count % 8); i <= m; ++i){\n            sum_scalar = _mm256_add_epi64(sum_scalar, _mm256_set1_epi64x(i));\n        }\n    }\n    else{\n        sum_scalar = sum_vec;\n    }\n    __m256i sum_high = _mm256_extractf128_si256(sum_scalar, 1);\n    __m128i sum_low = _mm256_castsi256_si128(sum_scalar);\n    sum_low = _mm_add_epi64(sum_low, sum_high);\n    while(_mm_movemask_epi8(_mm_cmpgt_epi64(sum_low, _mm_set1_epi64x(0))) != 0){\n        __m128i sum_high_half = _mm_unpackhi_epi64(sum_low, sum_low);\n        sum_low = _mm_add_epi64(sum_low, sum_high_half);\n    }\n    int64_t total_sum = _mm_cvtsi128_si64(sum_low);\n    int64_t avg = round((double)total_sum / (double)(m - n + 1));\n    return std::bitset<64>(avg).to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    __m256i sum_vec = _mm256_setzero_si256();\n    int count = 0;\n    for(int i = n; i <= m; i += 8){\n        __m256i vec = _mm256_set_epi64x(i+0, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        sum_vec = _mm256_add_epi64(sum_vec, vec);\n        count += 8;\n    }\n    __m256i sum_scalar;\n    if(count < 8){\n        sum_scalar = _mm256_set_epi64x(0,0,0,0,0,0,0,0);\n        for(int i = m - (count % 8); i <= m; ++i){\n            sum_scalar = _mm256_add_epi64(sum_scalar, _mm256_set1_epi64x(i));\n        }\n    }\n    else{\n        sum_scalar = sum_vec;\n    }\n    __m256i sum_high = _mm256_extractf128_si256(sum_scalar, 1);\n    __m128i sum_low = _mm256_castsi256_si128(sum_scalar);\n    sum_low = _mm_add_epi64(sum_low, sum_high);\n    while(_mm_movemask_epi8(_mm_cmpgt_epi64(sum_low, _mm_set1_epi64x(0))) != 0){\n        __m128i sum_higher = _mm_unpackhi_epi64(sum_low, sum_low);\n        sum_low = _mm_add_epi64(sum_low, sum_higher);\n    }\n    int64_t total_sum = _mm_cvtsi128_si64(sum_low);\n    int64_t avg = round((double)total_sum / (double)(m - n + 1));\n    return std::bitset<64>(avg).to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    __m256i sum_vec = _mm256_setzero_si256();\n    int count = 0;\n    for(int i = n; i <= m; i += 8){\n        __m256i vec = _mm256_setr_epi64x(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        sum_vec = _mm256_add_epi64(sum_vec, vec);\n        count += 8;\n    }\n    __m256i sum_scalar;\n    if(count < 8){\n        sum_scalar = _mm256_setzero_si256();\n        for(int i = m - (count % 8); i <= m; ++i){\n            sum_scalar = _mm256_add_epi64(sum_scalar, _mm256_set1_epi64x(i));\n        }\n    }\n    else{\n        sum_scalar = sum_vec;\n    }\n    __m256i sum_high = _mm256_extractf128_si256(sum_scalar, 1);\n    __m256i sum_low = _mm256_castsi256_si128(sum_scalar);\n    sum_high = _mm256_add_epi64(sum_high, sum_low);\n    sum_low = _mm256_castsi256_si128(sum_high);\n    sum_high = _mm256_permute2x128_si256(sum_high, sum_low, 1);\n    sum_high = _mm256_add_epi64(sum_high, sum_low);\n    int64_t total_sum = _mm256_cvtsi128_si64(_mm256_castsi256_si128(sum_high));\n    int64_t avg = (total_sum + (count / 2)) / count;\n    return std::bitset<64>(avg).to_string();\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i v_i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i v_factorial = _mm256_set1_epi32(1);\n    __m256i v_sum = _mm256_setzero_si256();\n    __m256i v_mask_even = _mm256_set_epi32(0, 0, 0, 0, 1, 1, 1, 1);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_ten_thousand = _mm256_set1_epi32(10000);\n\n    for(int i = 0; i < n; i += 8){\n        __m256i v_index = _mm256_add_epi32(v_i, _mm256_set1_epi32(i));\n        __m256i v_is_even = _mm256_testz_epi32(_mm256_or_epi32(v_index, v_one), v_one);\n\n        // Compute factorial for even indices\n        __m256i v_fact_temp = _mm256_mullo_epi32(v_factorial, v_index);\n        __m256i v_fact_mod = _mm256_mullox_epi32(v_fact_temp, v_ten_thousand);\n        __m256i v_fact_result = _mm256_blendv_epi8(v_fact_mod, v_factorial, v_mask_even);\n\n        // Compute sum for odd indices\n        __m256i v_sum_temp = _mm256_add_epi32(v_sum, v_index);\n        __m256i v_sum_result = _mm256_blendv_epi8(v_sum_temp, v_factorial, _mm256_xor_si256(v_mask_even, _mm256_set1_epi32(-1)));\n\n        // Store results\n        _mm256_storeu_si256((__m256i*)&result[i], v_fact_result);\n        _mm256_storeu_si256((__m256i*)&result[i], v_sum_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i v_i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i v_factorial = _mm256_set1_epi32(1);\n    __m256i v_sum = _mm256_setzero_si256();\n    __m256i v_mask_even = _mm256_set_epi32(0, 0, 0, 0, 1, 1, 1, 1);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_ten_thousand = _mm256_set1_epi32(10000);\n\n    for(int i=0; i<n; i+=8){\n        __m256i v_index = _mm256_add_epi32(v_i, _mm256_set_epi32(i, i, i, i, i, i, i, i));\n        \n        // Calculate factorial for even indices\n        __m256i v_is_even = _mm256_and_si256(_mm256_cmpgt_epi32(v_index, v_one), v_mask_even);\n        __m256i v_fact_temp = _mm256_mullo_epi32(v_factorial, v_index);\n        __m256i v_fact_mod = _mm256_and_si256(_mm256_mullo_epi32(v_fact_temp, v_mask_even), v_ten_thousand);\n        v_factorial = _mm256_blendv_epi8(v_factorial, v_fact_temp, v_is_even);\n        \n        // Calculate sum for odd indices\n        __m256i v_sum_temp = _mm256_add_epi32(v_sum, v_index);\n        __m256i v_sum_odd = _mm256_andnot_si256(v_mask_even, v_sum_temp);\n        v_sum = _mm256_blendv_epi8(v_sum, v_sum_temp, _mm256_xor_si256(v_mask_even, _mm256_set1_epi32(-1)));\n\n        // Store results\n        __m256i v_result = _mm256_blendv_epi8(v_sum_odd, v_fact_mod, v_is_even);\n        _mm256_storeu_si256((__m256i*)&result[i], v_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i v_i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i v_factorial = _mm256_set1_epi32(1);\n    __m256i v_sum = _mm256_setzero_si256();\n    __m256i v_mask_even = _mm256_set_epi32(0, 0, 0, 0, 1, 1, 1, 1);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_ten_thousand = _mm256_set1_epi32(10000);\n\n    for(int i = 0; i < n; i += 8){\n        __m256i v_index = _mm256_add_epi32(v_i, _mm256_set1_epi32(i));\n        __m256i v_is_even = _mm256_testz_epi32(v_index, v_mask_even);\n\n        // Compute factorial for even indices\n        __m256i v_fact_temp = _mm256_mullo_epi32(v_factorial, _mm256_permutevar8x32_epi32(v_index, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n        __m256i v_fact_mod = _mm256_and_si256(_mm256_mul_epu32(v_fact_temp, v_one), v_ten_thousand);\n        \n        // Compute sum for odd indices\n        __m256i v_sum_temp = _mm256_add_epi32(v_sum, _mm256_permutevar8x32_epi32(v_index, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n\n        // Select values based on even/odd\n        __m256i v_result = _mm256_blendv_epi8(v_sum_temp, v_fact_mod, v_is_even);\n\n        // Store results\n        _mm256_storeu_si256((__m256i*)&result[i], v_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i v_i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i v_factorial = _mm256_set1_epi32(1);\n    __m256i v_sum = _mm256_setzero_si256();\n    __m256i v_mask_even = _mm256_set_epi32(0, 0, 0, 0, 1, 1, 1, 1);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_ten_thousand = _mm256_set1_epi32(10000);\n\n    for(int i = 0; i < n; i += 8){\n        __m256i v_index = _mm256_add_epi32(v_i, _mm256_set1_epi32(i));\n        __m256i v_is_even = _mm256_testz_epi32(_mm256_or_epi32(v_index, v_one), v_one);\n\n        // Compute factorial for even indices\n        __m256i v_fact_temp = _mm256_mullo_epi32(v_factorial, _mm256_permutevar8x32_epi32(v_index, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0)));\n        __m256i v_fact_mod = _mm256_andnot_si256(v_mask_even, _mm256_mod_epu32(v_fact_temp, v_ten_thousand));\n\n        // Compute sum for odd indices\n        __m256i v_sum_temp = _mm256_add_epi32(v_sum, v_index);\n        __m256i v_sum_result = _mm256_and_si256(v_mask_even, v_sum_temp);\n\n        // Combine results\n        __m256i v_final = _mm256_blendv_epi8(v_sum_result, v_fact_mod, v_is_even);\n\n        // Store results\n        _mm256_storeu_si256((__m256i*)&result[i], v_final);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i v_i = _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0);\n    __m256i v_factorial = _mm256_set1_epi32(1);\n    __m256i v_sum = _mm256_setzero_si256();\n    __m256i v_mask_even = _mm256_set_epi32(0, 0, 0, 0, 1, 1, 1, 1);\n    __m256i v_one = _mm256_set1_epi32(1);\n    __m256i v_two = _mm256_set1_epi32(2);\n    __m256i v_ten_thousand = _mm256_set1_epi32(10000);\n\n    while(_mm256_movemask_epi8(_mm256_cmpgt_epi32(v_i, _mm256_setzero_si256())) != 0){\n        __m256i v_is_even = _mm256_and_si256(_mm256_cmpgt_epi32(v_i, _mm256_setzero_si256()), v_mask_even);\n        \n        // Compute factorial for even indices\n        __m256i v_fact_temp = _mm256_mullo_epi32(v_factorial, v_i);\n        __m256i v_fact_mod = _mm256_mullox_epi32(v_fact_temp, v_ten_thousand);\n        __m256i v_factorial_new = _mm256_blendv_epi8(v_factorial, v_fact_mod, v_is_even);\n        \n        // Compute sum for odd indices\n        __m256i v_sum_temp = _mm256_add_epi32(v_sum, v_i);\n        __m256i v_sum_new = _mm256_blendv_epi8(v_sum, v_sum_temp, _mm256_xor_si256(v_is_even, v_one));\n        \n        // Store results\n        __m256i v_results = _mm256_blendv_epi8(v_sum_new, v_factorial_new, v_is_even);\n        _mm256_storeu_si256((__m256i*)&result[0] + _mm256_extract_epi32(_mm256_setzero_si256(), 0), v_results);\n        \n        // Update counters\n        v_i = _mm256_sub_epi32(v_i, v_two);\n        v_factorial = v_factorial_new;\n        v_sum = v_sum_new;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2, 0);\n    __m256i even_mask = _mm256_set1_epi32(0);\n    __m256i odd_mask = _mm256_set1_epi32(0);\n\n    for(int i = 1; i <= n; i += 8){\n        __m256i num_vec = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i rev_num_vec = _mm256_setr_epi32(\n            reverse_digits(i), reverse_digits(i+1), reverse_digits(i+2),\n            reverse_digits(i+3), reverse_digits(i+4), reverse_digits(i+5),\n            reverse_digits(i+6), reverse_digits(i+7)\n        );\n\n        __m256i eq_mask = _mm256_cmpeq_epi32(num_vec, rev_num_vec);\n        __m256i even_check = _mm256_and_si256(eq_mask, _mm256_set1_epi32(0x1));\n        __m256i odd_check = _mm256_andnot_si256(even_check, eq_mask);\n\n        even_mask = _mm256_add_epi32(even_mask, even_check);\n        odd_mask = _mm256_add_epi32(odd_mask, odd_check);\n    }\n\n    int temp_even[8], temp_odd[8];\n    _mm256_storeu_si256((__m256i*)temp_even, even_mask);\n    _mm256_storeu_si256((__m256i*)temp_odd, odd_mask);\n\n    for(int j = 0; j < 8; ++j){\n        result[0] += temp_even[j];\n        result[1] += temp_odd[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2, 0);\n    if(n < 1) return result;\n\n    __m256i even_mask = _mm256_set1_epi32(0);\n    __m256i odd_mask = _mm256_set1_epi32(0);\n\n    int limit = n < 10 ? n : 9;\n    for(int i = 1; i <= limit; ++i){\n        __m256i value = _mm256_set1_epi32(i);\n        __m256i is_even = _mm256_and_si256(value, _mm256_set1_epi32(1));\n        even_mask = _mm256_add_epi32(even_mask, is_even);\n        odd_mask = _mm256_sub_epi32(_mm256_set1_epi32(1), is_even);\n    }\n\n    __m256i even_count = _mm256_setzero_si256();\n    __m256i odd_count = _mm256_setzero_si256();\n\n    for(int i = 10; i <= n; i = i + 8){\n        __m256i num = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i reversed = reverse_number_avx(num);\n        __m256i is_same = _mm256_cmpeq_epi32(num, reversed);\n        __m256i is_even = _mm256_and_si256(num, _mm256_set1_epi32(1));\n        even_count = _mm256_add_epi32(even_count, _mm256_and_si256(is_same, is_even));\n        odd_count = _mm256_add_epi32(odd_count, _mm256_andnot_si256(is_even, is_same));\n    }\n\n    int counts[8];\n    _mm256_storeu_si256((__m256i*)counts, even_count);\n    result[0] += counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    _mm256_storeu_si256((__m256i*)counts, odd_count);\n    result[1] += counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2, 0);\n    __m256i even_mask = _mm256_set1_epi32(0);\n    __m256i odd_mask = _mm256_set1_epi32(0);\n\n    for(int i = 1; i <= n; i += 8){\n        __m256i num_vec = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i rev_num_vec = _mm256_setr_epi32(\n            reverse_digits(i), reverse_digits(i+1), reverse_digits(i+2),\n            reverse_digits(i+3), reverse_digits(i+4), reverse_digits(i+5),\n            reverse_digits(i+6), reverse_digits(i+7)\n        );\n\n        __m256i eq_mask = _mm256_cmpeq_epi32(num_vec, rev_num_vec);\n        __m256i even_check = _mm256_and_si256(eq_mask, _mm256_set1_epi32(0x1));\n        __m256i odd_check = _mm256_andnot_si256(even_check, eq_mask);\n\n        even_mask = _mm256_add_epi32(even_mask, even_check);\n        odd_mask = _mm256_add_epi32(odd_mask, odd_check);\n    }\n\n    int temp_even[8], temp_odd[8];\n    _mm256_storeu_si256((__m256i*)temp_even, even_mask);\n    _mm256_storeu_si256((__m256i*)temp_odd, odd_mask);\n\n    for(int j = 0; j < 8; ++j){\n        result[0] += temp_even[j];\n        result[1] += temp_odd[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2, 0);\n    __m256i even_mask = _mm256_set1_epi32(0);\n    __m256i odd_mask = _mm256_set1_epi32(0);\n\n    for(int i = 1; i <= n; i += 8){\n        __m256i num_vec = _mm256_setr_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7);\n        __m256i rev_num_vec = _mm256_setr_epi32(\n            reverse_digits(i), reverse_digits(i+1), reverse_digits(i+2),\n            reverse_digits(i+3), reverse_digits(i+4), reverse_digits(i+5),\n            reverse_digits(i+6), reverse_digits(i+7)\n        );\n\n        __m256i eq_mask = _mm256_cmpeq_epi32(num_vec, rev_num_vec);\n        __m256i even_check = _mm256_and_si256(eq_mask, _mm256_set1_epi32(0x1));\n        __m256i odd_check = _mm256_andnot_si256(even_check, eq_mask);\n\n        even_mask = _mm256_add_epi32(even_mask, even_check);\n        odd_mask = _mm256_add_epi32(odd_mask, odd_check);\n    }\n\n    int temp_even[8], temp_odd[8];\n    _mm256_storeu_si256((__m256i*)temp_even, even_mask);\n    _mm256_storeu_si256((__m256i*)temp_odd, odd_mask);\n\n    for(int j = 0; j < 8; ++j){\n        result[0] += temp_even[j];\n        result[1] += temp_odd[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2, 0);\n    if(n < 1) return result;\n\n    __m256i even_mask = _mm256_set1_epi32(0);\n    __m256i odd_mask = _mm256_set1_epi32(0);\n\n    int limit = n < 10 ? n : 9;\n    for(int i = 1; i <= limit; ++i){\n        __m256i value = _mm256_set1_epi32(i);\n        __m256i is_even = _mm256_and_si256(value, _mm256_set1_epi32(1));\n        even_mask = _mm256_add_epi32(even_mask, is_even);\n        odd_mask = _mm256_sub_epi32(_mm256_set1_epi32(1), is_even);\n    }\n\n    __m256i even_count = _mm256_setzero_si256();\n    __m256i odd_count = _mm256_setzero_si256();\n\n    for(int i = 10; i <= n; i = i + 8){\n        __m256i num = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m256i reversed = reverse_number_avx(num);\n        __m256i eq_mask = _mm256_cmpeq_epi32(num, reversed);\n        __m256i is_even = _mm256_and_si256(eq_mask, _mm256_set1_epi32(1));\n        even_count = _mm256_add_epi32(even_count, is_even);\n        odd_count = _mm256_sub_epi32(_mm256_set1_epi32(8), _mm256_add_epi32(even_count, odd_count));\n    }\n\n    int counts[8];\n    _mm256_storeu_si256((__m256i*)counts, even_count);\n    result[0] += counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    _mm256_storeu_si256((__m256i*)counts, odd_count);\n    result[1] += counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    const int vec_size = 8; // AVX can handle 8 integers at once\n    int mask[vec_size];\n    __m256i threshold = _mm256_set1_epi32(0);\n    \n    for(size_t i = 0; i < n.size(); i += vec_size){\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sums = _mm256_setzero_si256();\n        \n        // Sum of digits logic\n        for(int j = 0; j < 4; ++j){\n            __m256i digits = _mm256_set_epi32(\n                (abs_nums.m256i_i32[7-j] / 100000000) % 10,\n                (abs_nums.m256i_i32[7-j] / 10000000) % 10,\n                (abs_nums.m256i_i32[7-j] / 1000000) % 10,\n                (abs_nums.m256i_i32[7-j] / 100000) % 10,\n                (abs_nums.m256i_i32[7-j] / 10000) % 10,\n                (abs_nums.m256i_i32[7-j] / 1000) % 10,\n                (abs_nums.m256i_i32[7-j] / 100) % 10,\n                (abs_nums.m256i_i32[7-j] / 10) % 10\n            );\n            \n            sums = _mm256_add_epi32(sums, digits);\n        }\n        \n        __m256i cmp_result = _mm256_cmpgt_epi32(sums, threshold);\n        _mm256_storeu_si256((__m256i*)mask, cmp_result);\n        \n        for(int k = 0; k < vec_size; ++k){\n            if(mask[k]){\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    const int vec_size = 8; // AVX2 supports 8 integers per register\n    int mask[vec_size];\n    __m256i threshold = _mm256_set1_epi32(0);\n    \n    if(n.size() == 0) return 0;\n    \n    for(int i = 0; i < n.size(); i += vec_size){\n        __m256i nums, cmp_result;\n        int j = i;\n        \n        // Load data into SIMD register\n        nums = _mm256_loadu_si256((__m256i*)&n[j]);\n        \n        // Compare each element with zero\n        cmp_result = _mm256_cmpgt_epi32(nums, threshold);\n        \n        // Extract mask from comparison result\n        _mm256_storeu_si256((__m256i*)mask, cmp_result);\n        \n        // Count elements greater than zero\n        for(int k = 0; k < vec_size; ++k){\n            if(mask[k]) count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```cpp\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    int size = n.size();\n    __m256i threshold = _mm256_set1_epi32(0);\n    __m256i mask = _mm256_set1_epi32(-1);\n    \n    for(int i=0; i<size; i+=8){\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sums = _mm256_sad_epu8(_mm256_setzero_si256(), abs_nums);\n        __m256i cmp = _mm256_cmpgt_epi32(sums, threshold);\n        count += _mm256_movemask_epi8(_mm256_castsi256_si128(cmp)) & 0xFF;\n        count += _mm256_movemask_epi8(_mm256_extracti128_si256(cmp)) & 0xFF;\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    const int vec_size = 8; // AVX2 allows 8 integers per register\n    int remaining = n.size() % vec_size;\n    __m256i threshold = _mm256_set1_epi32(0);\n    __m256i mask = _mm256_set1_epi32(1);\n\n    for(int i = 0; i < n.size() - remaining; i += vec_size){\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sum_digits = _mm256_setzero_si256();\n        \n        // Extract digits in parallel\n        for(int j = 0; j < 4; ++j){\n            __m256i temp = _mm256_and_si256(abs_nums, mask);\n            sum_digits = _mm256_add_epi32(sum_digits, temp);\n            abs_nums = _mm256_srli_epi32(abs_nums, 4);\n        }\n        \n        __m256i cmp_result = _mm256_cmpgt_epi32(sum_digits, threshold);\n        count += _mm256_movemask_epi8(cmp_result);\n    }\n\n    // Handle remaining elements\n    for(int i = n.size() - remaining; i < n.size(); ++i){\n        if(n[i] != 0)\n            count++;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c\nint count_nums_simd(std::vector<int> n){\n    int count = 0;\n    const int vec_size = 8; // AVX: 8 integers per register\n    int remaining = n.size() % vec_size;\n    __m256i threshold = _mm256_set1_epi32(0);\n    \n    for(int i = 0; i < n.size() - remaining; i += vec_size){\n        __m256i nums = _mm256_loadu_si256((__m256i*)&n[i]);\n        __m256i abs_nums = _mm256_abs_epi32(nums);\n        __m256i sums = _mm256_sad_epu8(_mm256_setzero_si256(), abs_nums);\n        __m256i cmp = _mm256_cmpgt_epi32(sums, threshold);\n        count += _mm256_movemask_epi8(cmp);\n    }\n    \n    for(int i = n.size() - remaining; i < n.size(); ++i){\n        int num = n[i];\n        int sum = 0;\n        if(num < 0){\n            sum += -(num % 10);\n            num /= 10;\n        }\n        while(num > 0){\n            sum += num % 10;\n            num /= 10;\n        }\n        if(sum > 0){\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int n = arr.size();\n    if(n == 0) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)arr.data());\n    __m256i sorted = _mm256_set_epi32(4, 3, 2, 1, 0, -1, -2, -3);\n    \n    bool is_sorted = true;\n    for(int i = 0; i < n; i += 8){\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(arr.data() + i));\n        is_sorted &= _mm256_cmpgt_epi32(_mm256_permutevar8x32_epi32(chunk, sorted), chunk) == _mm256_setzero_si256();\n    }\n\n    return is_sorted || _mm256_movemask_epi8(_mm256_cmpeq_epi32(vec, _mm256_permutevar8x32_epi32(vec, sorted))) == 0;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int n = arr.size();\n    if(n == 0) return true;\n    \n    __m256i vec = _mm256_loadu_si256((__m256i*)arr.data());\n    __m256i sorted = _mm256_set_epi32(4, 3, 2, 1, 0, -1, -2, -3);\n    \n    bool is_sorted = true;\n    for(int i=0; i<n; i+=8){\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(arr.data()+i));\n        is_sorted &= _mm256_cmpgt_epi32(_mm256_permutevar8x32_epi32(chunk, sorted), chunk)._mm256_movemask_epi8() == 0;\n    }\n    \n    return is_sorted || check_circular_shift(arr);\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int n = arr.size();\n    if(n == 0) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)arr.data());\n    __m256i sorted = _mm256_set_epi32(4, 3, 2, 1, 0, -1, -2, -3);\n    \n    bool is_sorted = true;\n    for(int i = 0; i < n; i += 8){\n        __m256i chunk = _mm256_loadu_si256((__m256i*)(arr.data() + i));\n        __m256i cmp = _mm256_cmpgt_epi32(sorted, chunk);\n        unsigned char mask = _mm256_movemask_epi8(cmp);\n        if(mask != 0){\n            is_sorted = false;\n            break;\n        }\n    }\n\n    if(is_sorted) return true;\n\n    // Check for rotation\n    for(int j = 0; j < n; ++j){\n        std::rotate(arr.begin(), arr.end()-1, arr.end());\n        is_sorted = true;\n        for(int i = 0; i < n; i += 8){\n            __m256i chunk = _mm256_loadu_si256((__m256i*)(arr.data() + i));\n            __m256i cmp = _mm256_cmpgt_epi32(sorted, chunk);\n            unsigned char mask = _mm256_movemask_epi8(cmp);\n            if(mask != 0){\n                is_sorted = false;\n                break;\n            }\n        }\n        if(is_sorted) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int n = arr.size();\n    if(n == 0) return true;\n\n    __m256i sorted = _mm256_loadu_si256((__m256i*)arr.data());\n    __m256i current = sorted;\n    \n    for(int i = 0; i < n; ++i){\n        // Check if the array is sorted\n        __m256i cmp = _mm256_cmpgt_epi32(current, _mm256_permutevar8x32_epi32(current, _mm256_setr_epi32(1, 2, 3, 4, 0, 5, 6, 7)));\n        int mask = _mm256_movemask_epi8(cmp);\n        if(mask == 0) return true;\n\n        // Perform right shift\n        current = _mm256_permutevar8x32_epi32(current, _mm256_setr_epi32(1, 2, 3, 4, 0, 5, 6, 7));\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    const int n = arr.size();\n    if(n == 0) return true;\n\n    __m256i sorted = _mm256_loadu_si256((__m256i*)arr.data());\n    __m256i unsorted = _mm256_loadu_si256((__m256i*)arr.data());\n\n    // Check if already sorted\n    __m256i cmp = _mm256_cmpgt_epi32(sorted, _mm256_permutevar8x32_epi32(sorted, _mm256_setr_epi32(1,2,3,4,0,5,6,7)));\n    int mask = _mm256_movemask_epi8(cmp);\n    if(mask == 0) return true;\n\n    // Perform rotations and compare\n    for(int i=1; i<n; ++i){\n        __m256i rotated = _mm256_alignr_epi8(unsorted, _mm256_permute4x64_epi64(unsorted, 1), 4*(n-1));\n        cmp = _mm256_cmpgt_epi32(rotated, _mm256_permutevar8x32_epi32(rotated, _mm256_setr_epi32(1,2,3,4,0,5,6,7)));\n        mask = _mm256_movemask_epi8(cmp);\n        if(mask == 0) return true;\n        unsorted = rotated;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vec_size = 8; // Using AVX2: 8 integers per register\n    size_t len1 = lst1.size();\n    size_t len2 = lst2.size();\n    size_t i = 0;\n\n    __m256i mask_even = _mm256_set1_epi32(0xAAAAAAAA); // Mask for checking even numbers (0xAAAAAAAA represents even positions in 32-bit)\n\n    while(i + vec_size <= len1 && i + vec_size <= len2){\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        __m256i even1 = _mm256_and_si256(v1, mask_even);\n        __m256i even2 = _mm256_and_si256(v2, mask_even);\n\n        if(!_mm256_cmp_epi32_mask(even1, mask_even, _MM_CMPINT_EQ)){\n            return \"NO\";\n        }\n        if(!_mm256_cmp_epi32_mask(even2, mask_even, _MM_CMPINT_EQ)){\n            return \"NO\";\n        }\n\n        i += vec_size;\n    }\n\n    // Handle remaining elements\n    while(i < len1 || i < len2){\n        if((lst1[i] & 1) != 0 && (lst2[i] & 1) != 0){\n            return \"NO\";\n        }\n        i++;\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vec_size = 8; // Using AVX2: 8 integers per register\n    size_t len1 = lst1.size();\n    size_t len2 = lst2.size();\n    size_t i = 0;\n\n    __m256i mask_even = _mm256_set1_epi32(0xAAAAAAAA); // Mask for checking even numbers\n\n    while(i + vec_size <= len1 && i + vec_size <= len2){\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        __m256i even1 = _mm256_and_si256(v1, mask_even);\n        __m256i even2 = _mm256_and_si256(v2, mask_even);\n\n        if(!_mm256_cmp_epi32_mask(even1, mask_even, _MM_CMPINT_EQ)){\n            return \"NO\";\n        }\n        if(!_mm256_cmp_epi32_mask(even2, mask_even, _MM_CMPINT_EQ)){\n            return \"NO\";\n        }\n\n        i += vec_size;\n    }\n\n    // Handle remaining elements\n    while(i < len1 || i < len2){\n        if((lst1[i] & 1) != 0 && (lst2[i] & 1) != 0){\n            return \"NO\";\n        }\n        i++;\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vec_size = 8; // AVX2 supports 8 integers per register\n    size_t len1 = lst1.size();\n    size_t len2 = lst2.size();\n    size_t i = 0;\n\n    __m256i mask_even = _mm256_set1_epi32(0x0); // Mask for checking even numbers\n    __m256i count_odd1 = _mm256_setzero_si256(); // Count odd numbers in lst1\n    __m256i count_odd2 = _mm256_setzero_si256(); // Count odd numbers in lst2\n\n    while (i + vec_size <= len1 && i + vec_size <= len2){\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        __m256i odd_mask1 = _mm256_and_si256(v1, _mm256_set1_epi32(1));\n        __m256i odd_mask2 = _mm256_and_si256(v2, _mm256_set1_epi32(1));\n\n        count_odd1 = _mm256_add_epi32(count_odd1, odd_mask1);\n        count_odd2 = _mm256_add_epi32(count_odd2, odd_mask2);\n\n        i += vec_size;\n    }\n\n    // Handle remaining elements\n    while (i < len1 || i < len2){\n        if (i < len1 && (lst1[i] & 1)) count_odd1 = _mm256_add_epi32(count_odd1, mask_even);\n        if (i < len2 && (lst2[i] & 1)) count_odd2 = _mm256_add_epi32(count_odd2, mask_even);\n        i++;\n    }\n\n    // Sum up odd counts\n    __m256i sum_odd1 = _mm256_hadd_epi32(count_odd1, count_odd1);\n    sum_odd1 = _mm256_hadd_epi32(sum_odd1, sum_odd1);\n    int total_odd1 = _mm256_extract_epi32(sum_odd1, 0);\n\n    __m256i sum_odd2 = _mm256_hadd_epi32(count_odd2, count_odd2);\n    sum_odd2 = _mm256_hadd_epi32(sum_odd2, sum_odd2);\n    int total_odd2 = _mm256_extract_epi32(sum_odd2, 0);\n\n    if (total_odd1 <= total_odd2){\n        return \"YES\";\n    }\n    else{\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vec_size = 8; // AVX2 supports 8 integers per register\n    size_t len1 = lst1.size();\n    size_t len2 = lst2.size();\n    size_t i = 0;\n\n    __m256i mask_even = _mm256_set1_epi32(0x0); // Mask for checking even numbers\n\n    while(i + vec_size <= len1 && i + vec_size <= len2){\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        __m256i even1 = _mm256_and_si256(v1, _mm256_set1_epi32(0x1)); // Check oddness in lst1\n        __m256i even2 = _mm256_and_si256(v2, _mm256_set1_epi32(0x1)); // Check oddness in lst2\n\n        if(!_mm256_testz_si256(even1, even1) || !_mm256_testz_si256(even2, even2)){\n            return \"NO\";\n        }\n\n        i += vec_size;\n    }\n\n    // Handle remaining elements\n    while(i < len1 || i < len2){\n        if((lst1[i] & 0x1) != 0 && (lst2[i] & 0x1) != 0){\n            return \"NO\";\n        }\n        i++;\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    const int vec_size = 8; // AVX2 supports 8 integers per register\n    size_t len1 = lst1.size();\n    size_t len2 = lst2.size();\n    size_t i = 0;\n\n    __m256i mask_even = _mm256_set1_epi32(0x0); // Mask for checking even numbers\n\n    while(i + vec_size <= len1 && i + vec_size <= len2){\n        __m256i v1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i v2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        __m256i even1 = _mm256_and_si256(v1, _mm256_set1_epi32(0x1)); // Check oddness in lst1\n        __m256i even2 = _mm256_and_si256(v2, _mm256_set1_epi32(0x1)); // Check oddness in lst2\n\n        if(!_mm256_testz_si256(even1, even1) || !_mm256_testz_si256(even2, even2)){\n            return \"NO\";\n        }\n\n        i += vec_size;\n    }\n\n    // Handle remaining elements\n    while(i < len1 || i < len2){\n        if((lst1[i] & 0x1) != 0 && (lst2[i] & 0x1) != 0){\n            return \"NO\";\n        }\n        i++;\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result(lst.size());\n    __m256i odd_mask = _mm256_set_epi8(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1);\n    for(size_t i = 0; i < lst.size(); i += 8){\n        __m256i counts = _mm256_setzero_si256();\n        size_t batch_size = std::min(lst.size() - i, static_cast<size_t>(8));\n        for(size_t j = 0; j < batch_size; ++j){\n            const std::string &s = lst[i + j];\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.c_str()));\n            __m256i cmp = _mm256_cmpgt_epi8(_mm256_and_si256(chars, 0xF), _mm256_set1_epi8('9'));\n            __m256i odd_chars = _mm256_and_si256(cmp, odd_mask);\n            counts = _mm256_add_epi8(counts, odd_chars);\n        }\n        __m256i counts_sum = _mm256_sad_epu8(counts, _mm256_setzero_si256());\n        int total[8] __attribute__((aligned(32)));\n        _mm256_store_si256((__m256i*)total, counts_sum);\n        for(size_t j = 0; j < batch_size; ++j){\n            result[i + j] = \"the number of odd elements \" + std::to_string(total[j]) + \"n the str\" + std::to_string(total[j]) + \"ng \" + std::to_string(total[j]) + \" of the \" + std::to_string(total[j]) + \"nput.\";\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n    \n    for(size_t i = 0; i < lst.size(); i += 8){\n        __m256i counts = _mm256_setzero_si256();\n        __m256i mask_odd = _mm256_set_epi8(\n            -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1,\n            -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1\n        );\n        \n        size_t end = std::min(i + 8, lst.size());\n        for(size_t j = i; j < end; ++j){\n            const std::string &s = lst[j];\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.c_str()));\n            __m256i cmp = _mm256_and_si256(chars, mask_odd);\n            counts = _mm256_add_epi8(counts, cmp);\n        }\n        \n        int count_array[8] __attribute__((aligned(32)));\n        _mm256_store_si256((__m256i*)count_array, counts);\n        \n        for(size_t j = i; j < end; ++j){\n            int count = count_array[j - i];\n            int odd_digits = (count + 1) / 2; // Count of odd digits\n            std::string res = \"the number of odd elements \";\n            res += std::to_string(odd_digits);\n            res += \"n the str\";\n            res += std::to_string(odd_digits);\n            res += \"ng \";\n            res += std::to_string(odd_digits);\n            res += \" of the \";\n            res += std::to_string(odd_digits);\n            res += \"nput.\";\n            result.push_back(res);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result(lst.size());\n    __m256i odd_mask = _mm256_set_epi8(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1);\n    for(size_t i = 0; i < lst.size(); i += 8){\n        __m256i counts = _mm256_setzero_si256();\n        size_t batch_size = std::min(lst.size() - i, static_cast<size_t>(8));\n        for(size_t j = 0; j < batch_size; ++j){\n            const std::string &s = lst[i + j];\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.c_str()));\n            __m256i cmp = _mm256_cmpeq_epi8(_mm256_and_si256(chars, odd_mask), odd_mask);\n            counts = _mm256_add_epi8(counts, _mm256_cvtepu8_epi32(cmp));\n        }\n        __m256i sums = _mm256_sad_epu8(counts, _mm256_setzero_si256());\n        uint32_t sum_array[8];\n        _mm256_storeu_si256((__m256i*)sum_array, sums);\n        for(size_t j = 0; j < batch_size; ++j){\n            result[i + j] = \"the number of odd elements \" + std::to_string(sum_array[j]) + \"n the str\" + std::to_string(sum_array[j]) + \"ng \" + std::to_string(sum_array[j]) + \" of the \" + std::to_string(sum_array[j]) + \"nput.\";\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    std::vector<std::string> result;\n    result.reserve(lst.size());\n    \n    for(size_t i = 0; i < lst.size(); i += 8){\n        __m256i counts = _mm256_setzero_si256();\n        size_t end = std::min(i + 8, lst.size());\n        \n        for(size_t j = i; j < end; ++j){\n            const std::string &s = lst[j];\n            __m256i chars = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(s.c_str()));\n            __m256i odds = _mm256_set1_epi8('_'); // Placeholder for odd mask\n            \n            // Compute odd mask\n            odds = _mm256_cmpgt_epi8(_mm256_and_si256(chars, _mm256_set1_epi8(1)), _mm256_setzero_si256());\n            \n            // Count odds\n            counts = _mm256_add_epi8(counts, odds);\n        }\n        \n        // Horizontal sum of counts\n        __m256i horizontal_sums = _mm256_setzero_si256();\n        horizontal_sums = _mm256_permutevar8x32_epi32(counts, _mm256_setr_epi32(0,1,2,3,4,5,6,7));\n        counts = _mm256_add_epi8(counts, horizontal_sums);\n        horizontal_sums = _mm256_permutevar8x32_epi32(counts, _mm256_setr_epi32(0,0,2,2,4,4,6,6));\n        counts = _mm256_add_epi8(counts, horizontal_sums);\n        horizontal_sums = _mm256_permutevar8x32_epi32(counts, _mm256_setr_epi32(0,0,0,0,4,4,4,4));\n        counts = _mm256_add_epi8(counts, horizontal_sums);\n        \n        int final_counts[8];\n        _mm256_storeu_si256((__m256i*)final_counts, counts);\n        \n        for(size_t k = i; k < end; ++k){\n            result.push_back(\"the number of odd elements \" + std::to_string(final_counts[k - i]) + \"n the str\" + std::to_string(final_counts[k - i]) + \"ng \" + std::to_string(final_counts[k - i]) + \" of the \" + std::to_string(final_counts[k - i]) + \"nput.\");\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int64_t* ptr = nums.data();\n    size_t n = nums.size();\n    if(n == 0) return 0;\n\n    // Initialize variables\n    __m256i min_sum = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_sum = _mm256_setzero_si256();\n    __m256i temp_sum;\n    size_t i = 0;\n\n    // Process data in chunks of 4 elements\n    for(; i <= n - 8; i += 8){\n        __m256i v = _mm256_loadu_si256((__m256i*)(ptr + i));\n        current_sum = _mm256_min_epi64(current_sum, _mm256_sub_epi64(v, _mm256_setzero_si256()));\n        temp_sum = _mm256_add_epi64(current_sum, _mm256_permutevar8x32_epi32(current_sum, _mm256_set_epi32(3, 2, 1, 0, 7, 6, 5, 4)));\n        min_sum = _mm256_min_epi64(min_sum, temp_sum);\n    }\n\n    // Handle remaining elements\n    int64_t min_sum_scalar = INT64_MAX;\n    for(; i < n; ++i){\n        min_sum_scalar = std::min(min_sum_scalar, static_cast<int64_t>(ptr[i]));\n    }\n\n    // Combine results from SIMD and scalar parts\n    int64_t final_min = _mm256_extract_epi64(min_sum, 0);\n    for(int j = 1; j < 4; ++j){\n        final_min = std::min(final_min, static_cast<int64_t>(_mm256_extract_epi64(min_sum, j)));\n    }\n    return std::min(final_min, min_sum_scalar);\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int64_t* ptr = nums.data();\n    size_t size = nums.size();\n    if(size == 0) return 0;\n\n    __m256i min_val = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_min = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for(; i <= size - 4; i += 4){\n        __m256i vec = _mm256_loadu_si256((__m256i*)(ptr + i));\n        current_min = _mm256_min_epi64(current_min, vec);\n        min_val = _mm256_min_epi64(min_val, current_min);\n    }\n\n    int64_t global_min = INT64_MAX;\n    for(; i < size; ++i){\n        global_min = std::min(global_min, ptr[i]);\n    }\n\n    if(_mm256_movemask_epi8(_mm256_castsi256_ps(min_val)) != 0){\n        global_min = std::min(global_min, _mm256_extract_epi64(min_val, 0));\n    }\n\n    return global_min;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int64_t* ptr = nums.data();\n    size_t size = nums.size();\n    if(size == 0) return 0;\n\n    __m256i min_val = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_min = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for(; i <= size - 4; i += 4){\n        __m256i vec = _mm256_loadu_si256((__m256i*)(ptr + i));\n        current_min = _mm256_min_epi64(current_min, vec);\n        min_val = _mm256_min_epi64(min_val, current_min);\n    }\n\n    int64_t global_min = INT64_MAX;\n    for(; i < size; ++i){\n        global_min = std::min(global_min, ptr[i]);\n    }\n\n    return _mm256_extract_epi64(_mm256_min_epi64(min_val, _mm256_set_epi64x(0, global_min, global_min, global_min)), 0);\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int64_t* ptr = nums.data();\n    size_t size = nums.size();\n    if(size == 0) return 0;\n\n    __m256i min_val = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_min = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for(; i <= size - 4; i += 4){\n        __m256i vec = _mm256_loadu_si256((__m256i*)(ptr + i));\n        current_min = _mm256_min_epi64(current_min, vec);\n        min_val = _mm256_min_epi64(min_val, current_min);\n    }\n\n    int64_t global_min = INT64_MAX;\n    for(; i < size; ++i){\n        global_min = std::min(global_min, ptr[i]);\n    }\n\n    return _mm256_extract_epi64(_mm256_min_epi64(min_val, _mm256_set_epi64x(0, global_min, global_min, global_min)), 0);\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int64_t* ptr = nums.data();\n    size_t size = nums.size();\n    if(size == 0) return 0;\n\n    __m256i min_val = _mm256_set1_epi64x(INT64_MAX);\n    __m256i current_min = _mm256_setzero_si256();\n\n    size_t i = 0;\n    for(; i <= size - 4; i += 4){\n        __m256i vec = _mm256_loadu_si256((__m256i*)(ptr + i));\n        current_min = _mm256_min_epi64(current_min, vec);\n        min_val = _mm256_min_epi64(min_val, current_min);\n    }\n\n    int64_t global_min = INT64_MAX;\n    for(; i < size; ++i){\n        global_min = std::min(global_min, static_cast<int64_t>(ptr[i]));\n    }\n\n    if(_mm256_movemask_epi8(_mm256_cmpgt_epi64(min_val, _mm256_setzero_si256())) != 0xFF){\n        min_val = _mm256_min_epi64(min_val, _mm256_permutevar8x32_epi32(min_val, _mm256_set_epi32(7,6,5,4,3,2,1,0)));\n    }\n\n    int64_t result = INT64_MAX;\n    for(int j = 0; j < 4; ++j){\n        result = std::min(result, _mm256_extract_epi64(min_val, j));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_trips = 0;\n    __m256i bucket_mask = _mm256_set1_epi32(capacity);\n    __m256i zero_mask = _mm256_setzero_si256();\n    \n    for (const auto & well : grid) {\n        int remaining = capacity;\n        int well_size = well.size();\n        \n        // Align to 8-byte boundary\n        size_t i = 0;\n        for (; i + 8 <= well_size; i += 8) {\n            __m256i well_vec = _mm256_loadu_si256((__m256i*)&well[i]);\n            __m256i ones_vec = _mm256_set1_epi32(1);\n            __m256i ones_count = _mm256_popcnt_epi32(well_vec);\n            \n            int ones_sum = _mm256_cvtsi256_si32(ones_count);\n            remaining -= ones_sum;\n            \n            if (remaining < 0) {\n                total_trips++;\n                remaining += capacity;\n            }\n        }\n        \n        // Handle remaining elements\n        for (; i < well_size; ++i) {\n            if (well[i] == 1) {\n                remaining--;\n                if (remaining < 0) {\n                    total_trips++;\n                    remaining += capacity;\n                }\n            }\n        }\n        \n        if (remaining < 0) {\n            total_trips++;\n        }\n    }\n    \n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_trips = 0;\n    __m256i bucket_mask = _mm256_set1_epi32(capacity);\n    __m256i zero_mask = _mm256_setzero_si256();\n    \n    for (const auto & well : grid) {\n        int remaining = capacity;\n        int well_size = well.size();\n        \n        for (size_t i = 0; i < well_size; i += 8) {\n            __m256i values;\n            if (i + 8 <= well_size) {\n                values = _mm256_loadu_si256((__m256i*)&well[i]);\n            } else {\n                values = _mm256_loadu_si256((__m256i*)&well[i]);\n                values = _mm256_andnot_si256(_mm256_setr_epi32(0, 0, 0, 0, 0, 0, 0, -1), values);\n            }\n            \n            __m256i extracted = _mm256_min_epi32(values, bucket_mask);\n            remaining -= _mm256_movemask_epi8(extracted);\n            total_trips += _mm256_movemask_epi8(extracted);\n            \n            if (remaining <= 0) {\n                total_trips++;\n                remaining = capacity;\n            }\n        }\n        \n        if (remaining < capacity) {\n            total_trips++;\n        }\n    }\n    \n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_trips = 0;\n    __m256i bucket_mask = _mm256_set1_epi32(capacity);\n    __m256i zero_mask = _mm256_setzero_si256();\n    \n    for (const auto & well : grid) {\n        int remaining = capacity;\n        int well_size = well.size();\n        \n        // Align to 8-byte boundary\n        size_t i = 0;\n        for (; i + 8 <= well_size; i += 8) {\n            __m256i well_vec = _mm256_loadu_si256((__m256i*)&well[i]);\n            __m256i ones_vec = _mm256_set1_epi32(1);\n            __m256i count_vec = _mm256_and_si256(well_vec, ones_vec);\n            \n            while (!_mm256_testz_si256(count_vec, count_vec)) {\n                __m256i temp = _mm256_sub_epi32(count_vec, bucket_mask);\n                if (_mm256_cmpgt_epi32(temp, zero_mask)) {\n                    count_vec = temp;\n                } else {\n                    count_vec = _mm256_andnot_si256(bucket_mask, count_vec);\n                }\n                total_trips++;\n            }\n        }\n        \n        // Handle remaining elements\n        for (; i < well_size; ++i) {\n            if (well[i] > 0) {\n                remaining -= 1;\n                if (remaining == 0) {\n                    total_trips++;\n                    remaining = capacity;\n                }\n            }\n        }\n        \n        if (remaining != capacity) {\n            total_trips++;\n        }\n    }\n    \n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_trips = 0;\n    for (const auto &well : grid) {\n        __m256i bucket = _mm256_setzero_si256();\n        __m256i threshold = _mm256_set1_epi32(capacity);\n        int remaining_units = 0;\n        for (size_t i = 0; i < well.size(); i += 8) {\n            __m256i well_chunk = _mm256_loadu_si256((__m256i*)&well[i]);\n            bucket = _mm256_add_epi32(bucket, well_chunk);\n        }\n        remaining_units = _mm256_extract_epi32(bucket, 0) + _mm256_extract_epi32(bucket, 1) +\n                          _mm256_extract_epi32(bucket, 2) + _mm256_extract_epi32(bucket, 3) +\n                          _mm256_extract_epi32(bucket, 4) + _mm256_extract_epi32(bucket, 5) +\n                          _mm256_extract_epi32(bucket, 6) + _mm256_extract_epi32(bucket, 7);\n        total_trips += (remaining_units + capacity - 1) / capacity;\n    }\n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_trips = 0;\n    __m256i bucket_mask = _mm256_set1_epi32(capacity);\n    __m256i zero_mask = _mm256_setzero_si256();\n    \n    for (const auto & well : grid) {\n        int remaining = capacity;\n        int well_size = well.size();\n        \n        for (size_t i = 0; i < well_size; i += 8) {\n            __m256i well_chunk;\n            if (i + 8 <= well_size) {\n                well_chunk = _mm256_loadu_si256((__m256i*)&well[i]);\n            } else {\n                well_chunk = _mm256_loadu_si256((__m256i*)&well[i]);\n                well_chunk = _mm256_andnot_si256(_mm256_setr_epi32(0, 0, 0, 0, 0, 0, 0, -1), well_chunk);\n            }\n            \n            __m256i subtract_result = _mm256_sub_epi32(bucket_mask, well_chunk);\n            __m256i negative_mask = _mm256_cmpgt_epi32(zero_mask, subtract_result);\n            __m256i valid_water = _mm256_andnot_si256(negative_mask, well_chunk);\n            \n            int extracted = _mm256_movemask_epi8(_mm256_castsi256_ps(valid_water));\n            remaining -= __builtin_popcount(extracted);\n            \n            if (remaining < 0) {\n                total_trips++;\n                remaining += capacity;\n            }\n        }\n        \n        if (remaining < capacity) {\n            total_trips++;\n        }\n    }\n    \n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    \n    __m256i mask_even = _mm256_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i zero = _mm256_setzero_si256();\n\n    while(i + 8 <= n){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_masked = _mm256_and_si256(vec, mask_even);\n        __m256i odd_masked = _mm256_sub_epi16(vec, even_masked);\n        \n        __m256i odd_mask = _mm256_cmpgt_epi16(_mm256_setzero_si256(), odd_masked);\n        __m256i odd_filtered = _mm256_and_si256(odd_masked, odd_mask);\n\n        __m256i sum_vec = _mm256_add_epi16(_mm256_permutevar8x32_epi32(odd_filtered, _mm256_setr_epi32(0,1,2,3,0,1,2,3)), zero);\n        sum += _mm256_cvtsi128_si32(_mm256_castsi256_si128(sum_vec));\n        \n        i += 8;\n    }\n\n    for(; i < n; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    \n    __m256i mask_even = _mm256_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while(i + 8 <= n){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_masked = _mm256_and_si256(vec, mask_even);\n        __m256i odd_masked = _mm256_sub_epi16(vec, even_masked);\n        \n        __m256i odd_mask = _mm256_cmpgt_epi16(_mm256_setzero_si256(), odd_masked);\n        __m256i odd_filtered = _mm256_and_si256(odd_masked, odd_mask);\n\n        __m256i sum_vec = _mm256_add_epi16(_mm256_setzero_si256(), odd_filtered);\n        __m128i sum_low = _mm256_castsi256_si128(sum_vec);\n        __m128i sum_high = _mm256_extractf128_si256(sum_vec, 1);\n        sum_low = _mm_add_epi16(sum_low, sum_high);\n        sum += _mm_cvtsi128_si32(sum_low);\n        \n        i += 8;\n    }\n\n    for(; i < n; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    \n    __m256i mask_even = _mm256_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while(i + 8 <= n){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i even_masked = _mm256_and_si256(vec, mask_even);\n        __m256i odd_masked = _mm256_sub_epi16(vec, even_masked);\n        \n        __m256i odd_mask = _mm256_cmpgt_epi16(_mm256_setzero_si256(), odd_masked);\n        __m256i odd_filtered = _mm256_blendv_epi8(zero_vec, odd_masked, odd_mask);\n\n        __m256i sum_vec = _mm256_add_epi64(_mm256_cvtepi16_epi64(odd_filtered), \n                                           _mm256_permute4x64_epi64(_mm256_cvtepi16_epi64(odd_filtered), 0b1110));\n        __m256i high_sum = _mm256_extractf128_epi64(sum_vec, 1);\n        __m256i low_sum = _mm256_castsi256_si128(sum_vec);\n        sum += _mm_cvtsi128_si64(low_sum) + _mm_cvtsi128_si64(high_sum);\n        \n        i += 8;\n    }\n\n    for(; i < n; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n    \n    __m256i mask_even = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i zero = _mm256_setzero_si256();\n\n    while(i + 16 <= n){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i even_masked = _mm256_and_si256(vec, mask_even);\n        __m256i odd_masked = _mm256_sub_epi16(vec, even_masked);\n\n        __m256i odd_mask = _mm256_cmpgt_epi16(_mm256_setzero_si256(), odd_masked);\n        __m256i odd_filtered = _mm256_and_si256(odd_masked, odd_mask);\n\n        __m256i sum_vec = _mm256_add_epi16(_mm256_permutevar8x32_epi32(odd_filtered, _mm256_set_epi32(3, 2, 1, 0, 3, 2, 1, 0)), \n                                            _mm256_permutevar8x32_epi32(zero, _mm256_set_epi32(3, 2, 1, 0, 3, 2, 1, 0)));\n\n        sum += _mm256_cvtsi128_si32(_mm256_castsi256_si128(sum_vec));\n        i += 16;\n    }\n\n    // Handle remaining elements\n    for(; i < n; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    const int vec_size = 8; // AVX2 processes 8 16-bit integers at once\n    int64_t sum = 0;\n    size_t i = 0;\n\n    __m256i mask_even = _mm256_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i zero_vec = _mm256_setzero_si256();\n\n    while(i + vec_size <= lst.size()){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i even_masked = _mm256_and_si256(vec, mask_even);\n        __m256i odd_masked = _mm256_sub_epi16(vec, even_masked);\n\n        __m256i odd_sign = _mm256_cmpgt_epi16(zero_vec, odd_masked);\n        __m256i odd_abs = _mm256_sub_epi16(_mm256_setzero_si256(), odd_masked);\n        __m256i odd_filtered = _mm256_blendv_epi8(zero_vec, odd_abs, odd_sign);\n\n        __m256i even_sign = _mm256_cmpgt_epi16(even_masked, zero_vec);\n        __m256i even_abs = _mm256_sub_epi16(even_masked, _mm256_setzero_si256());\n        __m256i even_filtered = _mm256_blendv_epi8(zero_vec, even_abs, even_sign);\n\n        __m256i combined = _mm256_add_epi16(odd_filtered, even_filtered);\n        __m256i sum_vec = _mm256_set1_epi16(sum);\n        sum_vec = _mm256_add_epi16(sum_vec, combined);\n        sum = _mm256_extract_epi64(sum_vec, 0);\n\n        i += vec_size;\n    }\n\n    for(; i < lst.size(); ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum += lst[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = k > arr.size() ? arr.size() : k;\n    const __m256i two_digits_mask = _mm256_set_epi32(99, 99, 99, 99, 99, 99, 99, 99);\n    \n    for(int i=0; i<limit; i+=8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i masked = _mm256_and_si256(vec, two_digits_mask);\n        __m256i cmp = _mm256_cmpgt_epi32(masked, vec);\n        __m256i valid = _mm256_blendv_epi8(_mm256_setzero_si256(), vec, cmp);\n        __m256i sums = _mm256_sad_epu8(valid, _mm256_setzero_si256());\n        sum += _mm256_extract_epi32(sums, 0) +\n               _mm256_extract_epi32(sums, 1) +\n               _mm256_extract_epi32(sums, 2) +\n               _mm256_extract_epi32(sums, 3);\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = k > arr.size() ? arr.size() : k;\n    const __m256i two_digits_limit = _mm256_set1_epi32(99);\n    const __m256i zero = _mm256_setzero_si256();\n\n    for(int i=0; i<limit; i+=8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpgt_epi32(two_digits_limit, vec);\n        vec = _mm256_and_si256(vec, mask);\n        __m256i cmp = _mm256_cmpeq_epi32(vec, zero);\n        mask = _mm256_xor_si256(mask, cmp);\n        __m256i filtered = _mm256_andnot_si256(mask, vec);\n        __m256i sums = _mm256_sad_epu8(_mm256_castsi256_si128(filtered), _mm256_extracti128_si256(filtered));\n        sums = _mm256_add_epi32(sums, _mm256_permute4x64_epi64(sums, 78)); // Add across lanes\n        sum += _mm256_cvtsi256_si32(sums);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = std::min(k, (int)arr.size());\n    const __m256i two_digits_mask = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 99, 9);\n    const __m256i zero = _mm256_setzero_si256();\n\n    for(int i=0; i<limit; i+=8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_set1_epi32(100), vec);\n        __m256i valid = _mm256_and_si256(mask, _mm256_cmpgt_epi32(vec, two_digits_mask));\n        __m256i filtered = _mm256_blendv_epi8(zero, vec, valid);\n        __m256i sums = _mm256_sad_epu8(_mm256_castsi256_si128(filtered), _mm256_extracti128_si256(filtered));\n        sum += _mm_cvtsi128_si32(sums);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = k > arr.size() ? arr.size() : k;\n    const __m256i two_digits_mask = _mm256_set_epi32(99, 99, 99, 99, 99, 99, 99, 99);\n    \n    for(int i=0; i<limit; i+=8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i masked = _mm256_and_si256(vec, two_digits_mask);\n        __m256i cmp = _mm256_cmpgt_epi32(masked, vec);\n        __m256i valid = _mm256_blendv_epi8(_mm256_setzero_si256(), vec, cmp);\n        \n        __m256i sums = _mm256_sad_epu8(valid, _mm256_setzero_si256());\n        sum += _mm256_extract_epi32(sums, 0) +\n               _mm256_extract_epi32(sums, 1) +\n               _mm256_extract_epi32(sums, 2) +\n               _mm256_extract_epi32(sums, 3);\n    }\n    \n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = k > arr.size() ? arr.size() : k;\n    __m256i mask = _mm256_set1_epi32(99);\n    __m256i zero = _mm256_setzero_si256();\n    for(int i=0; i < limit; i+=8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(mask, vec);\n        vec = _mm256_and_si256(vec, cmp);\n        __m256i cmp_neg = _mm256_cmpgt_epi32(vec, _mm256_sub_epi32(zero, mask));\n        vec = _mm256_and_si256(vec, cmp_neg);\n        __m256i sums = _mm256_sad_epu8(_mm256_castsi256_si128(vec), _mm256_extractf128_si256(vec,1));\n        sums = _mm256_add_epi32(sums, _mm256_permute4x64_epi64(sums, 78));\n        sum += _mm256_cvtsi128_si32(_mm256_castsi256_si128(sums));\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int vec_size = 8;\n    if(lst.size() < vec_size) {\n        for(size_t i = 0; i < lst.size()-1; ++i){\n            if(lst[i] > lst[i+1]) return false;\n            if(lst[i] == lst[i+1] && i < lst.size()-2 && lst[i] == lst[i+2]) return false;\n        }\n        return true;\n    }\n    __m256i prev_vec = _mm256_set1_epi32(-1);\n    __m256i dup_check = _mm256_setzero_si256();\n    for(size_t i = 0; i <= lst.size() - vec_size; i += vec_size){\n        __m256i curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i gt_mask = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n        __m256i eq_mask = _mm256_cmpeq_epi32(curr_vec, prev_vec);\n        if(_mm256_movemask_epi8(gt_mask)) return false;\n        dup_check = _mm256_or_si256(dup_check, eq_mask);\n        prev_vec = curr_vec;\n    }\n    int dup_result[8];\n    _mm256_storeu_si256((__m256i*)dup_result, dup_check);\n    for(int j = 0; j < vec_size; ++j){\n        if(dup_result[j] >= 2) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int vec_size = 8;\n    if(lst.size() < vec_size) {\n        for(size_t i = 0; i < lst.size()-1; ++i){\n            if(lst[i] > lst[i+1]) return false;\n            if(lst[i] == lst[i+1] && i < lst.size()-2 && lst[i] == lst[i+2]) return false;\n        }\n        return true;\n    }\n    __m256i prev_vec = _mm256_set1_epi32(-1);\n    __m256i dup_check = _mm256_setzero_si256();\n    for(size_t i = 0; i <= lst.size() - vec_size; i += vec_size){\n        __m256i curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i gt_mask = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n        __m256i eq_mask = _mm256_cmpeq_epi32(curr_vec, prev_vec);\n        if(_mm256_movemask_epi8(gt_mask)) return false;\n        dup_check = _mm256_or_si256(dup_check, eq_mask);\n        prev_vec = curr_vec;\n    }\n    int dup_result[8];\n    _mm256_storeu_si256((__m256i*)dup_result, dup_check);\n    for(int j = 0; j < vec_size; ++j){\n        if(dup_result[j] > 1) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n    int n = lst.size();\n    __m256i prev_vec = _mm256_set1_epi32(-1); // Initialize with a value less than any integer\n    __m256i dup_check = _mm256_setzero_si256();\n    int i = 0;\n    while(i + 8 <= n){\n        __m256i curr_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i cmp_gt = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n        __m256i cmp_eq = _mm256_cmpeq_epi32(curr_vec, prev_vec);\n        dup_check = _mm256_or_si256(dup_check, cmp_eq);\n        if(_mm256_movemask_epi8(cmp_gt)) return false;\n        prev_vec = curr_vec;\n        i += 8;\n    }\n    // Handle remaining elements\n    for(int j = i; j < n; ++j){\n        if(lst[j] < lst[j-1]) return false;\n        if(j > 0 && lst[j] == lst[j-1]) return false;\n    }\n    return !_mm256_movemask_epi8(dup_check);\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n    int n = lst.size();\n    __m256i prev_vec = _mm256_set1_epi32(-1); // Initialize with a value less than any integer\n    __m256i zero_vec = _mm256_setzero_si256();\n    __m256i one_vec = _mm256_set1_epi32(1);\n    __m256i two_vec = _mm256_set1_epi32(2);\n    \n    for(int i=0; i<n; i+=8){\n        __m256i curr_vec;\n        if(i + 8 <= n){\n            curr_vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        }\n        else{\n            curr_vec = _mm256_insert_epi32(zero_vec, lst[i], 0);\n            for(int j=1; j<8 && i+j<n; ++j){\n                curr_vec = _mm256_insert_epi32(curr_vec, lst[i+j], j);\n            }\n        }\n        \n        // Check ascending order\n        __m256i gt_mask = _mm256_cmpgt_epi32(prev_vec, curr_vec);\n        if(!_mm256_testz_si256(gt_mask, gt_mask)) return false;\n        \n        // Check duplicates\n        __m256i eq_mask = _mm256_cmpeq_epi32(curr_vec, prev_vec);\n        __m256i eq_count = _mm256_popcnt_epi32(eq_mask);\n        if(_mm256_movemask_epi8(eq_count) > 2) return false;\n        \n        prev_vec = curr_vec;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    const int vec_size = 8; // AVX2: 8 integers per register\n    if(lst.size() < vec_size + 1) {\n        for(size_t i = 0; i < lst.size() - 1; ++i){\n            if(lst[i] > lst[i+1]) return false;\n            if(i > 0 && lst[i-1] == lst[i] && lst[i] == lst[i+1]) return false;\n        }\n        return true;\n    }\n\n    __m256i prev_vec = _mm256_set1_epi32(lst[vec_size -1]);\n    __m256i duplicates = _mm256_setzero_si256();\n    \n    for(int i = vec_size -1; i < static_cast<int>(lst.size()) -1; i += vec_size){\n        __m256i current_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i next_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i + vec_size]));\n\n        __m256i greater_than_prev = _mm256_cmpgt_epi32(prev_vec, current_vec);\n        __m256i equal_to_prev = _mm256_cmpeq_epi32(prev_vec, current_vec);\n        __m256i equal_to_next = _mm256_cmpeq_epi32(current_vec, next_vec);\n\n        if(!_mm256_testz_si256(greater_than_prev, greater_than_prev)){\n            return false;\n        }\n\n        duplicates = _mm256_or_si256(duplicates, equal_to_prev);\n        prev_vec = current_vec;\n    }\n\n    // Check remaining elements\n    for(int i = lst.size() - (lst.size() % vec_size); i < lst.size() -1; ++i){\n        if(lst[i] > lst[i+1]) return false;\n        if(i > 0 && lst[i-1] == lst[i] && lst[i] == lst[i+1]) return false;\n    }\n\n    return !_mm256_testz_si256(duplicates, duplicates);\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n == 0) return -32768;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n\n    int i = 0;\n    for(; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, signs);\n    }\n\n    __m256i sum_scalar = _mm256_setzero_si256();\n    __m256i sign_prod_scalar = _mm256_setzero_si256();\n\n    // Horizontal sum for remaining elements\n    for(; i < n; ++i){\n        int val = arr[i];\n        sum_scalar = _mm256_insert_epi32(sum_scalar, abs(val), 0);\n        sign_prod_scalar = _mm256_insert_epi32(sign_prod_scalar, (val > 0) ? 1 : ((val < 0) ? -1 : 0), 0);\n    }\n\n    __m256i total_sum = _mm256_add_epi32(sum_vec, sum_scalar);\n    __m256i total_sign = _mm256_mullo_epi32(sign_prod_vec, sign_prod_scalar);\n\n    int final_sum = 0;\n    int final_sign = 1;\n\n    for(int j = 0; j < 8; ++j){\n        final_sum += _mm256_extract_epi32(total_sum, j);\n        final_sign *= _mm256_extract_epi32(total_sign, j);\n    }\n\n    return final_sum * final_sign;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n    const int n = arr.size();\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n    int i = 0;\n    for(; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, signs);\n    }\n    __m256i sum_scalar = _mm256_hadd_epi32(sum_vec, sum_vec);\n    sum_scalar = _mm256_hadd_epi32(sum_scalar, sum_scalar);\n    int sum_result = _mm256_extract_epi32(sum_scalar, 0);\n    int sign_result = _mm256_extract_epi32(sign_prod_vec, 0);\n    for(; i < n; ++i){\n        sum_result += abs(arr[i]);\n        sign_result *= (arr[i] > 0) ? 1 : ((arr[i] < 0) ? -1 : 0);\n    }\n    return sum_result * sign_result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n == 0) return -32768;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n\n    int i = 0;\n    for(; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, signs);\n    }\n\n    __m256i sum_scalar = _mm256_setzero_si256();\n    __m256i sign_prod_scalar = _mm256_setzero_si256();\n\n    // Horizontal sum for remaining elements\n    for(; i < n; ++i){\n        int val = arr[i];\n        sum_scalar = _mm256_insert_epi32(sum_scalar, abs(val), 0);\n        sign_prod_scalar = _mm256_insert_epi32(sign_prod_scalar, sign(val), 0);\n    }\n\n    __m256i total_sum = _mm256_add_epi32(sum_vec, sum_scalar);\n    __m256i total_sign = _mm256_mullo_epi32(sign_prod_vec, sign_prod_scalar);\n\n    int sum_result = _mm256_extract_epi32(total_sum, 0);\n    int sign_result = _mm256_extract_epi32(total_sign, 0);\n\n    return sum_result * sign_result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n    const int n = arr.size();\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n    int i = 0;\n    for(; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, signs);\n    }\n    __m256i sum_scalar = _mm256_hadd_epi32(sum_vec, sum_vec);\n    sum_scalar = _mm256_hadd_epi32(sum_scalar, sum_scalar);\n    int sum_result = _mm256_extract_epi32(sum_scalar, 0);\n    int sign_prod_result = _mm256_extract_epi32(sign_prod_vec, 0);\n    for(; i < n; ++i){\n        sum_result += abs(arr[i]);\n        sign_prod_result *= (arr[i] > 0) ? 1 : ((arr[i] < 0) ? -1 : 0);\n    }\n    return sum_result * sign_prod_result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n == 0) return -32768;\n\n    __m256i sum_vec = _mm256_setzero_si256();\n    __m256i sign_prod_vec = _mm256_set1_epi32(1);\n\n    int i = 0;\n    for(; i + 8 <= n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i signs = _mm256_sign_epi32(_mm256_set1_epi32(1), vec);\n        sum_vec = _mm256_add_epi32(sum_vec, abs_vec);\n        sign_prod_vec = _mm256_mullo_epi32(sign_prod_vec, signs);\n    }\n\n    __m256i sum_scalar = _mm256_setzero_si256();\n    __m256i sign_prod_scalar = _mm256_setzero_si256();\n\n    // Horizontal sum for remaining elements\n    for(int j = i; j < n; ++j){\n        int val = arr[j];\n        sum_scalar = _mm256_insert_epi32(sum_scalar, abs(val), j);\n        sign_prod_scalar = _mm256_insert_epi32(sign_prod_scalar, sign(val), j);\n    }\n\n    __m256i total_sum = _mm256_add_epi32(sum_vec, sum_scalar);\n    __m256i total_sign = _mm256_mullo_epi32(sign_prod_vec, sign_prod_scalar);\n\n    int final_sum = 0;\n    for(int k = 0; k < 8; ++k){\n        final_sum += _mm256_extract_epi32(total_sum, k);\n    }\n    final_sum *= _mm256_extract_epi32(total_sign, 0);\n\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m256i odd_mask = _mm256_setr_epi8(\n        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1\n    );\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi64x(1);\n\n    uint64_t remaining = n;\n    while (remaining != 0) {\n        __m256i vec_digits = _mm256_setr_epi8(\n            (remaining % 100000000) / 10000000 % 10,\n            (remaining % 10000000) / 1000000 % 10,\n            (remaining % 1000000) / 100000 % 10,\n            (remaining % 100000) / 10000 % 10,\n            (remaining % 10000) / 1000 % 10,\n            (remaining % 1000) / 100 % 10,\n            (remaining % 100) / 10 % 10,\n            remaining % 10\n        );\n\n        __m256i odd_digits = _mm256_and_si256(vec_digits, odd_mask);\n        __m256i non_zero = _mm256_cmpgt_epi8(odd_digits, zero);\n        __m256i valid_digits = _mm256_and_si256(odd_digits, non_zero);\n\n        if (!_mm256_movemask_epi8(non_zero)) {\n            return 0;\n        }\n\n        __m256i temp_product = _mm256_set1_epi64x(1);\n        for(int i = 0; i < 8; ++i){\n            __m256i digit = _mm256_set1_epi64x(valid_digits.m256i_i8[i]);\n            temp_product = _mm256_mullo_epi64(temp_product, digit);\n        }\n        product = _mm256_mullo_epi64(product, temp_product);\n\n        remaining /= 100000000;\n    }\n\n    __m256i result_vec = _mm256_permutevar8x32_epi32(product, _mm256_setr_epi32(0,0,0,0,0,0,0,0));\n    return _mm256_cvtsi256_si64(result_vec);\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m256i odd_mask = _mm256_set_epi8(\n        -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0,\n        -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0, -1, 0\n    );\n    __m256i data = _mm256_set_epi64x(\n        (long long)n, (long long)(n >> 32), 0, 0\n    );\n    __m256i shifted = _mm256_and_si256(data, _mm256_set1_epi8(0xF));\n    __m256i masked = _mm256_and_si256(shifted, odd_mask);\n    __m256i non_zero = _mm256_cmpgt_epi8(masked, _mm256_setzero_si256());\n    int mask_all_zero = _mm256_movemask_epi8(_mm256_cmpeq_epi8(non_zero, _mm256_setzero_si256()));\n    if (mask_all_zero == 0xFFFF) return 0;\n\n    __m256i ones = _mm256_set1_epi8(1);\n    __m256i result = _mm256_mullo_epi16(_mm256_cvtepu8_epi16(masked), ones);\n    result = _mm256_slli_epi32(result, 4);\n\n    uint64_t res = _mm256_extract_epi64(result, 0);\n    return res;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m256i odd_mask = _mm256_set_epi8(\n        -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1,\n        -1, -1, -1, -1, -1, -1, -1, -1\n    );\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi64x(1);\n\n    uint64_t num = n;\n    while (num != 0) {\n        __m256i digits = _mm256_set_epi8(\n            (num & 0x0F), ((num >> 4) & 0x0F), ((num >> 8) & 0x0F), ((num >> 12) & 0x0F),\n            ((num >> 16) & 0x0F), ((num >> 20) & 0x0F), ((num >> 24) & 0x0F), ((num >> 28) & 0x0F),\n            (num & 0x0F), ((num >> 4) & 0x0F), ((num >> 8) & 0x0F), ((num >> 12) & 0x0F),\n            ((num >> 16) & 0x0F), ((num >> 20) & 0x0F), ((num >> 24) & 0x0F), ((num >> 28) & 0x0F),\n            (num & 0x0F), ((num >> 4) & 0x0F), ((num >> 8) & 0x0F), ((num >> 12) & 0x0F),\n            ((num >> 16) & 0x0F), ((num >> 20) & 0x0F), ((num >> 24) & 0x0F), ((num >> 28) & 0x0F),\n            (num & 0x0F), ((num >> 4) & 0x0F), ((num >> 8) & 0x0F), ((num >> 12) & 0x0F),\n            ((num >> 16) & 0x0F), ((num >> 20) & 0x0F), ((num >> 24) & 0x0F), ((num >> 28) & 0x0F)\n        );\n\n        __m256i is_odd = _mm256_and_si256(digits, odd_mask);\n        __m256i is_even = _mm256_cmpeq_epi8(is_odd, zero);\n\n        // If all are even, set product to zero\n        if (!_mm256_movemask_epi8(_mm256_castsi256_ps(is_even))) {\n            return 0;\n        }\n\n        __m256i odd_digits = _mm256_blendv_epi8(zero, digits, is_odd);\n        __m256i odd_digits_product = _mm256_mullo_epi16(odd_digits, product);\n\n        product = odd_digits_product;\n\n        num /= 10000000000ULL;\n    }\n\n    __m256i result_vec = _mm256_permutevar8x32_epi32(product, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n    uint64_t result = _mm256_cvtsi256_si64(result_vec);\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m256i odd_mask = _mm256_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n                                        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi64x(1);\n\n    uint64_t remaining = n;\n    while (remaining != 0) {\n        __m256i digits = _mm256_set_epi64x((remaining >> 48) & 0xF, \n                                           (remaining >> 40) & 0xF, \n                                           (remaining >> 32) & 0xF, \n                                           (remaining >> 24) & 0xF, \n                                           (remaining >> 16) & 0xF, \n                                           (remaining >> 8) & 0xF, \n                                           remaining & 0xF, \n                                           0);\n\n        __m256i is_odd = _mm256_and_si256(digits, _mm256_set1_epi8(1));\n        __m256i valid_digits = _mm256_cmpeq_epi8(is_odd, odd_mask);\n\n        __m256i current_product = _mm256_broadcastq_epi64(_mm_set_epi64x(product, product));\n        __m256i updated_product = _mm256_blendv_epi8(current_product, \n                                                     _mm256_mullo_epi16(current_product, digits), \n                                                     valid_digits);\n\n        product = _mm256_extract_epi64(updated_product, 0);\n        remaining >>= 7;\n    }\n\n    return product;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m256i odd_mask = _mm256_setr_epi8(\n        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1\n    );\n    __m256i zero = _mm256_setzero_si256();\n    __m256i product = _mm256_set1_epi64x(1);\n\n    uint64_t num = n;\n    while (num != 0) {\n        __m256i digits = _mm256_setr_epi8(\n            (num >> 0) & 0xF, (num >> 4) & 0xF, (num >> 8) & 0xF, (num >> 12) & 0xF,\n            (num >> 16) & 0xF, (num >> 20) & 0xF, (num >> 24) & 0xF, (num >> 28) & 0xF,\n            (num >> 32) & 0xF, (num >> 36) & 0xF, (num >> 40) & 0xF, (num >> 44) & 0xF,\n            (num >> 48) & 0xF, (num >> 52) & 0xF, (num >> 56) & 0xF, (num >> 60) & 0xF\n        );\n\n        __m256i is_odd = _mm256_cmpeq_epi8(_mm256_and_si256(digits, odd_mask), odd_mask);\n        __m256i valid_digits = _mm256_blendv_epi8(zero, digits, is_odd);\n\n        __m256i temp_product = _mm256_set1_epi64x(1);\n        for(int i = 0; i < 16; ++i){\n            __m256i digit = _mm256_permutevar8x32_epi32(valid_digits, _mm256_set_epi32(0, 0, 0, 0, 0, 0, i, 0));\n            digit = _mm256_and_si256(digit, _mm256_set1_epi8(0xF));\n            temp_product = _mm256_mullo_epi64(temp_product, digit);\n        }\n\n        product = _mm256_mullo_epi64(product, temp_product);\n\n        num >>= 64;\n    }\n\n    __m256i result_vec = _mm256_setzero_si256();\n    for(int i = 0; i < 4; ++i){\n        __m256i temp = _mm256_extractf128_si256(product, i);\n        __m128i low = _mm256_castsi256_si128(temp);\n        __m128i high = _mm256_extractf128_si256(product, i+1);\n        result_vec = _mm256_add_epi64(result_vec, _mm_add_epi64(low, high));\n    }\n\n    uint64_t result = _mm256_cvtsi256_si64(result_vec);\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const int n = lst.size();\n    const int limit = n - (n % 8); // Ensure we process full vectors of 8 elements\n\n    __m256 zero = _mm256_setzero_ps();\n    __m256 sum_vec = zero;\n\n    for(int i = 0; i < limit; i += 8){\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        \n        // Round up each element: (int)(ceil(x)) = (int)(floor(x + 0.999))\n        __m256 add_val = _mm256_set1_ps(0.999f);\n        __m256 rounded = _mm256_floor_ps(_mm256_add_ps(vec, add_val));\n        \n        // Compute squares\n        __m256 squared = _mm256_mul_ps(rounded, rounded);\n        \n        // Horizontal sum of the vector\n        __m256 temp_sum = _mm256_hadd_ps(squared, squared);\n        temp_sum = _mm256_hadd_ps(temp_sum, temp_sum);\n        uint64_t partial_sum = _mm256_cvtsi128_epi64(_mm256_castps_si256(temp_sum));\n        \n        sum += partial_sum;\n    }\n\n    // Handle remaining elements\n    for(int i = limit; i < n; ++i){\n        float val = lst[i];\n        int rounded = (int)(val + 0.999f);\n        sum += rounded * rounded;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const int n = lst.size();\n    const int limit = n - (n % 8); // Ensure we process full vectors of 8 elements\n\n    __m256 zero = _mm256_setzero_ps();\n    __m256 sum_vec = zero;\n\n    for(int i = 0; i < limit; i += 8){\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        \n        // Round up each element: (int)(ceil(x)) = (int)(floor(x + 1))\n        __m256 add_one = _mm256_set1_ps(1.0f);\n        __m256 floor_input = _mm256_add_ps(vec, add_one);\n        __m256i rounded = _mm256_cvttps_epi32(floor_input);\n        __m256 rounded_float = _mm256_cvtepi32_ps(rounded);\n\n        // Compute squares\n        __m256 squared = _mm256_mul_ps(rounded_float, rounded_float);\n\n        // Horizontal sum of the vector\n        __m256 temp_sum = _mm256_hadd_ps(squared, squared);\n        temp_sum = _mm256_hadd_ps(temp_sum, temp_sum);\n        uint32_t hsum = _mm256_extract_epi32(_mm256_castps_si256(temp_sum), 0);\n        sum += hsum;\n    }\n\n    // Handle remaining elements\n    for(int i = limit; i < n; ++i){\n        float val = lst[i];\n        int rounded_val = (int)(val + 1.0f);\n        sum += rounded_val * rounded_val;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const size_t n = lst.size();\n    size_t i = 0;\n\n    // Process in chunks of 4 using AVX\n    for (; i + 7 < n; i += 8) {\n        __m256 input = _mm256_loadu_ps(&lst[i]);\n        \n        // Round up: add 0.999999f and use floor operation\n        __m256 rounded = _mm256_floor_ps(_mm256_add_ps(input, _mm256_set1_ps(0.999999f)));\n        \n        // Square the rounded values\n        __m256 squared = _mm256_mul_ps(rounded, rounded);\n        \n        // Horizontal sum of the squared values\n        __m256i summed = _mm256_cvttps_epi32(squared);\n        uint64_t chunk_sum = _mm256_castsi256_si64(summed) +\n                             _mm256_extract_epi32(summed, 1) +\n                             _mm256_extract_epi32(summed, 2) +\n                             _mm256_extract_epi32(summed, 3);\n        \n        sum += chunk_sum;\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        float val = lst[i];\n        int rounded = (int)(val + 0.999999f);\n        sum += rounded * rounded;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const size_t n = lst.size();\n    size_t i = 0;\n\n    // Ensure alignment for AVX\n    for (; i + 7 < n; i += 8) {\n        __m256 input = _mm256_loadu_ps(&lst[i]);\n        \n        // Round up each element\n        __m256 rounded = _mm256_ceil_ps(input);\n        \n        // Convert to integer\n        __m256i int_values = _mm256_cvttps_epi32(rounded);\n        \n        // Square each value\n        __m256i squared = _mm256_mullo_epi32(int_values, int_values);\n        \n        // Horizontal sum\n        __m256i summed = _mm256_sad_epu8(_mm256_castsi256_si128(squared),\n                                         _mm256_extracti128_si256(squared));\n        summed = _mm_add_epi32(summed, _mm256_extracti128_si256(summed, 1));\n        \n        // Add to total sum\n        sum += _mm_cvtsi128_si32(summed);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        float val = lst[i];\n        int rounded = static_cast<int>(std::ceil(val));\n        sum += rounded * rounded;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const int n = lst.size();\n    const int size = n / 8 * 8; // Process in chunks of 8 AVX elements\n\n    __m256 zero = _mm256_setzero_ps();\n    __m256 sum_vec = zero;\n\n    for(int i=0; i<size; i+=8){\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        \n        // Round up: add 0.5f and use floor operation\n        __m256 add_half = _mm256_set1_ps(0.5f);\n        __m256 rounded = _mm256_floor_ps(_mm256_add_ps(vec, add_half));\n        \n        // Square the rounded values\n        __m256 squared = _mm256_mul_ps(rounded, rounded);\n        \n        // Horizontal sum of the 8 elements\n        __m256 temp_sum = _mm256_hadd_ps(squared, squared);\n        temp_sum = _mm256_hadd_ps(temp_sum, temp_sum);\n        uint32_t hsum = _mm256_extract_epi32((__m256i)temp_sum, 0);\n        \n        sum += hsum;\n    }\n\n    // Handle remaining elements\n    for(int i=size; i<n; ++i){\n        float val = lst[i];\n        val = val >= 0 ? (float)(int)(val + 0.5f) : (float)(int)(val - 0.5f);\n        sum += (uint64_t)(val * val);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    int limit = n - 4;\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i curr;\n    __m256i mask;\n\n    for(int i=1; i<=limit; i+=4){\n        curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        mask = _mm256_cmpgt_epi32(prev, curr);\n        unsigned int mask_int;\n        _mm256_storeu_si256((__m256i*)&mask_int, mask);\n        if(mask_int != 0){\n            for(int j=0; j<4; ++j){\n                if((mask_int & (1 << j)) != 0){\n                    return i + j;\n                }\n            }\n        }\n        prev = curr;\n    }\n\n    for(int i=limit+1; i<n; ++i){\n        if(arr[i] < arr[i-1]){\n            return i;\n        }\n    }\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    int limit = n - 4; // Ensure enough elements for AVX processing\n    int result = -1;\n\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i curr;\n    __m256i mask;\n\n    for(int i=1; i<=limit; i+=4){\n        curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        mask = _mm256_cmpgt_epi32(curr, prev);\n        int mask_int = _mm256_movemask_epi8(mask);\n        if(mask_int != 0){\n            for(int j=0; j<4; ++j){\n                if((mask_int >> j) & 1){\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n        prev = curr;\n    }\n\n    if(result == -1 && limit < n-1){\n        for(int i=limit+1; i<n-1; ++i){\n            if(arr[i] >= arr[i+1]){\n                result = i;\n                break;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    int limit = n / 8 * 8;\n    int result = -1;\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n\n    for(int i=1; i<limit; i+=8){\n        __m256i current = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp = _mm256_cmpgt_epi32(current, prev);\n        __m256i mask = _mm256_movemask_epi8(cmp);\n        if(mask != 0){\n            for(int j=0; j<8; ++j){\n                if((mask & (1 << j)) != 0){\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n        prev = current;\n    }\n\n    for(int i=limit; i<n-1; ++i){\n        if(arr[i] >= arr[i+1]){\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    int limit = n - 4; // Ensure enough elements for full vector operations\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i curr;\n    __m256i mask;\n    int i = 1;\n\n    for(; i <= limit; i += 8){\n        curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        mask = _mm256_cmpgt_epi32(curr, prev);\n        if(!_mm256_testz_si256(mask, mask)){\n            break;\n        }\n        prev = curr;\n    }\n\n    // Check remaining elements\n    for(int j = i; j < n - 1; ++j){\n        if(arr[j] >= arr[j + 1]){\n            return j;\n        }\n    }\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    int limit = n - 4; // Ensure enough elements for full vector operations\n    __m256i prev = _mm256_set1_epi32(arr[0]);\n    __m256i curr;\n    __m256i mask;\n\n    for(int i=1; i<=limit; i+=4){\n        curr = _mm256_loadu_si256((__m256i*)&arr[i]);\n        mask = _mm256_cmpgt_epi32(prev, curr); // Check if prev > curr\n        unsigned int mask_int;\n        _mm256_storeu_si256((__m256i*)&mask_int, mask);\n        if(mask_int != 0){\n            for(int j=0; j<4; ++j){\n                if((mask_int >> j) & 1){\n                    return i + j;\n                }\n            }\n        }\n        prev = curr;\n    }\n\n    // Handle remaining elements\n    for(int i=limit+1; i<n; ++i){\n        if(arr[i-1] > arr[i]){\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 8; // AVX size: 8 integers\n    std::vector<int> result(2, 0);\n    if(lst.size() < vec_size) return result;\n\n    __m256i neg_max = _mm256_set1_epi32(0);\n    __m256i pos_min = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i < lst.size(); i += vec_size){\n        __m256i input = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i negatives = _mm256_cmpgt_epi32(_mm256_setzero_si256(), input);\n        __m256i positives = _mm256_cmpgt_epi32(input, _mm256_setzero_si256());\n\n        __m256i negs_masked = _mm256_blendv_epi8(_mm256_setzero_si256(), input, negatives);\n        __m256i poss_masked = _mm256_blendv_epi8(_mm256_setzero_si256(), input, positives);\n\n        neg_max = _mm256_max_epi32(neg_max, negs_masked);\n        pos_min = _mm256_min_epi32(pos_min, poss_masked);\n    }\n\n    __m256i neg_max_final = _mm256_max_epi32(neg_max, _mm256_permutevar8x32_epi32(neg_max, _mm256_set_epi32(7,6,5,4,3,2,1,0)));\n    __m256i pos_min_final = _mm256_min_epi32(pos_min, _mm256_permutevar8x32_epi32(pos_min, _mm256_set_epi32(7,6,5,4,3,2,1,0)));\n\n    int neg_max_arr[8];\n    int pos_min_arr[8];\n    _mm256_storeu_si256((__m256i*)neg_max_arr, neg_max_final);\n    _mm256_storeu_si256((__m256i*)pos_min_arr, pos_min_final);\n\n    result[0] = *std::max_element(neg_max_arr, neg_max_arr + 8);\n    result[1] = *std::min_element(pos_min_arr, pos_min_arr + 8);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 8; // AVX size: 8 integers\n    std::vector<int> result(2, 0);\n    if(lst.size() < vec_size) return result;\n\n    __m256i neg_max = _mm256_set1_epi32(0);\n    __m256i pos_min = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i < lst.size(); i += vec_size){\n        __m256i input = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), input);\n        __m256i pos_mask = _mm256_cmpgt_epi32(input, _mm256_setzero_si256());\n\n        __m256i neg_vals = _mm256_blendv_epi8(_mm256_setzero_si256(), input, neg_mask);\n        __m256i pos_vals = _mm256_blendv_epi8(_mm256_setzero_si256(), input, pos_mask);\n\n        neg_max = _mm256_max_epi32(neg_max, neg_vals);\n        pos_min = _mm256_min_epi32(pos_min, pos_vals);\n    }\n\n    __m256i neg_max_extract = _mm256_permutevar8x32_epi32(neg_max, _mm256_set_epi32(7,6,5,4,3,2,1,0));\n    __m256i pos_min_extract = _mm256_permutevar8x32_epi32(pos_min, _mm256_set_epi32(7,6,5,4,3,2,1,0));\n\n    int neg_max_arr[8], pos_min_arr[8];\n    _mm256_storeu_si256((__m256i*)neg_max_arr, neg_max_extract);\n    _mm256_storeu_si256((__m256i*)pos_min_arr, pos_min_extract);\n\n    for(int i = 0; i < 8; ++i){\n        if(neg_max_arr[i] != 0){\n            result[0] = neg_max_arr[i];\n            break;\n        }\n    }\n    for(int i = 0; i < 8; ++i){\n        if(pos_min_arr[i] != 0){\n            result[1] = pos_min_arr[i];\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 8; // AVX size\n    std::vector<int> result(2, 0);\n    if(lst.size() < vec_size) return result;\n\n    __m256i neg_max = _mm256_set1_epi32(0);\n    __m256i pos_min = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i < lst.size(); i += vec_size){\n        __m256i input = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), input);\n        __m256i pos_mask = _mm256_cmpgt_epi32(input, _mm256_setzero_si256());\n\n        __m256i neg_values = _mm256_blendv_epi8(_mm256_setzero_si256(), input, neg_mask);\n        __m256i pos_values = _mm256_blendv_epi8(_mm256_setzero_si256(), input, pos_mask);\n\n        neg_max = _mm256_max_epi32(neg_max, neg_values);\n        pos_min = _mm256_min_epi32(pos_min, pos_values);\n    }\n\n    __m256i neg_max_extract = _mm256_permutevar8x32_epi32(neg_max, _mm256_setr_epi32(0,1,2,3,4,5,6,7));\n    __m256i pos_min_extract = _mm256_permutevar8x32_epi32(pos_min, _mm256_setr_epi32(0,1,2,3,4,5,6,7));\n\n    int neg_max_arr[8];\n    int pos_min_arr[8];\n    _mm256_storeu_si256((__m256i*)neg_max_arr, neg_max_extract);\n    _mm256_storeu_si256((__m256i*)pos_min_arr, pos_min_extract);\n\n    for(int i = 0; i < vec_size; ++i){\n        if(neg_max_arr[i] != 0){\n            result[0] = neg_max_arr[i];\n            break;\n        }\n    }\n    for(int i = 0; i < vec_size; ++i){\n        if(pos_min_arr[i] != 0){\n            result[1] = pos_min_arr[i];\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 8; // AVX size: 8 integers\n    std::vector<int> result(2, 0);\n    if (lst.size() < vec_size) return result;\n\n    __m256i neg_max = _mm256_set1_epi32(0);\n    __m256i pos_min = _mm256_set1_epi32(0);\n\n    for (size_t i = 0; i < lst.size(); i += vec_size) {\n        __m256i input = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), input);\n        __m256i pos_mask = _mm256_cmpgt_epi32(input, _mm256_setzero_si256());\n\n        __m256i neg_candidates = _mm256_blendv_epi8(_mm256_setzero_si256(), input, neg_mask);\n        __m256i pos_candidates = _mm256_blendv_epi8(_mm256_setzero_si256(), input, pos_mask);\n\n        neg_max = _mm256_max_epi32(neg_max, neg_candidates);\n        pos_min = _mm256_min_epi32(pos_min, pos_candidates);\n    }\n\n    __m256i neg_max_extract = _mm256_permutevar8x32_epi32(neg_max, _mm256_set_epi32(7,6,5,4,3,2,1,0));\n    __m256i pos_min_extract = _mm256_permutevar8x32_epi32(pos_min, _mm256_set_epi32(7,6,5,4,3,2,1,0));\n\n    int neg_max_arr[8], pos_min_arr[8];\n    _mm256_storeu_si256((__m256i*)neg_max_arr, neg_max_extract);\n    _mm256_storeu_si256((__m256i*)pos_min_arr, pos_min_extract);\n\n    for (int j = 0; j < vec_size; ++j) {\n        if (neg_max_arr[j] != 0) result[0] = neg_max_arr[j];\n        if (pos_min_arr[j] != 0) result[1] = pos_min_arr[j];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 8; // AVX size\n    std::vector<int> result(2, 0);\n    if(lst.size() < vec_size) return result;\n\n    __m256i neg_max = _mm256_set1_epi32(0);\n    __m256i pos_min = _mm256_set1_epi32(0);\n\n    for(size_t i = 0; i < lst.size(); i += vec_size){\n        __m256i input = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), input);\n        __m256i pos_mask = _mm256_cmpgt_epi32(input, _mm256_setzero_si256());\n\n        __m256i neg_values = _mm256_blendv_epi8(_mm256_setzero_si256(), input, neg_mask);\n        __m256i pos_values = _mm256_blendv_epi8(_mm256_setzero_si256(), input, pos_mask);\n\n        neg_max = _mm256_max_epi32(neg_max, neg_values);\n        pos_min = _mm256_min_epi32(pos_min, pos_values);\n    }\n\n    __m256i neg_max_extract = _mm256_permutevar8x32_epi32(neg_max, _mm256_setr_epi32(0,1,2,3,4,5,6,7));\n    __m256i pos_min_extract = _mm256_permutevar8x32_epi32(pos_min, _mm256_setr_epi32(0,1,2,3,4,5,6,7));\n\n    int neg_max_arr[8], pos_min_arr[8];\n    _mm256_storeu_si256((__m256i*)neg_max_arr, neg_max_extract);\n    _mm256_storeu_si256((__m256i*)pos_min_arr, pos_min_extract);\n\n    for(int i = 0; i < vec_size; ++i){\n        if(neg_max_arr[i] != 0){\n            result[0] = neg_max_arr[i];\n            break;\n        }\n    }\n    for(int i = 0; i < vec_size; ++i){\n        if(pos_min_arr[i] != 0){\n            result[1] = pos_min_arr[i];\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m256i result = _mm256_set1_epi64x(1);\n    for(uint64_t i = 1; i <= n; i++){\n        __m256i fact = _mm256_set1_epi64x(1);\n        for(uint64_t j = 1; j <= i; j++){\n            fact = _mm256_mul_epu32(fact, _mm256_set1_epi64x(j));\n        }\n        result = _mm256_mul_epu32(result, fact);\n    }\n    uint64_t res[4];\n    _mm256_storeu_si256((__m256i*)res, result);\n    uint64_t final_result = res[0] * res[1] * res[2] * res[3];\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m256i result = _mm256_set1_epi64x(1);\n    for(uint64_t i = 1; i <= n; i++){\n        __m256i fact = _mm256_set1_epi64x(1);\n        for(uint64_t j = 1; j <= i; j++){\n            fact = _mm256_mul_epu32(fact, _mm256_set1_epi64x(j));\n        }\n        result = _mm256_mul_epu32(result, fact);\n    }\n    uint64_t res[4];\n    _mm256_storeu_si256((__m256i*)res, result);\n    uint64_t final_result = res[0] * res[1] * res[2] * res[3];\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m256i result = _mm256_set1_epi64x(1);\n    for(uint64_t i = 1; i <= n; i++){\n        __m256i fact = _mm256_set1_epi64x(1);\n        for(uint64_t j = 1; j <= i; j++){\n            fact = _mm256_mul_epu32(fact, _mm256_set1_epi64x(j));\n        }\n        result = _mm256_mul_epu32(result, fact);\n    }\n    uint64_t result_vec[4];\n    _mm256_storeu_si256((__m256i*)result_vec, result);\n    uint64_t final_result = 1;\n    for(int k = 0; k < 4; k++){\n        final_result *= result_vec[k];\n    }\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m256i result = _mm256_set1_epi64x(1);\n    for(uint64_t i = 1; i <= n; i++){\n        __m256i fact = _mm256_set1_epi64x(1);\n        for(uint64_t j = 1; j <= i; j++){\n            fact = _mm256_mul_epu32(fact, _mm256_set1_epi64x(j));\n        }\n        result = _mm256_mul_epu32(result, fact);\n    }\n    uint64_t res[4];\n    _mm256_storeu_si256((__m256i*)res, result);\n    uint64_t final_result = res[0] * res[1] * res[2] * res[3];\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m256i result = _mm256_set1_epi64x(1);\n    for(uint64_t i = 1; i <= n; i++){\n        __m256i fact = _mm256_set1_epi64x(1);\n        for(uint64_t j = 1; j <= i; j++){\n            fact = _mm256_mul_epu32(fact, _mm256_set1_epi64x(j));\n        }\n        result = _mm256_mul_epu32(result, fact);\n    }\n    uint64_t res[4];\n    _mm256_storeu_si256((__m256i*)res, result);\n    uint64_t final_result = res[0] * res[1] * res[2] * res[3];\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    __m256i sum_vec = _mm256_setzero_si256();\n    \n    for(int i=0; i<n; i+=8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask3 = _mm256_setr_epi32(i%3==0?1:0, (i+1)%3==0?1:0, (i+2)%3==0?1:0, (i+3)%3==0?1:0, \n                                          (i+4)%3==0?1:0, (i+5)%3==0?1:0, (i+6)%3==0?1:0, (i+7)%3==0?1:0);\n        __m256i mask4 = _mm256_setr_epi32(i%4==0?1:0, (i+1)%4==0?1:0, (i+2)%4==0?1:0, (i+3)%4==0?1:0, \n                                          (i+4)%4==0?1:0, (i+5)%4==0?1:0, (i+6)%4==0?1:0, (i+7)%4==0?1:0);\n        \n        __m256i squared = _mm256_mullo_epi32(vec, vec);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(vec, vec), vec);\n        \n        __m256i result = _mm256_add_epi32(\n            _mm256_sub_epi32(_mm256_add_epi32(squared, cubed), \n                             _mm256_andnot_epi32(_mm256_or_epi32(mask3, mask4), squared)),\n            _mm256_and_epi32(_mm256_or_epi32(mask3, mask4), cubed)\n        );\n        \n        sum_vec = _mm256_add_epi32(sum_vec, result);\n    }\n    \n    __m256i sum_scalar = _mm256_hadd_epi32(sum_vec, sum_vec);\n    sum_scalar = _mm256_hadd_epi32(sum_scalar, sum_scalar);\n    return _mm256_extract_epi32(sum_scalar, 0);\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> &lst) {\n    const int vec_size = 8; // AVX size for int is 8\n    int sum = 0;\n    int i = 0;\n\n    __m256i mask_mult3 = _mm256_set_epi32(0, 3, 0, 3, 0, 3, 0, 3);\n    __m256i mask_mult4 = _mm256_set_epi32(0, 0, 4, 4, 0, 0, 4, 4);\n\n    while (i + vec_size <= lst.size()) {\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n\n        __m256i mult3_mask = _mm256_cmp_epi32_mask(indices, mask_mult3, _MM_CMPINT_EQ);\n        __m256i mult4_mask = _mm256_cmp_epi32_mask(indices, mask_mult4, _MM_CMPINT_EQ);\n\n        __m256i squared = _mm256_mullo_epi32(v, v);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(v, v), v);\n\n        __m256i result = _mm256_mask_blend_epi32(mult3_mask, v, squared);\n        result = _mm256_mask_blend_epi32(mult4_mask, result, cubed);\n\n        __m256i temp_sum = _mm256_hadd_epi32(result, result);\n        temp_sum = _mm256_hadd_epi32(temp_sum, temp_sum);\n        sum += _mm256_cvtsi256_si32(temp_sum);\n\n        i += vec_size;\n    }\n\n    for (; i < lst.size(); ++i) {\n        if (i % 3 == 0)\n            sum += lst[i] * lst[i];\n        else if (i % 4 == 0)\n            sum += lst[i] * lst[i] * lst[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    const int n = lst.size();\n    const int align = 8; // AVX: 8 integers per register\n    __m256i mask_mult3 = _mm256_set_epi32(0, 3, 0, 3, 0, 3, 0, 3);\n    __m256i mask_mult4 = _mm256_set_epi32(0, 0, 4, 4, 0, 0, 4, 4);\n\n    for(int i=0; i<n; i+=align){\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i indices = _mm256_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n\n        __m256i mult3_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_mult3), mask_mult3);\n        __m256i mult4_mask = _mm256_cmpeq_epi32(_mm256_and_si256(indices, mask_mult4), mask_mult4);\n\n        __m256i squared = _mm256_mullo_epi32(v, v);\n        __m256i cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(v, v), v);\n\n        __m256i result = _mm256_blendv_epi8(v, squared, mult3_mask);\n        result = _mm256_blendv_epi8(result, cubed, mult4_mask);\n\n        sum += _mm256_cvtsi256_si32(result);\n        sum += _mm256_extract_epi32(result, 1);\n        sum += _mm256_extract_epi32(result, 2);\n        sum += _mm256_extract_epi32(result, 3);\n        sum += _mm256_extract_epi32(result, 4);\n        sum += _mm256_extract_epi32(result, 5);\n        sum += _mm256_extract_epi32(result, 6);\n        sum += _mm256_extract_epi32(result, 7);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n    int i = 0;\n\n    while(i + 8 <= n){\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        \n        // Indices: 0, 1, 2, 3, 4, 5, 6, 7\n        // Check multiples of 3: 0, 3, 6\n        // Check multiples of 4: 0, 4\n        // Result: 0, x, x, 3, x, x, 6, x\n        \n        __m256i mask3 = _mm256_setr_epi32(1,0,0,1,0,0,1,0);\n        __m256i mask4 = _mm256_setr_epi32(1,0,0,0,1,0,0,0);\n        __m256i mask = _mm256_and_si256(mask3, mask4);\n        \n        __m256i v_squared = _mm256_mullo_epi32(v, v);\n        __m256i v_cubed = _mm256_mullo_epi32(_mm256_mullo_epi32(v, v), v);\n        \n        __m256i v_transformed = _mm256_blendv_epi8(v, v_squared, mask);\n        v_transformed = _mm256_blendv_epi8(v_transformed, v_cubed, _mm256_add_epi32(mask, mask));\n        \n        sum = _mm256_add_epi32(sum, v_transformed);\n        \n        i += 8;\n    }\n\n    int remaining_sum = 0;\n    for(; i < n; ++i){\n        if(i % 3 == 0 && i % 4 != 0){\n            lst[i] *= lst[i] * lst[i];\n        }\n        else if(i % 4 == 0){\n            lst[i] *= lst[i] * lst[i] * lst[i];\n        }\n        remaining_sum += lst[i];\n    }\n\n    __m256i s = _mm256_setr_epi32(remaining_sum, 0, 0, 0, 0, 0, 0, 0);\n    sum = _mm256_add_epi32(sum, s);\n\n    int result[8];\n    _mm256_storeu_si256((__m256i*)result, sum);\n    int total = 0;\n    for(int j = 0; j < 8; ++j){\n        total += result[j];\n    }\n    return total;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> &lst) {\n    const int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n    int i = 0;\n\n    while (i + 8 <= n) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        \n        // Indices 0, 3, 6, 9... (multiples of 3)\n        __m256i mask3 = _mm256_setr_epi32(3, 0, 3, 0, 3, 0, 3, 0);\n        __m256i indices3 = _mm256_add_epi32(_mm256_set1_epi32(i), mask3);\n        __m256i values3 = _mm256_i32gather_epi32((const int*)&lst[0], indices3, 4);\n        __m256i squared3 = _mm256_mullo_epi32(values3, values3);\n        sum = _mm256_add_epi32(sum, squared3);\n\n        // Indices 0, 4, 8, 12... (multiples of 4)\n        __m256i mask4 = _mm256_setr_epi32(4, 4, 4, 4, 4, 4, 4, 4);\n        __m256i indices4 = _mm256_add_epi32(_mm256_set1_epi32(i), mask4);\n        __m256i values4 = _mm256_i32gather_epi32((const int*)&lst[0], indices4, 4);\n        __m256i cubed4 = _mm256_mullo_epi32(_mm256_mullo_epi32(values4, values4), values4);\n        __m256i mask_not3 = _mm256_cmpgt_epi32(mask3, mask4); // Not multiples of 3\n        cubed4 = _mm256_andnot_si256(mask_not3, cubed4);\n        sum = _mm256_add_epi32(sum, cubed4);\n\n        i += 8;\n    }\n\n    // Handle remaining elements\n    int remaining_sum = 0;\n    for (; i < n; ++i) {\n        if (i % 3 == 0) {\n            remaining_sum += lst[i] * lst[i];\n        } else if (i % 4 == 0) {\n            remaining_sum += lst[i] * lst[i] * lst[i];\n        }\n    }\n\n    // Combine results\n    __m256i remaining_vec = _mm256_set1_epi32(remaining_sum);\n    __m256i low_sum = _mm256_extractf128_si256(sum, 0);\n    __m256i high_sum = _mm256_extractf128_si256(sum, 1);\n    low_sum = _mm256_add_epi32(low_sum, high_sum);\n    int final_sum = _mm256_extract_epi32(low_sum, 0) + _mm256_extract_epi32(low_sum, 1) +\n                    _mm256_extract_epi32(low_sum, 2) + _mm256_extract_epi32(low_sum, 3);\n    return final_sum + remaining_sum;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 4; // AVX: 4 integers per register\n    int remaining = nums.size() % vec_size;\n    __m256i threshold = _mm256_set1_epi32(10);\n    __m256i odd_digits_mask = _mm256_setr_epi32(0x8000000F, 0x8000000F, 0x8000000F, 0x8000000F);\n    \n    for(int i = 0; i < nums.size() - remaining; i += vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i gt_mask = _mm256_cmpgt_epi32(v, threshold);\n        __m256i first_digit_mask = _mm256_and_si256(v, odd_digits_mask);\n        __m256i last_digit_mask = _mm256_and_si256(_mm256_srli_epi32(v, 28), odd_digits_mask);\n        __m256i combined_mask = _mm256_and_si256(gt_mask, _mm256_and_si256(first_digit_mask, last_digit_mask));\n        count += _mm256_movemask_epi8(combined_mask);\n    }\n    \n    for(int i = nums.size() - remaining; i < nums.size(); ++i){\n        if(nums[i] > 10 && (abs(nums[i]) % 10) % 2 == 1 && ((nums[i] / 10) % 10) % 2 == 1){\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 8; // AVX2 supports 8 integers per register\n    int remaining = nums.size() % vec_size;\n    __m256i threshold = _mm256_set1_epi32(10);\n    __m256i odd_digits = _mm256_setr_epi32(1, 3, 5, 7, 9, 1, 3, 5);\n\n    for(int i = 0; i < nums.size() - remaining; i += vec_size){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i gt_ten = _mm256_cmpgt_epi32(vec, threshold);\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        \n        __m256i first_digit_mask = _mm256_and_si256(abs_vec, odd_digits);\n        __m256i first_digit_check = _mm256_cmpeq_epi32(first_digit_mask, odd_digits);\n        \n        __m256i last_digit_mask = _mm256_srli_epi32(abs_vec, 28);\n        __m256i last_digit_check = _mm256_cmpeq_epi32(last_digit_mask, odd_digits);\n        \n        __m256i combined_mask = _mm256_and_si256(_mm256_and_si256(gt_ten, first_digit_check), last_digit_check);\n        count += _mm256_popcnt_epi32(combined_mask);\n    }\n\n    // Handle remaining elements\n    for(int i = nums.size() - remaining; i < nums.size(); ++i){\n        if(nums[i] > 10 && (abs(nums[i]) % 10) % 2 == 1 && ((abs(nums[i]) / 10) % 10) % 2 == 1){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 8; // AVX2 supports 8 integers per register\n    int remaining = nums.size() % vec_size;\n    __m256i threshold = _mm256_set1_epi32(10);\n    __m256i odd_digits_mask = _mm256_setr_epi32(0b11111111, 0b11111111, 0b11111111, 0b11111111,\n                                                0b11111111, 0b11111111, 0b11111111, 0b11111111);\n\n    for(int i = 0; i < nums.size() - remaining; i += vec_size){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i gt_threshold = _mm256_cmpgt_epi32(vec, threshold);\n        __m256i first_digit_odd = _mm256_and_si256(_mm256_set1_epi32(0b10000000), vec);\n        __m256i last_digit_odd = _mm256_and_si256(vec, _mm256_set1_epi32(0b00000001));\n        __m256i first_last_odd = _mm256_and_si256(first_digit_odd, last_digit_odd);\n        __m256i mask = _mm256_and_si256(gt_threshold, first_last_odd);\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi8(mask, odd_digits_mask));\n    }\n\n    if(remaining > 0){\n        __m256i vec = _mm256_setzero_si256();\n        for(int i = 0; i < remaining; ++i){\n            _mm256_storeu_si256(&vec, _mm256_insert_epi32(vec, nums[nums.size() - remaining + i], i));\n        }\n        __m256i gt_threshold = _mm256_cmpgt_epi32(vec, threshold);\n        __m256i first_digit_odd = _mm256_and_si256(_mm256_set1_epi32(0b10000000), vec);\n        __m256i last_digit_odd = _mm256_and_si256(vec, _mm256_set1_epi32(0b00000001));\n        __m256i first_last_odd = _mm256_and_si256(first_digit_odd, last_digit_odd);\n        __m256i mask = _mm256_and_si256(gt_threshold, first_last_odd);\n        count += _mm256_movemask_epi8(_mm256_cmpeq_epi8(mask, odd_digits_mask)) >> (8 - remaining);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 8; // AVX2 supports 8 integers per register\n    int remaining = nums.size() % vec_size;\n    __m256i threshold = _mm256_set1_epi32(10);\n    __m256i odd_digits_mask = _mm256_setr_epi32(0b11111111, 0b11111111, 0b11111111, 0b11111111,\n                                                0b11111111, 0b11111111, 0b11111111, 0b11111111);\n\n    for(int i = 0; i < nums.size() - remaining; i += vec_size){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i gt_threshold = _mm256_cmpgt_epi32(vec, threshold);\n        __m256i first_digit_odd = _mm256_and_si256(_mm256_set1_epi32(0b10000000), vec);\n        __m256i last_digit_odd = _mm256_and_si256(vec, _mm256_set1_epi32(0b00000001));\n        __m256i first_last_odd = _mm256_and_si256(first_digit_odd, last_digit_odd);\n        __m256i all_conditions = _mm256_and_si256(gt_threshold, first_last_odd);\n        count += _mm256_movemask_epi8(all_conditions);\n    }\n\n    // Handle remaining elements\n    for(int i = nums.size() - remaining; i < nums.size(); ++i){\n        if(nums[i] > 10 && ((abs(nums[i]) % 10) & 1) && (((nums[i] / 10) % 10) & 1)){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 4; // AVX operates on 4 integers at once\n    int remaining = nums.size() % vec_size;\n    int *data = (int*)&nums[0];\n    \n    for(int i = 0; i < nums.size() - remaining; i += vec_size){\n        __m256i v = _mm256_loadu_si256((__m256i*)&data[i]);\n        \n        // Extract first digit\n        __m256i v_abs = _mm256_abs_epi32(v);\n        __m256i v_first_digit = _mm256_and_si256(v_abs, _mm256_set1_epi32(0xF0000000));\n        v_first_digit = _mm256_srli_epi32(v_first_digit, 28);\n        \n        // Extract last digit\n        __m256i v_last_digit = _mm256_and_si256(v_abs, _mm256_set1_epi32(0xF));\n        \n        // Check if both first and last digits are odd\n        __m256i v_first_odd = _mm256_cmpeq_epi32(v_first_digit, _mm256_set1_epi32(1));\n        __m256i v_last_odd = _mm256_cmpeq_epi32(v_last_digit, _mm256_set1_epi32(1));\n        __m256i v_both_odd = _mm256_and_si256(v_first_odd, v_last_odd);\n        \n        // Check if number is greater than 10\n        __m256i v_gt_10 = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(10));\n        \n        // Combine conditions\n        __m256i v_condition = _mm256_and_si256(v_both_odd, v_gt_10);\n        \n        // Count the number of true conditions\n        __m256i v_mask = _mm256_movemask_epi8(v_condition);\n        count += __builtin_popcount(v_mask);\n    }\n    \n    // Handle remaining elements\n    for(int i = nums.size() - remaining; i < nums.size(); ++i){\n        if(nums[i] > 10 && (abs(nums[i]) % 10) % 2 == 1 && ((abs(nums[i]) / 10) % 10) % 2 == 1){\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n    uint64_t count = 0;\n    __m256i ones = _mm256_set1_epi32(1);\n    __m256i twos = _mm256_set1_epi32(2);\n    __m256i threes = _mm256_set1_epi32(3);\n    __m256i mask = _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    \n    for(uint64_t i=0; i<n-2; i+=8){\n        __m256i idx = _mm256_add_epi32(mask, _mm256_set1_epi32(i));\n        __m256i val = _mm256_sub_epi32(_mm256_mullo_epi32(idx, idx), idx);\n        val = _mm256_add_epi32(val, ones);\n        \n        __m256i mod = _mm256_rem_epu32(val, threes);\n        __m256i eq_zero = _mm256_cmpeq_epi32(mod, _mm256_setzero_si256());\n        count += _mm256_popcnt_epi32(eq_zero);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    __m256i primes_mask = _mm256_set1_epi32(-1);\n    __m256i numbers = _mm256_setr_epi32(n, n+1, n+2, n+3, n+4, n+5, n+6, n+7);\n\n    // Check primality for 8 numbers in parallel\n    for(int i=2; i*i <= n+7; ++i){\n        __m256i factor = _mm256_set1_epi32(i);\n        __m256i divisible = _mm256_cmpeq_epi32(_mm256_mod_epi32(numbers, factor), _mm256_setzero_si256());\n        primes_mask = _mm256_andnot_si256(divisible, primes_mask);\n    }\n\n    // If any number is not prime, clear the mask\n    primes_mask = _mm256_andnot_si256(_mm256_cmpgt_epi32(primes_mask, _mm256_setzero_si256()), primes_mask);\n\n    // Horizontal OR to check if all numbers are prime\n    __m256i horizontal_or = _mm256_set_epi32(\n        _mm_extract_epi32(primes_mask, 0),\n        _mm_extract_epi32(primes_mask, 1),\n        _mm_extract_epi32(primes_mask, 2),\n        _mm_extract_epi32(primes_mask, 3),\n        _mm_extract_epi32(primes_mask, 4),\n        _mm_extract_epi32(primes_mask, 5),\n        _mm_extract_epi32(primes_mask, 6),\n        _mm_extract_epi32(primes_mask, 7)\n    );\n\n    int all_prime = _mm256_movemask_epi8(_mm256_cmpgt_epi32(horizontal_or, _mm256_setzero_si256()));\n\n    return __builtin_popcount(all_prime) == 8 ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m256i primes_mask = _mm256_set1_epi32(-1);\n    __m256i numbers = _mm256_setr_epi32(n, n, n, n, n, n, n, n);\n    __m256i divisors = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n\n    for(int i = 2; i <= 19; i++){\n        __m256i divisor = _mm256_set1_epi32(i);\n        __m256i remainder = _mm256_mod_epi32(numbers, divisor);\n        __m256i is_not_prime = _mm256_cmpeq_epi32(remainder, _mm256_setzero_si256());\n        primes_mask = _mm256_andnot_si256(is_not_prime, primes_mask);\n    }\n\n    int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(primes_mask));\n    return (mask == 0xFF) ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m256i primes = _mm256_set1_epi32(n);\n    __m256i values = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n    __m256i mask = _mm256_cmpeq_epi32(primes, values);\n    int mask_bits = _mm256_movemask_epi8(mask);\n\n    if (mask_bits != 0){\n        return x;\n    }\n    else{\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m256i primes_mask = _mm256_set1_epi32(-1);\n    __m256i numbers = _mm256_setr_epi32(n, n, n, n, n, n, n, n);\n    __m256i divisors[] = {\n        _mm256_setr_epi32(2, 3, 4, 5, 6, 7, 8, 9),\n        _mm256_setr_epi32(10, 11, 12, 13, 14, 15, 16, 17),\n        _mm256_setr_epi32(18, 19, 20, 21, 22, 23, 24, 25)\n    };\n\n    for(int i = 0; i < 3; ++i){\n        __m256i remainder = _mm256_div_epi32(numbers, divisors[i]);\n        __m256i is_not_prime = _mm256_cmpgt_epi32(divisors[i], remainder);\n        primes_mask = _mm256_and_si256(primes_mask, is_not_prime);\n    }\n\n    int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(primes_mask));\n    return (mask == 0) ? x : y;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m256i primes_mask = _mm256_set1_epi32(-1);\n    __m256i numbers = _mm256_setr_epi32(n, n, n, n, n, n, n, n);\n    __m256i divisors = _mm256_setr_epi32(2, 3, 5, 7, 11, 13, 17, 19);\n\n    for(int i = 2; i <= 19; i++){\n        __m256i divisor = _mm256_set1_epi32(i);\n        __m256i remainder = _mm256_mod_epi32(numbers, divisor);\n        primes_mask = _mm256_and_si256(primes_mask, _mm256_cmpeq_epi32(remainder, _mm256_setzero_si256()));\n    }\n\n    int mask = _mm256_movemask_epi8(_mm256_castsi256_si128(primes_mask));\n    mask |= _mm256_movemask_epi8(_mm256_extracti128_si256(primes_mask));\n\n    return (mask == 0xFF) ? x : y;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if (lst.empty()) return 0;\n\n    uint64_t sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n\n    // Process in chunks of 8 floats using AVX\n    for (; i <= n - 8; i += 8) {\n        __m256 vec = _mm256_loadu_ps(&lst[i]);\n        \n        // Check for non-negative integers\n        __m256 mask_int = _mm256_cvtepi32_ps(_mm256_cvttps_epi32(vec));\n        __m256 mask_neg = _mm256_cmp_ps(vec, _mm256_setzero_ps(), _CMP_LT_OQ);\n        __m256 mask_odd = _mm256_andnot_ps(mask_neg, _mm256_cmpeq_ps(_mm256_sub_ps(vec, mask_int), _mm256_setzero_ps()));\n        __m256 mask_odd_final = _mm256_and_ps(mask_odd, _mm256_cmp_ps(vec, _mm256_set1_ps(0.5f), _CMP_GE_OS));\n\n        // Compute squares of odd numbers\n        __m256 squared = _mm256_mul_ps(vec, vec);\n\n        // Accumulate results\n        sum += _mm256_cvtsi128_si32(_mm256_castps256_ps128(_mm256_and_ps(squared, _mm256_castsi128_ps(mask_odd_final))));\n        sum += _mm256_cvtsi128_si32(_mm256_extractf128_ps(_mm256_and_ps(squared, _mm256_castsi128_ps(mask_odd_final)), 1));\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        float val = lst[i];\n        if (val >= 0 && std::floor(val) == val && static_cast<int>(val) % 2 != 0) {\n            sum += static_cast<uint64_t>(val * val);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 4; // AVX size: 4 floats per register\n    uint64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    __m256 mask_odd = _mm256_set_ps(0, 1, 0, 1, 0, 1, 0, 1); // Mask for checking oddness\n    __m256 mask_positive = _mm256_setzero_ps(); // Initialize positive mask\n\n    for (int i = 0; i < n; i += vec_size) {\n        __m256 v_vec = _mm256_loadu_ps(&lst[i]);\n        __m256 v_int = _mm256_roundeven_ps(v_vec); // Round to nearest integer\n        __m256 v_odd_mask = _mm256_and_ps(_mm256_cmp_ps(v_int, _mm256_floor_ps(v_int), _CMP_EQ_OQ), mask_odd);\n        __m256 v_positive_mask = _mm256_cmp_ps(v_vec, _mm256_setzero_ps(), _CMP_GT_OS);\n\n        __m256 v_filtered = _mm256_mul_ps(v_odd_mask, _mm256_mul_ps(v_positive_mask, v_vec));\n        __m256 v_squared = _mm256_mul_ps(v_filtered, v_filtered);\n\n        float result[vec_size];\n        _mm256_storeu_ps(result, v_squared);\n        for (int j = 0; j < vec_size && (i + j) < n; ++j) {\n            sum += static_cast<uint64_t>(result[j] + 0.5f); // Add 0.5 for proper rounding\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 8; // AVX: 8 single-precision floats\n    uint64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    int i = 0;\n    __m256 v_mask = _mm256_set1_ps(1.0f); // Mask for integer check\n    __m256 v_zero = _mm256_setzero_ps();\n    __m256 v_odd_check = _mm256_set1_ps(2.0f);\n    __m256 v_one = _mm256_set1_ps(1.0f);\n\n    while (i + vec_size <= n) {\n        __m256 v_data = _mm256_loadu_ps(&lst[i]);\n        __m256 v_int_check = _mm256_cvttps_epi32(v_data); // Convert to int\n        __m256 v_reconstruction = _mm256_cvtepi32_ps(v_int_check); // Reconstruct float\n        __m256 v_diff = _mm256_sub_ps(v_data, v_reconstruction);\n        __m256 v_is_int = _mm256_cmpeq_ps(v_diff, v_zero); // Check if integer\n\n        __m256 v_is_positive = _mm256_cmpgt_ps(v_data, v_zero); // Check if positive\n        __m256 v_is_odd = _mm256_and_ps(_mm256_mul_ps(v_data, v_odd_check), v_mask); // Check if odd\n\n        __m256 v_filtered = _mm256_and_ps(_mm256_and_ps(v_is_int, v_is_positive), v_is_odd);\n        __m256 v_squares = _mm256_mul_ps(v_filtered, v_filtered);\n\n        sum += _mm256_cvtsi256_si64(_mm256_castps256_pd(v_squares));\n        sum += _mm256_extract_epi32(_mm256_castps256_si256(v_squares), 4);\n\n        i += vec_size;\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (static_cast<int>(lst[i]) == lst[i] && lst[i] > 0 && static_cast<int>(lst[i]) % 2 != 0) {\n            sum += static_cast<uint64_t>(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 8; // AVX: 8 single-precision floats\n    uint64_t sum = 0;\n    int i = 0;\n\n    // Handle the case where the list is empty\n    if (lst.empty()) return 0;\n\n    // Align to the nearest multiple of vec_size\n    int n = lst.size();\n    int remaining = n % vec_size;\n    int main_length = n - remaining;\n\n    // Process full vectors using AVX\n    __m256 mask_odd = _mm256_setr_ps(1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f);\n    __m256 mask_negative = _mm256_setr_ps(-0.0f, -0.0f, -0.0f, -0.0f, -0.0f, -0.0f, -0.0f, -0.0f);\n\n    while (i < main_length) {\n        __m256 v = _mm256_loadu_ps(&lst[i]);\n        __m256 v_odd = _mm256_and_ps(v, mask_odd); // Extract fractional part\n        __m256 v_neg = _mm256_cmp_ps(v, mask_negative, _CMP_LT_OS); // Check for negative\n\n        // Keep only positive integers\n        __m256 v_keep = _mm256_andnot_ps(_mm256_or_ps(v_odd, v_neg), v);\n\n        // Compute squares\n        __m256 v_sq = _mm256_mul_ps(v_keep, v_keep);\n\n        // Horizontal sum\n        __m256i v_sum = _mm256_cvtps_epi32(v_sq);\n        sum += _mm256_cvtsi256_si32(v_sum) + _mm256_extract_epi32(v_sum, 1) +\n               _mm256_extract_epi32(v_sum, 2) + _mm256_extract_epi32(v_sum, 3);\n\n        i += vec_size;\n    }\n\n    // Handle remaining elements\n    __m256 last_vec = _mm256_setzero_ps();\n    for (; i < n; ++i) {\n        last_vec[i % vec_size] = lst[i];\n    }\n    __m256 last_v_odd = _mm256_and_ps(last_vec, mask_odd);\n    __m256 last_v_neg = _mm256_cmp_ps(last_vec, mask_negative, _CMP_LT_OS);\n    __m256 last_v_keep = _mm256_andnot_ps(_mm256_or_ps(last_v_odd, last_v_neg), last_vec);\n    __m256 last_v_sq = _mm256_mul_ps(last_v_keep, last_v_keep);\n\n    __m256i last_v_sum = _mm256_cvtps_epi32(last_v_sq);\n    sum += _mm256_cvtsi256_si32(last_v_sum) + _mm256_extract_epi32(last_v_sum, 1) +\n           _mm256_extract_epi32(last_v_sum, 2) + _mm256_extract_epi32(last_v_sum, 3);\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <cstdint>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst) {\n    const int vec_size = 8; // AVX: 8 single-precision floats\n    uint64_t sum = 0;\n    int n = lst.size();\n\n    if (n == 0) return 0;\n\n    __m256 mask_odd = _mm256_setr_ps(1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f);\n    __m256 mask_negative = _mm256_setr_ps(-0.0f, -0.0f, -0.0f, -0.0f, -0.0f, -0.0f, -0.0f, -0.0f);\n\n    for (int i = 0; i < n; i += vec_size) {\n        __m256 v = _mm256_loadu_ps(&lst[i]);\n        __m256 v_odd = _mm256_andnot_ps(mask_negative, _mm256_round_ps(v, _MM_FROUND_TO_NEAREST_INT));\n        __m256 v_int = _mm256_cvtepi32_ps(_mm256_cvttps_epi32(v_odd));\n        __m256 v_check = _mm256_cmpeq_ps(v_odd, v_int);\n        __m256 v_square = _mm256_mul_ps(v_int, v_int);\n        __m256 v_mask = _mm256_and_ps(v_check, mask_odd);\n        __m256 v_result = _mm256_mul_ps(v_square, v_mask);\n        sum += _mm256_cvtsi128_si32(_mm256_castps256_ps128(v_result));\n        if (vec_size * 2 <= n - i) {\n            sum += _mm256_cvtsi128_si32(_mm256_extractf128_si256(v_result, 1));\n        } else {\n            sum += _mm256_movemask_ps(_mm256_castps256_ps128(v_result)) & ((1 << (n - i)) - 1);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    size_t n = game.size();\n    std::vector<int> result(n, 0);\n    \n    if(n < 8){\n        for(size_t i = 0; i < n; ++i){\n            int diff = game[i] - guess[i];\n            result[i] = (diff == 0) ? 0 : abs(diff);\n        }\n        return result;\n    }\n\n    size_t i = 0;\n    for(; i <= n - 8; i += 8){\n        __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[i]));\n        __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[i]));\n\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n        __m256i zero_vec = _mm256_setzero_si256();\n        __m256i mask = _mm256_cmpeq_epi32(diff_vec, zero_vec);\n\n        __m256i final_vec = _mm256_blendv_epi8(zero_vec, abs_diff_vec, mask);\n\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), final_vec);\n    }\n\n    for(; i < n; ++i){\n        int diff = game[i] - guess[i];\n        result[i] = (diff == 0) ? 0 : abs(diff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    \n    if(n < 8){\n        for(int i = 0; i < n; ++i){\n            int diff = game[i] - guess[i];\n            result[i] = (diff == 0) ? 0 : abs(diff);\n        }\n        return result;\n    }\n\n    int num_blocks = n / 8;\n    __m256i zero = _mm256_setzero_si256();\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m256i g_game = _mm256_loadu_si256((__m256i*)&game[i*8]);\n        __m256i g_guess = _mm256_loadu_si256((__m256i*)&guess[i*8]);\n\n        __m256i diff = _mm256_sub_epi32(g_game, g_guess);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        __m256i mask = _mm256_cmpeq_epi32(diff, zero);\n        __m256i final_result = _mm256_blendv_epi8(zero, abs_diff, mask);\n\n        _mm256_storeu_si256((__m256i*)&result[i*8], final_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    \n    if(n < 8){\n        for(int i = 0; i < n; ++i){\n            int diff = game[i] - guess[i];\n            result[i] = (diff == 0) ? 0 : abs(diff);\n        }\n        return result;\n    }\n\n    int num_blocks = n / 8;\n    __m256i zero = _mm256_setzero_si256();\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m256i g_game = _mm256_loadu_si256((__m256i*)&game[i*8]);\n        __m256i g_guess = _mm256_loadu_si256((__m256i*)&guess[i*8]);\n\n        __m256i diff = _mm256_sub_epi32(g_game, g_guess);\n        __m256i abs_diff = _mm256_abs_epi32(diff);\n\n        __m256i mask = _mm256_cmpeq_epi32(diff, zero);\n        __m256i final_result = _mm256_blendv_epi8(zero, abs_diff, mask);\n\n        _mm256_storeu_si256((__m256i*)&result[i*8], final_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    int i = 0;\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256i v_game = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i v_guess = _mm256_loadu_si256((__m256i*)&guess[i]);\n        __m256i v_diff = _mm256_sub_epi32(v_game, v_guess);\n        __m256i v_abs_diff = _mm256_abs_epi32(v_diff);\n        __m256i v_zero = _mm256_setzero_si256();\n        __m256i v_mask = _mm256_cmpeq_epi32(v_diff, v_zero);\n        __m256i v_final = _mm256_blendv_epi8(v_zero, v_abs_diff, v_mask);\n        _mm256_storeu_si256((__m256i*)&result[i], v_final);\n    }\n    \n    for (; i < n; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    int i = 0;\n    \n    for (; i + 8 <= n; i += 8) {\n        __m256i v_game = _mm256_loadu_si256((__m256i*)&game[i]);\n        __m256i v_guess = _mm256_loadu_si256((__m256i*)&guess[i]);\n        __m256i v_diff = _mm256_sub_epi32(v_game, v_guess);\n        __m256i v_abs_diff = _mm256_abs_epi32(v_diff);\n        __m256i v_zero = _mm256_setzero_si256();\n        __m256i v_mask = _mm256_cmpeq_epi32(v_diff, v_zero);\n        __m256i v_final = _mm256_blendv_epi8(v_zero, v_abs_diff, v_mask);\n        _mm256_storeu_si256((__m256i*)&result[i], v_final);\n    }\n    \n    for (; i < n; ++i) {\n        result[i] = abs(game[i] - guess[i]);\n    }\n    \n    return result;\n}\n```"}

{"task_id": "SimdBench_0_SSE", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    while (remaining >= 2) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)(mask + i));\n        __m128i mask_high = _mm_unpackhi_epi64(mask_vec, mask_vec);\n        __m128i mask_low = mask_vec;\n        __m128i src_high = _mm_unpackhi_epi64(src_vec, src_vec);\n\n        __m128i dst_low = _mm_and_si128(src_low, mask_low);\n        __m128i dst_high = _mm_and_si128(src_high, mask_high);\n\n        _mm_storeu_si128((__m128i*)(dst + i), _mm_or_si128(dst_low, dst_high));\n\n        i += 2;\n        remaining -= 2;\n    }\n\n    while (remaining--) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_load_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_vec_not = _mm_xor_si128(mask_vec, _mm_set1_epi32(-1));\n        __m128i dst_vec = _mm_load_si128((__m128i*)&dst[i]);\n        __m128i result_vec = _mm_or_si128(_mm_and_si128(src_vec, mask_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_store_si128((__m128i*)&dst[i], result_vec);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_load_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_vec_float = _mm_castps_si128(_mm_castsi128_ps(mask_vec));\n        __m128i dst_vec = _mm_andnot_ps(_mm_castsi128_ps(mask_vec), _mm_castsi128_ps(_mm_load_si128((__m128i*)&dst[i])));\n        __m128i result = _mm_or_ps(_mm_castsi128_ps(dst_vec), _mm_castsi128_ps(src_vec)).m128_i32[0];\n        _mm_store_si128((__m128i*)&dst[i], result);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_true = _mm_set1_epi32(-1); // All bits set\n\n    for (; i <= length - 2; i += 2) {\n        __m128i m0 = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i s0 = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i d0 = _mm_loadu_si128((__m128i*)&dst[i]);\n\n        __m128i mask0 = _mm_cmpeq_epi32(m0, mask_true);\n        __m128i result0 = _mm_or_si128(_mm_and_si128(mask0, s0), _mm_andnot_si128(mask0, d0));\n        \n        _mm_storeu_si128((__m128i*)&dst[i], result0);\n    }\n\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_SSE", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_load_si128((__m128i*)&src[i]);\n        __m128i mask_vec = _mm_loadu_si128((__m128i*)&mask[i]);\n        __m128i mask_vec_not = _mm_xor_si128(mask_vec, _mm_set1_epi32(-1));\n        __m128i dst_vec = _mm_load_si128((__m128i*)&dst[i]);\n        __m128i dst_vec_updated = _mm_or_si128(_mm_and_si128(src_vec, mask_vec), _mm_andnot_si128(mask_vec, dst_vec));\n        _mm_store_si128((__m128i*)&dst[i], dst_vec_updated);\n    }\n    for (; i < length; i++) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    \n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Step 1: Calculate tmp1 as (element * scale) right-shifted by 3 bits\n        __m128i tmp1 = _mm_srai_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 2: Calculate tmp2 as (element * scale) left-shifted by 3 bits\n        __m128i tmp2 = _mm_slli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 3: Compute the final result as: (element * scale + tmp1 * tmp2) / 7\n        __m128i mul1 = _mm_mul_epi32(src_vec, scale_vec);\n        __m128i mul2 = _mm_mul_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(mul1, mul2);\n        __m128i div7 = _mm_div_epi32(sum, _mm_set1_epi32(7));\n        \n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], div7);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    \n    // Ensure length is a multiple of 4 for SIMD processing\n    size_t remaining = length % 4;\n    size_t limit = length - remaining;\n\n    for(i = 0; i < limit; i += 4){\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Step 1: Calculate tmp1 as (element * scale) right-shifted by 3 bits\n        __m128i mul1 = _mm_mullo_epi32(src_vec, scale_vec);\n        __m128i tmp1 = _mm_srai_epi32(mul1, 3);\n        \n        // Step 2: Calculate tmp2 as (element * scale) left-shifted by 3 bits\n        __m128i tmp2 = _mm_slli_epi32(mul1, 3);\n        \n        // Step 3: Compute the final result as: (element * scale + tmp1 * tmp2) / 7\n        __m128i mul2 = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i add = _mm_add_epi32(mul1, mul2);\n        __m128i div = _mm_div_epi32(add, _mm_set1_epi32(7));\n        \n        // Step 4: Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], div);\n    }\n    \n    // Handle remaining elements that are not a multiple of 4\n    for(; i < length; i++){\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    \n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Step 1: Calculate tmp1 as (element * scale) right-shifted by 3 bits\n        __m128i tmp1_vec = _mm_srai_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 2: Calculate tmp2 as (element * scale) left-shifted by 3 bits\n        __m128i tmp2_vec = _mm_slli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 3: Compute the final result as: (element * scale + tmp1 * tmp2) / 7\n        __m128i mul1_vec = _mm_mul_epi32(src_vec, scale_vec);\n        __m128i mul2_vec = _mm_mul_epi32(tmp1_vec, tmp2_vec);\n        __m128i sum_vec = _mm_add_epi32(mul1_vec, mul2_vec);\n        __m128i div_vec = _mm_div_epi32(sum_vec, _mm_set1_epi32(7));\n        \n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], div_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    \n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Step 1: Calculate tmp1 as (element * scale) right-shifted by 3 bits\n        __m128i tmp1 = _mm_srai_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 2: Calculate tmp2 as (element * scale) left-shifted by 3 bits\n        __m128i tmp2 = _mm_slli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        \n        // Step 3: Compute the final result as: (element * scale + tmp1 * tmp2) / 7\n        __m128i mul1 = _mm_mul_epi32(src_vec, scale_vec);\n        __m128i mul2 = _mm_mul_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(mul1, mul2);\n        __m128i div7 = _mm_div_epi32(sum, _mm_set1_epi32(7));\n        \n        // Store the result in dst\n        _mm_storeu_si128((__m128i*)&dst[i], div7);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = (element * scale + tmp1 * tmp2) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_1_SSE", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    size_t i = 0;\n    __m128i scale_vec = _mm_set1_epi32(scale);\n    \n    // Ensure length is a multiple of 4 for SIMD processing\n    size_t remaining = length % 4;\n    size_t limit = length - remaining;\n\n    for(i = 0; i < limit; i += 4){\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i scaled = _mm_mullo_epi32(src_vec, scale_vec);\n\n        __m128i tmp1 = _mm_srai_epi32(scaled, 3);\n        __m128i tmp2 = _mm_slli_epi32(scaled, 3);\n        __m128i mul = _mm_mullo_epi32(tmp1, tmp2);\n        __m128i sum = _mm_add_epi32(scaled, mul);\n        __m128i div = _mm_div_epi32(sum, _mm_set1_epi32(7));\n\n        _mm_storeu_si128((__m128i*)&dst[i], div);\n    }\n\n    // Handle remaining elements\n    for(; i < length; i++){\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        dst[i] = ((element * scale) + (tmp1 * tmp2)) / 7;\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    __m128d v_src, v_dst;\n\n    for(r = 0; r < rows; r += 2){\n        // Load two consecutive elements from src\n        v_src = _mm_load_sd(&src[r * cols + stride]);\n        v_src = _mm_insert_pd(v_src, src[(r+1) * cols + stride], 1);\n\n        // Store two consecutive elements into dst\n        _mm_store_sd(&dst[r], v_src);\n        _mm_store_sd(&dst[r+1], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    __m128d v_src, v_dst;\n\n    for(r = 0; r < rows; r += 2){\n        // Load two consecutive elements from src\n        v_src = _mm_load_sd(&src[r * cols + stride]);\n        v_src = _mm_insert_pd(v_src, src[(r+1) * cols + stride], 1);\n\n        // Store two consecutive elements into dst\n        _mm_store_sd(&dst[r], v_src);\n        _mm_store_sd(&dst[r+1], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    __m128d v_src, v_dst;\n\n    for(r = 0; r < rows; r += 2){\n        // Load two consecutive elements from src\n        v_src = _mm_load_sd(&src[r * cols + stride]);\n        v_src = _mm_insert_pd(v_src, src[(r+1) * cols + stride], 1);\n\n        // Store two consecutive elements into dst\n        _mm_store_sd(&dst[r], v_src);\n        _mm_store_sd(&dst[r+1], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    __m128d v_src, v_dst;\n\n    for(r = 0; r < rows; r += 2){\n        // Load two consecutive elements from src\n        v_src = _mm_load_sd(&src[r * cols + stride]);\n        v_src = _mm_insert_pd(v_src, src[(r+1) * cols + stride], 1);\n\n        // Store two consecutive elements into dst\n        _mm_store_sd(&dst[r], v_src);\n        _mm_store_sd(&dst[r+1], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_2_SSE", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    size_t r;\n    __m128d v_src, v_dst;\n\n    for(r = 0; r < rows; r += 2){\n        // Load two consecutive elements from src\n        v_src = _mm_load_sd(&src[r * cols + stride]);\n        v_src = _mm_insert_pd(v_src, src[(r+1) * cols + stride], 1);\n\n        // Store two consecutive elements into dst\n        _mm_store_sd(&dst[r], v_src);\n        _mm_store_sd(&dst[r+1], v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m128 xmm0, xmm1, xmm2, xmm3;\n    __m128i idx_xmm0, idx_xmm1;\n\n    // Process in chunks of 4 elements\n    for (; i <= length - 4; i += 4) {\n        // Gather operation\n        idx_xmm0 = _mm_loadu_si128((__m128i*)(indices + i));\n        xmm0 = _mm_setzero_ps();\n        xmm1 = _mm_setzero_ps();\n        xmm0 = _mm_i32gather_ps(src, idx_xmm0, 4);\n        \n        idx_xmm1 = _mm_add_epi32(idx_xmm0, _mm_set1_epi32(1));\n        xmm1 = _mm_i32gather_ps(src, idx_xmm1, 4);\n\n        // Scatter operation\n        _mm_storeu_ps(dst + i, xmm0);\n        _mm_storeu_ps(dst + idx_xmm0.m128i_i32[0], xmm0.m128_f32[0]);\n        _mm_storeu_ps(dst + idx_xmm0.m128i_i32[1], xmm0.m128_f32[1]);\n        _mm_storeu_ps(dst + idx_xmm0.m128i_i32[2], xmm0.m128_f32[2]);\n        _mm_storeu_ps(dst + idx_xmm0.m128i_i32[3], xmm0.m128_f32[3]);\n\n        _mm_storeu_ps(dst + idx_xmm1.m128i_i32[0], xmm1.m128_f32[0]);\n        _mm_storeu_ps(dst + idx_xmm1.m128i_i32[1], xmm1.m128_f32[1]);\n        _mm_storeu_ps(dst + idx_xmm1.m128i_i32[2], xmm1.m128_f32[2]);\n        _mm_storeu_ps(dst + idx_xmm1.m128i_i32[3], xmm1.m128_f32[3]);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src[indices[i]];\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m128 xmm0, xmm1, xmm2, xmm3;\n    __m128i idx_xmm0, idx_xmm1;\n\n    // Process in chunks of 4 elements\n    for (; i <= length - 4; i += 4) {\n        // Gather operation\n        idx_xmm0 = _mm_set_epi32(indices[i + 3], indices[i + 2], indices[i + 1], indices[i]);\n        xmm0 = _mm_set_ps(src[indices[i + 3]], src[indices[i + 2]], src[indices[i + 1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[indices[i + 3]], xmm0);\n        _mm_storeu_ps(&dst[indices[i + 2]], xmm0);\n        _mm_storeu_ps(&dst[indices[i + 1]], xmm0);\n        _mm_storeu_ps(&dst[indices[i]], xmm0);\n\n        // Scatter operation\n        xmm1 = _mm_loadu_ps(&src[indices[i + 3]]);\n        xmm2 = _mm_loadu_ps(&src[indices[i + 2]]);\n        xmm3 = _mm_loadu_ps(&src[indices[i + 1]]);\n        _mm_storeu_ps(&dst[indices[i + 3]], xmm1);\n        _mm_storeu_ps(&dst[indices[i + 2]], xmm2);\n        _mm_storeu_ps(&dst[indices[i + 1]], xmm3);\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; i++) {\n        dst[indices[i]] = src[indices[i]];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m128 xmm0, xmm1, xmm2, xmm3;\n    __m128i idx_xmm0, idx_xmm1;\n\n    // Process in chunks of 4 elements\n    for (; i <= length - 4; i += 4) {\n        // Gather operation\n        idx_xmm0 = _mm_set_epi32(indices[i + 3], indices[i + 2], indices[i + 1], indices[i]);\n        xmm0 = _mm_set_ps(src[indices[i + 3]], src[indices[i + 2]], src[indices[i + 1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[i + 0], xmm0);\n\n        // Scatter operation\n        xmm1 = _mm_loadu_ps(&src[i + 0]);\n        _mm_storeu_ps(&dst[idx_xmm0.m128i_i32[0]], _mm_shuffle_ps(xmm1, xmm1, _MM_SHUFFLE(0, 0, 0, 0)));\n        _mm_storeu_ps(&dst[idx_xmm0.m128i_i32[1]], _mm_shuffle_ps(xmm1, xmm1, _MM_SHUFFLE(1, 1, 1, 1)));\n        _mm_storeu_ps(&dst[idx_xmm0.m128i_i32[2]], _mm_shuffle_ps(xmm1, xmm1, _MM_SHUFFLE(2, 2, 2, 2)));\n        _mm_storeu_ps(&dst[idx_xmm0.m128i_i32[3]], _mm_shuffle_ps(xmm1, xmm1, _MM_SHUFFLE(3, 3, 3, 3)));\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m128 xmm0, xmm1, xmm2, xmm3;\n    __m128i idx_xmm0, idx_xmm1;\n\n    // Process in chunks of 4 elements\n    for (; i <= length - 4; i += 4) {\n        // Gather operation\n        idx_xmm0 = _mm_set_epi32(indices[i + 3], indices[i + 2], indices[i + 1], indices[i]);\n        xmm0 = _mm_set_ps(src[indices[i + 3]], src[indices[i + 2]], src[indices[i + 1]], src[indices[i]]);\n        _mm_storeu_ps(&dst[i + 0], xmm0);\n\n        // Scatter operation\n        xmm1 = _mm_loadu_ps(&src[i + 0]);\n        idx_xmm1 = _mm_castps_si128(xmm1);\n        _mm_storeu_ps(&dst[idx_xmm1.m128i_i32[0]], xmm1);\n        _mm_storeu_ps(&dst[idx_xmm1.m128i_i32[1]], xmm1);\n        _mm_storeu_ps(&dst[idx_xmm1.m128i_i32[2]], xmm1);\n        _mm_storeu_ps(&dst[idx_xmm1.m128i_i32[3]], xmm1);\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_SSE", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    size_t i = 0;\n    __m128 xmm0, xmm1, xmm2, xmm3;\n    __m128i idx_xmm0, idx_xmm1;\n\n    // Process in chunks of 4 elements\n    for (; i <= length - 4; i += 4) {\n        // Gather operation\n        idx_xmm0 = _mm_loadu_si128((__m128i*)&indices[i]);\n        xmm0 = _mm_setzero_ps();\n        xmm1 = _mm_setzero_ps();\n        xmm0 = _mm_i32gather_ps(&src[0], idx_xmm0, 4);\n        xmm1 = _mm_i32gather_ps(&src[0], _mm_add_epi32(idx_xmm0, _mm_set1_epi32(1)), 4);\n        xmm2 = _mm_i32gather_ps(&src[0], _mm_add_epi32(idx_xmm0, _mm_set1_epi32(2)), 4);\n        xmm3 = _mm_i32gather_ps(&src[0], _mm_add_epi32(idx_xmm0, _mm_set1_epi32(3)), 4);\n\n        // Scatter operation\n        _mm_storeu_ps(&dst[i], xmm0);\n        _mm_storeu_ps(&dst[i + 4], xmm1);\n        _mm_storeu_ps(&dst[i + 8], xmm2);\n        _mm_storeu_ps(&dst[i + 12], xmm3);\n\n        // Write back using scatter\n        _mm_stream_ps(&dst[idx_xmm0.m128i_i32[0]], _mm_loadu_ps(&src[i]));\n        _mm_stream_ps(&dst[idx_xmm0.m128i_i32[1]], _mm_loadu_ps(&src[i + 4]));\n        _mm_stream_ps(&dst[idx_xmm0.m128i_i32[2]], _mm_loadu_ps(&src[i + 8]));\n        _mm_stream_ps(&dst[idx_xmm0.m128i_i32[3]], _mm_loadu_ps(&src[i + 12]));\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; i++) {\n        dst[indices[i]] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    const __m128i zero = _mm_setzero_si128();\n    const uint64_t len = end - start + 1;\n    const uint64_t num_full_blocks = len / 8;\n    const uint64_t remainder = len % 8;\n\n    // Reverse full blocks of 8 elements\n    for (uint64_t i = 0; i < num_full_blocks; ++i) {\n        __m128i block = _mm_loadu_si128((__m128i*)&arr[start + i*8]);\n        __m128i reversed_block = _mm_or_si128(_mm_slli_epi16(block, 8), _mm_srli_epi16(block, 8));\n        _mm_storeu_si128((__m128i*)&arr[start + end - i*8], reversed_block);\n    }\n\n    // Handle remaining elements\n    for (uint64_t i = 0; i < remainder; ++i) {\n        arr[start + i] ^= arr[end - i];\n        arr[end - i] ^= arr[start + i];\n        arr[start + i] ^= arr[end - i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    const __m128i zero = _mm_setzero_si128();\n    const uint64_t length = end - start + 1;\n    \n    // Ensure start is even for alignment\n    if (start % 2 != 0) {\n        arr[start] = -arr[start];\n        start++;\n        length--;\n    }\n    \n    // Ensure length is divisible by 4 for 4-element processing\n    if (length % 4 != 0) {\n        int16_t tail[4] __attribute__((aligned(16)));\n        uint64_t tail_len = length % 4;\n        for (uint64_t i = 0; i < tail_len; i++) {\n            tail[i] = arr[end - i];\n        }\n        length -= tail_len;\n        end -= tail_len;\n    }\n    \n    for (uint64_t i = start; i <= end - length; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i reversed = _mm_shuffle_epi8(vec, _mm_setr_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0));\n        _mm_storeu_si128((__m128i*)&arr[i], reversed);\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    const __m128i zero = _mm_setzero_si128();\n    const uint64_t len = end - start + 1;\n    const uint64_t num_full_blocks = len / 8;\n    const uint64_t remainder = len % 8;\n\n    // Reverse full blocks of 8 elements\n    for (uint64_t i = 0; i < num_full_blocks; ++i) {\n        __m128i block = _mm_loadu_si128((__m128i*)&arr[start + i*8]);\n        __m128i reversed_block = _mm_or_si128(_mm_slli_epi16(block, 8), _mm_srli_epi16(block, 8));\n        _mm_storeu_si128((__m128i*)&arr[start + i*8], reversed_block);\n    }\n\n    // Handle remaining elements\n    for (uint64_t i = num_full_blocks * 8; i < len; ++i) {\n        arr[start + i] = arr[end - i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    const __m128i zero = _mm_setzero_si128();\n    uint64_t i = start;\n    \n    // Align start index\n    for (; i + 7 <= start; i += 8) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        _mm_storeu_si128((__m128i*)&arr[i], _mm_shuffle_epi8(vec, _mm_setr_epi8(\n            7, 6, 5, 4, 3, 2, 1, 0, 7, 6, 5, 4, 3, 2, 1, 0)));\n    }\n    \n    // Handle remaining elements\n    for (; i < end; ++i) {\n        arr[i] = arr[end - (i - start)];\n    }\n}\n```"}
{"task_id": "SimdBench_4_SSE", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    const __m128i zero = _mm_setzero_si128();\n    const uint64_t len = end - start + 1;\n    const uint64_t num_full_blocks = len / 8;\n    const uint64_t remainder = len % 8;\n\n    // Reverse full blocks of 8 elements\n    for (uint64_t i = 0; i < num_full_blocks; ++i) {\n        __m128i block = _mm_loadu_si128((__m128i*)&arr[start + i*8]);\n        __m128i reversed_block = _mm_shuffle_epi32(block, _MM_SHUFFLE(0, 1, 2, 3));\n        reversed_block = _mm_or_si128(_mm_slli_epi16(reversed_block, 8), _mm_srli_epi16(reversed_block, 8));\n        _mm_storeu_si128((__m128i*)&arr[start + (num_full_blocks - i - 1)*8], reversed_block);\n    }\n\n    // Handle remaining elements\n    for (uint64_t i = 0; i < remainder; ++i) {\n        arr[start + i] ^= arr[end - i];\n        arr[end - i] ^= arr[start + i];\n        arr[start + i] ^= arr[end - i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    for (; i + 7 < dim1 * dim2; i += 8) {\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i+0, i+3, i+2, i+1, i+0);\n        __m128i values = _mm_i32gather_epi32((const int*)tensor, indices, 1);\n        _mm_storeu_si128((__m128i*)(slice + i), values);\n    }\n    for (; i < dim1 * dim2; ++i) {\n        slice[i] = tensor[i * dim3 + slice_idx];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    \n    for (; i + 7 < dim1 * dim2; i += 8) {\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i+0, i+3, i+2, i+1, i+0);\n        __m128i base_addr = _mm_set_epi32(slice_idx*dim3 + (i+3)*dim3, slice_idx*dim3 + (i+2)*dim3,\n                                           slice_idx*dim3 + (i+1)*dim3, slice_idx*dim3 + (i+0)*dim3,\n                                           slice_idx*dim3 + (i+3)*dim3, slice_idx*dim3 + (i+2)*dim3,\n                                           slice_idx*dim3 + (i+1)*dim3, slice_idx*dim3 + (i+0)*dim3);\n        __m128i data = _mm_i32gather_epi32((const int*)tensor, indices, 4);\n        _mm_storeu_si128((__m128i*)(slice + i), data);\n    }\n    \n    for (; i < dim1 * dim2; ++i) {\n        slice[i] = tensor[slice_idx * dim3 + i * dim3];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    \n    for (; i + 7 < dim1 * dim2; i += 8) {\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i, i+3, i+2, i+1, i);\n        __m128i base_addr = _mm_add_epi64(_mm_cvtsi32_si128((int)slice_idx * dim3), _mm_cvtsi32_si128(i));\n        __m128i addr = _mm_add_epi64(base_addr, _mm_mullo_epi32(indices, _mm_cvtsi32_si128(dim3)));\n        __m128i values = _mm_i32gather_epi32((const int*)tensor, addr, 4);\n        _mm_storeu_si128((__m128i*)(slice + i), values);\n    }\n    \n    for (; i < dim1 * dim2; ++i) {\n        slice[i] = tensor[slice_idx * dim3 + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    \n    for (; i + 7 < dim1 * dim2; i += 8) {\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i+0, i+3, i+2, i+1, i+0);\n        __m128i base_addr = _mm_add_epi64(_mm_cvtsi32_si128((int)slice_idx), _mm_set_epi32(i+3, i+2, i+1, i+0));\n        __m128i addr = _mm_add_epi64(base_addr, _mm_mul_epu32(indices, _mm_set_epi32(dim3, dim3, dim3, dim3)));\n        __m128i values = _mm_i32gather_epi32((const int*)tensor, addr, 4);\n        _mm_storeu_si128((__m128i*)(slice + i), values);\n    }\n    \n    for (; i < dim1 * dim2; ++i) {\n        slice[i] = tensor[slice_idx * dim3 + i];\n    }\n}\n```"}
{"task_id": "SimdBench_5_SSE", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    size_t i = 0;\n    __m128i zero = _mm_setzero_si128();\n    \n    for (; i + 7 < dim1 * dim2; i += 8) {\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i+0, i+3, i+2, i+1, i+0);\n        __m128i base_addr = _mm_add_epi64(_mm_cvtsi64_si128((long long)tensor + slice_idx * dim1 * dim2), _mm_slli_epi64(indices, 3));\n        __m128i values = _mm_i32gather_epi32((__m128i*)tensor, base_addr, 4);\n        _mm_storeu_si128((__m128i*)(slice + i), values);\n    }\n    \n    for (; i < dim1 * dim2; i++) {\n        slice[i] = tensor[slice_idx * dim1 * dim2 + i];\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k;\n    size_t blocks = rows / block_size;\n\n    for (i = 0; i < blocks; ++i) {\n        for (j = 0; j < blocks; ++j) {\n            for (k = 0; k < block_size; ++k) {\n                __m128i row0 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size]);\n                __m128i row1 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size + 1]);\n                __m128i row2 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size + 2]);\n                __m128i row3 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size + 3]);\n\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size], row0);\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size + 1], row1);\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size + 2], row2);\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size + 3], row3);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t br = 0; br < rows; br += block_size) {\n        for (size_t bc = 0; bc < cols; bc += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                if (br + i >= rows) continue;\n                for (size_t j = 0; j < block_size; j += 4) {\n                    if (bc + j + 3 >= cols) break;\n                    __m128i row0 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 0)]);\n                    __m128i row1 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 1)]);\n                    __m128i row2 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 2)]);\n                    __m128i row3 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 3)]);\n                    \n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 0) * rows + (br + i)], row0);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 1) * rows + (br + i)], row1);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 2) * rows + (br + i)], row2);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 3) * rows + (br + i)], row3);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t br = 0; br < rows; br += block_size) {\n        for (size_t bc = 0; bc < cols; bc += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                if (br + i >= rows) continue;\n                for (size_t j = 0; j < block_size; j += 4) {\n                    if (bc + j + 3 >= cols) break;\n                    __m128i row0 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 0)]);\n                    __m128i row1 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 1)]);\n                    __m128i row2 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 2)]);\n                    __m128i row3 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 3)]);\n                    \n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 0) * rows + (br + i)], row0);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 1) * rows + (br + i)], row1);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 2) * rows + (br + i)], row2);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 3) * rows + (br + i)], row3);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    for (size_t br = 0; br < rows; br += block_size) {\n        for (size_t bc = 0; bc < cols; bc += block_size) {\n            for (size_t i = 0; i < block_size; ++i) {\n                if (br + i >= rows) continue;\n                for (size_t j = 0; j < block_size; j += 4) {\n                    if (bc + j + 3 >= cols) break;\n                    __m128i row0 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 0)]);\n                    __m128i row1 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 1)]);\n                    __m128i row2 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 2)]);\n                    __m128i row3 = _mm_loadu_si128((__m128i*)&src[(br + i) * cols + (bc + j + 3)]);\n                    \n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 0) * rows + (br + i)], row0);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 1) * rows + (br + i)], row1);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 2) * rows + (br + i)], row2);\n                    _mm_storeu_si128((__m128i*)&dst[(bc + j + 3) * rows + (br + i)], row3);\n                }\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_SSE", "completion": "```c\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    size_t i, j, k;\n    size_t blocks = rows / block_size;\n\n    for (i = 0; i < blocks; ++i) {\n        for (j = 0; j < blocks; ++j) {\n            for (k = 0; k < block_size; ++k) {\n                __m128i row0 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size]);\n                __m128i row1 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size + 4]);\n                __m128i row2 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size + 8]);\n                __m128i row3 = _mm_loadu_si128((__m128i*)&src[(i*block_size + k)*cols + j*block_size + 12]);\n\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size], row0);\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size + 4], row1);\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size + 8], row2);\n                _mm_storeu_si128((__m128i*)&dst[(j*block_size + k)*rows + i*block_size + 12], row3);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(array + i * (dim + 1));\n        sum_vec = _mm_add_ps(sum_vec, vec);\n    }\n\n    float sum = 0.0f;\n    _mm_store_ss(&sum, sum_vec);\n\n    for (; i < dim; i++) {\n        sum += array[i * (dim + 1)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(array + i * (dim + 1));\n        sum_vec = _mm_add_ps(sum_vec, vec);\n    }\n\n    float sum = 0.0f;\n    _mm_store_ss(&sum, sum_vec);\n\n    for (; i < dim; ++i) {\n        sum += array[i * (dim + 1)];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m128 vec0 = _mm_loadu_ps(array + i * dim * dim + i);\n        __m128 vec1 = _mm_loadu_ps(array + (i + 1) * dim * dim + (i + 1));\n        __m128 vec2 = _mm_loadu_ps(array + (i + 2) * dim * dim + (i + 2));\n        __m128 vec3 = _mm_loadu_ps(array + (i + 3) * dim * dim + (i + 3));\n\n        __m128 sum0 = _mm_add_ps(vec0, vec1);\n        __m128 sum1 = _mm_add_ps(vec2, vec3);\n\n        sum_vec = _mm_add_ps(sum_vec, sum0);\n        sum_vec = _mm_add_ps(sum_vec, sum1);\n    }\n\n    float sum = 0.0f;\n    _mm_store_ss(&sum, sum_vec);\n\n    for (; i < dim; ++i) {\n        sum += array[i * dim * dim + i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m128 vec0 = _mm_loadu_ps(array + i * dim * dim + i);\n        __m128 vec1 = _mm_loadu_ps(array + (i + 1) * dim * dim + (i + 1));\n        __m128 vec2 = _mm_loadu_ps(array + (i + 2) * dim * dim + (i + 2));\n        __m128 vec3 = _mm_loadu_ps(array + (i + 3) * dim * dim + (i + 3));\n\n        __m128 sum0 = _mm_add_ps(vec0, vec1);\n        __m128 sum1 = _mm_add_ps(vec2, vec3);\n        __m128 sum2 = _mm_add_ps(sum0, sum1);\n\n        sum_vec = _mm_add_ps(sum_vec, sum2);\n    }\n\n    float sum_array[4];\n    _mm_storeu_ps(sum_array, sum_vec);\n\n    float sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    for (; i < dim; ++i) {\n        sum += array[i * dim * dim + i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_SSE", "completion": "```c\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    __m128 sum_vec = _mm_setzero_ps();\n    size_t i = 0;\n\n    for (; i <= dim - 4; i += 4) {\n        __m128 vec0 = _mm_loadu_ps(array + i * dim * dim + i);\n        __m128 vec1 = _mm_loadu_ps(array + i * dim * dim + i + dim);\n        __m128 vec2 = _mm_loadu_ps(array + i * dim * dim + i + dim + dim);\n        __m128 vec3 = _mm_loadu_ps(array + i * dim * dim + i + dim + dim + dim);\n\n        __m128 sum0 = _mm_add_ps(vec0, vec1);\n        __m128 sum1 = _mm_add_ps(vec2, vec3);\n        __m128 sum2 = _mm_add_ps(sum0, sum1);\n\n        sum_vec = _mm_add_ps(sum_vec, sum2);\n    }\n\n    float sum_array[4];\n    _mm_storeu_ps(sum_array, sum_vec);\n\n    float sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n\n    for (; i < dim; i++) {\n        sum += array[i * dim * dim + i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d zero_vec = _mm_setzero_pd();\n    \n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(src + i * cols + j);\n            __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n            __m128d scaled = _mm_mul_pd(src_vec, scale_vec);\n            __m128d result = _mm_blendv_pd(src_vec, scaled, mask);\n            _mm_storeu_pd(dst + i * cols + j, result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d zero_vec = _mm_setzero_pd();\n\n    for (i = 0; i < rows; i++) {\n        for (j = 0; j < cols; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(src + i * cols + j);\n            __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n            __m128d scaled = _mm_mul_pd(src_vec, scale_vec);\n            __m128d result = _mm_or_pd(_mm_and_pd(mask, scaled), _mm_andnot_pd(mask, src_vec));\n            _mm_storeu_pd(dst + i * cols + j, result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d zero_vec = _mm_setzero_pd();\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(src + i * cols + j);\n            __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n            __m128d scaled = _mm_mul_pd(src_vec, scale_vec);\n            __m128d result = _mm_blendv_pd(src_vec, scaled, mask);\n            _mm_storeu_pd(dst + i * cols + j, result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d zero_vec = _mm_setzero_pd();\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j < cols; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(src + i * cols + j);\n            __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n            __m128d scaled = _mm_mul_pd(src_vec, scale_vec);\n            __m128d result = _mm_or_pd(_mm_and_pd(mask, scaled), _mm_andnot_pd(mask, src_vec));\n            _mm_storeu_pd(dst + i * cols + j, result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_SSE", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    size_t i, j;\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    __m128d scale_vec = _mm_set1_pd(scale);\n    __m128d zero_vec = _mm_setzero_pd();\n\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 2 <= cols; j += 2) {\n            __m128d src_vec = _mm_loadu_pd(src + j);\n            __m128d mask = _mm_cmpgt_pd(src_vec, threshold_vec);\n            __m128d scaled = _mm_mul_pd(src_vec, scale_vec);\n            __m128d result = _mm_blendv_pd(src_vec, scaled, mask);\n            _mm_storeu_pd(dst + j, result);\n        }\n        for (; j < cols; ++j) {\n            dst[j] = (src[j] > threshold) ? (src[j] * scale) : src[j];\n        }\n        src += cols;\n        dst += cols;\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    __m128d row_data[cols];\n    __m128i index_vec;\n\n    // Process full rows using SIMD\n    for (; i <= rows - 2; i += 2) {\n        // Load two rows from source into SIMD registers\n        for (size_t j = 0; j < cols; j += 2) {\n            _mm_loadu_pd(&src[i * cols + j], &row_data[j].m128d_f64[0]);\n            _mm_loadu_pd(&src[(i+1) * cols + j], &row_data[j].m128d_f64[1]);\n        }\n\n        // Load indices for these two rows\n        index_vec = _mm_set_epi64x(indices[i+1], indices[i]);\n\n        // Store reordered rows into destination\n        for (size_t j = 0; j < cols; j += 2) {\n            _mm_storeu_pd(&dst[indices[i] * cols + j], row_data[j].m128d_f64[0]);\n            _mm_storeu_pd(&dst[indices[i+1] * cols + j], row_data[j].m128d_f64[1]);\n        }\n    }\n\n    // Handle remaining rows if rows is not even\n    for (; i < rows; i++) {\n        size_t src_row = indices[i];\n        for (size_t j = 0; j < cols; j++) {\n            dst[i * cols + j] = src[src_row * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    __m128d row_data[cols];\n    __m128i index_vec;\n\n    // Process full rows using SIMD\n    for (; i <= rows - 2; i += 2) {\n        // Load two rows from src into SIMD registers\n        for (size_t j = 0; j < cols; j += 2) {\n            row_data[j] = _mm_load_sd(&src[i * cols + j]);\n            row_data[j + 1] = _mm_load_sd(&src[(i + 1) * cols + j]);\n        }\n\n        // Load indices for two rows\n        index_vec = _mm_set_epi64x(indices[i + 1], indices[i]);\n\n        // Reorder rows using indices\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d temp = row_data[j];\n            row_data[j] = row_data[_mm_extract_epi64(index_vec, 0)];\n            row_data[_mm_extract_epi64(index_vec, 0)] = temp;\n\n            temp = row_data[j + 1];\n            row_data[j + 1] = row_data[_mm_extract_epi64(index_vec, 1)];\n            row_data[_mm_extract_epi64(index_vec, 1)] = temp;\n        }\n\n        // Store reordered rows back to dst\n        for (size_t j = 0; j < cols; j += 2) {\n            _mm_store_sd(&dst[i * cols + j], row_data[j]);\n            _mm_store_sd(&dst[(i + 1) * cols + j], row_data[j + 1]);\n        }\n    }\n\n    // Handle remaining rows if rows is not even\n    for (; i < rows; i++) {\n        for (size_t j = 0; j < cols; j++) {\n            dst[i * cols + j] = src[indices[i] * cols + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    __m128d row_data[2];\n    __m128i index_vec;\n\n    // Ensure rows is even for simplicity in handling\n    if (rows % 2 != 0) {\n        rows -= 1;\n    }\n\n    for (; i < rows; i += 2) {\n        // Load two rows from src\n        row_data[0] = _mm_loadu_pd(src + indices[i] * cols);\n        row_data[1] = _mm_loadu_pd(src + indices[i + 1] * cols);\n\n        // Load two indices\n        index_vec = _mm_set_epi64x(indices[i + 1], indices[i]);\n\n        // Store reordered rows into dst\n        _mm_storeu_pd(dst + indices[i] * cols, row_data[1]);\n        _mm_storeu_pd(dst + indices[i + 1] * cols, row_data[0]);\n    }\n\n    // Handle remaining row if rows is odd\n    if (i < rows) {\n        _mm_storeu_pd(dst + indices[i] * cols, _mm_loadu_pd(src + indices[i] * cols));\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    __m128d row_data[2];\n    __m128i indices_vec[2];\n\n    // Process two rows at a time using SIMD\n    for (; i <= rows - 2; i += 2) {\n        // Load indices for two rows\n        indices_vec[0] = _mm_loadu_si128((__m128i*)(indices + i));\n        indices_vec[1] = _mm_loadu_si128((__m128i*)(indices + i + 1));\n\n        // Load two rows from source matrix\n        row_data[0] = _mm_load_pd(src + indices[i] * cols);\n        row_data[1] = _mm_load_pd(src + indices[i + 1] * cols);\n\n        // Store reordered rows into destination matrix\n        _mm_store_pd(dst + i * cols, row_data[0]);\n        _mm_store_pd(dst + (i + 1) * cols, row_data[1]);\n    }\n\n    // Handle remaining row if rows is odd\n    if (i < rows) {\n        indices_vec[0] = _mm_loadu_si128((__m128i*)(indices + i));\n        row_data[0] = _mm_load_pd(src + indices[i] * cols);\n        _mm_store_pd(dst + i * cols, row_data[0]);\n    }\n}\n```"}
{"task_id": "SimdBench_9_SSE", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    size_t i = 0;\n    __m128d row_data[2];\n    __m128i indices_vec[2];\n\n    // Process two rows at a time using SIMD\n    for (; i <= rows - 2; i += 2) {\n        // Load indices for two rows\n        indices_vec[0] = _mm_loadu_si128((__m128i*)(indices + i));\n        indices_vec[1] = _mm_loadu_si128((__m128i*)(indices + i + 1));\n\n        // Load two rows from source matrix\n        row_data[0] = _mm_load_pd(src + i * cols);\n        row_data[1] = _mm_load_pd(src + (i + 1) * cols);\n\n        // Store reordered rows into destination matrix\n        _mm_store_pd(dst + indices[i] * cols, row_data[0]);\n        _mm_store_pd(dst + indices[i + 1] * cols, row_data[1]);\n    }\n\n    // Handle remaining row if rows is odd\n    if (i < rows) {\n        __m128d last_row_data;\n        __m128i last_index_vec;\n\n        last_index_vec = _mm_loadu_si128((__m128i*)(indices + i));\n        last_row_data = _mm_load_pd(src + i * cols);\n\n        _mm_store_pd(dst + indices[i] * cols, last_row_data);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m128i xmm_scalar = _mm_set1_epi64x(scalar);\n\n    for (; i <= length - 4; i += 4) {\n        __m128i xmm_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i xmm_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i xmm_sum = _mm_add_epi64(xmm_src1, xmm_src2);\n        __m128i xmm_result = _mm_add_epi64(xmm_sum, xmm_scalar);\n        xmm_result = _mm_srai_epi64(xmm_result, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], xmm_result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i]) + scalar) >> 1;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m128i xmm_scalar = _mm_set1_epi64x(scalar);\n\n    for (; i <= length - 4; i += 4) {\n        __m128i xmm_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i xmm_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i xmm_sum = _mm_add_epi64(xmm_src1, xmm_src2);\n        __m128i xmm_result = _mm_add_epi64(xmm_sum, xmm_scalar);\n        xmm_result = _mm_srai_epi64(xmm_result, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], xmm_result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i]) + scalar) >> 1;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i = 0;\n    __m128i xmm_scalar = _mm_set1_epi64x(scalar);\n\n    for (; i <= length - 4; i += 4) {\n        __m128i xmm_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i xmm_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i xmm_sum = _mm_add_epi64(xmm_src1, xmm_src2);\n        __m128i xmm_result = _mm_add_epi64(xmm_sum, xmm_scalar);\n        xmm_result = _mm_srai_epi64(xmm_result, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], xmm_result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((src1[i] + src2[i]) + scalar) >> 1;\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i;\n    __m128i xmm_scalar = _mm_set1_epi64x(scalar);\n    for(i = 0; i + 2 <= length; i += 2){\n        __m128i xmm_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i xmm_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i xmm_sum = _mm_add_epi64(xmm_src1, xmm_src2);\n        __m128i xmm_result = _mm_add_epi64(xmm_sum, xmm_scalar);\n        xmm_result = _mm_srai_epi64(xmm_result, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], xmm_result);\n    }\n    if(i < length){\n        __m128i xmm_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i xmm_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i xmm_sum = _mm_add_epi64(xmm_src1, xmm_src2);\n        __m128i xmm_result = _mm_add_epi64(xmm_sum, xmm_scalar);\n        xmm_result = _mm_srai_epi64(xmm_result, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], xmm_result);\n    }\n}\n```"}
{"task_id": "SimdBench_10_SSE", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    size_t i;\n    __m128i xmm_scalar = _mm_set1_epi64x(scalar);\n    for(i = 0; i + 2 <= length; i += 2){\n        __m128i xmm_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i xmm_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i xmm_sum = _mm_add_epi64(xmm_src1, xmm_src2);\n        __m128i xmm_result = _mm_add_epi64(xmm_sum, xmm_scalar);\n        xmm_result = _mm_srai_epi64(xmm_result, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], xmm_result);\n    }\n    if(i < length){\n        __m128i xmm_src1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i xmm_src2 = _mm_loadu_si128((__m128i*)&src2[i]);\n        __m128i xmm_sum = _mm_add_epi64(xmm_src1, xmm_src2);\n        __m128i xmm_result = _mm_add_epi64(xmm_sum, xmm_scalar);\n        xmm_result = _mm_srai_epi64(xmm_result, 1);\n        _mm_storeu_si128((__m128i*)&dst[i], xmm_result);\n    }\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    \n    __m128i min_val = _mm_set1_epi32(__INT32_MAX__);\n    const int *end = src + length;\n    \n    while(src + 4 <= end){\n        __m128i vec = _mm_loadu_si128((__m128i*)src);\n        __m128i even_mask = _mm_setr_epi32(0, 1, 0, 1);\n        __m128i even_vals = _mm_and_si128(vec, even_mask);\n        \n        __m128i gt_mask = _mm_cmpgt_epi32(min_val, even_vals);\n        min_val = _mm_min_epi32(min_val, even_vals);\n        \n        src += 4;\n    }\n    \n    int result = _mm_extract_epi32(min_val, 0);\n    for(; src < end; ++src){\n        if((*src & 1) == 0 && *src < result){\n            result = *src;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    \n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF); // Initialize with max int\n    size_t i = 0;\n    \n    for(; i <= length - 8; i += 8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_setr_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000, 0x80000000);\n        __m128i even_vec = _mm_and_si128(vec, mask);\n        \n        __m128i cmp = _mm_cmpgt_epi32(min_val, even_vec);\n        min_val = _mm_min_epi32(min_val, even_vec);\n    }\n    \n    int result = min_val.m128i_i32[0];\n    for(size_t j = i; j < length; ++j){\n        if(src[j] % 2 == 0 && src[j] < result){\n            result = src[j];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    \n    __m128i min_val = _mm_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    size_t i = 0;\n    \n    for(; i <= length - 8; i += 8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m128i even_vec = _mm_and_si128(vec, mask);\n        \n        __m128i gt_mask = _mm_cmpgt_epi32(min_val, even_vec);\n        min_val = _mm_min_epi32(min_val, even_vec);\n    }\n    \n    int result = 0x7FFFFFFF;\n    for(size_t j = i; j < length; ++j){\n        if(src[j] % 2 == 0 && src[j] < result){\n            result = src[j];\n        }\n    }\n    \n    return result < 0x7FFFFFFF ? result : -1;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    \n    __m128i min_val = _mm_set1_epi32(__INT32_MAX__);\n    size_t i = 0;\n    \n    for(; i <= length - 8; i += 8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m128i even_vec = _mm_and_si128(vec, mask);\n        \n        __m128i gt_mask = _mm_cmpgt_epi32(min_val, even_vec);\n        min_val = _mm_min_epi32(min_val, even_vec);\n    }\n    \n    int final_min = _mm_extract_epi32(min_val, 0);\n    for(; i < length; ++i){\n        if(src[i] % 2 == 0 && src[i] < final_min){\n            final_min = src[i];\n        }\n    }\n    \n    return final_min;\n}\n```"}
{"task_id": "SimdBench_11_SSE", "completion": "```c\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n    \n    __m128i min_val = _mm_set1_epi32(__INT32_MAX__);\n    size_t i = 0;\n    \n    for(; i <= length - 8; i += 8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i mask = _mm_setr_epi32(1, 0, 1, 0, 1, 0, 1, 0);\n        __m128i even_vec = _mm_and_si128(vec, mask);\n        \n        __m128i gt_mask = _mm_cmpgt_epi32(min_val, even_vec);\n        min_val = _mm_min_epi32(min_val, even_vec);\n    }\n    \n    int final_min = _mm_extract_epi32(min_val, 0);\n    for(; i < length; ++i){\n        if(src[i] % 2 == 0 && src[i] < final_min){\n            final_min = src[i];\n        }\n    }\n    \n    return final_min;\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i shift_mask = _mm_set1_epi8(shift);\n    __m128i zero = _mm_setzero_si128();\n    __m128i src_vec, arith_shifted, logic_shifted;\n\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Arithmetic Right Shift\n        arith_shifted = _mm_srai_epi32(src_vec, shift);\n\n        // Logical Right Shift\n        src_vec = _mm_castps_si128(_mm_or_ps(_mm_castsi128_ps(src_vec), zero)); // Convert to unsigned\n        logic_shifted = _mm_srli_epi32(src_vec, shift);\n        logic_shifted = _mm_castsi128_ps(logic_shifted); // Cast back to signed\n\n        // Combine Results\n        __m128i combined = _mm_add_epi32(arith_shifted, logic_shifted);\n        _mm_storeu_si128((__m128i*)&dst[i], combined);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] >> shift) + ((unsigned int)(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i mask = _mm_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF);\n    __m128i shift_arith = _mm_cvtsi32_si128(shift);\n    __m128i shift_log = _mm_cvtsi32_si128(shift);\n\n    for (size_t i = 0; i + 4 <= length; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m128i arith_shifted = _mm_sra_epi32(src_vec, shift_arith);\n        \n        // Logical right shift\n        __m128i log_shifted = _mm_srli_epi32(_mm_and_si128(src_vec, mask), shift_log);\n        \n        // Combine results\n        __m128i result = _mm_add_epi32(arith_shifted, log_shifted);\n        \n        // Store the result\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Handle remaining elements if any\n    for (size_t i = length - (length % 4); i < length; ++i) {\n        dst[i] = ((src[i] >> shift) & 0x7FFFFFFF) + (src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i shift_mask = _mm_set1_epi8(shift);\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m128i arith_shift = _mm_sra_epi32(src_vec, _mm_cvtsi32_si128(shift));\n        \n        // Logical right shift\n        __m128i src_uint = _mm_castps_si128(_mm_cvtepi32_ps(src_vec));\n        __m128i log_shift = _mm_srl_epi32(src_uint, shift_mask);\n        __m128i log_shift_int = _mm_castsi128_ps(log_shift);\n        log_shift = _mm_castps_si128(log_shift_int);\n        \n        // Add results\n        __m128i result = _mm_add_epi32(arith_shift, log_shift);\n        \n        // Store result\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] >> shift) + ((uint32_t)src[i] >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i shift_mask = _mm_set1_epi8(shift);\n    __m128i zero = _mm_setzero_si128();\n    \n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        \n        // Arithmetic right shift\n        __m128i arith_shift = _mm_srai_epi32(src_vec, shift);\n        \n        // Logical right shift\n        __m128i src_uint = _mm_castsi128_ps(_mm_or_si128(src_vec, zero)); // Cast to uint\n        __m128i log_shift = _mm_cvttps_epi32(_mm_castsi128_ps(_mm_srli_epi32(_mm_castps_si128(src_uint), shift)));\n        \n        // Add results\n        __m128i result = _mm_add_epi32(arith_shift, log_shift);\n        \n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n    \n    for (; i < length; i++) {\n        int val = src[i];\n        dst[i] = (val >> shift) + ((unsigned int)val >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_SSE", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m128i shift_mask = _mm_set1_epi8(shift);\n    __m128i zero = _mm_setzero_si128();\n    __m128i src_vec, arith_shifted, logic_shifted;\n\n    size_t i = 0;\n    for (; i <= length - 4; i += 4) {\n        src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n\n        // Arithmetic Right Shift\n        arith_shifted = _mm_srai_epi32(src_vec, shift);\n\n        // Logical Right Shift\n        src_vec = _mm_castps_si128(_mm_or_ps(_mm_castsi128_ps(src_vec), zero)); // Convert to unsigned\n        logic_shifted = _mm_srli_epi32(src_vec, shift);\n\n        // Combine Results\n        __m128i combined = _mm_add_epi32(arith_shifted, logic_shifted);\n\n        _mm_storeu_si128((__m128i*)&dst[i], combined);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src[i] >> shift) + (static_cast<unsigned int>(src[i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            sum_vec = _mm_add_epi32(sum_vec, vec);\n        }\n        for (; j < cols; ++j) {\n            sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(matrix[i * cols + j]));\n        }\n    }\n    __m128i sum_low = _mm_hadd_epi32(sum_vec, sum_vec);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    return _mm_cvtsi128_si64(sum_low);\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            sum_vec = _mm_add_epi32(sum_vec, vec);\n        }\n        for (; j < cols; ++j) {\n            sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(matrix[i * cols + j]));\n        }\n    }\n    __m128i temp = _mm_hadd_epi32(sum_vec, sum_vec);\n    temp = _mm_hadd_epi32(temp, temp);\n    return _mm_cvtsi128_si64(temp);\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i, j;\n    size_t total_elements = rows * cols;\n\n    for (i = 0; i < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    __m128i summed = _mm_hadd_epi32(sum_vec, sum_vec);\n    summed = _mm_hadd_epi32(summed, summed);\n\n    int32_t low_word;\n    _mm_store_ss((float*)&low_word, _mm_castsi128_ps(summed));\n\n    int64_t total_sum = low_word;\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i, j;\n    for (i = 0; i < rows; ++i) {\n        for (j = 0; j + 4 <= cols; j += 4) {\n            __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            sum_vec = _mm_add_epi32(sum_vec, vec);\n        }\n        for (; j < cols; ++j) {\n            sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(matrix[i * cols + j]));\n        }\n    }\n    __m128i sum_low = _mm_hadd_epi32(sum_vec, sum_vec);\n    sum_low = _mm_hadd_epi32(sum_low, sum_low);\n    return _mm_cvtsi128_si64(sum_low);\n}\n```"}
{"task_id": "SimdBench_13_SSE", "completion": "```c\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i, j;\n    size_t total_elements = rows * cols;\n\n    for (i = 0; i < total_elements; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&matrix[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n    }\n\n    __m128i horizontal_sum = _mm_setzero_si128();\n    horizontal_sum = _mm_add_epi32(horizontal_sum, sum_vec);\n\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, horizontal_sum);\n\n    int64_t sum = temp[0] + temp[1] + temp[2] + temp[3];\n    return sum;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t max_idx = 0;\n    __m128i max_val = _mm_set1_epi8(src[0]);\n    size_t i = 0;\n\n    while (i + 16 <= length) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i mask = _mm_cmpgt_epi8(vec, max_val);\n        max_val = _mm_or_si128(max_val, _mm_and_si128(mask, vec));\n        size_t mask_idx = _mm_movemask_epi8(mask);\n        if (mask_idx != 0) {\n            max_idx += __builtin_ctz(mask_idx);\n            max_val = _mm_set1_epi8(src[max_idx]);\n        }\n        i += 16;\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_value = _mm_setzero_si128();\n    __m128i max_index_vec = _mm_set_epi32(3, 2, 1, 0);\n    size_t max_index = 0;\n\n    size_t i = 0;\n    while (i + 4 <= length) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i gt_mask = _mm_cmpgt_epi8(vec, max_value);\n        max_value = _mm_or_si128(max_value, _mm_and_si128(gt_mask, vec));\n        __m128i new_indices = _mm_add_epi32(max_index_vec, _mm_set1_epi32((int)i));\n        max_index_vec = _mm_or_si128(max_index_vec, _mm_and_si128(gt_mask, new_indices));\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if ((int)src[i] > (int)max_value.m128i_i8[0]) {\n            max_value = _mm_set1_epi8(src[i]);\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_val = _mm_setzero_si128();\n    __m128i max_idx_vec = _mm_set_epi32(3, 2, 1, 0);\n    size_t max_idx = 0;\n\n    size_t i = 0;\n    while (i + 4 <= length) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i gt_mask = _mm_cmpgt_epi8(vec, max_val);\n        max_val = _mm_or_si128(max_val, _mm_and_si128(gt_mask, vec));\n        __m128i new_max_idx = _mm_add_epi32(max_idx_vec, _mm_set1_epi32(i));\n        max_idx_vec = _mm_or_si128(max_idx_vec, _mm_and_si128(gt_mask, new_max_idx));\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    size_t max_idx = 0;\n    __m128i max_val = _mm_set1_epi8(src[0]);\n    size_t i = 0;\n\n    while (i + 16 <= length) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i mask = _mm_cmpgt_epi8(vec, max_val);\n        max_val = _mm_or_si128(max_val, _mm_and_si128(mask, vec));\n        uint16_t idx_mask = _mm_movemask_epi8(mask);\n        while (idx_mask) {\n            size_t idx = __builtin_ctz(idx_mask);\n            if (src[i + idx] > src[max_idx]) {\n                max_idx = i + idx;\n            }\n            idx_mask ^= (1 << idx);\n        }\n        i += 16;\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_idx]) {\n            max_idx = i;\n        }\n    }\n\n    return max_idx;\n}\n```"}
{"task_id": "SimdBench_14_SSE", "completion": "```c\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) return 0;\n\n    __m128i max_value = _mm_setzero_si128();\n    __m128i max_index_vec = _mm_set_epi32(3, 2, 1, 0);\n    size_t max_index = 0;\n\n    size_t i = 0;\n    while (i + 4 <= length) {\n        __m128i vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i gt_mask = _mm_cmpgt_epi8(vec, max_value);\n        max_value = _mm_or_si128(max_value, _mm_and_si128(gt_mask, vec));\n        __m128i new_indices = _mm_add_epi32(max_index_vec, _mm_set1_epi32((int)i));\n        max_index_vec = _mm_or_si128(_mm_andnot_si128(gt_mask, max_index_vec), _mm_and_si128(gt_mask, new_indices));\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        if ((int)src[i] > (int)max_value.m128i_i8[0]) {\n            max_value = _mm_set1_epi8(src[i]);\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set_epi32(mask, mask, mask, mask);\n\n    for (; i <= length - 8; i += 8) {\n        __m128i idx = _mm_set_epi32(i + 3, i + 2, i + 1, i + 0);\n        __m128i idx_masked = _mm_and_si128(idx, mask_vec);\n        __m128i src1_vec = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i mask_test = _mm_cmpeq_epi32(idx_masked, mask_vec);\n        __m128i result = _mm_blendv_epi8(src2_vec, src1_vec, mask_test);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set_epi32(mask, mask, mask, mask);\n\n    for (; i <= length - 8; i += 8) {\n        __m128i idx = _mm_set_epi32(i + 3, i + 2, i + 1, i + 0);\n        __m128i idx_masked = _mm_and_si128(idx, mask_vec);\n        __m128i src1_vec = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i mask_test = _mm_cmpeq_epi32(idx_masked, mask_vec);\n        __m128i result = _mm_or_si128(_mm_andnot_si128(mask_test, src2_vec), _mm_and_si128(mask_test, src1_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set_epi32(mask, mask, mask, mask);\n\n    for (; i <= length - 8; i += 8) {\n        __m128i idx = _mm_set_epi32(i + 3, i + 2, i + 1, i + 0);\n        __m128i idx_masked = _mm_and_si128(idx, mask_vec);\n        __m128i src1_vec = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i mask_test = _mm_cmpeq_epi32(idx_masked, mask_vec);\n        __m128i result = _mm_or_si128(_mm_andnot_si128(mask_test, src2_vec), _mm_and_si128(mask_test, src1_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set_epi32(mask, mask, mask, mask);\n\n    for (; i <= length - 8; i += 8) {\n        __m128i idx = _mm_set_epi32(i + 3, i + 2, i + 1, i + 0);\n        __m128i idx_masked = _mm_and_si128(idx, mask_vec);\n        __m128i src1_vec = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i blend_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), idx_masked);\n        __m128i result = _mm_blendv_epi8(src2_vec, src1_vec, blend_mask);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_15_SSE", "completion": "```c\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask_vec = _mm_set_epi32(mask, mask, mask, mask);\n\n    for (; i <= length - 8; i += 8) {\n        __m128i idx = _mm_set_epi32(i+3, i+2, i+1, i+0);\n        __m128i idx_masked = _mm_and_si128(idx, mask_vec);\n        __m128i src1_vec = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i src2_vec = _mm_loadu_si128((__m128i*)(src2 + i));\n        __m128i mask_test = _mm_cmpeq_epi32(idx_masked, mask_vec);\n        __m128i result = _mm_or_si128(_mm_andnot_si128(mask_test, src2_vec), _mm_and_si128(mask_test, src1_vec));\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    for (; i < length; i++) {\n        dst[i] = ((i & mask) != 0) ? src1[i] : src2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i pc_table = _mm_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    while (length >= 4) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)src);\n\n        // Step 1: Extract low nibble and look up population count\n        __m128i v_low = _mm_and_si128(v_src, _mm_set1_epi32(0x0F));\n        __m128i v_low_pc = _mm_shuffle_epi8(pc_table, v_low);\n\n        // Step 2: Shift right by 4 and extract high nibble\n        __m128i v_high = _mm_srli_epi32(v_src, 4);\n        __m128i v_high_pc = _mm_shuffle_epi8(pc_table, v_high);\n\n        // Combine low and high nibble counts\n        __m128i v_sum = _mm_add_epi8(v_low_pc, v_high_pc);\n\n        // Horizontal sum across 4 bytes\n        __m128i v_sum03 = _mm_add_epi16(v_sum, _mm_srli_si128(v_sum, 6));\n        __m128i v_sum0123 = _mm_add_epi16(v_sum03, _mm_srli_si128(v_sum03, 3));\n        __m128i v_pcount = _mm_shuffle_epi32(v_sum0123, _MM_SHUFFLE(0,0,0,0));\n\n        _mm_storeu_si128((__m128i*)dst, v_pcount);\n\n        src += 4;\n        dst += 4;\n        length -= 4;\n    }\n\n    // Handle remaining elements\n    while (length--) {\n        *dst++ = __builtin_popcount(*src++);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i pc_table = _mm_set_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    while (length >= 16) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)src);\n        __m128i v_b0 = _mm_and_si128(v_src, _mm_set1_epi32(0xF));\n        __m128i v_b1 = _mm_and_si128(_mm_srli_epi32(v_src, 4), _mm_set1_epi32(0xF));\n        __m128i v_b2 = _mm_and_si128(_mm_srli_epi32(v_src, 8), _mm_set1_epi32(0xF));\n        __m128i v_b3 = _mm_and_si128(_mm_srli_epi32(v_src, 12), _mm_set1_epi32(0xF));\n\n        __m128i v_pc0 = _mm_shuffle_epi8(pc_table, v_b0);\n        __m128i v_pc1 = _mm_shuffle_epi8(pc_table, v_b1);\n        __m128i v_pc2 = _mm_shuffle_epi8(pc_table, v_b2);\n        __m128i v_pc3 = _mm_shuffle_epi8(pc_table, v_b3);\n\n        __m128i v_sum = _mm_add_epi8(v_pc0, v_pc1);\n        __m128i v_sum2 = _mm_add_epi8(v_sum, v_pc2);\n        __m128i v_result = _mm_add_epi8(v_sum2, v_pc3);\n\n        _mm_storeu_si128((__m128i*)dst, v_result);\n\n        src += 16;\n        dst += 16;\n        length -= 16;\n    }\n\n    // Handle remaining elements\n    while (length > 0) {\n        *dst = __builtin_popcount(*src);\n        src++;\n        dst++;\n        length--;\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i pc_table = _mm_set_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    while (length >= 16) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)src);\n        __m128i v_b0 = _mm_and_si128(v_src, _mm_set1_epi32(0xF));\n        __m128i v_b1 = _mm_and_si128(_mm_srli_epi32(v_src, 4), _mm_set1_epi32(0xF));\n        __m128i v_b2 = _mm_and_si128(_mm_srli_epi32(v_src, 8), _mm_set1_epi32(0xF));\n        __m128i v_b3 = _mm_and_si128(_mm_srli_epi32(v_src, 12), _mm_set1_epi32(0xF));\n\n        __m128i v_pc0 = _mm_shuffle_epi8(pc_table, v_b0);\n        __m128i v_pc1 = _mm_shuffle_epi8(pc_table, v_b1);\n        __m128i v_pc2 = _mm_shuffle_epi8(pc_table, v_b2);\n        __m128i v_pc3 = _mm_shuffle_epi8(pc_table, v_b3);\n\n        __m128i v_sum = _mm_add_epi8(v_pc0, v_pc1);\n        __m128i v_sum2 = _mm_add_epi8(v_sum, v_pc2);\n        __m128i v_result = _mm_add_epi8(v_sum2, v_pc3);\n\n        _mm_storeu_si128((__m128i*)dst, v_result);\n\n        src += 16;\n        dst += 16;\n        length -= 16;\n    }\n\n    // Handle remaining elements\n    while (length--) {\n        *dst++ = __builtin_popcount(*src++);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i pc_table = _mm_set_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    while (length >= 16) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)src);\n        __m128i v_b0 = _mm_and_si128(v_src, _mm_set1_epi32(0xF));\n        __m128i v_b1 = _mm_and_si128(_mm_srli_epi32(v_src, 4), _mm_set1_epi32(0xF));\n        __m128i v_b2 = _mm_and_si128(_mm_srli_epi32(v_src, 8), _mm_set1_epi32(0xF));\n        __m128i v_b3 = _mm_and_si128(_mm_srli_epi32(v_src, 12), _mm_set1_epi32(0xF));\n\n        __m128i v_pc0 = _mm_shuffle_epi8(pc_table, v_b0);\n        __m128i v_pc1 = _mm_shuffle_epi8(pc_table, v_b1);\n        __m128i v_pc2 = _mm_shuffle_epi8(pc_table, v_b2);\n        __m128i v_pc3 = _mm_shuffle_epi8(pc_table, v_b3);\n\n        __m128i v_sum = _mm_add_epi8(v_pc0, v_pc1);\n        __m128i v_sum2 = _mm_add_epi8(v_sum, v_pc2);\n        __m128i v_result = _mm_add_epi8(v_sum2, v_pc3);\n\n        _mm_storeu_si128((__m128i*)dst, v_result);\n\n        src += 16;\n        dst += 16;\n        length -= 16;\n    }\n\n    // Handle remaining elements\n    while (length--) {\n        *dst++ = __builtin_popcount(*src++);\n    }\n}\n```"}
{"task_id": "SimdBench_16_SSE", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m128i zero = _mm_setzero_si128();\n    __m128i pc_table = _mm_setr_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    while (length >= 16) {\n        __m128i v_src = _mm_loadu_si128((__m128i*)src);\n        __m128i v_low = _mm_and_si128(v_src, _mm_set1_epi32(0x0F0F0F0F));\n        __m128i v_high = _mm_and_si128(_mm_srli_epi32(v_src, 4), _mm_set1_epi32(0x0F0F0F0F));\n        __m128i v_count = _mm_add_epi8(\n            _mm_shuffle_epi8(pc_table, v_low),\n            _mm_shuffle_epi8(pc_table, v_high)\n        );\n        v_count = _mm_add_epi8(v_count, _mm_shuffle_epi8(pc_table, _mm_srli_epi32(v_src, 8)));\n        v_count = _mm_add_epi8(v_count, _mm_shuffle_epi8(pc_table, _mm_srli_epi32(v_src, 12)));\n        v_count = _mm_add_epi8(v_count, _mm_shuffle_epi8(pc_table, _mm_srli_epi32(v_src, 16)));\n        v_count = _mm_add_epi8(v_count, _mm_shuffle_epi8(pc_table, _mm_srli_epi32(v_src, 20)));\n        v_count = _mm_add_epi8(v_count, _mm_shuffle_epi8(pc_table, _mm_srli_epi32(v_src, 24)));\n        v_count = _mm_add_epi8(v_count, _mm_shuffle_epi8(pc_table, _mm_srli_epi32(v_src, 28)));\n\n        _mm_storeu_si128((__m128i*)dst, v_count);\n\n        src += 16;\n        dst += 16;\n        length -= 16;\n    }\n\n    // Handle remaining elements\n    while (length--) {\n        *dst++ = __builtin_popcount(*src++);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i max_val = _mm_set1_epi16(UINT16_MAX);\n\n    // Process data in chunks of 8 elements using SSE2\n    for (; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        // Perform saturating addition\n        __m128i sum = _mm_adds_epu16(v1, v2);\n\n        // Clamp to UINT16_MAX if necessary\n        __m128i result = _mm_min_epu16(sum, max_val);\n\n        // Store the result back to memory\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i max_val = _mm_set1_epi16(UINT16_MAX);\n    \n    // Process data in chunks of 8 elements using SSE2\n    for (; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(src2 + i));\n        \n        // Perform saturating addition\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        \n        // Clamp to UINT16_MAX if necessary\n        __m128i result = _mm_min_epu16(sum, max_val);\n        \n        // Store the result back to memory\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] > UINT16_MAX) ? UINT16_MAX : (src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i max_val = _mm_set1_epi16(UINT16_MAX);\n    \n    // Process data in chunks of 8 elements using SSE2\n    for (; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(src2 + i));\n        \n        // Perform saturating addition\n        __m128i sum = _mm_adds_epu16(v1, v2);\n        \n        // Clamp to UINT16_MAX if necessary\n        __m128i result = _mm_min_epu16(sum, max_val);\n        \n        // Store the result back to memory\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (src1[i] + src2[i] > UINT16_MAX) ? UINT16_MAX : (src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i max_val = _mm_set1_epi16(UINT16_MAX);\n\n    // Process data in chunks of 8 elements using SSE2\n    for (; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)(src1 + i));\n        __m128i v2 = _mm_loadu_si128((__m128i*)(src2 + i));\n\n        // Perform saturating addition\n        __m128i sum = _mm_adds_epu16(v1, v2);\n\n        // Clamp to UINT16_MAX if necessary\n        __m128i result = _mm_min_epu16(sum, max_val);\n\n        // Store the result back to memory\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_17_SSE", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i max_val = _mm_set1_epi16(UINT16_MAX);\n\n    // Process full vectors\n    for (; i <= length - 8; i += 8) {\n        __m128i v1 = _mm_loadu_si128((__m128i*)&src1[i]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&src2[i]);\n\n        __m128i sum = _mm_add_epi16(v1, v2);\n        __m128i saturated = _mm_min_epi16(sum, max_val);\n\n        _mm_storeu_si128((__m128i*)&dst[i], saturated);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = src1[i] + src2[i];\n        if (dst[i] > UINT16_MAX) {\n            dst[i] = UINT16_MAX;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a_vec = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if both elements are within the range [-100, 100]\n                __m128d mask_a = _mm_cmpgt_pd(_mm_set1_pd(100.0), a_vec);\n                __m128d mask_b = _mm_cmpgt_pd(_mm_set1_pd(100.0), b_vec);\n                __m128d mask = _mm_and_pd(mask_a, mask_b);\n\n                // If both are within range, multiply and accumulate\n                __m128d product = _mm_mul_pd(a_vec, b_vec);\n                sum = _mm_add_pd(sum, _mm_and_pd(product, mask));\n            }\n            // Horizontal sum of the two elements in sum\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            double result = _mm_cvtsd_f64(temp);\n            C[i * p + j] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a_vec = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if both elements are within the range [-100, 100]\n                __m128d mask_a = _mm_cmpgt_pd(_mm_set1_pd(100.0), a_vec);\n                __m128d mask_b = _mm_cmpgt_pd(_mm_set1_pd(100.0), b_vec);\n                __m128d mask = _mm_and_pd(mask_a, mask_b);\n\n                // If both are within range, multiply and add to sum\n                __m128d product = _mm_mul_pd(a_vec, b_vec);\n                sum = _mm_add_pd(sum, _mm_and_pd(product, mask));\n            }\n            // Horizontal sum of the two elements in sum\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            double result = _mm_cvtsd_f64(temp);\n            C[i * p + j] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a_vec = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if both elements are within the range [-100, 100]\n                __m128d mask_a = _mm_cmpgt_pd(_mm_set1_pd(100.0), a_vec);\n                __m128d mask_b = _mm_cmpgt_pd(_mm_set1_pd(100.0), b_vec);\n                __m128d mask = _mm_and_pd(mask_a, mask_b);\n\n                // If both are within range, multiply and accumulate\n                __m128d product = _mm_mul_pd(a_vec, b_vec);\n                sum = _mm_add_pd(sum, _mm_and_pd(product, mask));\n            }\n            // Horizontal sum of the two elements in sum\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            double result = _mm_cvtsd_f64(temp);\n            C[i * p + j] += result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a_vec = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n\n                // Check if both elements are within range [-100, 100]\n                __m128d mask_a = _mm_cmpgt_pd(_mm_set1_pd(100.0), a_vec);\n                __m128d mask_b = _mm_cmpgt_pd(_mm_set1_pd(100.0), b_vec);\n                __m128d mask = _mm_and_pd(mask_a, mask_b);\n\n                // If both are within range, multiply and accumulate\n                __m128d product = _mm_mul_pd(a_vec, b_vec);\n                sum = _mm_add_pd(sum, _mm_and_pd(product, mask));\n            }\n            // Horizontal sum of the two elements in sum\n            __m128d temp = _mm_hadd_pd(sum, sum);\n            double result = _mm_cvtsd_f64(temp);\n            C[i * p + j] += result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_SSE", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a_vec = _mm_loadu_pd(&A[i * n + k]);\n                __m128d b_vec = _mm_loadu_pd(&B[k * p + j]);\n                __m128d mask = _mm_cmpgt_pd(_mm_set1_pd(100.0), a_vec);\n                mask = _mm_and_pd(mask, _mm_cmpgt_pd(a_vec, _mm_set1_pd(-100.0)));\n                mask = _mm_and_pd(mask, _mm_cmpgt_pd(_mm_set1_pd(100.0), b_vec));\n                mask = _mm_and_pd(mask, _mm_cmpgt_pd(b_vec, _mm_set1_pd(-100.0)));\n                __m128d product = _mm_mul_pd(a_vec, b_vec);\n                product = _mm_and_pd(product, mask);\n                sum = _mm_add_pd(sum, product);\n            }\n            double temp[2];\n            _mm_storeu_pd(temp, sum);\n            C[i * p + j] += temp[0] + temp[1];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 2 using SSE2\n    for (; i <= total_elements - 4; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vecSum = _mm_add_epi64(vecA, vecB);\n        _mm_storeu_si128((__m128i*)&C[i], vecSum);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 2 using SSE2\n    for (; i <= total_elements - 4; i += 4) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vec_B = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vec_C = _mm_add_epi64(vec_A, vec_B);\n        _mm_storeu_si128((__m128i*)&C[i], vec_C);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 2 using SSE2\n    for (; i <= total_elements - 4; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vecSum = _mm_add_epi64(vecA, vecB);\n        _mm_storeu_si128((__m128i*)&C[i], vecSum);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    // Process elements in chunks of 2 using SSE2\n    for (; i <= total_elements - 4; i += 4) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vec_B = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i vec_C = _mm_add_epi64(vec_A, vec_B);\n        _mm_storeu_si128((__m128i*)&C[i], vec_C);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_19_SSE", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    for (; i < total_elements - 3; i += 4) {\n        __m128i a = _mm_load_si128(A_vec + i);\n        __m128i b = _mm_load_si128(B_vec + i);\n        __m128i sum = _mm_add_epi64(a, b);\n        _mm_store_si128(C_vec + i, sum);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 v1, v2, vsum, vround, vint, vfrac;\n    __m128i vfrac_int;\n\n    // Process data in chunks of 4 elements using SSE\n    for (; i <= length - 4; i += 4) {\n        v1 = _mm_loadu_ps(src1 + i);\n        v2 = _mm_loadu_ps(src2 + i);\n\n        vsum = _mm_add_ps(v1, v2);\n\n        // Add 0.5 to each element for rounding\n        vround = _mm_add_ps(vsum, _mm_set1_ps(0.5f));\n\n        // Extract integer part\n        vint = _mm_cvttps_epi32(vround);\n\n        // Calculate fractional part\n        vfrac = _mm_sub_ps(vround, _mm_cvtepi32_ps(vint));\n\n        // Check if fractional part >= 0.5 and adjust if necessary\n        vfrac_int = _mm_cvtps_epi32(_mm_cmplt_ps(vfrac, _mm_set1_ps(0.5f)));\n        vint = _mm_add_epi32(vint, _mm_castps_si128(_mm_and_ps(_mm_castsi128_ps(vfrac_int), _mm_set1_ps(1.0f))));\n\n        // Store the result back to dst\n        _mm_storeu_ps(dst + i, _mm_cvtepi32_ps(vint));\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 v1, v2, vsum, vround, vint, vfrac;\n    __m128i vfrac_int;\n\n    // Process data in chunks of 4 elements\n    for (; i <= length - 4; i += 4) {\n        v1 = _mm_loadu_ps(src1 + i);\n        v2 = _mm_loadu_ps(src2 + i);\n\n        vsum = _mm_add_ps(v1, v2);\n\n        // Add 0.5 to each element for rounding\n        vround = _mm_add_ps(vsum, _mm_set1_ps(0.5f));\n\n        // Extract integer part\n        vint = _mm_cvttps_epi32(vround);\n\n        // Calculate fractional part\n        vfrac = _mm_sub_ps(vround, _mm_cvtepi32_ps(vint));\n\n        // Check if fractional part >= 0.5 and adjust if necessary\n        vfrac_int = _mm_cvtps_epi32(_mm_cmplt_ps(vfrac, _mm_set1_ps(0.5f)));\n        vint = _mm_add_epi32(vint, _mm_castps_si128(_mm_and_ps(_mm_castsi128_ps(vfrac_int), _mm_set1_ps(1.0f))));\n\n        // Store the result back to dst\n        _mm_storeu_ps(dst + i, _mm_cvtepi32_ps(vint));\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 v1, v2, vsum, vround, vint, vfrac, vresult;\n    __m128i vfrac_i;\n\n    // Align to 4 floats boundary\n    for (; i + 4 <= length; i += 4) {\n        v1 = _mm_loadu_ps(src1 + i);\n        v2 = _mm_loadu_ps(src2 + i);\n\n        vsum = _mm_add_ps(v1, v2);\n\n        // Add 0.5 to each element for rounding\n        vround = _mm_set1_ps(0.5f);\n        vsum = _mm_add_ps(vsum, vround);\n\n        // Extract integer part\n        vint = _mm_cvttps_epi32(vsum);\n\n        // Calculate fractional part\n        vfrac = _mm_sub_ps(vsum, _mm_cvtepi32_ps(vint));\n\n        // Check if fractional part >= 0.5 for correct rounding\n        vfrac_i = _mm_cvttps_epi32(_mm_mul_ps(vfrac, _mm_set1_ps(2.0f)));\n        vresult = _mm_add_ps(vsum, _mm_cvtepi32_ps(_mm_cmplt_epi32(vfrac_i, _mm_setzero_si128())));\n\n        _mm_storeu_ps(dst + i, vresult);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 v1, v2, vsum, vround, vint, vfrac, vadd;\n    __m128i vint_i;\n\n    // Process data in chunks of 4 elements\n    for (; i <= length - 4; i += 4) {\n        v1 = _mm_loadu_ps(src1 + i);\n        v2 = _mm_loadu_ps(src2 + i);\n\n        vsum = _mm_add_ps(v1, v2);\n\n        // Add 0.5 to each element for rounding\n        vround = _mm_set1_ps(0.5f);\n        vadd = _mm_add_ps(vsum, vround);\n\n        // Extract integer part\n        vint = _mm_cvttps_epi32(vadd);\n\n        // Calculate fractional part\n        vfrac = _mm_sub_ps(vadd, _mm_cvtepi32_ps(vint));\n\n        // Check if fractional part >= 0.5 and adjust if necessary\n        __m128 mask = _mm_cmpge_ps(vfrac, _mm_set1_ps(0.5f));\n        __m128 adjusted = _mm_blendv_ps(vadd, _mm_add_ps(vadd, _mm_set1_ps(1.0f)), mask);\n\n        // Store the rounded result back into dst\n        _mm_storeu_ps(dst + i, adjusted);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_20_SSE", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    size_t i = 0;\n    __m128 v1, v2, vsum, vround, vint, vfrac;\n    __m128i vfrac_int;\n\n    // Process data in chunks of 4 elements using SSE2\n    for (; i <= length - 4; i += 4) {\n        v1 = _mm_loadu_ps(src1 + i);\n        v2 = _mm_loadu_ps(src2 + i);\n\n        vsum = _mm_add_ps(v1, v2);\n\n        // Add 0.5 to each element for rounding\n        vround = _mm_add_ps(vsum, _mm_set1_ps(0.5f));\n\n        // Extract integer part\n        vint = _mm_cvttps_epi32(vround);\n\n        // Calculate fractional part\n        vfrac = _mm_sub_ps(vround, _mm_cvtepi32_ps(vint));\n\n        // Check if fractional part >= 0.5 and adjust if necessary\n        vfrac_int = _mm_cvtps_epi32(_mm_cmplt_ps(vfrac, _mm_set1_ps(0.5f)));\n        vint = _mm_add_epi32(vint, _mm_castps_si128(_mm_and_ps(_mm_castsi128_ps(vfrac_int), _mm_set1_ps(1.0f))));\n\n        // Store the result back to memory\n        _mm_storeu_ps(dst + i, _mm_cvtepi32_ps(vint));\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = roundf(src1[i] + src2[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 minus_one_vec = _mm_set1_ps(-1.0f);\n\n    for(i = 0; i + 4 <= length; i += 4){\n        __m128 src1_vec = _mm_loadu_ps(src1 + i);\n        __m128 src2_vec = _mm_loadu_ps(src2 + i);\n        __m128 idx_mask = _mm_set_epi32(3, 2, 1, 0);\n        __m128i idx_vec = _mm_cvttps_epi32(_mm_add_ps(src1_vec, idx_mask));\n        __m128 mul_vec = _mm_mul_ps(src1_vec, src2_vec);\n\n        // Check if index is a multiple of 2\n        __m128i idx_mod = _mm_and_si128(_mm_cvttps_epi32(_mm_setzero_ps()), _mm_set1_epi32(2));\n        __m128 mask_idx = _mm_cmpneq_epi32(idx_mod, _mm_setzero_si128());\n\n        // Calculate absolute difference\n        __m128 diff_vec = _mm_sub_ps(src1_vec, src2_vec);\n        __m128 abs_diff_vec = _mm_max_ps(diff_vec, _mm_sub_ps(_mm_setzero_ps(), diff_vec));\n\n        // Check if absolute difference is within 50\n        __m128 mask_abs_diff = _mm_cmple_ps(abs_diff_vec, threshold);\n\n        // Combine all masks\n        __m128 mask = _mm_and_ps(mask_idx, mask_abs_diff);\n\n        // Apply mask to multiplication result\n        __m128 mul_result = _mm_and_ps(mul_vec, _mm_castsi128_ps(mask));\n\n        // Add -1 where mask is zero\n        __m128 final_result = _mm_add_ps(mul_result, _mm_andnot_ps(mask, minus_one_vec));\n\n        // Store the result\n        _mm_storeu_ps(dst + i, final_result);\n    }\n\n    // Handle remaining elements\n    for(; i < length; i++){\n        if((i % 2 == 0) && fabs(src1[i] - src2[i]) <= 50){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 minus_one_vec = _mm_set1_ps(-1.0f);\n    \n    for(i = 0; i + 4 <= length; i += 4){\n        __m128 src1_vec = _mm_loadu_ps(src1 + i);\n        __m128 src2_vec = _mm_loadu_ps(src2 + i);\n        __m128 indices = _mm_set_epi32(3, 2, 1, 0);\n        \n        // Check if index is a multiple of 2\n        __m128i index_check = _mm_and_si128(_mm_castps_si128(indices), _mm_set1_epi32(1));\n        int mask = _mm_movemask_epi8(_mm_cmpeq_epi32(index_check, _mm_setzero_si128()));\n        \n        // Compute the absolute difference\n        __m128 diff = _mm_sub_ps(src1_vec, src2_vec);\n        __m128 abs_diff = _mm_max_ps(diff, _mm_sub_ps(_mm_setzero_ps(), diff));\n        \n        // Compare absolute difference with threshold\n        __m128 compare_mask = _mm_cmpgt_ps(threshold, abs_diff);\n        \n        // Element-wise multiplication\n        __m128 mul_result = _mm_mul_ps(src1_vec, src2_vec);\n        \n        // Apply conditions\n        __m128 result = _mm_blendv_ps(minus_one_vec, mul_result, _mm_and_ps(compare_mask, _mm_castsi128_ps(index_check)));\n        \n        // Store the result\n        _mm_storeu_ps(dst + i, result);\n    }\n    \n    // Handle remaining elements\n    for(; i < length; i++){\n        if((i % 2 == 0) && fabs(src1[i] - src2[i]) < 50.0f){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 minus_one_vec = _mm_set1_ps(-1.0f);\n    \n    for(i = 0; i + 4 <= length; i += 4){\n        __m128 src1_vec = _mm_loadu_ps(src1 + i);\n        __m128 src2_vec = _mm_loadu_ps(src2 + i);\n        __m128 indices = _mm_set_epi32(3, 2, 1, 0);\n        \n        // Check if index is a multiple of 2\n        __m128i index_check = _mm_and_si128(_mm_castps_si128(indices), _mm_set1_epi32(1));\n        int mask = _mm_movemask_epi8(_mm_cmpeq_epi32(index_check, _mm_setzero_si128()));\n        \n        // Calculate the difference and its absolute value\n        __m128 diff = _mm_sub_ps(src1_vec, src2_vec);\n        __m128 abs_diff = _mm_max_ps(diff, _mm_sub_ps(_mm_setzero_ps(), diff));\n        \n        // Compare absolute difference with threshold\n        __m128 compare_mask = _mm_cmpgt_ps(abs_diff, threshold);\n        \n        // Combine masks\n        int combined_mask = (mask << 1) | (_mm_movemask_ps(compare_mask) >> 31);\n        \n        if(combined_mask == 0){\n            __m128 mul_result = _mm_add_ps(_mm_mul_ps(src1_vec, src2_vec), _mm_set1_ps(0.5f));\n            __m128 rounded_result = _mm_cvtepi32_ps(_mm_cvttps_epi32(mul_result));\n            _mm_storeu_ps(dst + i, rounded_result);\n        }\n        else{\n            _mm_storeu_ps(dst + i, minus_one_vec);\n        }\n    }\n    \n    for(; i < length; i++){\n        if((i % 2 == 0) && fabs(src1[i] - src2[i]) <= 50.0f){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 minus_one_vec = _mm_set1_ps(-1.0f);\n\n    for(i = 0; i + 4 <= length; i += 4){\n        __m128 src1_vec = _mm_loadu_ps(src1 + i);\n        __m128 src2_vec = _mm_loadu_ps(src2 + i);\n        __m128 idx_mask = _mm_set_epi32(3, 2, 1, 0);\n        __m128i idx_vec = _mm_cvttps_epi32(_mm_add_ps(src1_vec, idx_mask));\n        __m128 mul_vec = _mm_mul_ps(src1_vec, src2_vec);\n\n        // Check if index is a multiple of 2\n        __m128i idx_mod_2 = _mm_and_si128(_mm_cvttps_epi32(_mm_set1_ps(2.0f)), idx_vec);\n        __m128 mask_idx = _mm_cmpeq_epi32(idx_mod_2, _mm_setzero_si128());\n\n        // Calculate absolute difference\n        __m128 diff_vec = _mm_sub_ps(src1_vec, src2_vec);\n        __m128 abs_diff_vec = _mm_max_ps(diff_vec, _mm_sub_ps(_mm_setzero_ps(), diff_vec));\n        \n        // Check if absolute difference is within 50\n        __m128 mask_diff = _mm_cmpgt_ps(threshold, abs_diff_vec);\n\n        // Combine masks\n        __m128 mask = _mm_and_ps(mask_idx, mask_diff);\n\n        // Apply mask to multiplication result\n        __m128 mul_result = _mm_and_ps(mul_vec, _mm_castsi128_ps(_mm_cmpeq_epi32(_mm_setzero_si128(), _mm_castps_si128(mask))));\n\n        // Add -1 where mask is zero\n        __m128 final_vec = _mm_add_ps(mul_result, _mm_andnot_ps(mask, minus_one_vec));\n\n        // Store the result\n        _mm_storeu_ps(dst + i, final_vec);\n    }\n\n    // Handle remaining elements\n    for(; i < length; i++){\n        if((fabs(src1[i] - src2[i]) <= 50.0f) && ((int)i % 2 == 0)){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_21_SSE", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    size_t i = 0;\n    __m128 threshold = _mm_set1_ps(50.0f);\n    __m128 minus_one = _mm_set1_ps(-1.0f);\n    __m128 zero = _mm_setzero_ps();\n    \n    for(i = 0; i + 4 <= length; i += 4){\n        __m128 src1_vec = _mm_loadu_ps(src1 + i);\n        __m128 src2_vec = _mm_loadu_ps(src2 + i);\n        __m128 indices = _mm_set_epi32(3, 2, 1, 0);\n        \n        // Check if index is a multiple of 2\n        __m128i index_check = _mm_and_si128(_mm_castps_si128(indices), _mm_set1_epi32(1));\n        int mask = _mm_movemask_epi8(_mm_cmpeq_epi32(index_check, _mm_setzero_si128()));\n        \n        // Compute the difference and its absolute value\n        __m128 diff = _mm_sub_ps(src1_vec, src2_vec);\n        __m128 abs_diff = _mm_max_ps(diff, _mm_sub_ps(zero, diff));\n        \n        // Compare absolute difference with threshold\n        __m128 compare_mask = _mm_cmpgt_ps(abs_diff, threshold);\n        \n        // Combine masks: valid if index is even AND abs diff <= 50\n        int combined_mask = (mask & ~_mm_movemask_ps(compare_mask)) ^ 0xF;\n        \n        if(combined_mask == 0xF){\n            // Multiply and round up\n            __m128 mul_result = _mm_div_ps(_mm_add_ps(_mm_mul_ps(src1_vec, src2_vec), _mm_set1_ps(0.5f)), _mm_set1_ps(1.0f));\n            _mm_storeu_ps(dst + i, mul_result);\n        }\n        else{\n            _mm_storeu_ps(dst + i, minus_one);\n        }\n    }\n    \n    // Handle remaining elements\n    for(; i < length; i++){\n        if((i % 2 == 0) && fabs(src1[i] - src2[i]) <= 50){\n            dst[i] = ceilf(src1[i] * src2[i]);\n        }\n        else{\n            dst[i] = -1.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (i = 0; i < m; i += 2) {\n        for (j = 0; j < n; j += 2) {\n            __m128d mat0 = _mm_loadu_pd(mat + i * n + j);\n            __m128d mat1 = _mm_loadu_pd(mat + i * n + j + 2);\n            __m128d mat2 = _mm_loadu_pd(mat + (i + 1) * n + j);\n            __m128d mat3 = _mm_loadu_pd(mat + (i + 1) * n + j + 2);\n\n            __m128d res0 = _mm_mul_pd(mat0, scalar_vec);\n            __m128d res1 = _mm_mul_pd(mat1, scalar_vec);\n            __m128d res2 = _mm_mul_pd(mat2, scalar_vec);\n            __m128d res3 = _mm_mul_pd(mat3, scalar_vec);\n\n            _mm_storeu_pd(dst + i * n + j, res0);\n            _mm_storeu_pd(dst + i * n + j + 2, res1);\n            _mm_storeu_pd(dst + (i + 1) * n + j, res2);\n            _mm_storeu_pd(dst + (i + 1) * n + j + 2, res3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < n; j += 2) {\n            __m128d mat_vec = _mm_loadu_pd(mat + i * n + j);\n            __m128d result_vec = _mm_mul_pd(mat_vec, scalar_vec);\n            _mm_storeu_pd(dst + i * n + j, result_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128d v_scalar = _mm_set1_pd(scalar);\n\n    for (i = 0; i < m; i += 2) {\n        for (j = 0; j < n; j += 2) {\n            __m128d v_mat0 = _mm_loadu_pd(mat + i * n + j);\n            __m128d v_mat1 = _mm_loadu_pd(mat + (i + 1) * n + j);\n            __m128d v_result0 = _mm_mul_pd(v_mat0, v_scalar);\n            __m128d v_result1 = _mm_mul_pd(v_mat1, v_scalar);\n            _mm_storeu_pd(dst + i * n + j, v_result0);\n            _mm_storeu_pd(dst + (i + 1) * n + j, v_result1);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for(i = 0; i < m; i += 2){\n        for(j = 0; j < n; j += 2){\n            __m128d mat0 = _mm_loadu_pd(mat + i*n + j);\n            __m128d mat1 = _mm_loadu_pd(mat + i*n + j + 2);\n            __m128d mat2 = _mm_loadu_pd(mat + (i+1)*n + j);\n            __m128d mat3 = _mm_loadu_pd(mat + (i+1)*n + j + 2);\n\n            __m128d res0 = _mm_mul_pd(mat0, scalar_vec);\n            __m128d res1 = _mm_mul_pd(mat1, scalar_vec);\n            __m128d res2 = _mm_mul_pd(mat2, scalar_vec);\n            __m128d res3 = _mm_mul_pd(mat3, scalar_vec);\n\n            _mm_storeu_pd(dst + i*n + j, res0);\n            _mm_storeu_pd(dst + i*n + j + 2, res1);\n            _mm_storeu_pd(dst + (i+1)*n + j, res2);\n            _mm_storeu_pd(dst + (i+1)*n + j + 2, res3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_22_SSE", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128d scalar_vec = _mm_set1_pd(scalar);\n\n    for(i = 0; i < m; i += 2){\n        for(j = 0; j < n; j += 2){\n            __m128d mat0 = _mm_loadu_pd(mat + i*n + j);\n            __m128d mat1 = _mm_loadu_pd(mat + i*n + j + 2);\n            __m128d mat2 = _mm_loadu_pd(mat + (i+1)*n + j);\n            __m128d mat3 = _mm_loadu_pd(mat + (i+1)*n + j + 2);\n\n            __m128d res0 = _mm_mul_pd(mat0, scalar_vec);\n            __m128d res1 = _mm_mul_pd(mat1, scalar_vec);\n            __m128d res2 = _mm_mul_pd(mat2, scalar_vec);\n            __m128d res3 = _mm_mul_pd(mat3, scalar_vec);\n\n            _mm_storeu_pd(dst + i*n + j, res0);\n            _mm_storeu_pd(dst + i*n + j + 2, res1);\n            _mm_storeu_pd(dst + (i+1)*n + j, res2);\n            _mm_storeu_pd(dst + (i+1)*n + j + 2, res3);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 row = _mm_loadu_ps(mat + i * n);\n        __m128 row_sq = _mm_mul_ps(row, row);\n\n        // Compute partial sums\n        __m128 s1 = _mm_movehl_ps(row_sq, row_sq);       // High quad\n        __m128 s2 = _mm_movelh_ps(row_sq, row_sq);       // Low quad\n        __m128 s3 = _mm_add_ps(s1, s2);                  // Sum of high and low quads\n\n        __m128 s4 = _mm_shuffle_ps(s3, s3, _MM_SHUFFLE(2,3,0,1)); // Swap pairs\n        __m128 s5 = _mm_add_ps(s3, s4);                  // Pairwise sum\n\n        float sum[4];\n        _mm_storeu_ps(sum, s5);\n        float l2_norm = sqrtf(sum[0] + sum[1] + sum[2] + sum[3]);\n\n        if (l2_norm > 0.0f) {\n            __m128 norm_factor = _mm_set1_ps(1.0f / l2_norm);\n            for (size_t j = 4; j < n; j += 4) {\n                __m128 current_row = _mm_loadu_ps(mat + i * n + j);\n                __m128 normalized_row = _mm_mul_ps(current_row, norm_factor);\n                _mm_storeu_ps(mat + i * n + j, normalized_row);\n            }\n        }\n\n        mat += n;\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 row = _mm_loadu_ps(mat + i * n);\n        __m128 row_sq = _mm_mul_ps(row, row);\n        \n        // Compute partial sums\n        __m128 s1 = _mm_movehl_ps(row_sq, row_sq);\n        __m128 s2 = _mm_add_ps(row_sq, s1);\n        __m128 s3 = _mm_shuffle_ps(s2, s2, 1);\n        __m128 s4 = _mm_add_ss(s2, s3);\n        \n        float sum[4];\n        _mm_store_ss(sum, s4);\n        float l2_norm = sqrtf(sum[0] * n); // Assuming n elements per row\n        \n        if (l2_norm > 0.0f) {\n            __m128 norm_factor = _mm_set1_ps(1.0f / l2_norm);\n            __m128 normalized_row = _mm_mul_ps(row, norm_factor);\n            _mm_storeu_ps(mat + i * n, normalized_row);\n        } else {\n            // Handle division by zero if necessary\n            for (size_t j = 0; j < n; j++) {\n                mat[i * n + j] = 0.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 row = _mm_loadu_ps(mat + i * n);\n        __m128 row_sq = _mm_mul_ps(row, row);\n\n        // Compute partial sums\n        __m128 s1 = _mm_movehl_ps(row_sq, row_sq);       // High quad to low quad\n        __m128 s2 = _mm_add_ps(row_sq, s1);              // Add high and low quads\n        __m128 s3 = _mm_shuffle_ps(s2, s2, 1);           // Swap low and high quads\n        __m128 s4 = _mm_add_ps(s2, s3);                  // Add swapped quads\n        float sum[4];\n        _mm_storeu_ps(sum, s4);\n\n        // Handle remaining elements if n is not a multiple of 4\n        float norm = sqrtf(sum[0]);\n        if (norm == 0.0f) continue;\n\n        for (size_t j = 4; j < n; j += 4) {\n            row = _mm_loadu_ps(mat + i * n + j);\n            row_sq = _mm_mul_ps(row, row);\n            s1 = _mm_movehl_ps(row_sq, row_sq);\n            s2 = _mm_add_ps(row_sq, s1);\n            s3 = _mm_shuffle_ps(s2, s2, 1);\n            s4 = _mm_add_ps(s2, s3);\n            _mm_storeu_ps(sum, s4);\n            norm += sum[0];\n        }\n\n        norm = sqrtf(norm);\n        for (size_t j = 0; j < n; j += 4) {\n            row = _mm_loadu_ps(mat + i * n + j);\n            __m128 norm_vec = _mm_set1_ps(norm);\n            __m128 normalized = _mm_div_ps(row, norm_vec);\n            _mm_storeu_ps(mat + i * n + j, normalized);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 row = _mm_loadu_ps(mat + i * n);\n        __m128 row_sq = _mm_mul_ps(row, row);\n\n        // Compute partial sums\n        __m128 s1 = _mm_movehl_ps(row_sq, row_sq);       // High quad to low quad\n        __m128 s2 = _mm_shuffle_ps(row_sq, row_sq, 1);   // Swap quads\n        s1 = _mm_add_ps(s1, row_sq);                     // Add high and low\n        s2 = _mm_add_ps(s2, s1);                         // Add swapped quads\n\n        // Horizontal sum\n        __m128 s3 = _mm_shuffle_ps(s2, s2, 2);           // Move high to low\n        s2 = _mm_add_ss(s2, s3);                         // Add high and low\n        float sum = _mm_cvtss_f32(s2);\n\n        float norm = sqrtf(sum);\n        if (norm > 0.0f) {\n            __m128 inv_norm = _mm_set1_ps(1.0f / norm);\n            for (size_t j = 0; j < n; j += 4) {\n                __m128 data = _mm_loadu_ps(mat + i * n + j);\n                data = _mm_mul_ps(data, inv_norm);\n                _mm_storeu_ps(mat + i * n + j, data);\n            }\n        } else {\n            for (size_t j = 0; j < n; j++) {\n                mat[i * n + j] = 0.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_SSE", "completion": "```c\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    for (size_t i = 0; i < m; i++) {\n        __m128 row = _mm_loadu_ps(mat + i * n);\n        __m128 row_sq = _mm_mul_ps(row, row);\n\n        // Compute partial sums\n        __m128 s1 = _mm_movehl_ps(row_sq, row_sq);       // High quad to low quad\n        __m128 s2 = _mm_shuffle_ps(row_sq, row_sq, 1);   // Swap quads\n        __m128 sum = _mm_add_ps(row_sq, s1);\n        sum = _mm_add_ps(sum, s2);\n\n        // Horizontal sum: reduce to scalar\n        float temp[4];\n        _mm_storeu_ps(temp, sum);\n        float norm_squared = temp[0] + temp[1] + temp[2] + temp[3];\n\n        float norm = sqrtf(norm_squared);\n        if (norm > 0.0f) {\n            __m128 norm_inv = _mm_set1_ps(1.0f / norm);\n            for (size_t j = 0; j < n; j += 4) {\n                __m128 data = _mm_loadu_ps(mat + i * n + j);\n                __m128 normalized = _mm_mul_ps(data, norm_inv);\n                _mm_storeu_ps(mat + i * n + j, normalized);\n            }\n        } else {\n            // Handle division by zero if necessary\n            for (size_t j = 0; j < n; j++) {\n                mat[i * n + j] = 0.0f;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(mat1 + i * n + k);\n                __m128d b = _mm_loadu_pd(mat2 + k * p + j);\n                sum = _mm_add_pd(sum, _mm_mul_pd(a, b));\n            }\n            double s[2];\n            _mm_storeu_pd(s, sum);\n            double result = round(s[0] + s[1]);\n            dst[i * p + j] = result;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(mat1 + i * n + k);\n                __m128d b0 = _mm_loadu_pd(mat2 + k * p + j);\n                __m128d b1 = _mm_loadu_pd(mat2 + (k + 1) * p + j);\n                __m128d prod0 = _mm_mul_pd(a, b0);\n                __m128d prod1 = _mm_mul_pd(_mm_shuffle_pd(a, a, 1), b1);\n                sum = _mm_add_pd(sum, prod0);\n                sum = _mm_add_pd(sum, prod1);\n            }\n            __m128d rounded = _mm_round_pd(sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm_storeu_pd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(mat1 + i * n + k);\n                __m128d b0 = _mm_loadu_pd(mat2 + k * p + j);\n                __m128d b1 = _mm_loadu_pd(mat2 + (k + 1) * p + j);\n                __m128d prod0 = _mm_mul_pd(a, b0);\n                __m128d prod1 = _mm_mul_pd(a, b1);\n                sum = _mm_add_pd(sum, prod0);\n                sum = _mm_add_pd(sum, prod1);\n            }\n            // Horizontal sum\n            __m128d tmp = _mm_hadd_pd(sum, sum);\n            __m128d rounded = _mm_round_pd(tmp, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm_storeu_sd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a = _mm_loadu_pd(mat1 + i * n + k);\n                __m128d b0 = _mm_loadu_pd(mat2 + k * p + j);\n                __m128d b1 = _mm_loadu_pd(mat2 + (k + 1) * p + j);\n                __m128d prod0 = _mm_mul_pd(a, b0);\n                __m128d prod1 = _mm_mul_pd(a, b1);\n                sum = _mm_add_pd(sum, prod0);\n                sum = _mm_add_pd(sum, prod1);\n            }\n            // Handle remaining elements if n is odd\n            if (n % 2 != 0) {\n                __m128d a = _mm_load_sd(mat1 + i * n + n - 1);\n                __m128d b = _mm_load_sd(mat2 + (n - 1) * p + j);\n                sum = _mm_add_sd(sum, _mm_mul_sd(a, b));\n            }\n            // Horizontal sum and round\n            __m128d rounded = _mm_round_pd(sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm_store_sd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_SSE", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m128d sum = _mm_setzero_pd();\n            for (size_t k = 0; k < n; k += 2) {\n                __m128d a0 = _mm_loadu_pd(mat1 + i * n + k);\n                __m128d b0 = _mm_loadu_pd(mat2 + k * p + j);\n                __m128d prod0 = _mm_mul_pd(a0, b0);\n                sum = _mm_add_pd(sum, prod0);\n\n                if (k + 1 < n) {\n                    __m128d a1 = _mm_loadu_pd(mat1 + i * n + k + 1);\n                    __m128d b1 = _mm_loadu_pd(mat2 + (k + 1) * p + j);\n                    __m128d prod1 = _mm_mul_pd(a1, b1);\n                    sum = _mm_add_pd(sum, prod1);\n                }\n            }\n            __m128d rounded = _mm_round_pd(sum, _MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC);\n            _mm_storeu_pd(dst + i * p + j, rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 round_mask = _mm_set1_ps(0.5f);\n    __m128i zero_i = _mm_setzero_si128();\n\n    for (r = 0; r < rows; r++) {\n        size_t c;\n        for (c = 0; c + 4 <= cols; c += 4) {\n            __m128 row0 = _mm_loadu_ps(src + r * cols + c);\n            __m128 row1 = _mm_loadu_ps(src + (r + 1) * cols + c);\n            __m128 row2 = _mm_loadu_ps(src + (r + 2) * cols + c);\n            __m128 row3 = _mm_loadu_ps(src + (r + 3) * cols + c);\n\n            __m128 transposed0 = _mm_unpacklo_ps(row0, row1);\n            __m128 transposed1 = _mm_unpackhi_ps(row0, row1);\n            __m128 transposed2 = _mm_unpacklo_ps(row2, row3);\n            __m128 transposed3 = _mm_unpackhi_ps(row2, row3);\n\n            __m128 final0 = _mm_unpacklo_pd(transposed0, transposed2);\n            __m128 final1 = _mm_unpackhi_pd(transposed0, transposed2);\n            __m128 final2 = _mm_unpacklo_pd(transposed1, transposed3);\n            __m128 final3 = _mm_unpackhi_pd(transposed1, transposed3);\n\n            __m128 rounded0 = _mm_add_ps(final0, round_mask);\n            __m128 rounded1 = _mm_add_ps(final1, round_mask);\n            __m128 rounded2 = _mm_add_ps(final2, round_mask);\n            __m128 rounded3 = _mm_add_ps(final3, round_mask);\n\n            __m128 floored0 = _mm_floor_ps(rounded0);\n            __m128 floored1 = _mm_floor_ps(rounded1);\n            __m128 floored2 = _mm_floor_ps(rounded2);\n            __m128 floored3 = _mm_floor_ps(rounded3);\n\n            __m128 scaled0 = _mm_mul_ps(floored0, quarter);\n            __m128 scaled1 = _mm_mul_ps(floored1, quarter);\n            __m128 scaled2 = _mm_mul_ps(floored2, quarter);\n            __m128 scaled3 = _mm_mul_ps(floored3, quarter);\n\n            _mm_storeu_ps(dst + c * rows + r, scaled0);\n            _mm_storeu_ps(dst + c * rows + r + 4, scaled1);\n            _mm_storeu_ps(dst + c * rows + r + 8, scaled2);\n            _mm_storeu_ps(dst + c * rows + r + 12, scaled3);\n        }\n        for (; c < cols; c++) {\n            dst[r * cols + c] = round(src[c * rows + r] / 4.0f) * 0.25f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 round_mask = _mm_set1_ps(0.5f);\n    __m128i zero_i = _mm_setzero_si128();\n\n    for (r = 0; r < rows; r++) {\n        size_t c;\n        for (c = 0; c + 4 <= cols; c += 4) {\n            __m128 row0 = _mm_loadu_ps(src + r * cols + c);\n            __m128 row1 = _mm_loadu_ps(src + (r + 1) * cols + c);\n            __m128 row2 = _mm_loadu_ps(src + (r + 2) * cols + c);\n            __m128 row3 = _mm_loadu_ps(src + (r + 3) * cols + c);\n\n            __m128 transposed0 = _mm_unpacklo_ps(row0, row1);\n            __m128 transposed1 = _mm_unpackhi_ps(row0, row1);\n            __m128 transposed2 = _mm_unpacklo_ps(row2, row3);\n            __m128 transposed3 = _mm_unpackhi_ps(row2, row3);\n\n            __m128 final0 = _mm_unpacklo_ps(transposed0, transposed2);\n            __m128 final1 = _mm_unpackhi_ps(transposed0, transposed2);\n            __m128 final2 = _mm_unpacklo_ps(transposed1, transposed3);\n            __m128 final3 = _mm_unpackhi_ps(transposed1, transposed3);\n\n            __m128 rounded0 = _mm_add_ps(final0, round_mask);\n            __m128 rounded1 = _mm_add_ps(final1, round_mask);\n            __m128 rounded2 = _mm_add_ps(final2, round_mask);\n            __m128 rounded3 = _mm_add_ps(final3, round_mask);\n\n            __m128 floored0 = _mm_floor_ps(rounded0);\n            __m128 floored1 = _mm_floor_ps(rounded1);\n            __m128 floored2 = _mm_floor_ps(rounded2);\n            __m128 floored3 = _mm_floor_ps(rounded3);\n\n            __m128 scaled0 = _mm_mul_ps(floored0, quarter);\n            __m128 scaled1 = _mm_mul_ps(floored1, quarter);\n            __m128 scaled2 = _mm_mul_ps(floored2, quarter);\n            __m128 scaled3 = _mm_mul_ps(floored3, quarter);\n\n            _mm_storeu_ps(dst + c * rows + r, scaled0);\n            _mm_storeu_ps(dst + c * rows + r + 4, scaled1);\n            _mm_storeu_ps(dst + c * rows + r + 8, scaled2);\n            _mm_storeu_ps(dst + c * rows + r + 12, scaled3);\n        }\n        for (; c < cols; c++) {\n            dst[r * cols + c] = round(src[c * rows + r] / 4.0f) * 0.25f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    __m128 quarter = _mm_set1_ps(0.25f);\n    __m128 round_mask = _mm_set1_ps(0.5f);\n    \n    for (r = 0; r < rows; r++) {\n        size_t c;\n        for (c = 0; c + 4 <= cols; c += 4) {\n            __m128 col0 = _mm_loadu_ps(src + r + c * rows);\n            __m128 col1 = _mm_loadu_ps(src + r + (c + 1) * rows);\n            __m128 col2 = _mm_loadu_ps(src + r + (c + 2) * rows);\n            __m128 col3 = _mm_loadu_ps(src + r + (c + 3) * rows);\n\n            __m128 transposed0 = _mm_add_ps(col0, quarter);\n            __m128 transposed1 = _mm_add_ps(col1, quarter);\n            __m128 transposed2 = _mm_add_ps(col2, quarter);\n            __m128 transposed3 = _mm_add_ps(col3, quarter);\n\n            transposed0 = _mm_round_ps(transposed0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            transposed1 = _mm_round_ps(transposed1, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            transposed2 = _mm_round_ps(transposed2, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n            transposed3 = _mm_round_ps(transposed3, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC);\n\n            _mm_storeu_ps(dst + c + r * cols, transposed0);\n            _mm_storeu_ps(dst + c + (r + 1) * cols, transposed1);\n            _mm_storeu_ps(dst + c + (r + 2) * cols, transposed2);\n            _mm_storeu_ps(dst + c + (r + 3) * cols, transposed3);\n        }\n        for (; c < cols; c++) {\n            dst[c + r * cols] = roundf(src[r + c * rows] + 0.25f);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    size_t c;\n\n    // Ensure that the number of columns is a multiple of 4 for SIMD processing\n    size_t aligned_cols = (cols / 4) * 4;\n\n    __m128 quarter = _mm_set1_ps(0.25f);\n\n    for (r = 0; r < rows; r++) {\n        for (c = 0; c < aligned_cols; c += 4) {\n            __m128 row_vals = _mm_loadu_ps(src + r * cols + c);\n            __m128 col_vals = _mm_loadu_ps(src + c * cols + r);\n\n            // Add 0.25 to handle rounding to nearest quarter\n            __m128 rounded_row = _mm_add_ps(row_vals, quarter);\n            __m128 rounded_col = _mm_add_ps(col_vals, quarter);\n\n            // Shift right by 2 (equivalent to dividing by 4)\n            __m128 shifted_row = _mm_mul_ps(rounded_row, quarter);\n            __m128 shifted_col = _mm_mul_ps(rounded_col, quarter);\n\n            // Store the results back into the destination matrices\n            _mm_storeu_ps(dst + r * cols + c, shifted_row);\n            _mm_storeu_ps(dst + c * cols + r, shifted_col);\n        }\n\n        // Handle remaining columns if cols is not a multiple of 4\n        for (c = aligned_cols; c < cols; c++) {\n            dst[r * cols + c] = round(src[r * cols + c] * 4) / 4.0f;\n            dst[c * cols + r] = round(src[c * cols + r] * 4) / 4.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_SSE", "completion": "```c\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    size_t r;\n    size_t c;\n\n    // Ensure that the number of columns is a multiple of 4 for SIMD processing\n    size_t aligned_cols = (cols / 4) * 4;\n\n    __m128 quarter = _mm_set1_ps(0.25f);\n\n    for (r = 0; r < rows; r++) {\n        for (c = 0; c < aligned_cols; c += 4) {\n            __m128 row_vals = _mm_loadu_ps(&src[r * cols + c]);\n            __m128i rounded = _mm_cvtps_epi32(_mm_add_ps(row_vals, quarter));\n            __m128 rounded_float = _mm_cvtepi32_ps(rounded);\n            _mm_storeu_ps(&dst[c * rows + r], rounded_float);\n        }\n\n        // Handle remaining columns if cols is not a multiple of 4\n        for (size_t col = aligned_cols; col < cols; col++) {\n            dst[col * rows + r] = round(src[r * cols + col] * 4.0f) / 4.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i, j, k;\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vector_size = 4; // SSE operates on 4 floats at a time\n\n    for (i = 0; i < total_elements; i += vector_size) {\n        __m128 t1_vec = _mm_loadu_ps(&tensor1[i]);\n        __m128 t2_vec = _mm_loadu_ps(&tensor2[i]);\n        __m128 sub_vec = _mm_sub_ps(t1_vec, t2_vec);\n\n        // Round towards zero using integer conversion\n        __m128i int_vec = _mm_cvttps_epi32(sub_vec);\n        __m128 rounded_vec = _mm_cvtepi32_ps(int_vec);\n\n        _mm_storeu_ps(&dst[i], rounded_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i, j, k;\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t vector_size = 4; // SSE operates on 4 floats at a time\n\n    for (i = 0; i < total_elements; i += vector_size) {\n        __m128 t1_vec = _mm_loadu_ps(&tensor1[i]);\n        __m128 t2_vec = _mm_loadu_ps(&tensor2[i]);\n        __m128 sub_vec = _mm_sub_ps(t1_vec, t2_vec);\n\n        // Round towards zero using integer conversion\n        __m128i int_vec = _mm_cvttps_epi32(sub_vec);\n        __m128 rounded_vec = _mm_cvtepi32_ps(int_vec);\n\n        _mm_storeu_ps(&dst[i], rounded_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i;\n    __m128 zero = _mm_setzero_ps();\n    __m128 round_mask = _mm_set1_ps(0.5f);\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m128 t1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 t2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 diff = _mm_sub_ps(t1, t2);\n\n        // Round towards zero\n        __m128 sign = _mm_and_ps(_mm_cmpgt_ps(zero, diff), round_mask);\n        __m128 abs_diff = _mm_add_ps(diff, sign);\n        __m128 floored = _mm_floor_ps(abs_diff);\n        __m128 rounded = _mm_sub_ps(floored, sign);\n\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i;\n    __m128 zero = _mm_setzero_ps();\n    __m128 round_mask = _mm_set1_ps(0.5f);\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m128 t1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 t2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 diff = _mm_sub_ps(t1, t2);\n\n        // Round towards zero\n        __m128 sign = _mm_and_ps(_mm_cmpgt_ps(zero, diff), round_mask);\n        __m128 int_part = _mm_trunc_ps(_mm_add_ps(diff, sign));\n        _mm_storeu_ps(&dst[i], int_part);\n    }\n}\n```"}
{"task_id": "SimdBench_26_SSE", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    size_t i;\n    __m128 zero = _mm_setzero_ps();\n    __m128 round_mask = _mm_set1_ps(0.5f);\n\n    for (i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m128 t1 = _mm_loadu_ps(&tensor1[i]);\n        __m128 t2 = _mm_loadu_ps(&tensor2[i]);\n        __m128 diff = _mm_sub_ps(t1, t2);\n\n        // Round towards zero\n        __m128 sign = _mm_and_ps(_mm_cmpgt_ps(zero, diff), round_mask);\n        __m128 abs_diff = _mm_sub_ps(diff, sign);\n        __m128 rounded = _mm_trunc_ps(abs_diff);\n\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 2);\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < limit; j += 2) {\n            __m128d vec1 = _mm_load_pd(&mat1[i * n + j]);\n            __m128d vec2 = _mm_load_pd(&mat2[i * n + j]);\n            __m128d result = _mm_mul_pd(vec1, vec2);\n            _mm_store_pd(&dst[i * n + j], result);\n        }\n        for (; j < n; j++) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 2);\n    \n    for(i = 0; i < m; i++) {\n        for(j = 0; j < limit; j += 2) {\n            __m128d vec1 = _mm_load_pd(mat1 + i * n + j);\n            __m128d vec2 = _mm_load_pd(mat2 + i * n + j);\n            __m128d result = _mm_mul_pd(vec1, vec2);\n            _mm_store_pd(dst + i * n + j, result);\n        }\n        for(; j < n; j++) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    size_t limit = n - (n % 2);\n\n    for(i = 0; i < m; i++) {\n        for(j = 0; j < limit; j += 2) {\n            __m128d vec1 = _mm_load_pd(mat1 + i * n + j);\n            __m128d vec2 = _mm_load_pd(mat2 + i * n + j);\n            __m128d result = _mm_mul_pd(vec1, vec2);\n            _mm_store_pd(dst + i * n + j, result);\n        }\n        for(; j < n; j++) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128d v1, v2;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < n; j += 2) {\n            v1 = _mm_load_pd(mat1 + i * n + j);\n            v2 = _mm_load_pd(mat2 + i * n + j);\n            v1 = _mm_mul_pd(v1, v2);\n            _mm_store_pd(dst + i * n + j, v1);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_SSE", "completion": "```c\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128d v1, v2;\n\n    for (i = 0; i < m; i++) {\n        for (j = 0; j < n; j += 2) {\n            v1 = _mm_load_pd(mat1 + i * n + j);\n            v2 = _mm_load_pd(mat2 + i * n + j);\n            v1 = _mm_mul_pd(v1, v2);\n            _mm_store_pd(dst + i * n + j, v1);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128 sum_vec = _mm_setzero_ps();\n    __m128 vec_vec = _mm_loadu_ps(&vec[0]);\n\n    for(i = 0; i < m; i += 2){\n        __m128 res1 = _mm_setzero_ps();\n        __m128 res2 = _mm_setzero_ps();\n\n        for(j = 0; j < n; j += 4){\n            __m128 mat_row1 = _mm_loadu_ps(&mat[(i+0)*n + j]);\n            __m128 mat_row2 = _mm_loadu_ps(&mat[(i+1)*n + j]);\n\n            __m128 prod1 = _mm_mul_ps(mat_row1, vec_vec);\n            __m128 prod2 = _mm_mul_ps(mat_row2, vec_vec);\n\n            res1 = _mm_add_ps(res1, prod1);\n            res2 = _mm_add_ps(res2, prod2);\n        }\n\n        __m128 sum1 = _mm_hadd_ps(res1, res1);\n        __m128 sum2 = _mm_hadd_ps(res2, res2);\n\n        _mm_storeu_ps(&dst[i+0], sum1);\n        _mm_storeu_ps(&dst[i+1], sum2);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128 sum, vec_reg;\n\n    // Load the vector once since it's reused across all rows\n    vec_reg = _mm_loadu_ps(vec);\n\n    for(i = 0; i + 4 <= m; i += 4) {\n        for(j = 0; j < n; j += 4) {\n            __m128 mat_reg0 = _mm_loadu_ps(mat + i * n + j);\n            __m128 mat_reg1 = _mm_loadu_ps(mat + i * n + j + 4);\n            __m128 mat_reg2 = _mm_loadu_ps(mat + i * n + j + 8);\n            __m128 mat_reg3 = _mm_loadu_ps(mat + i * n + j + 12);\n\n            __m128 prod0 = _mm_mul_ps(mat_reg0, vec_reg);\n            __m128 prod1 = _mm_mul_ps(mat_reg1, vec_reg);\n            __m128 prod2 = _mm_mul_ps(mat_reg2, vec_reg);\n            __m128 prod3 = _mm_mul_ps(mat_reg3, vec_reg);\n\n            sum = _mm_add_ps(prod0, prod1);\n            sum = _mm_add_ps(sum, prod2);\n            sum = _mm_add_ps(sum, prod3);\n\n            _mm_storeu_ps(dst + i, sum);\n        }\n    }\n\n    // Handle remaining rows if m is not a multiple of 4\n    for(; i < m; i++) {\n        float temp_sum = 0.0f;\n        for(j = 0; j < n; j++) {\n            temp_sum += mat[i * n + j] * vec[j];\n        }\n        dst[i] = temp_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128 sum, vec_reg;\n\n    // Load the vector once since it's reused across all rows\n    vec_reg = _mm_loadu_ps(vec);\n\n    for(i = 0; i + 4 <= m; i += 4) {\n        for(j = 0; j < n; j += 4) {\n            __m128 mat_reg0 = _mm_loadu_ps(mat + i * n + j);\n            __m128 mat_reg1 = _mm_loadu_ps(mat + i * n + j + 4);\n            __m128 mat_reg2 = _mm_loadu_ps(mat + i * n + j + 8);\n            __m128 mat_reg3 = _mm_loadu_ps(mat + i * n + j + 12);\n\n            __m128 prod0 = _mm_mul_ps(mat_reg0, vec_reg);\n            __m128 prod1 = _mm_mul_ps(mat_reg1, vec_reg);\n            __m128 prod2 = _mm_mul_ps(mat_reg2, vec_reg);\n            __m128 prod3 = _mm_mul_ps(mat_reg3, vec_reg);\n\n            sum = _mm_add_ps(prod0, prod1);\n            sum = _mm_add_ps(sum, prod2);\n            sum = _mm_add_ps(sum, prod3);\n\n            _mm_storeu_ps(dst + i, sum);\n        }\n    }\n\n    // Handle remaining rows if m is not a multiple of 4\n    for(; i < m; i++) {\n        dst[i] = 0.0f;\n        for(j = 0; j < n; j++) {\n            dst[i] += mat[i * n + j] * vec[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128 sum_vec = _mm_setzero_ps();\n    __m128 vec_reg;\n\n    // Load vector once since it's reused across all rows\n    vec_reg = _mm_loadu_ps(vec);\n\n    for(i = 0; i < m; i += 2) {\n        __m128 row1 = _mm_loadu_ps(mat + i * n);\n        __m128 row2 = _mm_loadu_ps(mat + (i + 1) * n);\n\n        __m128 mul1 = _mm_mul_ps(row1, vec_reg);\n        __m128 mul2 = _mm_mul_ps(row2, vec_reg);\n\n        sum_vec = _mm_add_ps(sum_vec, mul1);\n        sum_vec = _mm_add_ps(sum_vec, mul2);\n\n        _mm_storeu_ps(dst + i, sum_vec);\n        sum_vec = _mm_setzero_ps();\n\n        sum_vec = _mm_add_ps(sum_vec, mul1);\n        sum_vec = _mm_add_ps(sum_vec, mul2);\n\n        _mm_storeu_ps(dst + i + 1, sum_vec);\n        sum_vec = _mm_setzero_ps();\n    }\n\n    if(m % 2 != 0) {\n        __m128 row = _mm_loadu_ps(mat + (m - 1) * n);\n        __m128 mul = _mm_mul_ps(row, vec_reg);\n        _mm_storeu_ps(dst + m - 1, mul);\n    }\n}\n```"}
{"task_id": "SimdBench_28_SSE", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    size_t i, j;\n    __m128 sum_vec = _mm_setzero_ps();\n    __m128 vec_reg;\n\n    // Load the vector once since it's reused across all rows\n    vec_reg = _mm_loadu_ps(vec);\n\n    for(i = 0; i < m; i += 2) {\n        __m128 row1 = _mm_loadu_ps(mat + i*n);\n        __m128 row2 = _mm_loadu_ps(mat + (i+1)*n);\n\n        __m128 mul1 = _mm_mul_ps(row1, vec_reg);\n        __m128 mul2 = _mm_mul_ps(row2, vec_reg);\n\n        sum_vec = _mm_add_ps(sum_vec, mul1);\n        sum_vec = _mm_add_ps(sum_vec, mul2);\n\n        if(i + 2 < m) {\n            dst[i]     = _mm_cvtss_f32(sum_vec);\n            dst[i + 1] = _mm_cvtss_f32(_mm_movehl_ps(sum_vec, sum_vec));\n            sum_vec    = _mm_shuffle_ps(sum_vec, sum_vec, _MM_SHUFFLE(3, 3, 2, 1));\n        }\n    }\n\n    if(m % 2 != 0) {\n        __m128 row = _mm_loadu_ps(mat + (m-1)*n);\n        __m128 mul = _mm_mul_ps(row, vec_reg);\n        dst[m-1] = _mm_cvtss_f32(mul);\n    }\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i, j;\n    for (i = 0; i < m; i++) {\n        for (j = 0; j + 2 <= n; j += 2) {\n            __m128d row = _mm_loadu_pd(mat + i * n + j);\n            sum = _mm_add_pd(sum, row);\n        }\n        if (j < n) {\n            double tail[2] = {0};\n            _mm_storeu_pd(tail, sum);\n            tail[0] += mat[i * n + j];\n            sum = _mm_loadu_pd(tail);\n        }\n    }\n    __m128d count = _mm_set1_pd((double)(m * n));\n    sum = _mm_div_pd(sum, count);\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i, j;\n    for (i = 0; i < m; i++) {\n        for (j = 0; j + 2 <= n; j += 2) {\n            __m128d row = _mm_loadu_pd(mat + i * n + j);\n            sum = _mm_add_pd(sum, row);\n        }\n        if (j < n) {\n            double tail[2] = {0};\n            _mm_storeu_pd(tail, sum);\n            tail[0] += mat[i * n + j];\n            sum = _mm_loadu_pd(tail);\n        }\n    }\n    __m128d count = _mm_set1_pd((double)(m * n));\n    sum = _mm_div_pd(sum, count);\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0];\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum_vec = _mm_setzero_pd();\n    size_t i, j;\n    for (i = 0; i < m; i++) {\n        for (j = 0; j + 1 < n; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(mat + i * n + j);\n            sum_vec = _mm_add_pd(sum_vec, row_vec);\n        }\n        if (n % 2 == 1 && j < n) {\n            double remaining = mat[i * n + j];\n            sum_vec.m128d_f64[0] += remaining;\n        }\n    }\n    __m128d count_vec = _mm_set1_pd((double)(m * n));\n    sum_vec = _mm_div_pd(sum_vec, count_vec);\n    double sum_array[2];\n    _mm_storeu_pd(sum_array, sum_vec);\n    return sum_array[0] + sum_array[1];\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i, j;\n    for (i = 0; i < m; i++) {\n        for (j = 0; j + 1 < n; j += 2) {\n            __m128d row = _mm_loadu_pd(mat + i * n + j);\n            sum = _mm_add_pd(sum, row);\n        }\n        if (n % 2 == 1 && j < n) {\n            double remaining = mat[i * n + j];\n            sum.m128d_f64[0] += remaining;\n        }\n    }\n    __m128d count = _mm_set1_pd((double)(m * n));\n    sum = _mm_div_pd(sum, count);\n    double result[2];\n    _mm_storeu_pd(result, sum);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_29_SSE", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m128d sum_vec = _mm_setzero_pd();\n    size_t i, j;\n    for (i = 0; i < m; i++) {\n        for (j = 0; j + 1 < n; j += 2) {\n            __m128d row_vec = _mm_loadu_pd(mat + i * n + j);\n            sum_vec = _mm_add_pd(sum_vec, row_vec);\n        }\n        if (n % 2 != 0 && j < n) {\n            double remaining = mat[i * n + j];\n            sum_vec.m128d_f64[0] += remaining;\n        }\n    }\n    __m128d count_vec = _mm_set1_pd((double)(m * n));\n    __m128d avg_vec = _mm_div_pd(sum_vec, count_vec);\n    double avg;\n    _mm_store_sd(&avg, avg_vec);\n    return avg;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r += 2) {\n        __m128i prev_row_values[cols / 4];\n        __m128i curr_row_values[cols / 4];\n        __m128i ascending_mask;\n        __m128i descending_mask;\n        __m128i valid_mask = _mm_set1_epi32(-1);\n\n        if (r + 1 >= rows) break;\n\n        // Load two rows at a time\n        for (size_t c = 0; c < cols; c += 4) {\n            prev_row_values[c / 4] = _mm_loadu_si128((__m128i*)(matrix + r * cols + c));\n            curr_row_values[c / 4] = _mm_loadu_si128((__m128i*)(matrix + (r + 1) * cols + c));\n        }\n\n        // Check directions for both rows\n        ascending_mask = _mm_set1_epi32(directions[r] ? 0 : 1);\n        descending_mask = _mm_set1_epi32(directions[r] ? 1 : 0);\n\n        for (size_t c = 0; c < cols; c += 4) {\n            __m128i prev_values = prev_row_values[c / 4];\n            __m128i curr_values = curr_row_values[c / 4];\n\n            // Compare previous values with current values\n            __m128i ascending_check = _mm_cmpgt_epi32(prev_values, curr_values);\n            __m128i descending_check = _mm_cmplt_epi32(prev_values, curr_values);\n\n            // Apply direction mask\n            ascending_check = _mm_and_si128(ascending_check, ascending_mask);\n            descending_check = _mm_and_si128(descending_check, descending_mask);\n\n            // Combine checks\n            __m128i row_check = _mm_or_si128(ascending_check, descending_check);\n\n            // Update overall validity mask\n            valid_mask = _mm_and_si128(valid_mask, row_check);\n        }\n\n        // If any element is invalid, return false\n        if (!_mm_movemask_epi8(valid_mask)) {\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m128i prev_values[4];\n    __m128i current_values[4];\n    __m128i mask_asc = _mm_set_epi32(0, -1, 0, -1);\n    __m128i mask_desc = _mm_set_epi32(-1, 0, -1, 0);\n\n    for (size_t r = 0; r < rows; r++) {\n        if (directions[r]) {\n            _mm_prefetch((const char*)&matrix[(r + 1) * cols], _MM_HINT_T0);\n        } else {\n            _mm_prefetch((const char*)&matrix[(r + 1) * cols], _MM_HINT_T0);\n        }\n\n        for (size_t c = 0; c < cols; c += 4) {\n            __m128i current = _mm_loadu_si128((__m128i*)&matrix[r * cols + c]);\n\n            if (c == 0) {\n                for (int i = 0; i < 4; i++) {\n                    prev_values[i] = current;\n                }\n            } else {\n                for (int i = 0; i < 4; i++) {\n                    current_values[i] = current;\n                    __m128i cmp = _mm_cmpgt_epi32(prev_values[i], current_values[i]);\n                    if (!_mm_movemask_epi8(cmp)) continue;\n                    return false;\n                }\n            }\n\n            if (c + 4 >= cols) break;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r += 2) {\n        __m128i prev_values[cols / 4];\n        __m128i curr_values[cols / 4];\n        __m128i mask_asc = _mm_set1_epi32(0x7FFFFFFF);\n        __m128i mask_desc = _mm_set1_epi32(0x80000000);\n\n        bool row_valid = true;\n\n        for (size_t c = 0; c < cols && row_valid; c += 4) {\n            __m128i prev = _mm_loadu_si128((__m128i*)&matrix[r * cols + c]);\n            __m128i curr = _mm_loadu_si128((__m128i*)&matrix[(r + 1) * cols + c]);\n\n            if (directions[r]) {\n                prev_values[c / 4] = _mm_and_si128(prev, mask_asc);\n                curr_values[c / 4] = _mm_and_si128(curr, mask_asc);\n            } else {\n                prev_values[c / 4] = _mm_and_si128(prev, mask_desc);\n                curr_values[c / 4] = _mm_and_si128(curr, mask_desc);\n            }\n        }\n\n        for (size_t c = 0; c < cols / 4 && row_valid; c++) {\n            __m128i diff_asc = _mm_cmpgt_epi32(curr_values[c], prev_values[c]);\n            __m128i diff_desc = _mm_cmplt_epi32(curr_values[c], prev_values[c]);\n\n            if (directions[r]) {\n                row_valid &= _mm_movemask_epi8(diff_asc) == 0xFFF;\n            } else {\n                row_valid &= _mm_movemask_epi8(diff_desc) == 0xFFF;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t r = 0; r < rows; r += 2) {\n        __m128i prev_row_a = _mm_setzero_si128();\n        __m128i prev_row_b = _mm_setzero_si128();\n        bool row_a_sorted = true;\n        bool row_b_sorted = true;\n\n        if (r + 1 < rows) {\n            for (size_t c = 0; c < cols; c += 4) {\n                __m128i curr_row_a = _mm_loadu_si128((__m128i*)&matrix[(r + 0) * cols + c]);\n                __m128i curr_row_b = _mm_loadu_si128((__m128i*)&matrix[(r + 1) * cols + c]);\n\n                if (directions[r + 0]) {\n                    row_a_sorted &= _mm_movemask_epi8(_mm_cmpgt_epi32(prev_row_a, curr_row_a)) == 0;\n                    row_b_sorted &= _mm_movemask_epi8(_mm_cmpgt_epi32(prev_row_b, curr_row_b)) == 0;\n                } else {\n                    row_a_sorted &= _mm_movemask_epi8(_mm_cmplt_epi32(prev_row_a, curr_row_a)) == 0;\n                    row_b_sorted &= _mm_movemask_epi8(_mm_cmplt_epi32(prev_row_b, curr_row_b)) == 0;\n                }\n\n                prev_row_a = curr_row_a;\n                prev_row_b = curr_row_b;\n            }\n        } else {\n            for (size_t c = 0; c < cols; c += 4) {\n                __m128i curr_row_a = _mm_loadu_si128((__m128i*)&matrix[(r + 0) * cols + c]);\n\n                if (directions[r + 0]) {\n                    row_a_sorted &= _mm_movemask_epi8(_mm_cmpgt_epi32(prev_row_a, curr_row_a)) == 0;\n                } else {\n                    row_a_sorted &= _mm_movemask_epi8(_mm_cmplt_epi32(prev_row_a, curr_row_a)) == 0;\n                }\n\n                prev_row_a = curr_row_a;\n            }\n        }\n\n        if (!row_a_sorted || !row_b_sorted) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_SSE", "completion": "```c\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    for (size_t i = 0; i < rows; i++) {\n        __m128i prev_vec = _mm_setzero_si128();\n        bool sorted = true;\n        for (size_t j = 0; j + 4 <= cols; j += 4) {\n            __m128i current_vec = _mm_loadu_si128((__m128i*)&matrix[i * cols + j]);\n            __m128i compare_result;\n            if (directions[i]) {\n                compare_result = _mm_cmpgt_epi32(prev_vec, current_vec);\n            } else {\n                compare_result = _mm_cmplt_epi32(prev_vec, current_vec);\n            }\n            int mask = _mm_movemask_epi8(compare_result);\n            if (mask != 0) {\n                sorted = false;\n                break;\n            }\n            prev_vec = current_vec;\n        }\n        if (!sorted) continue;\n        // Handle remaining elements that don't fit into SIMD\n        for (size_t j = cols - (cols % 4); j < cols; j++) {\n            if (directions[i] && matrix[i * cols + j] < prev_vec.m128i_i32[0]) {\n                sorted = false;\n                break;\n            }\n            if (!directions[i] && matrix[i * cols + j] > prev_vec.m128i_i32[0]) {\n                sorted = false;\n                break;\n            }\n            prev_vec.m128i_i32[0] = matrix[i * cols + j];\n        }\n        if (!sorted) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m128i base_vec = _mm_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 8 elements using SSE2\n    for (; i <= length - 8; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);\n        __m128i floor_vec = _mm_floor_ps(_mm_cvtepi32_ps(div_vec));\n        __m128i mul_vec = _mm_mullo_epi16(floor_vec, base_vec);\n        _mm_storeu_si128((__m128i*)&dst[i], mul_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((src[i] / base) * base));\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m128i base_vec = _mm_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 8 elements\n    for (; i <= length - 8; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i div_vec = _mm_div_epi16(src_vec, base_vec);\n        __m128i floor_vec = _mm_floordiv_epi16(div_vec);\n        __m128i result_vec = _mm_mullo_epi16(floor_vec, base_vec);\n        \n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((int32_t)src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m128i base_vec = _mm_set1_epi16((int16_t)base);\n    \n    // Process 8 elements per iteration\n    for (; i <= length - 8; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i div_result = _mm_div_epi16(src_vec, base_vec);\n        __m128i floor_result = _mm_floordiv_epi16(div_result);\n        __m128i mul_result = _mm_mullo_epi16(floor_result, base_vec);\n        _mm_storeu_si128((__m128i*)&dst[i], mul_result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((int32_t)src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m128i base_vec = _mm_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 8 elements using SSE2\n    for (; i <= length - 8; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i div_result = _mm_div_epi16(src_vec, base_vec);\n        __m128i floored = _mm_floor_sd(div_result); // Simulate floor operation\n        __m128i mul_result = _mm_mullo_epi16(floored, base_vec);\n        _mm_storeu_si128((__m128i*)&dst[i], mul_result);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((int32_t)src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_31_SSE", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    size_t i = 0;\n    __m128i base_vec = _mm_set1_epi16((int16_t)base);\n    \n    // Process data in chunks of 8 elements using SSE2\n    for (; i <= length - 8; i += 8) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i div_result = _mm_div_epi16(src_vec, base_vec);\n        __m128i floored = _mm_mullo_epi16(div_result, base_vec);\n        \n        // Store the result back to dst\n        _mm_storeu_si128((__m128i*)&dst[i], floored);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = (int16_t)(((int32_t)src[i] / base) * base);\n    }\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec = _mm_load_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d row = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d eq = _mm_cmpeq_pd(row, vec);\n            int mask = _mm_movemask_pd(eq);\n            if (!mask) continue;\n            match = false;\n            break;\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec = _mm_load_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d row = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d eq = _mm_cmpeq_pd(row, vec);\n            int mask = _mm_movemask_pd(eq);\n            if (!mask) continue;\n            match = false;\n            break;\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec = _mm_load_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d row = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d eq = _mm_cmpeq_pd(row, vec);\n            int mask = _mm_movemask_pd(eq);\n            if (!mask) continue;\n            match = false;\n            break;\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec = _mm_load_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d row = _mm_load_pd(matrix + i * cols + j);\n            __m128d eq = _mm_cmpeq_pd(row, vec);\n            int mask = _mm_movemask_pd(eq);\n            if (!mask) continue;\n            match = false;\n            break;\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_SSE", "completion": "```c\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m128d vec = _mm_load_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        bool match = true;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d row = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d eq = _mm_cmpeq_pd(row, vec);\n            int mask = _mm_movemask_pd(eq);\n            if (!mask) continue;\n            match = false;\n            break;\n        }\n        if (match) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Calculate the total number of elements\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n\n    // Process elements in chunks of 16 using SSE/SSE2\n    for (; i <= total_elements - 16; i += 16) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vecB = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i eq_mask = _mm_cmpeq_epi8(vecA, vecB);\n        size_t mask = _mm_movemask_epi8(eq_mask);\n        total_count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Calculate the total number of elements\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n\n    // Process elements in chunks of 16 using SSE/SSE2\n    for (; i <= total_elements - 16; i += 16) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vec_B = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i eq_mask = _mm_cmpeq_epi8(vec_A, vec_B);\n        size_t eq_count = _mm_popcnt_u64(_mm_movemask_epi8(eq_mask));\n        total_count += eq_count;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Compute the total number of elements\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n\n    // Process elements in chunks of 16 using SSE/SSE2\n    for (; i <= total_elements - 16; i += 16) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vec_B = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i eq_mask = _mm_cmpeq_epi8(vec_A, vec_B);\n        size_t mask = _mm_movemask_epi8(eq_mask);\n        total_count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Compute the total number of elements\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n\n    // Process elements in chunks of 16 using SSE/SSE2\n    for (; i <= total_elements - 16; i += 16) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vec_B = _mm_loadu_si128((__m128i*)(B + i));\n        __m128i eq_mask = _mm_cmpeq_epi8(vec_A, vec_B);\n        size_t eq_count = _mm_popcnt_u64(_mm_movemask_epi8(eq_mask));\n        total_count += eq_count;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_SSE", "completion": "```c\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Compute the total number of elements\n    size_t total_elements = dims[0] * dims[1] * dims[2] * dims[3];\n\n    // Process elements in chunks of 16 using SSE/SSE2\n    for (; i <= total_elements - 16; i += 16) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i vec_B = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i eq_mask = _mm_cmpeq_epi8(vec_A, vec_B);\n        size_t mask = _mm_movemask_epi8(eq_mask);\n        total_count += __builtin_popcount(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; i++) {\n        if (A[i] == B[i]) {\n            total_count++;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i factor = _mm_set1_epi64x(a);\n    \n    for (; i <= length - 2; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vec_ax = _mm_mul_epi64(vec_x, factor);\n        __m128i vec_abs = _mm_abs_epi64(vec_ax);\n        __m128i vec_sub = _mm_sub_epi64(vec_abs, vec_ax);\n        _mm_storeu_si128((__m128i*)&y[i], vec_sub);\n    }\n    \n    for (; i < length; i++) {\n        y[i] = abs((int64_t)(a * x[i])) - (int64_t)(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i factor = _mm_set1_epi64x(a);\n    \n    // Process full chunks of 2 elements each\n    for (; i <= length - 4; i += 4) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vec_ax = _mm_mul_epi64(vec_x, factor);\n        __m128i vec_abs = _mm_abs_epi64(vec_ax);\n        __m128i vec_sub = _mm_sub_epi64(vec_abs, vec_ax);\n        _mm_storeu_si128((__m128i*)&y[i], vec_sub);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        y[i] = abs((int64_t)(a * x[i])) - (int64_t)(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i factor = _mm_set1_epi64x(a);\n    \n    for (; i <= length - 2; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vec_ax = _mm_mullo_epi64(vec_x, factor);\n        __m128i vec_abs = _mm_abs_epi64(vec_ax);\n        __m128i vec_sub = _mm_sub_epi64(vec_abs, vec_ax);\n        _mm_storeu_si128((__m128i*)&y[i], vec_sub);\n    }\n    \n    for (; i < length; i++) {\n        y[i] = abs((int64_t)(a * x[i])) - (int64_t)(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i factor = _mm_set1_epi64x(a);\n    \n    for (; i <= length - 2; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vec_ax = _mm_mul_epi64(vec_x, factor);\n        __m128i vec_abs = _mm_abs_epi64(vec_ax);\n        __m128i vec_sub = _mm_sub_epi64(vec_abs, vec_ax);\n        _mm_storeu_si128((__m128i*)&y[i], vec_sub);\n    }\n    \n    for (; i < length; i++) {\n        y[i] = abs((int64_t)(a * x[i])) - (int64_t)(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_34_SSE", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    size_t i = 0;\n    __m128i factor = _mm_set1_epi64x(a);\n    \n    // Process full vectors\n    for (; i <= length - 2; i += 2) {\n        __m128i vec_x = _mm_loadu_si128((__m128i*)&x[i]);\n        __m128i vec_ax = _mm_mul_epi64(vec_x, factor);\n        __m128i vec_abs = _mm_abs_epi64(vec_ax);\n        __m128i vec_sub = _mm_sub_epi64(vec_abs, vec_ax);\n        _mm_storeu_si128((__m128i*)&y[i], vec_sub);\n    }\n    \n    // Handle remaining elements\n    for (; i < length; i++) {\n        y[i] = abs((int64_t)(a * x[i])) - (int64_t)(a * x[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i max_val = _mm_set1_epi16(vec[0]);\n    __m128i min_val = _mm_set1_epi16(vec[0]);\n\n    size_t i = 0;\n    while (i + 8 <= length) {\n        __m128i v = _mm_loadu_si128((__m128i*)&vec[i]);\n        __m128i max_temp = _mm_max_epi16(max_val, v);\n        __m128i min_temp = _mm_min_epi16(min_val, v);\n        max_val = max_temp;\n        min_val = min_temp;\n        i += 8;\n    }\n\n    int16_t max_array[8];\n    int16_t min_array[8];\n    _mm_storeu_si128((__m128i*)max_array, max_val);\n    _mm_storeu_si128((__m128i*)min_array, min_val);\n\n    int16_t global_max = max_array[0];\n    int16_t global_min = min_array[0];\n\n    for (size_t j = 1; j < 8; ++j) {\n        if (max_array[j] > global_max) global_max = max_array[j];\n        if (min_array[j] < global_min) global_min = min_array[j];\n    }\n\n    for (; i < length; ++i) {\n        if (vec[i] > global_max) global_max = vec[i];\n        if (vec[i] < global_min) global_min = vec[i];\n    }\n\n    MinMaxPair result = {global_min, global_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i max_val = _mm_set1_epi16(vec[0]);\n    __m128i min_val = _mm_set1_epi16(vec[0]);\n\n    size_t i = 0;\n    while (i + 8 <= length) {\n        __m128i v = _mm_loadu_si128((__m128i*)&vec[i]);\n        __m128i v_min = _mm_min_epi16(v, min_val);\n        __m128i v_max = _mm_max_epi16(v, max_val);\n        min_val = v_min;\n        max_val = v_max;\n        i += 8;\n    }\n\n    __m128i mask = _mm_setr_epi32(0, 0, 0, -1);\n    __m128i tail_min = min_val;\n    __m128i tail_max = max_val;\n\n    if (length % 8 != 0) {\n        __m128i tail_vec = _mm_loadu_si128((__m128i*)&vec[i]);\n        tail_min = _mm_shuffle_epi32(_mm_min_epi16(tail_vec, min_val), 0);\n        tail_max = _mm_shuffle_epi32(_mm_max_epi16(tail_vec, max_val), 0);\n    }\n\n    int16_t final_min = _mm_extract_epi16(tail_min, 0);\n    int16_t final_max = _mm_extract_epi16(tail_max, 0);\n\n    MinMaxPair result = {final_min, final_max};\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i max_val = _mm_set1_epi16(vec[0]);\n    __m128i min_val = _mm_set1_epi16(vec[0]);\n\n    size_t i = 0;\n    while (i + 8 <= length) {\n        __m128i v = _mm_loadu_si128((__m128i*)&vec[i]);\n        __m128i max_temp = _mm_max_epi16(max_val, v);\n        __m128i min_temp = _mm_min_epi16(min_val, v);\n        max_val = max_temp;\n        min_val = min_temp;\n        i += 8;\n    }\n\n    int16_t max_array[8];\n    int16_t min_array[8];\n    _mm_storeu_si128((__m128i*)max_array, max_val);\n    _mm_storeu_si128((__m128i*)min_array, min_val);\n\n    int16_t global_max = max_array[0];\n    int16_t global_min = min_array[0];\n\n    for (size_t j = 1; j < 8; ++j) {\n        if (max_array[j] > global_max) global_max = max_array[j];\n        if (min_array[j] < global_min) global_min = min_array[j];\n    }\n\n    MinMaxPair result;\n    result.max_num = global_max;\n    result.min_num = global_min;\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i max_val = _mm_set1_epi16(vec[0]);\n    __m128i min_val = _mm_set1_epi16(vec[0]);\n\n    size_t i = 0;\n    while (i + 8 <= length) {\n        __m128i v = _mm_loadu_si128((__m128i*)&vec[i]);\n        __m128i v_max = _mm_max_epi16(v, max_val);\n        __m128i v_min = _mm_min_epi16(v, min_val);\n        max_val = v_max;\n        min_val = v_min;\n        i += 8;\n    }\n\n    int16_t max_result = max_val.m128i_i16[0];\n    int16_t min_result = min_val.m128i_i16[0];\n\n    for (; i < length; ++i) {\n        if (vec[i] > max_result) max_result = vec[i];\n        if (vec[i] < min_result) min_result = vec[i];\n    }\n\n    return (MinMaxPair){min_result, max_result};\n}\n```"}
{"task_id": "SimdBench_35_SSE", "completion": "```c\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i min_val = _mm_set1_epi16(vec[0]);\n    __m128i max_val = _mm_set1_epi16(vec[0]);\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i v = _mm_loadu_si128((__m128i*)&vec[i]);\n        __m128i vmin = _mm_min_epi16(v, min_val);\n        __m128i vmax = _mm_max_epi16(v, max_val);\n        min_val = vmin;\n        max_val = vmax;\n    }\n\n    int16_t min_num = min_val.m128i_i16[0];\n    int16_t max_num = max_val.m128i_i16[0];\n\n    for (; i < length; ++i) {\n        if (vec[i] < min_num) min_num = vec[i];\n        if (vec[i] > max_num) max_num = vec[i];\n    }\n\n    return (MinMaxPair){min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    __m128i prev_row = _mm_setzero_si128();\n    __m128i current_row;\n    __m128i cmp_mask;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m128i row_data = _mm_loadu_si128((__m128i*)row_ptr);\n\n        // Compare each element with the previous one\n        cmp_mask = _mm_cmpgt_epi32(prev_row, row_data);\n\n        // Check if any comparison failed\n        if (_mm_movemask_epi8(cmp_mask) != 0) {\n            return false;\n        }\n\n        prev_row = row_data;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    bool result = true;\n    size_t i = 0;\n\n    while (i + 1 < rows && result) {\n        const int* row1 = matrix + i * cols;\n        const int* row2 = matrix + (i + 1) * cols;\n\n        __m128i prev = _mm_setzero_si128();\n        __m128i curr = _mm_loadu_si128((__m128i*)(row2));\n        __m128i next = _mm_loadu_si128((__m128i*)(row1));\n\n        __m128i gt_mask = _mm_cmpgt_epi32(next, prev);\n        __m128i gt_curr_mask = _mm_cmpgt_epi32(curr, next);\n\n        __m128i valid_mask = _mm_and_si128(gt_mask, gt_curr_mask);\n\n        result = _mm_movemask_epi8(valid_mask) == 0xFFFF;\n\n        i++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0)\n        return false;\n\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i one = _mm_set1_epi32(1);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m128i prev_values = _mm_loadu_si128((__m128i*)(row_ptr));\n        bool valid = true;\n\n        for (size_t j = 1; j < cols; j += 4) {\n            __m128i current_values = _mm_loadu_si128((__m128i*)(row_ptr + j));\n            __m128i cmp_gt = _mm_cmpgt_epi32(prev_values, current_values);\n            if (!_mm_testz_si128(cmp_gt, cmp_gt)) {\n                valid = false;\n                break;\n            }\n            prev_values = current_values;\n        }\n\n        if (!valid)\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    __m128i prev_row = _mm_setzero_si128();\n    __m128i current_row;\n    __m128i mask;\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m128i current_row = _mm_loadu_si128((__m128i*)row_ptr);\n\n        // Compare each element with the previous one\n        mask = _mm_cmpgt_epi32(prev_row, current_row);\n\n        // If any element is greater than the next, return false\n        if (_mm_movemask_epi8(mask) != 0) {\n            return false;\n        }\n\n        prev_row = current_row;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_SSE", "completion": "```c\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) return false;\n\n    __m128i prev_row = _mm_setzero_si128();\n    __m128i current_row;\n    __m128i mask = _mm_set_epi32(3, 2, 1, 0);\n\n    for (size_t i = 0; i < rows; ++i) {\n        const int* row_ptr = matrix + i * cols;\n        __m128i current_row = _mm_loadu_si128((__m128i*)row_ptr);\n\n        // Shuffle to compare each element with the previous one\n        __m128i prev_shuffle = _mm_shuffle_epi32(prev_row, _MM_SHUFFLE(3, 2, 1, 0));\n        __m128i current_shuffle = _mm_shuffle_epi32(current_row, _MM_SHUFFLE(2, 1, 0, 3));\n\n        // Compare current element with the previous one\n        __m128i cmp_gt = _mm_cmpgt_epi32(prev_shuffle, current_shuffle);\n\n        // Check if any comparison failed\n        unsigned int result;\n        _mm_store_ss((float*)&result, _mm_castsi128_ps(cmp_gt));\n        if (result != 0) return false;\n\n        prev_row = current_row;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_size_bytes = block_size * sizeof(int);\n    size_t block_size_simd = block_size_bytes / sizeof(__m128i);\n    \n    for (size_t i = 1; i < num_blocks_row; ++i) {\n        for (size_t j = 1; j < num_blocks_col; ++j) {\n            const __m128i* block_ref = (__m128i*)(matrix + i * block_size * cols + j * block_size);\n            const __m128i* block_curr = (__m128i*)(matrix);\n            for (size_t k = 0; k < block_size_simd; ++k) {\n                __m128i ref = _mm_loadu_si128(block_ref + k);\n                __m128i curr = _mm_loadu_si128(block_curr + k);\n                __m128i eq_mask = _mm_cmpeq_epi32(ref, curr);\n                if (!_mm_movemask_epi8(eq_mask)) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_blocks_row = cols / block_size;\n    size_t num_blocks_col = rows / block_size;\n    size_t block_size_bytes = block_size * sizeof(int);\n    size_t block_size_simd = block_size / 4; // Assuming 4 integers per SIMD register\n\n    for (size_t i = 1; i < num_blocks_col; ++i) {\n        for (size_t j = 0; j < num_blocks_row; ++j) {\n            __m128i ref_block = _mm_loadu_si128((__m128i*)(matrix + i * block_size * cols + j * block_size));\n            __m128i curr_block = _mm_loadu_si128((__m128i*)(matrix + (i - 1) * block_size * cols + j * block_size));\n            __m128i eq_mask = _mm_cmpeq_epi32(ref_block, curr_block);\n            if (!_mm_movemask_epi8(eq_mask)) {\n                return false;\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_size_bytes = block_size * sizeof(int);\n    size_t block_size_simd = block_size_bytes / sizeof(__m128i);\n    \n    for (size_t i = 1; i < num_blocks_row; ++i) {\n        for (size_t j = 1; j < num_blocks_col; ++j) {\n            const int* current_block = matrix + i * block_size * cols + j * block_size;\n            const int* ref_block = matrix;\n            \n            for (size_t k = 0; k < block_size; k += 4) {\n                __m128i ref = _mm_loadu_si128((__m128i*)(ref_block + k));\n                __m128i curr = _mm_loadu_si128((__m128i*)(current_block + k));\n                __m128i eq = _mm_cmpeq_epi32(ref, curr);\n                if (!_mm_movemask_epi8(eq)) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_blocks_row = rows / block_size;\n    size_t num_blocks_col = cols / block_size;\n    size_t block_size_bytes = block_size * sizeof(int);\n    size_t block_size_simd = block_size_bytes / sizeof(__m128i);\n    \n    for (size_t i = 1; i < num_blocks_row; ++i) {\n        for (size_t j = 1; j < num_blocks_col; ++j) {\n            const __m128i* block_ref = (__m128i*)(matrix + i * block_size * cols + j * block_size);\n            const __m128i* block_curr = (__m128i*)(matrix);\n            for (size_t k = 0; k < block_size_simd; ++k) {\n                __m128i ref = _mm_loadu_si128(block_ref + k);\n                __m128i curr = _mm_loadu_si128(block_curr + k);\n                __m128i eq_mask = _mm_cmpeq_epi32(ref, curr);\n                if (!_mm_movemask_epi8(eq_mask)) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_SSE", "completion": "```c\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (rows % block_size != 0 || cols % block_size != 0) {\n        return false;\n    }\n    size_t num_rows = rows / block_size;\n    size_t num_cols = cols / block_size;\n    __m128i first_block[4] = { _mm_setzero_si128(), _mm_setzero_si128(),\n                                _mm_setzero_si128(), _mm_setzero_si128() };\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < block_size; ++j) {\n            size_t idx = i * cols + j;\n            first_block[i] = _mm_add_epi32(first_block[i], _mm_set1_epi32(matrix[idx]));\n        }\n    }\n    for (size_t r = 1; r < num_rows; ++r) {\n        for (size_t c = 1; c < num_cols; ++c) {\n            __m128i current_block[4] = { _mm_setzero_si128(), _mm_setzero_si128(),\n                                          _mm_setzero_si128(), _mm_setzero_si128() };\n            for (size_t i = 0; i < block_size; ++i) {\n                for (size_t j = 0; j < block_size; ++j) {\n                    size_t idx = (r * block_size + i) * cols + (c * block_size + j);\n                    current_block[i] = _mm_add_epi32(current_block[i], _mm_set1_epi32(matrix[idx]));\n                }\n            }\n            for (size_t k = 0; k < block_size; ++k) {\n                if (!_mm_cmpeq_epi32(first_block[k], current_block[k]).m128i_i32[0]) {\n                    return false;\n                }\n            }\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 2 != 0) return false;\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 0; i < num_blocks - 1; ++i) {\n        size_t offset = i * block_size;\n        __m128d block0 = _mm_loadu_pd(vec + offset);\n        __m128d block1 = _mm_loadu_pd(vec + offset + 2);\n\n        __m128d diff1 = _mm_sub_pd(block0, block1);\n        __m128d abs_diff1 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff1), diff1);\n\n        __m128d cmp1 = _mm_cmple_pd(abs_diff1, tol);\n\n        __m128d block2 = _mm_loadu_pd(vec + offset + 4);\n        __m128d block3 = _mm_loadu_pd(vec + offset + 6);\n\n        __m128d diff2 = _mm_sub_pd(block2, block3);\n        __m128d abs_diff2 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff2), diff2);\n\n        __m128d cmp2 = _mm_cmple_pd(abs_diff2, tol);\n\n        __m128d and_result = _mm_and_pd(cmp1, cmp2);\n\n        int mask = _mm_movemask_pd(and_result);\n        if (mask != 0b11) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 2 != 0) return false;\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 0; i < num_blocks - 1; ++i) {\n        size_t offset = i * block_size;\n        __m128d block0 = _mm_loadu_pd(vec + offset);\n        __m128d block1 = _mm_loadu_pd(vec + offset + 2);\n\n        __m128d diff1 = _mm_sub_pd(block0, block1);\n        __m128d abs_diff1 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff1), diff1);\n\n        __m128d cmp1 = _mm_cmple_pd(abs_diff1, tol);\n\n        __m128d block2 = _mm_loadu_pd(vec + offset + 4);\n        __m128d block3 = _mm_loadu_pd(vec + offset + 6);\n\n        __m128d diff2 = _mm_sub_pd(block2, block3);\n        __m128d abs_diff2 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff2), diff2);\n\n        __m128d cmp2 = _mm_cmple_pd(abs_diff2, tol);\n\n        __m128d and_result = _mm_and_pd(cmp1, cmp2);\n\n        int mask = _mm_movemask_pd(and_result);\n        if (mask != 0b11) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 2 != 0) return false;\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 0; i < num_blocks - 1; ++i) {\n        size_t offset = i * block_size;\n        __m128d block0 = _mm_loadu_pd(vec + offset);\n        __m128d block1 = _mm_loadu_pd(vec + offset + 2);\n\n        __m128d diff1 = _mm_sub_pd(block0, block1);\n        __m128d abs_diff1 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff1), diff1);\n\n        __m128d cmp1 = _mm_cmple_pd(abs_diff1, tol);\n\n        __m128d block2 = _mm_loadu_pd(vec + offset + 4);\n        __m128d block3 = _mm_loadu_pd(vec + offset + 6);\n\n        __m128d diff2 = _mm_sub_pd(block2, block3);\n        __m128d abs_diff2 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff2), diff2);\n\n        __m128d cmp2 = _mm_cmple_pd(abs_diff2, tol);\n\n        __m128d and_result = _mm_and_pd(cmp1, cmp2);\n\n        int mask = _mm_movemask_pd(and_result);\n        if (mask != 0b11) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 2 != 0) return false;\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    size_t num_blocks = length / block_size;\n\n    for (size_t i = 0; i < num_blocks - 1; ++i) {\n        size_t offset = i * block_size;\n        __m128d block0 = _mm_loadu_pd(vec + offset);\n        __m128d block1 = _mm_loadu_pd(vec + offset + 2);\n\n        __m128d diff1 = _mm_sub_pd(block0, block1);\n        __m128d abs_diff1 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff1), diff1);\n\n        __m128d cmp1 = _mm_cmple_pd(abs_diff1, tol);\n\n        __m128d block2 = _mm_loadu_pd(vec + offset + 4);\n        __m128d block3 = _mm_loadu_pd(vec + offset + 6);\n\n        __m128d diff2 = _mm_sub_pd(block2, block3);\n        __m128d abs_diff2 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff2), diff2);\n\n        __m128d cmp2 = _mm_cmple_pd(abs_diff2, tol);\n\n        __m128d and_result = _mm_and_pd(cmp1, cmp2);\n\n        int mask = _mm_movemask_pd(and_result);\n        if (mask != 0b11) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_SSE", "completion": "```c\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (vec == NULL || length < block_size || block_size % 2 != 0) return false;\n\n    __m128d tol = _mm_set1_pd(tolerance);\n    size_t i = 0;\n\n    while (i + block_size <= length) {\n        __m128d block1 = _mm_loadu_pd(&vec[i]);\n        __m128d block2 = _mm_loadu_pd(&vec[i + block_size / 2]);\n\n        __m128d diff1 = _mm_sub_pd(block1, block2);\n        __m128d abs_diff1 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff1), diff1);\n\n        __m128d diff2 = _mm_sub_pd(_mm_loadu_pd(&vec[i + block_size / 4]), _mm_loadu_pd(&vec[i + 3 * block_size / 4]));\n        __m128d abs_diff2 = _mm_max_pd(_mm_sub_pd(_mm_setzero_pd(), diff2), diff2);\n\n        if (!_mm_comigt_pd(abs_diff1, tol) || !_mm_comigt_pd(abs_diff2, tol)) {\n            return false;\n        }\n\n        i += block_size;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    for (size_t i = 0; i < rows; i++) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols / 2; j++) {\n            __m128d vec = _mm_loadu_pd(matrix + 2 * j);\n            vec = _mm_mul_pd(vec, vec);\n            sum = _mm_add_pd(sum, vec);\n        }\n        if (cols % 2 != 0) {\n            __m128d last = _mm_set_sd(matrix[cols - 1] * matrix[cols - 1]);\n            sum = _mm_add_pd(sum, last);\n        }\n        sum = _mm_hadd_pd(sum, sum);\n        sum = _mm_hadd_pd(sum, sum);\n        double norm = _mm_cvtss_f64(sum);\n        result[i] = norm > thresholds[i];\n        matrix += cols;\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    for (size_t i = 0; i < rows; i++) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols / 2; j++) {\n            __m128d vec = _mm_loadu_pd(matrix + 2 * j);\n            vec = _mm_mul_pd(vec, vec);\n            sum = _mm_add_pd(sum, vec);\n        }\n        if (cols % 2 != 0) {\n            __m128d last = _mm_set1_pd(matrix[cols - 1]);\n            last = _mm_mul_sd(last, last);\n            sum = _mm_add_sd(sum, last);\n        }\n        sum = _mm_hadd_pd(sum, sum);\n        sum = _mm_hadd_pd(sum, sum);\n        double norm;\n        _mm_store_sd(&norm, sum);\n        result[i] = (norm > thresholds[i]);\n        matrix += cols;\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    for (size_t i = 0; i < rows; i++) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols / 2; j++) {\n            __m128d vec = _mm_loadu_pd(matrix + 2 * j);\n            vec = _mm_mul_pd(vec, vec);\n            sum = _mm_add_pd(sum, vec);\n        }\n        if (cols % 2 != 0) {\n            __m128d last = _mm_set_sd(matrix[cols - 1] * matrix[cols - 1]);\n            sum = _mm_add_pd(sum, last);\n        }\n        sum = _mm_hadd_pd(sum, sum);\n        sum = _mm_hadd_pd(sum, sum);\n        double norm = _mm_cvtsd_f64(sum);\n        double threshold = thresholds[i];\n        result[i] = norm > threshold;\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    for (size_t i = 0; i < rows; ++i) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols; j += 2) {\n            __m128d row_vals = _mm_loadu_pd(matrix + i * cols + j);\n            __m128d squared = _mm_mul_pd(row_vals, row_vals);\n            sum = _mm_add_pd(sum, squared);\n        }\n        // Horizontal sum\n        __m128d tmp = _mm_hadd_pd(sum, sum);\n        double norm_squared = _mm_cvtsd_f64(tmp);\n        double threshold = thresholds[i];\n        result[i] = (norm_squared > threshold * threshold);\n    }\n}\n```"}
{"task_id": "SimdBench_39_SSE", "completion": "```c\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m128d zero = _mm_setzero_pd();\n    for (size_t i = 0; i < rows; i++) {\n        __m128d sum = zero;\n        for (size_t j = 0; j < cols / 2; j++) {\n            __m128d vec = _mm_loadu_pd(matrix + 2 * j);\n            vec = _mm_mul_pd(vec, vec);\n            sum = _mm_add_pd(sum, vec);\n        }\n        if (cols % 2 != 0) {\n            __m128d last = _mm_set_sd(matrix[cols - 1] * matrix[cols - 1]);\n            sum = _mm_add_pd(sum, last);\n        }\n        sum = _mm_hadd_pd(sum, sum);\n        sum = _mm_hadd_pd(sum, sum);\n        double norm = _mm_cvtsd_f64(sum);\n        result[i] = norm > thresholds[i];\n        matrix += cols;\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process data in chunks of 4 elements (SSE)\n    for (; i < length - 3; i += 4) {\n        __m128i a = _mm_loadu_si128(A_vec + i);\n        __m128i b = _mm_loadu_si128(B_vec + i);\n        __m128i result = _mm_or_si128(a, b);\n        _mm_storeu_si128(C_vec + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    // Process data in chunks of 4 using SSE2\n    __m128i* A_sse = (__m128i*)A;\n    __m128i* B_sse = (__m128i*)B;\n    __m128i* C_sse = (__m128i*)C;\n\n    while (remaining >= 4) {\n        _mm_storeu_si128(C_sse + i, _mm_or_si128(_mm_loadu_si128(A_sse + i), _mm_loadu_si128(B_sse + i)));\n        i++;\n        remaining -= 4;\n    }\n\n    // Handle remaining elements\n    uint32_t* A_ptr = (uint32_t*)(A_sse + i);\n    uint32_t* B_ptr = (uint32_t*)(B_sse + i);\n    uint32_t* C_ptr = (uint32_t*)(C_sse + i);\n\n    for (; remaining > 0; remaining--) {\n        *C_ptr++ = *A_ptr++ | *B_ptr++;\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process data in chunks of 4 elements (SSE)\n    for (; i < length - 3; i += 4) {\n        __m128i a = _mm_loadu_si128(A_vec + i);\n        __m128i b = _mm_loadu_si128(B_vec + i);\n        __m128i result = _mm_or_si128(a, b);\n        _mm_storeu_si128(C_vec + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i = 0;\n    size_t remaining = length;\n\n    // Process data in chunks of 4 using SSE2\n    __m128i* A_sse = (__m128i*)A;\n    __m128i* B_sse = (__m128i*)B;\n    __m128i* C_sse = (__m128i*)C;\n\n    while (remaining >= 4) {\n        _mm_storeu_si128(C_sse + i, _mm_or_si128(_mm_loadu_si128(A_sse + i), _mm_loadu_si128(B_sse + i)));\n        i++;\n        remaining -= 4;\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_SSE", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    size_t i;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process full vectors\n    for(i = 0; i < length / 4; i++) {\n        __m128i a = _mm_loadu_si128(A_vec + i);\n        __m128i b = _mm_loadu_si128(B_vec + i);\n        __m128i result = _mm_or_si128(a, b);\n        _mm_storeu_si128(C_vec + i, result);\n    }\n\n    // Handle remaining elements\n    for(i = length - (length % 4); i < length; i++) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_squared = _mm_mullo_epi16(a_vec, a_vec);\n        __m128i b_squared = _mm_mullo_epi16(b_vec, b_vec);\n\n        __m128i ab_product = _mm_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_unpacklo_epi16(a_squared, b_squared));\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_unpacklo_epi16(ab_product, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int a = A[i];\n        int b = B[i];\n        sum_of_squares.m128i_i32[0] += a * a;\n        sum_of_products.m128i_i32[0] += a * b;\n    }\n\n    __m128i result_vec = _mm_sub_epi64(sum_of_squares, sum_of_products);\n    int64_t result = result_vec.m128i_i64[0] + result_vec.m128i_i64[1];\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_sq = _mm_mullo_epi16(a_vec, a_vec);\n        __m128i b_sq = _mm_mullo_epi16(b_vec, b_vec);\n\n        __m128i ab = _mm_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_unpacklo_epi16(a_sq, b_sq));\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_unpacklo_epi16(ab, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        int8_t a_val = A[i];\n        int8_t b_val = B[i];\n        sum_of_squares.m128i_i32[0] += a_val * a_val;\n        sum_of_squares.m128i_i32[1] += b_val * b_val;\n        sum_of_products.m128i_i32[0] += a_val * b_val;\n    }\n\n    __m128i result_vec = _mm_sub_epi64(sum_of_squares, sum_of_products);\n    int64_t result = result_vec.m128i_i64[0] - result_vec.m128i_i64[1];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_squared = _mm_mullo_epi16(a_vec, a_vec);\n        __m128i b_squared = _mm_mullo_epi16(b_vec, b_vec);\n\n        __m128i ab_product = _mm_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_unpacklo_epi16(a_squared, b_squared));\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_unpacklo_epi16(ab_product, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements\n    int64_t residual_sum_of_squares = 0;\n    int64_t residual_sum_of_products = 0;\n    for (; i < length; ++i) {\n        int8_t a_val = A[i];\n        int8_t b_val = B[i];\n        residual_sum_of_squares += a_val * a_val + b_val * b_val;\n        residual_sum_of_products += a_val * b_val;\n    }\n\n    // Combine results\n    __m128i sum_of_squares_low = _mm_cvtsi64_si128(residual_sum_of_squares);\n    __m128i sum_of_products_low = _mm_cvtsi64_si128(residual_sum_of_products);\n\n    sum_of_squares = _mm_add_epi64(sum_of_squares, sum_of_squares_low);\n    sum_of_products = _mm_add_epi64(sum_of_products, sum_of_products_low);\n\n    int64_t total_sum_of_squares = _mm_cvtsi128_si64(sum_of_squares);\n    int64_t total_sum_of_products = _mm_cvtsi128_si64(sum_of_products);\n\n    return total_sum_of_squares - total_sum_of_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_squared = _mm_mullo_epi16(a_vec, a_vec);\n        __m128i b_squared = _mm_mullo_epi16(b_vec, b_vec);\n\n        __m128i ab_product = _mm_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_unpacklo_epi16(a_squared, b_squared));\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_unpacklo_epi16(ab_product, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements\n    int64_t residual_sum_of_squares = 0;\n    int64_t residual_sum_of_products = 0;\n    for (; i < length; ++i) {\n        int8_t a_val = A[i];\n        int8_t b_val = B[i];\n        residual_sum_of_squares += a_val * a_val + b_val * b_val;\n        residual_sum_of_products += a_val * b_val;\n    }\n\n    // Combine results\n    __m128i sum_of_squares_low = _mm_cvtsi128_si64(sum_of_squares);\n    __m128i sum_of_squares_high = _mm_cvtsi128_si64(_mm_srli_si128(sum_of_squares, 8));\n    int64_t total_sum_of_squares = sum_of_squares_low + sum_of_squares_high + residual_sum_of_squares;\n\n    __m128i sum_of_products_low = _mm_cvtsi128_si64(sum_of_products);\n    __m128i sum_of_products_high = _mm_cvtsi128_si64(_mm_srli_si128(sum_of_products, 8));\n    int64_t total_sum_of_products = sum_of_products_low + sum_of_products_high + residual_sum_of_products;\n\n    return total_sum_of_squares - total_sum_of_products;\n}\n```"}
{"task_id": "SimdBench_41_SSE", "completion": "```c\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m128i sum_of_squares = _mm_setzero_si128();\n    __m128i sum_of_products = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_squared = _mm_mullo_epi16(a_vec, a_vec);\n        __m128i b_squared = _mm_mullo_epi16(b_vec, b_vec);\n\n        __m128i ab_product = _mm_mullo_epi16(a_vec, b_vec);\n\n        sum_of_squares = _mm_add_epi32(sum_of_squares, _mm_unpacklo_epi16(a_squared, b_squared));\n        sum_of_products = _mm_add_epi32(sum_of_products, _mm_unpacklo_epi16(ab_product, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        int a = A[i];\n        int b = B[i];\n        sum_of_squares.m128i_i32[0] += a * a;\n        sum_of_squares.m128i_i32[1] += b * b;\n        sum_of_products.m128i_i32[0] += a * b;\n    }\n\n    __m128i result_vec = _mm_sub_epi64(sum_of_squares, sum_of_products);\n    int64_t result = result_vec.m128i_i64[0] - result_vec.m128i_i64[1];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process full vectors\n    for (; i < length - 3; i += 4) {\n        __m128i a = _mm_load_si128(A_vec + i);\n        __m128i b = _mm_load_si128(B_vec + i);\n        __m128i not_b = _mm_xor_si128(_mm_set1_epi64x(~0ULL), b);\n        __m128i result = _mm_and_si128(a, not_b);\n        _mm_store_si128(C_vec + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process full vectors\n    for (; i < length - 3; i += 4) {\n        __m128i a = _mm_load_si128(A_vec + i);\n        __m128i b = _mm_load_si128(B_vec + i);\n        __m128i b_not = _mm_xor_si128(b, _mm_set1_epi64x(~0ULL));\n        __m128i result = _mm_and_si128(a, b_not);\n        _mm_store_si128(C_vec + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process data in chunks of 2 vectors (4 elements per vector)\n    for (; i < length - 3; i += 4) {\n        __m128i a = _mm_load_si128(A_vec + i);\n        __m128i b = _mm_load_si128(B_vec + i);\n        __m128i b_neg = _mm_xor_si128(_mm_set1_epi64x(~0ULL), b); // ~b\n        __m128i res = _mm_and_si128(a, b_neg);\n        _mm_store_si128(C_vec + i, res);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process data in chunks of 2 vectors (4 elements per vector)\n    for (; i < length - 3; i += 4) {\n        __m128i a = _mm_load_si128(A_vec + i);\n        __m128i b = _mm_load_si128(B_vec + i);\n        __m128i b_neg = _mm_xor_si128(_mm_set1_epi64x(~0ULL), b); // ~b\n        __m128i result = _mm_and_si128(a, b_neg);\n        _mm_store_si128(C_vec + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; ++i) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_42_SSE", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128i* B_vec = (__m128i*)B;\n    __m128i* C_vec = (__m128i*)C;\n\n    // Process full vectors\n    for (; i < length - 3; i += 4) {\n        __m128i a = _mm_loadu_si128(A_vec + i);\n        __m128i b = _mm_loadu_si128(B_vec + i);\n        __m128i b_not = _mm_xor_si128(b, _mm_set1_epi64x(~0ULL));\n        __m128i result = _mm_and_si128(a, b_not);\n        _mm_storeu_si128(C_vec + i, result);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        C[i] = A[i] & (~B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i mask_pos = _mm_set1_epi8(0x7F); // Mask for positive values\n    __m128i mask_neg = _mm_set1_epi8(0x80); // Mask for negative values\n    __m128i pos_mask = _mm_set1_epi8(1);    // Result mask for positive\n    __m128i neg_mask = _mm_set1_epi8(-1);   // Result mask for negative\n    __m128i zero_mask = _mm_setzero_si128(); // Result mask for zero\n\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_positive = _mm_and_si128(a, mask_pos);\n        __m128i b_positive = _mm_and_si128(b, mask_pos);\n        __m128i both_positive = _mm_cmpeq_epi8(a_positive, b_positive);\n\n        __m128i a_negative = _mm_andnot_si128(mask_pos, a);\n        __m128i b_negative = _mm_andnot_si128(mask_pos, b);\n        __m128i both_negative = _mm_cmpeq_epi8(a_negative, b_negative);\n\n        __m128i result = _mm_or_si128(\n            _mm_and_si128(both_positive, pos_mask),\n            _mm_and_si128(both_negative, neg_mask)\n        );\n\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; i++) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i mask_pos = _mm_set1_epi8(0x7F); // Mask for positive values\n    __m128i mask_neg = _mm_set1_epi8(0x80); // Mask for negative values\n    __m128i pos_mask = _mm_set1_epi8(1);    // Result mask for positive\n    __m128i neg_mask = _mm_set1_epi8(-1);   // Result mask for negative\n    __m128i zero_mask = _mm_setzero_si128(); // Result mask for zero\n\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_pos = _mm_and_si128(a, mask_pos); // Extract positive part of A\n        __m128i b_pos = _mm_and_si128(b, mask_pos); // Extract positive part of B\n        __m128i a_neg = _mm_andnot_si128(mask_pos, a); // Extract negative part of A\n        __m128i b_neg = _mm_andnot_si128(mask_pos, b); // Extract negative part of B\n\n        __m128i pos_eq = _mm_cmpeq_epi8(a_pos, b_pos); // Compare positive parts\n        __m128i neg_eq = _mm_cmpeq_epi8(a_neg, b_neg); // Compare negative parts\n\n        __m128i pos_res = _mm_blendv_epi8(zero_mask, pos_mask, pos_eq); // If positive parts match, set to 1\n        __m128i neg_res = _mm_blendv_epi8(zero_mask, neg_mask, neg_eq); // If negative parts match, set to -1\n\n        __m128i res = _mm_or_si128(pos_res, neg_res); // Combine results\n        _mm_storeu_si128((__m128i*)(C + i), res); // Store result\n    }\n\n    for (; i < length; i++) {\n        if ((A[i] > 0 && B[i] > 0) || (A[i] < 0 && B[i] < 0)) {\n            C[i] = (A[i] > 0 && B[i] > 0) ? 1 : -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i mask_pos = _mm_set1_epi8(0x7F); // Mask for positive values\n    __m128i mask_neg = _mm_set1_epi8(0x80); // Mask for negative values\n    __m128i pos_mask = _mm_set1_epi8(1);    // Result for both positive\n    __m128i neg_mask = _mm_set1_epi8(-1);   // Result for both negative\n    __m128i zero_mask = _mm_setzero_si128();// Result for other cases\n\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_pos = _mm_and_si128(a, mask_pos);\n        __m128i b_pos = _mm_and_si128(b, mask_pos);\n        __m128i a_neg = _mm_andnot_si128(mask_pos, a);\n        __m128i b_neg = _mm_andnot_si128(mask_pos, b);\n\n        __m128i pos_check = _mm_cmpeq_epi8(_mm_or_si128(a_pos, b_pos), mask_pos);\n        __m128i neg_check = _mm_cmpeq_epi8(_mm_or_si128(a_neg, b_neg), mask_neg);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(pos_check, pos_mask),\n                                      _mm_or_si128(_mm_and_si128(neg_check, neg_mask),\n                                                   _mm_andnot_si128(_mm_or_si128(pos_check, neg_check), zero_mask)));\n\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for (; i < length; ++i) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i mask_pos = _mm_set1_epi8(0x7F); // Mask for positive values\n    __m128i mask_neg = _mm_set1_epi8(0x80); // Mask for negative values\n    __m128i pos_mask = _mm_set1_epi8(1);    // Result mask for positive\n    __m128i neg_mask = _mm_set1_epi8(-1);   // Result mask for negative\n    __m128i zero_mask = _mm_setzero_si128(); // Result mask for zero\n\n    for (; i <= length - 16; i += 16) {\n        __m128i a = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_pos = _mm_and_si128(a, mask_pos); // Extract positive part of A\n        __m128i b_pos = _mm_and_si128(b, mask_pos); // Extract positive part of B\n        __m128i a_neg = _mm_andnot_si128(mask_pos, a); // Extract negative part of A\n        __m128i b_neg = _mm_andnot_si128(mask_pos, b); // Extract negative part of B\n\n        __m128i pos_eq = _mm_cmpeq_epi8(a_pos, b_pos); // Compare positive parts\n        __m128i neg_eq = _mm_cmpeq_epi8(a_neg, b_neg); // Compare negative parts\n\n        __m128i pos_res = _mm_blendv_epi8(zero_mask, pos_mask, pos_eq); // If positive, set to 1\n        __m128i neg_res = _mm_blendv_epi8(zero_mask, neg_mask, neg_eq); // If negative, set to -1\n\n        __m128i res = _mm_or_si128(pos_res, neg_res); // Combine results\n        _mm_storeu_si128((__m128i*)(C + i), res); // Store result\n    }\n\n    for (; i < length; i++) {\n        if ((A[i] > 0) && (B[i] > 0)) {\n            C[i] = 1;\n        } else if ((A[i] < 0) && (B[i] < 0)) {\n            C[i] = -1;\n        } else {\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_43_SSE", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    size_t i = 0;\n    __m128i mask_pos = _mm_set1_epi8(0x7F); // Mask for positive values\n    __m128i mask_neg = _mm_set1_epi8(0x80); // Mask for negative values\n    __m128i pos_mask = _mm_set1_epi8(1);    // Result for both positive\n    __m128i neg_mask = _mm_set1_epi8(-1);   // Result for both negative\n    __m128i zero_mask = _mm_setzero_si128();// Result for other cases\n\n    for(i = 0; i + 16 <= length; i += 16){\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i b_vec = _mm_loadu_si128((__m128i*)(B + i));\n\n        __m128i a_pos = _mm_and_si128(a_vec, mask_pos);\n        __m128i b_pos = _mm_and_si128(b_vec, mask_pos);\n\n        __m128i a_neg = _mm_andnot_si128(mask_pos, a_vec);\n        __m128i b_neg = _mm_andnot_si128(mask_pos, b_vec);\n\n        __m128i pos_check = _mm_cmpeq_epi8(_mm_or_si128(a_pos, b_pos), mask_pos);\n        __m128i neg_check = _mm_cmpeq_epi8(_mm_or_si128(a_neg, b_neg), mask_neg);\n\n        __m128i result = _mm_or_si128(_mm_and_si128(pos_check, pos_mask),\n                                      _mm_and_si128(neg_check, neg_mask));\n        result = _mm_or_si128(result, zero_mask);\n\n        _mm_storeu_si128((__m128i*)(C + i), result);\n    }\n\n    for(; i < length; i++){\n        if((A[i] > 0) && (B[i] > 0)){\n            C[i] = 1;\n        }\n        else if((A[i] < 0) && (B[i] < 0)){\n            C[i] = -1;\n        }\n        else{\n            C[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Load mid-point value from A\n    __m128i mid_value = _mm_set1_epi16(A[mid]);\n\n    // Process elements before the midpoint\n    for (; i < mid; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        __m128i and_vec = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&C[i], xor_vec);\n        _mm_storeu_si128((__m128i*)&C[i + 4], and_vec);\n    }\n\n    // Store the mid-point value from A into C\n    C[mid] = A[mid];\n\n    // Process elements after the midpoint\n    for (; i < length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        __m128i and_vec = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&C[i], xor_vec);\n        _mm_storeu_si128((__m128i*)&C[i + 4], and_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Load mid-point value from A\n    __m128i mid_value = _mm_set1_epi16(A[mid]);\n\n    // Process elements before the midpoint\n    for (; i < mid; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        __m128i and_vec = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&C[i], xor_vec);\n        _mm_storeu_si128((__m128i*)&C[i + 4], and_vec);\n    }\n\n    // Store the mid-point value from A into C\n    C[mid] = A[mid];\n\n    // Process elements after the midpoint\n    for (; i < length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        __m128i and_vec = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&C[i], xor_vec);\n        _mm_storeu_si128((__m128i*)&C[i + 4], and_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Load mid-point value from A\n    __m128i mid_value = _mm_set1_epi16(A[mid]);\n\n    // Process elements before the midpoint\n    for (; i < mid; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        __m128i and_vec = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&C[i], xor_vec);\n        _mm_storeu_si128((__m128i*)&C[i + 4], and_vec);\n    }\n\n    // Store the mid-point value from A into C\n    C[mid] = A[mid];\n\n    // Process elements after the midpoint\n    for (; i < length; i += 8) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vec = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        __m128i and_vec = _mm_and_si128(a_vec, b_vec);\n        _mm_storeu_si128((__m128i*)&C[i], xor_vec);\n        _mm_storeu_si128((__m128i*)&C[i + 4], and_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Load mid-point value from A\n    __m128i mid_value = _mm_set1_epi16(A[mid]);\n\n    // Process elements before the midpoint\n    for (; i < mid; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        // Select XOR for first half, AND for second half\n        __m128i mask = _mm_setr_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n        __m128i result = _mm_or_si128(_mm_and_si128(xor_result, mask), _mm_andnot_si128(mask, and_result));\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Store mid-point value from A\n    C[mid] = A[mid];\n\n    // Process elements after the midpoint\n    for (; i + 8 <= length; i += 8) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        // Select XOR for first half, AND for second half\n        __m128i mask = _mm_setr_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n        __m128i result = _mm_or_si128(_mm_and_si128(xor_result, mask), _mm_andnot_si128(mask, and_result));\n\n        _mm_storeu_si128((__m128i*)&C[i], result);\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; i++) {\n        if (i < mid) {\n            C[i] = A[i] ^ B[i];\n        } else if (i > mid) {\n            C[i] = A[i] & B[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_44_SSE", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    size_t i = 0;\n    size_t mid = length / 2;\n\n    // Load mid-point value\n    __m128i mid_val = _mm_set1_epi16(A[mid]);\n\n    // Process elements before the midpoint\n    for (; i < mid; i += 8) {\n        __m128i a_vals = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vals = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vals = _mm_xor_si128(a_vals, b_vals);\n        __m128i and_vals = _mm_and_si128(a_vals, b_vals);\n        _mm_storeu_si128((__m128i*)&C[i], xor_vals);\n    }\n\n    // Store mid-point value\n    C[mid] = A[mid];\n\n    // Process elements after the midpoint\n    for (; i + 8 <= length; i += 8) {\n        __m128i a_vals = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i b_vals = _mm_loadu_si128((__m128i*)&B[i]);\n        __m128i xor_vals = _mm_xor_si128(a_vals, b_vals);\n        __m128i and_vals = _mm_and_si128(a_vals, b_vals);\n        _mm_storeu_si128((__m128i*)&C[i], and_vals);\n    }\n\n    // Handle remaining elements if any\n    for (; i < length; ++i) {\n        if (i < mid) {\n            C[i] = A[i] ^ B[i];\n        } else {\n            C[i] = A[i] & B[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    while (i + 4 <= total_elements) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 c_vec = _mm_loadu_ps(&C[i]);\n        __m128 d_vec = _mm_loadu_ps(&D[i]);\n\n        __m128 cond1_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond1[i]));\n        __m128 cond2_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond2[i]));\n\n        __m128 mask1 = _mm_and_ps(cond1_vec, cond2_vec); // cond1 && cond2\n        __m128 mask2 = _mm_andnot_ps(mask1, cond1_vec); // cond1 && !cond2\n        __m128 mask3 = _mm_andnot_ps(_mm_or_ps(mask1, mask2), cond2_vec); // !cond1 && cond2\n        __m128 mask4 = _mm_andnot_ps(_mm_or_ps(_mm_or_ps(mask1, mask2), mask3), _mm_castps_si128(_mm_set1_epi32(-1))); // !cond1 && !cond2\n\n        __m128 result = _mm_or_ps(\n            _mm_and_ps(a_vec, mask1),\n            _mm_or_ps(\n                _mm_and_ps(b_vec, mask2),\n                _mm_or_ps(\n                    _mm_and_ps(c_vec, mask3),\n                    _mm_and_ps(d_vec, mask4)\n                )\n            )\n        );\n\n        _mm_storeu_ps(&out[i], result);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 mask1_true = _mm_set1_ps(1.0f); // Mask for cond1 == true\n    __m128 mask1_false = _mm_set1_ps(0.0f); // Mask for cond1 == false\n    __m128 mask2_true = _mm_set1_ps(1.0f); // Mask for cond2 == true\n    __m128 mask2_false = _mm_set1_ps(0.0f); // Mask for cond2 == false\n\n    while (i + 4 <= total_elements) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128 cond1_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond1[i])); // Load cond1 as int mask\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond2[i])); // Load cond2 as int mask\n\n        // Compute masks\n        __m128 mask_a = _mm_and_ps(cond1_mask, cond2_mask); // cond1 && cond2\n        __m128 mask_b = _mm_and_ps(cond1_mask, _mm_xor_ps(mask1_true, cond2_mask)); // cond1 && !cond2\n        __m128 mask_c = _mm_and_ps(_mm_xor_ps(mask1_true, cond1_mask), cond2_mask); // !cond1 && cond2\n        __m128 mask_d = _mm_andnot_ps(_mm_or_ps(_mm_or_ps(mask_a, mask_b), mask_c), mask1_true); // !(mask_a || mask_b || mask_c)\n\n        // Select values\n        __m128 result = _mm_add_ps(_mm_mul_ps(a, mask_a),\n                                   _mm_add_ps(_mm_mul_ps(b, mask_b),\n                                              _mm_add_ps(_mm_mul_ps(c, mask_c),\n                                                         _mm_mul_ps(d, mask_d))));\n\n        _mm_storeu_ps(&out[i], result);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 true_mask = _mm_set1_ps(1.0f);\n    __m128 false_mask = _mm_setzero_ps();\n\n    while (i + 4 <= total_elements) {\n        __m128 a = _mm_loadu_ps(&A[i]);\n        __m128 b = _mm_loadu_ps(&B[i]);\n        __m128 c = _mm_loadu_ps(&C[i]);\n        __m128 d = _mm_loadu_ps(&D[i]);\n\n        __m128 cond1_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond1[i]));\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond2[i]));\n\n        // cond1 && cond2 -> A\n        __m128 result1 = _mm_and_ps(cond1_mask, cond2_mask);\n        __m128 selected_a = _mm_andnot_ps(_mm_xor_ps(result1, true_mask), a);\n\n        // cond1 && !cond2 -> B\n        __m128 result2 = _mm_and_ps(cond1_mask, _mm_andnot_ps(cond2_mask, true_mask));\n        __m128 selected_b = _mm_andnot_ps(_mm_xor_ps(result2, true_mask), b);\n\n        // !cond1 && cond2 -> C\n        __m128 result3 = _mm_andnot_ps(cond1_mask, cond2_mask);\n        __m128 selected_c = _mm_andnot_ps(_mm_xor_ps(result3, true_mask), c);\n\n        // !cond1 && !cond2 -> D\n        __m128 result4 = _mm_andnot_ps(cond1_mask, _mm_andnot_ps(cond2_mask, true_mask));\n        __m128 selected_d = _mm_andnot_ps(_mm_xor_ps(result4, true_mask), d);\n\n        // Combine all results\n        __m128 final_result = _mm_or_ps(_mm_or_ps(selected_a, selected_b), _mm_or_ps(selected_c, selected_d));\n\n        _mm_storeu_ps(&out[i], final_result);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 true_mask = _mm_set1_ps(1.0f);\n    __m128 false_mask = _mm_setzero_ps();\n\n    while (i + 4 <= total_elements) {\n        __m128 a_vals = _mm_loadu_ps(&A[i]);\n        __m128 b_vals = _mm_loadu_ps(&B[i]);\n        __m128 c_vals = _mm_loadu_ps(&C[i]);\n        __m128 d_vals = _mm_loadu_ps(&D[i]);\n\n        __m128 cond1_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond1[i]));\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond2[i]));\n\n        // cond1 && cond2 -> A\n        __m128 result1 = _mm_and_ps(cond1_mask, cond2_mask);\n        __m128 selected_a = _mm_andnot_ps(_mm_xor_ps(result1, true_mask), a_vals);\n\n        // cond1 && !cond2 -> B\n        __m128 result2 = _mm_and_ps(cond1_mask, _mm_andnot_ps(cond2_mask, true_mask));\n        __m128 selected_b = _mm_andnot_ps(_mm_xor_ps(result2, true_mask), b_vals);\n\n        // !cond1 && cond2 -> C\n        __m128 result3 = _mm_andnot_ps(cond1_mask, cond2_mask);\n        __m128 selected_c = _mm_andnot_ps(_mm_xor_ps(result3, true_mask), c_vals);\n\n        // !cond1 && !cond2 -> D\n        __m128 result4 = _mm_andnot_ps(cond1_mask, _mm_andnot_ps(cond2_mask, true_mask));\n        __m128 selected_d = _mm_andnot_ps(_mm_xor_ps(result4, true_mask), d_vals);\n\n        // Combine all results\n        __m128 final_result = _mm_or_ps(_mm_or_ps(selected_a, selected_b), _mm_or_ps(selected_c, selected_d));\n\n        _mm_storeu_ps(&out[i], final_result);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_SSE", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128 true_mask = _mm_set1_ps(1.0f);\n    __m128 false_mask = _mm_setzero_ps();\n\n    while (i + 4 <= total_elements) {\n        __m128 a_vals = _mm_loadu_ps(&A[i]);\n        __m128 b_vals = _mm_loadu_ps(&B[i]);\n        __m128 c_vals = _mm_loadu_ps(&C[i]);\n        __m128 d_vals = _mm_loadu_ps(&D[i]);\n\n        __m128 cond1_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond1[i]));\n        __m128 cond2_mask = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&cond2[i]));\n\n        // cond1 && cond2 -> A\n        __m128 mask1 = _mm_and_ps(cond1_mask, cond2_mask);\n        __m128 result1 = _mm_andnot_ps(mask1, _mm_setzero_ps());\n        __m128 result2 = _mm_or_ps(result1, _mm_and_ps(mask1, a_vals));\n\n        // cond1 && !cond2 -> B\n        __m128 mask2 = _mm_and_ps(cond1_mask, _mm_cmpeq_ps(cond2_mask, false_mask));\n        __m128 result3 = _mm_andnot_ps(mask2, _mm_setzero_ps());\n        __m128 result4 = _mm_or_ps(result3, _mm_and_ps(mask2, b_vals));\n\n        // !cond1 && cond2 -> C\n        __m128 mask3 = _mm_andnot_ps(cond1_mask, cond2_mask);\n        __m128 result5 = _mm_andnot_ps(mask3, _mm_setzero_ps());\n        __m128 result6 = _mm_or_ps(result5, _mm_and_ps(mask3, c_vals));\n\n        // !cond1 && !cond2 -> D\n        __m128 mask4 = _mm_andnot_ps(cond1_mask, _mm_cmpeq_ps(cond2_mask, false_mask));\n        __m128 result7 = _mm_andnot_ps(mask4, _mm_setzero_ps());\n        __m128 result8 = _mm_or_ps(result7, _mm_and_ps(mask4, d_vals));\n\n        // Combine all results\n        __m128 final_result = _mm_or_ps(_mm_or_ps(result2, result4), _mm_or_ps(result6, result8));\n\n        _mm_storeu_ps(&out[i], final_result);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    while (i < total_elements) {\n        if (cond1[i] && cond2[i]) {\n            out[i] = A[i];\n        } else if (cond1[i] && !cond2[i]) {\n            out[i] = B[i];\n        } else if (!cond1[i] && cond2[i]) {\n            out[i] = C[i];\n        } else {\n            out[i] = D[i];\n        }\n        i++;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i bit_mask = _mm_set_epi8(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1);\n\n    while (i + 16 <= total_elements) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vec_out = _mm_setzero_si128();\n\n        // Count bits using popcount-like approach with shifts and masks\n        __m128i v1 = _mm_and_si128(vec_A, bit_mask);\n        __m128i v2 = _mm_and_si128(_mm_srli_epi32(vec_A, 1), bit_mask);\n        __m128i v3 = _mm_and_si128(_mm_srli_epi32(vec_A, 2), bit_mask);\n        __m128i v4 = _mm_and_si128(_mm_srli_epi32(vec_A, 3), bit_mask);\n\n        vec_out = _mm_add_epi8(vec_out, _mm_popcnt_epi8(v1));\n        vec_out = _mm_add_epi8(vec_out, _mm_popcnt_epi8(v2));\n        vec_out = _mm_add_epi8(vec_out, _mm_popcnt_epi8(v3));\n        vec_out = _mm_add_epi8(vec_out, _mm_popcnt_epi8(v4));\n\n        // Horizontal sum across bytes\n        __m128i sum1 = _mm_sad_epu8(vec_out, _mm_setzero_si128());\n        _mm_storeu_si128((__m128i*)&out[i], sum1);\n\n        i += 16;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        uint8_t count = 0;\n        for (int j = 0; j < 32; ++j) {\n            count += (val >> j) & 1;\n        }\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    __m128i bit_mask = _mm_set_epi8(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1);\n    __m128i sum_mask = _mm_set_epi8(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1);\n    \n    for (size_t i = 0; i < total_elements; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i bit_counts = _mm_popcnt_epi32(data);\n        __m128i horizontal_sum = _mm_hadd_epi32(bit_counts, bit_counts);\n        horizontal_sum = _mm_hadd_epi32(horizontal_sum, horizontal_sum);\n        horizontal_sum = _mm_shuffle_epi32(horizontal_sum, _MM_SHUFFLE(0, 0, 0, 0));\n        _mm_storeu_si128((__m128i*)(out + i), horizontal_sum);\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i bit_mask = _mm_set_epi8(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1);\n\n    while (i + 16 <= total_elements) {\n        __m128i vec_A = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i vec_out = _mm_setzero_si128();\n\n        // Count bits using popcount-like approach with shifts and masks\n        __m128i v1 = _mm_and_si128(vec_A, bit_mask);\n        __m128i v2 = _mm_and_si128(_mm_srli_epi32(vec_A, 1), bit_mask);\n        __m128i v3 = _mm_and_si128(_mm_srli_epi32(vec_A, 2), bit_mask);\n        __m128i v4 = _mm_and_si128(_mm_srli_epi32(vec_A, 3), bit_mask);\n        __m128i v5 = _mm_and_si128(_mm_srli_epi32(vec_A, 4), bit_mask);\n        __m128i v6 = _mm_and_si128(_mm_srli_epi32(vec_A, 5), bit_mask);\n        __m128i v7 = _mm_and_si128(_mm_srli_epi32(vec_A, 6), bit_mask);\n        __m128i v8 = _mm_and_si128(_mm_srli_epi32(vec_A, 7), bit_mask);\n\n        vec_out = _mm_add_epi8(vec_out, v1);\n        vec_out = _mm_add_epi8(vec_out, v2);\n        vec_out = _mm_add_epi8(vec_out, v3);\n        vec_out = _mm_add_epi8(vec_out, v4);\n        vec_out = _mm_add_epi8(vec_out, v5);\n        vec_out = _mm_add_epi8(vec_out, v6);\n        vec_out = _mm_add_epi8(vec_out, v7);\n        vec_out = _mm_add_epi8(vec_out, v8);\n\n        // Horizontal sum across bytes\n        __m128i sum1 = _mm_add_epi8(vec_out, _mm_srli_epi16(vec_out, 4));\n        __m128i sum2 = _mm_add_epi8(sum1, _mm_srli_epi16(sum1, 8));\n        _mm_storeu_si128((__m128i*)&out[i], sum2);\n\n        i += 16;\n    }\n\n    // Handle remaining elements\n    for (; i < total_elements; ++i) {\n        uint32_t val = A[i];\n        uint8_t count = 0;\n        for (int j = 0; j < 32; ++j) {\n            count += (val >> j) & 1;\n        }\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    size_t total_elements = dim1 * dim2 * dim3;\n    size_t i = 0;\n\n    __m128i bit_counts = _mm_setzero_si128();\n    __m128i mask = _mm_set_epi8(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1);\n\n    while (i + 16 <= total_elements) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i temp = _mm_and_si128(vec, mask);\n        __m128i ones = _mm_popcnt_epi32(vec);\n        bit_counts = _mm_add_epi8(bit_counts, _mm_cvtepu32_epi8(ones));\n        i += 16;\n    }\n\n    // Store the results back to out\n    _mm_storeu_si128((__m128i*)&out[0], bit_counts);\n}\n```"}
{"task_id": "SimdBench_46_SSE", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const size_t total_elements = dim1 * dim2 * dim3;\n    __m128i bit_counts = _mm_setzero_si128();\n    __m128i lookup_table = _mm_set_epi8(\n        0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4\n    );\n\n    for (size_t i = 0; i < total_elements; i += 16) {\n        __m128i data = _mm_loadu_si128((__m128i*)(A + i));\n        __m128i packed_bits = _mm_and_si128(data, _mm_set1_epi32(0xF));\n        __m128i counts = _mm_shuffle_epi8(lookup_table, packed_bits);\n        __m128i shifted_data = _mm_srli_epi32(data, 4);\n        __m128i shifted_bits = _mm_and_si128(shifted_data, _mm_set1_epi32(0xF));\n        __m128i shifted_counts = _mm_shuffle_epi8(lookup_table, shifted_bits);\n        bit_counts = _mm_add_epi8(bit_counts, counts);\n        bit_counts = _mm_add_epi8(bit_counts, shifted_counts);\n    }\n\n    for (size_t i = 0; i < total_elements; ++i) {\n        out[i] = (uint8_t)_mm_cvtsi128_si32(bit_counts);\n        bit_counts = _mm_srli_si128(bit_counts, 1);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n\n    __m128i mask = _mm_set_epi32(0, 1, 2, 3); // Mask for modulo operation\n\n    for (i = 0; i < limit; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i a_mod = _mm_mul_epu32(a_vec, mask); // Multiply by [0,1,2,3] to get remainders\n        __m128i abs_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), a_mod); // Compare remainders with zero\n        __m128i abs_a = _mm_sub_epi32(a_vec, _mm_andnot_si128(abs_mask, a_vec)); // Compute |a| if divisible by 3\n        __m128i result = _mm_or_si128(abs_a, _mm_andnot_si128(abs_mask, a_vec)); // Select based on condition\n        _mm_storeu_si128((__m128i*)&B[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n\n    __m128i mask = _mm_set_epi32(0, 0, 0, 0); // Initialize mask\n\n    for (i = 0; i < limit; i += 4) {\n        __m128i vecA = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 integers from A\n        __m128i vecB = _mm_loadu_si128((__m128i*)&B[i]); // Load 4 integers from B\n\n        // Check divisibility by 3: (A % 3 == 0)\n        __m128i divBy3 = _mm_and_si128(_mm_cmpgt_epi32(vecA, _mm_setzero_si128()), \n                                        _mm_cmpeq_epi32(_mm_and_si128(vecA, _mm_set1_epi32(3)), _mm_setzero_si128()));\n\n        // Compute absolute values\n        __m128i absA = _mm_sub_epi32(vecA, _mm_andnot_si128(divBy3, _mm_slli_epi32(vecA, 31)));\n\n        // Store the result back to B\n        _mm_storeu_si128((__m128i*)&B[i], absA);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n\n    __m128i mask = _mm_set_epi32(0, 0, 0, 0);\n    __m128i zero = _mm_setzero_si128();\n    __m128i temp_a, temp_b, temp_c;\n\n    for (i = 0; i < limit; i += 4) {\n        // Load 4 elements from A\n        temp_a = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Check divisibility by 3 using modulo operation\n        // Load 4 constants (3, 3, 3, 3)\n        mask = _mm_set1_epi32(3);\n\n        // Compute modulo (temp_a % 3)\n        temp_b = _mm_mul_epu32(_mm_cvtepu32_epi64(temp_a), _mm_set1_epi32(0x55555555));\n        temp_b = _mm_sub_epi32(temp_a, _mm_srli_epi32(temp_b, 2));\n        temp_b = _mm_add_epi32(temp_b, _mm_and_si128(_mm_srli_epi32(temp_b, 1), mask));\n\n        // Compare result with zero to create a mask\n        mask = _mm_cmpeq_epi32(temp_b, zero);\n\n        // Compute absolute values\n        temp_c = _mm_xor_si128(temp_a, _mm_and_si128(mask, _mm_sign_epi32(temp_a, temp_a)));\n\n        // Store results into B\n        _mm_storeu_si128((__m128i*)&B[i], temp_c);\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t num_elements = rows * cols;\n\n    // Ensure that the number of elements is a multiple of 4 for SSE2 alignment\n    size_t aligned_elements = (num_elements / 4) * 4;\n\n    __m128i mask;\n    __m128i a_vec, b_vec;\n    __m128i abs_mask = _mm_set_epi32(0, 1 << 31, 0, 1 << 31);\n\n    for (i = 0; i < aligned_elements; i += 4) {\n        // Load 4 integers from A\n        a_vec = _mm_loadu_si128((__m128i*)&A[i]);\n\n        // Check divisibility by 3 (using modulo operation)\n        // Compute (a_vec % 3) == 0\n        // This is done by multiplying by the modular inverse of 3 and checking if the result is zero\n        __m128i a_times_inv3 = _mm_mul_epi32(a_vec, _mm_set1_epi32(0xaaaaaaab)); // Modular inverse of 3\n        __m128i is_divisible_by_3 = _mm_cmpeq_epi32(a_times_inv3, _mm_setzero_si128());\n\n        // Create a mask where elements divisible by 3 are all bits set\n        mask = _mm_andnot_si128(is_divisible_by_3, abs_mask);\n\n        // Compute absolute values where needed\n        b_vec = _mm_xor_si128(a_vec, mask);\n        b_vec = _mm_sub_epi32(b_vec, mask);\n\n        // Store the result back to B\n        _mm_storeu_si128((__m128i*)&B[i], b_vec);\n    }\n\n    // Handle remaining elements\n    for (j = aligned_elements; j < num_elements; j++) {\n        if (A[j] % 3 == 0) {\n            B[j] = abs(A[j]);\n        } else {\n            B[j] = A[j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_47_SSE", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    size_t i, j;\n    size_t limit = rows * cols;\n\n    __m128i mask = _mm_set_epi32(0, 1, 2, 3); // Mask for modulo operation\n\n    for (i = 0; i < limit; i += 4) {\n        __m128i a_vec = _mm_loadu_si128((__m128i*)&A[i]); // Load 4 integers from A\n        __m128i a_mod = _mm_mul_epu32(a_vec, mask);        // Multiply each element by its index\n        __m128i zero = _mm_setzero_si128();               // Zero vector for comparison\n        __m128i cond = _mm_cmpeq_epi32(a_mod, zero);      // Compare mod result with zero\n\n        __m128i abs_a = _mm_sub_epi32(a_vec, _mm_slli_epi32(a_vec, 31)); // Compute abs(A)\n        __m128i result = _mm_or_si128(_mm_andnot_si128(cond, a_vec), _mm_and_si128(cond, abs_a));\n\n        _mm_storeu_si128((__m128i*)&B[i], result); // Store result back to B\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i = 0;\n    __m128i mask = _mm_set_epi32(0x0F000000, 0x0F000000, 0x0F000000, 0x0F000000);\n    __m128i ones = _mm_set1_epi32(0xF);\n    __m128i rot_mask = _mm_set1_epi32((1 << rotate_bits) - 1);\n    \n    for(i = 0; i + 4 <= length; i += 4){\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i src_low4 = _mm_and_si128(src_vec, mask);\n        __m128i cmp_result = _mm_cmpeq_epi32(src_low4, ones);\n        \n        __m128i src_rotated = _mm_or_si128(_mm_slli_epi32(src_vec, rotate_bits), _mm_srli_epi32(src_vec, 32 - rotate_bits));\n        __m128i src_negated = _mm_xor_si128(src_vec, rot_mask);\n        \n        __m128i result = _mm_blendv_epi8(src_negated, src_rotated, cmp_result);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n    \n    for(; i < length; i++){\n        uint32_t src_val = src[i];\n        uint32_t dst_val = ((src_val & 0xF) == 0xF) ? ((src_val << rotate_bits) | (src_val >> (32 - rotate_bits))) : ~src_val;\n        dst[i] = dst_val;\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    size_t i;\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i ones = _mm_set1_epi32(0xF);\n    __m128i rot_mask = _mm_set1_epi32((1 << rotate_bits) - 1);\n    \n    for(i = 0; i + 4 <= length; i += 4){\n        __m128i src_vec = _mm_loadu_si128((__m128i*)(src + i));\n        __m128i low_nibble = _mm_and_si128(src_vec, mask);\n        __m128i cond = _mm_cmpeq_epi32(low_nibble, ones);\n        \n        __m128i rotated = _mm_or_si128(_mm_slli_epi32(src_vec, rotate_bits), \n                                        _mm_srli_epi32(src_vec, (32 - rotate_bits)));\n        __m128i negated = _mm_xor_si128(src_vec, rot_mask);\n        \n        __m128i result = _mm_blendv_epi8(negated, rotated, cond);\n        _mm_storeu_si128((__m128i*)(dst + i), result);\n    }\n    \n    for(; i < length; i++){\n        uint32_t src_val = src[i];\n        uint32_t dst_val;\n        if((src_val & 0xF) == 0xF){\n            dst_val = ((src_val << rotate_bits) | (src_val >> (32 - rotate_bits)));\n        }\n        else{\n            dst_val = ~src_val;\n        }\n        dst[i] = dst_val;\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i ones = _mm_set1_epi32(0xF);\n    __m128i rot_mask = _mm_set1_epi32((1u << rotate_bits) - 1);\n    \n    for(size_t i = 0; i < length; i += 4){\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i src_low4 = _mm_and_si128(src_vec, mask);\n        __m128i cond = _mm_cmpeq_epi32(src_low4, ones);\n        \n        __m128i src_rotated = _mm_or_si128(\n            _mm_andnot_si128(cond, _mm_xor_si128(src_vec, _mm_set1_epi32(-1))),\n            _mm_and_si128(cond, _mm_srl_epi32(_mm_sll_epi32(src_vec, _mm_cvtsi32_si128(rotate_bits)), rot_mask))\n        );\n        \n        _mm_storeu_si128((__m128i*)&dst[i], src_rotated);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    __m128i mask = _mm_set_epi32(0x0F000000, 0x0F000000, 0x0F000000, 0x0F000000);\n    __m128i ones = _mm_set1_epi32(0xF);\n    __m128i rot_mask = _mm_set1_epi32((1u << rotate_bits) - 1);\n    \n    for(size_t i = 0; i < length; i += 4){\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i low_nybble = _mm_and_si128(src_vec, mask);\n        __m128i compare = _mm_cmpeq_epi32(low_nybble, ones);\n        \n        __m128i src_rotated = _mm_or_si128(_mm_slli_epi32(src_vec, rotate_bits), _mm_srli_epi32(src_vec, 32 - rotate_bits));\n        __m128i src_negated = _mm_xor_si128(src_vec, rot_mask);\n        \n        __m128i result = _mm_blendv_epi8(src_negated, src_rotated, compare);\n        _mm_storeu_si128((__m128i*)&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_SSE", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length){\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i ones = _mm_set1_epi32(0xF);\n    __m128i rot_mask = _mm_set1_epi32((1u << rotate_bits) - 1);\n\n    for(size_t i = 0; i < length; i += 4){\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i low_nibble = _mm_and_si128(src_vec, mask);\n        __m128i cond = _mm_cmpeq_epi32(low_nibble, ones);\n        __m128i src_rot = _mm_or_si128(_mm_slli_epi32(src_vec, rotate_bits), _mm_srli_epi32(src_vec, 32 - rotate_bits));\n        __m128i src_neg = _mm_xor_si128(src_vec, rot_mask);\n        __m128i dst_vec = _mm_blendv_epi8(src_neg, src_rot, cond);\n        _mm_storeu_si128((__m128i*)&dst[i], dst_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 zero = _mm_setzero_ps();\n    __m128 greater_mask = _mm_set1_ps(1.0f);\n    __m128 less_mask = _mm_set1_ps(-1.0f);\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_abs_ps(diff_vec);\n        __m128 greater_than_eps = _mm_cmpgt_ps(abs_diff_vec, eps);\n        __m128 less_than_neg_eps = _mm_cmplt_ps(abs_diff_vec, _mm_sub_ps(zero, eps));\n\n        __m128 result_vec = _mm_blendv_ps(_mm_blendv_ps(less_mask, zero, greater_than_eps), greater_mask, less_than_neg_eps);\n\n        _mm_storeu_si128((__m128i*)(out + i), _mm_castps_si128(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (fabsf(A[i] - B[i]) > epsilon) {\n            out[i] = (A[i] - B[i] > 0) ? 1 : -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m128 eps_vec = _mm_set1_ps(epsilon);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 pos_mask = _mm_set1_ps(1.0f);\n    __m128 neg_mask = _mm_set1_ps(-1.0f);\n\n    for (; i + 4 <= length; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_max_ps(_mm_sub_ps(zero_vec, diff_vec), diff_vec);\n        __m128 cmp_pos = _mm_cmpgt_ps(abs_diff_vec, eps_vec);\n        __m128 cmp_neg = _mm_cmplt_ps(abs_diff_vec, _mm_sub_ps(zero_vec, eps_vec));\n\n        __m128 result_vec = _mm_blendv_ps(_mm_blendv_ps(neg_mask, zero_vec, cmp_neg), pos_mask, cmp_pos);\n\n        _mm_storeu_si128((__m128i*)(&out[i]), _mm_cvtps_epi32(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 zero = _mm_setzero_ps();\n    __m128 greater_mask = _mm_set1_ps(1.0f);\n    __m128 less_mask = _mm_set1_ps(-1.0f);\n\n    for (; i <= length - 8; i += 8) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_max_ps(_mm_sub_ps(zero, diff_vec), _mm_sub_ps(diff_vec, zero));\n        \n        __m128 greater_than_eps = _mm_cmpgt_ps(abs_diff_vec, eps);\n        __m128 less_than_neg_eps = _mm_cmplt_ps(abs_diff_vec, _mm_sub_ps(zero, eps));\n\n        __m128 result_vec = _mm_blendv_ps(_mm_blendv_ps(greater_mask, less_mask, greater_than_eps), zero, less_than_neg_eps);\n\n        _mm_storeu_si128((__m128i*)(out + i), _mm_cvtps_epi32(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m128 eps = _mm_set1_ps(epsilon);\n    __m128 zero = _mm_setzero_ps();\n    __m128 greater_mask = _mm_set1_ps(1.0f);\n    __m128 less_mask = _mm_set1_ps(-1.0f);\n\n    for (; i <= length - 8; i += 8) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_max_ps(_mm_sub_ps(zero, diff_vec), _mm_sub_ps(diff_vec, zero));\n        \n        __m128 greater_than_eps = _mm_cmpgt_ps(abs_diff_vec, eps);\n        __m128 less_than_neg_eps = _mm_cmplt_ps(abs_diff_vec, _mm_sub_ps(zero, eps));\n\n        __m128 result_vec = _mm_blendv_ps(_mm_blendv_ps(greater_mask, less_mask, greater_than_eps), zero, less_than_neg_eps);\n\n        _mm_storeu_si128((__m128i*)(out + i), _mm_cvtps_epi32(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_49_SSE", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    size_t i = 0;\n    __m128 eps_vec = _mm_set1_ps(epsilon);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n    __m128 neg_one_vec = _mm_set1_ps(-1.0f);\n\n    for (; i <= length - 4; i += 4) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 b_vec = _mm_loadu_ps(&B[i]);\n        __m128 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m128 abs_diff_vec = _mm_max_ps(_mm_sub_ps(zero_vec, diff_vec), diff_vec);\n        __m128 cmp_gt_vec = _mm_cmpgt_ps(abs_diff_vec, eps_vec);\n        __m128 cmp_lt_vec = _mm_cmplt_ps(abs_diff_vec, eps_vec);\n\n        __m128 result_vec = _mm_blendv_ps(_mm_blendv_ps(neg_one_vec, zero_vec, cmp_gt_vec), one_vec, cmp_lt_vec);\n\n        _mm_storeu_si128((__m128i*)(out + i), _mm_castps_si128(result_vec));\n    }\n\n    for (; i < length; i++) {\n        if (A[i] - B[i] > epsilon) {\n            out[i] = 1;\n        } else if (A[i] - B[i] < -epsilon) {\n            out[i] = -1;\n        } else {\n            out[i] = 0;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128 max_val = _mm_set1_ps((float)INT32_MAX);\n    __m128 min_val = _mm_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vals = _mm_loadu_ps(A + i);\n        __m128 saturated = _mm_max_ps(a_vals, min_val);\n        saturated = _mm_min_ps(saturated, max_val);\n\n        __m128i b_vals = _mm_cvttps_epi32(saturated);\n        _mm_storeu_si128((__m128i*)(B + i), b_vals);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128 max_val = _mm_set1_ps((float)INT32_MAX);\n    __m128 min_val = _mm_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vals = _mm_loadu_ps(A + i);\n        __m128 saturated = _mm_max_ps(a_vals, min_val);\n        saturated = _mm_min_ps(saturated, max_val);\n\n        __m128i b_vals = _mm_cvttps_epi32(saturated);\n        _mm_storeu_si128((__m128i*)(B + i), b_vals);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128 max_val = _mm_set1_ps((float)INT32_MAX);\n    __m128 min_val = _mm_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vals = _mm_loadu_ps(&A[i]);\n        __m128 saturated = _mm_max_ps(a_vals, min_val);\n        saturated = _mm_min_ps(saturated, max_val);\n\n        __m128i b_vals = _mm_cvttps_epi32(saturated);\n        _mm_storeu_si128((__m128i*)&B[i], b_vals);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128 max_val = _mm_set1_ps((float)INT32_MAX);\n    __m128 min_val = _mm_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vals = _mm_loadu_ps(&A[i]);\n        __m128 saturated = _mm_max_ps(a_vals, min_val);\n        saturated = _mm_min_ps(saturated, max_val);\n\n        // Convert to integer\n        __m128i int_vals = _mm_cvttps_epi32(saturated);\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)&B[i], int_vals);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_50_SSE", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128 max_val = _mm_set1_ps((float)INT32_MAX);\n    __m128 min_val = _mm_set1_ps((float)INT32_MIN);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vals = _mm_loadu_ps(&A[i]);\n        __m128 saturated = _mm_max_ps(a_vals, min_val);\n        saturated = _mm_min_ps(saturated, max_val);\n\n        __m128i b_vals = _mm_cvttps_epi32(saturated);\n        _mm_storeu_si128((__m128i*)&B[i], b_vals);\n    }\n\n    for (; i < size; i++) {\n        float val = A[i];\n        if (val > INT32_MAX) {\n            B[i] = INT32_MAX;\n        } else if (val < INT32_MIN) {\n            B[i] = INT32_MIN;\n        } else {\n            B[i] = (int32_t)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i;\n    __m128 scale_vec = _mm_set1_ps((float)(1 << scale));\n    \n    for(i = 0; i + 4 <= size; i += 4){\n        __m128i a_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128 b_vec = _mm_cvtepi32_ps(a_vec);\n        b_vec = _mm_mul_ps(b_vec, scale_vec);\n        _mm_storeu_ps(B + i, b_vec);\n    }\n    \n    for(; i < size; i++){\n        B[i] = (float)A[i] * (1 << scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i;\n    __m128 scale_vec = _mm_set1_ps((float)(1 << scale));\n    \n    for(i = 0; i + 4 <= size; i += 4){\n        __m128i A_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128 A_float = _mm_cvtepi32_ps(A_vec);\n        __m128 scaled = _mm_mul_ps(A_float, scale_vec);\n        _mm_storeu_ps(B + i, scaled);\n    }\n    \n    for(; i < size; i++){\n        B[i] = (float)A[i] / (1 << scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i;\n    __m128 scale_vec = _mm_set1_ps((float)(1 << scale));\n    \n    for(i = 0; i + 4 <= size; i += 4){\n        __m128i A_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128 A_float = _mm_cvtepi32_ps(A_vec);\n        __m128 scaled = _mm_mul_ps(A_float, scale_vec);\n        _mm_storeu_ps(B + i, scaled);\n    }\n    \n    for(; i < size; i++){\n        B[i] = (float)A[i] / (1 << scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i;\n    __m128 scale_vec = _mm_set1_ps((float)(1 << scale));\n    \n    for(i = 0; i + 4 <= size; i += 4){\n        __m128i A_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128 A_float = _mm_cvtepi32_ps(A_vec);\n        __m128 scaled = _mm_mul_ps(A_float, scale_vec);\n        _mm_storeu_ps(B + i, scaled);\n    }\n    \n    for(; i < size; i++){\n        B[i] = (float)A[i] / (1 << scale);\n    }\n}\n```"}
{"task_id": "SimdBench_51_SSE", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    size_t i;\n    __m128 scale_vec = _mm_set1_ps((float)(1 << scale));\n    \n    for(i = 0; i + 4 <= size; i += 4){\n        __m128i A_vec = _mm_loadu_si128((__m128i*)(A + i));\n        __m128 A_float = _mm_cvtepi32_ps(A_vec);\n        __m128 scaled = _mm_mul_ps(A_float, scale_vec);\n        _mm_storeu_ps(B + i, scaled);\n    }\n    \n    for(; i < size; i++){\n        B[i] = (float)A[i] / (1 << scale);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 a_vec, b_vec, result_vec;\n    __m128i result_int_vec;\n\n    // Process data in chunks of 4 elements (SSE)\n    for (; i <= size - 4; i += 4) {\n        a_vec = _mm_loadu_ps(A + i);\n        b_vec = _mm_loadu_ps(B + i);\n\n        // Perform division\n        result_vec = _mm_div_ps(a_vec, b_vec);\n\n        // Convert to integer, rounding to nearest\n        result_int_vec = _mm_cvttps_epi32(result_vec);\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)(C + i), result_int_vec);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 a_vec, b_vec, result_vec;\n    __m128i result_int_vec;\n\n    // Process data in chunks of 4 elements using SIMD\n    for (; i <= size - 4; i += 4) {\n        a_vec = _mm_loadu_ps(A + i);\n        b_vec = _mm_loadu_ps(B + i);\n\n        // Perform division\n        result_vec = _mm_div_ps(a_vec, b_vec);\n\n        // Convert to integer, rounding to nearest\n        result_int_vec = _mm_cvttps_epi32(result_vec);\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)(C + i), result_int_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        C[i] = (int32_t)(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 a_vec, b_vec, result_vec;\n    __m128i result_int_vec;\n\n    // Process data in chunks of 4 elements using SIMD\n    for (; i <= size - 4; i += 4) {\n        a_vec = _mm_loadu_ps(&A[i]);\n        b_vec = _mm_loadu_ps(&B[i]);\n\n        // Perform division\n        result_vec = _mm_div_ps(a_vec, b_vec);\n\n        // Convert to integer and round\n        result_int_vec = _mm_cvttps_epi32(result_vec);\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)&C[i], result_int_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        C[i] = static_cast<int32_t>(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 a_vec, b_vec, result_vec;\n    __m128i result_int_vec;\n\n    // Process data in chunks of 4 elements using SIMD\n    for (; i <= size - 4; i += 4) {\n        a_vec = _mm_loadu_ps(&A[i]);\n        b_vec = _mm_loadu_ps(&B[i]);\n\n        // Perform division\n        result_vec = _mm_div_ps(a_vec, b_vec);\n\n        // Convert to integer, rounding to nearest\n        result_int_vec = _mm_cvttps_epi32(result_vec);\n\n        // Store the result back to memory\n        _mm_storeu_si128((__m128i*)&C[i], result_int_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        C[i] = (int32_t)roundf(A[i] / B[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_52_SSE", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    size_t i = 0;\n    __m128 a_vec, b_vec, result_vec;\n    __m128i result_int_vec;\n\n    // Process data in chunks of 4 elements (SSE width)\n    for (; i <= size - 4; i += 4) {\n        a_vec = _mm_loadu_ps(A + i);\n        b_vec = _mm_loadu_ps(B + i);\n\n        // Perform division\n        result_vec = _mm_div_ps(a_vec, b_vec);\n\n        // Convert to integer (rounding happens here)\n        result_int_vec = _mm_cvttps_epi32(result_vec);\n\n        // Store the result\n        _mm_storeu_si128((__m128i*)(C + i), result_int_vec);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        C[i] = static_cast<int32_t>(roundf(A[i] / B[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128d max_val = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_val = _mm_set1_pd(-(double)FLT_MAX);\n    __m128 sat_max = _mm_set1_ps(FLT_MAX);\n    __m128 sat_min = _mm_set1_ps(-FLT_MAX);\n\n    for (; i <= size - 4; i += 4) {\n        __m128d vec_d = _mm_loadu_pd(&A[i]);\n        __m128d vec_d2 = _mm_loadu_pd(&A[i + 2]);\n\n        __m128d mask_max = _mm_cmpgt_pd(vec_d, max_val);\n        __m128d mask_min = _mm_cmplt_pd(vec_d, min_val);\n        __m128d mask_max2 = _mm_cmpgt_pd(vec_d2, max_val);\n        __m128d mask_min2 = _mm_cmplt_pd(vec_d2, min_val);\n\n        __m128 vec_f = _mm_cvtpd_ps(vec_d);\n        __m128 vec_f2 = _mm_cvtpd_ps(vec_d2);\n\n        vec_f = _mm_or_ps(_mm_andnot_ps(mask_max, vec_f), _mm_and_ps(mask_max, sat_max));\n        vec_f = _mm_or_ps(_mm_andnot_ps(mask_min, vec_f), _mm_and_ps(mask_min, sat_min));\n\n        vec_f2 = _mm_or_ps(_mm_andnot_ps(mask_max2, vec_f2), _mm_and_ps(mask_max2, sat_max));\n        vec_f2 = _mm_or_ps(_mm_andnot_ps(mask_min2, vec_f2), _mm_and_ps(mask_min2, sat_min));\n\n        _mm_storeu_ps(&B[i], vec_f);\n        _mm_storeu_ps(&B[i + 2], vec_f2);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        float result = (val > (double)FLT_MAX) ? FLT_MAX : ((val < -(double)FLT_MAX) ? -FLT_MAX : (float)val);\n        B[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128d max_val_d = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_val_d = _mm_set1_pd((double)-FLT_MAX);\n    __m128 max_val_f = _mm_set1_ps(FLT_MAX);\n    __m128 min_val_f = _mm_set1_ps(-FLT_MAX);\n\n    for (; i <= size - 4; i += 4) {\n        __m128d a0 = _mm_loadu_pd(&A[i]);\n        __m128d a1 = _mm_loadu_pd(&A[i + 2]);\n        __m128d mask0 = _mm_cmpgt_pd(a0, max_val_d);\n        __m128d mask1 = _mm_cmplt_pd(a0, min_val_d);\n        __m128d mask2 = _mm_cmpgt_pd(a1, max_val_d);\n        __m128d mask3 = _mm_cmplt_pd(a1, min_val_d);\n        __m128d saturated0 = _mm_or_pd(_mm_and_pd(mask0, max_val_d), _mm_andnot_pd(mask0, a0));\n        __m128d saturated1 = _mm_or_pd(_mm_and_pd(mask1, min_val_d), _mm_andnot_pd(mask1, a0));\n        __m128d saturated2 = _mm_or_pd(_mm_and_pd(mask2, max_val_d), _mm_andnot_pd(mask2, a1));\n        __m128d saturated3 = _mm_or_pd(_mm_and_pd(mask3, min_val_d), _mm_andnot_pd(mask3, a1));\n        __m128 saturated0_f = _mm_cvtpd_ps(saturated0);\n        __m128 saturated1_f = _mm_cvtpd_ps(saturated1);\n        __m128 saturated2_f = _mm_cvtpd_ps(saturated2);\n        __m128 saturated3_f = _mm_cvtpd_ps(saturated3);\n        __m128 final_mask0 = _mm_cmple_ps(saturated0_f, max_val_f);\n        __m128 final_mask1 = _mm_cmpeq_ps(saturated0_f, min_val_f);\n        __m128 final_mask2 = _mm_cmple_ps(saturated1_f, max_val_f);\n        __m128 final_mask3 = _mm_cmpeq_ps(saturated1_f, min_val_f);\n        __m128 final_mask4 = _mm_cmple_ps(saturated2_f, max_val_f);\n        __m128 final_mask5 = _mm_cmpeq_ps(saturated2_f, min_val_f);\n        __m128 final_mask6 = _mm_cmple_ps(saturated3_f, max_val_f);\n        __m128 final_mask7 = _mm_cmpeq_ps(saturated3_f, min_val_f);\n        __m128 final_result0 = _mm_or_ps(_mm_and_ps(final_mask0, saturated0_f), _mm_andnot_ps(final_mask0, max_val_f));\n        __m128 final_result1 = _mm_or_ps(_mm_and_ps(final_mask1, saturated1_f), _mm_andnot_ps(final_mask1, min_val_f));\n        __m128 final_result2 = _mm_or_ps(_mm_and_ps(final_mask2, saturated2_f), _mm_andnot_ps(final_mask2, max_val_f));\n        __m128 final_result3 = _mm_or_ps(_mm_and_ps(final_mask3, saturated3_f), _mm_andnot_ps(final_mask3, min_val_f));\n        _mm_storeu_ps(&B[i], final_result0);\n        _mm_storeu_ps(&B[i + 4], final_result1);\n        _mm_storeu_ps(&B[i + 8], final_result2);\n        _mm_storeu_ps(&B[i + 12], final_result3);\n    }\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > (double)FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < (double)-FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128d max_val = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_val = _mm_set1_pd(-(double)FLT_MAX);\n    __m128 sat_max = _mm_set1_ps(FLT_MAX);\n    __m128 sat_min = _mm_set1_ps(-FLT_MAX);\n\n    for (; i <= size - 4; i += 4) {\n        __m128d vec_d = _mm_loadu_pd(&A[i]);\n        __m128d vec_d_high = _mm_unpackhi_pd(vec_d, vec_d);\n        __m128d vec_d_low = _mm_unpacklo_pd(vec_d, vec_d);\n\n        __m128d min_mask_low = _mm_cmplt_pd(vec_d_low, min_val);\n        __m128d max_mask_low = _mm_cmpgt_pd(vec_d_low, max_val);\n        __m128d min_mask_high = _mm_cmplt_pd(vec_d_high, min_val);\n        __m128d max_mask_high = _mm_cmpgt_pd(vec_d_high, max_val);\n\n        __m128d saturated_low = _mm_or_pd(_mm_and_pd(min_mask_low, min_val), _mm_andnot_pd(min_mask_low, vec_d_low));\n        __m128d saturated_high = _mm_or_pd(_mm_and_pd(max_mask_high, max_val), _mm_andnot_pd(max_mask_high, vec_d_high));\n\n        __m128 saturated_low_f = _mm_cvtpd_ps(saturated_low);\n        __m128 saturated_high_f = _mm_cvtpd_ps(saturated_high);\n\n        _mm_storeu_ps(&B[2 * i], saturated_low_f);\n        _mm_storeu_ps(&B[2 * i + 4], saturated_high_f);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        float result = (val > (double)FLT_MAX) ? FLT_MAX : ((val < -(double)FLT_MAX) ? -FLT_MAX : (float)val);\n        B[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128d max_val = _mm_set1_pd((double)FLT_MAX);\n    __m128d min_val = _mm_set1_pd(-(double)FLT_MAX);\n    __m128 sat_max = _mm_set1_ps(FLT_MAX);\n    __m128 sat_min = _mm_set1_ps(-FLT_MAX);\n\n    for (; i <= size - 4; i += 4) {\n        __m128d a0 = _mm_loadu_pd(&A[i]);\n        __m128d a1 = _mm_loadu_pd(&A[i + 2]);\n        __m128d mask0 = _mm_cmpgt_pd(a0, max_val);\n        __m128d mask1 = _mm_cmplt_pd(a0, min_val);\n        __m128d mask2 = _mm_cmpgt_pd(a1, max_val);\n        __m128d mask3 = _mm_cmplt_pd(a1, min_val);\n\n        __m128 a0f = _mm_cvtpd_ps(a0);\n        __m128 a1f = _mm_cvtpd_ps(a1);\n\n        a0f = _mm_or_ps(_mm_andnot_ps(mask0, a0f), _mm_and_ps(mask0, sat_max));\n        a0f = _mm_or_ps(_mm_andnot_ps(mask1, a0f), _mm_and_ps(mask1, sat_min));\n\n        a1f = _mm_or_ps(_mm_andnot_ps(mask2, a1f), _mm_and_ps(mask2, sat_max));\n        a1f = _mm_or_ps(_mm_andnot_ps(mask3, a1f), _mm_and_ps(mask3, sat_min));\n\n        _mm_storeu_ps(&B[i], a0f);\n        _mm_storeu_ps(&B[i + 4], a1f);\n    }\n\n    for (; i < size; i++) {\n        double val = A[i];\n        if (val > (double)FLT_MAX) {\n            B[i] = FLT_MAX;\n        } else if (val < -(double)FLT_MAX) {\n            B[i] = -FLT_MAX;\n        } else {\n            B[i] = (float)val;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_53_SSE", "completion": "```c\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    size_t i;\n    __m128d max_val = _mm_set1_pd(FLT_MAX);\n    __m128d min_val = _mm_set1_pd(-FLT_MAX);\n    __m128 sat_max = _mm_set1_ps(FLT_MAX);\n    __m128 sat_min = _mm_set1_ps(-FLT_MAX);\n\n    for(i = 0; i + 2 <= size; i += 2){\n        __m128d a = _mm_loadu_pd(&A[i]);\n        __m128d cmp_max = _mm_cmpgt_pd(a, max_val);\n        __m128d cmp_min = _mm_cmplt_pd(a, min_val);\n        \n        __m128 a_conv = _mm_cvtpd_ps(a);\n        __m128 max_conv = _mm_cvtpd_ps(max_val);\n        __m128 min_conv = _mm_cvtpd_ps(min_val);\n\n        __m128 result = _mm_or_ps(\n            _mm_andnot_ps(cmp_max, _mm_andnot_ps(cmp_min, a_conv)),\n            _mm_or_ps(_mm_and_ps(cmp_max, sat_max), _mm_and_ps(cmp_min, sat_min))\n        );\n        \n        _mm_storeu_ps(&B[i], result);\n    }\n\n    // Handle remaining elements if size is not even\n    for(; i < size; i++) {\n        double val = A[i];\n        float conv = (val > FLT_MAX) ? FLT_MAX : ((val < -FLT_MAX) ? -FLT_MAX : (float)val);\n        B[i] = conv;\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128i* src = (__m128i*)A;\n    __m128i* dst = (__m128i*)B;\n\n    // Process full vectors\n    for (; i < size - 7; i += 8) {\n        __m128i vec = _mm_load_si128(src + i);\n        __m128i extended = _mm_srai_epi32(_mm_slli_epi32(vec, 16), 16);\n        _mm_store_si128(dst + i, extended);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128i* src = (__m128i*)A;\n    __m128i* dst = (__m128i*)B;\n\n    // Process full vectors\n    for (; i < size - 7; i += 8) {\n        __m128i vec = _mm_load_si128(src + i);\n        __m128i extended = _mm_cvtepi16_epi32(vec);\n        _mm_store_si128(dst + i, extended);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128i* src = (__m128i*)A;\n    __m128i* dst = (__m128i*)B;\n\n    // Process full vectors\n    for (; i < size - 7; i += 8) {\n        __m128i vec = _mm_load_si128(src + i);\n        __m128i extended = _mm_srai_epi32(_mm_slli_epi32(vec, 16), 16);\n        _mm_store_si128(dst + i, extended);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128i* src = (__m128i*)A;\n    __m128i* dst = (__m128i*)B;\n\n    // Process full vectors\n    for (; i < size - 7; i += 8) {\n        __m128i vec = _mm_load_si128(src + i);\n        __m128i extended = _mm_srai_epi32(_mm_slli_epi32(vec, 16), 16);\n        _mm_store_si128(dst + i, extended);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (int32_t)A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_54_SSE", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    size_t i = 0;\n    __m128i* src = (__m128i*)A;\n    __m128i* dst = (__m128i*)B;\n\n    // Process full vectors\n    for (; i < size - 7; i += 8) {\n        __m128i vec = _mm_loadu_si128(src + i);\n        __m128i extended = _mm_cvtepi16_epi32(vec);\n        _mm_storeu_si128(dst + i, extended);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = static_cast<int32_t>(A[i]);\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 c_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&control[i]));\n        \n        // Compute (A[i] - min_val)\n        __m128 diff_vec = _mm_sub_ps(a_vec, min_val_vec);\n\n        // Compute (max_val - min_val)\n        __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n\n        // Conditional check: If control > 0, perform normalization\n        __m128 mask = _mm_cmpgt_epi32(_mm_castps_si128(c_vec), _mm_setzero_si128());\n        __m128 normalized_vec = _mm_div_ps(diff_vec, range_vec);\n        normalized_vec = _mm_max_ps(_mm_min_ps(normalized_vec, one_vec), zero_vec);\n\n        // If control <= 0, copy A[i] to B[i]\n        __m128 a_copy_vec = _mm_loadu_ps(&A[i]);\n        __m128 result_vec = _mm_or_ps(\n            _mm_andnot_ps(mask, a_copy_vec),\n            _mm_and_ps(mask, normalized_vec)\n        );\n\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 c_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&control[i]));\n        \n        // Compute (A[i] - min_val)\n        __m128 diff_vec = _mm_sub_ps(a_vec, min_val_vec);\n\n        // Compute (max_val - min_val)\n        __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n\n        // Compute (A[i] - min_val) / (max_val - min_val)\n        __m128 norm_vec = _mm_div_ps(diff_vec, range_vec);\n\n        // Clamp to [0, 1]\n        __m128 clamp_low = _mm_max_ps(norm_vec, zero_vec);\n        __m128 clamp_high = _mm_min_ps(clamp_low, one_vec);\n\n        // Compare control values with 0\n        __m128i cmp_mask = _mm_cmpgt_epi32(_mm_castps_si128(c_vec), _mm_setzero_si128());\n\n        // Select between normalized and original values\n        __m128 result_vec = _mm_blendv_ps(a_vec, clamp_high, _mm_castsi128_ps(cmp_mask));\n\n        // Store the result\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (control[i] > 0) ? fmaxf(0.0f, fminf((A[i] - min_val) / (max_val - min_val), 1.0f)) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    while (i + 4 <= size) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 c_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&control[i]));\n        __m128 mask = _mm_cmpgt_epi32(_mm_castps_si128(c_vec), _mm_setzero_si128());\n\n        __m128 normalized = _mm_sub_ps(a_vec, min_val_vec);\n        normalized = _mm_div_ps(normalized, _mm_sub_ps(max_val_vec, min_val_vec));\n        normalized = _mm_max_ps(zero_vec, _mm_min_ps(normalized, one_vec));\n\n        __m128 a_copy = _mm_shuffle_ps(a_vec, a_vec, _MM_SHUFFLE(0, 0, 0, 0));\n        __m128 result = _mm_or_ps(_mm_and_ps(mask, normalized), _mm_andnot_ps(mask, a_copy));\n\n        _mm_storeu_ps(&B[i], result);\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 c_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&control[i]));\n        \n        // Compute (A[i] - min_val)\n        __m128 diff_vec = _mm_sub_ps(a_vec, min_val_vec);\n\n        // Compute (max_val - min_val)\n        __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n\n        // Compute (A[i] - min_val) / (max_val - min_val)\n        __m128 norm_vec = _mm_div_ps(diff_vec, range_vec);\n\n        // Clamp to [0, 1]\n        __m128 clamp_low = _mm_max_ps(norm_vec, zero_vec);\n        __m128 clamp_high = _mm_min_ps(clamp_low, one_vec);\n\n        // Compare control values with 0\n        __m128i cmp_mask = _mm_cmpgt_epi32(_mm_castps_si128(c_vec), _mm_setzero_si128());\n\n        // Select between normalized and original values\n        __m128 result_vec = _mm_blendv_ps(a_vec, clamp_high, _mm_castsi128_ps(cmp_mask));\n\n        // Store the result\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = (control[i] > 0) ? fmaxf(0.0f, fminf((A[i] - min_val) / (max_val - min_val), 1.0f)) : A[i];\n    }\n}\n```"}
{"task_id": "SimdBench_55_SSE", "completion": "```c\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    size_t i = 0;\n    __m128 min_val_vec = _mm_set1_ps(min_val);\n    __m128 max_val_vec = _mm_set1_ps(max_val);\n    __m128 zero_vec = _mm_setzero_ps();\n    __m128 one_vec = _mm_set1_ps(1.0f);\n\n    for (; i <= size - 8; i += 8) {\n        __m128 a_vec = _mm_loadu_ps(&A[i]);\n        __m128 c_vec = _mm_castsi128_ps(_mm_loadu_si128((__m128i*)&control[i]));\n        \n        // Compute (A[i] - min_val)\n        __m128 diff_vec = _mm_sub_ps(a_vec, min_val_vec);\n\n        // Compute (max_val - min_val)\n        __m128 range_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n\n        // Conditional check for control > 0\n        __m128 mask_vec = _mm_cmpgt_epi32(_mm_castps_si128(c_vec), _mm_setzero_si128());\n\n        // Normalize only where control > 0\n        __m128 norm_vec = _mm_div_ps(diff_vec, range_vec);\n        norm_vec = _mm_max_ps(norm_vec, zero_vec);\n        norm_vec = _mm_min_ps(norm_vec, one_vec);\n\n        // Blend between original value and normalized value\n        __m128 result_vec = _mm_blendv_ps(a_vec, norm_vec, mask_vec);\n\n        // Store the result\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (control[i] > 0) {\n            float val = (A[i] - min_val) / (max_val - min_val);\n            B[i] = val < 0.0f ? 0.0f : (val > 1.0f ? 1.0f : val);\n        } else {\n            B[i] = A[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128i v_a;\n    __m128 v_b;\n\n    // Process 4 elements per iteration\n    for (; i <= size - 4; i += 4) {\n        // Load 4 uint32_t values from A\n        v_a = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Shift right by 16 to get the high 16 bits of each uint32_t\n        v_a = _mm_srli_epi32(v_a, 16);\n\n        // Convert the 4 uint32_t values to 4 float values\n        v_b = _mm_cvtepi32_ps(v_a);\n\n        // Compute the square root of the 4 float values\n        v_b = _mm_sqrt_ps(v_b);\n\n        // Store the 4 float results into B\n        _mm_storeu_ps(B + i, v_b);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128i v_a;\n    __m128 v_b;\n\n    // Process full vectors\n    for (; i <= size - 4; i += 4) {\n        // Load 4 uint32_t values from A\n        v_a = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Shift right by 16 to get the high 16 bits\n        v_a = _mm_srli_epi32(v_a, 16);\n\n        // Convert to float\n        v_b = _mm_cvtepi32_ps(v_a);\n\n        // Compute the square root\n        v_b = _mm_sqrt_ps(v_b);\n\n        // Store the result in B\n        _mm_storeu_ps(B + i, v_b);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128i v_a;\n    __m128 v_b;\n\n    // Process 4 elements per iteration\n    for (; i <= size - 4; i += 4) {\n        // Load 4 uint32_t values from A\n        v_a = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Shift right by 16 to get the high 16 bits of each uint32_t\n        v_a = _mm_srli_epi32(v_a, 16);\n\n        // Convert the 4 uint32_t values to 4 float values\n        v_b = _mm_cvtepi32_ps(v_a);\n\n        // Compute the square root of the 4 float values\n        v_b = _mm_sqrt_ps(v_b);\n\n        // Store the 4 float results into B\n        _mm_storeu_ps(B + i, v_b);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128i v_a;\n    __m128 v_b;\n\n    // Process 4 elements per iteration\n    for (; i <= size - 4; i += 4) {\n        // Load 4 uint32_t values from A\n        v_a = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Shift right by 16 to get the high 16 bits\n        v_a = _mm_srli_epi32(v_a, 16);\n\n        // Convert to float\n        v_b = _mm_cvtepi32_ps(v_a);\n\n        // Compute the square root\n        v_b = _mm_sqrt_ps(v_b);\n\n        // Store the result in B\n        _mm_storeu_ps(B + i, v_b);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_56_SSE", "completion": "```c\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128i v_a;\n    __m128 v_b;\n\n    // Process full vectors\n    for (; i <= size - 4; i += 4) {\n        // Load 4 uint32_t values from A\n        v_a = _mm_loadu_si128((__m128i*)(A + i));\n\n        // Shift right by 16 to get the high 16 bits\n        v_a = _mm_srli_epi32(v_a, 16);\n\n        // Convert to float\n        v_b = _mm_cvtepi32_ps(v_a);\n\n        // Compute the square root\n        v_b = _mm_sqrt_ps(v_b);\n\n        // Store the result in B\n        _mm_storeu_ps(B + i, v_b);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = sqrtf((float)((A[i] >> 16) & 0xFFFF));\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128* B_vec = (__m128*)B;\n\n    // Process full vectors\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec_a = _mm_loadu_si128(A_vec + i);\n        __m128 vec_b = _mm_castsi128_ps(vec_a);\n        _mm_storeu_ps(B_vec + i, vec_b);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i = 0;\n    __m128i* A_vec = (__m128i*)A;\n    __m128* B_vec = (__m128*)B;\n\n    // Process full vectors\n    for (; i + 4 <= size; i += 4) {\n        __m128i vec_a = _mm_loadu_si128(A_vec + i);\n        __m128 vec_b = _mm_castsi128_ps(vec_a);\n        _mm_storeu_ps(B_vec + i, vec_b);\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    __m128i vec_in;\n    __m128 vec_out;\n\n    for (i = 0; i + 4 <= size; i += 4) {\n        vec_in = _mm_loadu_si128((__m128i*)(A + i));\n        vec_out = _mm_castsi128_ps(vec_in);\n        _mm_storeu_ps(B + i, vec_out);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    __m128i vec_in;\n    __m128 vec_out;\n\n    for (i = 0; i + 4 <= size; i += 4) {\n        vec_in = _mm_loadu_si128((__m128i*)(A + i));\n        vec_out = _mm_castsi128_ps(vec_in);\n        _mm_storeu_ps(B + i, vec_out);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_57_SSE", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    size_t i;\n    __m128i vec_in;\n    __m128 vec_out;\n\n    for (i = 0; i + 4 <= size; i += 4) {\n        vec_in = _mm_loadu_si128((__m128i*)(A + i));\n        vec_out = _mm_castsi128_ps(vec_in);\n        _mm_storeu_ps(B + i, vec_out);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (; i < size; i++) {\n        B[i] = *(float*)(A + i);\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i prev = _mm_setzero_si128();\n    __m128 zero = _mm_setzero_ps();\n\n    size_t i = 0;\n    for (; i + 4 <= size; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i c = _mm_castps_si128(_mm_loadu_ps((float*)&cond[i]));\n        __m128 d_prev = _mm_cvtepi32_ps(prev);\n\n        __m128 d_a = _mm_cvtepi32_ps(a);\n        __m128 d_diff = _mm_sub_ps(d_a, d_prev);\n\n        __m128 mask = _mm_castsi128_ps(c);\n        d_diff = _mm_and_ps(d_diff, mask);\n\n        _mm_storeu_ps(&diff[i], d_diff);\n\n        prev = a;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128 zero = _mm_setzero_ps();\n    __m128i prev = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m128i curr = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Compute differences\n        __m128i diff_int = _mm_sub_epi32(curr, prev);\n\n        // Convert to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Apply condition mask\n        __m128 masked_diff = _mm_and_ps(_mm_castsi128_ps(mask), diff_float);\n\n        // Store result\n        _mm_storeu_ps(&diff[i], masked_diff);\n\n        // Update previous value\n        prev = curr;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128i prev = _mm_setzero_si128();\n    size_t i = 0;\n\n    // Process data in chunks of 4 elements\n    for (; i + 4 <= size; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i c = _mm_loadu_si128((__m128i*)&cond[i]);\n        __m128i mask = _mm_castps_si128(_mm_andnot_ps(_mm_castsi128_ps(c), _mm_set1_ps(1.0f)));\n\n        __m128 diff_vec = _mm_cvtepi32_ps(_mm_sub_epi32(a, prev));\n        diff_vec = _mm_andnot_ps(_mm_castsi128_ps(mask), diff_vec);\n\n        _mm_storeu_ps(&diff[i], diff_vec);\n\n        prev = _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 0, 0, 0));\n    }\n\n    // Handle remaining elements\n    for (; i < size; i++) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128 zero = _mm_setzero_ps();\n    __m128i prev = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m128i curr = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        __m128 diff_vec = _mm_cvtepi32_ps(_mm_sub_epi32(curr, prev));\n        __m128 masked_diff = _mm_and_ps(_mm_castsi128_ps(mask), diff_vec);\n\n        __m128 prev_diff = _mm_loadu_ps(&diff[i]);\n        __m128 result = _mm_add_ps(masked_diff, prev_diff);\n\n        _mm_storeu_ps(&diff[i], result);\n\n        prev = curr;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_SSE", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m128 zero = _mm_setzero_ps();\n    __m128i prev = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i <= size - 8; i += 8) {\n        __m128i curr = _mm_loadu_si128((__m128i*)&A[i]);\n        __m128i mask = _mm_loadu_si128((__m128i*)&cond[i]);\n\n        // Compute differences\n        __m128i diff_int = _mm_sub_epi32(curr, prev);\n\n        // Convert to float\n        __m128 diff_float = _mm_cvtepi32_ps(diff_int);\n\n        // Apply condition mask\n        __m128 masked_diff = _mm_and_ps(_mm_castsi128_ps(mask), diff_float);\n\n        // Store result\n        _mm_storeu_ps(&diff[i], masked_diff);\n\n        // Update previous value\n        prev = curr;\n    }\n\n    // Handle remaining elements\n    for (; i < size; ++i) {\n        if (cond[i]) {\n            diff[i] = static_cast<float>(A[i] - A[i - 1]);\n        } else {\n            diff[i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask = _mm_set_epi32(-1, -1, -1, -1);\n\n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i extended_vec = _mm_cvtepu32_epi64(src_vec);\n        __m128i result_vec = _mm_or_si128(extended_vec, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask = _mm_set_epi32(-1, -1, -1, -1);\n\n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i extended_vec = _mm_cvtepu32_epi64(src_vec);\n        __m128i result_vec = _mm_or_si128(extended_vec, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask = _mm_set_epi32(-1, -1, -1, -1);\n\n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i extended_vec = _mm_cvtepu32_epi64(src_vec);\n        __m128i result_vec = _mm_or_si128(extended_vec, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask = _mm_set_epi32(-1, -1, -1, -1);\n\n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i extended_vec = _mm_cvtepu32_epi64(src_vec);\n        __m128i result_vec = _mm_or_si128(extended_vec, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_59_SSE", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    size_t i = 0;\n    __m128i mask = _mm_set_epi32(-1, -1, -1, -1);\n\n    // Process full vectors\n    for (; i <= length - 4; i += 4) {\n        __m128i src_vec = _mm_loadu_si128((__m128i*)&src[i]);\n        __m128i extended_vec = _mm_cvtepu32_epi64(src_vec);\n        __m128i result_vec = _mm_or_si128(extended_vec, mask);\n        _mm_storeu_si128((__m128i*)&dst[i], result_vec);\n    }\n\n    // Handle remaining elements\n    for (; i < length; i++) {\n        dst[i] = ((uint64_t)src[i]) | 0xFFFFFFFF00000000ULL;\n    }\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 2 elements using SIMD\n    for (; i <= length - 4; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v0 = _mm_load_sd(&vec[idx[0]]);\n        __m128d v1 = _mm_load_sd(&vec[idx[1]]);\n        __m128d v2 = _mm_load_sd(&vec[idx[2]]);\n        __m128d v3 = _mm_load_sd(&vec[idx[3]]);\n        __m128d temp = _mm_add_pd(v0, v1);\n        temp = _mm_add_pd(temp, v2);\n        temp = _mm_add_pd(temp, v3);\n        sum = _mm_add_pd(sum, temp);\n    }\n\n    // Handle remaining elements\n    double scalar_sum = 0.0;\n    for (; i < length; i++) {\n        scalar_sum += vec[index[i]];\n    }\n\n    // Combine results\n    __m128d s0 = _mm_hadd_pd(sum, sum);\n    __m128d s1 = _mm_hadd_pd(s0, s0);\n    double final_sum = _mm_cvtsd_f64(s1) + scalar_sum;\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 2 elements using SSE2\n    for (; i <= length - 4; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v0 = _mm_load_sd(&vec[idx[0]]);\n        __m128d v1 = _mm_load_sd(&vec[idx[1]]);\n        __m128d v2 = _mm_load_sd(&vec[idx[2]]);\n        __m128d v3 = _mm_load_sd(&vec[idx[3]]);\n        __m128d temp = _mm_add_pd(v0, v1);\n        temp = _mm_add_pd(temp, v2);\n        temp = _mm_add_pd(temp, v3);\n        sum = _mm_add_pd(sum, temp);\n    }\n\n    // Handle remaining elements\n    double scalar_sum = 0.0;\n    for (; i < length; i++) {\n        scalar_sum += vec[index[i]];\n    }\n\n    // Combine results\n    __m128d s0 = _mm_hadd_pd(sum, sum);\n    __m128d s1 = _mm_hadd_pd(s0, s0);\n    double final_sum = _mm_cvtsd_f64(s1) + scalar_sum;\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 2 elements using SIMD\n    for (; i <= length - 4; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v0 = _mm_load_sd(&vec[idx[0]]);\n        __m128d v1 = _mm_load_sd(&vec[idx[1]]);\n        __m128d v2 = _mm_load_sd(&vec[idx[2]]);\n        __m128d v3 = _mm_load_sd(&vec[idx[3]]);\n        __m128d temp = _mm_add_pd(v0, v1);\n        temp = _mm_add_pd(temp, v2);\n        temp = _mm_add_pd(temp, v3);\n        sum = _mm_add_pd(sum, temp);\n    }\n\n    // Handle remaining elements\n    double scalar_sum = 0.0;\n    for (; i < length; i++) {\n        scalar_sum += vec[index[i]];\n    }\n\n    // Combine SIMD and scalar results\n    __m128d s_scalar = _mm_set_sd(scalar_sum);\n    __m128d s_sum = _mm_hadd_pd(sum, sum);\n    s_sum = _mm_hadd_pd(s_sum, s_sum);\n    double final_sum = s_sum[0] + s_scalar[0];\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 2 elements using SSE2\n    for (; i <= length - 4; i += 4) {\n        __m128i idx = _mm_loadu_si128((__m128i*)&index[i]);\n        __m128d v0 = _mm_load_sd(&vec[idx[0]]);\n        __m128d v1 = _mm_load_sd(&vec[idx[1]]);\n        __m128d v2 = _mm_load_sd(&vec[idx[2]]);\n        __m128d v3 = _mm_load_sd(&vec[idx[3]]);\n        __m128d temp = _mm_add_pd(v0, v1);\n        temp = _mm_add_pd(temp, v2);\n        temp = _mm_add_pd(temp, v3);\n        sum = _mm_add_pd(sum, temp);\n    }\n\n    // Handle remaining elements\n    double residual_sum = 0.0;\n    for (; i < length; i++) {\n        residual_sum += vec[index[i]];\n    }\n\n    // Combine results\n    __m128d res = _mm_add_pd(sum, _mm_set_sd(residual_sum));\n    double result[2];\n    _mm_storeu_pd(result, res);\n    return result[0] + result[1];\n}\n```"}
{"task_id": "SimdBench_60_SSE", "completion": "```c\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m128d sum = _mm_setzero_pd();\n    size_t i = 0;\n\n    // Process in chunks of 2 elements using SIMD\n    for (; i <= length - 4; i += 4) {\n        __m128i idx = _mm_set_epi16(index[i+3], index[i+2], index[i+1], index[i]);\n        __m128d v0 = _mm_load_sd(&vec[idx[0]]);\n        __m128d v1 = _mm_load_sd(&vec[idx[1]]);\n        __m128d v2 = _mm_load_sd(&vec[idx[2]]);\n        __m128d v3 = _mm_load_sd(&vec[idx[3]]);\n        __m128d temp = _mm_add_pd(v0, v1);\n        temp = _mm_add_pd(temp, v2);\n        temp = _mm_add_pd(temp, v3);\n        sum = _mm_add_pd(sum, temp);\n    }\n\n    // Handle remaining elements\n    double scalar_sum = 0.0;\n    for (; i < length; i++) {\n        scalar_sum += vec[index[i]];\n    }\n\n    // Combine SIMD and scalar results\n    __m128d s0 = _mm_set_sd(scalar_sum);\n    __m128d s1 = _mm_shuffle_pd(sum, s0, 0);\n    sum = _mm_add_pd(sum, s1);\n\n    return _mm_cvtsd_f64(sum);\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t i, j, k, l;\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d zero = _mm_setzero_pd();\n    __m128d relu_mask = _mm_set1_pd(0.0);\n\n    for (i = 0; i < output_size; i++) {\n        for (j = 0; j < output_size; j++) {\n            __m128d sum = zero;\n            for (k = 0; k < kernel_size; k++) {\n                for (l = 0; l < kernel_size; l++) {\n                    __m128d input_val = _mm_load_sd(&input[(i + k) * input_size + (j + l)]);\n                    __m128d kernel_val = _mm_load_sd(&kernel[k * kernel_size + l]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m128d result = _mm_max_pd(sum, zero);\n            _mm_store_sd(&output[i * output_size + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t i, j, k, l;\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d zero = _mm_setzero_pd();\n    __m128d relu_mask = _mm_set1_pd(0.0);\n\n    for (i = 0; i < output_size; i++) {\n        for (j = 0; j < output_size; j++) {\n            __m128d sum = zero;\n            for (k = 0; k < kernel_size; k++) {\n                for (l = 0; l < kernel_size; l++) {\n                    __m128d input_val = _mm_load_sd(&input[(i + k) * input_size + (j + l)]);\n                    __m128d kernel_val = _mm_load_sd(&kernel[k * kernel_size + l]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m128d result = _mm_max_pd(sum, zero);\n            _mm_store_sd(&output[i * output_size + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t i, j, k, l;\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d zero = _mm_setzero_pd();\n    __m128d relu_mask = _mm_set1_pd(0.0);\n\n    for (i = 0; i < output_size; i++) {\n        for (j = 0; j < output_size; j++) {\n            __m128d sum = zero;\n            for (k = 0; k < kernel_size; k++) {\n                for (l = 0; l < kernel_size; l++) {\n                    __m128d input_val = _mm_load_sd(&input[(i + k) * input_size + (j + l)]);\n                    __m128d kernel_val = _mm_load_sd(&kernel[k * kernel_size + l]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m128d result = _mm_max_pd(sum, zero);\n            _mm_store_sd(&output[i * output_size + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    const __m128d zero = _mm_setzero_pd();\n    const size_t output_size = input_size - kernel_size + 1;\n    const size_t kernel_area = kernel_size * kernel_size;\n\n    for (size_t i = 0; i < output_size; ++i) {\n        for (size_t j = 0; j < output_size; ++j) {\n            __m128d sum = zero;\n            for (size_t m = 0; m < kernel_size; ++m) {\n                for (size_t n = 0; n < kernel_size; ++n) {\n                    __m128d input_val = _mm_load_sd(&input[(i + m) * input_size + (j + n)]);\n                    __m128d kernel_val = _mm_load_sd(&kernel[m * kernel_size + n]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m128d relu_mask = _mm_cmpgt_pd(sum, zero);\n            sum = _mm_and_pd(sum, relu_mask);\n            _mm_store_sd(&output[i * output_size + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_61_SSE", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    size_t i, j, k, l;\n    size_t output_size = input_size - kernel_size + 1;\n    __m128d zero = _mm_setzero_pd();\n    __m128d relu_mask = _mm_set1_pd(0.0);\n\n    for (i = 0; i < output_size; i++) {\n        for (j = 0; j < output_size; j++) {\n            __m128d sum = zero;\n            for (k = 0; k < kernel_size; k++) {\n                for (l = 0; l < kernel_size; l++) {\n                    __m128d input_val = _mm_load_sd(&input[(i + k) * input_size + (j + l)]);\n                    __m128d kernel_val = _mm_load_sd(&kernel[k * kernel_size + l]);\n                    sum = _mm_add_pd(sum, _mm_mul_pd(input_val, kernel_val));\n                }\n            }\n            __m128d result = _mm_max_pd(sum, zero);\n            _mm_store_sd(&output[i * output_size + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vec_size = 2; // SSE processes 4 floats or 2 doubles at a time\n    const int size = numbers.size();\n    \n    if(size < 2) return false;\n    \n    __m128d thresh_vec = _mm_set1_pd(threshold);\n    \n    for(int i=0; i <= size - vec_size; i += vec_size){\n        __m128d nums = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(nums, _mm_permute_pd(nums, 1));\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff);\n        \n        __m128d mask = _mm_cmpgt_pd(abs_diff, thresh_vec);\n        if(!_mm_movemask_pd(mask)){\n            return true;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vec_size = 2; // SSE can handle 2 double precision floats\n    if(numbers.size() < 2) return false;\n    \n    __m128d thresh_vec = _mm_set1_pd(threshold);\n    \n    for(size_t i = 0; i + vec_size <= numbers.size(); i += vec_size){\n        __m128d nums = _mm_loadu_pd(&numbers[i]);\n        __m128d diffs = _mm_sub_pd(nums, _mm_permute_pd(nums, 1));\n        __m128d abs_diffs = _mm_andnot_pd(_mm_set1_pd(-0.0), diffs);\n        \n        __m128d mask = _mm_cmpgt_pd(abs_diffs, thresh_vec);\n        if(!_mm_movemask_pd(mask)){\n            return true;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int n = numbers.size();\n    if(n < 2) return false;\n\n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    bool has_close = false;\n\n    for(int i = 0; i + 1 < n; i += 2){\n        __m128d v1 = _mm_loadu_pd(&numbers[i]);\n        if(i + 1 >= n) break;\n        __m128d v2 = _mm_loadu_pd(&numbers[i + 1]);\n\n        __m128d diff = _mm_sub_pd(v1, v2);\n        diff = _mm_abs_pd(diff);\n\n        __m128d mask = _mm_cmpgt_pd(threshold_vec, diff);\n        has_close |= _mm_movemask_pd(mask) != 0;\n        if(has_close) return true;\n    }\n\n    return has_close;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vec_size = 2; // SSE can handle 4 floats or 2 doubles\n    const int size = numbers.size();\n    \n    if(size < 2)\n        return false;\n    \n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    \n    for(int i = 0; i <= size - vec_size; i += vec_size){\n        __m128d nums = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(nums, _mm_permute_pd(nums, 1));\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff);\n        \n        __m128d mask = _mm_cmpgt_pd(abs_diff, threshold_vec);\n        if(!_mm_movemask_pd(mask))\n            return true;\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_62_SSE", "completion": "```cpp\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int vec_size = 2; // SSE can handle 4 floats or 2 doubles\n    const int size = numbers.size();\n    \n    if(size < 2)\n        return false;\n    \n    __m128d threshold_vec = _mm_set1_pd(threshold);\n    \n    for(int i = 0; i <= size - vec_size; i += vec_size){\n        __m128d nums = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(nums, _mm_permute_pd(nums, 1));\n        __m128d abs_diff = _mm_andnot_pd(_mm_set1_pd(-0.0), diff);\n        \n        __m128d mask = _mm_cmpgt_pd(abs_diff, threshold_vec);\n        if(!_mm_movemask_pd(mask))\n            return true;\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    size_t len = operations.size();\n    __m128i threshold = _mm_setzero_si128();\n    \n    for (size_t i = 0; i < len; i += vec_size) {\n        __m128i ops = _mm_setzero_si128();\n        __m128i sum = _mm_setzero_si128();\n        \n        for (int j = 0; j < vec_size && (i + j) < len; ++j) {\n            ops = _mm_insert_epi32(ops, operations[i + j], j);\n            sum = _mm_add_epi32(sum, _mm_set1_epi32(operations[i + j]));\n        }\n        \n        __m128i below_zero = _mm_cmpgt_epi32(threshold, sum);\n        if (_mm_movemask_epi8(below_zero) != 0) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    int size = operations.size();\n    __m128i threshold = _mm_setzero_si128();\n    \n    for (int i = 0; i < size; i += vec_size) {\n        __m128i ops = _mm_setzero_si128();\n        bool any_negative = false;\n        \n        for (int j = 0; j < vec_size && (i + j) < size; ++j) {\n            ops = _mm_insert_epi32(ops, operations[i + j], j);\n        }\n        \n        __m128i acc = _mm_add_epi32(threshold, ops);\n        __m128i negative_mask = _mm_cmplt_epi32(acc, _mm_setzero_si128());\n        any_negative |= _mm_movemask_epi8(negative_mask) != 0;\n        \n        if (any_negative) {\n            return true;\n        }\n        \n        threshold = acc;\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int vec_size = 4; // SSE operates on 4 integers at once\n    int n = operations.size();\n    \n    if (n == 0) return false;\n\n    __m128i threshold = _mm_setzero_si128();\n    __m128i current_balance = _mm_setzero_si128();\n\n    for (size_t i = 0; i + vec_size <= n; i += vec_size) {\n        __m128i ops = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[i]));\n        current_balance = _mm_add_epi32(current_balance, ops);\n\n        __m128i below_zero_mask = _mm_cmpgt_epi32(threshold, current_balance);\n        if (_mm_movemask_epi8(below_zero_mask) != 0) {\n            return true;\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = vec_size * (n / vec_size); i < n; ++i) {\n        current_balance.m128i_i32[0] += operations[i];\n        if (current_balance.m128i_i32[0] < 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int vec_size = 4; // SSE operates on 4 integers at once\n    int n = operations.size();\n    __m128i current_balance = _mm_setzero_si128();\n\n    for (size_t i = 0; i < n; i += vec_size) {\n        __m128i ops = _mm_setzero_si128();\n        if (i + 0 < n) ops = _mm_insert_epi32(ops, operations[i + 0], 0);\n        if (i + 1 < n) ops = _mm_insert_epi32(ops, operations[i + 1], 1);\n        if (i + 2 < n) ops = _mm_insert_epi32(ops, operations[i + 2], 2);\n        if (i + 3 < n) ops = _mm_insert_epi32(ops, operations[i + 3], 3);\n\n        current_balance = _mm_add_epi32(current_balance, ops);\n\n        __m128i mask = _mm_cmpgt_epi32(_mm_setzero_si128(), current_balance);\n        if (_mm_movemask_epi8(mask) != 0) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(const std::vector<int>& operations) {\n    const int vec_size = 4; // SSE operates on 4 integers at once\n    int n = operations.size();\n    __m128i current_balance = _mm_setzero_si128();\n\n    for (int i = 0; i < n; i += vec_size) {\n        __m128i ops = _mm_setzero_si128();\n        if (i + 0 < n) ops = _mm_insert_epi32(ops, operations[i + 0], 0);\n        if (i + 1 < n) ops = _mm_insert_epi32(ops, operations[i + 1], 1);\n        if (i + 2 < n) ops = _mm_insert_epi32(ops, operations[i + 2], 2);\n        if (i + 3 < n) ops = _mm_insert_epi32(ops, operations[i + 3], 3);\n\n        current_balance = _mm_add_epi32(current_balance, ops);\n\n        __m128i negative_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), current_balance);\n        if (_mm_movemask_epi8(negative_mask) != 0) {\n            return true;\n        }\n    }\n\n    int final_balance[4];\n    _mm_storeu_si128((__m128i*)final_balance, current_balance);\n    for (int j = 0; j < vec_size; ++j) {\n        if (final_balance[j] < 0) return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    __m128d sum = _mm_setzero_pd();\n    __m128d count = _mm_setzero_pd();\n    __m128d mean = _mm_setzero_pd();\n\n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, data);\n        count = _mm_add_pd(count, _mm_set1_pd(1.0));\n    }\n\n    mean = _mm_div_pd(sum, count);\n\n    __m128d mad_sum = _mm_setzero_pd();\n\n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean);\n        __m128d abs_diff = _mm_abs_pd(diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double mad_result[2];\n    _mm_storeu_pd(mad_result, mad_sum);\n\n    double mad = (mad_result[0] + mad_result[1]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    __m128d sum = _mm_setzero_pd();\n    __m128d count = _mm_setzero_pd();\n    __m128d mean = _mm_setzero_pd();\n\n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, data);\n        count = _mm_add_pd(count, _mm_set1_pd(1.0));\n    }\n\n    mean = _mm_div_pd(sum, count);\n\n    __m128d mad_sum = _mm_setzero_pd();\n    \n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean);\n        __m128d abs_diff = _mm_abs_pd(diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double result[2];\n    _mm_storeu_pd(result, mad_sum);\n    double total_mad = result[0] + result[1];\n\n    return total_mad / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    __m128d sum = _mm_setzero_pd();\n    __m128d count = _mm_setzero_pd();\n    __m128d mean = _mm_setzero_pd();\n\n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, data);\n        count = _mm_add_pd(count, _mm_set1_pd(1.0));\n    }\n\n    mean = _mm_div_pd(sum, count);\n\n    __m128d mad_sum = _mm_setzero_pd();\n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean);\n        __m128d abs_diff = _mm_abs_pd(diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double result[2];\n    _mm_storeu_pd(result, mad_sum);\n    double total_mad = result[0] + result[1];\n\n    return total_mad / n;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    __m128d sum = _mm_setzero_pd();\n    __m128d count = _mm_setzero_pd();\n    __m128d mean = _mm_setzero_pd();\n\n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        sum = _mm_add_pd(sum, data);\n        count = _mm_add_pd(count, _mm_set1_pd(1.0));\n    }\n\n    mean = _mm_div_pd(sum, count);\n\n    __m128d mad_sum = _mm_setzero_pd();\n    for(int i=0; i<n; i+=2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d diff = _mm_sub_pd(data, mean);\n        __m128d abs_diff = _mm_abs_pd(diff);\n        mad_sum = _mm_add_pd(mad_sum, abs_diff);\n    }\n\n    double mad_result[2];\n    _mm_storeu_pd(mad_result, mad_sum);\n    double mad = (mad_result[0] + mad_result[1]) / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_SSE", "completion": "```cpp\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    int n = numbers.size();\n    if(n == 0) return 0.0;\n\n    // Load data into SIMD registers\n    __m128d sum = _mm_setzero_pd();\n    __m128d sum_abs_dev = _mm_setzero_pd();\n\n    for(int i = 0; i < n; i += 2){\n        __m128d data = _mm_loadu_pd(&numbers[i]);\n        __m128d mean = _mm_set1_pd(sum / (i / 2));\n        __m128d abs_dev = _mm_abs_pd(_mm_sub_pd(data, mean));\n        sum = _mm_add_pd(sum, data);\n        sum_abs_dev = _mm_add_pd(sum_abs_dev, abs_dev);\n    }\n\n    // Horizontal sum for total sum and total abs dev\n    __m128d shuffle = _mm_movehdup_pd(sum);\n    double total_sum = _mm_cvtsd_f64(shuffle);\n\n    shuffle = _mm_movehdup_pd(sum_abs_dev);\n    double total_abs_dev = _mm_cvtsd_f64(shuffle);\n\n    double mean_val = total_sum / n;\n    double mad = total_abs_dev / n;\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    const int n = numbers.size();\n    std::vector<int> result(n * 2 - 1);\n    __m128i delim_vec = _mm_set1_epi32(delimeter);\n    \n    if(n == 0){\n        return {};\n    }\n    \n    int i = 0;\n    for(; i <= n - 4; i += 4){\n        __m128i nums = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i nums_with_delim = _mm_unpacklo_epi32(nums, delim_vec);\n        _mm_storeu_si128((__m128i*)&result[i*2], nums_with_delim);\n    }\n    \n    for(int j = i; j < n; ++j){\n        result[j*2] = numbers[j];\n        if(j != n-1){\n            result[j*2 + 1] = delimeter;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    const int n = numbers.size();\n    std::vector<int> result(n * 2 - 1);\n    __m128i delim_vec = _mm_set1_epi32(delimeter);\n    \n    if(n == 0){\n        return {};\n    }\n    \n    int i = 0;\n    for(; i <= n - 4; i += 4){\n        __m128i nums = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i nums_with_delim = _mm_unpacklo_epi32(nums, delim_vec);\n        _mm_storeu_si128((__m128i*)&result[i*2], nums_with_delim);\n    }\n    \n    for(int j = i; j < n; ++j){\n        result[j*2] = numbers[j];\n        if(j != n-1){\n            result[j*2 + 1] = delimeter;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    int n = numbers.size();\n    std::vector<int> result(n * 2 - 1);\n    __m128i delim_vec = _mm_set1_epi32(delimeter);\n    \n    if(n == 0){\n        return {};\n    }\n    \n    int i = 0;\n    for(; i <= n - 4; i += 4){\n        __m128i nums = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i nums_with_delim = _mm_insert_epi32(nums, delimeter, 1);\n        __m128i nums_with_delim2 = _mm_insert_epi32(nums_with_delim, delimeter, 3);\n        __m128i nums_with_delim3 = _mm_insert_epi32(nums_with_delim2, delimeter, 5);\n        __m128i nums_with_delim4 = _mm_insert_epi32(nums_with_delim3, delimeter, 7);\n        \n        _mm_storeu_si128((__m128i*)&result[i*2], nums_with_delim4);\n    }\n    \n    for(; i < n; ++i){\n        result[i*2] = numbers[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    int n = numbers.size();\n    std::vector<int> result(n * 2 - 1);\n    __m128i delim_vec = _mm_set1_epi32(delimeter);\n    \n    if(n == 0){\n        return {};\n    }\n    \n    int i = 0;\n    for(; i <= n - 4; i += 4){\n        __m128i nums = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i nums_with_delim = _mm_insert_epi32(nums, delimeter, 1);\n        __m128i nums_with_delim2 = _mm_insert_epi32(nums_with_delim, delimeter, 3);\n        __m128i nums_with_delim3 = _mm_insert_epi32(nums_with_delim2, delimeter, 5);\n        __m128i nums_with_delim4 = _mm_insert_epi32(nums_with_delim3, delimeter, 7);\n        \n        _mm_storeu_si128((__m128i*)&result[i*2], nums_with_delim4);\n    }\n    \n    for(; i < n; ++i){\n        result[i*2] = numbers[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_65_SSE", "completion": "```cpp\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    int n = numbers.size();\n    std::vector<int> result(n * 2 - 1);\n    __m128i delim_vec = _mm_set1_epi32(delimeter);\n    \n    if(n == 0){\n        return {};\n    }\n    \n    int i = 0;\n    for(; i <= n - 4; i += 4){\n        __m128i nums = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i nums_with_delim = _mm_unpacklo_epi32(nums, delim_vec);\n        _mm_storeu_si128((__m128i*)&result[i*2], nums_with_delim);\n    }\n    \n    for(; i < n; ++i){\n        result[i*2] = numbers[i];\n        if(i != n-1){\n            result[i*2 + 1] = delimeter;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // SSE can handle 4 int16_t elements at once\n    int size = numbers.size();\n    \n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n    __m128i prod_vec = _mm_set_epi32(1, 1, 1, 1); // Initialize product vector to ones\n    \n    for(int i = 0; i < size; i += vec_size){\n        __m128i current = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 4 elements\n        \n        sum_vec = _mm_add_epi16(sum_vec, current); // Parallel addition\n        prod_vec = _mm_mullo_epi16(prod_vec, current); // Parallel multiplication\n    }\n    \n    // Horizontal sum and product reduction\n    __m128i temp_sum = _mm_add_epi16(_mm_unpacklo_epi16(sum_vec, _mm_setzero_si128()),\n                                     _mm_unpackhi_epi16(sum_vec, _mm_setzero_si128()));\n    __m128i temp_prod = _mm_add_epi16(_mm_unpacklo_epi16(prod_vec, _mm_setzero_si128()),\n                                      _mm_unpackhi_epi16(prod_vec, _mm_setzero_si128()));\n    \n    int64_t total_sum = _mm_cvtsi128_si64(temp_sum);\n    int64_t total_prod = _mm_cvtsi128_si64(temp_prod);\n    \n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // SSE processes 4 integers at a time\n    int size = numbers.size();\n    \n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n    __m128i prod_vec = _mm_set_epi32(1, 1, 1, 1); // Initialize product vector to ones\n    \n    for(int i = 0; i < size; i += vec_size){\n        __m128i nums;\n        if(i + vec_size <= size){\n            nums = _mm_loadu_si128((__m128i*)&numbers[i]);\n        }\n        else{\n            nums = _mm_set_epi32(0, 0, 0, 0);\n            int count = size - i;\n            for(int j = 0; j < count; ++j){\n                _mm_storel_epi64((__m128i*)&nums, _mm_insert_epi32(nums, numbers[i + j], j));\n            }\n        }\n        \n        __m128i new_sum = _mm_add_epi32(sum_vec, nums);\n        __m128i new_prod = _mm_mullo_epi32(prod_vec, nums);\n        \n        sum_vec = new_sum;\n        prod_vec = new_prod;\n    }\n    \n    int64_t sum = _mm_cvtsi128_si64(sum_vec) + _mm_cvtsi128_si64(_mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(3,2,1,0)));\n    int64_t prod = _mm_cvtsi128_si64(prod_vec) * _mm_cvtsi128_si64(_mm_shuffle_epi32(prod_vec, _MM_SHUFFLE(3,2,1,0)));\n    \n    return {sum, prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // SSE can handle 4 int16_t values at once\n    int size = numbers.size();\n    \n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to 0\n    __m128i prod_vec = _mm_set_epi32(1, 1, 1, 1); // Initialize product vector to 1\n    \n    for(int i = 0; i < size; i += vec_size){\n        __m128i current = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 4 elements\n        \n        sum_vec = _mm_add_epi16(sum_vec, current); // Sum reduction\n        prod_vec = _mm_mullo_epi16(prod_vec, current); // Product reduction\n    }\n    \n    // Horizontal sum and product extraction\n    __m128i temp_sum = _mm_hadd_epi16(sum_vec, sum_vec);\n    temp_sum = _mm_hadd_epi16(temp_sum, temp_sum);\n    int64_t total_sum = _mm_cvtsi128_si64(temp_sum);\n    \n    __m128i temp_prod = _mm_hadd_epi16(prod_vec, prod_vec);\n    temp_prod = _mm_hadd_epi16(temp_prod, temp_prod);\n    int64_t total_prod = _mm_cvtsi128_si64(temp_prod);\n    \n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // SSE can handle 4 int16_t values at once\n    int size = numbers.size();\n    \n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to 0\n    __m128i prod_vec = _mm_set_epi32(1, 1, 1, 1); // Initialize product vector to 1\n    \n    for(int i = 0; i < size; i += vec_size){\n        __m128i current = _mm_loadu_si128((__m128i*)&numbers[i]); // Load 4 elements\n        \n        sum_vec = _mm_add_epi16(sum_vec, current); // Sum reduction\n        prod_vec = _mm_mullo_epi16(prod_vec, current); // Product reduction\n    }\n    \n    // Horizontal sum of sum_vec\n    __m128i temp_sum = _mm_add_epi16(_mm_unpacklo_epi16(sum_vec, _mm_setzero_si128()),\n                                     _mm_unpackhi_epi16(sum_vec, _mm_setzero_si128()));\n    int64_t total_sum = _mm_cvtsi128_si64(temp_sum);\n    \n    // Horizontal sum of prod_vec\n    __m128i temp_prod = _mm_add_epi16(_mm_unpacklo_epi16(prod_vec, _mm_setzero_si128()),\n                                      _mm_unpackhi_epi16(prod_vec, _mm_setzero_si128()));\n    int64_t total_prod = _mm_cvtsi128_si64(temp_prod);\n    \n    return {total_sum, total_prod};\n}\n```"}
{"task_id": "SimdBench_66_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    const int vec_size = 4; // SSE can process 4 16-bit integers at once\n    int size = numbers.size();\n    \n    __m128i sum_vec = _mm_setzero_si128(); // Initialize sum vector to zero\n    __m128i prod_vec = _mm_set_epi32(1, 1, 1, 1); // Initialize product vector to ones\n    \n    for(int i = 0; i < size; i += vec_size){\n        __m128i nums;\n        if(i + vec_size <= size){\n            nums = _mm_loadu_si128((__m128i*)&numbers[i]);\n        }\n        else{\n            nums = _mm_setzero_si128();\n            int count = size - i;\n            for(int j = 0; j < count; ++j){\n                _mm_storel_epi64((__m128i*)&nums, _mm_insert_epi16(nums, numbers[i + j], j));\n            }\n        }\n        \n        __m128i new_sum = _mm_add_epi16(sum_vec, nums);\n        __m128i new_prod = _mm_mullo_epi16(prod_vec, nums);\n        \n        sum_vec = new_sum;\n        prod_vec = new_prod;\n    }\n    \n    __m128i sum_result = _mm_hadd_epi16(sum_vec, sum_vec);\n    sum_result = _mm_hadd_epi16(sum_result, sum_result);\n    \n    __m128i prod_result = _mm_hadd_epi16(prod_vec, prod_vec);\n    prod_result = _mm_hadd_epi16(prod_result, prod_result);\n    \n    int64_t sum_final = _mm_cvtsi128_si64(sum_result);\n    int64_t prod_final = _mm_cvtsi128_si64(prod_result);\n    \n    return {sum_final, prod_final};\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    if(n == 0) return result;\n    \n    int i = 0;\n    __m128i max_val = _mm_set1_epi32(numbers[0]);\n    \n    // Process in chunks of 4 elements\n    for(; i <= n - 4; i += 4){\n        __m128i current_vals = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i new_max = _mm_max_epi32(max_val, current_vals);\n        _mm_storeu_si128((__m128i*)&result[i], new_max);\n        max_val = new_max;\n    }\n    \n    // Handle remaining elements\n    for(; i < n; ++i){\n        if(numbers[i] > result[i-1]){\n            result[i] = numbers[i];\n            max_val = _mm_set1_epi32(result[i]);\n        }\n        else{\n            result[i] = result[i-1];\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    if(n == 0) return result;\n    \n    int i = 0;\n    __m128i prev_max = _mm_set1_epi32(numbers[0]);\n    __m128i curr_val, max_val;\n    \n    // Process in chunks of 4\n    for (; i <= n - 4; i += 4) {\n        curr_val = _mm_loadu_si128((__m128i*)&numbers[i]);\n        max_val = _mm_max_epi32(prev_max, curr_val);\n        _mm_storeu_si128((__m128i*)&result[i], max_val);\n        prev_max = max_val;\n    }\n    \n    // Handle remaining elements\n    for (; i < n; ++i) {\n        result[i] = std::max(result[i-1], numbers[i]);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    __m128i max_val = _mm_setzero_si128();\n    int i = 0;\n    \n    for (; i <= n - 4; i += 4) {\n        __m128i current = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i compare = _mm_cmpgt_epi32(current, max_val);\n        max_val = _mm_or_si128(_mm_and_si128(compare, current), _mm_andnot_si128(compare, max_val));\n        _mm_storeu_si128((__m128i*)&result[i], max_val);\n    }\n    \n    for (; i < n; ++i) {\n        if (numbers[i] > max_val.m128i_i32[0]) {\n            max_val = _mm_set1_epi32(numbers[i]);\n        }\n        result[i] = max_val.m128i_i32[0];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    if(n == 0) return result;\n    \n    // Ensure we process in chunks of 4 due to SSE2 register size\n    int i = 0;\n    for(; i <= n - 4; i += 4){\n        __m128i current = _mm_set_epi32(numbers[i+3], numbers[i+2], numbers[i+1], numbers[i]);\n        __m128i max_val = current;\n        \n        // Compare with previous max and update\n        for(int j = 0; j < i; j += 4){\n            __m128i prev = _mm_loadu_si128((__m128i*)&result[j]);\n            max_val = _mm_max_epi32(max_val, prev);\n        }\n        \n        // Store the result back\n        _mm_storeu_si128((__m128i*)&result[i], max_val);\n    }\n    \n    // Handle remaining elements\n    for(; i < n; ++i){\n        result[i] = std::max(numbers[i], (i > 0) ? result[i-1] : numbers[i]);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    int n = numbers.size();\n    std::vector<int> result(n);\n    \n    __m128i max_val = _mm_setzero_si128();\n    int i = 0;\n    \n    for (; i <= n - 4; i += 4){\n        __m128i current = _mm_loadu_si128((__m128i*)&numbers[i]);\n        __m128i compare = _mm_cmpgt_epi32(current, max_val);\n        max_val = _mm_or_si128(_mm_and_si128(compare, current), _mm_andnot_si128(compare, max_val));\n        _mm_storeu_si128((__m128i*)&result[i], max_val);\n    }\n    \n    for (; i < n; ++i){\n        if(numbers[i] > max_val.m128i_i32[0]){\n            max_val = _mm_set1_epi32(numbers[i]);\n        }\n        result[i] = max_val.m128i_i32[0];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    int len = a.size();\n    if (len != b.size()) {\n        return \"\"; // Handle error: strings must be of equal length\n    }\n\n    int vec_len = len / 16 * 16; // Process in 128-bit chunks\n    __m128i zero = _mm_setzero_si128();\n    std::vector<char> result(vec_len + 16, 0);\n\n    for (int i = 0; i < vec_len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&a[i]));\n        __m128i b_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&b[i]));\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), xor_vec);\n    }\n\n    // Handle remaining characters if any\n    for (int i = vec_len; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return std::string(result.begin(), result.begin() + len);\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\nstd::string string_xor_simd(std::string a, std::string b){\n    size_t len = a.size();\n    __m128i result = _mm_setzero_si128();\n    size_t i = 0;\n\n    // Ensure length is multiple of 4 by processing remaining characters later\n    for (; i <= len - 4; i += 4){\n        __m128i vec_a = _mm_loadu_si128((__m128i*)&a[i]);\n        __m128i vec_b = _mm_loadu_si128((__m128i*)&b[i]);\n        __m128i xor_vec = _mm_xor_si128(vec_a, vec_b);\n        result = _mm_or_si128(result, xor_vec);\n    }\n\n    // Handle remaining characters\n    uint8_t remainder[16] = {0};\n    for (; i < len; ++i){\n        remainder[i % 4] ^= (a[i] != b[i]) ? 1 : 0;\n    }\n    __m128i rem_vec = _mm_loadu_si128((__m128i*)remainder);\n    result = _mm_or_si128(result, rem_vec);\n\n    // Convert result back to string\n    char result_str[17];\n    _mm_storeu_si128((__m128i*)result_str, result);\n    result_str[16] = '\\0';\n    return std::string(result_str);\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    int len = a.size();\n    if (len != b.size()) {\n        return \"\"; // Handle error: strings must be of equal length\n    }\n\n    int vec_len = len / 16 * 16; // Process in 128-bit chunks (16 characters)\n    __m128i zero = _mm_setzero_si128();\n    std::vector<char> result_vec(vec_len / 16 * 16, 0);\n    __m128i* result = reinterpret_cast<__m128i*>(result_vec.data());\n\n    for (int i = 0; i < vec_len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&a[i]));\n        __m128i b_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&b[i]));\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128(&result[i / 16], xor_vec);\n    }\n\n    // Handle remaining characters if any\n    for (int i = vec_len; i < len; ++i) {\n        result_vec[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return std::string(result_vec.begin(), result_vec.end());\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    int len = a.size();\n    if (len != b.size()) {\n        throw std::invalid_argument(\"Strings must be of equal length\");\n    }\n\n    int vec_len = len / 16 * 16; // Process in 128-bit chunks (16 characters)\n    __m128i zero = _mm_setzero_si128();\n    std::vector<char> result(len + 1, 0);\n\n    for (int i = 0; i < vec_len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&a[i]));\n        __m128i b_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&b[i]));\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), xor_vec);\n    }\n\n    // Handle remaining characters\n    for (int i = vec_len; i < len; ++i) {\n        result[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return std::string(result.begin(), result.end());\n}\n```"}
{"task_id": "SimdBench_68_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string string_xor_simd(std::string a, std::string b) {\n    int len = a.size();\n    if (len != b.size()) {\n        return \"\"; // Invalid input\n    }\n\n    int vec_len = len / 16 * 16; // Process in 128-bit chunks\n    __m128i zero = _mm_setzero_si128();\n    std::vector<char> result_vec(vec_len / 16 * 16, 0);\n    __m128i* result = reinterpret_cast<__m128i*>(result_vec.data());\n\n    for (int i = 0; i < vec_len; i += 16) {\n        __m128i a_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&a[i]));\n        __m128i b_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&b[i]));\n        __m128i xor_vec = _mm_xor_si128(a_vec, b_vec);\n        _mm_storeu_si128(&result[i / 16], xor_vec);\n    }\n\n    // Handle remaining characters if any\n    for (int i = vec_len; i < len; ++i) {\n        result_vec[i] = (a[i] == b[i]) ? '0' : '1';\n    }\n\n    return std::string(result_vec.begin(), result_vec.end());\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n\n    __m128i max_len = _mm_setzero_si128();\n    __m128i max_idx = _mm_setzero_si128();\n    int max_len_scalar = 0;\n    size_t max_idx_scalar = 0;\n\n    for(size_t i = 0; i < strings.size(); i += 4){\n        __m128i len_vec = _mm_setzero_si128();\n        for(int j = 0; j < 4 && (i + j) < strings.size(); ++j){\n            const std::string& s = strings[i + j];\n            len_vec = _mm_insert_epi32(len_vec, static_cast<int>(s.length()), j);\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(len_vec, max_len);\n        max_len = _mm_max_epi32(max_len, len_vec);\n        max_idx = _mm_or_si128(_mm_and_si128(cmp, _mm_set_epi32(i+3, i+2, i+1, i)), \n                               _mm_andnot_si128(cmp, max_idx));\n    }\n\n    _mm_storeu_si128(&max_len_scalar, max_len);\n    _mm_storeu_si128(&max_idx_scalar, max_idx);\n\n    return strings[max_idx_scalar];\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n    \n    __m128i max_len_vec = _mm_setzero_si128();\n    __m128i max_idx_vec = _mm_setzero_si128();\n    int max_len = 0;\n    int max_idx = 0;\n\n    for(int i = 0; i < strings.size(); i += 4){\n        __m128i len_vec = _mm_setzero_si128();\n        __m128i idx_vec = _mm_set_epi32(i+3, i+2, i+1, i);\n\n        for(int j = 0; j < 4 && i+j < strings.size(); ++j){\n            const std::string &s = strings[i+j];\n            len_vec = _mm_insert_epi32(len_vec, static_cast<int>(s.length()), j);\n        }\n\n        __m128i gt_mask = _mm_cmpgt_epi32(len_vec, max_len_vec);\n        max_len_vec = _mm_max_epi32(max_len_vec, len_vec);\n        max_idx_vec = _mm_or_si128(\n            _mm_and_si128(gt_mask, idx_vec),\n            _mm_andnot_si128(gt_mask, max_idx_vec)\n        );\n    }\n\n    _mm_storeu_si128(&max_len_vec, max_len_vec);\n    _mm_storeu_si128(&max_idx_vec, max_idx_vec);\n\n    max_len = _mm_extract_epi32(max_len_vec, 0);\n    max_idx = _mm_extract_epi32(max_idx_vec, 0);\n\n    for(int i = 0; i < strings.size(); ++i){\n        if(strings[i].length() == max_len){\n            return strings[i];\n        }\n    }\n\n    return \"\";\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n\n    __m128i max_len = _mm_setzero_si128();\n    __m128i max_idx = _mm_setzero_si128();\n    int max_len_scalar = 0;\n    size_t max_idx_scalar = 0;\n\n    for(size_t i = 0; i < strings.size(); i += 4){\n        __m128i lengths = _mm_setzero_si128();\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n\n        for(int j = 0; j < 4 && (i+j) < strings.size(); ++j){\n            const std::string &s = strings[i+j];\n            int len = s.length();\n            lengths = _mm_insert_epi32(lengths, len, j);\n        }\n\n        __m128i gt_mask = _mm_cmpgt_epi32(lengths, max_len);\n        max_len = _mm_max_epi32(max_len, lengths);\n        max_idx = _mm_or_si128(_mm_and_si128(gt_mask, indices), _mm_andnot_si128(gt_mask, max_idx));\n    }\n\n    _mm_storeu_si128(&max_len_scalar, max_len);\n    _mm_storeu_si128(&max_idx_scalar, max_idx);\n\n    for(int i = 0; i < 4; ++i){\n        if(max_len_scalar == strings[max_idx_scalar].length()){\n            return strings[max_idx_scalar];\n        }\n        max_idx_scalar++;\n    }\n\n    return \"\";\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n    \n    __m128i max_len_vec = _mm_setzero_si128();\n    __m128i max_idx_vec = _mm_setzero_si128();\n    int max_len = 0;\n    int max_idx = 0;\n\n    for(int i = 0; i < strings.size(); i += 4){\n        __m128i len_vec = _mm_setzero_si128();\n        for(int j = 0; j < 4 && (i + j) < strings.size(); ++j){\n            const std::string &s = strings[i + j];\n            len_vec = _mm_insert_epi32(len_vec, static_cast<int>(s.length()), j);\n        }\n\n        __m128i cmp_gt = _mm_cmpgt_epi32(len_vec, max_len_vec);\n        max_len_vec = _mm_max_epi32(max_len_vec, len_vec);\n        max_idx_vec = _mm_or_si128(\n            _mm_and_si128(cmp_gt, _mm_set_epi32(i+3, i+2, i+1, i)),\n            _mm_andnot_si128(cmp_gt, max_idx_vec)\n        );\n    }\n\n    _mm_storeu_si128(&max_idx, max_idx_vec);\n    max_len = _mm_extract_epi32(max_len_vec, 0);\n    max_idx = _mm_extract_epi32(max_idx_vec, 0);\n\n    for(int i = 0; i < strings.size(); ++i){\n        if(strings[i].length() == max_len){\n            return strings[i];\n        }\n    }\n\n    return \"\";\n}\n```"}
{"task_id": "SimdBench_69_SSE", "completion": "```cpp\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n\n    __m128i max_len = _mm_setzero_si128();\n    __m128i max_idx = _mm_setzero_si128();\n    int max_len_scalar = 0;\n    size_t idx_scalar = 0;\n\n    for(size_t i = 0; i < strings.size(); i += 4){\n        __m128i len_vec = _mm_setzero_si128();\n        for(int j = 0; j < 4 && (i + j) < strings.size(); ++j){\n            const std::string& s = strings[i + j];\n            len_vec = _mm_insert_epi32(len_vec, static_cast<int>(s.length()), j);\n        }\n\n        __m128i cmp = _mm_cmpgt_epi32(len_vec, max_len);\n        max_len = _mm_max_epi32(max_len, len_vec);\n        max_idx = _mm_or_si128(_mm_and_si128(cmp, _mm_set_epi32(i+3, i+2, i+1, i)), \n                               _mm_andnot_si128(cmp, max_idx));\n    }\n\n    _mm_storeu_si128(&max_len_scalar, max_len);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&idx_scalar), max_idx);\n\n    for(int i = 0; i < 4; ++i){\n        if(idx_scalar == i){\n            return strings[max_len_scalar];\n        }\n    }\n\n    return \"\";\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    // SSE/SSE2 does not directly support GCD operations on integers.\n    // This function cannot be implemented using SSE/SSE2 intrinsics for parallelism.\n    return __gcd(a, b);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m128i v_a = _mm_set_epi64x(a, 0);\n    __m128i v_b = _mm_set_epi64x(b, 0);\n\n    while (!_mm_movemask_epi8(_mm_cmpeq_epi64(v_a, v_b))) {\n        __m128i v_r = _mm_sub_epi64(v_a, v_b);\n        v_r = _mm_and_si128(v_r, _mm_cmpgt_epi64(v_r, v_b));\n        v_b = _mm_sub_epi64(v_b, v_r);\n        v_a = _mm_min_epi64(v_a, v_b);\n    }\n\n    return _mm_cvtsi128_si64(v_a);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m128i a_vec = _mm_set_epi64x(a, a);\n    __m128i b_vec = _mm_set_epi64x(b, b);\n    \n    while (!_mm_movemask_epi8(_mm_cmpeq_epi64(a_vec, b_vec))) {\n        __m128i greater = _mm_cmpgt_epi64(a_vec, b_vec);\n        __m128i less = _mm_andnot_si128(greater, _mm_set1_epi64x(1));\n        __m128i diff = _mm_sub_epi64(a_vec, b_vec);\n        \n        a_vec = _mm_or_si128(diff, less);\n        b_vec = _mm_and_si128(b_vec, greater);\n    }\n    \n    return _mm_cvtsi128_si64(a_vec);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m128i a_vec = _mm_set_epi64x(b, a);\n    __m128i b_vec = _mm_set_epi64x(a, b);\n\n    while (!_mm_movemask_epi8(_mm_cmpeq_epi64(a_vec, b_vec))) {\n        __m128i gt_mask = _mm_cmpgt_epi64(a_vec, b_vec);\n        __m128i diff_vec = _mm_sub_epi64(a_vec, b_vec);\n        a_vec = _mm_blendv_epi8(a_vec, diff_vec, gt_mask);\n        b_vec = _mm_blendv_epi8(b_vec, diff_vec, _mm_xor_si128(gt_mask, _mm_set1_epi64x(-1)));\n    }\n\n    return _mm_cvtsi128_si64(a_vec);\n}\n```"}
{"task_id": "SimdBench_70_SSE", "completion": "```c\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m128i a_vec = _mm_set_epi64x(b, a);\n    __m128i b_vec = _mm_set_epi64x(a, b);\n\n    while (!_mm_movemask_epi8(_mm_cmpeq_epi64(a_vec, b_vec))) {\n        __m128i gt_mask = _mm_cmpgt_epi64(a_vec, b_vec);\n        __m128i diff_vec = _mm_sub_epi64(a_vec, b_vec);\n\n        a_vec = _mm_blendv_epi8(a_vec, diff_vec, gt_mask);\n        b_vec = _mm_blendv_epi8(b_vec, diff_vec, _mm_xor_si128(gt_mask, _mm_set1_epi64x(-1)));\n    }\n\n    return _mm_cvtsi128_si64(a_vec);\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    __m128 min_diff = _mm_set1_ps(__FLT_MAX__);\n    __m128 smallest_pair = _mm_setzero_ps();\n    \n    for(size_t i = 0; i + 3 < numbers.size(); i += 4){\n        __m128 v0 = _mm_loadu_ps(&numbers[i]);\n        __m128 v1 = _mm_loadu_ps(&numbers[i+1]);\n        __m128 v2 = _mm_loadu_ps(&numbers[i+2]);\n        __m128 v3 = _mm_loadu_ps(&numbers[i+3]);\n        \n        __m128 diff0 = _mm_sub_ps(v0, v1);\n        __m128 diff1 = _mm_sub_ps(v0, v2);\n        __m128 diff2 = _mm_sub_ps(v0, v3);\n        __m128 diff3 = _mm_sub_ps(v1, v2);\n        __m128 diff4 = _mm_sub_ps(v1, v3);\n        __m128 diff5 = _mm_sub_ps(v2, v3);\n        \n        __m128 abs_diff0 = _mm_abs_ps(diff0);\n        __m128 abs_diff1 = _mm_abs_ps(diff1);\n        __m128 abs_diff2 = _mm_abs_ps(diff2);\n        __m128 abs_diff3 = _mm_abs_ps(diff3);\n        __m128 abs_diff4 = _mm_abs_ps(diff4);\n        __m128 abs_diff5 = _mm_abs_ps(diff5);\n        \n        __m128 min_diff_temp = _mm_min_ps(abs_diff0, _mm_min_ps(abs_diff1, _mm_min_ps(abs_diff2, _mm_min_ps(abs_diff3, _mm_min_ps(abs_diff4, abs_diff5)))));\n        \n        __m128 mask0 = _mm_cmpeq_ps(min_diff_temp, abs_diff0);\n        __m128 mask1 = _mm_cmpeq_ps(min_diff_temp, abs_diff1);\n        __m128 mask2 = _mm_cmpeq_ps(min_diff_temp, abs_diff2);\n        __m128 mask3 = _mm_cmpeq_ps(min_diff_temp, abs_diff3);\n        __m128 mask4 = _mm_cmpeq_ps(min_diff_temp, abs_diff4);\n        __m128 mask5 = _mm_cmpeq_ps(min_diff_temp, abs_diff5);\n        \n        __m128 pair0 = _mm_blendv_ps(_mm_setzero_ps(), _mm_set_ps(0, 0, 0, 0), mask0);\n        __m128 pair1 = _mm_blendv_ps(_mm_setzero_ps(), _mm_set_ps(0, 0, 0, 0), mask1);\n        __m128 pair2 = _mm_blendv_ps(_mm_setzero_ps(), _mm_set_ps(0, 0, 0, 0), mask2);\n        __m128 pair3 = _mm_blendv_ps(_mm_setzero_ps(), _mm_set_ps(0, 0, 0, 0), mask3);\n        __m128 pair4 = _mm_blendv_ps(_mm_setzero_ps(), _mm_set_ps(0, 0, 0, 0), mask4);\n        __m128 pair5 = _mm_blendv_ps(_mm_setzero_ps(), _mm_set_ps(0, 0, 0, 0), mask5);\n        \n        __m128 selected_pair = _mm_max_ps(pair0, _mm_max_ps(pair1, _mm_max_ps(pair2, _mm_max_ps(pair3, _mm_max_ps(pair4, pair5)))));\n        \n        __m128 min_diff_mask = _mm_cmpgt_ps(min_diff, min_diff_temp);\n        min_diff = _mm_blendv_ps(min_diff, min_diff_temp, min_diff_mask);\n        smallest_pair = _mm_blendv_ps(smallest_pair, selected_pair, min_diff_mask);\n    }\n    \n    float result[4];\n    _mm_storeu_ps(result, smallest_pair);\n    \n    std::vector<float> closest_elements;\n    closest_elements.push_back(result[0]);\n    closest_elements.push_back(result[1]);\n    \n    return closest_elements;\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    __m128 min_diff = _mm_set1_ps(__FLT_MAX__);\n    __m128 result_pair = _mm_setzero_ps();\n    \n    for(size_t i = 0; i + 4 <= numbers.size(); i += 4){\n        __m128 vec_a = _mm_loadu_ps(&numbers[i]);\n        __m128 vec_b = _mm_loadu_ps(&numbers[i + 4]);\n        \n        __m128 diff = _mm_sub_ps(vec_a, vec_b);\n        diff = _mm_abs_ps(diff);\n        \n        __m128 mask = _mm_cmpgt_ps(diff, min_diff);\n        __m128 mask_inv = _mm_xor_ps(mask, _mm_set1_ps(-0.0f));\n        \n        min_diff = _mm_andnot_ps(mask_inv, diff);\n        result_pair = _mm_or_ps(_mm_and_ps(mask, result_pair), _mm_andnot_ps(mask, vec_a));\n    }\n    \n    float min_diff_arr[4];\n    _mm_storeu_ps(min_diff_arr, min_diff);\n    \n    std::vector<float> result;\n    for(int j = 0; j < 4; ++j){\n        if(min_diff_arr[j] == min_diff_arr[0]){\n            result.push_back(result_pair.m128_f32[j]);\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    __m128 min_diff = _mm_set1_ps(__FLT_MAX__);\n    __m128 smallest_pair = _mm_setzero_ps();\n    \n    for(size_t i = 0; i + 3 < numbers.size(); i += 4){\n        __m128 v0 = _mm_loadu_ps(&numbers[i]);\n        __m128 v1 = _mm_loadu_ps(&numbers[i+1]);\n        __m128 v2 = _mm_loadu_ps(&numbers[i+2]);\n        __m128 v3 = _mm_loadu_ps(&numbers[i+3]);\n        \n        __m128 diff0 = _mm_sub_ps(v0, v1);\n        __m128 diff1 = _mm_sub_ps(v0, v2);\n        __m128 diff2 = _mm_sub_ps(v0, v3);\n        __m128 diff3 = _mm_sub_ps(v1, v2);\n        __m128 diff4 = _mm_sub_ps(v1, v3);\n        __m128 diff5 = _mm_sub_ps(v2, v3);\n        \n        __m128 abs_diff0 = _mm_abs_ps(diff0);\n        __m128 abs_diff1 = _mm_abs_ps(diff1);\n        __m128 abs_diff2 = _mm_abs_ps(diff2);\n        __m128 abs_diff3 = _mm_abs_ps(diff3);\n        __m128 abs_diff4 = _mm_abs_ps(diff4);\n        __m128 abs_diff5 = _mm_abs_ps(diff5);\n        \n        __m128 min_diff_temp = _mm_min_ps(_mm_min_ps(abs_diff0, abs_diff1), _mm_min_ps(abs_diff2, abs_diff3));\n        __m128 final_min_diff = _mm_min_ps(min_diff_temp, _mm_min_ps(abs_diff4, abs_diff5));\n        \n        __m128 mask = _mm_cmplt_ps(final_min_diff, min_diff);\n        min_diff = _mm_or_ps(min_diff, _mm_andnot_ps(mask, final_min_diff));\n        \n        __m128 pair = _mm_shuffle_ps(v0, v1, _MM_SHUFFLE(0,1,0,1));\n        pair = _mm_blendv_ps(pair, _mm_shuffle_ps(v2, v3, _MM_SHUFFLE(0,1,0,1)), mask);\n        \n        smallest_pair = _mm_blendv_ps(smallest_pair, pair, mask);\n    }\n    \n    float result[4];\n    _mm_storeu_ps(result, smallest_pair);\n    return {result[0], result[1]};\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    __m128 min_diff = _mm_set1_ps(__FLT_MAX__);\n    __m128 smallest_pair = _mm_setzero_ps();\n    \n    for(size_t i = 0; i + 3 < numbers.size(); i += 4){\n        __m128 v0 = _mm_loadu_ps(&numbers[i]);\n        __m128 v1 = _mm_loadu_ps(&numbers[i+1]);\n        __m128 v2 = _mm_loadu_ps(&numbers[i+2]);\n        __m128 v3 = _mm_loadu_ps(&numbers[i+3]);\n\n        __m128 diff01 = _mm_sub_ps(v0, v1);\n        __m128 abs_diff01 = _mm_abs_ps(diff01);\n        \n        __m128 diff02 = _mm_sub_ps(v0, v2);\n        __m128 abs_diff02 = _mm_abs_ps(diff02);\n        \n        __m128 diff03 = _mm_sub_ps(v0, v3);\n        __m128 abs_diff03 = _mm_abs_ps(diff03);\n        \n        __m128 diff12 = _mm_sub_ps(v1, v2);\n        __m128 abs_diff12 = _mm_abs_ps(diff12);\n        \n        __m128 diff13 = _mm_sub_ps(v1, v3);\n        __m128 abs_diff13 = _mm_abs_ps(diff13);\n        \n        __m128 diff23 = _mm_sub_ps(v2, v3);\n        __m128 abs_diff23 = _mm_abs_ps(diff23);\n\n        __m128 min_diff_temp = _mm_min_ps(abs_diff01, abs_diff02);\n        min_diff_temp = _mm_min_ps(min_diff_temp, abs_diff03);\n        min_diff_temp = _mm_min_ps(min_diff_temp, abs_diff12);\n        min_diff_temp = _mm_min_ps(min_diff_temp, abs_diff13);\n        min_diff_temp = _mm_min_ps(min_diff_temp, abs_diff23);\n\n        __m128 mask = _mm_cmpeq_ps(min_diff_temp, abs_diff01);\n        __m128 pair01 = _mm_blendv_ps(_mm_setzero_ps(), _mm_add_ps(v0, v1), mask);\n        \n        mask = _mm_cmpeq_ps(min_diff_temp, abs_diff02);\n        __m128 pair02 = _mm_blendv_ps(_mm_setzero_ps(), _mm_add_ps(v0, v2), mask);\n        \n        mask = _mm_cmpeq_ps(min_diff_temp, abs_diff03);\n        __m128 pair03 = _mm_blendv_ps(_mm_setzero_ps(), _mm_add_ps(v0, v3), mask);\n        \n        mask = _mm_cmpeq_PS(min_diff_temp, abs_diff12);\n        __m128 pair12 = _mm_blendv_ps(_mm_setzero_ps(), _mm_add_ps(v1, v2), mask);\n        \n        mask = _mm_cmpeq_PS(min_diff_temp, abs_diff13);\n        __m128 pair13 = _mm_blendv_ps(_mm_setzero_ps(), _mm_add_ps(v1, v3), mask);\n        \n        mask = _mm_cmpeq_PS(min_diff_temp, abs_diff23);\n        __m128 pair23 = _mm_blendv_ps(_mm_setzero_ps(), _mm_add_ps(v2, v3), mask);\n\n        __m128 min_pairs = _mm_min_ps(pair01, pair02);\n        min_pairs = _mm_min_ps(min_pairs, pair03);\n        min_pairs = _mm_min_ps(min_pairs, pair12);\n        min_pairs = _mm_min_ps(min_pairs, pair13);\n        min_pairs = _mm_min_ps(min_pairs, pair23);\n\n        __m128 min_diff_mask = _mm_cmpeq_ps(min_diff_temp, min_diff);\n        __m128 new_min_diff = _mm_blendv_ps(min_diff, min_diff_temp, min_diff_mask);\n        __m128 new_smallest_pair = _mm_blendv_ps(smallest_pair, min_pairs, min_diff_mask);\n\n        min_diff = new_min_diff;\n        smallest_pair = new_smallest_pair;\n    }\n    \n    std::vector<float> result;\n    result.push_back(_mm_cvtss_f32(smallest_pair));\n    result.push_back(_mm_cvtss_f32(_mm_shuffle_ps(smallest_pair, smallest_pair, _MM_SHUFFLE(0,0,0,1))));\n    return result;\n}\n```"}
{"task_id": "SimdBench_71_SSE", "completion": "```cpp\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    __m128 min_diff = _mm_set1_ps(__FLT_MAX__);\n    __m128 result_pair = _mm_setzero_ps();\n    \n    for(size_t i = 0; i + 3 < numbers.size(); i += 4){\n        __m128 vec_a = _mm_loadu_ps(&numbers[i]);\n        __m128 vec_b = _mm_loadu_ps(&numbers[i+4]);\n        \n        __m128 diff_ab = _mm_sub_ps(vec_a, vec_b);\n        __m128 abs_diff_ab = _mm_abs_ps(diff_ab);\n        \n        __m128 mask = _mm_cmpgt_ps(abs_diff_ab, min_diff);\n        __m128 mask_inv = _mm_xor_ps(mask, _mm_set1_ps(-0.0f));\n        \n        min_diff = _mm_andnot_ps(mask_inv, abs_diff_ab);\n        result_pair = _mm_or_ps(_mm_and_ps(mask, result_pair), _mm_andnot_ps(mask, vec_a));\n    }\n    \n    float min_diff_arr[4];\n    _mm_storeu_ps(min_diff_arr, min_diff);\n    \n    std::vector<float> result;\n    for(int j = 0; j < 4; ++j){\n        if(min_diff_arr[j] == min_diff_arr[0]){\n            result.push_back(result_pair.m128_f32[j]);\n            result.push_back(result_pair.m128_f32[j+1]);\n            break;\n        }\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if(numbers.size() < 2)\n        return numbers;\n\n    // Load data into SIMD registers\n    __m128 min_val = _mm_set1_ps(numbers[0]);\n    __m128 max_val = _mm_set1_ps(numbers[0]);\n\n    for(size_t i = 1; i + 3 < numbers.size(); i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_val = _mm_min_ps(min_val, vec);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n\n    // Handle remaining elements\n    for(size_t i = numbers.size() & ~size_t(3); i < numbers.size(); ++i){\n        min_val = _mm_min_ss(min_val, _mm_set_ss(numbers[i]));\n        max_val = _mm_max_ss(max_val, _mm_set_ss(numbers[i]));\n    }\n\n    float min_scalar = _mm_cvtss_f32(min_val);\n    float max_scalar = _mm_cvtss_f32(max_val);\n\n    std::vector<float> result(numbers.size());\n    for(size_t i = 0; i + 3 < numbers.size(); i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 scaled = _mm_sub_ps(vec, _mm_set1_ps(min_scalar));\n        scaled = _mm_div_ps(scaled, _mm_set1_ps(max_scalar - min_scalar));\n        _mm_storeu_ps(&result[i], scaled);\n    }\n\n    for(size_t i = numbers.size() & ~size_t(3); i < numbers.size(); ++i){\n        result[i] = (numbers[i] - min_scalar) / (max_scalar - min_scalar);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if(numbers.size() < 2)\n        return numbers;\n\n    // Load data into SIMD registers\n    __m128 min_val = _mm_set1_ps(*std::min_element(numbers.begin(), numbers.end()));\n    __m128 max_val = _mm_set1_ps(*std::max_element(numbers.begin(), numbers.end()));\n\n    __m128 diff = _mm_sub_ps(max_val, min_val);\n    __m128 zero = _mm_setzero_ps();\n    __m128 one = _mm_set1_ps(1.0f);\n\n    size_t i = 0;\n    __m128 result;\n\n    // Process in chunks of 4 elements\n    for (; i <= numbers.size() - 4; i += 4) {\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        __m128 mask = _mm_cmpneq_ps(vec, min_val); // Avoid division by zero\n        __m128 inv_diff = _mm_rcp_ps(_mm_and_ps(mask, diff)); // Reciprocal approximation\n        __m128 scaled = _mm_sub_ps(one, _mm_mul_ps(_mm_mul_ps(vec, inv_diff), zero));\n        _mm_storeu_ps(&numbers[i], scaled);\n    }\n\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        numbers[i] = (numbers[i] != min_val) ? (1.0f - (numbers[i] / max_val)) : 0.0f;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if(numbers.size() < 2)\n        return numbers;\n\n    // Load data into SIMD registers\n    __m128 min_val = _mm_set1_ps(numbers[0]);\n    __m128 max_val = _mm_set1_ps(numbers[0]);\n\n    for(size_t i = 1; i < numbers.size(); i += 2){\n        __m128 current_vals = _mm_loadu_ps(&numbers[i]);\n        min_val = _mm_min_ps(min_val, current_vals);\n        max_val = _mm_max_ps(max_val, current_vals);\n    }\n\n    float global_min = _mm_cvtss_f32(min_val);\n    float global_max = _mm_cvtss_f32(max_val);\n\n    // Handle odd number of elements\n    if(numbers.size() % 2 != 0){\n        global_min = std::min(global_min, numbers[numbers.size() - 1]);\n        global_max = std::max(global_max, numbers[numbers.size() - 1]);\n    }\n\n    std::vector<float> result(numbers.size());\n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m128 input = _mm_loadu_ps(&numbers[i]);\n        __m128 scaled = _mm_sub_ps(input, _mm_set1_ps(global_min));\n        __m128 range = _mm_sub_ps(_mm_set1_ps(global_max), _mm_set1_ps(global_min));\n        scaled = _mm_div_ps(scaled, range);\n        _mm_storeu_ps(&result[i], scaled);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if(numbers.size() < 2)\n        return numbers;\n\n    // Load data into SIMD registers\n    __m128 min_val = _mm_set1_ps(numbers[0]);\n    __m128 max_val = _mm_set1_ps(numbers[0]);\n\n    int i = 0;\n    for(; i + 4 <= numbers.size(); i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_val = _mm_min_ps(min_val, vec);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n\n    float global_min = _mm_cvtss_f32(min_val);\n    float global_max = _mm_cvtss_f32(max_val);\n\n    for(int j = 0; j < i; j += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[j]);\n        __m128 scaled = _mm_sub_ps(vec, _mm_set1_ps(global_min));\n        scaled = _mm_div_ps(scaled, _mm_set1_ps(global_max - global_min));\n        _mm_storeu_ps(&numbers[j], scaled);\n    }\n\n    // Handle remaining elements\n    for(int j = i; j < numbers.size(); ++j){\n        numbers[j] = (numbers[j] - global_min) / (global_max - global_min);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    if(numbers.size() < 2)\n        return numbers;\n\n    // Load data into SIMD registers\n    __m128 min_val = _mm_set1_ps(numbers[0]);\n    __m128 max_val = _mm_set1_ps(numbers[0]);\n\n    int i = 0;\n    for(; i + 4 <= numbers.size(); i += 4){\n        __m128 vec = _mm_loadu_ps(&numbers[i]);\n        min_val = _mm_min_ps(min_val, vec);\n        max_val = _mm_max_ps(max_val, vec);\n    }\n\n    float local_min = _mm_cvtss_f32(min_val);\n    float local_max = _mm_cvtss_f32(max_val);\n\n    for(int j = 0; j < i; ++j){\n        numbers[j] = (numbers[j] - local_min) / (local_max - local_min);\n    }\n\n    for(; i < numbers.size(); ++i){\n        numbers[i] = (numbers[i] - local_min) / (local_max - local_min);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n){\n    __m128i num = _mm_set1_epi64x(n);\n    __m128i divisor = _mm_set_epi64x(0, 1);\n    __m128i zero = _mm_setzero_si128();\n    \n    while(!_mm_movemask_epi8(_mm_cmpeq_epi64(_mm_mul_epu32(divisor, _mm_set1_epi64x(2)), num))) {\n        divisor = _mm_add_epi64(divisor, _mm_set1_epi64x(2));\n    }\n    \n    return _mm_cvtsi128_si64(_mm_sub_epi64(num, _mm_mul_epu32(divisor, _mm_set1_epi64x(2))));\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n){\n    __m128i input = _mm_set1_epi64x(n);\n    __m128i divisor = _mm_set_epi64x(1, 1);\n    __m128i mask = _mm_set_epi64x(~0ULL, ~0ULL);\n\n    while(_mm_movemask_epi8(_mm_cmpeq_epi64(_mm_setzero_si128(), _mm_rem_epu64(input, divisor))) == 0){\n        divisor = _mm_add_epi64(divisor, _mm_set1_epi64x(1));\n        mask = _mm_andnot_si128(_mm_cmpgt_epi64(divisor, input), mask);\n    }\n\n    uint64_t result;\n    _mm_storeu_si128((__m128i*)&result, _mm_and_si128(divisor, _mm_set1_epi64x(n)));\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n){\n    __m128i input = _mm_set1_epi64x(n);\n    __m128i divisor = _mm_set_epi64x(1, 1);\n    __m128i mask = _mm_set_epi64x(~0ULL, ~0ULL);\n\n    while(_mm_movemask_epi8(_mm_cmpeq_epi64(_mm_setzero_si128(), _mm_rem_epu64(input, divisor)))) {\n        divisor = _mm_add_epi64(divisor, _mm_set1_epi64x(1));\n    }\n\n    return _mm_cvtsi128_si64(_mm_sub_epi64(divisor, _mm_set1_epi64x(1)));\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n){\n    __m128i num = _mm_set1_epi64x(n);\n    __m128i divisor = _mm_set_epi64x(0, 1);\n    __m128i zero = _mm_setzero_si128();\n    \n    while(!_mm_movemask_epi8(_mm_cmpeq_epi64(_mm_mul_epu32(divisor, _mm_set_epi64x(1, 1)), num))){\n        divisor = _mm_sub_epi64(divisor, _mm_set_epi64x(1, 1));\n    }\n    \n    return _mm_cvtsi128_si64(divisor);\n}\n```"}
{"task_id": "SimdBench_73_SSE", "completion": "```c\nuint64_t largest_divisor_simd(uint64_t n){\n    __m128i num = _mm_set1_epi64x(n);\n    __m128i divisor = _mm_set_epi64x(0, 1);\n    __m128i one = _mm_set1_epi64x(1);\n    __m128i zero = _mm_setzero_si128();\n    \n    while(!_mm_movemask_epi8(_mm_cmpeq_epi64(num, _mm_mul_epu32(divisor, one)))){\n        divisor = _mm_sub_epi64(divisor, one);\n    }\n    \n    return _mm_cvtsi128_si64(divisor);\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    \n    // Define SIMD registers\n    __m128i num = _mm_set1_epi32(n);\n    __m128i divisor = _mm_set_epi32(29, 23, 19, 17); // Prime divisors\n    __m128i mask = _mm_setzero_si128();\n    __m128i count = _mm_setzero_si128();\n    \n    // Loop until all numbers are reduced to 1\n    while (!_mm_movemask_epi8(_mm_cmpeq_epi32(num, _mm_setzero_si128()))){\n        // Check divisibility for each element\n        __m128i remainder = _mm_rem_epi32(num, divisor);\n        mask = _mm_cmpeq_epi32(remainder, _mm_setzero_si128());\n        \n        // If divisible, store the factor and update the number\n        __m128i temp = _mm_and_si128(mask, divisor);\n        factors.insert(factors.end(), (int*)&temp, (int*)&temp + 4);\n        \n        __m128i updated_num = _mm_div_epi32(num, divisor);\n        num = _mm_or_si128(num, _mm_andnot_si128(mask, updated_num));\n    }\n    \n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    \n    // Check divisibility by 2\n    __m128i num = _mm_set1_epi32(n);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i zero = _mm_setzero_si128();\n    __m128i divisible = _mm_cmpeq_epi32(_mm_and_si128(num, one), zero);\n    while(_mm_movemask_epi8(divisible) == 0xFFFF){\n        factors.push_back(2);\n        num = _mm_div_epi32(num, two);\n        divisible = _mm_cmpeq_epi32(_mm_and_si128(num, one), zero);\n    }\n    \n    // Check divisibility by odd numbers starting from 3\n    __m128i current = _mm_add_epi32(two, two);\n    while(!_mm_testz_si128(current, current)){\n        __m128i temp = _mm_div_epi32(num, current);\n        __m128i remainder = _mm_rem_epi32(num, current);\n        __m128i is_divisible = _mm_cmpeq_epi32(remainder, zero);\n        if(_mm_movemask_epi8(is_divisible) != 0){\n            factors.push_back(_mm_extract_epi32(current, 0));\n            num = temp;\n        }\n        current = _mm_add_epi32(current, two);\n    }\n    \n    // If remaining number is greater than 1\n    if(!_mm_testz_si128(num, num)){\n        factors.push_back(_mm_extract_epi32(num, 0));\n    }\n    \n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    __m128i number = _mm_set1_epi32(n);\n    __m128i limit = _mm_sqrt_ps(_mm_cvtepi32_ps(number));\n    __m128i i = _mm_set_epi32(4, 3, 2, 1);\n    \n    while (_mm_cmpgt_epi32(limit, i)) {\n        __m128i divisible = _mm_cmpeq_epi32(_mm_and_si128(number, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i factor = _mm_set1_epi32(i);\n        \n        while (_mm_movemask_epi8(divisible) == 0) {\n            factors.push_back(factor);\n            number = _mm_div_epi32(number, factor);\n            divisible = _mm_cmpeq_epi32(_mm_and_si128(number, _mm_set1_epi32(1)), _mm_setzero_si128());\n        }\n        \n        i = _mm_add_epi32(i, _mm_set1_epi32(2));\n        limit = _mm_sqrt_ps(_mm_cvtepi32_ps(number));\n    }\n    \n    if (!_mm_movemask_epi8(_mm_cmpeq_epi32(number, _mm_set1_epi32(1)))) {\n        factors.push_back(_mm_extract_epi32(number, 0));\n    }\n    \n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    __m128i number = _mm_set1_epi32(n);\n    __m128i limit = _mm_sqrt_epu32(_mm_set1_epi32(n));\n    __m128i candidate = _mm_set_epi32(7, 5, 3, 2);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i zero = _mm_setzero_si128();\n\n    while (_mm_movemask_epi8(_mm_cmpgt_epi32(candidate, one))) {\n        __m128i divisible = _mm_cmpeq_epi32(_mm_and_si128(number, one), zero);\n        int mask = _mm_movemask_epi8(divisible);\n        while (mask) {\n            int bit = __builtin_ctz(mask);\n            factors.insert(factors.end(), 4 - __builtin_popcount(mask), candidate.m128i_i32[bit]);\n            number = _mm_div_epi32(number, _mm_shuffle_epi32(candidate, _MM_SHUFFLE(0, 0, 0, 0)));\n            mask &= mask - 1;\n        }\n        candidate = _mm_add_epi32(candidate, _mm_set_epi32(4, 2, 2, 2));\n    }\n\n    if (!_mm_movemask_epi8(_mm_cmpeq_epi32(number, one))) {\n        factors.push_back(_mm_extract_epi32(number, 0));\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_SSE", "completion": "```cpp\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    \n    if(n < 2){\n        return factors;\n    }\n    \n    __m128i num = _mm_set1_epi32(n);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i limit = _mm_sqrt_ps(_mm_cvtepi32_ps(num));\n    __m128i current = two;\n    \n    while(!_mm_movemask_epi8(_mm_cmpeq_epi32(current, limit))){\n        __m128i rem = _mm_rem_epi32(num, current);\n        \n        if(_mm_movemask_epi8(_mm_cmpeq_epi32(rem, _mm_setzero_si128()))){\n            factors.push_back(_mm_extract_epi32(current, 0));\n            num = _mm_div_epi32(num, current);\n        }\n        else{\n            __m128i next = _mm_add_epi32(current, _mm_set1_epi32(1));\n            current = next;\n        }\n    }\n    \n    if(!_mm_movemask_epi8(_mm_cmpeq_epi32(num, _mm_set1_epi32(1)))){\n        factors.push_back(_mm_extract_epi32(num, 0));\n    }\n    \n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int size = l.size();\n    __m128 threshold = _mm_setzero_ps();\n    std::vector<float> result;\n    result.reserve(size); // Preallocate memory for efficiency\n\n    for(int i = 0; i < size; i += 4){\n        __m128 input = _mm_loadu_ps(&l[i]);\n        __m128 positive_mask = _mm_cmpgt_ps(input, threshold);\n        __m128 filtered = _mm_and_ps(input, positive_mask);\n        float temp[4];\n        _mm_storeu_ps(temp, filtered);\n        for(int j = 0; j < 4; ++j){\n            if(temp[j] != 0.0f){\n                result.push_back(temp[j]);\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int size = l.size();\n    __m128 threshold = _mm_setzero_ps();\n    std::vector<float> result;\n    result.reserve(size); // Preallocate memory for efficiency\n\n    for(int i = 0; i < size; i += 4){\n        __m128 input = _mm_loadu_ps(&l[i]);\n        __m128 positive_mask = _mm_cmpgt_ps(input, threshold);\n        __m128 filtered = _mm_and_ps(input, positive_mask);\n        float data[4];\n        _mm_storeu_ps(data, filtered);\n        for(int j = 0; j < 4; ++j){\n            if(data[j] != 0.0f){\n                result.push_back(data[j]);\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int size = l.size();\n    __m128 threshold = _mm_setzero_ps();\n    std::vector<float> result;\n    \n    for(int i = 0; i < size; i += 4){\n        __m128 input = _mm_loadu_ps(&l[i]);\n        __m128 positive_mask = _mm_cmpgt_ps(input, threshold);\n        __m128 filtered = _mm_and_ps(input, positive_mask);\n        float data[4];\n        _mm_storeu_ps(data, filtered);\n        for(int j = 0; j < 4; ++j){\n            if(data[j] > 0.0f){\n                result.push_back(data[j]);\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int size = l.size();\n    __m128 threshold = _mm_setzero_ps();\n    std::vector<float> result;\n    result.reserve(size); // Preallocate memory for efficiency\n\n    for(int i = 0; i < size; i += 4){\n        __m128 input = _mm_loadu_ps(&l[i]);\n        __m128 positive_mask = _mm_cmpgt_ps(input, threshold);\n        __m128 filtered = _mm_and_ps(input, positive_mask);\n        float data[4];\n        _mm_storeu_ps(data, filtered);\n        for(int j = 0; j < 4; ++j){\n            if(data[j] != 0.0f){\n                result.push_back(data[j]);\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_75_SSE", "completion": "```cpp\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    int size = l.size();\n    __m128 threshold = _mm_setzero_ps();\n    std::vector<float> result;\n    result.reserve(size); // Preallocate memory for efficiency\n\n    for(int i = 0; i < size; i += 4){\n        __m128 input = _mm_loadu_ps(&l[i]);\n        __m128 positive_mask = _mm_cmpgt_ps(input, threshold);\n        __m128 filtered = _mm_and_ps(input, positive_mask);\n        float data[4];\n        _mm_storeu_ps(data, filtered);\n        for(int j = 0; j < 4; ++j){\n            if(data[j] != 0.0f){\n                result.push_back(data[j]);\n            }\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m128i mask = _mm_set_epi32(0, 0, 0, 0);\n    __m128i numbers = _mm_set_epi32(5 + 4*3, 5 + 2*3, 5, 5 - 2*3);\n\n    for (__m128i limit = _mm_set1_epi32(n); _mm_movemask_epi8(_mm_cmple_epi32(numbers, limit)) != 0; ){\n        __m128i div = _mm_set1_epi32(1);\n        __m128i remainder = _mm_setzero_si128();\n\n        for (int i = 0; i < 4; ++i){\n            remainder = _mm_rem_epu32(numbers, div);\n            mask = _mm_or_si128(mask, _mm_cmpeq_epi32(remainder, _mm_setzero_si128()));\n            div = _mm_add_epi32(div, _mm_set1_epi32(2));\n        }\n\n        numbers = _mm_add_epi32(numbers, _mm_set_epi32(4, 2, 0, -2));\n    }\n\n    return !_mm_movemask_epi8(mask);\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n % 2 == 0 && n != 2) return false;\n\n    __m128i mask = _mm_set_epi32(0, 0, 0, 1);\n    __m128i primes = _mm_set_epi32(0, 0, 0, 1);\n\n    for(int i = 3; i * i <= n; i += 2){\n        __m128i factor = _mm_set1_epi32(i);\n        __m128i remainder = _mm_mul_epu32(factor, mask);\n        __m128i divisible = _mm_cmpeq_epi32(_mm_and_si128(primes, remainder), remainder);\n        primes = _mm_andnot_si128(divisible, primes);\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, primes);\n    return result[0] == 1;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m128i mask = _mm_set_epi32(0, 0, 0, 0);\n    __m128i numbers = _mm_set_epi32(n-4, n-6, n-8, n-10);\n\n    for(int i = 5; i*i <= n; i += 6){\n        __m128i factor = _mm_set1_epi32(i);\n        __m128i next_factor = _mm_add_epi32(factor, _mm_set1_epi32(2));\n\n        __m128i mod1 = _mm_rem_epu32(numbers, factor);\n        __m128i mod2 = _mm_rem_epu32(numbers, next_factor);\n\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi32(mod1, _mm_setzero_si128()));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi32(mod2, _mm_setzero_si128()));\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, mask);\n    for(int i = 0; i < 4; ++i){\n        if(result[i] != 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m128i mask = _mm_set_epi32(0, 0, 0, 0);\n    __m128i numbers = _mm_set_epi32(n-65, n-63, n-61, n-59);\n\n    for(int i = 5; i*i <= n; i += 6){\n        __m128i factor = _mm_set1_epi32(i);\n        __m128i next_factor = _mm_add_epi32(factor, _mm_set1_epi32(2));\n\n        __m128i mod1 = _mm_rem_epu32(numbers, factor);\n        __m128i mod2 = _mm_rem_epu32(numbers, next_factor);\n\n        __m128i cmp1 = _mm_cmpeq_epi32(mod1, _mm_setzero_si128());\n        __m128i cmp2 = _mm_cmpeq_epi32(mod2, _mm_setzero_si128());\n\n        mask = _mm_or_si128(mask, _mm_andnot_si128(cmp1, cmp2));\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, mask);\n    for(int i = 0; i < 4; ++i){\n        if(result[i] != 0){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_SSE", "completion": "```c\nbool is_prime_simd(int64_t n){\n    if (n <= 1) return false;\n    if (n <= 3) return true;\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    __m128i v_n = _mm_set1_epi64x(n);\n    __m128i v_i = _mm_set_epi64x(5, 1);\n\n    while (_mm_movemask_epi8(_mm_cmplt_epi64(v_i, v_n)) != 0){\n        __m128i v_mod = _mm_rem_epi64(v_n, v_i);\n        int mask = _mm_movemask_epi8(_mm_cmpeq_epi64(v_mod, _mm_setzero_si128()));\n        if (mask & 1) return false;\n        v_i = _mm_add_epi64(v_i, _mm_set1_epi64x(2));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    const int vec_size = 4;\n    const __m128i mask_lower = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_upper = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_alpha = _mm_set_epi8(\n        0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20\n    );\n    const __m128i mask_digit = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    int len = s.size();\n    if(len == 0) return \"\";\n\n    std::string result = s;\n    int i = 0;\n\n    while(i + vec_size <= len){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower = _mm_and_si128(vec, mask_lower);\n        __m128i upper = _mm_and_si128(vec, mask_upper);\n        __m128i alpha_mask = _mm_or_si128(lower, upper);\n        __m128i digit_mask = _mm_andnot_si128(alpha_mask, mask_digit);\n\n        // Check if there are any alphabetic characters\n        __m128i cmp_alpha = _mm_cmpeq_epi8(alpha_mask, mask_alpha);\n        int all_non_alpha = _mm_movemask_epi8(cmp_alpha) == 0xFFFF;\n\n        if(all_non_alpha){\n            // Reverse the vector\n            __m128i reversed = _mm_shuffle_epi8(vec, _mm_set_epi8(15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0));\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), reversed);\n        }\n        else{\n            // Toggle case for alphabetic characters\n            __m128i toggled = _mm_xor_si128(vec, mask_alpha);\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), toggled);\n        }\n\n        i += vec_size;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = s[i];\n        if(isalpha(c)){\n            result[i] = islower(c) ? toupper(c) : tolower(c);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    const int vec_size = 4;\n    const __m128i mask_lower = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_upper = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_alpha = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    \n    int n = s.size();\n    std::string result = s;\n    int i = 0;\n\n    while(i + vec_size <= n){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_mask = _mm_cmpgt_epi8(vec, mask_lower);\n        __m128i upper_mask = _mm_cmpgt_epi8(_mm_sub_epi8(mask_lower, vec), mask_lower);\n        __m128i alpha_mask = _mm_or_si128(lower_mask, upper_mask);\n\n        __m128i lower_case = _mm_andnot_si128(lower_mask, vec);\n        __m128i upper_case = _mm_andnot_si128(upper_mask, vec);\n        __m128i flipped_case = _mm_or_si128(lower_case, upper_case);\n\n        __m128i unchanged = _mm_andnot_si128(alpha_mask, vec);\n        __m128i transformed = _mm_or_si128(flipped_case, unchanged);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), transformed);\n        i += vec_size;\n    }\n\n    if(i < n){\n        // Handle remaining characters\n        for(int j = i; j < n; ++j){\n            char c = s[j];\n            if(c >= 'a' && c <= 'z') c -= 32;\n            else if(c >= 'A' && c <= 'Z') c += 32;\n            result[j] = c;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    const int vec_size = 4;\n    const __m128i mask_lower = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_upper = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_alpha = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    \n    int len = s.size();\n    std::string result = s;\n    int i = 0;\n\n    while(i + vec_size <= len){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_mask = _mm_cmplt_epi8(vec, _mm_set1_epi8('`'));\n        __m128i upper_mask = _mm_cmpgt_epi8(vec, _mm_set1_epi8('{'));\n        __m128i alpha_mask = _mm_or_si128(lower_mask, upper_mask);\n\n        __m128i flipped_case = _mm_xor_si128(vec, _mm_set1_epi8(0x20));\n        __m128i processed_chars = _mm_or_si128(_mm_andnot_si128(alpha_mask, vec), _mm_and_si128(alpha_mask, flipped_case));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), processed_chars);\n        i += vec_size;\n    }\n\n    // Handle remaining characters\n    while(i < len){\n        char c = s[i];\n        if(c >= 'a' && c <= 'z') c -= 32;\n        else if(c >= 'A' && c <= 'Z') c += 32;\n        result[i] = c;\n        i++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    const int vec_size = 4;\n    const __m128i mask_lower = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_upper = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_alpha = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_digit = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    \n    int len = s.size();\n    std::string result = s;\n    int i = 0;\n\n    while(i + vec_size <= len){\n        __m128i vec_s = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i vec_result = _mm_xor_si128(vec_s, mask_lower);\n        \n        // Check if character is alphabetic\n        __m128i is_alpha = _mm_cmpeq_epi8(_mm_and_si128(vec_s, mask_alpha), mask_alpha);\n        __m128i flipped_case = _mm_or_si128(_mm_andnot_si128(is_alpha, vec_result), _mm_and_si128(is_alpha, _mm_xor_si128(vec_s, mask_upper)));\n        \n        // Check if character is digit\n        __m128i is_digit = _mm_cmpeq_epi8(_mm_and_si128(vec_s, mask_digit), mask_digit);\n        flipped_case = _mm_or_si128(_mm_andnot_si128(is_digit, flipped_case), _mm_and_si128(is_digit, vec_s));\n        \n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), flipped_case);\n        i += vec_size;\n    }\n\n    // Handle remaining characters\n    while(i < len){\n        char c = s[i];\n        if(c >= 'A' && c <= 'Z') result[i] = c + ('a' - 'A');\n        else if(c >= 'a' && c <= 'z') result[i] = c - ('a' - 'A');\n        else if(!isalpha(c)) result[i] = s[len - (i + 1)];\n        i++;\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_77_SSE", "completion": "```cpp\nstd::string solve_simd(const std::string & s){\n    const int vec_size = 4;\n    const __m128i mask_lower = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_upper = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_alpha = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_digit = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    \n    int len = s.size();\n    std::string result = s;\n    int i = 0;\n\n    while(i + vec_size <= len){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_mask = _mm_or_si128(_mm_cmpgt_epi8(vec, _mm_set1_epi8('a' - 1)), mask_lower);\n        __m128i upper_mask = _mm_or_si128(_mm_cmpgt_epi8(vec, _mm_set1_epi8('A' - 1)), mask_upper);\n        __m128i alpha_mask = _mm_andnot_si128(_mm_or_si128(lower_mask, upper_mask), mask_alpha);\n        __m128i digit_mask = _mm_and_si128(_mm_cmpgt_epi8(vec, _mm_set1_epi8('9')), mask_digit);\n\n        __m128i lower_case = _mm_xor_si128(vec, _mm_set1_epi8(0x20));\n        __m128i upper_case = _mm_xor_si128(vec, _mm_set1_epi8(0x20));\n        \n        __m128i modified_vec = _mm_or_si128(_mm_andnot_si128(alpha_mask, vec),\n                                             _mm_or_si128(_mm_and_si128(lower_mask, lower_case),\n                                                          _mm_and_si128(upper_mask, upper_case)));\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), modified_vec);\n        i += vec_size;\n    }\n\n    // Handle remaining characters\n    for(; i < len; ++i){\n        char c = s[i];\n        if(c >= 'a' && c <= 'z') result[i] = c - ('a' - 'A');\n        else if(c >= 'A' && c <= 'Z') result[i] = c + ('a' - 'A');\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n    __m128i v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15;\n    int i = 0;\n\n    while(i + 16 <= n){\n        // Load 16 elements into SIMD registers\n        v0 = _mm_loadu_si128((__m128i*)&l[i+0]);\n        v1 = _mm_loadu_si128((__m128i*)&l[i+1]);\n        v2 = _mm_loadu_si128((__m128i*)&l[i+2]);\n        v3 = _mm_loadu_si128((__m128i*)&l[i+3]);\n        v4 = _mm_loadu_si128((__m128i*)&l[i+4]);\n        v5 = _mm_loadu_si128((__m128i*)&l[i+5]);\n        v6 = _mm_loadu_si128((__m128i*)&l[i+6]);\n        v7 = _mm_loadu_si128((__m128i*)&l[i+7]);\n        v8 = _mm_loadu_si128((__m128i*)&l[i+8]);\n        v9 = _mm_loadu_si128((__m128i*)&l[i+9]);\n        v10 = _mm_loadu_si128((__m128i*)&l[i+10]);\n        v11 = _mm_loadu_si128((__m128i*)&l[i+11]);\n        v12 = _mm_loadu_si128((__m128i*)&l[i+12]);\n        v13 = _mm_loadu_si128((__m128i*)&l[i+13]);\n        v14 = _mm_loadu_si128((__m128i*)&l[i+14]);\n        v15 = _mm_loadu_si128((__m128i*)&l[i+15]);\n\n        // Sort every 3rd element (indices 0, 3, 6, ...)\n        v0 = _mm_shuffle_epi32(v0, _MM_SHUFFLE(3,2,1,0)); // Swap lanes\n        v3 = _mm_shuffle_epi32(v3, _MM_SHUFFLE(3,2,1,0));\n        v6 = _mm_shuffle_epi32(v6, _MM_SHUFFLE(3,2,1,0));\n        v9 = _mm_shuffle_epi32(v9, _MM_SHUFFLE(3,2,1,0));\n        v12 = _mm_shuffle_epi32(v12, _MM_SHUFFLE(3,2,1,0));\n\n        // Store back the sorted values\n        _mm_storeu_si128((__m128i*)&result[i+0], v0);\n        _mm_storeu_si128((__m128i*)&result[i+3], v3);\n        _mm_storeu_si128((__m128i*)&result[i+6], v6);\n        _mm_storeu_si128((__m128i*)&result[i+9], v9);\n        _mm_storeu_si128((__m128i*)&result[i+12], v12);\n\n        i += 16;\n    }\n\n    // Handle remaining elements if any\n    for(; i < n; ++i){\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n    __m128i v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, v12, v13, v14, v15;\n    int i = 0;\n\n    while(i + 16 <= n){\n        // Load 16 elements into SIMD registers\n        v0 = _mm_loadu_si128((__m128i*)&l[i+0]);\n        v1 = _mm_loadu_si128((__m128i*)&l[i+1]);\n        v2 = _mm_loadu_si128((__m128i*)&l[i+2]);\n        v3 = _mm_loadu_si128((__m128i*)&l[i+3]);\n        v4 = _mm_loadu_si128((__m128i*)&l[i+4]);\n        v5 = _mm_loadu_si128((__m128i*)&l[i+5]);\n        v6 = _mm_loadu_si128((__m128i*)&l[i+6]);\n        v7 = _mm_loadu_si128((__m128i*)&l[i+7]);\n        v8 = _mm_loadu_si128((__m128i*)&l[i+8]);\n        v9 = _mm_loadu_si128((__m128i*)&l[i+9]);\n        v10 = _mm_loadu_si128((__m128i*)&l[i+10]);\n        v11 = _mm_loadu_si128((__m128i*)&l[i+11]);\n        v12 = _mm_loadu_si128((__m128i*)&l[i+12]);\n        v13 = _mm_loadu_si128((__m128i*)&l[i+13]);\n        v14 = _mm_loadu_si128((__m128i*)&l[i+14]);\n        v15 = _mm_loadu_si128((__m128i*)&l[i+15]);\n\n        // Sort every 3rd element (indices 0, 3, 6, ...)\n        v0 = _mm_shuffle_epi32(v0, _MM_SHUFFLE(0,0,2,0));\n        v3 = _mm_shuffle_epi32(v3, _MM_SHUFFLE(0,0,2,0));\n        v6 = _mm_shuffle_epi32(v6, _MM_SHUFFLE(0,0,2,0));\n        v9 = _mm_shuffle_epi32(v9, _MM_SHUFFLE(0,0,2,0));\n        v12 = _mm_shuffle_epi32(v12, _MM_SHUFFLE(0,0,2,0));\n        v15 = _mm_shuffle_epi32(v15, _MM_SHUFFLE(0,0,2,0));\n\n        // Store sorted values back\n        _mm_storeu_si128((__m128i*)&result[i+0], v0);\n        _mm_storeu_si128((__m128i*)&result[i+3], v3);\n        _mm_storeu_si128((__m128i*)&result[i+6], v6);\n        _mm_storeu_si128((__m128i*)&result[i+9], v9);\n        _mm_storeu_si128((__m128i*)&result[i+12], v12);\n        _mm_storeu_si128((__m128i*)&result[i+15], v15);\n\n        i += 16;\n    }\n\n    // Handle remaining elements if any\n    for(; i < n; ++i){\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n    __m128i v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, va, vb, vc, vd, ve, vf;\n\n    for(int i=0; i<n-15; i+=16){\n        v0 = _mm_loadu_si128((__m128i*)&l[i+0]);\n        v1 = _mm_loadu_si128((__m128i*)&l[i+4]);\n        v2 = _mm_loadu_si128((__m128i*)&l[i+8]);\n        v3 = _mm_loadu_si128((__m128i*)&l[i+12]);\n\n        // Extract elements divisible by 3\n        v4 = _mm_shuffle_epi32(v0, _MM_SHUFFLE(3,2,1,0));\n        v5 = _mm_shuffle_epi32(v1, _MM_SHUFFLE(3,2,1,0));\n        v6 = _mm_shuffle_epi32(v2, _MM_SHUFFLE(3,2,1,0));\n        v7 = _mm_shuffle_epi32(v3, _MM_SHUFFLE(3,2,1,0));\n\n        // Sort each group of 4 elements\n        v4 = _mm_min_epi32(v4, v5);\n        v6 = _mm_max_epi32(v6, v7);\n        v5 = _mm_permute_ps(_mm_castsi128_ps(v4), 0x55);\n        v7 = _mm_permute_ps(_mm_castsi128_ps(v6), 0x55);\n\n        v4 = _mm_max_epi32(v4, v5);\n        v6 = _mm_min_epi32(v6, v7);\n\n        // Store back the sorted elements\n        _mm_storeu_si128((__m128i*)&result[i+0], v4);\n        _mm_storeu_si128((__m128i*)&result[i+4], v6);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    std::vector<int> result = l;\n    \n    if(n < 3){\n        return result;\n    }\n    \n    __m128i* vec = reinterpret_cast<__m128i*>(result.data());\n    int num_full_vecs = n / 4; // Each vector has 4 integers\n\n    for(int i = 0; i < num_full_vecs; ++i){\n        __m128i v = vec[i];\n        \n        // Extract elements at indices divisible by 3\n        __m128i v0 = _mm_shuffle_epi32(v, _MM_SHUFFLE(0, 0, 0, 0)); // Extract first element\n        __m128i v1 = _mm_shuffle_epi32(v, _MM_SHUFFLE(1, 1, 1, 1)); // Extract second element\n        __m128i v2 = _mm_shuffle_epi32(v, _MM_SHUFFLE(2, 2, 2, 2)); // Extract third element\n        __m128i v3 = _mm_shuffle_epi32(v, _MM_SHUFFLE(3, 3, 3, 3)); // Extract fourth element\n        \n        // Sort the extracted elements\n        __m128i sorted = _mm_minpos_epu16(_mm_packus_epi16(v0, v1));\n        sorted = _mm_unpacklo_epi8(sorted, _mm_setzero_si128());\n        \n        // Pack back into the original vector\n        vec[i] = _mm_or_si128(\n            _mm_andnot_si128(_mm_set_epi32(0, 0, 0, -1), v),\n            _mm_and_si128(_mm_set_epi32(0, 0, 0, -1), sorted)\n        );\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_78_SSE", "completion": "```cpp\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    __m128i unsorted = _mm_setzero_si128();\n    __m128i sorted = _mm_set_epi32(0, 0, 0, 0);\n    std::vector<int> result = l;\n\n    for(int i = 0; i < n; i += 4){\n        if(i + 4 <= n){\n            unsorted = _mm_loadu_si128((__m128i*)&l[i]);\n            __m128i a = _mm_shuffle_epi32(unsorted, _MM_SHUFFLE(0, 0, 0, 0));\n            __m128i b = _mm_shuffle_epi32(unsorted, _MM_SHUFFLE(1, 1, 1, 1));\n            __m128i c = _mm_shuffle_epi32(unsorted, _MM_SHUFFLE(2, 2, 2, 2));\n            __m128i d = _mm_shuffle_epi32(unsorted, _MM_SHUFFLE(3, 3, 3, 3));\n\n            if(a > b){\n                a = _mm_min_epu32(a, b);\n                b = _mm_max_epu32(a, b);\n            }\n            if(c > d){\n                c = _mm_min_epu32(c, d);\n                d = _mm_max_epu32(c, d);\n            }\n            if(b > d){\n                __m128i temp = b;\n                b = d;\n                d = temp;\n            }\n            if(a > c){\n                __m128i temp = a;\n                a = c;\n                c = temp;\n            }\n\n            sorted = _mm_set_epi32(d.m128i_i32[0], c.m128i_i32[1], b.m128i_i32[2], a.m128i_i32[3]);\n\n            _mm_storeu_si128((__m128i*)&result[i], sorted);\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    __m128 max_val = _mm_setzero_ps();\n    int size = l.size();\n    \n    for(int i = 0; i < size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, max_val);\n        max_val = _mm_or_ps(_mm_and_ps(mask, vec), _mm_andnot_ps(mask, max_val));\n    }\n    \n    float result[4];\n    _mm_storeu_ps(result, max_val);\n    \n    float max_f = result[0];\n    for(int j = 1; j < 4; ++j){\n        if(result[j] > max_f){\n            max_f = result[j];\n        }\n    }\n    \n    return max_f;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l){\n    __m128 max_val = _mm_setzero_ps();\n    int size = l.size();\n    \n    for(int i = 0; i < size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, max_val);\n        max_val = _mm_or_ps(_mm_and_ps(mask, vec), _mm_andnot_ps(mask, max_val));\n    }\n    \n    float result[4];\n    _mm_storeu_ps(result, max_val);\n    \n    float max_f = result[0];\n    for(int j = 1; j < 4; ++j){\n        if(result[j] > max_f){\n            max_f = result[j];\n        }\n    }\n    \n    return max_f;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l){\n    __m128 max_val = _mm_setzero_ps();\n    int size = l.size();\n    \n    for(int i = 0; i < size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, max_val);\n        max_val = _mm_or_ps(_mm_and_ps(mask, vec), _mm_andnot_ps(mask, max_val));\n    }\n    \n    float result[4];\n    _mm_storeu_ps(result, max_val);\n    \n    float max_result = result[0];\n    for(int j = 1; j < 4; ++j){\n        if(result[j] > max_result){\n            max_result = result[j];\n        }\n    }\n    \n    return max_result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    __m128 max_val = _mm_setzero_ps();\n    int size = l.size();\n    \n    for(int i=0; i<size; i+=4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, max_val);\n        max_val = _mm_or_ps(_mm_and_ps(mask, vec), _mm_andnot_ps(mask, max_val));\n    }\n    \n    __m128 shuffle_mask = _mm_setr_ps(0, 0, 0, 0);\n    max_val = _mm_shuffle_ps(max_val, max_val, _MM_SHUFFLE(0,0,0,0));\n    \n    float result[4];\n    _mm_storeu_ps(result, max_val);\n    \n    float max_result = result[0];\n    for(int i=1;i<4;i++) {\n        if(result[i] > max_result) {\n            max_result = result[i];\n        }\n    }\n    \n    return max_result;\n}\n```"}
{"task_id": "SimdBench_79_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nfloat max_element_simd(std::vector<float> l){\n    __m128 max_val = _mm_setzero_ps();\n    int size = l.size();\n    \n    for(int i = 0; i < size; i += 4){\n        __m128 vec = _mm_loadu_ps(&l[i]);\n        __m128 mask = _mm_cmpgt_ps(vec, max_val);\n        max_val = _mm_or_ps(_mm_and_ps(mask, vec), _mm_andnot_ps(mask, max_val));\n    }\n    \n    float result[4];\n    _mm_storeu_ps(result, max_val);\n    \n    float max_f = result[0];\n    for(int j = 1; j < 4; ++j){\n        if(result[j] > max_f){\n            max_f = result[j];\n        }\n    }\n    \n    return max_f;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(n);\n    __m128i divisor1 = _mm_set1_epi32(11);\n    __m128i divisor2 = _mm_set1_epi32(13);\n    __m128i mask7 = _mm_setr_epi8(\n        0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,1,0\n    );\n    \n    for(int i=0; i<n; i+=4){\n        __m128i numbers = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i divisible1 = _mm_cmpeq_epi32(_mm_mod_epi32(numbers, divisor1), _mm_setzero_si128());\n        __m128i divisible2 = _mm_cmpeq_epi32(_mm_mod_epi32(numbers, divisor2), _mm_setzero_si128());\n        __m128i divisible = _mm_or_si128(divisible1, divisible2);\n        \n        __m128i digits = _mm_and_si128(_mm_set1_epi32(0xF), _mm_div_epi32(numbers, _mm_set1_epi32(10)));\n        __m128i sevens = _mm_cmpeq_epi8(digits, mask7);\n        sevens = _mm_and_si128(sevens, divisible);\n        \n        count += _mm_popcnt_epi32(sevens).m128i_u32[0];\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(n);\n    __m128i divisor1 = _mm_set1_epi32(11);\n    __m128i divisor2 = _mm_set1_epi32(13);\n    __m128i mask7 = _mm_setr_epi8(\n        0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,1\n    );\n    \n    for(int i=0; i<n; i+=8){\n        __m128i numbers = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i div1_mask = _mm_cmpeq_epi32(_mm_and_si128(numbers, divisor1), _mm_setzero_si128());\n        __m128i div2_mask = _mm_cmpeq_epi32(_mm_and_si128(numbers, divisor2), _mm_setzero_si128());\n        __m128i div_mask = _mm_or_si128(div1_mask, div2_mask);\n        \n        __m128i digits = _mm_and_si128(numbers, mask7);\n        __m128i seven_mask = _mm_cmpeq_epi8(digits, _mm_set1_epi8('7'));\n        int local_count[8];\n        _mm_storeu_si128((__m128i*)local_count, seven_mask);\n        \n        for(int j=0; j<8; ++j){\n            if(local_count[j] && _mm_extract_epi32(div_mask, j)){\n                count++;\n            }\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(n);\n    __m128i divisor1 = _mm_set1_epi32(11);\n    __m128i divisor2 = _mm_set1_epi32(13);\n    __m128i mask7 = _mm_setr_epi8(\n        0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,1,0\n    );\n    \n    for(int i=0; i<n; i+=8){\n        __m128i nums = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i div1_mask = _mm_cmplt_epi32(nums, threshold);\n        __m128i mod1 = _mm_and_si128(div1_mask, _mm_mod_epi32(nums, divisor1));\n        __m128i mod2 = _mm_and_si128(div1_mask, _mm_mod_epi32(nums, divisor2));\n        __m128i divisible = _mm_or_si128(mod1, mod2);\n        __m128i digits = _mm_and_si128(_mm_set1_epi32(100), nums);\n        __m128i sevens = _mm_and_si128(digits, mask7);\n        count += _mm_popcnt_epi32(sevens);\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(n);\n    __m128i divisor1 = _mm_set1_epi32(11);\n    __m128i divisor2 = _mm_set1_epi32(13);\n    __m128i mask7 = _mm_setr_epi8(\n        0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,1,0\n    );\n    \n    for(int i=0; i<n; i+=4){\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i div1_mask = _mm_cmplt_epi32(_mm_mod_epi32(nums, divisor1), _mm_setzero_si128());\n        __m128i div2_mask = _mm_cmplt_epi32(_mm_mod_epi32(nums, divisor2), _mm_setzero_si128());\n        __m128i div_mask = _mm_or_si128(div1_mask, div2_mask);\n        \n        __m128i digits = _mm_and_si128(_mm_set1_epi32(0xF), nums);\n        __m128i seven_mask = _mm_cmpeq_epi8(digits, mask7);\n        seven_mask = _mm_or_si128(seven_mask, _mm_srli_si128(seven_mask, 1));\n        seven_mask = _mm_or_si128(seven_mask, _mm_srli_si128(seven_mask, 2));\n        seven_mask = _mm_or_si128(seven_mask, _mm_srli_si128(seven_mask, 3));\n        \n        __m128i valid_sevens = _mm_and_si128(seven_mask, div_mask);\n        count += _mm_popcnt_epi32(valid_sevens).m128i_u32[0];\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_80_SSE", "completion": "```c\nint fizz_buzz_simd(int n){\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(n);\n    __m128i divisor1 = _mm_set1_epi32(11);\n    __m128i divisor2 = _mm_set1_epi32(13);\n    __m128i mask7 = _mm_setr_epi8(\n        0,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,1,0\n    );\n    \n    for(int i=0; i<n; i+=4){\n        __m128i nums = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i div1_mask = _mm_cmplt_epi32(_mm_rem_epu32(nums, divisor1), threshold);\n        __m128i div2_mask = _mm_cmplt_epi32(_mm_rem_epu32(nums, divisor2), threshold);\n        __m128i div_mask = _mm_or_si128(div1_mask, div2_mask);\n        __m128i digits = _mm_cvtepu32_epi8(nums);\n        __m128i seven_mask = _mm_cmpeq_epi8(digits, mask7);\n        __m128i seven_count = _mm_movemask_epi8(seven_mask);\n        count += __builtin_popcount(seven_count & _mm_movemask_epi8(div_mask));\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    std::vector<float> result(n);\n    \n    if(n == 0) return result;\n    \n    // Load data into SIMD registers\n    __m128 v0 = _mm_loadu_ps(&l[0]);\n    __m128 v1 = _mm_loadu_ps(&l[4]);\n    __m128 v2 = _mm_loadu_ps(&l[8]);\n    __m128 v3 = _mm_loadu_ps(&l[12]);\n\n    // Sort each pair of elements in parallel\n    __m128 sorted0 = _mm_shuffle_ps(v0, v0, _MM_SHUFFLE(0,0,2,0)); // Sort first two\n    __m128 sorted1 = _mm_shuffle_ps(v1, v1, _MM_SHUFFLE(0,0,2,0)); // Sort next two\n    __m128 sorted2 = _mm_shuffle_ps(v2, v2, _MM_SHUFFLE(0,0,2,0)); // Sort next two\n    __m128 sorted3 = _mm_shuffle_ps(v3, v3, _MM_SHUFFLE(0,0,2,0)); // Sort last two\n\n    // Store sorted results back\n    _mm_storeu_ps(&result[0], sorted0);\n    _mm_storeu_ps(&result[4], sorted1);\n    _mm_storeu_ps(&result[8], sorted2);\n    _mm_storeu_ps(&result[12], sorted3);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int n = l.size();\n    std::vector<float> result(n);\n    \n    if(n == 0) return result;\n    \n    // Load data into SIMD registers\n    __m128 v0 = _mm_loadu_ps(&l[0]);\n    __m128 v1 = _mm_loadu_ps(&l[4]);\n    __m128 v2 = _mm_loadu_ps(&l[8]);\n    __m128 v3 = _mm_loadu_ps(&l[12]);\n    \n    // Sort each pair of elements in parallel\n    __m128 sorted0 = _mm_shuffle_ps(v0, v0, _MM_SHUFFLE(0,0,2,0)); // Sort first two\n    __m128 sorted1 = _mm_shuffle_ps(v1, v1, _MM_SHUFFLE(0,0,2,0)); // Sort next two\n    __m128 sorted2 = _mm_shuffle_ps(v2, v2, _MM_SHUFFLE(0,0,2,0)); // Sort next two\n    __m128 sorted3 = _mm_shuffle_ps(v3, v3, _MM_SHUFFLE(0,0,2,0)); // Sort last two\n    \n    // Store sorted results back\n    _mm_storeu_ps(&result[0], sorted0);\n    _mm_storeu_ps(&result[4], sorted1);\n    _mm_storeu_ps(&result[8], sorted2);\n    _mm_storeu_ps(&result[12], sorted3);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int size = l.size();\n    std::vector<float> result(size);\n    \n    if(size < 2){\n        return l;\n    }\n    \n    // Ensure size is even for simplicity\n    if(size % 2 != 0){\n        size -= 1;\n    }\n\n    __m128* input = (__m128*)&l[0];\n    __m128* output = (__m128*)&result[0];\n\n    for(int i = 0; i < size; i += 4){\n        __m128 v0 = _mm_load_ps(&l[i + 0]);\n        __m128 v1 = _mm_load_ps(&l[i + 4]);\n\n        // Sort each pair of elements\n        __m128 low0 = _mm_min_ps(v0, v1);\n        __m128 high0 = _mm_max_ps(v0, v1);\n\n        // Store back into the result\n        _mm_store_ps(&result[i + 0], low0);\n        _mm_store_ps(&result[i + 4], high0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int size = l.size();\n    std::vector<float> result(size);\n    \n    if(size < 2){\n        return l;\n    }\n    \n    // Ensure size is even for simplicity\n    if(size % 2 != 0){\n        size -= 1;\n    }\n\n    __m128* input = (__m128*)&l[0];\n    __m128* output = (__m128*)&result[0];\n\n    for(int i = 0; i < size; i += 4){\n        __m128 v0 = _mm_load_ps(&l[i + 0]);\n        __m128 v1 = _mm_load_ps(&l[i + 4]);\n\n        // Sort each pair of elements\n        __m128 low0 = _mm_min_ps(v0, v1);\n        __m128 high0 = _mm_max_ps(v0, v1);\n\n        // Store back into the result\n        _mm_store_ps(&result[i + 0], low0);\n        _mm_store_ps(&result[i + 4], high0);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_81_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    int size = l.size();\n    std::vector<float> result(size);\n    \n    if(size < 2){\n        return l;\n    }\n    \n    // Ensure size is even for simplicity\n    if(size % 2 != 0){\n        size -= 1;\n    }\n\n    __m128* input = (__m128*)&l[0];\n    __m128* output = (__m128*)&result[0];\n\n    for(int i = 0; i < size; i += 4){\n        __m128 v0 = _mm_load_ps(&input[i].m128_f32[0]);\n        __m128 v1 = _mm_load_ps(&input[i].m128_f32[4]);\n\n        // Sort each pair of elements\n        __m128 low0 = _mm_min_ps(v0, v1);\n        __m128 high0 = _mm_max_ps(v0, v1);\n\n        // Store back to output\n        _mm_store_ps(&output[i].m128_f32[0], low0);\n        _mm_store_ps(&output[i].m128_f32[4], high0);\n    }\n\n    // Handle remaining elements if size is not multiple of 4\n    for(int i = size; i < size + 2; ++i){\n        result[i] = l[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int n = l.size();\n    if(n < 3) return false;\n    \n    __m128i zero = _mm_setzero_si128();\n    bool found = false;\n\n    for(int i=0; i<n-2 && !found; i++){\n        for(int j=i+1; j<n-1 && !found; j++){\n            __m128i target = _mm_sub_epi32(zero, _mm_set_epi32(l[j], l[j], l[i], l[i]));\n            \n            // Load next 4 elements starting from k=i+2\n            int k_start = j + 1;\n            for(int k=k_start; k<=n-4; k+=4){\n                __m128i vec = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i mask = _mm_cmpeq_epi32(vec, target);\n                int mask_int = _mm_movemask_epi8(_mm_castsi128_ps(mask));\n                if(mask_int != 0){\n                    found = true;\n                    break;\n                }\n            }\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    size_t n = l.size();\n    if(n < 3) return false;\n    \n    __m128i zero = _mm_setzero_si128();\n    bool found = false;\n\n    for(size_t i=0; i<n-2 && !found; i++){\n        for(size_t j=i+1; j<n-1 && !found; j++){\n            __m128i sum_vec = _mm_set_epi32(l[i], l[j], 0, 0);\n            for(size_t k=j+1; k<n && !found; k++){\n                sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(l[k]));\n                __m128i cmp = _mm_cmpeq_epi32(sum_vec, zero);\n                unsigned int mask = _mm_movemask_epi8(cmp);\n                if(mask != 0){\n                    found = true;\n                }\n            }\n        }\n    }\n    return found;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    size_t n = l.size();\n    if(n < 3) return false;\n    \n    __m128i zero = _mm_setzero_si128();\n    bool found = false;\n\n    for(size_t i=0; i<n-2 && !found; i++){\n        for(size_t j=i+1; j<n-1 && !found; j++){\n            __m128i sum_vec = _mm_set_epi32(l[i], l[j], 0, 0);\n            \n            for(size_t k=j+1; k<n && !found; k++){\n                sum_vec = _mm_add_epi32(sum_vec, _mm_set1_epi32(l[k]));\n                \n                // Check if any element in the vector is zero\n                __m128i mask = _mm_cmpeq_epi32(sum_vec, zero);\n                int mask_int = _mm_movemask_epi8(_mm_castsi128_ps(mask));\n                \n                if(mask_int != 0){\n                    found = true;\n                }\n            }\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    size_t n = l.size();\n    if(n < 3) return false;\n    \n    __m128i zero = _mm_setzero_si128();\n    for(size_t i=0; i<n-2; i+=4){\n        __m128i a = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i b = _mm_loadu_si128((__m128i*)&l[i+1]);\n        __m128i c = _mm_loadu_si128((__m128i*)&l[i+2]);\n        \n        __m128i sum1 = _mm_add_epi32(a, b);\n        __m128i sum2 = _mm_add_epi32(sum1, c);\n        \n        int mask = _mm_movemask_epi8(_mm_cmpeq_epi32(sum2, zero));\n        if(mask != 0) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_SSE", "completion": "```cpp\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    size_t n = l.size();\n    if(n < 3) return false;\n    \n    __m128i zero = _mm_setzero_si128();\n    bool found = false;\n\n    for(size_t i=0; i<n-2 && !found; i++){\n        for(size_t j=i+1; j<n-1 && !found; j++){\n            __m128i target = _mm_sub_epi32(zero, _mm_set1_epi32(l[i] + l[j]));\n            \n            for(size_t k=j+1; k<n && !found; k+=4){\n                __m128i vec = _mm_loadu_si128((__m128i*)&l[k]);\n                __m128i mask = _mm_cmpeq_epi32(vec, target);\n                int mask_int = _mm_movemask_epi8(mask);\n                if(mask_int != 0){\n                    found = true;\n                }\n            }\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    \n    if(size == 0) return result;\n    \n    // Ensure size is a multiple of 2 for SSE2 alignment\n    int aligned_size = (size / 2) * 2;\n    \n    __m128i* in = (__m128i*)l.data();\n    __m128i* out = (__m128i*)result.data();\n    \n    for(int i = 0; i < aligned_size; i += 2){\n        __m128i vec = _mm_load_si128(in + i);\n        __m128i inc = _mm_add_epi32(vec, _mm_set1_epi32(1));\n        _mm_store_si128(out + i, inc);\n    }\n    \n    // Handle remaining elements\n    for(int i = aligned_size; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    \n    if(size == 0) return result;\n    \n    // Ensure size is a multiple of 2 for SSE2 alignment\n    int aligned_size = (size / 2) * 2;\n    \n    __m128i* in = (__m128i*)l.data();\n    __m128i* out = (__m128i*)result.data();\n    \n    for(int i = 0; i < aligned_size; i += 2){\n        __m128i vec = _mm_loadu_si128(in + i);\n        __m128i inc = _mm_add_epi32(vec, _mm_set1_epi32(1));\n        _mm_storeu_si128(out + i, inc);\n    }\n    \n    // Handle remaining elements\n    for(int i = aligned_size; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    \n    if(size == 0) return result;\n    \n    // Ensure size is a multiple of 2 for SSE2 alignment\n    int aligned_size = (size / 2) * 2;\n    \n    __m128i* in = (__m128i*)l.data();\n    __m128i* out = (__m128i*)result.data();\n    \n    for(int i = 0; i < aligned_size; i += 2){\n        __m128i vec = _mm_loadu_si128(in + i);\n        __m128i inc = _mm_add_epi32(vec, _mm_set1_epi32(1));\n        _mm_storeu_si128(out + i, inc);\n    }\n    \n    // Handle remaining elements\n    for(int i = aligned_size; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    \n    if(size == 0) return result;\n    \n    // Ensure size is a multiple of 2 for SSE2 alignment\n    int aligned_size = (size / 2) * 2;\n    \n    __m128i* in = (__m128i*)l.data();\n    __m128i* out = (__m128i*)result.data();\n    \n    for(int i = 0; i < aligned_size; i += 2){\n        __m128i vec = _mm_load_si128(in + i);\n        __m128i inc = _mm_add_epi32(vec, _mm_set1_epi32(1));\n        _mm_store_si128(out + i, inc);\n    }\n    \n    // Handle remaining elements\n    for(int i = aligned_size; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_83_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int size = l.size();\n    std::vector<int> result(size);\n    \n    if(size == 0) return result;\n    \n    // Ensure size is a multiple of 2 for SSE2 alignment\n    int aligned_size = (size / 2) * 2;\n    \n    __m128i* in = (__m128i*)l.data();\n    __m128i* out = (__m128i*)result.data();\n    \n    for(int i = 0; i < aligned_size; i += 2){\n        __m128i vec = _mm_load_si128(in + i);\n        __m128i inc = _mm_add_epi32(vec, _mm_set1_epi32(1));\n        _mm_store_si128(out + i, inc);\n    }\n    \n    // Handle remaining elements\n    for(int i = aligned_size; i < size; ++i){\n        result[i] = l[i] + 1;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    int n = l.size();\n    if(n < 2) return false;\n\n    __m128i zero_vec = _mm_setzero_si128();\n    std::vector<int> padded_l(l);\n    \n    // Pad the vector to make its size a multiple of 4\n    while(padded_l.size() % vec_size != 0) {\n        padded_l.push_back(0);\n    }\n\n    n = padded_l.size();\n\n    for(int i=0; i<n-vec_size+1; i+=vec_size){\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&padded_l[i]);\n        __m128i neg_current_vec = _mm_sub_epi32(zero_vec, current_vec);\n\n        // Check if any element in neg_current_vec exists in the rest of the vector\n        for(int j=i+vec_size; j<n-vec_size+1; j+=vec_size){\n            __m128i compare_vec = _mm_loadu_si128((__m128i*)&padded_l[j]);\n            __m128i match_mask = _mm_cmpeq_epi32(compare_vec, neg_current_vec);\n            \n            if(_mm_movemask_epi8(match_mask) != 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    int n = l.size();\n    if(n < 2) return false;\n\n    __m128i zero_vec = _mm_setzero_si128();\n    std::vector<int> padded_l(l);\n    \n    // Pad the vector to make its size a multiple of 4\n    while(padded_l.size() % vec_size != 0) {\n        padded_l.push_back(0);\n    }\n\n    n = padded_l.size();\n\n    for(int i=0; i<n-vec_size+1; i+=vec_size){\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&padded_l[i]);\n        __m128i neg_current_vec = _mm_sub_epi32(zero_vec, current_vec);\n\n        // Check if any element in neg_current_vec exists in the rest of the vector\n        for(int j=i+vec_size; j<n-vec_size+1; j+=vec_size){\n            __m128i compare_vec = _mm_loadu_si128((__m128i*)&padded_l[j]);\n            __m128i match_mask = _mm_cmpeq_epi32(compare_vec, neg_current_vec);\n            if(_mm_movemask_epi8(match_mask) != 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 4;\n    if(l.size() < vec_size) return false;\n    \n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i neg_vecs[vec_size];\n    for(int i=0; i<vec_size; ++i){\n        neg_vecs[i] = _mm_set1_epi32(-l[i]);\n    }\n    \n    for(size_t i=0; i+l.size()-vec_size >=0; i+=vec_size){\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i eq_mask = _mm_cmpeq_epi32(current_vec, neg_vecs[0]);\n        for(int j=1; j<vec_size; ++j){\n            __m128i temp_mask = _mm_cmpeq_epi32(current_vec, neg_vecs[j]);\n            eq_mask = _mm_or_si128(eq_mask, temp_mask);\n        }\n        int mask = _mm_movemask_epi8(eq_mask);\n        if(mask != 0) return true;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 4;\n    if(l.size() < vec_size){\n        for(size_t i = 0; i < l.size(); ++i){\n            for(size_t j = i + 1; j < l.size(); ++j){\n                if(l[i] + l[j] == 0){\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n\n    __m128i zero_vec = _mm_setzero_si128();\n    size_t i = 0;\n    for(; i <= l.size() - vec_size; i += vec_size){\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i neg_current_vec = _mm_sub_epi32(zero_vec, current_vec);\n        for(size_t j = i + vec_size; j <= l.size() - vec_size; j += vec_size){\n            __m128i compare_vec = _mm_loadu_si128((__m128i*)&l[j]);\n            __m128i eq_mask = _mm_cmpeq_epi32(neg_current_vec, compare_vec);\n            if(_mm_movemask_epi8(eq_mask) != 0){\n                return true;\n            }\n        }\n    }\n    for(size_t k = i; k < l.size(); ++k){\n        for(size_t m = k + 1; m < l.size(); ++m){\n            if(l[k] + l[m] == 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_SSE", "completion": "```cpp\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    const int vec_size = 4;\n    if(l.size() < vec_size){\n        for(size_t i = 0; i < l.size(); ++i){\n            for(size_t j = i + 1; j < l.size(); ++j){\n                if(l[i] + l[j] == 0){\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n\n    __m128i zero_vec = _mm_setzero_si128();\n    size_t i = 0;\n    for(; i <= l.size() - vec_size; i += vec_size){\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i neg_current_vec = _mm_sub_epi32(zero_vec, current_vec);\n        for(int j = 0; j < vec_size; ++j){\n            if(_mm_movemask_epi8(_mm_cmpeq_epi32(_mm_shuffle_epi32(current_vec, _MM_SHUFFLE(j,j,j,j)), neg_current_vec)) & 0x0F){\n                return true;\n            }\n        }\n    }\n    for(; i < l.size(); ++i){\n        for(size_t j = i + 1; j < l.size(); ++j){\n            if(l[i] + l[j] == 0){\n                return true;\n            }\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    const int8_t SIMD_WIDTH = 4;\n    __m128i dividend = _mm_set1_epi64x(x);\n    __m128i divisor = _mm_set1_epi8(base);\n    __m128i remainders = _mm_setzero_si128();\n    std::string result;\n\n    while (_mm_movemask_epi8(_mm_cmpeq_epi8(dividend, _mm_setzero_si128())) == 0){\n        __m128i div_result = _mm_div_epu32(dividend, divisor);\n        __m128i rem_result = _mm_sub_epi8(dividend, _mm_mul_epu32(div_result, divisor));\n        remainders = _mm_insert_epi64(remainders, _mm_extract_epi64(rem_result, 0), 0);\n        dividend = div_result;\n    }\n\n    for(int i=0; i<SIMD_WIDTH; ++i){\n        int8_t digit = _mm_extract_epi8(remainders, i);\n        result += ('0' + digit);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    // This is a placeholder for the actual implementation.\n    // The task requires implementing the function using SSE/SSE2 intrinsics,\n    // but the problem description involves string manipulation which is not directly\n    // supported by SIMD instructions. Thus, a full SIMD implementation of the described\n    // functionality is not feasible without significant deviation from standard SIMD use cases.\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    // This is a placeholder implementation.\n    // The actual implementation requires detailed logic for base conversion with SIMD.\n    // Due to the complexity of base conversion and string manipulation in SIMD,\n    // the exact implementation would need careful handling of remainders, quotients,\n    // and character storage which is non-trivial without external libraries.\n    return \"\";\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    const int vec_size = 4; // Use 4 elements per vector for SSE2\n    __m128i quotient_vec = _mm_setzero_si128();\n    __m128i remainder_vec = _mm_setzero_si128();\n    __m128i base_vec = _mm_set1_epi32(base);\n    __m128i x_vec = _mm_set1_epi64x(x);\n    std::string result;\n\n    while (!_mm_movemask_epi8(_mm_cmpeq_epi32(quotient_vec, _mm_setzero_si128()))){\n        // Divide by base and store quotient and remainder\n        __m128i div_result = _mm_div_epu32(x_vec, base_vec);\n        quotient_vec = _mm_shuffle_epi32(div_result, _MM_SHUFFLE(0,0,0,0));\n        remainder_vec = _mm_and_si128(x_vec, base_vec);\n\n        // Convert remainders to characters and prepend to result\n        char chars[vec_size];\n        for(int i=0;i<vec_size;i++){\n            char c = '0' + _mm_extract_epi8(remainder_vec, i);\n            chars[vec_size -1 -i] = c;\n        }\n        result.insert(result.begin(), chars, chars + vec_size);\n\n        x_vec = div_result;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_SSE", "completion": "```cpp\nstd::string change_base_simd(int64_t x, int8_t base){\n    // This is a placeholder for the actual implementation.\n    // The task requires implementing the conversion logic using SSE/SSE2 intrinsics.\n    // Due to the complexity of the task and the need for string manipulation,\n    // a direct SIMD implementation of base conversion is non-trivial and may not yield benefits.\n    // Consider breaking down the problem into smaller parts or using scalar operations for clarity.\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\nbool is_palindrome_simd(std::string text){\n    const int len = text.length();\n    if(len <= 1) return true;\n\n    __m128i* ptr = (__m128i*)text.c_str();\n    int i = 0;\n    int remaining = len;\n\n    while(remaining >= 32){\n        __m128i vec = _mm_loadu_si128(ptr + i);\n        __m128i rev_vec = _mm_shuffle_epi8(vec, _mm_setr_epi8(\n            15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0\n        ));\n        if(!_mm_test_all_zeros(_mm_cmpeq_epi8(vec, rev_vec), _mm_set1_epi8(-1))){\n            return false;\n        }\n        i += 4;\n        remaining -= 32;\n    }\n\n    // Handle remaining characters\n    for(int j = len - (remaining % len); j < len; ++j){\n        if(text[j] != text[len - j - 1]){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```c\nbool is_palindrome_simd(std::string text){\n    const char* str = text.c_str();\n    int len = text.length();\n    \n    if(len <= 1) return true;\n    \n    __m128i left, right;\n    __m128i mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1);\n    \n    int i = 0;\n    while(i + 16 <= len){\n        left = _mm_loadu_si128((__m128i*)(str + i));\n        right = _mm_loadu_si128((__m128i*)(str + len - i - 16));\n        \n        left = _mm_or_si128(left, mask);\n        right = _mm_or_si128(right, mask);\n        \n        __m128i cmp = _mm_cmpeq_epi8(left, right);\n        if(!_mm_movemask_epi8(cmp)) return false;\n        \n        i += 16;\n    }\n    \n    for(int j = i; j < len / 2; ++j){\n        if(str[j] != str[len - j - 1]) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_palindrome_simd(std::string text){\n    const int len = text.length();\n    if(len <= 1) return true;\n\n    __m128i forward, backward;\n    __m128i mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1);\n\n    for(int i = 0; i < len / 8; ++i){\n        forward = _mm_loadu_si128((__m128i*)&text[i*8]);\n        backward = _mm_alignr_epi8(forward, forward, 7);\n        forward = _mm_and_si128(forward, mask);\n        backward = _mm_and_si128(backward, mask);\n        if(!_mm_testz_si128(_mm_cmpeq_epi8(forward, backward), mask)){\n            return false;\n        }\n    }\n\n    if(len % 8 != 0){\n        forward = _mm_loadu_si128((__m128i*)&text[len - (len % 8)]);\n        backward = _mm_alignr_epi8(forward, forward, 7);\n        forward = _mm_and_si128(forward, mask);\n        backward = _mm_and_si128(backward, mask);\n        if(!_mm_testz_si128(_mm_cmpeq_epi8(forward, backward), mask)){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```cpp\nbool is_palindrome_simd(std::string text){\n    const int length = text.length();\n    if(length <= 1) return true;\n\n    __m128i* ptr = (__m128i*)text.c_str();\n    int remaining = length & ~3;\n    __m128i forward = _mm_loadu_si128(ptr);\n    __m128i backward = _mm_loadl_epi64((__m128i*)(text.c_str() + length - 4));\n\n    while(remaining > 0){\n        __m128i forward_rev = _mm_shuffle_epi8(forward, _mm_set_epi8(\n            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n        ));\n        if(!_mm_test_all_zeros(_mm_xor_si128(forward_rev, backward), _mm_set1_epi8(-1))){\n            return false;\n        }\n        remaining -= 16;\n        ptr++;\n        forward = _mm_loadu_si128(ptr);\n        backward = _mm_loadl_epi64((__m128i*)(text.c_str() + length - 4 - remaining));\n    }\n\n    const unsigned char* end_ptr = (unsigned char*)ptr;\n    for(int i = remaining; i < length; ++i){\n        if(text[i] != text[length - 1 - i]) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_86_SSE", "completion": "```c\nbool is_palindrome_simd(std::string text){\n    const int length = text.length();\n    if(length <= 1) return true;\n\n    __m128i forward, backward;\n    int i = 0;\n\n    // Align to 16-byte boundary\n    for(; i + 16 <= length; i += 16){\n        forward = _mm_loadu_si128((__m128i*)&text[i]);\n        backward = _mm_loadu_si128((__m128i*)&text[length - i - 16]);\n\n        // Compare each byte in the 16-byte block\n        __m128i cmp = _mm_cmpeq_epi8(forward, backward);\n        if(!_mm_movemask_epi8(cmp)){\n            return false;\n        }\n    }\n\n    // Handle remaining characters that don't fit into 16-byte blocks\n    for(; i < length; ++i){\n        if(text[i] != text[length - i - 1]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m128i base = _mm_set1_epi8('a');\n    const __m128i shift = _mm_set1_epi8(5);\n    const __m128i mod = _mm_set1_epi8(26);\n\n    int len = s.length();\n    std::string out;\n    out.reserve(len);\n\n    for(int i=0; i<len; i+=16){\n        __m128i chars;\n        if(i + 16 <= len){\n            chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        }\n        else{\n            // Handle remaining characters\n            char buffer[16];\n            memset(buffer, ' ', 16);\n            memcpy(buffer, &s[i], len - i);\n            chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(buffer));\n        }\n\n        __m128i decoded = _mm_sub_epi8(chars, shift);\n        decoded = _mm_add_epi8(decoded, _mm_mul_epu32(mod, _mm_srli_epi32(_mm_mullo_epi16(decoded, _mm_set1_epi16(1)), 4)));\n        decoded = _mm_add_epi8(decoded, base);\n\n        char result[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result), decoded);\n\n        out.append(result, 16);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m128i shift = _mm_set1_epi8(5);\n    const __m128i base = _mm_set1_epi8('a');\n    const __m128i mod = _mm_set1_epi8(26);\n\n    std::string out;\n    out.reserve(s.size());\n\n    for(size_t i = 0; i + 16 <= s.size(); i += 16){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i decoded = _mm_sub_epi8(chars, shift);\n        decoded = _mm_add_epi8(decoded, _mm_mul_epu32(mod, _mm_srai_epi32(_mm_mullo_epi16(decoded, _mm_set1_epi16(1)), 4)));\n        __m128i wrapped = _mm_add_epi8(decoded, base);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), wrapped);\n    }\n\n    for(size_t i = out.size(); i < s.size(); ++i){\n        char c = s[i];\n        int w = ((int)c - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out += (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m128i base = _mm_set1_epi8('a');\n    const __m128i shift = _mm_set1_epi8(5);\n    const __m128i mod = _mm_set1_epi8(26);\n\n    int len = s.length();\n    std::string out;\n    out.reserve(len);\n\n    for(int i=0; i<len; i+=16){\n        __m128i chars, shifted, decoded;\n\n        // Load 16 characters into SIMD register\n        chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data()+i));\n\n        // Shift each character back by 5\n        shifted = _mm_sub_epi8(chars, shift);\n\n        // Compute modulo 26\n        decoded = _mm_sub_epi8(shifted, _mm_mul_epu32(_mm_div_epu32(_mm_cvtepu8_epi32(shifted), mod), mod));\n\n        // Add 'a' back to get the decoded character\n        decoded = _mm_add_epi8(decoded, base);\n\n        // Store the result back to memory\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(out.data()+i), decoded);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m128i shift = _mm_set1_epi8(5);\n    const __m128i base = _mm_set1_epi8('a');\n    const __m128i mod = _mm_set1_epi8(26);\n\n    std::string out;\n    out.reserve(s.size());\n\n    for (size_t i = 0; i + 16 <= s.size(); i += 16) {\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i decoded = _mm_sub_epi8(chars, shift);\n        decoded = _mm_add_epi8(decoded, mod);\n        decoded = _mm_sub_epi8(decoded, base);\n        decoded = _mm_add_epi8(decoded, base);\n        decoded = _mm_mod_epu8(decoded, mod);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), decoded);\n    }\n\n    for (size_t i = out.size(); i < s.size(); ++i) {\n        int w = ((int)s[i] - 5 - (int)'a' + 26) % 26 + (int)'a';\n        out.push_back((char)w);\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_87_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nstd::string decode_shift_simd(std::string s){\n    const __m128i base = _mm_set1_epi8('a');\n    const __m128i shift = _mm_set1_epi8(5);\n    const __m128i mod = _mm_set1_epi8(26);\n\n    size_t len = s.length();\n    size_t i = 0;\n\n    // Ensure length is a multiple of 16 for simplicity\n    size_t remainder = len % 16;\n    std::string out(s.begin(), s.end());\n\n    // Process full blocks of 16 characters\n    for (; i <= len - 16; i += 16) {\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i decoded = _mm_sub_epi8(chars, shift);\n        decoded = _mm_add_epi8(decoded, _mm_mul_epu32(_mm_div_epu32(_mm_cvtepu8_epi32(decoded), mod), mod));\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&out[i]), decoded);\n    }\n\n    // Handle remaining characters\n    for (; i < len; i++) {\n        int w = ((int)s[i] - 5 - (int)'a') % 26 + (int)'a';\n        out[i] = (char)w;\n    }\n\n    return out;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m128i threshold = _mm_set1_epi32(t);\n    int size = l.size();\n    if(size == 0) return true;\n    int i = 0;\n    for(; i <= size - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, threshold);\n        if(!_mm_testz_si128(cmp, cmp)) return false;\n    }\n    for(; i < size; ++i){\n        if(l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m128i threshold = _mm_set1_epi32(t);\n    int size = l.size();\n    if(size == 0) return true;\n    int i = 0;\n    for(; i <= size - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, threshold);\n        if(!_mm_testz_si128(cmp, cmp)) return false;\n    }\n    for(; i < size; ++i){\n        if(l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m128i threshold = _mm_set1_epi32(t);\n    int size = l.size();\n    if(size == 0) return true;\n    int i = 0;\n    for(; i <= size - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, threshold);\n        if(!_mm_testz_si128(cmp, cmp)) return false;\n    }\n    for(; i < size; ++i){\n        if(l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m128i threshold = _mm_set1_epi32(t);\n    int size = l.size();\n    if(size == 0) return true;\n    int i = 0;\n    for(; i <= size - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, threshold);\n        if(!_mm_testz_si128(cmp, cmp)) return false;\n    }\n    for(; i < size; ++i){\n        if(l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_88_SSE", "completion": "```cpp\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m128i threshold = _mm_set1_epi32(t);\n    int size = l.size();\n    if(size == 0) return true;\n    int i = 0;\n    for(; i <= size - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&l[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, threshold);\n        if(!_mm_testz_si128(cmp, cmp)) return false;\n    }\n    for(; i < size; ++i){\n        if(l[i] >= t) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets){\n    const size_t len = brackets.size();\n    if(len == 0) return true;\n    \n    __m128i open_mask = _mm_setzero_si128();\n    __m128i close_mask = _mm_setzero_si128();\n    __m128i accum = _mm_setzero_si128();\n    \n    size_t i = 0;\n    for(; i <= len - 4; i += 4){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        \n        __m128i open = _mm_cmpeq_epi8(chars, _mm_set1_epi8('<'));\n        __m128i close = _mm_cmpeq_epi8(chars, _mm_set1_epi8('>'));\n        \n        open_mask = _mm_add_epi8(open_mask, open);\n        close_mask = _mm_add_epi8(close_mask, close);\n        \n        __m128i diff = _mm_sub_epi8(open_mask, close_mask);\n        accum = _mm_or_si128(accum, diff);\n    }\n    \n    // Handle remaining characters\n    for(; i < len; ++i){\n        if(brackets[i] == '<') open_mask += 1;\n        else if(brackets[i] == '>') close_mask += 1;\n    }\n    \n    accum = _mm_add_epi8(accum, _mm_sub_epi8(open_mask, close_mask));\n    \n    __m128i zero = _mm_setzero_si128();\n    int result = _mm_movemask_epi8(_mm_cmpeq_epi8(accum, zero));\n    return result == ((1 << len) - 1);\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE operates on 4 floats, use 1 char per element\n    const int length = brackets.size();\n    __m128i open_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0);\n    __m128i close_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1);\n    __m128i open_count = _mm_setzero_si128();\n    __m128i close_count = _mm_setzero_si128();\n\n    for(int i = 0; i < length; i += vec_size){\n        __m128i chars = _mm_loadu_si128((__m128i*)&brackets[i]);\n        __m128i open_test = _mm_cmpeq_epi8(chars, open_mask);\n        __m128i close_test = _mm_cmpeq_epi8(chars, close_mask);\n        open_count = _mm_add_epi8(open_count, open_test);\n        close_count = _mm_add_epi8(close_count, close_test);\n    }\n\n    __m128i diff = _mm_sub_epi8(open_count, close_count);\n    __m128i all_zero = _mm_cmpeq_epi8(diff, _mm_setzero_si128());\n    int mask = _mm_movemask_epi8(all_zero);\n    return mask == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE operates on 4 floats or 4 integers\n    __m128i open_mask = _mm_set_epi32(0x3c3c3c3c, 0x3c3c3c3c, 0x3c3c3c3c, 0x3c3c3c3c); // '<' in each lane\n    __m128i close_mask = _mm_set_epi32(0x3e3e3e3e, 0x3e3e3e3e, 0x3e3e3e3e, 0x3e3e3e3e); // '>' in each lane\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i balance = zero_vec;\n    \n    for(size_t i = 0; i + vec_size <= brackets.size(); i += vec_size){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        \n        __m128i open_match = _mm_cmpeq_epi8(chars, open_mask);\n        __m128i close_match = _mm_cmpeq_epi8(chars, close_mask);\n        \n        balance = _mm_add_epi8(balance, _mm_sub_epi8(open_match, close_match));\n        \n        // Check for overflow/underflow\n        __m128i negative_check = _mm_cmpgt_epi8(zero_vec, balance);\n        __m128i positive_check = _mm_cmpgt_epi8(balance, zero_vec);\n        if(!_mm_movemask_epi8(_mm_andnot_si128(negative_check, positive_check))){\n            return false;\n        }\n    }\n    \n    // Handle remaining characters\n    for(size_t i = brackets.size() - (brackets.size() % vec_size); i < brackets.size(); ++i){\n        if(brackets[i] == '<') balance = _mm_add_epi8(balance, _mm_set1_epi8(1));\n        else if(brackets[i] == '>') balance = _mm_sub_epi8(balance, _mm_set1_epi8(1));\n    }\n    \n    int final_balance = 0;\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&final_balance), balance);\n    \n    return final_balance == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE operates on 4 floats or 4 integers at once\n    const int length = brackets.size();\n    __m128i open_mask = _mm_set_epi32(0x3c3c3c3c, 0x3c3c3c3c, 0x3c3c3c3c, 0x3c3c3c3c); // '<' in each lane\n    __m128i close_mask = _mm_set_epi32(0x3e3e3e3e, 0x3e3e3e3e, 0x3e3e3e3e, 0x3e3e3e3e); // '>' in each lane\n    __m128i balance = _mm_setzero_si128();\n\n    for(int i = 0; i < length; i += vec_size){\n        __m128i chars;\n        if(i + vec_size <= length){\n            chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        }\n        else{\n            // Handle remaining characters\n            int remaining = length - i;\n            chars = _mm_setzero_si128();\n            for(int j = 0; j < remaining; ++j){\n                ((char*)&chars)[j] = brackets[i + j];\n            }\n        }\n\n        __m128i open = _mm_cmpeq_epi8(chars, open_mask);\n        __m128i close = _mm_cmpeq_epi8(chars, close_mask);\n\n        balance = _mm_add_epi32(balance, _mm_cvtepi8_epi32(open));\n        balance = _mm_sub_epi32(balance, _mm_cvtepi8_epi32(close));\n\n        // Check for overflow\n        __m128i overflow = _mm_cmpgt_epi32(_mm_setzero_si128(), balance);\n        if(!_mm_testz_si128(overflow, overflow)){\n            return false;\n        }\n    }\n\n    __m128i final_balance = _mm_hadd_epi32(balance, balance);\n    final_balance = _mm_hadd_epi32(final_balance, final_balance);\n    int result = _mm_extract_epi32(final_balance, 0);\n\n    return result == 0;\n}\n```"}
{"task_id": "SimdBench_89_SSE", "completion": "```cpp\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE operates on 4 floats or 4 integers\n    const int length = brackets.size();\n    __m128i open_mask = _mm_set_epi32(0x3c3c3c3c, 0x3c3c3c3c, 0x3c3c3c3c, 0x3c3c3c3c); // '<'<\n    __m128i close_mask = _mm_set_epi32(0x3e3e3e3e, 0x3e3e3e3e, 0x3e3e3e3e, 0x3e3e3e3e); // '>'>\n    __m128i balance = _mm_setzero_si128();\n    \n    for(int i = 0; i < length; i += vec_size){\n        __m128i chars;\n        if(i + vec_size <= length){\n            chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        }\n        else{\n            // Handle remaining characters\n            int remaining = length - i;\n            chars = _mm_setzero_si128();\n            for(int j = 0; j < remaining; ++j){\n                ((char*)&chars)[j] = brackets[i + j];\n            }\n        }\n        \n        __m128i open = _mm_cmpeq_epi8(chars, open_mask);\n        __m128i close = _mm_cmpeq_epi8(chars, close_mask);\n        \n        balance = _mm_add_epi32(balance, _mm_cvtepi8_epi32(open));\n        balance = _mm_sub_epi32(balance, _mm_cvtepi8_epi32(close));\n    }\n    \n    int result[4];\n    _mm_storeu_si128((__m128i*)result, balance);\n    for(int i = 0; i < 4; ++i){\n        if(result[i] != 0){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE processes 4 characters at a time\n    const __m128i open_mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1);\n    const __m128i close_mask = _mm_set_epi8(-1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i open_paren = _mm_set1_epi8('(');\n    const __m128i close_paren = _mm_set1_epi8(')');\n    \n    int len = brackets.size();\n    if(len == 0) return true;\n    \n    int i = 0;\n    __m128i accum = _mm_setzero_si128();\n    \n    for(i = 0; i + vec_size <= len; i += vec_size){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        \n        __m128i opens = _mm_cmpeq_epi8(_mm_and_si128(chars, open_mask), open_paren);\n        __m128i closes = _mm_cmpeq_epi8(_mm_and_si128(chars, close_mask), close_paren);\n        \n        accum = _mm_add_epi32(accum, _mm_sub_epi32(opens, closes));\n    }\n    \n    int remaining = len - i;\n    if(remaining > 0){\n        __m128i last_chars = _mm_setzero_si128();\n        for(int j = 0; j < remaining; ++j){\n            last_chars = _mm_insert_epi8(last_chars, brackets[i + j], j);\n        }\n        \n        __m128i opens = _mm_cmpeq_epi8(_mm_and_si128(last_chars, open_mask), open_paren);\n        __m128i closes = _mm_cmpeq_epi8(_mm_and_si128(last_chars, close_mask), close_paren);\n        \n        accum = _mm_add_epi32(accum, _mm_sub_epi32(opens, closes));\n    }\n    \n    __m128i zero_check = _mm_cmpeq_epi32(accum, _mm_setzero_si128());\n    int mask = _mm_movemask_epi8(zero_check);\n    return mask == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const char* str = brackets.c_str();\n    int len = brackets.length();\n    \n    __m128i open_mask = _mm_set1_epi8('(');\n    __m128i close_mask = _mm_set1_epi8(')');\n    __m128i acc = _mm_setzero_si128();\n    \n    for(int i=0; i<len; i+=16){\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i));\n        \n        __m128i open_cmp = _mm_cmpeq_epi8(chars, open_mask);\n        __m128i close_cmp = _mm_cmpeq_epi8(chars, close_mask);\n        \n        acc = _mm_add_epi8(acc, _mm_sub_epi8(open_cmp, close_cmp));\n        \n        // Check overflow/underflow\n        __m128i overflow = _mm_cmpgt_epi8(acc, _mm_setzero_si128());\n        __m128i underflow = _mm_cmplt_epi8(acc, _mm_setzero_si128());\n        \n        if(_mm_movemask_epi8(overflow) || _mm_movemask_epi8(underflow)){\n            return false;\n        }\n    }\n    \n    return _mm_testz_si128(acc, acc);\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE operates on 4 floats or 4 integers of 32 bits each\n    const __m128i open_bracket = _mm_set1_epi8('(');\n    const __m128i close_bracket = _mm_set1_epi8(')');\n    const __m128i mask = _mm_set1_epi8(0x80); // Mask for sign bit\n    const __m128i zero = _mm_setzero_si128();\n\n    int len = brackets.size();\n    if(len == 0) return true;\n\n    int i = 0;\n    __m128i accum = zero;\n\n    while(i + vec_size <= len){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open_mask = _mm_cmpeq_epi8(chunk, open_bracket);\n        __m128i close_mask = _mm_cmpeq_epi8(chunk, close_bracket);\n\n        __m128i open_sign = _mm_srai_epi32(_mm_madd_epi16(open_mask, mask), 31);\n        __m128i close_sign = _mm_srai_epi32(_mm_madd_epi16(close_mask, mask), 31);\n\n        accum = _mm_add_epi32(accum, _mm_sub_epi32(open_sign, close_sign));\n        i += vec_size;\n    }\n\n    int remaining = len - i;\n    if(remaining > 0){\n        __m128i last_chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        __m128i open_mask = _mm_cmpeq_epi8(last_chunk, open_bracket);\n        __m128i close_mask = _mm_cmpeq_epi8(last_chunk, close_bracket);\n\n        __m128i open_sign = _mm_srai_epi32(_mm_madd_epi16(open_mask, mask), 31);\n        __m128i close_sign = _mm_srai_epi32(_mm_madd_epi16(close_mask, mask), 31);\n\n        accum = _mm_add_epi32(accum, _mm_sub_epi32(open_sign, close_sign));\n    }\n\n    __m128i final_check = _mm_cmpeq_epi32(accum, zero);\n    return _mm_movemask_epi8(final_check) == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE operates on 4 floats or 4 integers of 32 bits each\n    __m128i open_mask = _mm_set_epi32(0x29292929, 0x29292929, 0x29292929, 0x29292929); // ')')')')\n    __m128i close_mask = _mm_set_epi32(0x28282828, 0x28282828, 0x28282828, 0x28282828); // '(''(''(''(')\n    __m128i balance = _mm_setzero_si128();\n    int len = brackets.length();\n\n    for(int i=0; i<len; i+=vec_size){\n        __m128i chars;\n        if(i + vec_size <= len){\n            chars = _mm_loadu_si128((__m128i*)&brackets[i]);\n        }\n        else{\n            // Handle remaining characters less than vec_size\n            chars = _mm_setzero_si128();\n            for(int j=i; j<len; ++j){\n                ((char*)&chars)[j-i] = brackets[j];\n            }\n        }\n\n        __m128i open = _mm_cmpeq_epi32(chars, open_mask);\n        __m128i close = _mm_cmpeq_epi32(chars, close_mask);\n\n        balance = _mm_add_epi32(balance, _mm_sub_epi32(close, open));\n    }\n\n    __m128i zero = _mm_setzero_si128();\n    int mask = _mm_movemask_epi8(_mm_cmpeq_epi32(balance, zero));\n    return mask == 0xFFFF;\n}\n```"}
{"task_id": "SimdBench_90_SSE", "completion": "```c\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int vec_size = 4; // SSE operates on 4 floats or 4 integers at once\n    const int vec_bytes = vec_size * sizeof(int);\n    \n    if (brackets.empty()) return true;\n    \n    __m128i open_mask = _mm_set_epi32(0x20000000, 0x20000000, 0x20000000, 0x20000000);\n    __m128i close_mask = _mm_set_epi32(0x40000000, 0x40000000, 0x40000000, 0x40000000);\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i balance = zero_vec;\n    \n    for(size_t i = 0; i + vec_size <= brackets.size(); i += vec_size){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&brackets[i]));\n        \n        __m128i open_match = _mm_cmpeq_epi32(chunk, open_mask);\n        __m128i close_match = _mm_cmpeq_epi32(chunk, close_mask);\n        \n        balance = _mm_add_epi32(balance, _mm_sub_epi32(open_match, close_match));\n    }\n    \n    int remaining[vec_size];\n    _mm_storeu_si128((__m128i*)remaining, balance);\n    \n    for(int j = 0; j < vec_size; ++j){\n        if(remaining[j] != 0) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int size = xs.size();\n    if(size <=1){\n        return std::vector<float>();\n    }\n    \n    int aligned_size = size -1;\n    aligned_size &= ~1; // Round down to nearest multiple of 2\n\n    std::vector<float> result(aligned_size);\n    __m128* res_ptr = reinterpret_cast<__m128*>(&result[0]);\n    __m128* xs_ptr = reinterpret_cast<__m128*>(&xs[1]);\n\n    for(int i=0;i<aligned_size;i+=2){\n        __m128 coeff0 = _mm_loadu_ps(&xs_ptr[i].m128_f32[0]);\n        __m128 coeff1 = _mm_loadu_ps(&xs_ptr[i+1].m128_f32[0]);\n        __m128 mul0 = _mm_set1_ps(1.0f);\n        __m128 mul1 = _mm_set1_ps(2.0f);\n        \n        __m128 der0 = _mm_mul_ps(coeff0, mul0);\n        __m128 der1 = _mm_mul_ps(coeff1, mul1);\n        \n        _mm_storeu_ps(&res_ptr[i].m128_f32[0], der0);\n        _mm_storeu_ps(&res_ptr[i+1].m128_f32[0], der1);\n    }\n\n    // Handle remaining elements if any\n    for(int i=aligned_size;i<size-1;i++){\n        result[i] = xs[i+1] * (i+1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    if(n <= 1){\n        return std::vector<float>();\n    }\n    \n    std::vector<float> result;\n    result.reserve(n - 1);\n    \n    __m128* coeffs = (__m128*)&xs[0];\n    __m128* res = (__m128*)&result[0];\n    int numFullVectors = (n - 1) / 2;\n\n    for(int i = 0; i < numFullVectors; ++i){\n        __m128 indices = _mm_setr_ps(1.0f, 2.0f, 3.0f, 4.0f);\n        __m128 multiplied = _mm_mul_ps(*coeffs, indices);\n        *res = multiplied;\n        coeffs++;\n        res++;\n    }\n\n    int remainder = (n - 1) % 2;\n    if(remainder == 1){\n        float lastValue = xs[n - 2] * (n - 1);\n        result.push_back(lastValue);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    if(n <= 1){\n        return std::vector<float>();\n    }\n    \n    int aligned_size = (n - 1) & ~1; // Align to 2 elements\n    std::vector<float> result(aligned_size);\n    \n    __m128* coeffs = reinterpret_cast<__m128*>(&xs[0]);\n    __m128* res = reinterpret_cast<__m128*>(&result[0]);\n    \n    for(int i = 0; i < aligned_size; i += 2){\n        __m128 coeff = _mm_loadu_ps(coeffs + i);\n        __m128 index = _mm_set1_ps(static_cast<float>(i + 1));\n        __m128 der = _mm_mul_ps(coeff, index);\n        _mm_storeu_ps(res + i, der);\n    }\n    \n    // Handle remaining element if size is odd\n    if((n - 1) % 2 != 0){\n        float last_coeff = static_cast<float>(n - 1) * xs[n - 1];\n        result[n - 2] = last_coeff;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int size = xs.size();\n    if(size <=1){\n        return std::vector<float>();\n    }\n    \n    int newSize = size -1;\n    std::vector<float> result(newSize);\n    \n    __m128* input = reinterpret_cast<__m128*>(&xs[0]);\n    __m128* output = reinterpret_cast<__m128*>(&result[0]);\n    \n    int i = 0;\n    for(; i < newSize/4; ++i){\n        __m128 vec = _mm_loadu_ps(input + i);\n        __m128 indices = _mm_setr_ps(1.0f, 2.0f, 3.0f, 4.0f);\n        __m128 der = _mm_mul_ps(vec, indices);\n        _mm_storeu_ps(output + i, der);\n    }\n    \n    for(int j = i*4; j < newSize; ++j){\n        result[j] = xs[j+1] * (j+1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int size = xs.size();\n    if(size <=1){\n        return std::vector<float>();\n    }\n    \n    int newSize = size -1;\n    std::vector<float> result(newSize);\n    \n    __m128* input = reinterpret_cast<__m128*>(&xs[0]);\n    __m128* output = reinterpret_cast<__m128*>(&result[0]);\n    \n    int i = 0;\n    for(; i < newSize/4; ++i){\n        __m128 vec = _mm_loadu_ps(input + i);\n        __m128 indices = _mm_setr_ps(1.0f, 2.0f, 3.0f, 4.0f);\n        __m128 mul = _mm_mul_ps(vec, indices);\n        _mm_storeu_ps(output + i, mul);\n    }\n    \n    for(int j = i*4; j < newSize; ++j){\n        result[j] = xs[j+1] * (j+1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(std::string s){\n    const __m128i mask_a = _mm_set1_epi8('a' | 0x20);\n    const __m128i mask_e = _mm_set1_epi8('e' | 0x20);\n    const __m128i mask_i = _mm_set1_epi8('i' | 0x20);\n    const __m128i mask_o = _mm_set1_epi8('o' | 0x20);\n    const __m128i mask_u = _mm_set1_epi8('u' | 0x20);\n    const __m128i mask_y = _mm_set1_epi8('y' | 0x20);\n\n    int count = 0;\n    size_t i = 0;\n\n    while(i + 16 <= s.size()){\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_data = _mm_or_si128(data, _mm_set1_epi8(0x20));\n        \n        __m128i match_a = _mm_cmpeq_epi8(lower_data, mask_a);\n        __m128i match_e = _mm_cmpeq_epi8(lower_data, mask_e);\n        __m128i match_i = _mm_cmpeq_epi8(lower_data, mask_i);\n        __m128i match_o = _mm_cmpeq_epi8(lower_data, mask_o);\n        __m128i match_u = _mm_cmpeq_epi8(lower_data, mask_u);\n        __m128i match_y = _mm_cmpeq_epi8(_mm_srli_si128(lower_data, 1), mask_y);\n\n        __m128i total_match = _mm_or_si128(match_a, match_e);\n        total_match = _mm_or_si128(total_match, match_i);\n        total_match = _mm_or_si128(total_match, match_o);\n        total_match = _mm_or_si128(total_match, match_u);\n        total_match = _mm_or_si128(total_match, match_y);\n\n        count += _mm_popcnt_u32(_mm_movemask_epi8(total_match));\n        i += 16;\n    }\n\n    for(; i < s.size(); ++i){\n        char c = s[i];\n        if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u'){\n            count++;\n        }\n        else if(c == 'y' && i == s.size() - 1){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(std::string s){\n    const __m128i mask_a = _mm_set1_epi8('a' | 0x20);\n    const __m128i mask_e = _mm_set1_epi8('e' | 0x20);\n    const __m128i mask_i = _mm_set1_epi8('i' | 0x20);\n    const __m128i mask_o = _mm_set1_epi8('o' | 0x20);\n    const __m128i mask_u = _mm_set1_epi8('u' | 0x20);\n    const __m128i mask_y = _mm_set1_epi8('y' | 0x20);\n    \n    int count = 0;\n    int i = 0;\n    const char* data = s.c_str();\n    \n    while(i + 16 <= s.size()){\n        __m128i vec = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i lower_vec = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(\n            _mm_cmpeq_epi8(_mm_and_si128(vec, mask_a), mask_a),\n            _mm_cmpeq_epi8(_mm_and_si128(vec, mask_e), mask_e)),\n            _mm_cmpeq_epi8(_mm_and_si128(vec, mask_i), mask_i)),\n            _mm_cmpeq_epi8(_mm_and_si128(vec, mask_o), mask_o)),\n            _mm_cmpeq_epi8(_mm_and_si128(vec, mask_u), mask_u));\n        \n        __m128i y_mask = _mm_cmpeq_epi8(_mm_and_si128(_mm_srli_epi16(vec, 8), mask_y), mask_y);\n        __m128i y_test = _mm_cmpeq_epi8(_mm_srli_epi16(vec, 8), _mm_setzero_si128());\n        y_mask = _mm_andnot_si128(y_test, y_mask);\n        \n        lower_vec = _mm_or_si128(lower_vec, y_mask);\n        \n        count += _mm_movemask_epi8(lower_vec);\n        i += 16;\n    }\n    \n    for(; i < s.size(); ++i){\n        char c = data[i] | 0x20;\n        if(c >= 'a' && c <= 'z'){\n            if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u'){\n                count++;\n            }\n            else if(c == 'y' && (i == s.size() - 1)){\n                count++;\n            }\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(std::string s){\n    const __m128i mask_a = _mm_set1_epi8('a' | 0x20);\n    const __m128i mask_e = _mm_set1_epi8('e' | 0x20);\n    const __m128i mask_i = _mm_set1_epi8('i' | 0x20);\n    const __m128i mask_o = _mm_set1_epi8('o' | 0x20);\n    const __m128i mask_u = _mm_set1_epi8('u' | 0x20);\n    const __m128i mask_y = _mm_set1_epi8('y' | 0x20);\n    \n    int count = 0;\n    int i = 0;\n    const char* data = s.c_str();\n    \n    while(i + 16 <= s.size()){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        __m128i lower = _mm_or_si128(_mm_or_si128(_mm_or_si128(_mm_or_si128(\n            _mm_cmpeq_epi8(_mm_and_si128(chars, mask_a), mask_a),\n            _mm_cmpeq_epi8(_mm_and_si128(chars, mask_e), mask_e)),\n            _mm_cmpeq_epi8(_mm_and_si128(chars, mask_i), mask_i)),\n            _mm_cmpeq_epi8(_mm_and_si128(chars, mask_o), mask_o)),\n            _mm_cmpeq_epi8(_mm_and_si128(chars, mask_u), mask_u));\n        \n        __m128i y_check = _mm_cmpeq_epi8(_mm_and_si128(chars, mask_y), mask_y);\n        __m128i last_char_mask = _mm_cmpeq_epi8(_mm_setzero_si128(), _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1));\n        y_check = _mm_and_si128(y_check, last_char_mask);\n        \n        lower = _mm_or_si128(lower, y_check);\n        \n        count += _mm_popcnt_u32(_mm_movemask_epi8(lower));\n        i += 16;\n    }\n    \n    for(; i < s.size(); ++i){\n        if(data[i] == 'a' || data[i] == 'e' || data[i] == 'i' || data[i] == 'o' || data[i] == 'u'){\n            count++;\n        }\n        else if(data[i] == 'y' && i == s.size() - 1){\n            count++;\n        }\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(std::string s){\n    const __m128i mask_a = _mm_set1_epi8('a' | 0x20);\n    const __m128i mask_e = _mm_set1_epi8('e' | 0x20);\n    const __m128i mask_i = _mm_set1_epi8('i' | 0x20);\n    const __m128i mask_o = _mm_set1_epi8('o' | 0x20);\n    const __m128i mask_u = _mm_set1_epi8('u' | 0x20);\n    const __m128i mask_y = _mm_set1_epi8('y' | 0x20);\n\n    int count = 0;\n    int i = 0;\n\n    while(i + 16 <= s.size()){\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_data = _mm_or_si128(data, _mm_set1_epi8(0x20));\n        \n        __m128i match_a = _mm_cmpeq_epi8(lower_data, mask_a);\n        __m128i match_e = _mm_cmpeq_epi8(lower_data, mask_e);\n        __m128i match_i = _mm_cmpeq_epi8(lower_data, mask_i);\n        __m128i match_o = _mm_cmpeq_epi8(lower_data, mask_o);\n        __m128i match_u = _mm_cmpeq_epi8(lower_data, mask_u);\n        __m128i match_y = _mm_cmpeq_epi8(_mm_srli_epi16(lower_data, 1), mask_y);\n\n        __m128i total_match = _mm_or_si128(match_a, match_e);\n        total_match = _mm_or_si128(total_match, match_i);\n        total_match = _mm_or_si128(total_match, match_o);\n        total_match = _mm_or_si128(total_match, match_u);\n        total_match = _mm_or_si128(total_match, match_y);\n\n        count += _mm_popcnt_u32(_mm_movemask_epi8(total_match));\n        i += 16;\n    }\n\n    for(; i < s.size(); ++i){\n        char c = s[i];\n        if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u'){\n            count++;\n        }\n        else if(c == 'y' && i == s.size() - 1){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_SSE", "completion": "```cpp\nint vowels_count_simd(std::string s){\n    const __m128i mask_a = _mm_set1_epi8('a' | 0x20);\n    const __m128i mask_e = _mm_set1_epi8('e' | 0x20);\n    const __m128i mask_i = _mm_set1_epi8('i' | 0x20);\n    const __m128i mask_o = _mm_set1_epi8('o' | 0x20);\n    const __m128i mask_u = _mm_set1_epi8('u' | 0x20);\n    const __m128i mask_y = _mm_set1_epi8('y' | 0x20);\n\n    int count = 0;\n    int i = 0;\n\n    while(i + 16 <= s.size()){\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n        __m128i lower_data = _mm_or_si128(_mm_and_si128(data, _mm_set1_epi8(0xDF)), _mm_set1_epi8(0x20));\n        \n        __m128i eq_a = _mm_cmpeq_epi8(lower_data, mask_a);\n        __m128i eq_e = _mm_cmpeq_epi8(lower_data, mask_e);\n        __m128i eq_i = _mm_cmpeq_epi8(lower_data, mask_i);\n        __m128i eq_o = _mm_cmpeq_epi8(lower_data, mask_o);\n        __m128i eq_u = _mm_cmpeq_epi8(lower_data, mask_u);\n        __m128i eq_y = _mm_cmpeq_epi8(_mm_sub_epi8(lower_data, _mm_set1_epi8(1)), mask_y);\n\n        __m128i mask_end_y = _mm_cmpeq_epi8(_mm_setzero_si128(), _mm_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1));\n\n        __m128i total = _mm_or_si128(eq_a, _mm_or_si128(eq_e, _mm_or_si128(eq_i, _mm_or_si128(eq_o, _mm_or_si128(eq_u, eq_y)))));\n        total = _mm_and_si128(total, mask_end_y);\n\n        count += _mm_popcnt_epi8(total).m128i_u32[0];\n        i += 16;\n    }\n\n    for(int j = i; j < s.size(); ++j){\n        if(s[j] == 'a' || s[j] == 'e' || s[j] == 'i' || s[j] == 'o' || s[j] == 'u'){\n            count++;\n        }\n        else if(s[j] == 'y'){\n            if(j == s.size() - 1){\n                count++;\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```c\nint digitSum_simd(std::string s){\n    const char* str = s.c_str();\n    int sum = 0;\n    __m128i upper_mask = _mm_set1_epi8(0x20); // Mask to check for uppercase letters\n    __m128i zero_check = _mm_setzero_si128();  // Used to check if all characters are zero\n    __m128i acc = _mm_setzero_si128();         // Accumulator for sums\n\n    while(*str != '\\0'){\n        __m128i chars = _mm_loadu_si128((__m128i*)str); // Load 16 bytes from string\n        __m128i upper = _mm_andnot_si128(upper_mask, chars); // Keep only uppercase letters\n        __m128i valid = _mm_cmpeq_epi8(chars, zero_check);   // Check for end of string\n        __m128i mask = _mm_movemask_epi8(valid);             // Create a mask for valid characters\n        if(mask == 0xFFFF){\n            break; // End of string reached\n        }\n        acc = _mm_add_epi8(acc, upper);                      // Add ASCII values of uppercase letters\n        str += 16 - __builtin_popcount(mask);                // Move pointer forward\n    }\n\n    // Horizontal sum of the accumulator\n    __m128i shuffle1 = _mm_shuffle_epi32(acc, _MM_SHUFFLE(0,0,3,2));\n    __m128i sum1 = _mm_add_epi16(acc, shuffle1);\n    __m128i shuffle2 = _mm_shufflehi_epi16(sum1, _MM_SHUFFLE(0,0,3,2));\n    __m128i sum2 = _mm_add_epi16(sum1, shuffle2);\n    __m128i shuffle3 = _mm_shufflelo_epi16(sum2, _MM_SHUFFLE(0,0,3,2));\n    __m128i sum3 = _mm_add_epi16(sum2, shuffle3);\n    sum = _mm_cvtsi128_si32(sum3);\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```c\nint digitSum_simd(std::string s){\n    const int vec_size = 4;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i upper_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n    \n    for(int i = 0; i < s.size(); i += vec_size){\n        __m128i chars_vec = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i upper_chars_vec = _mm_or_si128(\n            _mm_cmpgt_epi8(chars_vec, _mm_set1_epi8('A')),\n            _mm_cmpgt_epi8(_mm_set1_epi8('Z'), chars_vec)\n        );\n        __m128i ascii_vec = _mm_sub_epi8(chars_vec, _mm_set1_epi8('A'));\n        __m128i upper_ascii_vec = _mm_and_si128(ascii_vec, upper_mask);\n        sum_vec = _mm_add_epi8(sum_vec, upper_ascii_vec);\n    }\n    \n    int sum = 0;\n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum_vec);\n    for(int i = 0; i < vec_size; ++i){\n        sum += temp[i];\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```c\nint digitSum_simd(std::string s){\n    const int vec_size = 4;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i upper_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n    \n    for(int i = 0; i < s.size(); i += vec_size){\n        __m128i chars_vec = _mm_loadu_si128((__m128i*)&s[i]);\n        __m128i upper_chars_vec = _mm_and_si128(chars_vec, upper_mask);\n        __m128i is_upper_vec = _mm_cmpgt_epi8(_mm_sub_epi8(chars_vec, _mm_set1_epi8('A')), _mm_setzero_si128());\n        is_upper_vec = _mm_andnot_si128(is_upper_vec, _mm_set1_epi8(0xFF));\n        is_upper_vec = _mm_or_si128(is_upper_vec, _mm_set1_epi8(0x20));\n        __m128i ascii_vec = _mm_add_epi8(chars_vec, _mm_set1_epi8(-'A'));\n        ascii_vec = _mm_and_si128(ascii_vec, is_upper_vec);\n        sum_vec = _mm_add_epi32(sum_vec, _mm_cvtepu8_epi32(ascii_vec));\n    }\n    \n    int sum = 0;\n    sum += _mm_extract_epi32(sum_vec, 0);\n    sum += _mm_extract_epi32(sum_vec, 1);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```c\nint digitSum_simd(std::string s){\n    const char* str = s.c_str();\n    int len = s.length();\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i=0; i<len; i+=16){\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i upper_mask = _mm_cmpgt_epi8(_mm_set1_epi8('_'), chars);\n        __m128i upper_chars = _mm_and_si128(chars, upper_mask);\n        __m128i ascii_sum = _mm_sub_epi8(upper_chars, _mm_set1_epi8('A' - 1));\n        sum_vec = _mm_add_epi8(sum_vec, ascii_sum);\n    }\n    \n    __m128i horizontal_sum = _mm_setzero_si128();\n    horizontal_sum = _mm_add_epi8(horizontal_sum, sum_vec);\n    horizontal_sum = _mm_add_epi8(horizontal_sum, _mm_srli_si128(sum_vec, 8));\n    horizontal_sum = _mm_add_epi8(horizontal_sum, _mm_srli_si128(sum_vec, 4));\n    horizontal_sum = _mm_add_epi8(horizontal_sum, _mm_srli_si128(sum_vec, 2));\n    horizontal_sum = _mm_add_epi8(horizontal_sum, _mm_srli_si128(sum_vec, 1));\n    \n    return _mm_cvtsi128_si32(horizontal_sum);\n}\n```"}
{"task_id": "SimdBench_93_SSE", "completion": "```c\nint digitSum_simd(std::string s){\n    const char* str = s.c_str();\n    int len = s.length();\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i=0; i<len; i+=16){\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i));\n        __m128i upper_mask = _mm_cmpgt_epi8(_mm_set1_epi8('_'), chars);\n        __m128i lower_mask = _mm_cmpgt_epi8(chars, _mm_set1_epi8('`'));\n        __m128i alpha_mask = _mm_and_si128(upper_mask, lower_mask);\n        __m128i ascii_vals = _mm_add_epi8(chars, _mm_set1_epi8(-'A' + 'a' - 1));\n        __m128i upper_chars = _mm_sub_epi8(chars, _mm_andnot_si128(alpha_mask, _mm_set1_epi8('A' - 1)));\n        __m128i upper_ascii = _mm_add_epi8(upper_chars, _mm_set1_epi8(-'A' + 'a' - 1));\n        sum_vec = _mm_add_epi32(sum_vec, _mm_sad_epu8(upper_ascii, _mm_setzero_si128()));\n    }\n    \n    int sum = _mm_cvtsi128_si32(sum_vec);\n    __m128i shuffle = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(3,2,1,0));\n    sum += _mm_cvtsi128_si32(shuffle);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    const int simd_width = 4;\n    __m128i min_even = _mm_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    __m128i min_index = _mm_set1_epi32(-1);        // Initialize to invalid index\n    __m128i mask = _mm_set1_epi32(1);              // Mask for checking evenness\n\n    if(arr.size() < simd_width) {\n        // Handle small arrays manually\n        int local_min = -1;\n        int local_index = -1;\n        for(int i = 0; i < arr.size(); ++i){\n            if(arr[i] % 2 == 0 && (local_min == -1 || arr[i] < local_min || (arr[i] == local_min && i < local_index))){\n                local_min = arr[i];\n                local_index = i;\n            }\n        }\n        if(local_min != -1){\n            return {local_min, local_index};\n        }\n        return {};\n    }\n\n    for(int i = 0; i + simd_width <= arr.size(); i += simd_width){\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(data, mask), _mm_setzero_si128());\n        __m128i valid_data = _mm_blendv_epi8(_mm_setzero_si128(), data, even_mask);\n        \n        __m128i cmp_min = _mm_min_epu32(min_even, valid_data);\n        __m128i cmp_index = _mm_add_epi32(min_index, _mm_cmpgt_epi32(cmp_min, min_even));\n        cmp_index = _mm_sub_epi32(cmp_index, _mm_cmpgt_epi32(valid_data, cmp_min));\n\n        min_even = cmp_min;\n        min_index = cmp_index;\n    }\n\n    // Gather results from min_even and min_index\n    std::vector<int> result;\n    for(int i = 0; i < simd_width; ++i){\n        int val = _mm_extract_epi32(min_even, i);\n        int idx = _mm_extract_epi32(min_index, i);\n        if(val < _mm_extract_epi32(min_even, i+1)){\n            result = {val, idx};\n            break;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    const int simd_width = 4;\n    __m128i min_even_values = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i min_even_indices = _mm_set1_epi32(-1);\n    \n    if(arr.size() < simd_width) {\n        // Handle small arrays without SIMD\n        int min_even = -1;\n        int min_index = -1;\n        for(int i = 0; i < arr.size(); ++i){\n            if(arr[i] % 2 == 0){\n                if(min_even == -1 || arr[i] < min_even || (arr[i] == min_even && i < min_index)){\n                    min_even = arr[i];\n                    min_index = i;\n                }\n            }\n        }\n        if(min_even != -1){\n            return {min_even, min_index};\n        }\n        return {};\n    }\n\n    for(int i = 0; i + simd_width <= arr.size(); i += simd_width){\n        __m128i values = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i evens = _mm_and_si128(values, _mm_set1_epi32(1));\n        \n        // Mask out odd numbers\n        __m128i valid_mask = _mm_cmpeq_epi32(evens, _mm_setzero_si128());\n        __m128i valid_values = _mm_blendv_epi8(_mm_setzero_si128(), values, valid_mask);\n        __m128i valid_indices = _mm_blendv_epi8(_mm_setzero_si128(), indices, valid_mask);\n\n        // Compare with current minimums\n        __m128i compare_mask = _mm_cmpgt_epi32(valid_values, min_even_values);\n        __m128i new_min_values = _mm_blendv_epi8(min_even_values, valid_values, compare_mask);\n        __m128i new_min_indices = _mm_blendv_epi8(min_even_indices, valid_indices, compare_mask);\n\n        // Update min_even_values and min_even_indices\n        min_even_values = new_min_values;\n        min_even_indices = new_min_indices;\n    }\n\n    // Gather results from min_even_values and min_even_indices\n    std::vector<int> result;\n    for(int i = 0; i < simd_width; ++i){\n        if(min_even_indices.m128i_i32[i] != -1){\n            result.push_back(min_even_values.m128i_i32[i]);\n            result.push_back(min_even_indices.m128i_i32[i]);\n            break;\n        }\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m128i min_even = _mm_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    __m128i min_index = _mm_set1_epi32(-1);        // Initialize to invalid index\n    __m128i mask_even = _mm_set1_epi32(1);         // Mask to check odd/even\n\n    size_t i = 0;\n    while(i + 4 <= arr.size()){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i evens = _mm_and_si128(vec, mask_even);\n        __m128i is_even = _mm_cmpeq_epi32(evens, _mm_setzero_si128());\n\n        __m128i current_min = _mm_shuffle_epi32(vec, _MM_SHUFFLE(0,0,0,0));\n        __m128i current_index = _mm_set1_epi32((int)i);\n\n        current_min = _mm_and_si128(current_min, is_even);\n        current_index = _mm_and_si128(current_index, is_even);\n\n        __m128i compare_min = _mm_cmpgt_epi32(min_even, current_min);\n        min_even = _mm_blendv_epi8(min_even, current_min, compare_min);\n        min_index = _mm_blendv_epi8(min_index, current_index, compare_min);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for(; i < arr.size(); ++i){\n        if(arr[i] % 2 == 0 && arr[i] < min_even.m128i_i32[0]){\n            min_even.m128i_i32[0] = arr[i];\n            min_index.m128i_i32[0] = i;\n        }\n    }\n\n    if(min_index.m128i_i32[0] == -1) return {};\n    return {min_even.m128i_i32[0], min_index.m128i_i32[0]};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m128i min_even = _mm_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    __m128i min_index = _mm_set1_epi32(-1);        // Initialize to invalid index\n    __m128i mask_even = _mm_set1_epi32(1);         // Mask to check odd/even\n\n    size_t i = 0;\n    while(i + 4 <= arr.size()){\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(data, mask_even), _mm_setzero_si128());\n        __m128i valid_data = _mm_blendv_epi8(_mm_setzero_si128(), data, even_mask);\n        __m128i valid_min = _mm_min_epu32(min_even, valid_data);\n        __m128i valid_min_index = _mm_add_epi32(min_index, _mm_cmpgt_epi32(valid_min, min_even));\n        min_even = _mm_blendv_epi8(min_even, valid_min, _mm_cmpeq_epi32(valid_min, min_even));\n        min_index = _mm_blendv_epi8(min_index, valid_min_index, _mm_cmpeq_epi32(valid_min, min_even));\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for(size_t j = i; j < arr.size(); ++j){\n        if(arr[j] % 2 == 0 && arr[j] < min_even.m128i_u32[0]){\n            min_even.m128i_u32[0] = arr[j];\n            min_index.m128i_u32[0] = j;\n        }\n    }\n\n    if(min_even.m128i_u32[0] == 0x7FFFFFFF) return {};\n    return {min_even.m128i_u32[0], min_index.m128i_u32[0]};\n}\n```"}
{"task_id": "SimdBench_94_SSE", "completion": "```cpp\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m128i min_even = _mm_set1_epi32(0x7FFFFFFF); // Initialize to max int\n    __m128i min_index = _mm_set1_epi32(-1);        // Initialize to invalid index\n    __m128i mask_even = _mm_set1_epi32(1);         // Mask to check odd/even\n\n    size_t i = 0;\n    while(i + 4 <= arr.size()){\n        __m128i data = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(data, mask_even), _mm_setzero_si128());\n        \n        __m128i valid_data = _mm_blendv_epi8(_mm_setzero_si128(), data, even_mask);\n        __m128i valid_min = _mm_min_epu32(min_even, valid_data);\n        __m128i valid_min_mask = _mm_cmpeq_epi32(valid_min, valid_data);\n        min_even = _mm_blendv_epi8(min_even, valid_min, valid_min_mask);\n\n        __m128i indices = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i valid_indices = _mm_blendv_epi8(_mm_setzero_si128(), indices, even_mask);\n        __m128i min_index_valid = _mm_minpos_epu16(_mm_castps_si128(_mm_cvtepi32_ps(valid_indices)));\n        min_index = _mm_blendv_epi8(min_index, min_index_valid, valid_min_mask);\n\n        i += 4;\n    }\n\n    // Handle remaining elements\n    for(; i < arr.size(); ++i){\n        if(arr[i] % 2 == 0 && arr[i] < min_even.m128i_u32[0]){\n            min_even = _mm_set1_epi32(arr[i]);\n            min_index = _mm_set1_epi32(i);\n        }\n    }\n\n    if(min_even.m128i_u32[0] == 0x7FFFFFFF) return {};\n    return {min_even.m128i_u32[0], min_index.m128i_u32[0]};\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_val = -1;\n    if(lst.empty()) return max_val;\n\n    // Determine the size of the vector\n    int n = lst.size();\n\n    // Align the vector size to a multiple of 4 for SSE2\n    int aligned_size = (n / 4) * 4;\n    \n    // Create a mask for unaligned elements\n    bool has_unaligned = n % 4 != 0;\n\n    // Create a histogram array with 256 entries (assuming values are positive integers)\n    int histogram[256] = {0};\n\n    // Count frequencies using SSE2\n    for(int i = 0; i < aligned_size; i += 4){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i count = _mm_set1_epi32(1);\n        _mm_storeu_si128((__m128i*)&count, _mm_add_epi32(_mm_loadu_si128((__m128i*)&histogram[v[0]]), count));\n        _mm_storeu_si128((__m128i*)&histogram[v[1]], _mm_add_epi32(_mm_loadu_si128((__m128i*)&histogram[v[1]]), count));\n        _mm_storeu_si128((__m128i*)&histogram[v[2]], _mm_add_epi32(_mm_loadu_si128((__m128i*)&histogram[v[2]]), count));\n        _mm_storeu_si128((__m128i*)&histogram[v[3]], _mm_add_epi32(_mm_loadu_si128((__m128i*)&histogram[v[3]]), count));\n    }\n\n    // Handle remaining unaligned elements\n    if(has_unaligned){\n        for(int i = aligned_size; i < n; ++i){\n            histogram[lst[i]]++;\n        }\n    }\n\n    // Find the maximum value satisfying the condition\n    for(int i = 1; i < 256; ++i){\n        if(histogram[i] >= i && i > max_val){\n            max_val = i;\n        }\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_result = -1;\n    if(lst.empty()) return max_result;\n\n    // Determine the size of the vector\n    int n = lst.size();\n    \n    // Align the vector size to a multiple of 4 for SSE2\n    int aligned_size = (n / 4) * 4;\n    \n    // Create a frequency array initialized to zero\n    __m128i freq[256];\n    for(int i=0;i<256;i++) {\n        freq[i] = _mm_setzero_si128();\n    }\n\n    // Count frequencies using SSE2\n    for(int i=0; i<aligned_size; i+=4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask_1 = _mm_cmpeq_epi32(vec, _mm_set1_epi32(1));\n        __m128i mask_2 = _mm_cmpeq_epi32(vec, _mm_set1_epi32(2));\n        __m128i mask_3 = _mm_cmpeq_epi32(vec, _mm_set1_epi32(3));\n        __m128i mask_4 = _mm_cmpeq_epi32(vec, _mm_set1_epi32(4));\n\n        freq[1] = _mm_add_epi32(freq[1], mask_1);\n        freq[2] = _mm_add_epi32(freq[2], mask_2);\n        freq[3] = _mm_add_epi32(freq[3], mask_3);\n        freq[4] = _mm_add_epi32(freq[4], mask_4);\n    }\n\n    // Handle remaining elements\n    for(int i=aligned_size; i<n; ++i){\n        if(lst[i] >=1 && lst[i] <=4){\n            freq[lst[i]] = _mm_add_epi32(freq[lst[i]], _mm_set1_epi32(1));\n        }\n    }\n\n    // Check for valid results\n    for(int val=4; val>=1; --val){\n        __m128i count = freq[val];\n        __m128i threshold = _mm_set1_epi32(val);\n        __m128i cmp = _mm_cmpgt_epi32(count, threshold);\n        int result = _mm_movemask_epi8(cmp);\n        if(result != 0){\n            max_result = val;\n            break;\n        }\n    }\n\n    return max_result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_result = -1;\n    if(lst.empty()) return max_result;\n\n    // Determine the size of the list and align to 4 for SSE2\n    size_t n = lst.size();\n    size_t i = 0;\n\n    // Create a frequency array using SIMD\n    __m128i freq[1024] __attribute__((aligned(16)));\n    for(int j=0; j<1024; j++) {\n        freq[j] = _mm_setzero_si128();\n    }\n\n    // Count frequencies using SIMD\n    for(i=0; i + 4 <= n; i+=4){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask1 = _mm_cmpgt_epi32(v, _mm_setzero_si128());\n        __m128i mask2 = _mm_cmpeq_epi32(v, _mm_set1_epi32(1));\n        __m128i mask3 = _mm_cmpeq_epi32(v, _mm_set1_epi32(2));\n        __m128i mask4 = _mm_cmpeq_epi32(v, _mm_set1_epi32(3));\n\n        freq[1] = _mm_add_epi32(freq[1], mask2);\n        freq[2] = _mm_add_epi32(freq[2], mask3);\n        freq[3] = _mm_add_epi32(freq[3], mask4);\n    }\n\n    // Handle remaining elements\n    for(; i < n; ++i){\n        if(lst[i] > 0 && lst[i] < 1024){\n            freq[lst[i]] = _mm_add_epi32(freq[lst[i]], _mm_set1_epi32(1));\n        }\n    }\n\n    // Check for valid results\n    for(int val=1; val<1024; val++){\n        __m128i cmp = _mm_cmpgt_epi32(freq[val], _mm_set1_epi32(val));\n        int mask = _mm_movemask_epi8(cmp);\n        if(mask != 0){\n            max_result = std::max(max_result, val);\n        }\n    }\n\n    return max_result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_result = -1;\n    if(lst.empty()) return max_result;\n\n    // Determine the size of the vector and align to 4 for SSE2\n    size_t n = lst.size();\n    size_t i = 0;\n\n    // Create a histogram array initialized to zero\n    __m128i histogram[16] = { _mm_setzero_si128(), _mm_setzero_si128(), _mm_setzero_si128(),\n                              _mm_setzero_si128(), _mm_setzero_si128(), _mm_setzero_si128(),\n                              _mm_setzero_si128(), _mm_setzero_si128(), _mm_setzero_si128(),\n                              _mm_setzero_si128(), _mm_setzero_si128(), _mm_setzero_si128(),\n                              _mm_setzero_si128(), _mm_setzero_si128(), _mm_setzero_si128(),\n                              _mm_setzero_si128() };\n\n    // Process data in chunks of 4 using SSE2\n    for (; i + 4 <= n; i += 4) {\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask1 = _mm_cmpgt_epi32(v, _mm_set1_epi32(0));\n        __m128i mask2 = _mm_cmpgt_epi32(_mm_set1_epi32(15), v);\n\n        __m128i valid_mask = _mm_and_si128(mask1, mask2);\n        __m128i indices = _mm_and_si128(v, valid_mask);\n\n        __m128i counts = _mm_add_epi32(histogram[0], valid_mask);\n        histogram[0] = counts;\n\n        counts = _mm_add_epi32(histogram[1], _mm_slli_epi32(valid_mask, 1));\n        histogram[1] = counts;\n\n        counts = _mm_add_epi32(histogram[2], _mm_slli_epi32(valid_mask, 2));\n        histogram[2] = counts;\n\n        counts = _mm_add_epi32(histogram[3], _mm_slli_epi32(valid_mask, 3));\n        histogram[3] = counts;\n\n        counts = _mm_add_epi32(histogram[4], _mm_slli_epi32(valid_mask, 4));\n        histogram[4] = counts;\n\n        counts = _mm_add_epi32(histogram[5], _mm_slli_epi32(valid_mask, 5));\n        histogram[5] = counts;\n\n        counts = _mm_add_epi32(histogram[6], _mm_slli_epi32(valid_mask, 6));\n        histogram[6] = counts;\n\n        counts = _mm_add_epi32(histogram[7], _mm_slli_epi32(valid_mask, 7));\n        histogram[7] = counts;\n\n        counts = _mm_add_epi32(histogram[8], _mm_slli_epi32(valid_mask, 8));\n        histogram[8] = counts;\n\n        counts = _mm_add_epi32(histogram[9], _mm_slli_epi32(valid_mask, 9));\n        histogram[9] = counts;\n\n        counts = _mm_add_epi32(histogram[10], _mm_slli_epi32(valid_mask, 10));\n        histogram[10] = counts;\n\n        counts = _mm_add_epi32(histogram[11], _mm_slli_epi32(valid_mask, 11));\n        histogram[11] = counts;\n\n        counts = _mm_add_epi32(histogram[12], _mm_slli_epi32(valid_mask, 12));\n        histogram[12] = counts;\n\n        counts = _mm_add_epi32(histogram[13], _mm_slli_epi32(valid_mask, 13));\n        histogram[13] = counts;\n\n        counts = _mm_add_epi32(histogram[14], _mm_slli_epi32(valid_mask, 14));\n        histogram[14] = counts;\n\n        counts = _mm_add_epi32(histogram[15], _mm_slli_epi32(valid_mask, 15));\n        histogram[15] = counts;\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        if (lst[i] > 0 && lst[i] < 16) {\n            histogram[lst[i]]++;\n        }\n    }\n\n    // Find the maximum result\n    for(int j = 15; j > 0; --j){\n        if(histogram[j] >= j){\n            max_result = j;\n            break;\n        }\n    }\n\n    return max_result;\n}\n```"}
{"task_id": "SimdBench_95_SSE", "completion": "```cpp\nint search_simd(std::vector<int> lst){\n    int max_val = -1;\n    if(lst.empty()) return max_val;\n\n    // Determine the size of the vector\n    int n = lst.size();\n    \n    // Align the vector size to a multiple of 4 for SSE2\n    int aligned_size = (n / 4) * 4;\n    \n    // Create masks for comparison\n    __m128i threshold = _mm_set1_epi32(0);\n    __m128i mask_gt_zero = _mm_set1_epi32(0);\n    \n    for(int i=0; i<aligned_size; i+=4){\n        // Load 4 elements at a time\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        \n        // Compare each element with zero\n        __m128i gt_zero = _mm_cmpgt_epi32(vec, threshold);\n        \n        // Count the number of elements greater than zero\n        int count = _mm_movemask_epi8(gt_zero);\n        count = __builtin_popcount(count);\n        \n        // Check if any element meets the condition\n        __m128i freq_check = _mm_cmpeq_epi32(vec, _mm_set1_epi32(count));\n        int result_mask = _mm_movemask_epi8(freq_check);\n        \n        if(result_mask != 0){\n            int index = __builtin_ctz(result_mask);\n            int candidate = lst[i + index];\n            if(candidate > max_val){\n                max_val = candidate;\n            }\n        }\n    }\n    \n    // Handle remaining elements\n    for(int i=aligned_size; i<n; ++i){\n        if(lst[i] > 0 && lst[i] <= n && std::count(lst.begin(), lst.end(), lst[i]) >= lst[i]){\n            if(lst[i] > max_val){\n                max_val = lst[i];\n            }\n        }\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    if (q.empty()) return false;\n\n    size_t n = q.size();\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i balance_vec = _mm_setzero_si128();\n\n    for (size_t i = 0; i + 4 <= n; i += 4) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i rev_vec = _mm_shuffle_epi32(vec, _MM_SHUFFLE(0, 1, 2, 3));\n        balance_vec = _mm_add_epi32(balance_vec, _mm_cmpeq_epi32(vec, rev_vec));\n\n        __m128i sum = _mm_hadd_epi32(vec, vec);\n        sum = _mm_hadd_epi32(sum, sum);\n        sum_vec = _mm_add_epi32(sum_vec, sum);\n    }\n\n    // Handle remaining elements\n    __m128i tail_sum = _mm_setzero_si128();\n    for (size_t i = n - (n % 4); i < n; ++i) {\n        tail_sum = _mm_add_epi32(tail_sum, _mm_cvtsi32_si128(q[i]));\n    }\n\n    sum_vec = _mm_add_epi32(sum_vec, tail_sum);\n\n    // Horizontal sum for balance check\n    __m128i balance_check = _mm_cmpeq_epi32(balance_vec, _mm_set1_epi32(4));\n    int balance_result = _mm_movemask_epi8(balance_check) == 0xFFFF ? 1 : 0;\n\n    // Horizontal sum for total sum\n    __m128i sum_high = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(3, 2, 1, 0));\n    sum_vec = _mm_add_epi32(sum_vec, sum_high);\n    sum_high = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(1, 0, 3, 2));\n    sum_vec = _mm_add_epi32(sum_vec, sum_high);\n    int total_sum = _mm_cvtsi128_si32(sum_vec);\n\n    return balance_result && total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\nbool will_it_fly_simd(std::vector<int> q, int w){\n    if(q.empty()) return false;\n    \n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i balance_vec = _mm_setzero_si128();\n    int i = 0;\n    int n = q.size();\n\n    // Process in chunks of 4 using SSE\n    for(; i <= n - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n        __m128i rev_vec = _mm_shuffle_epi32(vec, _MM_SHUFFLE(0,1,2,3));\n        balance_vec = _mm_add_epi32(balance_vec, _mm_cmpeq_epi32(vec, rev_vec));\n    }\n\n    // Horizontal sum for balance check\n    __m128i shuffle_mask = _mm_set_epi32(0, 1, 2, 3);\n    __m128i temp = _mm_shuffle_epi32(balance_vec, shuffle_mask);\n    balance_vec = _mm_add_epi32(balance_vec, temp);\n    temp = _mm_shuffle_epi32(balance_vec, shuffle_mask);\n    balance_vec = _mm_add_epi32(balance_vec, temp);\n    temp = _mm_shuffle_epi32(balance_vec, shuffle_mask);\n    balance_vec = _mm_add_epi32(balance_vec, temp);\n\n    // Check if all elements are equal (balanced)\n    bool is_balanced = _mm_movemask_epi8(_mm_cmpeq_epi32(balance_vec, _mm_set1_epi32(n)));\n\n    // Sum remaining elements\n    __m128i sum_remaining = _mm_setzero_si128();\n    for(; i < n; ++i){\n        sum_remaining = _mm_add_epi32(sum_remaining, _mm_set1_epi32(q[i]));\n    }\n    __m128i total_sum = _mm_add_epi32(sum_vec, sum_remaining);\n\n    // Horizontal sum for total sum\n    temp = _mm_shuffle_epi32(total_sum, shuffle_mask);\n    total_sum = _mm_add_epi32(total_sum, temp);\n    temp = _mm_shuffle_epi32(total_sum, shuffle_mask);\n    total_sum = _mm_add_epi32(total_sum, temp);\n    temp = _mm_shuffle_epi32(total_sum, shuffle_mask);\n    total_sum = _mm_add_epi32(total_sum, temp);\n\n    int total_sum_int = _mm_cvtsi128_si32(total_sum);\n    return is_balanced && total_sum_int <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\nbool will_it_fly_simd(std::vector<int> q, int w){\n    if(q.empty()) return false;\n    \n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i balance_vec = _mm_setzero_si128();\n    int i = 0;\n    int n = q.size();\n\n    // Process elements in chunks of 4 using SSE\n    for(; i + 4 <= n; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        \n        // Calculate sum\n        __m128i prev_sum = sum_vec;\n        sum_vec = _mm_add_epi32(sum_vec, vec);\n        \n        // Check balance (palindrome condition)\n        __m128i first = _mm_shuffle_epi32(vec, _MM_SHUFFLE(0,0,3,3));\n        __m128i second = _mm_shuffle_epi32(vec, _MM_SHUFFLE(1,1,2,2));\n        __m128i balance_check = _mm_cmpeq_epi32(first, second);\n        balance_vec = _mm_and_si128(balance_vec, balance_check);\n    }\n\n    // Horizontal sum for remaining elements\n    int sum = 0;\n    for(; i < n; ++i){\n        sum += q[i];\n    }\n    sum += _mm_extract_epi32(sum_vec, 0) + _mm_extract_epi32(sum_vec, 1);\n\n    // Horizontal check for balance\n    bool balance = _mm_movemask_epi8(balance_vec) == 0xFFFF;\n\n    return balance && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\nbool will_it_fly_simd(std::vector<int> q, int w){\n    if(q.empty()) return false;\n    \n    __m128i sum = _mm_setzero_si128();\n    __m128i balance_check = _mm_setzero_si128();\n    int i = 0;\n    int n = q.size();\n\n    // Process elements in chunks of 4 using SSE\n    for(; i <= n - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        sum = _mm_add_epi32(sum, vec);\n        __m128i rev_vec = _mm_shuffle_epi32(vec, _MM_SHUFFLE(0,1,3,2));\n        balance_check = _mm_and_si128(balance_check, _mm_cmpeq_epi32(vec, rev_vec));\n    }\n\n    // Handle remaining elements\n    for(; i < n; ++i){\n        sum.m128i_i32[0] += q[i];\n        if(q[i] != q[n - i - 1]) balance_check.m128i_i32[0] &= 0;\n    }\n\n    // Check balance\n    int balance = _mm_movemask_epi8(balance_check) == 0xFFFF;\n\n    // Sum reduction\n    int s = sum.m128i_i32[0] + sum.m128i_i32[1] + sum.m128i_i32[2] + sum.m128i_i32[3];\n\n    return balance && s <= w;\n}\n```"}
{"task_id": "SimdBench_96_SSE", "completion": "```cpp\nbool will_it_fly_simd(std::vector<int> q, int w){\n    if(q.empty()) return false;\n    \n    __m128i sum = _mm_setzero_si128();\n    __m128i balance_check = _mm_setzero_si128();\n    int i = 0;\n    int n = q.size();\n\n    // Process in chunks of 4 using SSE\n    for(; i <= n - 4; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&q[i]);\n        __m128i rev_vec = _mm_shuffle_epi32(vec, _MM_SHUFFLE(0,1,2,3));\n        \n        sum = _mm_add_epi32(sum, vec);\n        balance_check = _mm_and_si128(balance_check, _mm_cmpeq_epi32(vec, rev_vec));\n    }\n\n    // Handle remaining elements\n    for(; i < n; ++i){\n        balance_check = _mm_and_si128(balance_check, _mm_cmpeq_epi32(_mm_set1_epi32(q[i]), _mm_set1_epi32(q[n - i - 1])));\n    }\n\n    // Horizontal sum for balance check\n    __m128i mask = _mm_set_epi32(0, 0, 0, -1);\n    __m128i temp = _mm_and_si128(balance_check, mask);\n    int balance_result = _mm_movemask_epi8(temp);\n\n    // Check if all elements are balanced\n    bool is_balanced = __builtin_popcount(balance_result) == n;\n\n    // Sum reduction\n    int total_sum = 0;\n    asm(\"movd %0, %%eax\" : : \"r\"(sum) : \"%eax\");\n    total_sum += __builtin_bswap32(_mm_cvtsi128_si32(sum));\n    total_sum += __builtin_bswap32(_mm_cvtsi128_si32(_mm_srli_si128(sum, 4)));\n    total_sum += __builtin_bswap32(_mm_cvtsi128_si32(_mm_srli_si128(sum, 8)));\n    total_sum += __builtin_bswap32(_mm_cvtsi128_si32(_mm_srli_si128(sum, 12)));\n\n    return is_balanced && total_sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    int n = arr.size();\n    __m128i *vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_blocks = n / 4;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i left = vec[i];\n        __m128i right = vec[n - i - 1];\n        __m128i diff = _mm_sub_epi32(left, right);\n        diff = _mm_abs_epi32(diff);\n        changes += _mm_movemask_epi8(_mm_cmpeq_epi32(diff, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for(int i = num_blocks * 4; i < n; ++i){\n        if(arr[i] != arr[n - i - 1]){\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    int n = arr.size();\n    __m128i *vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_blocks = n / 4;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i left = _mm_loadu_si128(vec + i);\n        __m128i right = _mm_loadu_si128(vec + num_blocks - i - 1);\n        __m128i diff = _mm_cmpeq_epi32(left, right);\n        changes += _mm_movemask_epi8(diff);\n    }\n\n    // Handle remaining elements if n % 4 != 0\n    for(int i = num_blocks * 4; i < n; ++i){\n        if(arr[i] != arr[n - i - 1]){\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    int n = arr.size();\n    __m128i *vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_blocks = n / 4;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i left = vec[i];\n        __m128i right = vec[n - i - 1];\n        __m128i diff = _mm_sub_epi32(left, right);\n        diff = _mm_abs_epi32(diff);\n        changes += _mm_movemask_epi8(_mm_cmpeq_epi32(diff, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for(int i = num_blocks * 4; i < n; ++i){\n        if(arr[i] != arr[n - i - 1]){\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    int n = arr.size();\n    __m128i *vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_blocks = n / 4;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i left = vec[i];\n        __m128i right = vec[n - i - 1];\n        __m128i diff = _mm_sub_epi32(left, right);\n        diff = _mm_abs_epi32(diff);\n        changes += _mm_movemask_epi8(_mm_cmpeq_epi32(diff, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for(int i = num_blocks * 4; i < n; ++i){\n        if(arr[i] != arr[n - i - 1]){\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_SSE", "completion": "```cpp\nint smallest_change_simd(std::vector<int> arr){\n    int changes = 0;\n    int n = arr.size();\n    __m128i *vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_blocks = n / 4;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i left = vec[i];\n        __m128i right = vec[n - i - 1];\n        __m128i diff = _mm_sub_epi32(left, right);\n        diff = _mm_abs_epi32(diff);\n        changes += _mm_movemask_epi8(_mm_cmpeq_epi32(diff, _mm_setzero_si128()));\n    }\n\n    // Handle remaining elements if n is not a multiple of 4\n    for(int i = num_blocks * 4; i < n; ++i){\n        if(arr[i] != arr[n - i - 1]){\n            changes++;\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst1.size() && i + 4 <= lst2.size(); i += 4) {\n        __m128i len1 = _mm_set_epi32(lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i+0].length());\n        __m128i len2 = _mm_set_epi32(lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i+0].length());\n\n        sum1 = _mm_add_epi32(sum1, len1);\n        sum2 = _mm_add_epi32(sum2, len2);\n    }\n\n    __m128i cmp_result = _mm_cmpeq_epi32(sum1, sum2);\n\n    // Handle remaining elements\n    for (; i < lst1.size(); ++i) {\n        sum1 = _mm_add_epi32(sum1, _mm_cvtsi32_si128((int)lst1[i].length()));\n    }\n    for (; i < lst2.size(); ++i) {\n        sum2 = _mm_add_epi32(sum2, _mm_cvtsi32_si128((int)lst2[i].length()));\n    }\n\n    int result = _mm_movemask_epi8(cmp_result);\n    if (result == 0xFFFF) {\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int len1 = 0;\n    int len2 = 0;\n\n    // Calculate total length for lst1 using SSE\n    __m128i sum1 = _mm_setzero_si128();\n    for(int i=0; i<lst1.size(); i+=4){\n        __m128i v1 = _mm_set_epi32(lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i].length());\n        sum1 = _mm_add_epi32(sum1, v1);\n    }\n    len1 += _mm_cvtsi128_si32(sum1);\n    len1 += _mm_extract_epi32(_mm_shuffle_epi32(sum1, _MM_SHUFFLE(1,0,3,2)), 0);\n\n    // Calculate total length for lst2 using SSE\n    __m128i sum2 = _mm_setzero_si128();\n    for(int i=0; i<lst2.size(); i+=4){\n        __m128i v2 = _mm_set_epi32(lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i].length());\n        sum2 = _mm_add_epi32(sum2, v2);\n    }\n    len2 += _mm_cvtsi128_si32(sum2);\n    len2 += _mm_extract_epi32(_mm_shuffle_epi32(sum2, _MM_SHUFFLE(1,0,3,2)), 0);\n\n    // Compare lengths and return appropriate vector\n    if(len1 < len2){\n        return lst1;\n    } else if(len2 < len1){\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int len1 = 0;\n    int len2 = 0;\n    \n    // Calculate total length of strings in lst1 using SSE\n    __m128i sum1 = _mm_setzero_si128();\n    for(int i=0; i<lst1.size(); i+=4){\n        __m128i lengths = _mm_set_epi32(lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i].length());\n        sum1 = _mm_add_epi32(sum1, lengths);\n    }\n    len1 = _mm_cvtsi128_si32(sum1);\n\n    // Calculate total length of strings in lst2 using SSE\n    __m128i sum2 = _mm_setzero_si128();\n    for(int i=0; i<lst2.size(); i+=4){\n        __m128i lengths = _mm_set_epi32(lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i].length());\n        sum2 = _mm_add_epi32(sum2, lengths);\n    }\n    len2 = _mm_cvtsi128_si32(sum2);\n\n    // Compare total lengths and return appropriate vector\n    if(len1 < len2){\n        return lst1;\n    } else if(len2 < len1){\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int len1 = 0;\n    int len2 = 0;\n\n    // Calculate total length for lst1 using SSE\n    __m128i sum1 = _mm_setzero_si128();\n    for(int i=0; i<lst1.size(); i+=4){\n        __m128i v1 = _mm_set_epi32(lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i].length());\n        sum1 = _mm_add_epi32(sum1, v1);\n    }\n    len1 += _mm_cvtsi128_si32(sum1);\n    len1 += _mm_extract_epi32(_mm_shuffle_epi32(sum1, _MM_SHUFFLE(3,2,1,0)), 0);\n\n    // Calculate total length for lst2 using SSE\n    __m128i sum2 = _mm_setzero_si128();\n    for(int i=0; i<lst2.size(); i+=4){\n        __m128i v2 = _mm_set_epi32(lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i].length());\n        sum2 = _mm_add_epi32(sum2, v2);\n    }\n    len2 += _mm_cvtsi128_si32(sum2);\n    len2 += _mm_extract_epi32(_mm_shuffle_epi32(sum2, _MM_SHUFFLE(3,2,1,0)), 0);\n\n    // Compare lengths and return appropriate vector\n    if(len1 < len2){\n        return lst1;\n    } else if(len2 < len1){\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_98_SSE", "completion": "```cpp\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    __m128i sum1 = _mm_setzero_si128();\n    __m128i sum2 = _mm_setzero_si128();\n\n    size_t i = 0;\n    for (; i + 4 <= lst1.size() && i + 4 <= lst2.size(); i += 4) {\n        __m128i len1 = _mm_set_epi32(lst1[i+3].length(), lst1[i+2].length(), lst1[i+1].length(), lst1[i+0].length());\n        __m128i len2 = _mm_set_epi32(lst2[i+3].length(), lst2[i+2].length(), lst2[i+1].length(), lst2[i+0].length());\n\n        sum1 = _mm_add_epi32(sum1, len1);\n        sum2 = _mm_add_epi32(sum2, len2);\n    }\n\n    __m128i cmp_result = _mm_cmpeq_epi32(sum1, sum2);\n\n    // Handle remaining elements if any\n    for (; i < lst1.size(); ++i) {\n        sum1 = _mm_add_epi32(sum1, _mm_cvtsi32_si128((int)lst1[i].length()));\n    }\n    for (; i < lst2.size(); ++i) {\n        sum2 = _mm_add_epi32(sum2, _mm_cvtsi32_si128((int)lst2[i].length()));\n    }\n\n    int result = _mm_movemask_epi8(cmp_result);\n    if (result == 0xFFFF) {\n        return lst2;\n    } else {\n        return lst1;\n    }\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if(n == 1){\n        return x == 1;\n    }\n    if(x < n){\n        return false;\n    }\n    __m128i nx = _mm_set_epi64x(n, n);\n    __m128i xx = _mm_set_epi64x(x, x);\n    __m128i one = _mm_set1_epi64x(1);\n    __m128i zero = _mm_setzero_si128();\n    while(_mm_movemask_epi8(_mm_cmpgt_epi64(xx, nx)) != 0){\n        __m128i temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(temp, temp);\n        temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(nx, temp);\n        temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(nx, temp);\n        temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(nx, temp);\n        temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(nx, temp);\n        temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(nx, temp);\n        temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(nx, temp);\n        temp = _mm_mul_epu32(nx, nx);\n        nx = _mm_add_epi64(nx, temp);\n        if(_mm_movemask_epi8(_mm_cmpeq_epi64(xx, nx)) != 0){\n            return true;\n        }\n        if(_mm_movemask_epi8(_mm_cmpgt_epi64(xx, nx)) == 0){\n            break;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if(n == 1){\n        return x == 1;\n    }\n    if(x < n){\n        return false;\n    }\n    __m128i nx = _mm_set_epi64x(n, n);\n    __m128i xx = _mm_set_epi64x(x, x);\n    __m128i one = _mm_set1_epi64x(1);\n    __m128i zero = _mm_setzero_si128();\n    while(_mm_movemask_epi8(_mm_cmplt_epi64(xx, nx)) == 0){\n        __m128i cmp = _mm_cmpeq_epi64(xx, nx);\n        if(_mm_movemask_epi8(cmp) != 0){\n            return true;\n        }\n        xx = _mm_mul_epu32(xx, xx);\n        if(_mm_movemask_epi8(_mm_cmpgt_epi64(xx, one)) == 0){\n            break;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if(n == 1){\n        return x == 1;\n    }\n    __m128i base = _mm_set1_epi64x(n);\n    __m128i current = _mm_set1_epi64x(1);\n    __m128i limit = _mm_set1_epi64x(x);\n    int mask = -1;\n    while(current[0] <= x && current[1] <= x){\n        __m128i cmp = _mm_cmpeq_epi64(current, limit);\n        mask &= cmp[0];\n        mask &= cmp[1];\n        if(mask == 0){\n            break;\n        }\n        current = _mm_mul_epu32(current, base);\n    }\n    return mask != 0;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```cpp\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if(n == 1){\n        return x == 1;\n    }\n    if(x < n){\n        return false;\n    }\n    __m128i vx = _mm_set_epi64x(x, x);\n    __m128i vn = _mm_set_epi64x(n, n);\n    __m128i vone = _mm_set1_epi64x(1);\n    while(_mm_movemask_epi8(_mm_cmplt_epi64(vn, vx)) != 0){\n        __m128i vmod = _mm_mod_epi64(vx, vn);\n        if(!_mm_testz_si128(vmod, vmod)){\n            break;\n        }\n        vx = _mm_div_epi64(vx, vn);\n        if(_mm_movemask_epi8(_mm_cmpeq_epi64(vx, vone)) != 0){\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_99_SSE", "completion": "```c\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    if(n == 1){\n        return x == 1;\n    }\n    if(x < n){\n        return false;\n    }\n    __m128i nx = _mm_set_epi64x(n, n);\n    __m128i xx = _mm_set_epi64x(x, x);\n    __m128i one = _mm_set1_epi64x(1);\n    while(_mm_movemask_epi8(_mm_cmplt_epi64(nx, xx)) != 0){\n        __m128i next = _mm_mullo_epi64(nx, nx);\n        if(_mm_movemask_epi8(_mm_cmpeq_epi64(next, xx)) != 0){\n            return true;\n        }\n        if(_mm_movemask_epi8(_mm_cmpgt_epi64(next, xx)) != 0){\n            break;\n        }\n        nx = next;\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m128i primes_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, // 'D'\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, // 'B'\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, // '7'\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, // '5'\n        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0  // '3'\n    );\n    int count = 0;\n    const char *data = num.c_str();\n    size_t len = num.size();\n\n    for(size_t i = 0; i + 16 <= len; i += 16){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i masked = _mm_and_si128(vec, primes_mask);\n        int popcnt = _mm_popcnt_u32(_mm_cvtsi128_si32(masked)) +\n                     _mm_popcnt_u32(_mm_extract_epi16(masked, 1)) +\n                     _mm_popcnt_u32(_mm_extract_epi16(masked, 2)) +\n                     _mm_popcnt_u32(_mm_extract_epi16(masked, 3));\n        count += popcnt;\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m128i primes_mask = _mm_set_epi8(\n        /* 0xF */ 0, 0, 0, 0,\n        /* 0xE */ 0, 0, 0, 0,\n        /* 0xD */ 1, 0, 0, 0,\n        /* 0xC */ 0, 0, 0, 0,\n        /* 0xB */ 1, 0, 0, 0,\n        /* 0xA */ 0, 0, 0, 0,\n        /* 0x9 */ 0, 0, 0, 0,\n        /* 0x8 */ 0, 0, 0, 0,\n        /* 0x7 */ 1, 0, 0, 0,\n        /* 0x6 */ 0, 0, 0, 0,\n        /* 0x5 */ 1, 0, 0, 0,\n        /* 0x4 */ 0, 0, 0, 0,\n        /* 0x3 */ 1, 0, 0, 0,\n        /* 0x2 */ 1, 0, 0, 0,\n        /* 0x1 */ 0, 0, 0, 0,\n        /* 0x0 */ 0, 0, 0, 0\n    );\n    int count = 0;\n    size_t i = 0;\n    while(i + 16 <= num.size()){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        __m128i masked = _mm_and_si128(vec, primes_mask);\n        int mask = _mm_movemask_epi8(masked);\n        count += __builtin_popcount(mask);\n        i += 16;\n    }\n    for(size_t j = i; j < num.size(); ++j){\n        char c = num[j];\n        if(c == '2' || c == '3' || c == '5' || c == '7' ||\n           c == 'B' || c == 'D'){\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m128i primes_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, // D\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, // B\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, // 7\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, // 5\n        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0  // 3\n    );\n    int count = 0;\n    int i = 0;\n    for (; i <= static_cast<int>(num.size()) - 16; i += 16){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        __m128i cmp_res = _mm_cmpeq_epi8(chars, _mm_set1_epi8('D'));\n        __m128i mask = _mm_and_si128(cmp_res, primes_mask);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n    for (; i < num.size(); ++i){\n        if (num[i] == 'D' || num[i] == 'B' || num[i] == '7' ||\n            num[i] == '5' || num[i] == '3'){\n            ++count;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m128i primes_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, // D\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, // B\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, // 7\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, // 5\n        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0  // 3\n    );\n    \n    int count = 0;\n    size_t i = 0;\n    size_t len = num.size();\n\n    while(i + 16 <= len){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&num[i]));\n        __m128i masked = _mm_and_si128(chars, primes_mask);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(masked));\n        i += 16;\n    }\n\n    for(; i < len; ++i){\n        char c = num[i];\n        if(c == '2' || c == '3' || c == '5' || c == '7' ||\n           c == 'B' || c == 'D'){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_SSE", "completion": "```cpp\nint hex_key_simd(const std::string & num){\n    const __m128i primes_mask = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, // 'D'\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, // 'B'\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, // '7'\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, // '5'\n        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0  // '3'\n    );\n    int count = 0;\n    const char * data = num.c_str();\n    size_t len = num.size();\n\n    for(size_t i = 0; i + 16 <= len; i += 16){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i masked = _mm_and_si128(vec, primes_mask);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(masked));\n    }\n\n    for(size_t i = len - (len % 16); i < len; ++i){\n        if(strchr(\"2357BD\", data[i])){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char * str = s.c_str();\n    int len = s.length();\n\n    if(len < 3) return false;\n\n    __m128i prev_prev = _mm_setzero_si128();\n    __m128i prev = _mm_setzero_si128();\n    __m128i current = _mm_setzero_si128();\n\n    for(int i=0; i<len-2; i+=4){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i mask1 = _mm_cmpeq_epi8(chunk, prev_prev);\n        __m128i mask2 = _mm_cmpeq_epi8(chunk, prev);\n        __m128i mask3 = _mm_cmpeq_epi8(chunk, _mm_alignr_epi8(chunk, prev, 1));\n        __m128i any_match = _mm_or_si128(_mm_or_si128(mask1, mask2), mask3);\n\n        if(!_mm_testz_si128(any_match, any_match)){\n            return false;\n        }\n\n        prev_prev = prev;\n        prev = current;\n        current = chunk;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char * str = s.c_str();\n    int len = s.length();\n\n    if(len < 3) return false;\n\n    __m128i prev_prev = _mm_setzero_si128();\n    __m128i prev = _mm_setzero_si128();\n    __m128i current = _mm_setzero_si128();\n\n    for(int i=0; i<len-2; i+=4){\n        __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        __m128i mask1 = _mm_cmpeq_epi8(chunk, prev_prev);\n        __m128i mask2 = _mm_cmpeq_epi8(chunk, prev);\n        __m128i mask3 = _mm_cmpeq_epi8(chunk, _mm_alignr_epi8(chunk, prev, 1));\n        __m128i any_match = _mm_or_si128(_mm_or_si128(mask1, mask2), mask3);\n\n        if(!_mm_testz_si128(any_match, any_match)){\n            return false;\n        }\n\n        prev_prev = prev;\n        prev = current;\n        current = chunk;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char* str = s.c_str();\n    int len = s.length();\n\n    if(len < 3)\n        return false;\n\n    __m128i prev_prev = _mm_setzero_si128();\n    __m128i prev = _mm_setzero_si128();\n    __m128i current = _mm_setzero_si128();\n\n    for(int i=0; i<len; i+=4){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n        \n        // Shift registers to make room for new characters\n        prev_prev = prev;\n        prev = current;\n        current = chars;\n\n        // Compare each character with the previous two in parallel\n        __m128i mask1 = _mm_cmpeq_epi8(current, prev);\n        __m128i mask2 = _mm_cmpeq_epi8(current, prev_prev);\n\n        // Combine masks to check for duplicates\n        __m128i duplicate_mask = _mm_or_si128(mask1, mask2);\n\n        // If any duplicate is found, return false\n        if(_mm_movemask_epi8(duplicate_mask) != 0)\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char* str = s.c_str();\n    int len = s.length();\n\n    if(len < 3) return false;\n\n    __m128i prev_prev = _mm_setzero_si128();\n    __m128i prev = _mm_setzero_si128();\n    __m128i current = _mm_setzero_si128();\n\n    for(int i=0; i<len; i+=4){\n        __m128i chars = _mm_loadu_si128((__m128i*)(str + i));\n        current = _mm_andnot_si128(_mm_set1_epi8(0x7F), chars);\n\n        if(i >= 8){\n            __m128i mask = _mm_cmpeq_epi8(current, prev_prev);\n            if(_mm_movemask_epi8(mask) != 0) return false;\n\n            mask = _mm_cmpeq_epi8(current, prev);\n            if(_mm_movemask_epi8(mask) != 0) return false;\n        }\n\n        prev_prev = prev;\n        prev = current;\n    }\n\n    // Check last 3 characters\n    if(len > 2){\n        if(s[len-3] == s[len-2] || s[len-3] == s[len-1] || s[len-2] == s[len-1]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_SSE", "completion": "```cpp\nbool is_happy_simd(const std::string & s){\n    const char * str = s.c_str();\n    size_t len = s.length();\n\n    // Check if the string length is less than 3\n    if(len < 3){\n        return false;\n    }\n\n    // Load the first three characters into an SSE register\n    __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str));\n\n    // Extract individual bytes from the register\n    char c0 = _mm_extract_epi8(chars, 0);\n    char c1 = _mm_extract_epi8(chars, 1);\n    char c2 = _mm_extract_epi8(chars, 2);\n\n    // Check if the first three characters are distinct\n    if(!(c0 != c1 && c1 != c2 && c0 != c2)){\n        return false;\n    }\n\n    // Process the rest of the string in chunks of 16 bytes\n    for(size_t i = 3; i + 16 <= len; i += 16){\n        // Load the next chunk of 16 characters\n        __m128i current_chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n\n        // Compare each character with the previous two characters\n        __m128i prev_chunk = _mm_alignr_epi8(current_chunk, chars, 1);\n        __m128i prev_prev_chunk = _mm_alignr_epi8(current_chunk, chars, 2);\n\n        // Check if all characters in the current chunk are distinct from the previous two chunks\n        __m128i mask = _mm_cmpeq_epi8(current_chunk, prev_chunk);\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(current_chunk, prev_prev_chunk));\n        mask = _mm_or_si128(mask, _mm_cmpeq_epi8(prev_chunk, prev_prev_chunk));\n\n        // If any byte in the mask is zero, it means there are non-distinct characters\n        if(!_mm_movemask_epi8(mask)){\n            return false;\n        }\n\n        // Update the reference characters for the next iteration\n        chars = current_chunk;\n    }\n\n    // Check the remaining characters that couldn't fit into a full 16-byte chunk\n    for(size_t i = len - (len % 16); i < len - 2; ++i){\n        if(!(str[i] != str[i+1] && str[i+1] != str[i+2] && str[i] != str[i+2])){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```cpp\nbool prime_length_simd(const std::string & str){\n    const int len = str.length();\n    if(len < 2) return false;\n    __m128i primes = _mm_set_epi32(7,5,3,2);\n    __m128i mask = _mm_set_epi32(0,0,0,0);\n    for(int i=0;i<4;i++){\n        __m128i val = _mm_set1_epi32(len);\n        __m128i prime = _mm_shuffle_epi32(primes, _MM_SHUFFLE(i,i,i,i));\n        __m128i div = _mm_div_epu32(val, prime);\n        __m128i rem = _mm_sub_epi32(val, _mm_mul_epu32(div, prime));\n        mask = _mm_or_si128(mask, rem);\n    }\n    int mask_bits;\n    _mm_storeu_si128((__m128i*)&mask_bits, mask);\n    return !(mask_bits & 0b1111);\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int length = str.length();\n    if(length < 2) return false;\n\n    __m128i primes = _mm_set_epi32(7, 5, 3, 2);\n    __m128i mask = _mm_set_epi32(0, 0, 0, 0);\n    __m128i divisors = _mm_setzero_si128();\n\n    for(int i = 0; i < 4; ++i){\n        __m128i divisor = _mm_set1_epi32(_mm_extract_epi32(primes, i));\n        __m128i remainder = _mm_div_epi32(_mm_set1_epi32(length), divisor);\n        divisors = _mm_or_si128(divisors, _mm_cmpeq_epi32(remainder, _mm_setzero_si128()));\n    }\n\n    mask = _mm_packs_epi32(divisors, divisors);\n    bool result = !_mm_movemask_epi8(mask);\n    return result;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int length = str.length();\n    if(length < 2) return false;\n\n    __m128i primes = _mm_set_epi32(7, 5, 3, 2);\n    __m128i mask = _mm_set_epi32(0, 0, 0, 0);\n\n    for(int i = 0; i < 4; ++i){\n        __m128i div = _mm_set1_epi32(primes.m128i_i32[i]);\n        __m128i cmp = _mm_cmpeq_epi32(_mm_div_epi32(_mm_set1_epi32(length), div), _mm_setzero_si128());\n        mask = _mm_or_si128(mask, cmp);\n    }\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, mask);\n    for(int i = 0; i < 4; ++i){\n        if(result[i] == 0) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int len = str.length();\n    if(len < 2) return false;\n    __m128i num = _mm_set1_epi32(len);\n    __m128i mask = _mm_set_epi32(7, 5, 3, 2);\n    __m128i divisors = _mm_and_si128(_mm_set1_epi32(len), mask);\n    __m128i zero = _mm_setzero_si128();\n    __m128i remainder = _mm_div_epi32(num, divisors);\n    __m128i is_prime = _mm_cmpeq_epi32(remainder, zero);\n    int result = _mm_movemask_epi8(is_prime);\n    return (result == 0);\n}\n```"}
{"task_id": "SimdBench_102_SSE", "completion": "```c\nbool prime_length_simd(const std::string & str){\n    const int len = str.length();\n    if(len < 2) return false;\n    __m128i num = _mm_set1_epi32(len);\n    __m128i mask = _mm_set_epi32(7,5,3,2);\n    __m128i divisors = _mm_and_si128(_mm_set1_epi32(len), mask);\n    __m128i zero = _mm_setzero_si128();\n    __m128i cmp = _mm_div_epi32(num, divisors);\n    __m128i rem = _mm_sub_epi32(num, _mm_mul_epu32(cmp, divisors));\n    __m128i is_prime = _mm_cmpeq_epi32(rem, zero);\n    int result = _mm_movemask_epi8(is_prime);\n    return (result == 0);\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int SIMD_WIDTH = 4; // Using 4 packed integers with SSE2\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i sum_reg = _mm_setzero_si128();\n    \n    uint64_t data[2] = {N, 0};\n    __m128i input = _mm_loadu_si128((__m128i*)data);\n    \n    // Extract digits in parallel\n    __m128i low_nibble = _mm_and_si128(input, mask);\n    __m128i high_nibble = _mm_and_si128(_mm_srli_epi64(input, 4), mask);\n    \n    // Sum digits in parallel\n    __m128i sum_low = _mm_sad_epu8(low_nibble, _mm_setzero_si128());\n    __m128i sum_high = _mm_sad_epu8(high_nibble, _mm_setzero_si128());\n    sum_reg = _mm_add_epi32(sum_reg, sum_low);\n    sum_reg = _mm_add_epi32(sum_reg, sum_high);\n    \n    // Horizontal sum across SIMD lanes\n    __m128i tmp = _mm_add_epi32(sum_reg, _mm_shuffle_epi32(sum_reg, _MM_SHUFFLE(0, 3, 2, 1)));\n    __m128i horizontal_sum = _mm_add_epi32(tmp, _mm_shuffle_epi32(tmp, _MM_SHUFFLE(1, 0, 3, 2)));\n    \n    int final_sum;\n    _mm_store_ss((float*)&final_sum, _mm_castsi128_ps(horizontal_sum));\n    \n    char buffer[12];\n    snprintf(buffer, sizeof(buffer), \"%d\", final_sum);\n    return std::string(buffer);\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int SIMD_WIDTH = 4; // Using 4 double words for SSE2\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i sum_reg = _mm_setzero_si128();\n    uint64_t temp = N;\n\n    for(int i = 0; i < 4; ++i){\n        __m128i num = _mm_cvtsi64_si128(temp);\n        num = _mm_and_si128(num, mask); // Extract last 4 bytes\n        sum_reg = _mm_add_epi8(sum_reg, num);\n        temp >>= 4;\n    }\n\n    __m128i carry;\n    do{\n        carry = _mm_srai_epi16(_mm_unpacklo_epi8(sum_reg, _mm_setzero_si128()), 4);\n        sum_reg = _mm_add_epi8(sum_reg, carry);\n    }while(_mm_movemask_epi8(carry) != 0);\n\n    int result[16] __attribute__((aligned(16)));\n    _mm_storeu_si128((__m128i*)result, sum_reg);\n\n    std::string binary;\n    for(int i = 0; i < 16; ++i){\n        if(result[i]) binary += '1';\n        else binary += '0';\n    }\n    while(binary.size() > 1 && binary.back() == '0') binary.pop_back();\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int SIMD_WIDTH = 4; // Using 4 doublewords for SSE2\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i sum_reg = _mm_setzero_si128();\n    uint64_t temp = N;\n\n    // Load data into SIMD registers\n    for(int i = 0; i < sizeof(uint64_t)/sizeof(__m128i); ++i){\n        __m128i data = _mm_cvtsi64_si128(temp & 0xFFFFFFFF);\n        temp >>= 32;\n        __m128i digits = _mm_and_si128(data, mask);\n        digits = _mm_add_epi8(digits, _mm_sub_epi8(_mm_setzero_si128(), digits));\n        digits = _mm_sad_epu8(digits, _mm_setzero_si128());\n        sum_reg = _mm_add_epi32(sum_reg, digits);\n    }\n\n    // Horizontal sum across the SIMD register\n    __m128i tmp = _mm_hadd_epi32(sum_reg, sum_reg);\n    tmp = _mm_hadd_epi32(tmp, tmp);\n    int final_sum = _mm_cvtsi128_si32(tmp);\n\n    // Convert the sum to binary string\n    std::string result = \"\";\n    while(final_sum > 0){\n        result += (final_sum % 2 == 0 ? '0' : '1');\n        final_sum /= 2;\n    }\n    return result.empty() ? \"0\" : std::string(result.rbegin(), result.rend());\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int SIMD_WIDTH = 4; // Using 4 packed 32-bit integers with SSE2\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i sum_reg = _mm_setzero_si128();\n    \n    uint64_t data[2] = {N, 0};\n    __m128i input = _mm_loadu_si128((__m128i*)data);\n\n    // Process two 64-bit values at once\n    for(int i = 0; i < 2; ++i){\n        __m128i digits = _mm_and_si128(input, mask); // Extract low 4 bits\n        __m128i shifted = _mm_srli_epi32(input, 4);  // Shift right by 4 bits\n        sum_reg = _mm_add_epi32(sum_reg, digits);    // Accumulate digit sums\n\n        input = shifted;\n    }\n\n    // Horizontal sum across the SIMD register\n    __m128i tmp = _mm_add_epi32(sum_reg, _mm_srli_si128(sum_reg, 8));\n    tmp = _mm_add_epi32(tmp, _mm_srli_si128(tmp, 4));\n    uint32_t final_sum = _mm_cvtsi128_si32(tmp);\n\n    char buffer[12]; // Max length for 64-bit sum in binary\n    int pos = sprintf(buffer, \"%u\", final_sum);\n    return std::string(buffer, pos);\n}\n```"}
{"task_id": "SimdBench_103_SSE", "completion": "```cpp\nstd::string solve_simd(uint64_t N){\n    const int SIMD_WIDTH = 4; // SSE can handle 4 doublewords at once\n    __m128i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F);\n    __m128i sum_reg = _mm_setzero_si128();\n    uint64_t temp = N;\n\n    for(int i = 0; i < 16; i += SIMD_WIDTH){\n        __m128i input = _mm_set_epi64x(temp >> (i + 8), temp >> i);\n        input = _mm_and_si128(input, mask); // Isolate lower 4 bits\n        sum_reg = _mm_add_epi32(sum_reg, input);\n    }\n\n    __m128i carry;\n    do{\n        carry = _mm_slli_epi32(sum_reg, 4);\n        sum_reg = _mm_add_epi32(sum_reg, carry);\n    }while(_mm_movemask_epi8(_mm_cmpgt_epi32(carry, _mm_setzero_si128())));\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, sum_reg);\n\n    std::string binary;\n    for(int i = 0; i < 4; ++i){\n        int digit = result[i];\n        while(digit){\n            binary += (digit % 2) ? '1' : '0';\n            digit /= 2;\n        }\n    }\n    std::reverse(binary.begin(), binary.end());\n    return binary;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set_epi32(0, 2, 1, 3);\n    __m128i mask_even = _mm_set_epi32(3, 1, 2, 0);\n\n    for(int i=0; i<n; i+=8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i odd_elements = _mm_shuffle_epi32(vec, mask_odd);\n        __m128i even_elements = _mm_shuffle_epi32(vec, mask_even);\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(even_elements, _mm_set1_epi32(1)), zero);\n        __m128i odd_mask = _mm_cmpeq_epi32(_mm_and_si128(odd_elements, _mm_set1_epi32(1)), zero);\n        __m128i odd_sum = _mm_add_epi32(_mm_and_si128(odd_elements, odd_mask), _mm_and_si128(even_elements, even_mask));\n        sum += _mm_cvtsi128_si32(odd_sum);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```cpp\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    int n = lst.size();\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask_odd = _mm_set_epi32(0, 2, 0, 1); // Mask to extract odd indices\n    __m128i mask_even = _mm_set_epi32(0, 0, 0, -2147483648); // Mask to check even numbers\n\n    for(int i=0; i<n; i+=8){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i odd_indices = _mm_shuffle_epi32(v, mask_odd);\n        __m128i even_masked = _mm_and_si128(odd_indices, mask_even);\n        __m128i non_zero = _mm_cmpgt_epi32(even_masked, zero);\n        __m128i even_sum = _mm_and_si128(even_masked, non_zero);\n        sum += _mm_cvtsi128_si32(even_sum);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```c\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    const int n = lst.size();\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set_epi32(0, 1, 0, 1);\n    for(int i=0; i<n; i+=8){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i even_mask = _mm_and_si128(_mm_set1_epi32(1), _mm_srli_epi32(v, 1));\n        __m128i odd_mask = _mm_andnot_si128(even_mask, mask);\n        __m128i values_at_odd_indices = _mm_andnot_si128(even_mask, v);\n        __m128i even_values_at_odd_indices = _mm_and_si128(values_at_odd_indices, odd_mask);\n        __m128i sum_vec = _mm_add_epi32(sum_vec, even_values_at_odd_indices);\n    }\n    __m128i result = _mm_hadd_epi32(sum_vec, sum_vec);\n    result = _mm_hadd_epi32(result, result);\n    sum = _mm_cvtsi128_si32(result);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```c\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    const int n = lst.size();\n    __m128i xmm0, xmm1, mask, result;\n\n    // Load mask for even numbers: 0x00000000 00000000 00000000 00000000\n    mask = _mm_set_epi32(0, 0, 0, 0);\n\n    for(int i = 1; i < n - 3; i += 2){\n        xmm0 = _mm_loadu_si128((__m128i*)&lst[i]);\n        result = _mm_and_si128(xmm0, mask);\n        result = _mm_cmpeq_epi32(result, mask); // Compare for even numbers\n        result = _mm_and_si128(xmm0, result);   // Keep only even numbers\n        sum += _mm_cvtsi128_si32(result);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_104_SSE", "completion": "```c\nint add_simd(std::vector<int> lst){\n    int sum = 0;\n    const int n = lst.size();\n    __m128i zero = _mm_setzero_si128();\n    __m128i mask = _mm_set_epi32(0, 1, 0, 1);\n    for(int i=0; i<n; i+=8){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i even = _mm_and_si128(v, mask);\n        __m128i is_even = _mm_cmpeq_epi32(even, zero);\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i odd_indices = _mm_andnot_si128(_mm_set1_epi32(2), indices);\n        __m128i cond = _mm_and_si128(is_even, _mm_cmpgt_epi32(odd_indices, zero));\n        __m128i selected = _mm_blendv_epi8(_mm_setzero_si128(), even, cond);\n        sum += _mm_cvtsi128_si32(selected);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    const int vec_size = 4; // SSE can handle 4 characters at once (4 * 8 bits)\n    const char shift = 4;   // Two places multiplied by two\n    const char a = 'a';\n    const char z = 'z';\n    \n    int len = s.size();\n    std::string result;\n    result.reserve(len);\n\n    for(int i = 0; i < len; i += vec_size){\n        __m128i input = _mm_setzero_si128();\n        __m128i output = _mm_setzero_si128();\n\n        // Load up to 4 characters into the SIMD register\n        if(i + 0 < len) input = _mm_insert_epi8(input, s[i + 0], 0);\n        if(i + 1 < len) input = _mm_insert_epi8(input, s[i + 1], 1);\n        if(i + 2 < len) input = _mm_insert_epi8(input, s[i + 2], 2);\n        if(i + 3 < len) input = _mm_insert_epi8(input, s[i + 3], 3);\n\n        // Shift each character by 'shift' positions\n        output = _mm_add_epi8(input, _mm_set1_epi8(shift));\n\n        // Handle wrap-around for lowercase letters ('a' to 'z')\n        __m128i lower_a = _mm_set1_epi8(a);\n        __m128i lower_z = _mm_set1_epi8(z);\n        __m128i mask = _mm_cmpgt_epi8(lower_z, output);\n        output = _mm_sub_epi8(output, _mm_and_si128(mask, _mm_set1_epi8(26)));\n\n        // Store the result back into the string\n        if(i + 0 < len) result += (char)_mm_extract_epi8(output, 0);\n        if(i + 1 < len) result += (char)_mm_extract_epi8(output, 1);\n        if(i + 2 < len) result += (char)_mm_extract_epi8(output, 2);\n        if(i + 3 < len) result += (char)_mm_extract_epi8(output, 3);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    const int vec_size = 4; // SSE operates on 4 characters at a time\n    const char shift = 4;   // Two multiplied by two places shift\n    const char a_val = 'a';\n    const char z_val = 'z';\n\n    size_t len = s.size();\n    std::string result;\n    result.reserve(len);\n\n    for(size_t i = 0; i < len; i += vec_size){\n        __m128i input = _mm_setzero_si128();\n        __m128i output = _mm_setzero_si128();\n\n        // Load up to 4 characters into the vector\n        if(i + 0 < len) input = _mm_insert_epi8(input, s[i + 0], 0);\n        if(i + 1 < len) input = _mm_insert_epi8(input, s[i + 1], 1);\n        if(i + 2 < len) input = _mm_insert_epi8(input, s[i + 2], 2);\n        if(i + 3 < len) input = _mm_insert_epi8(input, s[i + 3], 3);\n\n        // Shift each character by 'shift' positions\n        output = _mm_add_epi8(input, _mm_set1_epi8(shift));\n\n        // Handle wrap-around for lowercase letters ('a' to 'z')\n        __m128i lower_a = _mm_set1_epi8(a_val);\n        __m128i lower_z = _mm_set1_epi8(z_val);\n        __m128i mask = _mm_cmpgt_epi8(lower_z, output);\n        output = _mm_sub_epi8(output, _mm_and_si128(mask, _mm_set1_epi8(26)));\n\n        // Store the result back to memory\n        char buffer[vec_size];\n        _mm_storeu_si128((__m128i*)buffer, output);\n\n        // Append the processed characters to the result string\n        for(int j = 0; j < vec_size && i + j < len; ++j){\n            result += buffer[j];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string &s) {\n    const int vec_size = 4; // Using 4 characters per SIMD register\n    const char shift = 4;   // Shift by 2 * 2 positions\n    const char base = 'a';\n    const char mask = 0x1f;\n\n    int len = s.size();\n    std::string result(len, 0);\n\n    for (int i = 0; i < len; i += vec_size) {\n        __m128i input, output, shift_mask;\n\n        // Load data into SIMD register\n        input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(s.data() + i));\n\n        // Perform the encryption: (c - 'a' + shift) % 26 + 'a'\n        shift_mask = _mm_set1_epi8(base);\n        input = _mm_sub_epi8(input, shift_mask);\n        input = _mm_add_epi8(input, _mm_set1_epi8(shift));\n        input = _mm_and_si128(input, _mm_set1_epi8(mask));\n        input = _mm_add_epi8(input, shift_mask);\n\n        // Store the result back\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(result.data() + i), input);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\nstd::string encrypt_simd(const std::string & s){\n    const int vec_size = 4; // SSE operates on 4 characters at once\n    const char shift = 4;   // Two multiplied by two places shift\n    const char base = 'a';\n    const char mask = 0x1f;\n\n    size_t len = s.size();\n    std::string result(len, 0);\n\n    for(size_t i = 0; i < len; i += vec_size){\n        __m128i input = _mm_setzero_si128();\n        __m128i output = _mm_setzero_si128();\n\n        if(i + vec_size <= len){\n            // Load 4 characters into SIMD register\n            input = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&s[i]));\n\n            // Perform the encryption: (c - 'a' + shift) % 26 + 'a'\n            __m128i shifted = _mm_add_epi8(input, _mm_set1_epi8(shift));\n            __m128i wrapped = _mm_sub_epi8(shifted, _mm_set1_epi8(base));\n            wrapped = _mm_and_si128(wrapped, _mm_set1_epi8(mask));\n            __m128i encrypted = _mm_add_epi8(wrapped, _mm_set1_epi8(base));\n\n            // Store the result back\n            _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), encrypted);\n        }\n        else{\n            // Handle remaining characters\n            for(size_t j = i; j < len; ++j){\n                char c = s[j];\n                if(c >= 'a' && c <= 'z'){\n                    char enc = ((c - base + shift) % 26) + base;\n                    result[j] = enc;\n                }\n                else{\n                    result[j] = c;\n                }\n            }\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string &s) {\n    const int vec_size = 4; // Use 4 characters per SIMD operation\n    const char shift = 4;   // Shift by 2 * 2 positions\n    const char base = 'a';\n    const char mask = 0x1f;\n\n    int len = s.size();\n    std::string result(len, 0);\n\n    for (int i = 0; i < len; i += vec_size) {\n        __m128i input = _mm_setzero_si128();\n        __m128i output = _mm_setzero_si128();\n\n        // Load data into SIMD register\n        for (int j = 0; j < vec_size && (i + j) < len; ++j) {\n            input = _mm_insert_epi8(input, s[i + j] - base, j);\n        }\n\n        // Perform encryption: (x + shift) % 26\n        __m128i shifted = _mm_add_epi8(input, _mm_set1_epi8(shift));\n        __m128i mod_mask = _mm_set1_epi8(mask);\n        __m128i modulo = _mm_and_si128(shifted, mod_mask);\n\n        // Store the result back\n        for (int j = 0; j < vec_size && (i + j) < len; ++j) {\n            result[i + j] = (char)(_mm_extract_epi8(modulo, j) + base);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\nstd::string encode_simd(const std::string & message){\n    const __m128i mask_vowels = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n    const __m128i shift_vowels = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n    const __m128i swap_case = _mm_set_epi8(\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n    );\n\n    int i = 0;\n    std::string result = message;\n    __m128i data, swapped, shifted, masked, encoded;\n\n    while(i + 16 <= message.size()){\n        data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n\n        // Swap case\n        swapped = _mm_xor_si128(data, swap_case);\n\n        // Shift vowels\n        shifted = _mm_or_si128(swapped, shift_vowels);\n\n        // Mask non-vowels\n        masked = _mm_and_si128(shifted, mask_vowels);\n\n        // Encode\n        encoded = _mm_or_si128(masked, swapped);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), encoded);\n        i += 16;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\nstd::string encode_simd(const std::string & message){\n    const __m128i mask_vowels = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i shift_vowels = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i swap_case = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    \n    int i = 0;\n    std::string result = message;\n    __m128i data, vowels, shifted, swapped, mask, res;\n\n    while(i + 16 <= message.size()){\n        data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n\n        // Apply transformations using SSE/SSE2 instructions\n        vowels = _mm_and_si128(data, mask_vowels);\n        shifted = _mm_add_epi8(vowels, shift_vowels);\n        swapped = _mm_xor_si128(data, swap_case);\n\n        // Combine results\n        res = _mm_or_si128(shifted, swapped);\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), res);\n        i += 16;\n    }\n\n    // Handle remaining characters\n    for(; i < message.size(); ++i){\n        char c = message[i];\n        if(c >= 'A' && c <= 'Z'){\n            result[i] = c + ('a' - 'A');\n        }\n        else if(c >= 'a' && c <= 'z'){\n            result[i] = c - ('a' - 'A');\n        }\n        if(c == 'a' || c == 'e' || c == 'i' || c == 'o' || c == 'u'){\n            result[i] = static_cast<char>(c + 2);\n        }\n        else if(c == 'A' || c == 'E' || c == 'I' || c == 'O' || c == 'U'){\n            result[i] = static_cast<char>(c + 2 - ('a' - 'A'));\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string &message) {\n    const __m128i mask_upper = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_lower = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i vowels = _mm_set_epi8(\n        'u', 'o', 'i', 'e', 'a', 'U', 'O', 'I', 'E', 'A',\n        0, 0, 0, 0, 0, 0\n    );\n    const __m128i shift_vowels = _mm_set_epi8(\n        'w', 'q', 'k', 'g', 'c', 'W', 'Q', 'K', 'G', 'C',\n        0, 0, 0, 0, 0, 0\n    );\n\n    int len = message.size();\n    std::string result(len, ' ');\n\n    for (size_t i = 0; i < len; i += 16) {\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n        __m128i upper_mask = _mm_cmpgt_epi8(chars, _mm_set1_epi8('Z'));\n        __m128i lower_mask = _mm_cmpgt_epi8(_mm_set1_epi8('Z'), chars);\n        __m128i is_vowel = _mm_cmpeq_epi8(chars, vowels);\n\n        // Swap case\n        __m128i swapped_case = _mm_or_si128(\n            _mm_andnot_si128(lower_mask, _mm_add_epi8(chars, _mm_set1_epi8('A') - 'a')),\n            _mm_andnot_si128(upper_mask, _mm_add_epi8(chars, _mm_set1_epi8('a') - 'A'))\n        );\n\n        // Replace vowels\n        __m128i replaced_vowels = _mm_or_si128(\n            _mm_andnot_si128(is_vowel, swapped_case),\n            _mm_and_si128(is_vowel, shift_vowels)\n        );\n\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[i]), replaced_vowels);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string &message) {\n    const __m128i mask_upper = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i mask_lower = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    const __m128i vowels = _mm_set_epi8(\n        'u', 'o', 'i', 'e', 'a', 'U', 'O', 'I', 'E', 'A',\n        0, 0, 0, 0, 0, 0\n    );\n    const __m128i shift_vowels = _mm_set_epi8(\n        'w', 'q', 'k', 'g', 'c', 'W', 'Q', 'K', 'G', 'C',\n        0, 0, 0, 0, 0, 0\n    );\n    const __m128i swap_case_mask = _mm_set1_epi8(32);\n\n    int len = message.size();\n    std::string result;\n    result.reserve(len);\n\n    for (int i = 0; i < len; i += 16) {\n        __m128i chars = i + 15 < len ? _mm_loadu_si128(reinterpret_cast<const __m128i*>(message.data() + i)) : _mm_setzero_si128();\n        __m128i upper = _mm_or_si128(chars, mask_upper);\n        __m128i lower = _mm_andnot_si128(mask_upper, chars);\n        __m128i swapped_case = _mm_xor_si128(lower, swap_case_mask);\n\n        __m128i is_vowel = _mm_cmpeq_epi8(_mm_shuffle_epi8(vowels, chars), _mm_setzero_si128());\n        __m128i shifted_vowels = _mm_shuffle_epi8(shift_vowels, chars);\n        __m128i encoded_chars = _mm_blendv_epi8(swapped_case, shifted_vowels, is_vowel);\n\n        char buffer[16];\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(buffer), encoded_chars);\n        result.append(buffer, 16);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encode_simd(const std::string &message) {\n    const __m128i mask_upper = _mm_set_epi8(\n        0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20,\n        0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20, 0x20\n    );\n    const __m128i vowels_lower = _mm_set_epi8(\n        'u', 'o', 'i', 'e', 'a', 0, 0, 0,\n        'u', 'o', 'i', 'e', 'a', 0, 0, 0\n    );\n    const __m128i vowels_upper = _mm_set_epi8(\n        'U', 'O', 'I', 'E', 'A', 0, 0, 0,\n        'U', 'O', 'I', 'E', 'A', 0, 0, 0\n    );\n    const __m128i shift_vowels_lower = _mm_set_epi8(\n        'w', 'q', 'k', 'g', 'c', 0, 0, 0,\n        'w', 'q', 'k', 'g', 'c', 0, 0, 0\n    );\n    const __m128i shift_vowels_upper = _mm_set_epi8(\n        'W', 'Q', 'K', 'G', 'C', 0, 0, 0,\n        'W', 'Q', 'K', 'G', 'C', 0, 0, 0\n    );\n\n    int len = message.size();\n    std::string result;\n    result.reserve(len);\n\n    for (int i = 0; i + 16 <= len; i += 16) {\n        __m128i data = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&message[i]));\n        __m128i upper_masked = _mm_xor_si128(data, mask_upper);\n        __m128i lower_masked = _mm_and_si128(data, mask_upper);\n\n        __m128i lower_vowels_test = _mm_cmpeq_epi8(lower_masked, vowels_lower);\n        __m128i upper_vowels_test = _mm_cmpeq_epi8(upper_masked, vowels_upper);\n\n        __m128i shifted_lower_vowels = _mm_shuffle_epi8(shift_vowels_lower, lower_vowels_test);\n        __m128i shifted_upper_vowels = _mm_shuffle_epi8(shift_vowels_upper, upper_vowels_test);\n\n        __m128i swapped_case = _mm_or_si128(upper_masked, lower_masked);\n        __m128i encoded_vowels = _mm_or_si128(shifted_lower_vowels, shifted_upper_vowels);\n\n        __m128i final_data = _mm_or_si128(swapped_case, encoded_vowels);\n        _mm_storeu_si128(reinterpret_cast<__m128i*>(&result), final_data);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_mask = _mm_set1_epi8(0x20);\n    __m128i zero_mask = _mm_setzero_si128();\n\n    for(auto &pair : dict){\n        const std::string &key = pair.first;\n        if(key.size() < 16) continue; // Skip keys shorter than SIMD width\n\n        __m128i key_vec;\n        memcpy(&key_vec, key.c_str(), 16);\n\n        __m128i key_lower = _mm_and_si128(key_vec, lower_mask);\n        __m128i key_upper = _mm_or_si128(_mm_andnot_si128(lower_mask, key_vec), \n                                          _mm_set1_epi8('A' - 'a'));\n\n        __m128i lower_check = _mm_cmpeq_epi8(key_lower, key_vec);\n        __m128i upper_check = _mm_cmpeq_epi8(key_upper, key_vec);\n\n        all_lower &= _mm_movemask_epi8(lower_check) == 0xFFFF;\n        all_upper &= _mm_movemask_epi8(upper_check) == 0xFFFF;\n\n        if(!all_lower && !all_upper) break;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    bool all_lower = true;\n    bool all_upper = true;\n\n    __m128i lower_mask = _mm_set1_epi8(0x20);\n    __m128i upper_mask = _mm_set1_epi8(0x20);\n    __m128i zero_mask = _mm_setzero_si128();\n\n    for(auto &pair : dict){\n        const std::string &key = pair.first;\n\n        if(key.size() < 16) continue; // Skip small keys\n\n        const char *key_data = key.c_str();\n        __m128i key_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key_data));\n\n        __m128i key_lower = _mm_and_si128(key_vec, lower_mask);\n        __m128i key_upper = _mm_or_si128(_mm_andnot_si128(lower_mask, key_vec), \n                                          _mm_set1_epi8('A' - 'a'));\n\n        __m128i lower_check = _mm_cmpeq_epi8(key_lower, _mm_setzero_si128());\n        __m128i upper_check = _mm_cmpeq_epi8(key_upper, _mm_setzero_si128());\n\n        int lower_mask_all = _mm_movemask_epi8(lower_check);\n        int upper_mask_all = _mm_movemask_epi8(upper_check);\n\n        if(lower_mask_all != 0xFFFF) all_lower = false;\n        if(upper_mask_all != 0xFFFF) all_upper = false;\n\n        if(!all_lower && !all_upper) return false;\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    bool all_upper = true;\n    bool all_lower = true;\n\n    __m128i upper_mask = _mm_set1_epi8(0x40); // Mask for checking uppercase\n    __m128i lower_mask = _mm_set1_epi8(0x20); // Mask for checking lowercase\n\n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n\n        if(key.empty()) continue;\n\n        const char *str = key.c_str();\n        size_t len = key.length();\n\n        // Process in chunks of 16 bytes using SSE\n        for(size_t i=0; i<len; i+=16){\n            size_t chunk_len = (len - i >= 16) ? 16 : len - i;\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str + i));\n            __m128i upper_check = _mm_cmpeq_epi8(_mm_and_si128(chunk, upper_mask), _mm_setzero_si128());\n            __m128i lower_check = _mm_cmpeq_epi8(_mm_and_si128(chunk, lower_mask), _mm_setzero_si128());\n\n            // If any byte is uppercase, set all bits in upper_check\n            // If any byte is lowercase, set all bits in lower_check\n            upper_check = _mm_movemask_epi8(upper_check);\n            lower_check = _mm_movemask_epi8(lower_check);\n\n            if(upper_check != 0xFFFF) all_upper = false;\n            if(lower_check != 0xFFFF) all_lower = false;\n\n            if(!all_upper && !all_lower) return false;\n        }\n    }\n\n    return all_upper || all_lower;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    bool all_upper = true;\n    bool all_lower = true;\n\n    __m128i upper_mask = _mm_set1_epi8(0x40); // Mask for checking uppercase\n    __m128i lower_mask = _mm_set1_epi8(0x20); // Mask for checking lowercase\n\n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n\n        if(key.empty()) continue;\n\n        const char *str = key.c_str();\n        size_t len = key.length();\n\n        while(len >= 16){\n            __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(str));\n            __m128i upper_check = _mm_cmpeq_epi8(_mm_and_si128(vec, upper_mask), _mm_setzero_si128());\n            __m128i lower_check = _mm_cmpeq_epi8(_mm_and_si128(vec, lower_mask), _mm_setzero_si128());\n\n            int upper_mask_all = _mm_movemask_epi8(upper_check);\n            int lower_mask_all = _mm_movemask_epi8(lower_check);\n\n            if(upper_mask_all != 0xFFFF){\n                all_upper = false;\n            }\n            if(lower_mask_all != 0xFFFF){\n                all_lower = false;\n            }\n\n            str += 16;\n            len -= 16;\n        }\n\n        for(size_t i = 0; i < len; ++i){\n            char c = str[i];\n            if(c >= 'A' && c <= 'Z') {\n                all_lower = false;\n            }\n            else if(c >= 'a' && c <= 'z') {\n                all_upper = false;\n            }\n            else{\n                all_upper = false;\n                all_lower = false;\n                break;\n            }\n        }\n\n        if(!all_upper && !all_lower){\n            return false;\n        }\n    }\n\n    return all_upper || all_lower;\n}\n```"}
{"task_id": "SimdBench_107_SSE", "completion": "```cpp\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    bool all_upper = true;\n    bool all_lower = true;\n\n    __m128i upper_mask = _mm_set1_epi8(0x40); // Mask for checking uppercase\n    __m128i lower_mask = _mm_set1_epi8(0x20); // Mask for checking lowercase\n\n    for(const auto &pair : dict){\n        const std::string &key = pair.first;\n\n        if(key.size() < 16) continue; // Skip keys shorter than SIMD width\n\n        const char *key_data = key.c_str();\n        __m128i key_vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(key_data));\n\n        __m128i key_upper = _mm_and_si128(key_vec, upper_mask);\n        __m128i key_lower = _mm_and_si128(key_vec, lower_mask);\n\n        __m128i upper_check = _mm_cmpeq_epi8(key_upper, _mm_setzero_si128());\n        __m128i lower_check = _mm_cmpeq_epi8(key_lower, _mm_setzero_si128());\n\n        int upper_mask_all = _mm_movemask_epi8(upper_check);\n        int lower_mask_all = _mm_movemask_epi8(lower_check);\n\n        if(upper_mask_all != 0xFFFF) all_upper = false;\n        if(lower_mask_all != 0xFFFF) all_lower = false;\n\n        if(!all_upper && !all_lower) return false;\n    }\n\n    return all_upper || all_lower;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // Allocate space for up to 4 primes per iteration\n    __m128i sieve = _mm_set_epi32(0,0,0,0);\n    __m128i mask = _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000);\n\n    for(int i=2; i<n; i+=4){\n        __m128i val = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i cmp = _mm_cmpeq_epi32(_mm_and_si128(val, mask), _mm_setzero_si128());\n        sieve = _mm_or_si128(sieve, cmp);\n    }\n\n    int remaining = n % 4;\n    if(remaining > 0){\n        __m128i val = _mm_set_epi32(n-1, n-2, n-3, n-4);\n        __m128i cmp = _mm_cmpeq_epi32(_mm_and_si128(val, mask), _mm_setzero_si128());\n        sieve = _mm_or_si128(sieve, cmp);\n    }\n\n    // Extract primes from sieve\n    for(int i=0; i<4; ++i){\n        if(_mm_extract_epi32(sieve, i)){\n            primes.push_back(i+2);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // Allocate space for up to 4 primes per iteration\n    __m128i sieve = _mm_set_epi32(0,0,0,0);\n    __m128i mask = _mm_set_epi32(1 << 1, 1 << 2, 1 << 3, 1 << 4);\n\n    for(int i = 2; i < n; i += 4){\n        __m128i current = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i cmp = _mm_cmpeq_epi32(_mm_and_si128(sieve, current), _mm_setzero_si128());\n        int mask_bits = _mm_movemask_epi8(cmp);\n        while(mask_bits){\n            int bit_pos = __builtin_ctz(mask_bits);\n            int prime = i + (bit_pos >> 3);\n            primes.push_back(prime);\n            __m128i step = _mm_set1_epi32(prime);\n            sieve = _mm_or_si128(sieve, _mm_slli_epi32(_mm_set1_epi32(1 << (bit_pos & 7)), prime));\n            mask_bits = mask_bits & (mask_bits - 1);\n        }\n    }\n\n    // Handle remaining numbers\n    for(int i = 16; i < n; ++i){\n        if((sieve & (1 << i)) == 0){\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // Allocate space for up to 4 primes per iteration\n    __m128i sieve = _mm_set_epi32(0,0,0,0);\n    __m128i mask = _mm_set_epi32(1 << 3, 1 << 2, 1 << 1, 1 << 0);\n\n    for(int i = 2; i < n; i++){\n        __m128i num = _mm_set1_epi32(i);\n        __m128i test = _mm_and_si128(_mm_mullo_epi32(num, sieve), mask);\n        int mask_result = _mm_movemask_epi8(test);\n        if(mask_result == 0){\n            primes.push_back(i);\n            sieve = _mm_or_si128(sieve, mask);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // Allocate space for 4 boolean values (32 bits total)\n    __m128i sieve = _mm_set_epi32(0,0,0,0);\n    int limit = (n - 1) / 2;\n\n    for(int i = 0; i <= limit; i += 4){\n        int idx = i * 2 + 1;\n        if(idx >= n) break;\n\n        // Load sieve state\n        __m128i mask = _mm_set_epi32(-1,-1,-1,-1);\n\n        // Check if each number is prime\n        __m128i num = _mm_add_epi32(_mm_set1_epi32(idx), _mm_set_epi32(0,1,2,3));\n        __m128i sqrt_n = _mm_set1_epi32((int)sqrt(n));\n        __m128i div = _mm_set_epi32(2,2,2,2);\n        __m128i rem = _mm_setzero_si128();\n\n        while(_mm_movemask_epi8(_mm_cmple_epi32(div, sqrt_n)) != 0){\n            __m128i temp = _mm_mullo_epi32(div, div);\n            __m128i cmp = _mm_cmpeq_epi32(_mm_and_si128(num, temp), temp);\n            rem = _mm_rem_epi32(num, div);\n            mask = _mm_andnot_si128(cmp, mask);\n            div = _mm_add_epi32(div, _mm_set1_epi32(1));\n        }\n\n        // Store primes\n        __m128i prime_mask = _mm_and_si128(mask, _mm_set_epi32(1,1,1,1));\n        int mask_array[4];\n        _mm_storeu_si128((__m128i*)mask_array, prime_mask);\n        for(int j = 0; j < 4; ++j){\n            if(mask_array[j]){\n                primes.push_back(idx + j);\n            }\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n < 2) return primes;\n\n    // Allocate space for up to 4 primes per iteration\n    __m128i sieve = _mm_set_epi32(0,0,0,0);\n    __m128i mask = _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000);\n\n    for(int i=2; i<n; i+=4){\n        __m128i val = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i cmp = _mm_cmpeq_epi32(_mm_and_si128(val, mask), _mm_setzero_si128());\n        sieve = _mm_or_si128(sieve, cmp);\n    }\n\n    // Horizontal sum of sieve bits\n    int sum = _mm_movemask_epi8(sieve);\n    while(sum > 0){\n        int bit = __builtin_ctz(sum);\n        primes.push_back(bit);\n        sum &= sum - 1;\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    int len = s.length();\n\n    __m128i upper_vowels = _mm_set_epi8('U', 'O', 'I', 'E', 'A', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m128i mask_even = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1);\n\n    for(int i = 0; i + 15 < len; i += 16){\n        __m128i chars = _mm_loadu_si128((__m128i*)(data + i));\n        __m128i even_chars = _mm_and_si128(chars, mask_even);\n        __m128i cmp_result = _mm_cmpeq_epi8(even_chars, upper_vowels);\n        int mask = _mm_movemask_epi8(cmp_result);\n        count += __builtin_popcount(mask);\n    }\n\n    for(int i = len - (len % 16); i < len; ++i){\n        if(i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && strchr(\"AEIOU\", data[i])){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    int len = s.length();\n\n    __m128i upper_vowels = _mm_set_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m128i zero = _mm_setzero_si128();\n\n    for(int i = 0; i + 15 < len; i += 16){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i even_chars = _mm_shuffle_epi8(chars, _mm_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, -1, -1, -1, -1, -1, -1, -1, -1));\n        __m128i mask = _mm_cmpeq_epi8(even_chars, upper_vowels);\n        int mask_bits = _mm_movemask_epi8(mask);\n        count += __builtin_popcount(mask_bits);\n    }\n\n    for(int i = len - (len % 16); i < len; ++i){\n        if(i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U')){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = reinterpret_cast<const char*>(s.c_str());\n    size_t len = s.length();\n    \n    __m128i upper_vowels = _mm_set_epi8('U', 'O', 'I', 'O', 'U', 'O', 'I', 'O', 'U', 'O', 'I', 'O', 'U', 'O', 'I', 'O');\n    __m128i zero = _mm_setzero_si128();\n\n    for(size_t i = 0; i + 15 < len; i += 16){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&data[i]));\n        __m128i upper_mask = _mm_cmpeq_epi8(chars, upper_vowels);\n        int mask = _mm_movemask_epi8(upper_mask);\n        count += __builtin_popcount(mask);\n    }\n\n    for(size_t i = len - (len % 16); i < len; ++i){\n        if(i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U')){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    int len = s.length();\n\n    __m128i upper_vowels = _mm_set_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m128i zero = _mm_setzero_si128();\n\n    for(int i = 0; i + 15 < len; i += 16){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i even_chars = _mm_shuffle_epi8(chars, _mm_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, -1, -1, -1, -1, -1, -1, -1, -1));\n        __m128i mask = _mm_cmpeq_epi8(even_chars, upper_vowels);\n        int mask_bits = _mm_movemask_epi8(mask);\n        count += __builtin_popcount(mask_bits);\n    }\n\n    for(int i = len - (len % 16); i < len; ++i){\n        if(i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U')){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_SSE", "completion": "```c\nint count_upper_simd(const std::string & s){\n    int count = 0;\n    const char *data = s.c_str();\n    int len = s.length();\n\n    __m128i upper_vowels = _mm_set_epi8('A', 'E', 'I', 'O', 'U', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m128i zero = _mm_setzero_si128();\n\n    for(int i = 0; i + 15 < len; i += 16){\n        __m128i chars = _mm_loadu_si128(reinterpret_cast<const __m128i*>(data + i));\n        __m128i mask_even = _mm_set_epi8(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n        __m128i chars_even = _mm_and_si128(chars, mask_even);\n\n        __m128i cmp = _mm_cmpeq_epi8(chars_even, upper_vowels);\n        int mask = _mm_movemask_epi8(cmp);\n        count += __builtin_popcount(mask);\n    }\n\n    for(int i = len - (len % 16); i < len; ++i){\n        if(i % 2 == 0 && data[i] >= 'A' && data[i] <= 'Z' && (data[i] == 'A' || data[i] == 'E' || data[i] == 'I' || data[i] == 'O' || data[i] == 'U')){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result;\n    result.reserve(n);\n    \n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_index = _mm_setzero_si128();\n    __m128i v_even = _mm_set1_epi32(2);\n    __m128i v_odd = _mm_set1_epi32(1);\n    \n    int remaining = n;\n    \n    while(remaining >= 4){\n        __m128i v_current = _mm_add_epi32(v_n, _mm_sub_epi32(_mm_add_epi32(v_index, v_index), v_odd));\n        \n        // Store the results\n        __m128i* res_ptr = (__m128i*)&result[0] + result.size() / 4;\n        _mm_storeu_si128(res_ptr, v_current);\n        \n        // Update index and remaining\n        v_index = _mm_add_epi32(v_index, v_even);\n        remaining -= 4;\n    }\n    \n    // Handle remaining elements\n    for(int i=0; i<remaining; ++i){\n        int current = n + (i * 2) + ((n % 2 == 0) ? 0 : 1);\n        result.push_back(current);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result;\n    __m128i n_vec = _mm_set_epi32(n+3, n+2, n+1, n);\n    \n    int remaining = n % 4;\n    int i = 0;\n    \n    // Process full vectors\n    while(i <= n - remaining){\n        __m128i current = n_vec;\n        __m128i next_odd_even = _mm_add_epi32(current, _mm_set1_epi32(2));\n        \n        // Store results\n        __m128i* res_ptr = reinterpret_cast<__m128i*>(&result);\n        _mm_storeu_si128(res_ptr + i, current);\n        \n        // Update n_vec for next iteration\n        n_vec = next_odd_even;\n        i += 4;\n    }\n    \n    // Handle remaining elements\n    for(int j = 0; j < remaining; ++j){\n        int next_val = n + 2*j + 1;\n        result.push_back(next_val);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result;\n    result.reserve(n);\n\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_index = _mm_setzero_si128();\n    __m128i v_even = _mm_set1_epi32(2);\n    __m128i v_odd = _mm_set1_epi32(1);\n\n    while (_mm_movemask_epi8(_mm_cmplt_epi32(v_index, v_n)) != 0){\n        __m128i v_current = _mm_add_epi32(_mm_mullo_epi32(v_index, v_even), v_n);\n        __m128i v_mask = _mm_and_si128(v_n, v_odd);\n        v_current = _mm_add_epi32(v_current, v_mask);\n\n        result.insert(result.end(), (int*)&v_current, (int*)&v_current + 4);\n\n        v_index = _mm_add_epi32(v_index, _mm_set1_epi32(4));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(4);\n    __m128i v_n = _mm_set1_epi32(n);\n    __m128i v_even = _mm_set_epi32(0, 2, 4, 6);\n    __m128i v_odd = _mm_set_epi32(1, 3, 5, 7);\n\n    // Determine if n is even or odd\n    __m128i v_is_even = _mm_and_si128(v_n, _mm_set1_epi32(1));\n    \n    // Select even or odd based on parity\n    __m128i v_next_numbers = _mm_blendv_epi8(v_even, v_odd, v_is_even);\n\n    // Add n to get the next numbers\n    __m128i v_result = _mm_add_epi32(v_n, v_next_numbers);\n\n    // Store the result\n    _mm_storeu_si128((__m128i*)result.data(), v_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result;\n    result.reserve(n);\n\n    __m128i odd = _mm_set_epi32(7, 5, 3, 1);\n    __m128i even = _mm_set_epi32(8, 6, 4, 2);\n    __m128i current = _mm_set_epi32(1, 1, 1, 1);\n    __m128i step_odd = _mm_set_epi32(2, 2, 2, 2);\n    __m128i step_even = _mm_set_epi32(2, 2, 2, 2);\n    int remaining = n;\n\n    while (remaining >= 4){\n        __m128i cmp = _mm_cmpeq_epi32(_mm_and_si128(current, 1), _mm_setzero_si128());\n        __m128i next = _mm_blendv_epi8(even, odd, cmp);\n        result.insert(result.end(), (int*)&next, (int*)&next + 4);\n\n        current = _mm_add_epi32(current, step_odd);\n        odd = _mm_add_epi32(odd, step_odd);\n        even = _mm_add_epi32(even, step_even);\n        remaining -= 4;\n    }\n\n    while (remaining--){\n        int next = (_mm_extract_epi32(current, 0) % 2 == 0) ? _mm_extract_epi32(even, 0) : _mm_extract_epi32(odd, 0);\n        result.push_back(next);\n        current = _mm_add_epi32(current, step_odd);\n        odd = _mm_add_epi32(odd, step_odd);\n        even = _mm_add_epi32(even, step_even);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    \n    __m128i start = _mm_set_epi32(m - 16, m - 12, m - 8, m - 4);\n    __m128i end = _mm_set_epi32(m, m + 4, m + 8, m + 12);\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i = 0; i < 4; ++i){\n        __m128i current = _mm_add_epi32(start, _mm_set1_epi32(i * 4));\n        __m128i mask = _mm_cmpgt_epi32(current, _mm_set1_epi32(n));\n        current = _mm_and_si128(current, mask);\n        sum_vec = _mm_add_epi32(sum_vec, current);\n    }\n    \n    int32_t sum_array[4];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int total_sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    \n    int count = m - n + 1;\n    int avg = n + (total_sum / count);\n    avg += (total_sum % count >= (count / 2));\n    \n    char binary_str[64];\n    int idx = 0;\n    while(avg > 0){\n        binary_str[idx++] = (avg & 1) ? '1' : '0';\n        avg >>= 1;\n    }\n    binary_str[idx] = '\\0';\n    \n    std::reverse(binary_str, binary_str + idx);\n    return std::string(binary_str);\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    \n    __m128i start = _mm_set_epi32(m - 4, m - 3, m - 2, m - 1);\n    __m128i end = _mm_set_epi32(m, m, m, m);\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i = 0; i < ((m - n + 1) + 3) / 4; ++i){\n        __m128i current = _mm_add_epi32(start, end);\n        sum_vec = _mm_add_epi32(sum_vec, current);\n        start = _mm_sub_epi32(start, _mm_set_epi32(4, 4, 4, 4));\n    }\n    \n    int32_t sum_array[4];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int total_sum = 0;\n    for(int i = 0; i < 4; ++i){\n        total_sum += sum_array[i];\n    }\n    \n    int count = m - n + 1;\n    int average = (total_sum + (count / 2)) / count;\n    return std::bitset<32>(average).to_string();\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    \n    __m128i start = _mm_set_epi32(m - 8, m - 4, m, m + 4);\n    __m128i end = _mm_set_epi32(n - 8, n - 4, n, n + 4);\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i = 0; i < ((m - n) / 16) + 1; ++i){\n        __m128i current = _mm_add_epi32(start, _mm_set_epi32(i*4, i*4, i*4, i*4));\n        __m128i mask = _mm_cmpgt_epi32(current, end);\n        sum_vec = _mm_add_epi32(sum_vec, _mm_and_si128(current, mask));\n    }\n    \n    int32_t sum_array[4];\n    _mm_storeu_si128((__m128i*)sum_array, sum_vec);\n    int64_t sum = sum_array[0] + sum_array[1] + sum_array[2] + sum_array[3];\n    int count = ((m - n) / 4) + 1;\n    int avg = (sum + (count / 2)) / count;\n    return std::bitset<32>(avg).to_string();\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    \n    __m128i start = _mm_set_epi32(m - 16, m - 12, m - 8, m - 4);\n    __m128i end = _mm_set_epi32(m, m + 4, m + 8, m + 12);\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i = 0; i < 4; ++i){\n        int current = m - 16 + i * 4;\n        if(current >= n){\n            __m128i range = _mm_add_epi32(start, _mm_set1_epi32(i * 4));\n            __m128i sum = _mm_madd_epi16(range, _mm_set1_epi32(1));\n            sum_vec = _mm_add_epi32(sum_vec, sum);\n        }\n    }\n    \n    int total_sum = _mm_cvtsi128_si32(sum_vec);\n    int count = m - n + 1;\n    int average = total_sum / count;\n    \n    // Rounding logic\n    int remainder = total_sum % count;\n    if(remainder >= count / 2){\n        average += 1;\n    }\n    \n    // Convert to binary string\n    std::string binary = \"\";\n    while(average > 0){\n        binary = std::to_string(average % 2) + binary;\n        average /= 2;\n    }\n    return binary.empty() ? \"0\" : binary;\n}\n```"}
{"task_id": "SimdBench_111_SSE", "completion": "```cpp\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m){\n        return \"-1\";\n    }\n    \n    __m128i start = _mm_set_epi32(m - 16, m - 8, m, m + 8);\n    __m128i end = _mm_set_epi32(n - 16, n - 8, n, n + 8);\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i = 0; i < 4; ++i){\n        int64_t current_start = _mm_extract_epi64(start, i);\n        int64_t current_end = _mm_extract_epi64(end, i);\n        \n        if(current_start > current_end){\n            continue;\n        }\n        \n        __m128i seq = _mm_set_epi64x(0, 0, 0, 0);\n        for(int j = current_start; j <= current_end; j += 4){\n            __m128i vec = _mm_set_epi64x(j + 3, j + 2, j + 1, j);\n            seq = _mm_add_epi64(seq, vec);\n        }\n        sum_vec = _mm_add_epi64(sum_vec, seq);\n    }\n    \n    __m128i count_vec = _mm_set_epi32((m - 16) / 4, (m - 8) / 4, m / 4, (m + 8) / 4);\n    count_vec = _mm_sub_epi32(count_vec, _mm_set_epi32((n - 16) / 4, (n - 8) / 4, n / 4, (n + 8) / 4));\n    \n    __m128i avg_vec = _mm_div_epi64(sum_vec, count_vec);\n    \n    __m128i rounded_avg = _mm_round\u4e0d\u5b9a\u503c\u5904\u7406\u90e8\u5206\u9700\u8981\u8865\u5145\u5b8c\u6574\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m128i factorials = _mm_setzero_si128();\n    __m128i sums = _mm_set_epi32(0, 0, 0, 0);\n    __m128i mask_even = _mm_set_epi32(2, 2, 2, 2);\n    __m128i mask_odd = _mm_set_epi32(1, 1, 1, 1);\n    __m128i modulo = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n\n    for(int i=1; i<n; i+=8){\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        \n        // Calculate factorial for even indices\n        __m128i fact_temp = one;\n        for(int j=1; j<=8; ++j){\n            fact_temp = _mm_mullo_epi32(fact_temp, _mm_add_epi32(indices, _mm_set1_epi32(j-1)));\n        }\n        __m128i fact_mod = _mm_and_si128(fact_temp, modulo);\n        \n        // Calculate sum for odd indices\n        __m128i sum_temp = _mm_add_epi32(indices, one);\n        sum_temp = _mm_sub_epi32(_mm_mullo_epi32(sum_temp, _mm_add_epi32(sum_temp, one)), indices);\n        sum_temp = _mm_div_epi32(sum_temp, two);\n        __m128i sum_mod = _mm_and_si128(sum_temp, modulo);\n        \n        // Select based on even/odd\n        __m128i values = _mm_blendv_epi8(sum_mod, fact_mod, _mm_cmpeq_epi32(indices, mask_even));\n        \n        // Store results\n        _mm_storeu_si128((__m128i*)&result[i], values);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m128i* res = (__m128i*)&result[0];\n    __m128i factorials = _mm_setzero_si128();\n    __m128i sums = _mm_set_epi32(3, 2, 1, 0);\n    __m128i counter = _mm_set_epi32(4, 3, 2, 1);\n    __m128i limit = _mm_set1_epi32(n);\n    __m128i even_mask = _mm_set1_epi32(1);\n    __m128i modulo = _mm_set1_epi32(10000);\n\n    while(_mm_movemask_epi8(_mm_cmplt_epi32(counter, limit)) != 0){\n        __m128i is_even = _mm_and_si128(counter, even_mask);\n        __m128i fact_or_sum = _mm_blendv_epi8(sums, factorials, is_even);\n\n        // Compute factorial for even indices\n        __m128i fact_temp = _mm_mullo_epi32(factorials, counter);\n        __m128i fact_mod = _mm_mullo_epi32(fact_temp, modulo);\n        factorials = _mm_add_epi32(fact_mod, _mm_set1_epi32(1));\n\n        // Compute sum for odd indices\n        __m128i sum_temp = _mm_add_epi32(sums, counter);\n        sums = _mm_add_epi32(sum_temp, _mm_set1_epi32(1));\n\n        // Store the appropriate value based on even/odd\n        __m128i final_val = _mm_blendv_epi8(fact_mod, sum_temp, is_even);\n        _mm_storeu_si128(res++, final_val);\n        \n        // Increment counters\n        counter = _mm_add_epi32(counter, _mm_set1_epi32(4));\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m128i factorials = _mm_setzero_si128();\n    __m128i sums = _mm_set_epi32(0, 1, 2, 3);\n    __m128i mask_even = _mm_set_epi32(0, 0, 0, 1);\n    __m128i mask_odd = _mm_set_epi32(1, 0, 0, 0);\n    __m128i modulo = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i three = _mm_set1_epi32(3);\n    __m128i four = _mm_set1_epi32(4);\n    __m128i five = _mm_set1_epi32(5);\n\n    for(int i=0; i<n; i+=4){\n        __m128i indices = _mm_add_epi32(sums, _mm_set_epi32(i+1, i+2, i+3, i+4));\n        \n        // Calculate factorial for even indices\n        __m128i even_mask = _mm_and_si128(indices, mask_even);\n        __m128i factorial_result = _mm_set1_epi32(1);\n        __m128i current_index = _mm_set1_epi32(1);\n        for(int j=1; j<=4; j++){\n            __m128i cond = _mm_cmpeq_epi32(_mm_and_si128(indices, _mm_set1_epi32(j)), even_mask);\n            __m128i temp = _mm_mullo_epi32(factorial_result, current_index);\n            factorial_result = _mm_blendv_epi8(factorial_result, temp, cond);\n            current_index = _mm_add_epi32(current_index, one);\n        }\n        factorial_result = _mm_mullo_epi32(factorial_result, _mm_set1_epi32(10000));\n        factorial_result = _mm_mod_epi32(factorial_result, modulo);\n\n        // Calculate sum for odd indices\n        __m128i odd_mask = _mm_andnot_si128(mask_even, indices);\n        __m128i sum_result = _mm_setzero_si128();\n        for(int j=1; j<=4; j++){\n            __m128i cond = _mm_cmpeq_epi32(_mm_and_si128(indices, _mm_set1_epi32(j)), odd_mask);\n            sum_result = _mm_add_epi32(sum_result, _mm_sub_epi32(_mm_add_epi32(current_index, one), one));\n        }\n\n        // Combine results\n        __m128i combined = _mm_blendv_epi8(factorial_result, sum_result, mask_even);\n        _mm_storeu_si128((__m128i*)&result[i], combined);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m128i factorials = _mm_setzero_si128();\n    __m128i sums = _mm_set_epi32(0, 1, 2, 3);\n    __m128i mask_even = _mm_set_epi32(0, 1, 0, 1);\n    __m128i mask_odd = _mm_set_epi32(1, 0, 1, 0);\n    __m128i modulo = _mm_set1_epi32(10000);\n\n    for(int i = 1; i <= n; i += 4){\n        __m128i current_index = _mm_set1_epi32(i);\n        __m128i current_factorial = factorials;\n        __m128i current_sum = sums;\n\n        // Compute factorial for even indices\n        __m128i even_mask = _mm_and_si128(current_index, mask_even);\n        __m128i even_factorial = _mm_blendv_epi8(_mm_setzero_si128(), current_factorial, even_mask);\n        even_factorial = _mm_mullo_epi32(even_factorial, current_index);\n        even_factorial = _mm_sub_epi32(current_index, _mm_set1_epi32(1));\n        even_factorial = _mm_mullo_epi32(even_factorial, modulo);\n\n        // Compute sum for odd indices\n        __m128i odd_mask = _mm_and_si128(current_index, mask_odd);\n        __m128i odd_sum = _mm_add_epi32(current_sum, current_index);\n        odd_sum = _mm_sub_epi32(odd_sum, _mm_set1_epi32(1));\n\n        // Combine results\n        factorials = _mm_or_si128(factorials, even_factorial);\n        sums = _mm_or_si128(sums, odd_sum);\n    }\n\n    // Store results back to vector\n    for(int i = 0; i < n; ++i){\n        result[i] = (i % 2 == 0) ? factorials.m128i_i32[i] : sums.m128i_i32[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_SSE", "completion": "```cpp\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m128i factorials = _mm_setzero_si128();\n    __m128i sums = _mm_set_epi32(0, 0, 0, 0);\n    __m128i mask_even = _mm_set_epi32(2, 2, 2, 2);\n    __m128i mask_odd = _mm_set_epi32(1, 1, 1, 1);\n    __m128i mod_val = _mm_set1_epi32(10000);\n    __m128i one = _mm_set1_epi32(1);\n    __m128i two = _mm_set1_epi32(2);\n    __m128i i_val = _mm_set_epi32(4, 3, 2, 1);\n\n    while(i_val[0] <= n){\n        __m128i even_mask = _mm_cmpeq_epi32(_mm_and_si128(i_val, mask_even), mask_even);\n        __m128i odd_mask = _mm_cmpeq_epi32(_mm_and_si128(i_val, mask_odd), mask_odd);\n\n        // Compute factorial for even indices\n        __m128i fact_temp = _mm_mullo_epi32(factorials, i_val);\n        factorials = _mm_add_epi32(fact_temp, one);\n        factorials = _mm_mullo_epi32(factorials, mod_val);\n\n        // Compute sum for odd indices\n        sums = _mm_add_epi32(sums, i_val);\n\n        // Combine results based on parity\n        __m128i combined = _mm_or_si128(_mm_and_si128(even_mask, factorials), _mm_andnot_si128(even_mask, sums));\n\n        // Store results\n        _mm_storeu_si128((__m128i*)&result[i_val[0]-1], combined);\n\n        // Increment i_val by 2 to process next pair\n        i_val = _mm_add_epi32(i_val, two);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    __m128i even_count = _mm_setzero_si128();\n    __m128i odd_count = _mm_setzero_si128();\n\n    for(int i = 1; i <= n; i += 8){\n        __m128i num = _mm_set_epi32(i+7, i+6, i+5, i+4);\n        __m128i mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        even_count = _mm_add_epi32(even_count, _mm_cvtepu32_epi32(mask));\n        odd_count = _mm_add_epi32(odd_count, _mm_sub_epi32(_mm_set1_epi32(1), mask));\n\n        num = _mm_set_epi32(i+3, i+2, i+1, i);\n        mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        even_count = _mm_add_epi32(even_count, _mm_cvtepu32_epi32(mask));\n        odd_count = _mm_add_epi32(odd_count, _mm_sub_epi32(_mm_set1_epi32(1), mask));\n    }\n\n    int counts[4];\n    _mm_storeu_si128((__m128i*)counts, even_count);\n    _mm_storeu_si128((__m128i*)(counts+2), odd_count);\n\n    return {counts[0] + counts[1], counts[2] + counts[3]};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    __m128i even_count = _mm_setzero_si128();\n    __m128i odd_count = _mm_setzero_si128();\n\n    for(int i=1; i<=n; i+=8){\n        __m128i num = _mm_set_epi32(i+7, i+6, i+5, i+4);\n        __m128i mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i temp_even = _mm_and_si128(mask, _mm_set1_epi32(1));\n        __m128i temp_odd = _mm_andnot_si128(mask, _mm_set1_epi32(1));\n\n        even_count = _mm_add_epi32(even_count, temp_even);\n        odd_count = _mm_add_epi32(odd_count, temp_odd);\n\n        num = _mm_set_epi32(i+3, i+2, i+1, i);\n        mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        temp_even = _mm_and_si128(mask, _mm_set1_epi32(1));\n        temp_odd = _mm_andnot_si128(mask, _mm_set1_epi32(1));\n\n        even_count = _mm_add_epi32(even_count, temp_even);\n        odd_count = _mm_add_epi32(odd_count, temp_odd);\n    }\n\n    int counts[8];\n    _mm_storeu_si128((__m128i*)counts, even_count);\n    _mm_storeu_si128((__m128i*)(counts+4), odd_count);\n\n    int total_even = 0, total_odd = 0;\n    for(int i=0;i<8;i++) {\n        total_even += counts[i];\n        total_odd += counts[i+4];\n    }\n\n    return {total_even, total_odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    __m128i even_count = _mm_setzero_si128();\n    __m128i odd_count = _mm_setzero_si128();\n\n    for(int i=1; i<=n; i+=8){\n        __m128i num = _mm_set_epi32(i+7, i+6, i+5, i+4);\n        __m128i mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        even_count = _mm_add_epi32(even_count, _mm_cvtepu32_epi32(mask));\n        odd_count = _mm_add_epi32(odd_count, _mm_sub_epi32(_mm_set1_epi32(1), mask));\n\n        num = _mm_set_epi32(i+3, i+2, i+1, i);\n        mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        even_count = _mm_add_epi32(even_count, _mm_cvtepu32_epi32(mask));\n        odd_count = _mm_add_epi32(odd_count, _mm_sub_epi32(_mm_set1_epi32(1), mask));\n    }\n\n    int counts[8];\n    _mm_storeu_si128((__m128i*)counts, even_count);\n    _mm_storeu_si128((__m128i*)(counts+4), odd_count);\n\n    int even_total = 0, odd_total = 0;\n    for(int i=0; i<8; ++i){\n        even_total += counts[i];\n        odd_total += counts[4+i];\n    }\n\n    return {even_total, odd_total};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    __m128i even_count = _mm_setzero_si128();\n    __m128i odd_count = _mm_setzero_si128();\n\n    for(int i=1; i<=n; i+=8){\n        __m128i num = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        __m128i temp_even = _mm_and_si128(mask, _mm_set1_epi32(1));\n        __m128i temp_odd = _mm_andnot_si128(mask, _mm_set1_epi32(1));\n\n        even_count = _mm_add_epi32(even_count, temp_even);\n        odd_count = _mm_add_epi32(odd_count, temp_odd);\n    }\n\n    int counts[8];\n    _mm_storeu_si128((__m128i*)counts, even_count);\n    int total_even = counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    _mm_storeu_si128((__m128i*)counts, odd_count);\n    int total_odd = counts[0] + counts[1] + counts[2] + counts[3] + counts[4] + counts[5] + counts[6] + counts[7];\n\n    return {total_even, total_odd};\n}\n```"}
{"task_id": "SimdBench_113_SSE", "completion": "```cpp\nstd::vector<int> even_odd_palindrome_simd(int n){\n    __m128i even_count = _mm_setzero_si128();\n    __m128i odd_count = _mm_setzero_si128();\n\n    for(int i = 1; i <= n; i += 8){\n        __m128i num = _mm_set_epi32(i+7, i+6, i+5, i+4);\n        __m128i mask = _mm_cmpeq_epi32(_mm_and_si128(num, _mm_set1_epi32(1)), _mm_setzero_si128());\n        even_count = _mm_add_epi32(even_count, _mm_castps_si128(_mm_and_ps(_mm_castsi128_ps(mask), _mm_set1_ps(1))));\n        odd_count = _mm_add_epi32(odd_count, _mm_castps_si128(_mm_andnot_ps(_mm_castsi128_ps(mask), _mm_set1_ps(1))));\n    }\n\n    int even[4] = {0};\n    int odd[4] = {0};\n\n    _mm_storeu_si128((__m128i*)even, even_count);\n    _mm_storeu_si128((__m128i*)odd, odd_count);\n\n    return {even[0] + even[1] + even[2] + even[3], odd[0] + odd[1] + odd[2] + odd[3]};\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```c\nint count_nums_simd(std::vector<int> n){\n    const int vec_size = 4; // SSE processes 4 single-precision floats at once, use int size instead\n    const __m128i zero = _mm_setzero_si128();\n    int count = 0;\n    int i = 0;\n\n    // Process in chunks of 4\n    for (; i <= n.size() - vec_size; i += vec_size){\n        __m128i vec = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i mask = _mm_cmpgt_epi32(abs_vec, zero);\n        count += _mm_movemask_epi8(mask);\n    }\n\n    // Handle remaining elements\n    for (; i < n.size(); i++){\n        if (abs(n[i]) > 0){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```c\nint count_nums_simd(std::vector<int> n){\n    const int vec_size = 4;\n    __m128i threshold = _mm_set1_epi32(0);\n    __m128i count_vec = _mm_setzero_si128();\n    int count = 0;\n\n    for(size_t i = 0; i < n.size(); i += vec_size){\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_nums = _mm_abs_epi32(nums);\n        __m128i sums = _mm_sad_epu8(_mm_setzero_si128(), _mm_cvtepi32_epi8(abs_nums));\n        __m128i cmp_res = _mm_cmpgt_epi32(sums, threshold);\n        count_vec = _mm_add_epi32(count_vec, cmp_res);\n    }\n\n    int* count_arr = (int*)&count_vec;\n    count += count_arr[0];\n    count += count_arr[1];\n    count += count_arr[2];\n    count += count_arr[3];\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```cpp\nint count_nums_simd(std::vector<int> n){\n    const int vec_size = 4; // SSE processes 4 single-precision floats at once, use int size instead\n    const int size = n.size();\n    __m128i threshold = _mm_set1_epi32(0);\n    __m128i count_vec = _mm_setzero_si128();\n    int total_count = 0;\n\n    for(int i=0; i<size; i+=vec_size){\n        __m128i data_vec;\n        if(i+vec_size <= size){\n            data_vec = _mm_loadu_si128((__m128i*)&n[i]);\n        }\n        else{\n            int remaining = size - i;\n            data_vec = _mm_setr_epi32(\n                (remaining >=3)?n[i+2]:0,\n                (remaining >=2)?n[i+1]:0,\n                (remaining >=1)?n[i+0]:0,\n                0\n            );\n        }\n\n        __m128i abs_data_vec = _mm_abs_epi32(data_vec);\n        __m128i digit_sum_vec = _mm_setzero_si128();\n\n        // Compute digit sums for each element in the vector\n        for(int d=0; d<4; ++d){\n            __m128i temp = abs_data_vec;\n            int mask = 0x0F;\n            __m128i sum = _mm_setzero_si128();\n            for(int s=0; s<8; ++s){\n                __m128i digit = _mm_and_si128(temp, _mm_set1_epi32(mask));\n                sum = _mm_add_epi32(sum, digit);\n                temp = _mm_srli_epi32(temp, 4);\n            }\n            digit_sum_vec = _mm_add_epi32(digit_sum_vec, sum);\n            abs_data_vec = _mm_slli_epi32(abs_data_vec, 32);\n        }\n\n        __m128i compare_result = _mm_cmpgt_epi32(digit_sum_vec, threshold);\n        count_vec = _mm_add_epi32(count_vec, compare_result);\n    }\n\n    int count_array[4];\n    _mm_storeu_si128((__m128i*)count_array, count_vec);\n    for(int j=0; j<vec_size; ++j){\n        total_count += count_array[j];\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```c\nint count_nums_simd(std::vector<int> n){\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    int count = 0;\n    __m128i threshold = _mm_set1_epi32(0);\n    __m128i mask = _mm_set1_epi32(1 << 31);\n\n    if(n.empty()) return count;\n\n    int size = n.size();\n    int i = 0;\n\n    // Process full vectors\n    for(; i <= size - vec_size; i += vec_size){\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i signs = _mm_and_si128(nums, mask);\n        __m128i abs_nums = _mm_sub_epi32(nums, signs);\n        __m128i sums = _mm_add_epi32(abs_nums, signs);\n        __m128i gt_zero = _mm_cmpgt_epi32(sums, threshold);\n        count += _mm_movemask_epi8(gt_zero);\n    }\n\n    // Handle remaining elements\n    for(; i < size; i++){\n        int num = n[i];\n        int sum = 0;\n        int temp = abs(num);\n        while(temp > 0){\n            sum += temp % 10;\n            temp /= 10;\n        }\n        if((num < 0 && sum - (num % 10) > 0) || (num >= 0 && sum > 0)){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_114_SSE", "completion": "```c\nint count_nums_simd(std::vector<int> n){\n    const int vec_size = 4;\n    __m128i threshold = _mm_set1_epi32(0);\n    __m128i count_vec = _mm_setzero_si128();\n    int total_count = 0;\n    \n    for(size_t i = 0; i < n.size(); i += vec_size){\n        __m128i nums = _mm_loadu_si128((__m128i*)&n[i]);\n        __m128i abs_nums = _mm_abs_epi32(nums);\n        __m128i sums = _mm_sad_epu8(_mm_setzero_si128(), abs_nums);\n        __m128i cmp_res = _mm_cmpgt_epi32(sums, threshold);\n        count_vec = _mm_add_epi32(count_vec, cmp_res);\n    }\n    \n    int count_arr[vec_size];\n    _mm_storeu_si128((__m128i*)count_arr, count_vec);\n    for(int j = 0; j < vec_size; ++j){\n        total_count += count_arr[j];\n    }\n    \n    return total_count;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n    \n    __m128i* vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_full_vectors = n / 4;\n\n    // Load first and last vectors\n    __m128i first_vector = _mm_loadu_si128(vec);\n    __m128i last_vector = _mm_loadu_si128(vec + num_full_vectors - 1);\n\n    // Compare first and last vectors\n    __m128i cmp_result = _mm_cmpgt_epi32(first_vector, last_vector);\n\n    // Horizontal compare across all elements\n    int mask = _mm_movemask_epi8(cmp_result);\n\n    // Check if any element in first vector is greater than any element in last vector\n    if(mask != 0){\n        // Additional checks for remaining elements if n % 4 != 0\n        for(int i = num_full_vectors * 4; i < n; ++i){\n            if(arr[i] > arr[i - (n - 4)]){\n                return false;\n            }\n        }\n        return true;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n    \n    __m128i* vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_full_vectors = n / 4;\n    bool is_sorted = true;\n\n    for(int i = 0; i < num_full_vectors; ++i){\n        __m128i current = vec[i];\n        __m128i next = vec[(i + 1) % num_full_vectors];\n        \n        __m128i gt_mask = _mm_cmpgt_epi32(current, next);\n        int mask = _mm_movemask_epi8(gt_mask);\n        if(mask != 0){\n            is_sorted = false;\n            break;\n        }\n    }\n\n    if(is_sorted) return true;\n\n    // Check remaining elements if any\n    for(int i = num_full_vectors * 4; i < n - 1; ++i){\n        if(arr[i] > arr[i + 1]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n    \n    __m128i* vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_full_vectors = n / 4;\n    bool has_been_decreasing = false;\n\n    for(int i = 0; i < num_full_vectors; ++i){\n        __m128i current = vec[i];\n        __m128i next = (i+1 < num_full_vectors) ? vec[i+1] : _mm_setzero_si128();\n\n        __m128i gt_mask = _mm_cmpgt_epi32(current, next);\n        int mask = _mm_movemask_epi8(gt_mask);\n\n        if(mask != 0){\n            if(has_been_decreasing) return false;\n            has_been_decreasing = true;\n        }\n    }\n\n    // Handle remaining elements if n % 4 != 0\n    for(int i = num_full_vectors * 4; i < n - 1; ++i){\n        if(arr[i] > arr[i + 1]){\n            if(has_been_decreasing) return false;\n            has_been_decreasing = true;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n    \n    __m128i* vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_full_vectors = n / 4;\n\n    // Load first vector\n    __m128i prev = _mm_loadu_si128(vec);\n    __m128i curr;\n\n    for(int i = 1; i < num_full_vectors; ++i){\n        curr = _mm_loadu_si128(vec + i);\n        \n        // Check if the current vector is greater than or equal to the previous one\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n        if(!_mm_testz_si128(cmp, cmp)){\n            return false;\n        }\n        prev = curr;\n    }\n\n    // Handle remaining elements if any\n    int remainder = n % 4;\n    if(remainder > 0){\n        __m128i last_elements = _mm_loadu_si128(reinterpret_cast<__m128i*>(arr.data() + n - remainder));\n        __m128i mask = _mm_setr_epi32(-1, -1, -1, 0);\n        __m128i masked_last = _mm_and_si128(last_elements, mask);\n\n        if(!_mm_testz_si128(masked_last, masked_last)){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_SSE", "completion": "```cpp\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n    \n    __m128i* vec = reinterpret_cast<__m128i*>(arr.data());\n    int num_full_vectors = n / 4;\n    bool has_break = false;\n\n    for(int i = 0; i < num_full_vectors; ++i){\n        __m128i current = vec[i];\n        __m128i next = vec[(i + 1) % num_full_vectors];\n        \n        __m128i gt_mask = _mm_cmpgt_epi32(current, next);\n        int mask = _mm_movemask_epi8(gt_mask);\n        if(mask != 0){\n            has_break = true;\n            break;\n        }\n    }\n\n    if(has_break){\n        return false;\n    }\n    else{\n        return true;\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    __m128i mask1 = _mm_set1_epi32(0);\n    __m128i mask2 = _mm_set1_epi32(0);\n\n    for(int i=0; i<n/4; i++){\n        __m128i v1 = _mm_loadu_si128((__m128i*)&lst1[i*4]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&lst2[i*4]);\n\n        __m128i even1 = _mm_and_si128(v1, _mm_set1_epi32(1));\n        __m128i even2 = _mm_and_si128(v2, _mm_set1_epi32(1));\n\n        mask1 = _mm_or_si128(mask1, even1);\n        mask2 = _mm_or_si128(mask2, even2);\n    }\n\n    int result1 = _mm_movemask_epi8(mask1);\n    int result2 = _mm_movemask_epi8(mask2);\n\n    if(result1 == 0 && __builtin_popcount(result2) >= __builtin_popcount(result1)){\n        return \"YES\";\n    }\n    else{\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    if(n != lst2.size()){\n        return \"NO\";\n    }\n\n    __m128i mask1 = _mm_set1_epi32(0xAAAAAAAA); // Mask for odd numbers in lst1\n    __m128i mask2 = _mm_set1_epi32(0xAAAAAAAA); // Mask for odd numbers in lst2\n\n    int num_blocks = n / 4;\n    bool has_odd_in_lst1 = false;\n    bool has_even_in_lst2 = false;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i v1 = _mm_loadu_si128((__m128i*)&lst1[i*4]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&lst2[i*4]);\n\n        __m128i odd_mask1 = _mm_and_si128(v1, mask1);\n        __m128i odd_mask2 = _mm_and_si128(v2, mask1);\n\n        if(!_mm_testz_si128(odd_mask1, mask1)){\n            has_odd_in_lst1 = true;\n        }\n        if(!_mm_testz_si128(odd_mask2, mask1)){\n            has_even_in_lst2 = true;\n        }\n    }\n\n    // Handle remaining elements\n    for(int i = num_blocks * 4; i < n; ++i){\n        if(lst1[i] % 2 != 0){\n            has_odd_in_lst1 = true;\n        }\n        if(lst2[i] % 2 == 0){\n            has_even_in_lst2 = true;\n        }\n    }\n\n    if(has_odd_in_lst1 && has_even_in_lst2){\n        return \"YES\";\n    }\n    else{\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    __m128i v_even_mask = _mm_set1_epi32(1); // Mask for checking odd numbers (0th bit set)\n    \n    if(n < 4){\n        // Fallback to scalar for small sizes\n        for(int i = 0; i < n; ++i){\n            if((lst1[i] % 2 != 0) && (lst2[i] % 2 == 0)){\n                continue;\n            }\n            else{\n                return \"NO\";\n            }\n        }\n        return \"YES\";\n    }\n\n    int remaining = n % 4;\n    int i = 0;\n\n    // Process full SSE blocks\n    for(; i <= n - 4; i += 4){\n        __m128i v_lst1 = _mm_loadu_si128((__m128i*)&lst1[i]);\n        __m128i v_lst2 = _mm_loadu_si128((__m128i*)&lst2[i]);\n\n        __m128i v_odd1 = _mm_and_si128(v_lst1, v_even_mask);\n        __m128i v_odd2 = _mm_and_si128(v_lst2, v_even_mask);\n\n        int mask1 = _mm_movemask_epi8(_mm_cmpeq_epi32(v_odd1, v_even_mask));\n        int mask2 = _mm_movemask_epi8(_mm_cmpeq_epi32(v_odd2, v_even_mask));\n\n        if(mask1 != 0 || mask2 != 0){\n            return \"NO\";\n        }\n    }\n\n    // Handle remaining elements\n    for(; i < n; ++i){\n        if((lst1[i] % 2 != 0) && (lst2[i] % 2 == 0)){\n            continue;\n        }\n        else{\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    if(n != lst2.size()){\n        return \"NO\";\n    }\n    \n    __m128i mask1 = _mm_set1_epi32(0xAAAAAAAA); // Mask for odd numbers in lst1\n    __m128i mask2 = _mm_set1_epi32(0xAAAAAAAA); // Mask for odd numbers in lst2\n    \n    int num_blocks = n / 4;\n    bool has_odd_in_lst1 = false;\n    bool has_even_in_lst2 = false;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i v1 = _mm_loadu_si128((__m128i*)&lst1[i*4]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&lst2[i*4]);\n\n        __m128i odd_mask1 = _mm_and_si128(v1, mask1);\n        __m128i odd_mask2 = _mm_and_si128(v2, mask1);\n\n        if(!_mm_testz_si128(odd_mask1, odd_mask1)){\n            has_odd_in_lst1 = true;\n        }\n        if(!_mm_testz_si128(odd_mask2, odd_mask2)){\n            has_even_in_lst2 = true;\n        }\n    }\n\n    // Handle remaining elements\n    for(int i = num_blocks * 4; i < n; ++i){\n        if(lst1[i] % 2 != 0){\n            has_odd_in_lst1 = true;\n        }\n        if(lst2[i] % 2 == 0){\n            has_even_in_lst2 = true;\n        }\n    }\n\n    if(has_odd_in_lst1 && has_even_in_lst2){\n        return \"YES\";\n    }\n    else{\n        return \"NO\";\n    }\n}\n```"}
{"task_id": "SimdBench_116_SSE", "completion": "```cpp\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    if(n != lst2.size()){\n        return \"NO\";\n    }\n    \n    __m128i mask_even = _mm_set_epi32(0x55555555, 0x55555555, 0x55555555, 0x55555555);\n    int num_blocks = n / 4;\n    bool can_exchange = true;\n\n    for(int i = 0; i < num_blocks; ++i){\n        __m128i v1 = _mm_loadu_si128((__m128i*)&lst1[i*4]);\n        __m128i v2 = _mm_loadu_si128((__m128i*)&lst2[i*4]);\n\n        __m128i odd_mask1 = _mm_and_si128(v1, mask_even);\n        __m128i odd_mask2 = _mm_and_si128(v2, mask_even);\n\n        if(!_mm_testz_si128(odd_mask1, odd_mask2)){\n            can_exchange = false;\n            break;\n        }\n    }\n\n    // Handle remaining elements\n    for(int i = num_blocks * 4; i < n; ++i){\n        if((lst1[i] & 1) && (lst2[i] & 1)){\n            can_exchange = false;\n            break;\n        }\n    }\n\n    return can_exchange ? \"YES\" : \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_117_SSE", "completion": "```cpp\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    // Implementation goes here\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if(nums.empty()) return 0;\n    size_t n = nums.size();\n    __m128i min_sum = _mm_set1_epi64x(INT64_MAX);\n    __m128i current_sum = _mm_setzero_si128();\n    \n    for(size_t i = 0; i + 2 <= n; i += 2){\n        __m128i num_vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        current_sum = _mm_add_epi64(current_sum, num_vec);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n    \n    int64_t remaining_min = INT64_MAX;\n    for(size_t i = n - (n % 2); i < n; ++i){\n        remaining_min = std::min(remaining_min, static_cast<int64_t>(nums[i]));\n    }\n    \n    __m128i min_remaining = _mm_set1_epi64x(remaining_min);\n    min_sum = _mm_min_epi64(min_sum, min_remaining);\n    \n    int64_t result[2];\n    _mm_storeu_si128((__m128i*)result, min_sum);\n    return result[0] < result[1] ? result[0] : result[1];\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if(nums.empty()) return 0;\n    size_t n = nums.size();\n    __m128i min_sum = _mm_set1_epi64x(INT64_MAX);\n    __m128i current_sum = _mm_setzero_si128();\n    \n    for(size_t i = 0; i + 2 <= n; i += 2){\n        __m128i num_vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        current_sum = _mm_add_epi64(current_sum, num_vec);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n    \n    int64_t remaining_min = INT64_MAX;\n    for(size_t i = n - (n % 2); i < n; ++i){\n        remaining_min = std::min(remaining_min, static_cast<int64_t>(nums[i]));\n    }\n    \n    __m128i min_sum_vec = _mm_shuffle_epi32(min_sum, _MM_SHUFFLE(1,0,1,0));\n    min_sum = _mm_min_epi64(min_sum, min_sum_vec);\n    min_sum_vec = _mm_shuffle_epi32(min_sum, _MM_SHUFFLE(0,0,0,0));\n    min_sum = _mm_min_epi64(min_sum, min_sum_vec);\n    \n    int64_t result = _mm_cvtsi128_si64(min_sum);\n    return result < remaining_min ? result : remaining_min;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if(nums.empty()) return 0;\n    int n = nums.size();\n    __m128i min_sum = _mm_set1_epi64x(INT64_MAX);\n    __m128i current_sum = _mm_setzero_si128();\n    \n    for(int i=0; i<n; i+=2){\n        __m128i num1 = _mm_set_epi64x(0, nums[i]);\n        __m128i num2 = i+1 < n ? _mm_set_epi64x(0, nums[i+1]) : _mm_setzero_si128();\n        \n        current_sum = _mm_add_epi64(current_sum, num1);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n        \n        current_sum = _mm_add_epi64(current_sum, num2);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n    \n    int64_t min_sum_scalar = INT64_MAX;\n    for(int i=0; i<2; ++i){\n        int64_t temp;\n        _mm_storel_epi64((__m128i*)&temp, _mm_shuffle_epi32(min_sum, i*8));\n        min_sum_scalar = std::min(min_sum_scalar, temp);\n    }\n    \n    return min_sum_scalar;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if(nums.empty()) return 0;\n    int n = nums.size();\n    __m128i min_sum = _mm_set1_epi64x(INT64_MAX);\n    __m128i current_sum = _mm_setzero_si128();\n    \n    for(int i=0; i<n; i+=2){\n        __m128i num1 = _mm_set1_epi64x(nums[i]);\n        __m128i num2 = i+1 < n ? _mm_set1_epi64x(nums[i+1]) : _mm_setzero_si128();\n        \n        current_sum = _mm_add_epi64(current_sum, num1);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n        \n        current_sum = _mm_add_epi64(current_sum, num2);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n    \n    int64_t result = INT64_MAX;\n    for(int i=0; i<2; ++i){\n        __m128i lane = _mm_shuffle_epi32(min_sum, _MM_SHUFFLE(i,i,i,i));\n        int64_t val;\n        _mm_storel_epi64((__m128i*)&val, lane);\n        result = std::min(result, val);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_SSE", "completion": "```cpp\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    if(nums.empty()) return 0;\n    size_t n = nums.size();\n    __m128i min_sum = _mm_set1_epi64x(INT64_MAX);\n    __m128i current_sum = _mm_setzero_si128();\n    \n    for(size_t i = 0; i + 2 <= n; i += 2){\n        __m128i num_vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        current_sum = _mm_add_epi64(current_sum, num_vec);\n        min_sum = _mm_min_epi64(min_sum, current_sum);\n    }\n    \n    int64_t remaining_min = INT64_MAX;\n    for(size_t i = n - (n % 2); i < n; ++i){\n        remaining_min = std::min(remaining_min, static_cast<int64_t>(nums[i]));\n    }\n    \n    __m128i min_sum_vec = _mm_shuffle_epi32(min_sum, _MM_SHUFFLE(1,0,1,0));\n    min_sum = _mm_min_epi64(min_sum, min_sum_vec);\n    min_sum_vec = _mm_shuffle_epi32(min_sum, _MM_SHUFFLE(0,0,0,0));\n    min_sum = _mm_min_epi64(min_sum, min_sum_vec);\n    \n    return std::min(_mm_cvtsi128_si64(min_sum), remaining_min);\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int total_trips = 0;\n    __m128i bucket_cap = _mm_set1_epi32(capacity);\n    __m128i zero = _mm_setzero_si128();\n    \n    for(const auto & well : grid){\n        __m128i well_vec;\n        __m128i sum = zero;\n        int remaining = capacity;\n        \n        for(int i=0; i < well.size(); i+=4){\n            well_vec = _mm_loadu_si128((__m128i*)&well[i]);\n            sum = _mm_add_epi32(sum, well_vec);\n        }\n        \n        sum = _mm_sad_epu8(zero, sum); // Horizontal sum\n        int total_water = _mm_extract_epi32(sum, 0);\n        \n        total_trips += (total_water + remaining -1)/remaining;\n    }\n    \n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int total_trips = 0;\n    __m128i bucket_cap = _mm_set1_epi32(capacity);\n    __m128i zero = _mm_setzero_si128();\n    \n    for(const auto & well : grid){\n        __m128i well_vec = _mm_loadu_si128((__m128i*)well.data());\n        __m128i remaining = _mm_sub_epi32(bucket_cap, well_vec);\n        \n        // Count the number of non-zero elements\n        __m128i mask = _mm_cmpgt_epi32(remaining, zero);\n        int count = _mm_popcnt_u32(_mm_movemask_epi8(mask));\n        \n        total_trips += ((well.size() + 3) / 4) * 4; // Round up to nearest multiple of 4\n        total_trips -= count;\n    }\n    \n    return (total_trips + capacity - 1) / capacity;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int total_trips = 0;\n    for(const auto & well : grid){\n        __m128i bucket = _mm_setzero_si128();\n        __m128i threshold = _mm_set1_epi32(capacity);\n        int remaining_units = 0;\n        for(int i=0; i<well.size(); i+=4){\n            __m128i well_chunk = _mm_loadu_si128((__m128i*)&well[i]);\n            bucket = _mm_add_epi32(bucket, well_chunk);\n        }\n        while(!_mm_testz_si128(bucket, bucket)){\n            __m128i filled = _mm_min_epi32(bucket, threshold);\n            bucket = _mm_sub_epi32(bucket, filled);\n            total_trips += _mm_cvtsi128_si32(filled);\n        }\n    }\n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity){\n    int total_trips = 0;\n    __m128i bucket_cap = _mm_set1_epi32(capacity);\n    __m128i zero = _mm_setzero_si128();\n    \n    for(const auto & well : grid){\n        __m128i well_vec;\n        __m128i sum = zero;\n        int remaining = capacity;\n        \n        for(int i=0; i < well.size(); i+=4){\n            well_vec = _mm_loadu_si128((__m128i*)&well[i]);\n            sum = _mm_add_epi32(sum, well_vec);\n        }\n        \n        sum = _mm_sad_epu8(sum, zero); // Horizontal sum\n        int total_water = _mm_extract_epi32(sum, 0);\n        \n        total_trips += (total_water + remaining -1)/remaining;\n    }\n    \n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_119_SSE", "completion": "```cpp\nint max_fill_simd(const std::vector<std::vector<int> > & grid, int capacity) {\n    int total_trips = 0;\n    __m128i bucket_cap = _mm_set1_epi32(capacity);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_mask = _mm_set_epi32(4, 3, 2, 1);\n\n    for (const auto & well : grid) {\n        int well_size = well.size();\n        int remaining = well_size % 4 ? 1 : 0;\n        int chunks = well_size / 4 + remaining;\n\n        __m128i current_sum = zero;\n        for (int i = 0; i < chunks - 1; ++i) {\n            __m128i chunk = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&well[i * 4]));\n            current_sum = _mm_add_epi32(current_sum, chunk);\n        }\n\n        if (remaining) {\n            __m128i last_chunk = _mm_set_epi32(0, 0, 0, well[well_size - 1]);\n            current_sum = _mm_add_epi32(current_sum, last_chunk);\n        }\n\n        __m128i sums = _mm_hadd_epi32(current_sum, zero);\n        sums = _mm_hadd_epi32(sums, zero);\n        __m128i final_sum = _mm_shuffle_epi32(sums, _MM_SHUFFLE(0, 0, 0, 0));\n\n        int water_units = _mm_extract_epi32(final_sum, 0);\n        total_trips += (water_units + capacity - 1) / capacity;\n    }\n\n    return total_trips;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    const int vec_size = 4;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask_even = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m128i mask_odd = _mm_set_epi16(1, 0, 1, 0, 1, 0, 1, 0);\n\n    for(size_t i = 0; i + vec_size <= lst.size(); i += vec_size){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i even_masked = _mm_and_si128(vec, mask_even);\n        __m128i odd_masked = _mm_and_si128(vec, mask_odd);\n        __m128i odd_in_even_pos = _mm_or_si128(even_masked, odd_masked);\n        __m128i odd_mask = _mm_packs_epi16(_mm_setzero_si128(), odd_in_even_pos);\n        sum_vec = _mm_add_epi16(sum_vec, odd_mask);\n    }\n\n    __m128i horizontal_sum = _mm_setzero_si128();\n    horizontal_sum = _mm_add_epi16(horizontal_sum, sum_vec);\n    horizontal_sum = _mm_add_epi16(horizontal_sum, _mm_srli_si128(sum_vec, 8));\n    horizontal_sum = _mm_add_epi16(horizontal_sum, _mm_srli_si128(sum_vec, 4));\n    horizontal_sum = _mm_add_epi16(horizontal_sum, _mm_srli_si128(sum_vec, 2));\n\n    int64_t result = _mm_cvtsi128_si32(horizontal_sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i = 0;\n    size_t len = lst.size();\n\n    while(i + 8 <= len){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask_even = _mm_setr_epi16(1, 0, 1, 0, 1, 0, 1, 0);\n        __m128i mask_odd = _mm_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n\n        __m128i even_pos = _mm_and_si128(vec, mask_even);\n        __m128i odd_values = _mm_and_si128(vec, mask_odd);\n\n        __m128i odd_in_even_pos = _mm_and_si128(even_pos, odd_values);\n\n        sum_vec = _mm_add_epi16(sum_vec, odd_in_even_pos);\n\n        i += 8;\n    }\n\n    __m128i sum_horizontal = _mm_setzero_si128();\n    sum_horizontal = _mm_hadd_epi16(sum_vec, sum_horizontal);\n    sum_horizontal = _mm_hadd_epi16(sum_horizontal, sum_horizontal);\n\n    int64_t sum_scalar = _mm_cvtsi128_si64(sum_horizontal);\n    return sum_scalar;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i = 0;\n    while(i + 4 <= lst.size()){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i indices = _mm_set_epi16(4, 3, 2, 1, 0, -1, -2, -3);\n        __m128i masked = _mm_and_si128(vec, _mm_set1_epi16(0x0001));\n        __m128i shifted = _mm_slli_epi16(masked, 1);\n        __m128i even_masked = _mm_andnot_si128(shifted, vec);\n        __m128i odd_masked = _mm_and_si128(vec, shifted);\n        __m128i even_parity = _mm_cmpgt_epi16(indices, _mm_setzero_si128());\n        __m128i even_odd = _mm_or_si128(even_parity, _mm_set1_epi16(0x0002));\n        __m128i final_mask = _mm_and_si128(even_odd, _mm_set1_epi16(0x0003));\n        __m128i filtered = _mm_blendv_epi8(_mm_setzero_si128(), odd_masked, final_mask);\n        sum_vec = _mm_add_epi16(sum_vec, filtered);\n        i += 4;\n    }\n    __m128i sum_horizontal = _mm_hadd_epi16(sum_vec, sum_vec);\n    sum_horizontal = _mm_hadd_epi16(sum_horizontal, sum_horizontal);\n    int64_t sum_scalar = _mm_cvtsi128_si64(sum_horizontal);\n    for(; i < lst.size(); ++i){\n        if(lst[i] % 2 != 0 && i % 2 == 0){\n            sum_scalar += lst[i];\n        }\n    }\n    return sum_scalar;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    __m128i sum_vec = _mm_setzero_si128();\n    size_t i = 0;\n    size_t len = lst.size();\n\n    while(i + 8 <= len){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask_even = _mm_setr_epi16(1, 0, 1, 0, 1, 0, 1, 0);\n        __m128i mask_odd = _mm_setr_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n        __m128i even_pos = _mm_and_si128(vec, mask_even);\n        __m128i odd_values = _mm_and_si128(vec, mask_odd);\n        __m128i odd_mask = _mm_cmpeq_epi16(_mm_and_si128(odd_values, _mm_set1_epi16(1)), _mm_set1_epi16(1));\n        __m128i odd_sum = _mm_and_si128(even_pos, odd_mask);\n        sum_vec = _mm_add_epi16(sum_vec, odd_sum);\n        i += 8;\n    }\n\n    __m128i sum_horizontal = _mm_hadd_epi16(sum_vec, sum_vec);\n    sum_horizontal = _mm_hadd_epi16(sum_horizontal, sum_horizontal);\n    int64_t sum_scalar = _mm_cvtsi128_si64(sum_horizontal);\n\n    for(; i < len; ++i){\n        if(i % 2 == 0 && lst[i] % 2 != 0){\n            sum_scalar += lst[i];\n        }\n    }\n\n    return sum_scalar;\n}\n```"}
{"task_id": "SimdBench_120_SSE", "completion": "```cpp\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    const int vec_size = 4;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i mask_even = _mm_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m128i mask_odd = _mm_set_epi16(1, 0, 1, 0, 1, 0, 1, 0);\n\n    for(size_t i = 0; i + vec_size <= lst.size(); i += vec_size){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n        __m128i even_masked = _mm_and_si128(vec, mask_even);\n        __m128i odd_masked = _mm_and_si128(vec, mask_odd);\n        __m128i odd_masked_shifted = _mm_slli_epi16(odd_masked, 1);\n        __m128i combined = _mm_or_si128(even_masked, odd_masked_shifted);\n        sum_vec = _mm_add_epi16(sum_vec, combined);\n    }\n\n    __m128i sum_horizontal = _mm_setzero_si128();\n    sum_horizontal = _mm_hadd_epi16(sum_vec, sum_horizontal);\n    sum_horizontal = _mm_hadd_epi16(sum_horizontal, sum_horizontal);\n    int64_t sum = _mm_cvtsi128_si64(sum_horizontal);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = std::min(k, (int)arr.size());\n    __m128i mask = _mm_set_epi32(0, 99, 99, 99);\n    __m128i zero = _mm_setzero_si128();\n    for(int i=0; i<limit; i+=4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i masked = _mm_min_epi32(vec, mask);\n        __m128i cmp = _mm_cmpeq_epi32(vec, masked);\n        __m128i valid = _mm_and_si128(cmp, _mm_set1_epi32(1));\n        __m128i sums = _mm_mullo_epi32(valid, vec);\n        sum += _mm_cvtsi128_si32(sums);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int* data = arr.data();\n    int limit = k - 3; // Ensure enough elements for SIMD processing\n\n    __m128i mask = _mm_set_epi32(99, 99, 99, 99); // Mask for checking two digits\n    __m128i zero = _mm_setzero_si128();\n\n    for(int i = 0; i < limit; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&data[i]);\n        __m128i cmp = _mm_cmpgt_epi32(vec, mask); // Compare each element with 99\n        __m128i valid = _mm_andnot_si128(cmp, vec); // Keep only elements <= 99\n        __m128i sums = _mm_add_epi32(valid, valid); // Sum valid elements\n        __m128i high = _mm_shuffle_epi32(sums, _MM_SHUFFLE(2,3,0,1));\n        __m128i sum_pair = _mm_add_epi32(sums, high);\n        __m128i high2 = _mm_shuffle_epi32(sum_pair, _MM_SHUFFLE(1,0,3,2));\n        __m128i final_sum = _mm_add_epi32(sum_pair, high2);\n        sum += _mm_cvtsi128_si32(final_sum);\n    }\n\n    // Handle remaining elements\n    for(int i = limit; i < k; ++i){\n        if(data[i] <= 99){\n            sum += data[i];\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    __m128i mask = _mm_set_epi32(0x000000FF, 0x000000FF, 0x000000FF, 0x000000FF);\n    __m128i sum_vec = _mm_setzero_si128();\n    int sum = 0;\n\n    for(int i = 0; i < k; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i filtered = _mm_and_si128(vec, mask);\n        sum_vec = _mm_add_epi32(sum_vec, filtered);\n    }\n\n    sum = _mm_extract_epi32(sum_vec, 0) + _mm_extract_epi32(sum_vec, 1) +\n          _mm_extract_epi32(sum_vec, 2) + _mm_extract_epi32(sum_vec, 3);\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = std::min(k, (int)arr.size());\n    __m128i mask = _mm_set_epi32(0, 99, 99, 99);\n    __m128i zero = _mm_setzero_si128();\n    for(int i=0; i<limit; i+=4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i masked = _mm_min_epu32(vec, mask);\n        __m128i cmp = _mm_cmpeq_epi32(mask, masked);\n        __m128i valid = _mm_andnot_si128(cmp, vec);\n        sum += _mm_cvtsi128_si32(valid);\n        valid = _mm_srli_si128(valid, 4);\n        sum += _mm_cvtsi128_si32(valid);\n        valid = _mm_srli_si128(valid, 4);\n        sum += _mm_cvtsi128_si32(valid);\n        valid = _mm_srli_si128(valid, 4);\n        sum += _mm_cvtsi128_si32(valid);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_121_SSE", "completion": "```cpp\nint add_elements_simd(std::vector<int> arr, int k){\n    int sum = 0;\n    int limit = std::min(k, (int)arr.size());\n    __m128i mask = _mm_set_epi32(0, 99, 99, 99);\n    __m128i zero = _mm_setzero_si128();\n    for(int i=0; i<limit; i+=4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i masked = _mm_min_epi32(vec, mask);\n        __m128i cmp = _mm_cmpeq_epi32(vec, masked);\n        __m128i valid = _mm_and_si128(cmp, _mm_set1_epi32(1));\n        __m128i sums = _mm_mullo_epi32(valid, vec);\n        __m128i partial_sum = _mm_hadd_epi32(sums, sums);\n        partial_sum = _mm_hadd_epi32(partial_sum, partial_sum);\n        sum += _mm_cvtsi128_si32(partial_sum);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() < 2)\n        return true;\n    \n    int n = lst.size();\n    __m128i prev_block = _mm_set1_epi32(-1); // Initialize with an invalid value\n    __m128i current_block;\n    __m128i duplicates_mask = _mm_setzero_si128();\n    __m128i sorted_mask = _mm_set1_epi32(0);\n    \n    for(int i = 0; i <= n - 4; i += 4){\n        current_block = _mm_loadu_si128((__m128i*)&lst[i]);\n        \n        // Check for duplicates within the block\n        __m128i block_duplicates = _mm_cmpeq_epi32(current_block, prev_block);\n        duplicates_mask = _mm_or_si128(duplicates_mask, block_duplicates);\n        \n        // Check if the current block is sorted\n        __m128i sorted = _mm_cmpgt_epi32(prev_block, current_block);\n        sorted_mask = _mm_or_si128(sorted_mask, sorted);\n        \n        prev_block = current_block;\n    }\n    \n    // Horizontal OR to check for duplicates across all blocks\n    __m128i dup_check = _mm_movemask_epi8(duplicates_mask) != 0;\n    if(dup_check)\n        return false;\n    \n    // Horizontal OR to check for unsorted elements\n    int sorted_check = _mm_movemask_epi8(sorted_mask) != 0;\n    if(sorted_check)\n        return false;\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n    int n = lst.size();\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i current;\n    for(int i = 1; i + 3 < n; i += 4){\n        current = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i cmp_gt = _mm_cmpgt_epi32(current, prev);\n        __m128i cmp_eq = _mm_cmpeq_epi32(current, prev);\n        int mask_gt = _mm_movemask_epi8(cmp_gt);\n        int mask_eq = _mm_movemask_epi8(cmp_eq);\n        if(mask_gt) return false;\n        if(mask_eq > 1) return false;\n        prev = current;\n    }\n    for(int i = n - (n % 4); i < n - 1; ++i){\n        if(lst[i] > lst[i + 1]) return false;\n        if(i < n - 2 && lst[i] == lst[i + 1] && lst[i + 1] == lst[i + 2]) return false;\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n    int n = lst.size();\n    __m128i prev = _mm_set1_epi32(lst[0]);\n    __m128i current;\n    __m128i duplicates = _mm_setzero_si128();\n    for(int i=1; i+1 < n; i+=2){\n        current = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i cmp_prev = _mm_cmpgt_epi32(prev, current);\n        __m128i cmp_curr = _mm_cmpeq_epi32(current, _mm_set1_epi32(lst[i+1]));\n        if(!_mm_testz_si128(cmp_prev, cmp_prev)) return false;\n        duplicates = _mm_or_si128(duplicates, cmp_curr);\n        prev = current;\n    }\n    if(n % 2 != 0){\n        current = _mm_set1_epi32(lst[n-1]);\n        __m128i cmp_prev = _mm_cmpgt_epi32(prev, current);\n        if(!_mm_testz_si128(cmp_prev, cmp_prev)) return false;\n    }\n    return !_mm_movemask_epi8(duplicates);\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n    int n = lst.size();\n    __m128i prev_vec = _mm_set1_epi32(-1); // Initialize with a value less than any integer\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i one_vec = _mm_set1_epi32(1);\n    __m128i two_vec = _mm_set1_epi32(2);\n    \n    for(int i=0; i<n-3; i+=4){\n        __m128i curr_vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        \n        // Check duplicates in the current vector\n        __m128i dup_mask = _mm_cmpeq_epi32(_mm_srli_epi32(curr_vec, 1), curr_vec);\n        int dup_count = _mm_movemask_epi8(dup_mask);\n        if(dup_count != 0) return false;\n        \n        // Check ascending order\n        __m128i gt_mask = _mm_cmpgt_epi32(curr_vec, prev_vec);\n        int gt_count = _mm_movemask_epi8(gt_mask);\n        if(gt_count != 0 && gt_count != 0b1111) return false;\n        \n        prev_vec = curr_vec;\n    }\n    \n    // Handle remaining elements\n    for(int i=n-3; i<n; ++i){\n        if(lst[i] > lst[i+1]) return false;\n        if(i < n-2 && lst[i] == lst[i+1] && lst[i] == lst[i+2]) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_122_SSE", "completion": "```cpp\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n    int n = lst.size();\n    __m128i prev_vec = _mm_set1_epi32(-1); // Initialize with a value less than any integer\n    __m128i zero_vec = _mm_setzero_si128();\n    __m128i one_vec = _mm_set1_epi32(1);\n    __m128i two_vec = _mm_set1_epi32(2);\n    \n    for(int i=0; i<n-3; i+=4){\n        __m128i current_vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        \n        // Check duplicates in the current vector\n        __m128i eq_vec = _mm_cmpeq_epi32(current_vec, prev_vec);\n        int eq_mask = _mm_movemask_epi8(eq_vec);\n        if(eq_mask & (eq_mask >> 1)) return false;\n        \n        // Check ascending order\n        __m128i gt_vec = _mm_cmpgt_epi32(current_vec, prev_vec);\n        int gt_mask = _mm_movemask_epi8(gt_vec);\n        if(gt_mask != 0b1111) return false;\n        \n        prev_vec = current_vec;\n    }\n    \n    // Handle remaining elements\n    for(int i=n-3; i<n; ++i){\n        if(lst[i] > lst[i+1]) return false;\n        if(lst[i] == lst[i+1]) return false;\n    }\n    \n    return true;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i sign_vec = _mm_setzero_si128();\n    int i = 0;\n    for(; i + 4 <= arr.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i sign = _mm_sign_epi32(_mm_set1_epi32(1), vec);\n        sum_vec = _mm_add_epi32(sum_vec, abs_vec);\n        sign_vec = _mm_xor_si128(sign_vec, sign);\n    }\n    __m128i sum_horizontal = _mm_setzero_si128();\n    __m128i sign_horizontal = _mm_setzero_si128();\n    for(int j = 0; j < 4; ++j){\n        __m128i shuf = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(j,j,j,j));\n        __m128i shuf_sign = _mm_shuffle_epi32(sign_vec, _MM_SHUFFLE(j,j,j,j));\n        sum_horizontal = _mm_add_epi32(sum_horizontal, shuf);\n        sign_horizontal = _mm_add_epi32(sign_horizontal, shuf_sign);\n    }\n    int sum_result[4];\n    _mm_storeu_si128((__m128i*)sum_result, sum_horizontal);\n    int sign_result[4];\n    _mm_storeu_si128((__m128i*)sign_result, sign_horizontal);\n    int total_sum = 0;\n    for(int k = 0; k < 4; ++k){\n        total_sum += sum_result[k] * sign_result[k];\n    }\n    for(; i < arr.size(); ++i){\n        total_sum += abs(arr[i]) * prod_sign(arr[i]);\n    }\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i sign_vec = _mm_setzero_si128();\n    int i = 0;\n    for(; i + 4 <= arr.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i sign = _mm_sign_epi32(_mm_set1_epi32(1), vec);\n        sum_vec = _mm_add_epi32(sum_vec, abs_vec);\n        sign_vec = _mm_add_epi32(sign_vec, sign);\n    }\n    __m128i sum_horizontal = _mm_setzero_si128();\n    __m128i sign_horizontal = _mm_setzero_si128();\n    for(int j = 0; j < 4; ++j){\n        __m128i shuffle_sum = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(j,j,j,j));\n        __m128i shuffle_sign = _mm_shuffle_epi32(sign_vec, _MM_SHUFFLE(j,j,j,j));\n        sum_horizontal = _mm_add_epi32(sum_horizontal, shuffle_sum);\n        sign_horizontal = _mm_add_epi32(sign_horizontal, shuffle_sign);\n    }\n    int remaining_sum[4];\n    int remaining_sign[4];\n    _mm_storeu_si128((__m128i*)remaining_sum, sum_horizontal);\n    _mm_storeu_si128((__m128i*)remaining_sign, sign_horizontal);\n    int total_sum = 0;\n    int total_sign = 0;\n    for(int k = i; k < arr.size(); ++k){\n        total_sum += abs(arr[k]);\n        total_sign += sign(arr[k]);\n    }\n    total_sum += remaining_sum[0] + remaining_sum[1] + remaining_sum[2] + remaining_sum[3];\n    total_sign += remaining_sign[0] + remaining_sign[1] + remaining_sign[2] + remaining_sign[3];\n    return total_sum * total_sign;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i sign_vec = _mm_setzero_si128();\n    int i = 0;\n    for(; i + 4 <= arr.size(); i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i sign = _mm_sign_epi32(_mm_set1_epi32(1), vec);\n        sum_vec = _mm_add_epi32(sum_vec, _mm_mullo_epi32(abs_vec, sign));\n        sign_vec = _mm_or_si128(sign_vec, sign);\n    }\n    __m128i sum_horizontal = _mm_setzero_si128();\n    for(int j = 0; j < 4; ++j){\n        __m128i shuf = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(j,j,j,j));\n        __m128i temp = _mm_hadd_epi32(shuf, shuf);\n        temp = _mm_hadd_epi32(temp, temp);\n        sum_horizontal = _mm_add_epi32(sum_horizontal, temp);\n    }\n    int sum_scalar = _mm_cvtsi128_si32(sum_horizontal);\n    __m128i sign_horizontal = _mm_setzero_si128();\n    for(int j = 0; j < 4; ++j){\n        __m128i shuf = _mm_shuffle_epi32(sign_vec, _MM_SHUFFLE(j,j,j,j));\n        sign_horizontal = _mm_or_si128(sign_horizontal, shuf);\n    }\n    int sign_scalar = _mm_cvtsi128_si32(sign_horizontal);\n    for(; i < arr.size(); ++i){\n        sum_scalar += arr[i] * sign_scalar;\n    }\n    return sum_scalar;\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i sign_vec = _mm_setzero_si128();\n    int i = 0;\n    for(; i + 4 <= arr.size(); i += 4){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i sign = _mm_sign_epi32(_mm_set1_epi32(1), vec);\n        sum_vec = _mm_add_epi32(sum_vec, abs_vec);\n        sign_vec = _mm_add_epi32(sign_vec, sign);\n    }\n    __m128i sum_horizontal = _mm_setzero_si128();\n    __m128i sign_horizontal = _mm_setzero_si128();\n    __m128i shuffle_mask = _mm_set_epi8(15, 15, 15, 15, 11, 11, 11, 11, 7, 7, 7, 7, 3, 3, 3, 3);\n    for(int j = 0; j < 4; ++j){\n        __m128i temp_sum = _mm_shuffle_epi8(sum_vec, shuffle_mask);\n        __m128i temp_sign = _mm_shuffle_epi8(sign_vec, shuffle_mask);\n        sum_horizontal = _mm_add_epi32(sum_horizontal, temp_sum);\n        sign_horizontal = _mm_add_epi32(sign_horizontal, temp_sign);\n        shuffle_mask = _mm_shuffle_epi8(shuffle_mask, shuffle_mask);\n    }\n    int remaining_sum[4];\n    int remaining_sign[4];\n    _mm_storeu_si128((__m128i*)remaining_sum, sum_horizontal);\n    _mm_storeu_si128((__m128i*)remaining_sign, sign_horizontal);\n    int total_sum = 0;\n    for(int k = i; k < arr.size(); ++k){\n        total_sum += abs(arr[k]) * sign(arr[k]);\n    }\n    return total_sum + remaining_sum[0] * remaining_sign[0];\n}\n```"}
{"task_id": "SimdBench_123_SSE", "completion": "```cpp\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n    __m128i sum_vec = _mm_setzero_si128();\n    __m128i sign_vec = _mm_setzero_si128();\n    int i = 0;\n    for(; i + 4 <= arr.size(); i += 4){\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&arr[i]));\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        __m128i sign = _mm_sign_epi32(_mm_set1_epi32(1), vec);\n        sum_vec = _mm_add_epi32(sum_vec, abs_vec);\n        sign_vec = _mm_add_epi32(sign_vec, sign);\n    }\n    __m128i sum_horizontal = _mm_setzero_si128();\n    __m128i sign_horizontal = _mm_setzero_si128();\n    for(int j = 0; j < 4; ++j){\n        __m128i shuffle_sum = _mm_shuffle_epi32(sum_vec, _MM_SHUFFLE(j,j,j,j));\n        __m128i shuffle_sign = _mm_shuffle_epi32(sign_vec, _MM_SHUFFLE(j,j,j,j));\n        sum_horizontal = _mm_add_epi32(sum_horizontal, shuffle_sum);\n        sign_horizontal = _mm_add_epi32(sign_horizontal, shuffle_sign);\n    }\n    int remaining_sum[4];\n    int remaining_sign[4];\n    _mm_storeu_si128((__m128i*)remaining_sum, sum_horizontal);\n    _mm_storeu_si128((__m128i*)remaining_sign, sign_horizontal);\n    int total_sum = 0;\n    int total_sign = 0;\n    for(int k = i; k < arr.size(); ++k){\n        total_sum += abs(arr[k]);\n        total_sign += sign(arr[k]);\n    }\n    total_sum += remaining_sum[0] + remaining_sum[1] + remaining_sum[2] + remaining_sum[3];\n    total_sign += remaining_sign[0] + remaining_sign[1] + remaining_sign[2] + remaining_sign[3];\n    return total_sum * total_sign;\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_124_SSE", "completion": "```cpp\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m128i odd_mask = _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0);\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi64x(1);\n\n    while (n != 0){\n        __m128i num = _mm_set1_epi64x(n);\n        __m128i digits = _mm_cvtepu8_epi64(_mm_and_si128(_mm_set1_epi64x(0xFF), num));\n        __m128i is_odd = _mm_cmpeq_epi8(_mm_and_si128(digits, _mm_set1_epi64x(1)), zero);\n        __m128i digit_product = _mm_blendv_epi8(_mm_set1_epi64x(1), digits, is_odd);\n\n        product = _mm_mullo_epi64(product, digit_product);\n\n        n /= 10;\n    }\n\n    return _mm_cvtsi128_si64(product);\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m128i odd_mask = _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0);\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set_epi64x(1, 1);\n\n    for(int i = 0; i < 16; i++){\n        __m128i digit = _mm_cvtsi64_si128(n >> (i * 4));\n        digit = _mm_and_si128(digit, _mm_set1_epi8(0xF));\n        __m128i is_odd = _mm_cmpeq_epi8(_mm_and_si128(digit, _mm_set1_epi8(1)), _mm_set1_epi8(1));\n        __m128i adjusted_digit = _mm_sub_epi8(digit, _mm_andnot_si128(is_odd, _mm_set1_epi8(2)));\n        product = _mm_mullo_epi16(product, adjusted_digit);\n    }\n\n    __m128i result = _mm_shuffle_epi32(product, _MM_SHUFFLE(0,0,0,0));\n    return _mm_cvtsi128_si64(result);\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m128i odd_mask = _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0);\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi64x(1);\n\n    for(int i = 0; i < 16; ++i){\n        __m128i digit = _mm_and_si128(_mm_set1_epi64x(n), _mm_set1_epi64x(0xF));\n        __m128i is_odd = _mm_cmpeq_epi8(_mm_and_si128(digit, _mm_set1_epi64x(1)), _mm_setzero_si128());\n        __m128i digit_value = _mm_add_epi8(digit, _mm_set1_epi64x(48));\n        __m128i char_to_int = _mm_cvtepu8_epi64(_mm_andnot_si128(is_odd, digit_value));\n        product = _mm_mul_epu32(product, char_to_int);\n        n = _mm_srli_epi64(n, 4);\n    }\n\n    __m128i result = _mm_shuffle_epi32(product, _MM_SHUFFLE(0,0,0,0));\n    return _mm_cvtsi128_si64(result);\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m128i odd_mask = _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0);\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi64x(1);\n\n    for(int i = 0; i < 16; ++i){\n        __m128i digit = _mm_and_si128(_mm_set1_epi64x(n), _mm_set1_epi64x(0xF));\n        __m128i is_odd = _mm_cmpeq_epi8(_mm_and_si128(digit, _mm_set1_epi64x(1)), _mm_setzero_si128());\n        __m128i digit_value = _mm_sub_epi8(digit, _mm_andnot_si128(is_odd, _mm_set1_epi64x(2)));\n        __m128i updated_product = _mm_mullo_epi64(product, digit_value);\n        product = _mm_blendv_epi8(product, updated_product, is_odd);\n        n = _mm_srli_epi64(n, 4);\n    }\n\n    __m128i result = _mm_shuffle_epi32(product, _MM_SHUFFLE(0,0,0,0));\n    return _mm_cvtsi128_si64(result);\n}\n```"}
{"task_id": "SimdBench_125_SSE", "completion": "```c\nuint64_t digits_simd(uint64_t n){\n    __m128i odd_mask = _mm_set_epi8(-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0);\n    __m128i zero = _mm_setzero_si128();\n    __m128i product = _mm_set1_epi64x(1);\n\n    while (n != 0){\n        __m128i num = _mm_set1_epi64x(n & 0xFFFFFFFFFFFFFFFF);\n        __m128i digits = _mm_cvtepu8_epi64(_mm_and_si128(num, _mm_set1_epi64x(0xFF)));\n\n        __m128i is_odd = _mm_and_si128(digits, odd_mask);\n        __m128i is_even = _mm_cmpeq_epi64(is_odd, zero);\n\n        __m128i adjusted_digits = _mm_sub_epi64(digits, _mm_and_si128(is_even, _mm_set1_epi64x(2)));\n        __m128i current_product = _mm_shuffle_epi32(product, _MM_SHUFFLE(0,0,0,0));\n\n        current_product = _mm_mul_epu32(current_product, adjusted_digits);\n        product = _mm_shuffle_epi32(current_product, _MM_SHUFFLE(0,0,0,0));\n\n        n >>= 8;\n    }\n\n    return _mm_cvtsi128_si64(product);\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const size_t n = lst.size();\n    __m128 upper_mask = _mm_set1_ps(-0.5f);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n    \n    for(size_t i = 0; i + 4 <= n; i += 4){\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        __m128 rounded = _mm_add_ps(_mm_floor_ps(v), _mm_castsi128_ps(upper_mask));\n        __m128 squared = _mm_mul_ps(rounded, rounded);\n        __m128i int_squared = _mm_cvtps_epi32(squared);\n        sum = _mm_add_epi32(sum, int_squared);\n    }\n    \n    uint64_t partial_sum[4];\n    _mm_storeu_si128((__m128i*)partial_sum, sum);\n    uint64_t total = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3];\n    \n    for(size_t i = n - (n % 4); i < n; ++i){\n        float val = lst[i];\n        int rounded = static_cast<int>(val) + (val > 0 ? 1 : 0);\n        total += rounded * rounded;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const size_t n = lst.size();\n    __m128 upper_mask = _mm_set_ps1(0.999999f);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_vec = _mm_setzero_si128();\n\n    for(size_t i = 0; i + 4 <= n; i += 4){\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        __m128 ceil_v = _mm_add_ps(_mm_floor_ps(v), _mm_cmplt_ss(_mm_setzero_ps(), _mm_sub_ps(v, _mm_floor_ps(v))));\n        __m128i int_v = _mm_cvtps_epi32(ceil_v);\n        __m128i square_v = _mm_mullo_epi32(int_v, int_v);\n        sum_vec = _mm_add_epi32(sum_vec, square_v);\n    }\n\n    sum_vec = _mm_hadd_epi32(sum_vec, zero);\n    sum_vec = _mm_hadd_epi32(sum_vec, zero);\n    sum = _mm_cvtsi128_si32(sum_vec);\n\n    for(size_t i = n - (n % 4); i < n; ++i){\n        float val = lst[i];\n        int ceil_val = static_cast<int>(val) + (val > static_cast<int>(val) ? 1 : 0);\n        sum += ceil_val * ceil_val;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const size_t n = lst.size();\n    __m128 upper_mask = _mm_set1_ps(-0.5f);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum = _mm_setzero_si128();\n    \n    for(size_t i = 0; i + 4 <= n; i += 4){\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        __m128 rounded = _mm_add_ps(_mm_floor_ps(v), upper_mask);\n        __m128 squared = _mm_mul_ps(rounded, rounded);\n        __m128i int_values = _mm_cvtps_epi32(squared);\n        sum = _mm_add_epi32(sum, int_values);\n    }\n    \n    uint64_t partial_sum[4];\n    _mm_storeu_si128((__m128i*)partial_sum, sum);\n    \n    uint64_t total = partial_sum[0] + partial_sum[1] + partial_sum[2] + partial_sum[3];\n    \n    for(size_t i = n - (n % 4); i < n; ++i){\n        float val = lst[i];\n        int rounded = static_cast<int>(std::ceil(val));\n        total += rounded * rounded;\n    }\n    \n    return total;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const size_t n = lst.size();\n    size_t i = 0;\n\n    // Load 4 floats at a time using _mm_loadu_ps\n    for (; i + 4 <= n; i += 4) {\n        __m128 v = _mm_loadu_ps(&lst[i]);\n\n        // Round up each float to the nearest integer\n        __m128i v_rounded = _mm_cvttps_epi32(v);\n        __m128 mask = _mm_cmpgt_ps(_mm_setzero_ps(), v);\n        __m128i mask_i = _mm_castps_si128(mask);\n        __m128i v_ceil = _mm_add_epi32(v_rounded, _mm_and_si128(mask_i, _mm_set1_epi32(1)));\n\n        // Compute squares\n        __m128i v_squared = _mm_mullo_epi32(v_ceil, v_ceil);\n\n        // Horizontal sum\n        __m128i v_sum = _mm_hadd_epi32(v_squared, v_squared);\n        v_sum = _mm_hadd_epi32(v_sum, v_sum);\n        sum += _mm_cvtsi128_si32(v_sum);\n    }\n\n    // Handle remaining elements\n    for (; i < n; ++i) {\n        float x = lst[i];\n        int rounded = (x > 0) ? static_cast<int>(x + 1) : static_cast<int>(x);\n        sum += rounded * rounded;\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_SSE", "completion": "```cpp\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    uint64_t sum = 0;\n    const int n = lst.size();\n    __m128 ceil_mask = _mm_set1_ps(-0.5f);\n    __m128i int_mask = _mm_set1_epi32(0x7FFFFFFF);\n    __m128i zero = _mm_setzero_si128();\n    __m128i sum_vec = _mm_setzero_si128();\n\n    for(int i=0; i<n; i+=4){\n        __m128 v_float = _mm_loadu_ps(&lst[i]);\n        __m128 v_ceil = _mm_add_ps(v_float, ceil_mask);\n        __m128i v_int = _mm_cvtps_epi32(v_ceil);\n        __m128i v_int_abs = _mm_and_si128(v_int, int_mask);\n        __m128i v_sq = _mm_mullo_epi32(v_int_abs, v_int_abs);\n        sum_vec = _mm_add_epi32(sum_vec, v_sq);\n    }\n\n    sum_vec = _mm_hadd_epi32(sum_vec, zero);\n    sum_vec = _mm_hadd_epi32(sum_vec, zero);\n    sum = _mm_cvtsi128_si32(sum_vec);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```c\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    __m128i prev = _mm_set1_epi32(arr[0]);\n    int result = -1;\n    bool found = false;\n\n    for(int i=1; i+1 < n; i+=2){\n        __m128i curr = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n        unsigned char mask = _mm_movemask_epi8(cmp);\n\n        if(mask != 0){\n            found = true;\n            for(int j=0; j<2; ++j){\n                if(mask & (1 << (j*4))){\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n\n        prev = curr;\n    }\n\n    if(!found && n % 2 == 1){\n        if(arr[n-2] > arr[n-1]){\n            result = n-2;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```c\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    __m128i prev = _mm_set1_epi32(arr[0]);\n    int result = -1;\n    bool found = false;\n\n    for(int i=1; i+1 < n; i+=2){\n        __m128i curr = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n        unsigned char mask = _mm_movemask_epi8(cmp);\n\n        if(mask != 0){\n            found = true;\n            for(int j=0; j<2; ++j){\n                if(mask & (1 << (j*4))){\n                    result = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n\n        prev = curr;\n    }\n\n    if(!found && n % 2 == 1){\n        if(arr[n-2] > arr[n-1]){\n            result = n-2;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    int result = -1;\n    __m128i prev = _mm_set1_epi32(arr[0]);\n    __m128i mask = _mm_setzero_si128();\n\n    for(int i=1; i<n-3; i+=4){\n        __m128i curr = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n        mask = _mm_or_si128(mask, cmp);\n\n        if(!_mm_movemask_epi8(cmp)){\n            prev = curr;\n            continue;\n        }\n\n        for(int j=0; j<4; ++j){\n            if(_mm_extract_epi32(cmp, j)){\n                result = i + j;\n                goto end;\n            }\n        }\n        prev = curr;\n    }\n\n    for(int i=n-(n%4); i<n-1; ++i){\n        if(arr[i] >= arr[i+1]){\n            result = i;\n            goto end;\n        }\n    }\n\n    end:\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    if(arr.size() < 2) return -1;\n    \n    __m128i prev = _mm_set1_epi32(arr[0]);\n    int idx = -1;\n    \n    for(size_t i = 1; i + 1 < arr.size(); i += 2){\n        __m128i curr = _mm_loadu_si128((__m128i*)&arr[i]);\n        __m128i cmp = _mm_cmpgt_epi32(prev, curr);\n        \n        unsigned char mask = _mm_movemask_epi8(cmp);\n        \n        if(mask != 0){\n            for(int j = 0; j < 2; ++j){\n                if( (mask >> j) & 1 ){\n                    idx = i + j;\n                    break;\n                }\n            }\n            break;\n        }\n        \n        prev = curr;\n    }\n    \n    if(idx == -1 && arr.size() % 2 == 1){\n        if(arr[arr.size()-1] < arr[arr.size()-2]){\n            idx = arr.size()-1;\n        }\n    }\n    \n    return idx;\n}\n```"}
{"task_id": "SimdBench_127_SSE", "completion": "```cpp\nint can_arrange_simd(const std::vector<int> & arr){\n    const int n = arr.size();\n    if(n < 2) return -1;\n\n    int result = -1;\n    __m128i prev = _mm_set1_epi32(arr[0]);\n    __m128i current;\n    __m128i mask;\n\n    for(int i=1; i<n-3; i+=4){\n        current = _mm_loadu_si128((__m128i*)&arr[i]);\n        mask = _mm_cmpgt_epi32(prev, current);\n        int mask_int = _mm_movemask_epi8(mask);\n        if(mask_int != 0){\n            result = i + __builtin_ctz(mask_int >> 8);\n            break;\n        }\n        prev = current;\n    }\n\n    if(result == -1){\n        for(int i=n-3; i<n; ++i){\n            if(arr[i] > arr[i+1]){\n                return i;\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    std::vector<int> result(2, 0);\n    __m128i max_negative = _mm_setzero_si128();\n    __m128i min_positive = _mm_set1_epi32(0x7FFFFFFF); // Max positive int\n\n    if (lst.size() < vec_size) {\n        return result;\n    }\n\n    for (size_t i = 0; i <= lst.size() - vec_size; i += vec_size) {\n        __m128i vec = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&lst[i]));\n\n        // Find max negative\n        __m128i negatives = _mm_cmpgt_epi32(_mm_setzero_si128(), vec);\n        __m128i current_negatives = _mm_and_si128(vec, negatives);\n        max_negative = _mm_max_epi32(max_negative, current_negatives);\n\n        // Find min positive\n        __m128i positives = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n        __m128i current_positives = _mm_and_si128(vec, positives);\n        min_positive = _mm_min_epi32(min_positive, current_positives);\n    }\n\n    // Horizontal reduction for max_negative and min_positive\n    __m128i shuffle_mask = _mm_set_epi8(12, 13, 14, 15,\n                                        8, 9, 10, 11,\n                                        4, 5, 6, 7,\n                                        0, 1, 2, 3);\n    __m128i max_negative_h = _mm_shuffle_epi8(max_negative, shuffle_mask);\n    __m128i min_positive_h = _mm_shuffle_epi8(min_positive, shuffle_mask);\n\n    max_negative_h = _mm_max_epi32(max_negative_h, _mm_srli_si128(max_negative_h, 4));\n    max_negative_h = _mm_max_epi32(max_negative_h, _mm_srli_si128(max_negative_h, 8));\n    min_positive_h = _mm_min_epi32(min_positive_h, _mm_srli_si128(min_positive_h, 4));\n    min_positive_h = _mm_min_epi32(min_positive_h, _mm_srli_si128(min_positive_h, 8));\n\n    result[0] = _mm_cvtsi128_si32(max_negative_h);\n    result[1] = _mm_cvtsi128_si32(min_positive_h);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    std::vector<int> result(2, 0);\n    if(lst.size() < vec_size) return result;\n\n    __m128i largest_neg = _mm_setzero_si128();\n    __m128i smallest_pos = _mm_setzero_si128();\n\n    for(size_t i = 0; i < lst.size(); i += vec_size){\n        __m128i vec;\n        if(i + vec_size <= lst.size()){\n            vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        }\n        else{\n            vec = _mm_setr_epi32(\n                (i+0 < lst.size()) ? lst[i+0] : 0,\n                (i+1 < lst.size()) ? lst[i+1] : 0,\n                (i+2 < lst.size()) ? lst[i+2] : 0,\n                (i+3 < lst.size()) ? lst[i+3] : 0\n            );\n        }\n\n        __m128i neg_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), vec);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n\n        __m128i neg_vals = _mm_blendv_epi8(_mm_setzero_si128(), vec, neg_mask);\n        __m128i pos_vals = _mm_blendv_epi8(_mm_setzero_si128(), vec, pos_mask);\n\n        largest_neg = _mm_max_epi32(largest_neg, neg_vals);\n        smallest_pos = _mm_min_epi32(smallest_pos, pos_vals);\n    }\n\n    __m128i max_neg = _mm_max_epi32(largest_neg, _mm_shuffle_epi32(largest_neg, _MM_SHUFFLE(0,0,3,2)));\n    max_neg = _mm_max_epi32(max_neg, _mm_shuffle_epi32(max_neg, _MM_SHUFFLE(0,0,1,0)));\n\n    __m128i min_pos = _mm_min_epi32(smallest_pos, _mm_shuffle_epi32(smallest_pos, _MM_SHUFFLE(0,0,3,2)));\n    min_pos = _mm_min_epi32(min_pos, _mm_shuffle_epi32(min_pos, _MM_SHUFFLE(0,0,1,0)));\n\n    int max_neg_arr[4];\n    int min_pos_arr[4];\n    _mm_storeu_si128((__m128i*)max_neg_arr, max_neg);\n    _mm_storeu_si128((__m128i*)min_pos_arr, min_pos);\n\n    result[0] = max_neg_arr[0];\n    result[1] = min_pos_arr[0];\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int n = lst.size();\n    if(n == 0) return {0, 0};\n    \n    __m128i max_neg = _mm_setzero_si128();\n    __m128i min_pos = _mm_set1_epi32(0);\n    \n    for(int i = 0; i < n; i += 4){\n        __m128i vec;\n        if(i + 4 <= n){\n            vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        }\n        else{\n            vec = _mm_setzero_si128();\n            int count = n - i;\n            while(count < 4){\n                _mm_insert_epi32(vec, lst[i + count], count);\n                count++;\n            }\n        }\n        \n        __m128i neg_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), vec);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n        \n        __m128i neg_vals = _mm_and_si128(vec, neg_mask);\n        __m128i pos_vals = _mm_and_si128(vec, pos_mask);\n        \n        max_neg = _mm_max_epi32(max_neg, neg_vals);\n        min_pos = _mm_min_epi32(min_pos, pos_vals);\n    }\n    \n    int results[4] = {0, 0, 0, 0};\n    _mm_storeu_si128((__m128i*)results, max_neg);\n    _mm_storeu_si128((__m128i*)(results + 2), min_pos);\n    \n    return {results[0], results[2]};\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    std::vector<int> result(2, 0);\n    __m128i largest_neg = _mm_setzero_si128();\n    __m128i smallest_pos = _mm_set1_epi32(0);\n\n    if (lst.size() < vec_size) {\n        return result;\n    }\n\n    for (size_t i = 0; i <= lst.size() - vec_size; i += vec_size) {\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i neg_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), vec);\n        __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n\n        __m128i neg_vals = _mm_and_si128(vec, neg_mask);\n        __m128i pos_vals = _mm_and_si128(vec, pos_mask);\n\n        largest_neg = _mm_max_epi32(largest_neg, neg_vals);\n        smallest_pos = _mm_min_epi32(smallest_pos, pos_vals);\n    }\n\n    __m128i largest_neg_scalar = _mm_shuffle_epi32(largest_neg, _MM_SHUFFLE(0,0,0,0));\n    __m128i smallest_pos_scalar = _mm_shuffle_epi32(smallest_pos, _MM_SHUFFLE(0,0,0,0));\n\n    _mm_store_ss((float*)&result[0], _mm_castsi128_ps(largest_neg_scalar));\n    _mm_store_ss((float*)&result[1], _mm_castsi128_ps(smallest_pos_scalar));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    const int vec_size = 4; // SSE operates on 4 integers at a time\n    std::vector<int> result(2, 0);\n    __m128i largest_negative = _mm_setzero_si128();\n    __m128i smallest_positive = _mm_set1_epi32(0x7FFFFFFF); // Max positive int\n\n    if (lst.size() < vec_size) {\n        for (size_t i = 0; i < lst.size(); ++i) {\n            int val = lst[i];\n            if (val < 0 && val > _mm_extract_epi32(largest_negative, 0)) {\n                largest_negative = _mm_set_epi32(0, 0, 0, val);\n            }\n            if (val > 0 && val < _mm_extract_epi32(smallest_positive, 0)) {\n                smallest_positive = _mm_set_epi32(0, 0, 0, val);\n            }\n        }\n    } else {\n        for (size_t i = 0; i < lst.size(); i += vec_size) {\n            __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n            __m128i neg_mask = _mm_cmpgt_epi32(_mm_setzero_si128(), vec);\n            __m128i pos_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si128());\n\n            __m128i neg_vals = _mm_and_si128(vec, neg_mask);\n            __m128i pos_vals = _mm_and_si128(vec, pos_mask);\n\n            largest_negative = _mm_max_epi32(largest_negative, neg_vals);\n            smallest_positive = _mm_min_epi32(smallest_positive, pos_vals);\n        }\n    }\n\n    result[0] = _mm_extract_epi32(largest_negative, 0);\n    result[1] = _mm_extract_epi32(smallest_positive, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m128i result = _mm_set_epi64x(1, 1);\n    for(uint64_t i = 1; i <= n; ++i){\n        __m128i fact = _mm_set1_epi64x(i);\n        for(uint64_t j = 1; j < i; ++j){\n            fact = _mm_mul_epu32(fact, _mm_set1_epi64x(j));\n        }\n        result = _mm_mul_epu32(result, fact);\n    }\n    uint64_t res[2];\n    _mm_storeu_si128((__m128i*)res, result);\n    return res[0] * res[1];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m128i result_vec = _mm_set_epi64x(1, 1);\n    uint64_t result_scalar = 1;\n    \n    for(uint64_t i = 1; i <= n; ++i){\n        uint64_t fact = 1;\n        for(uint64_t j = 1; j <= i; ++j){\n            fact *= j;\n        }\n        __m128i fact_vec = _mm_set1_epi64x(fact);\n        result_vec = _mm_mul_epu32(result_vec, fact_vec);\n    }\n    \n    result_vec = _mm_add_epi64(_mm_unpackhi_epi64(result_vec, result_vec), result_vec);\n    result_vec = _mm_add_epi64(_mm_unpacklo_epi64(result_vec, result_vec), _mm_srli_si128(result_vec, 8));\n    result_scalar *= _mm_cvtsi128_si64(result_vec);\n    \n    return result_scalar;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m128i result = _mm_set_epi64x(1, 1);\n    for(uint64_t i = 1; i <= n; ++i){\n        __m128i fact = _mm_set1_epi64x(i);\n        for(uint64_t j = 1; j < i; ++j){\n            fact = _mm_mul_epu32(fact, _mm_set1_epi64x(j));\n        }\n        result = _mm_mul_epu32(result, fact);\n    }\n    uint64_t res[2];\n    _mm_storeu_si128((__m128i*)res, result);\n    return res[0] * res[1];\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m128i result = _mm_set_epi64x(1, 1);\n    for(uint64_t i = 1; i <= n; ++i){\n        __m128i fact = _mm_set1_epi64x(i);\n        uint64_t carry = 0;\n        for(int j = 0; j < 2; ++j){\n            __m128i prod = _mm_mul_epu32(result, fact);\n            __m128i high = _mm_srli_si128(prod, 8);\n            __m128i low = _mm_and_si128(prod, _mm_set1_epi64x(0xFFFFFFFF));\n            low = _mm_add_epi64(low, _mm_set_epi64x(carry, 0));\n            carry = _mm_cvtsi128_si64(_mm_srli_si128(high, 8));\n            result = _mm_or_si128(low, high);\n        }\n        result = _mm_add_epi64(result, _mm_set_epi64x(carry, 0));\n    }\n    uint64_t final_result = _mm_cvtsi128_si64(result);\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_SSE", "completion": "```c\nuint64_t special_factorial_simd(uint64_t n){\n    __m128i result = _mm_set_epi64x(1, 1);\n    for(uint64_t i = 1; i <= n; ++i){\n        __m128i fact = _mm_set1_epi64x(1);\n        for(uint64_t j = 1; j <= i; ++j){\n            fact = _mm_mul_epu32(fact, _mm_set1_epi64x(j));\n        }\n        result = _mm_mul_epu32(result, fact);\n    }\n    uint64_t res[2];\n    _mm_storeu_si128((__m128i*)res, result);\n    return res[0] * res[1];\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    __m128i sum = _mm_setzero_si128();\n    \n    for(int i=0; i<n; i+=8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i mask3 = _mm_setr_epi32(i%3==0?1:0, (i+1)%3==0?1:0, (i+2)%3==0?1:0, (i+3)%3==0?1:0);\n        __m128i mask4 = _mm_setr_epi32(i%4==0?1:0, (i+1)%4==0?1:0, (i+2)%4==0?1:0, (i+3)%4==0?1:0);\n        \n        __m128i squared = _mm_mullo_epi32(vec, vec);\n        __m128i cubed = _mm_mullo_epi32(_mm_mullo_epi32(vec, vec), vec);\n        \n        __m128i result = _mm_blendv_epi8(cubed, squared, mask3);\n        result = _mm_blendv_epi8(result, vec, _mm_andnot_si128(mask3, mask4));\n        \n        sum = _mm_add_epi32(sum, result);\n    }\n    \n    int temp[4];\n    _mm_storeu_si128((__m128i*)temp, sum);\n    int total_sum = temp[0] + temp[1] + temp[2] + temp[3];\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    __m128i sum = _mm_setzero_si128();\n    \n    for(int i=0; i<n; i+=8){\n        __m128i v = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i indices = _mm_set_epi32(7,6,5,4,3,2,1,0);\n        \n        // Mask for multiples of 3\n        __m128i mask3 = _mm_and_si128(_mm_set1_epi32(3), _mm_srli_epi32(indices, 2));\n        __m128i mask3_result = _mm_blendv_epi8(v, _mm_mullo_epi32(v, v), mask3);\n        \n        // Mask for multiples of 4 but not 3\n        __m128i mask4 = _mm_and_si128(_mm_set1_epi32(4), _mm_srli_epi32(indices, 2));\n        __m128i mask4_not3 = _mm_andnot_si128(mask3, mask4);\n        __m128i mask4_result = _mm_blendv_epi8(mask3_result, _mm_mullo_epi32(v, _mm_mullo_epi32(v, v)), mask4_not3);\n        \n        sum = _mm_add_epi32(sum, mask4_result);\n    }\n    \n    int result[4];\n    _mm_storeu_si128((__m128i*)result, sum);\n    int total = result[0] + result[1] + result[2] + result[3];\n    return total;\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    __m128i sum = _mm_setzero_si128();\n    \n    for(int i=0; i<n; i+=8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i indices = _mm_set_epi32(7,6,5,4,3,2,1,0);\n        \n        // Mask for multiples of 3\n        __m128i mask3 = _mm_and_si128(indices, _mm_set1_epi32(3));\n        __m128i isMultipleOf3 = _mm_cmpeq_epi32(mask3, _mm_setzero_si128());\n        \n        // Mask for multiples of 4\n        __m128i mask4 = _mm_and_si128(indices, _mm_set1_epi32(4));\n        __m128i isMultipleOf4 = _mm_cmpeq_epi32(mask4, _mm_setzero_si128());\n        \n        // Combine masks: Multiple of 4 but not 3\n        __m128i isMultipleOf4Not3 = _mm_andnot_si128(isMultipleOf3, isMultipleOf4);\n        \n        // Square elements where index is multiple of 3\n        __m128i squared = _mm_mullo_epi32(vec, vec);\n        __m128i result1 = _mm_blendv_epi8(vec, squared, isMultipleOf3);\n        \n        // Cube elements where index is multiple of 4 but not 3\n        __m128i cubed = _mm_mullo_epi32(_mm_mullo_epi32(result1, result1), result1);\n        __m128i result2 = _mm_blendv_epi8(result1, cubed, isMultipleOf4Not3);\n        \n        // Accumulate results\n        sum = _mm_add_epi32(sum, result2);\n    }\n    \n    // Horizontal sum\n    __m128i shuffle1 = _mm_shuffle_epi32(sum, _MM_SHUFFLE(2,3,0,1));\n    __m128i sum1 = _mm_add_epi32(sum, shuffle1);\n    __m128i shuffle2 = _mm_shuffle_epi32(sum1, _MM_SHUFFLE(0,1,2,3));\n    __m128i sum2 = _mm_add_epi32(sum1, shuffle2);\n    \n    return _mm_cvtsi128_si32(sum2);\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    __m128i sum_vec = _mm_setzero_si128();\n    \n    for(int i=0; i<n; i+=8){\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        __m128i mask3 = _mm_and_si128(_mm_set1_epi32(3), indices);\n        __m128i mask4 = _mm_and_si128(_mm_set1_epi32(4), indices);\n        \n        __m128i values = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i squares = _mm_mul_epi32(values, values);\n        __m128i cubes = _mm_mul_epi32(values, _mm_mullo_epi16(values, values));\n        \n        __m128i cond3 = _mm_cmpeq_epi32(mask3, _mm_setzero_si128());\n        __m128i cond4 = _mm_cmpeq_epi32(mask4, _mm_setzero_si128());\n        __m128i cond4_not3 = _mm_andnot_si128(cond3, cond4);\n        \n        __m128i result = _mm_or_si128(\n            _mm_blendv_epi8(_mm_setzero_si128(), squares, cond3),\n            _mm_blendv_epi8(_mm_setzero_si128(), cubes, cond4_not3)\n        );\n        \n        sum_vec = _mm_add_epi32(sum_vec, result);\n    }\n    \n    __m128i sum_scalar = _mm_hadd_epi32(sum_vec, sum_vec);\n    sum_scalar = _mm_hadd_epi32(sum_scalar, sum_scalar);\n    return _mm_cvtsi128_si32(sum_scalar);\n}\n```"}
{"task_id": "SimdBench_130_SSE", "completion": "```cpp\nint sum_squares_simd(const std::vector<int> & lst){\n    const int n = lst.size();\n    __m128i sum = _mm_setzero_si128();\n    \n    for(int i=0; i<n; i+=8){\n        __m128i vec = _mm_loadu_si128((__m128i*)&lst[i]);\n        __m128i indices = _mm_set_epi32(i+7, i+6, i+5, i+4, i+3, i+2, i+1, i);\n        \n        // Check if index is multiple of 3\n        __m128i mask3 = _mm_and_si128(_mm_set1_epi32(3), _mm_set1_epi32(3));\n        __m128i isMultipleOf3 = _mm_cmpeq_epi32(_mm_and_si128(indices, mask3), _mm_setzero_si128());\n        \n        // Check if index is multiple of 4\n        __m128i mask4 = _mm_set1_epi32(4);\n        __m128i isMultipleOf4 = _mm_cmpeq_epi32(_mm_and_si128(indices, mask4), _mm_setzero_si128());\n        \n        // Compute squares for multiples of 3\n        __m128i squares = _mm_mullo_epi32(vec, vec);\n        squares = _mm_blendv_epi8(squares, vec, isMultipleOf3);\n        \n        // Compute cubes for multiples of 4 but not 3\n        __m128i cubes = _mm_mullo_epi32(_mm_mullo_epi32(vec, vec), vec);\n        cubes = _mm_blendv_epi8(cubes, squares, isMultipleOf4);\n        \n        // Update sum\n        sum = _mm_add_epi32(sum, cubes);\n    }\n    \n    // Horizontal sum\n    __m128i tmp = _mm_hadd_epi32(sum, sum);\n    tmp = _mm_hadd_epi32(tmp, tmp);\n    return _mm_cvtsi128_si32(tmp);\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int n = nums.size();\n    if(n < 4) {\n        for(int i = 0; i < n; ++i){\n            int num = nums[i];\n            if(num > 10 && (abs(num % 10) % 2 == 1) && (abs(num / static_cast<int>(pow(10, floor(log10(abs(num)))))) % 2 == 1)){\n                count++;\n            }\n        }\n        return count;\n    }\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i oddMaskFirstDigit = _mm_set_epi32(9, 7, 5, 3);\n    __m128i oddMaskLastDigit = _mm_set_epi32(9, 7, 5, 3);\n    for(int i = 0; i <= n - 4; i += 4){\n        __m128i v = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i gtThreshold = _mm_cmpgt_epi32(v, threshold);\n        __m128i firstDigitMask = _mm_and_si128(_mm_set1_epi32(1 << 30), v);\n        firstDigitMask = _mm_srl_epi32(firstDigitMask, _mm_set1_epi32(30));\n        firstDigitMask = _mm_cmpeq_epi32(firstDigitMask, oddMaskFirstDigit);\n        __m128i lastDigitMask = _mm_and_si128(_mm_set1_epi32(15), v);\n        lastDigitMask = _mm_cmpeq_epi32(lastDigitMask, oddMaskLastDigit);\n        __m128i combinedMask = _mm_and_si128(gtThreshold, _mm_and_si128(firstDigitMask, lastDigitMask));\n        count += _mm_popcnt_epi32(combinedMask);\n    }\n    for(int i = (n/4)*4; i < n; ++i){\n        int num = nums[i];\n        if(num > 10 && (abs(num % 10) % 2 == 1) && (abs(num / static_cast<int>(pow(10, floor(log10(abs(num)))))) % 2 == 1)){\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 4; // SSE processes 4 integers at a time\n    int i = 0;\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i mask_odd = _mm_setr_epi8(\n        0x01, 0x00, 0x00, 0x00,\n        0x01, 0x00, 0x00, 0x00,\n        0x01, 0x00, 0x00, 0x00,\n        0x01, 0x00, 0x00, 0x00\n    );\n    \n    if(nums.size() < vec_size) {\n        for(auto num : nums){\n            int abs_num = std::abs(num);\n            int first_digit = abs_num;\n            while(first_digit >= 10){\n                first_digit /= 10;\n            }\n            int last_digit = abs_num % 10;\n            if(num > 10 && (first_digit % 2 == 1) && (last_digit % 2 == 1)){\n                count++;\n            }\n        }\n        return count;\n    }\n\n    for(i = 0; i + vec_size <= nums.size(); i += vec_size){\n        __m128i v = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&nums[i]));\n        __m128i gt_mask = _mm_cmpgt_epi32(v, threshold);\n        __m128i abs_v = _mm_abs_epi32(v);\n        __m128i first_digits = _mm_cvtepu32_epi8(_mm_and_si128(abs_v, mask_odd));\n        __m128i last_digits = _mm_and_si128(_mm_cvtepu32_epi8(_mm_srli_epi32(abs_v, 28)), mask_odd);\n        __m128i digit_mask = _mm_and_si128(first_digits, last_digits);\n        __m128i final_mask = _mm_and_si128(gt_mask, _mm_cmpeq_epi8(digit_mask, mask_odd));\n        count += _mm_popcnt_epi32(final_mask).m128i_i32[0];\n    }\n\n    for(; i < nums.size(); ++i){\n        int abs_num = std::abs(nums[i]);\n        int first_digit = abs_num;\n        while(first_digit >= 10){\n            first_digit /= 10;\n        }\n        int last_digit = abs_num % 10;\n        if(nums[i] > 10 && (first_digit % 2 == 1) && (last_digit % 2 == 1)){\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 4; // SSE processes 4 integers at a time\n    int i = 0;\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i odd_digits = _mm_set_epi32(9, 7, 5, 3); // Predefined odd digits for comparison\n\n    // Ensure the vector size is a multiple of 4 for simplicity\n    int n = nums.size();\n    if(n == 0) return 0;\n\n    // Process vectors of 4 elements at a time\n    for(i = 0; i + vec_size <= n; i += vec_size){\n        __m128i v = _mm_loadu_si128((__m128i*)&nums[i]);\n        \n        // Check if number > 10\n        __m128i gt_ten = _mm_cmpgt_epi32(v, threshold);\n\n        // Extract first digit\n        __m128i first_digit = _mm_abs_epi32(_mm_srai_epi32(v, 31));\n        first_digit = _mm_and_si128(first_digit, _mm_set1_epi32(0x7FFF0000));\n        first_digit = _mm_srli_epi32(first_digit, 16);\n\n        // Extract last digit\n        __m128i last_digit = _mm_and_si128(v, _mm_set1_epi32(0xFFFF));\n\n        // Check if both first and last digits are odd\n        __m128i is_first_odd = _mm_cmpeq_epi32(_mm_and_si128(first_digit, odd_digits), odd_digits);\n        __m128i is_last_odd = _mm_cmpeq_epi32(_mm_and_si128(last_digit, odd_digits), odd_digits);\n\n        // Combine all conditions\n        __m128i mask = _mm_and_si128(gt_ten, _mm_and_si128(is_first_odd, is_last_odd));\n        count += _mm_popcnt_epi32(mask).m128i_u32[0];\n    }\n\n    // Handle remaining elements\n    for(; i < n; ++i){\n        if(nums[i] > 10 && (abs(nums[i]) % 10) % 2 != 0 && ((nums[i] >> 16) & 0xFF) % 2 != 0){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int vec_size = 4; // SSE processes 4 integers at a time\n    int i = 0;\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i mask_odd_digits = _mm_set_epi32(0x98765432, 0x98765432, 0x98765432, 0x98765432); // Mask for odd digits\n\n    if(nums.size() < vec_size) {\n        for(auto num : nums){\n            int abs_num = abs(num);\n            int first_digit = abs_num;\n            while(first_digit >= 10){\n                first_digit /= 10;\n            }\n            int last_digit = abs_num % 10;\n            if(num > 10 && (first_digit & 1) && (last_digit & 1)){\n                count++;\n            }\n        }\n        return count;\n    }\n\n    // Process full vectors\n    for(i = 0; i + vec_size <= nums.size(); i += vec_size){\n        __m128i v = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i gt_mask = _mm_cmpgt_epi32(v, threshold);\n        __m128i abs_v = _mm_abs_epi32(v);\n        __m128i first_digits = _mm_cvtepu32_epi32(_mm_and_si128(abs_v, mask_odd_digits));\n        __m128i last_digits = _mm_cvtepu32_epi32(_mm_and_si128(abs_v, _mm_set1_epi32(0xF)));\n        __m128i first_odd_mask = _mm_and_si128(first_digits, _mm_set1_epi32(1));\n        __m128i last_odd_mask = _mm_and_si128(last_digits, _mm_set1_epi32(1));\n        __m128i final_mask = _mm_and_si128(_mm_and_si128(gt_mask, first_odd_mask), last_odd_mask);\n        count += _mm_movemask_epi8(final_mask);\n    }\n\n    // Handle remaining elements\n    for(; i < nums.size(); ++i){\n        int abs_num = abs(nums[i]);\n        int first_digit = abs_num;\n        while(first_digit >= 10){\n            first_digit /= 10;\n        }\n        int last_digit = abs_num % 10;\n        if(nums[i] > 10 && (first_digit & 1) && (last_digit & 1)){\n            count++;\n        }\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_SSE", "completion": "```cpp\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    const int n = nums.size();\n    __m128i threshold = _mm_set1_epi32(10);\n    __m128i odd_digits = _mm_setr_epi32(1, 3, 5, 7, 9, 11, 13, 15);\n\n    for(int i = 0; i + 4 <= n; i += 4){\n        __m128i vec = _mm_loadu_si128((__m128i*)&nums[i]);\n        __m128i gt_mask = _mm_cmpgt_epi32(vec, threshold);\n        __m128i abs_vec = _mm_abs_epi32(vec);\n        \n        __m128i first_digit = _mm_and_si128(abs_vec, _mm_set1_epi32(10000));\n        first_digit = _mm_srli_epi32(first_digit, 16);\n        first_digit = _mm_and_si128(first_digit, _mm_set1_epi32(15));\n\n        __m128i last_digit = _mm_and_si128(abs_vec, _mm_set1_epi32(15));\n\n        __m128i first_odd_mask = _mm_cmpeq_epi32(first_digit, odd_digits);\n        __m128i last_odd_mask = _mm_cmpeq_epi32(last_digit, odd_digits);\n\n        __m128i combined_mask = _mm_and_si128(gt_mask, _mm_and_si128(first_odd_mask, last_odd_mask));\n        count += _mm_popcnt_epi32(combined_mask).m128i_u32[0];\n    }\n\n    for(int i = n - (n % 4); i < n; ++i){\n        if(nums[i] > 10 && (abs(nums[i]) % 10) % 2 == 1 && ((abs(nums[i]) / 10000) % 10) % 2 == 1){\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n    uint64_t count = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i twos = _mm_set1_epi32(2);\n    __m128i threes = _mm_set1_epi32(3);\n    \n    for(uint64_t i=1; i<=n-2; i+=4){\n        __m128i idx = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i val = _mm_sub_epi32(_mm_mullo_epi32(idx, _mm_add_epi32(idx, ones)), idx);\n        val = _mm_add_epi32(val, ones);\n        \n        __m128i mod = _mm_rem_epu32(val, threes);\n        __m128i mask = _mm_cmpeq_epi32(mod, _mm_setzero_si128());\n        count += _mm_popcnt_u32(_mm_movemask_epi8((__m128i)mask));\n    }\n    \n    // Handle remaining elements\n    for(uint64_t i=n-(n%4); i<n-2; ++i){\n        int a_i = i * i - i + 1;\n        if(a_i % 3 == 0) count++;\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n    \n    // Allocate space for a vector of size n\n    __m128i a[16];\n    uint64_t count = 0;\n\n    // Initialize a vector with values i*i - i + 1\n    for(uint64_t i = 0; i < n; i += 16){\n        __m128i indices = _mm_set_epi64x(i+1, i+2);\n        __m128i square = _mm_madd_epi16(indices, indices); // i*i\n        __m128i minus_i = _mm_sub_epi64(square, indices); // i*i - i\n        __m128i plus_one = _mm_add_epi64(minus_i, _mm_set1_epi64x(1)); // i*i - i + 1\n        a[i/16] = plus_one;\n    }\n\n    // Check all combinations of triples\n    for(uint64_t i = 0; i < n-2; i++){\n        for(uint64_t j = i+1; j < n-1; j++){\n            for(uint64_t k = j+1; k < n; k++){\n                __m128i sum = _mm_add_epi64(_mm_add_epi64(a[i], a[j]), a[k]);\n                __m128i mod = _mm_and_si128(sum, _mm_set1_epi64x(3));\n                if(_mm_movemask_epi8(_mm_cmpeq_epi8(mod, _mm_setzero_si128())) == 0xFF){\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n    uint64_t count = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i twos = _mm_set1_epi32(2);\n    __m128i threes = _mm_set1_epi32(3);\n    \n    for(uint64_t i=1; i<=n-2; i+=4){\n        __m128i idx = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i val = _mm_sub_epi32(_mm_mullo_epi32(idx, _mm_add_epi32(idx, ones)), idx);\n        val = _mm_add_epi32(val, ones);\n        \n        __m128i mod = _mm_rem_epu32(val, threes);\n        __m128i mask = _mm_cmpeq_epi32(mod, _mm_setzero_si128());\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```c\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n    uint64_t count = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i twos = _mm_set1_epi32(2);\n    __m128i threes = _mm_set1_epi32(3);\n    __m128i zero = _mm_setzero_si128();\n    \n    for(uint64_t i=1; i<=n-2; i+=4){\n        __m128i idx = _mm_set_epi32(i+3, i+2, i+1, i);\n        __m128i val = _mm_sub_epi32(_mm_mullo_epi32(idx, _mm_add_epi32(idx, ones)), idx);\n        val = _mm_add_epi32(val, ones);\n        \n        __m128i mod = _mm_rem_epu32(val, threes);\n        __m128i mask = _mm_cmpeq_epi32(mod, zero);\n        \n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_SSE", "completion": "```cpp\nuint64_t get_max_triples_simd(uint64_t n){\n    if(n < 3) return 0;\n    uint64_t count = 0;\n    __m128i ones = _mm_set1_epi32(1);\n    __m128i twos = _mm_set1_epi32(2);\n    __m128i threes = _mm_set1_epi32(3);\n    __m128i mask = _mm_set_epi32(3, 2, 1, 0);\n    \n    uint64_t limit = n - 2;\n    for(uint64_t i=0; i < limit; i+=4){\n        __m128i idx = _mm_add_epi32(_mm_set_epi32(i+3, i+2, i+1, i), ones);\n        __m128i val = _mm_sub_epi32(_mm_mullo_epi32(idx, idx), idx);\n        val = _mm_add_epi32(val, ones);\n        \n        __m128i mod_res = _mm_rem_epu32(val, threes);\n        __m128i cmp_eq_zero = _mm_cmpeq_epi32(mod_res, ones);\n        uint32_t eq_mask = _mm_movemask_epi8(cmp_eq_zero);\n        count += __builtin_popcount(eq_mask);\n    }\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m128i primes = _mm_set_epi32(7, 5, 3, 2);\n    __m128i test_vals = _mm_set1_epi32(n);\n    __m128i eq_mask = _mm_cmpeq_epi32(primes, test_vals);\n    int mask = _mm_movemask_epi8(eq_mask);\n\n    if(mask != 0){\n        return x;\n    }\n    else{\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m128i primes = _mm_set_epi32(7, 5, 3, 2);\n    __m128i numbers = _mm_set1_epi32(n);\n    __m128i cmp_result = _mm_cmpeq_epi32(numbers, primes);\n    int mask = _mm_movemask_epi8(cmp_result);\n\n    if (mask != 0){\n        return x;\n    }\n    else{\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m128i primes = _mm_set_epi32(7, 5, 3, 2);\n    __m128i test_vals = _mm_set_epi32(n, n, n, n);\n    __m128i cmp_result = _mm_cmpeq_epi32(test_vals, primes);\n    int mask = _mm_movemask_epi8(cmp_result);\n\n    if (mask != 0){\n        return x;\n    }\n    else{\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m128i primes = _mm_set_epi32(7, 5, 3, 2);\n    __m128i test_vals = _mm_set_epi32(n, n, n, n);\n    __m128i eq_mask = _mm_cmpeq_epi32(test_vals, primes);\n    int mask = _mm_movemask_epi8(eq_mask);\n\n    if(mask != 0){\n        return x;\n    }\n    else{\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_133_SSE", "completion": "```c\nint x_or_y_simd(int n, int x, int y){\n    __m128i primes = _mm_set_epi32(7, 5, 3, 2);\n    __m128i test_vals = _mm_set_epi32(n, n, n, n);\n    __m128i cmp_result = _mm_cmpeq_epi32(test_vals, primes);\n    int mask = _mm_movemask_epi8(cmp_result);\n\n    if (mask != 0){\n        return x;\n    }\n    else{\n        return y;\n    }\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i mask_odd = _mm_set_epi32(0x40000001, 0x40000001, 0x40000001, 0x40000001);\n    uint64_t sum = 0;\n    size_t i = 0;\n    size_t n = lst.size();\n\n    while(i + 4 <= n){\n        __m128 v_float = _mm_loadu_ps(&lst[i]);\n        __m128i v_int = _mm_cvttps_epi32(v_float);\n        __m128i v_abs = _mm_abs_epi32(v_int);\n        __m128i v_odd_mask = _mm_and_si128(_mm_cmpgt_epi32(v_abs, zero), mask_odd);\n\n        __m128i v_square = _mm_mullo_epi32(v_int, v_int);\n        __m128i v_filtered = _mm_and_si128(v_square, v_odd_mask);\n\n        sum += _mm_cvtsi128_si32(v_filtered);\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(v_filtered, 4));\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(v_filtered, 8));\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(v_filtered, 12));\n\n        i += 4;\n    }\n\n    for(; i < n; ++i){\n        float val = lst[i];\n        if(val > 0 && (val == static_cast<int>(val)) && ((static_cast<int>(val) % 2) != 0)){\n            sum += static_cast<uint64_t>(val * val);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i mask_odd = _mm_set_epi32(0x80000003, 0x80000003, 0x80000003, 0x80000003);\n    uint64_t sum = 0;\n\n    if(lst.empty()) return 0;\n\n    size_t i = 0;\n    for(; i + 4 <= lst.size(); i += 4){\n        __m128 v_float = _mm_loadu_ps(&lst[i]);\n        __m128i v_int = _mm_cvttps_epi32(v_float);\n        __m128i v_abs = _mm_abs_epi32(v_int);\n        __m128i v_odd_mask = _mm_and_si128(v_abs, mask_odd);\n\n        // Check if integer part is odd and positive\n        __m128i v_positive = _mm_cmpgt_epi32(v_abs, zero);\n        __m128i v_odd = _mm_and_si128(v_odd_mask, v_positive);\n\n        // Square the values\n        __m128i v_squared = _mm_mullo_epi32(v_int, v_int);\n\n        // Accumulate only odd values\n        sum += _mm_cvtsi128_si32(_mm_and_si128(v_squared, v_odd));\n    }\n\n    // Handle remaining elements\n    for(; i < lst.size(); ++i){\n        if(static_cast<int>(lst[i]) > 0 && static_cast<int>(lst[i]) % 2 == 1){\n            sum += static_cast<uint64_t>(lst[i] * lst[i]);\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i mask_odd = _mm_set_epi32(0x80000000, 0x80000000, 0x80000000, 0x80000000);\n    uint64_t sum = 0;\n    int i = 0;\n    while(i + 4 <= lst.size()){\n        __m128 v_float = _mm_loadu_ps(&lst[i]);\n        __m128i v_int = _mm_cvtps_epi32(v_float);\n        __m128i v_abs = _mm_abs_epi32(v_int);\n        __m128i v_odd_mask = _mm_and_si128(v_abs, mask_odd);\n        __m128i v_odd_check = _mm_cmpeq_epi32(v_abs, v_odd_mask);\n        __m128i v_square = _mm_mullo_epi32(v_int, v_int);\n        __m128i v_filtered = _mm_and_si128(v_odd_check, v_square);\n        sum += _mm_cvtsi128_si32(v_filtered);\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(v_filtered, 4));\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(v_filtered, 8));\n        sum += _mm_cvtsi128_si32(_mm_srli_si128(v_filtered, 12));\n        i += 4;\n    }\n    for(; i < lst.size(); ++i){\n        if(lst[i] > 0 && static_cast<int>(lst[i]) == lst[i]){\n            int val = static_cast<int>(lst[i]);\n            if(val & 1){\n                sum += val * val;\n            }\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const __m128i zero = _mm_setzero_si128();\n    uint64_t sum = 0;\n    int i = 0;\n    while(i + 4 <= lst.size()){\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        __m128i int_part = _mm_cvttps_epi32(v);\n        __m128i mask_sign = _mm_cmpgt_epi32(_mm_setzero_si128(), int_part);\n        __m128i mask_neg = _mm_cmpeq_epi32(int_part, _mm_andnot_si128(mask_sign, int_part));\n        __m128i mask_odd = _mm_and_si128(_mm_set1_epi32(1), int_part);\n        __m128i mask_valid = _mm_and_si128(_mm_andnot_si128(mask_sign, mask_neg), mask_odd);\n        __m128i squared = _mm_mul_epi32(mask_valid, mask_valid);\n        sum += _mm_cvtsi128_si32(squared);\n        i += 4;\n    }\n    for(; i < lst.size(); ++i){\n        if(lst[i] > 0 && floorf(lst[i]) == lst[i] && ((int)lst[i] % 2 == 1)){\n            sum += (uint64_t)((int)lst[i]) * ((int)lst[i]);\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_134_SSE", "completion": "```cpp\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    const __m128i zero = _mm_setzero_si128();\n    const __m128i mask_odd = _mm_set_epi32(0x40000003, 0x40000003, 0x40000003, 0x40000003);\n    uint64_t sum = 0;\n    int i = 0;\n    while(i + 4 <= lst.size()){\n        __m128 v = _mm_loadu_ps(&lst[i]);\n        __m128i v_int = _mm_cvttps_epi32(v);\n        __m128i v_abs = _mm_abs_epi32(v_int);\n        __m128i v_odd = _mm_and_si128(v_abs, mask_odd);\n        __m128i v_mask = _mm_cmpeq_epi32(v_int, v_abs);\n        __m128i v_square = _mm_mullo_epi32(v_odd, v_odd);\n        sum += _mm_cvtsi128_si32(_mm_or_si128(v_square, zero));\n        i += 4;\n    }\n    for(; i < lst.size(); ++i){\n        if(lst[i] > 0 && static_cast<int>(lst[i]) == lst[i]){\n            int val = static_cast<int>(lst[i]);\n            if(val & 1){\n                sum += val * val;\n            }\n        }\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    \n    if(n < 8) {\n        for(int i = 0; i < n; ++i){\n            int diff = std::abs(game[i] - guess[i]);\n            result[i] = diff;\n        }\n        return result;\n    }\n\n    __m128i* game_ptr = (__m128i*)&game[0];\n    __m128i* guess_ptr = (__m128i*)&guess[0];\n    __m128i* result_ptr = (__m128i*)&result[0];\n\n    for(int i = 0; i < n / 4; ++i){\n        __m128i g1 = _mm_loadu_si128(game_ptr + i);\n        __m128i g2 = _mm_loadu_si128(guess_ptr + i);\n\n        __m128i diff = _mm_sub_epi32(g1, g2);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128(result_ptr + i, abs_diff);\n    }\n\n    for(int i = n / 4 * 4; i < n; ++i){\n        int diff = std::abs(game[i] - guess[i]);\n        result[i] = diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    \n    if(n < 8) {\n        for(int i = 0; i < n; ++i){\n            int diff = std::abs(game[i] - guess[i]);\n            result[i] = diff;\n        }\n        return result;\n    }\n\n    __m128i* game_ptr = (__m128i*)&game[0];\n    __m128i* guess_ptr = (__m128i*)&guess[0];\n    __m128i* result_ptr = (__m128i*)&result[0];\n\n    for(int i = 0; i < n / 4; ++i){\n        __m128i g1 = _mm_loadu_si128(game_ptr + i);\n        __m128i g2 = _mm_loadu_si128(guess_ptr + i);\n\n        __m128i diff = _mm_sub_epi32(g1, g2);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128(result_ptr + i, abs_diff);\n    }\n\n    for(int i = n / 4 * 4; i < n; ++i){\n        int diff = std::abs(game[i] - guess[i]);\n        result[i] = diff;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    \n    __m128i* game_ptr = (__m128i*)&game[0];\n    __m128i* guess_ptr = (__m128i*)&guess[0];\n    __m128i* result_ptr = (__m128i*)&result[0];\n    \n    int i = 0;\n    for(i = 0; i + 4 <= n; i += 4){\n        __m128i g1 = _mm_loadu_si128(game_ptr + i);\n        __m128i g2 = _mm_loadu_si128(guess_ptr + i);\n        \n        __m128i diff = _mm_sub_epi32(g1, g2);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n        \n        _mm_storeu_si128(result_ptr + i, abs_diff);\n    }\n    \n    for(int j = i; j < n; ++j){\n        result[j] = std::abs(game[j] - guess[j]);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    \n    if(n < 8) {\n        for(int i = 0; i < n; ++i){\n            int diff = std::abs(game[i] - guess[i]);\n            result[i] = diff;\n        }\n        return result;\n    }\n\n    __m128i* game_ptr = (__m128i*)&game[0];\n    __m128i* guess_ptr = (__m128i*)&guess[0];\n    __m128i* result_ptr = (__m128i*)&result[0];\n\n    for(int i = 0; i < n / 4; ++i){\n        __m128i g1 = _mm_loadu_si128(game_ptr + i);\n        __m128i g2 = _mm_loadu_si128(guess_ptr + i);\n\n        __m128i diff = _mm_sub_epi32(g1, g2);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128(result_ptr + i, abs_diff);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_SSE", "completion": "```cpp\n#include <xmmintrin.h>\n#include <emmintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    std::vector<int> result(n, 0);\n    \n    if(n < 8) {\n        for(int i = 0; i < n; ++i){\n            int diff = std::abs(game[i] - guess[i]);\n            result[i] = diff;\n        }\n        return result;\n    }\n\n    __m128i* game_ptr = (__m128i*)&game[0];\n    __m128i* guess_ptr = (__m128i*)&guess[0];\n    __m128i* result_ptr = (__m128i*)&result[0];\n\n    for(int i = 0; i < n / 4; ++i){\n        __m128i g1 = _mm_loadu_si128(game_ptr + i);\n        __m128i g2 = _mm_loadu_si128(guess_ptr + i);\n\n        __m128i diff = _mm_sub_epi32(g1, g2);\n        __m128i abs_diff = _mm_abs_epi32(diff);\n\n        _mm_storeu_si128(result_ptr + i, abs_diff);\n    }\n\n    return result;\n}\n```"}

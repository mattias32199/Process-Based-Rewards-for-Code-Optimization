{"task_id": "SimdBench_0_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i *src_vec = (__m256i *)src;\n    __m256i *dst_vec = (__m256i *)dst;\n    __m256i *mask_vec = (__m256i *)mask;\n\n    while (length >= 32) {\n        __m256i src_block = _mm256_loadu_si256(src_vec++);\n        __m256i dst_block = _mm256_loadu_si256(dst_vec++);\n        __m256i mask_block = _mm256_loadu_si256(mask_vec++);\n\n        __m256i result_block = _mm256_blendv_epi64(src_block, dst_block, mask_block);\n        _mm256_storeu_si256(dst_vec++, result_block);\n        length -= 32;\n    }\n\n    while (length >= 16) {\n        __m128i src_word = _mm_loadl_epi64((__m128i *)src);\n        __m128i dst_word = _mm_loadl_epi64((__m128i *)dst);\n        __m128i mask_word = _mm_loadl_epi64((__m128i *)mask);\n\n        __m128i result_word = _mm_blendv_epi64(src_word, dst_word, mask_word);\n        _mm_storel_epi64((__m128i *)dst, result_word);\n        length -= 16;\n    }\n\n    while (length > 0) {\n        *dst = *src;\n        src++;\n        dst++;\n        length--;\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c++\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i *src_ptr = (__m256i *)src;\n    __m256i *dst_ptr = (__m256i *)dst;\n    __m256i *mask_ptr = (__m256i *)mask;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i src_block = _mm256_loadu_si256(src_ptr + i);\n        __m256i dst_block = _mm256_loadu_si256(dst_ptr + i);\n        __m256i mask_block = _mm256_loadu_si256(mask_ptr + i);\n\n        __m256i masked_block = _mm256_and_si256(mask_block, src_block);\n        _mm256_storeu_si256(dst_ptr + i, masked_block);\n    }\n\n    // Handle remaining elements if length is not divisible by 32\n    for (size_t i = length % 32; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i *src_ptr = (__m256i *)src;\n    __m256i *dst_ptr = (__m256i *)dst;\n    __m256i *mask_ptr = (__m256i *)mask;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i src_block = _mm256_loadu_si256(src_ptr + i);\n        __m256i dst_block = _mm256_loadu_si256(dst_ptr + i);\n        __m256i mask_block = _mm256_loadu_si256(mask_ptr + i);\n\n        __m256i masked_block = _mm256_and_si256(mask_block, src_block);\n        _mm256_storeu_si256(dst_ptr + i, masked_block);\n    }\n\n    // Handle remaining elements if length is not a multiple of 32\n    for (size_t i = length % 32; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c++\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i *src_ptr = (__m256i *)src;\n    __m256i *dst_ptr = (__m256i *)dst;\n    __m256i *mask_ptr = (__m256i *)mask;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i src_block = _mm256_loadu_si256(src_ptr + i);\n        __m256i dst_block = _mm256_loadu_si256(dst_ptr + i);\n        __m256i mask_block = _mm256_loadu_si256(mask_ptr + i);\n\n        __m256i masked_block = _mm256_and_si256(mask_block, src_block);\n        _mm256_storeu_si256(dst_ptr + i, masked_block);\n    }\n\n    // Handle remaining elements if length is not a multiple of 32\n    for (size_t i = length % 32; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_0_AVX", "completion": "```c\nvoid conditional_move_simd(const int64_t *src, int64_t *dst, const bool *mask, size_t length) {\n    __m256i *src_vec = (__m256i *)src;\n    __m256i *dst_vec = (__m256i *)dst;\n    __m256i *mask_vec = (__m256i *)mask;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i src_block = _mm256_loadu_si256(src_vec + i);\n        __m256i dst_block = _mm256_loadu_si256(dst_vec + i);\n        __m256i mask_block = _mm256_loadu_si256(mask_vec + i);\n\n        __m256i masked_block = _mm256_and_si256(mask_block, src_block);\n        _mm256_storeu_si256(dst_vec + i, masked_block);\n    }\n\n    // Handle remaining elements if length is not divisible by 32\n    for (size_t i = length % 32; i < length; ++i) {\n        if (mask[i]) {\n            dst[i] = src[i];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm_set_epi32(scale, scale, scale, scale);\n    __m256i zero_vec = _mm_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm_loadu_si256((__m256i *)(src + i));\n        __m256i tmp1_vec = _mm_srli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i tmp2_vec = _mm_slli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i result_vec = _mm_div_epi32(_mm_add_epi32(src_vec, _mm_mul_epi32(tmp1_vec, tmp2_vec)), 7);\n\n        _mm_storeu_si256((__m256i *)(dst + i), result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm_set_epi32(scale, scale, scale, scale);\n    __m256i zero_vec = _mm_setzero_si256();\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i src_vec = _mm_loadu_si256((__m256i *)(src + i * 4));\n        __m256i tmp1_vec = _mm_srl_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i tmp2_vec = _mm_sll_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i result_vec = _mm_add_epi32(_mm_mul_epi32(src_vec, scale_vec), _mm_mul_epi32(tmp1_vec, tmp2_vec));\n        __m256i div_result_vec = _mm_div_epi32(result_vec, _mm_set_epi32(7, 7, 7, 7));\n        _mm_storeu_si256((__m256i *)(dst + i * 4), div_result_vec);\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (size_t i = length % 4; i > 0; --i) {\n        int element = src[length - i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        int result = (element * scale + tmp1 * tmp2) / 7;\n        dst[length - i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm_set_epi32(scale, scale, scale, scale);\n    __m256i tmp1_vec, tmp2_vec;\n\n    for (size_t i = 0; i < length - 3; i += 4) {\n        __m256i src_vec = _mm_loadu_si256((__m256i *)(src + i));\n        tmp1_vec = _mm_srli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        tmp2_vec = _mm_slli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i result_vec = _mm_div_epi32(_mm_add_epi32(src_vec, _mm_mul_epi32(tmp1_vec, tmp2_vec)), 7);\n        _mm_storeu_si256((__m256i *)(dst + i), result_vec);\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (size_t i = length - 3; i < length; ++i) {\n        int element = src[i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        int result = (element * scale + tmp1 * tmp2) / 7;\n        dst[i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm_set_epi32(scale, scale, scale, scale);\n    __m256i zero_vec = _mm_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm_loadu_si256((__m256i *)(src + i));\n        __m256i tmp1_vec = _mm_srli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i tmp2_vec = _mm_slli_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i result_vec = _mm_div_epi32(_mm_add_epi32(src_vec, _mm_mul_epi32(tmp1_vec, tmp2_vec)), 7);\n\n        _mm_storeu_si256((__m256i *)(dst + i), result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_1_AVX", "completion": "```c\nvoid load_modify_store_simd(const int *src, int *dst, int scale, size_t length) {\n    __m256i scale_vec = _mm_set_epi32(scale, scale, scale, scale);\n    __m256i tmp1_vec, tmp2_vec;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i src_vec = _mm_loadu_si256((__m256i *)(&src[i * 4]));\n        tmp1_vec = _mm_srl_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        tmp2_vec = _mm_sll_epi32(_mm_mul_epi32(src_vec, scale_vec), 3);\n        __m256i result_vec = _mm_div_epi32(_mm_add_epi32(src_vec, _mm_mul_epi32(tmp1_vec, tmp2_vec)), 7);\n        _mm_storeu_si256((__m256i *)(&dst[i * 4]), result_vec);\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (size_t i = length % 4; i > 0; --i) {\n        int element = src[length - i];\n        int tmp1 = (element * scale) >> 3;\n        int tmp2 = (element * scale) << 3;\n        int result = (element * scale + tmp1 * tmp2) / 7;\n        dst[length - i] = result;\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m256i v_stride = _mm_set_epi32(stride, stride, stride, stride);\n    __m256i v_cols = _mm_set_epi32(cols, cols, cols, cols);\n\n    for (size_t r = 0; r < rows; ++r) {\n        const __m256i v_src = _mm_loadu_si256((__m256i *)(&src[r * cols]));\n        const __m256i v_dst = _mm_loadu_si256((__m256i *)(&dst[r]));\n\n        __m256i v_result = _mm_add_epi32(v_dst, v_stride);\n        _mm_storeu_si256((__m256i *)(&dst[r]), v_result);\n\n        v_dst = _mm_add_epi32(v_dst, v_cols);\n        _mm_storeu_si256((__m256i *)(&dst[r]), v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m256i v_stride = _mm_set_epi32(stride, stride, stride, stride);\n    __m256i v_cols = _mm_set_epi32(cols, cols, cols, cols);\n\n    for (size_t r = 0; r < rows; ++r) {\n        __m256i v_src = _mm_loadu_si256((__m256i *)(&src[r * cols]));\n        __m256i v_dst = _mm_loadu_si256((__m256i *)(&dst[r]));\n\n        v_dst = _mm_add_epi32(v_dst, v_stride);\n        v_dst = _mm_mul_epi32(v_dst, v_cols);\n\n        _mm_storeu_si256((__m256i *)(&dst[r]), v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m256i v_stride = _mm_set_epi32(stride, stride, stride, stride);\n    __m256i v_cols = _mm_set_epi32(cols, cols, cols, cols);\n\n    for (size_t r = 0; r < rows; ++r) {\n        __m256i v_src = _mm_loadu_si256((__m256i const *)(&src[r * cols]));\n        __m256i v_dst = _mm_loadu_si256((__m256i const *)(&dst[r]));\n\n        v_dst = _mm_add_epi32(v_dst, v_stride);\n        _mm_storeu_si256((__m256i *)(&dst[r]), v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m256i v_stride = _mm_set_epi32(stride, stride, stride, stride);\n    __m256i v_cols = _mm_set_epi32(cols, cols, cols, cols);\n\n    for (size_t r = 0; r < rows; ++r) {\n        __m256d v_src = _mm256_loadu_pd(src + r * cols + stride);\n        _mm256_storeu_pd(dst + r, v_src);\n    }\n}\n```"}
{"task_id": "SimdBench_2_AVX", "completion": "```c\nvoid strided_load_store_simd(const double *src, double *dst, size_t rows, size_t cols, size_t stride) {\n    __m256i v_stride = _mm_set_epi32(stride, stride, stride, stride);\n    __m256i v_cols = _mm_set_epi32(cols, cols, cols, cols);\n\n    for (size_t r = 0; r < rows; ++r) {\n        __m256i v_src = _mm_loadu_si256((__m256i *)(&src[r * cols]));\n        __m256i v_dst = _mm_loadu_si256((__m256i *)(&dst[r]));\n\n        __m256i v_result = _mm_add_epi32(v_dst, v_stride);\n        _mm_storeu_si256((__m256i *)(&dst[r]), v_result);\n\n        v_src = _mm_loadu_si256((__m256i *)(&src[v_result[0] / cols]));\n        v_dst = _mm_loadu_si256((__m256i *)(&dst[v_result[0] % cols]));\n\n        v_result = _mm_add_epi32(v_dst, v_stride);\n        _mm_storeu_si256((__m256i *)(&dst[v_result[0] / cols]), v_result);\n\n        v_src = _mm_loadu_si256((__m256i *)(&src[v_result[0] % cols]));\n        v_dst = _mm_loadu_si256((__m256i *)(&dst[v_result[0] % cols]));\n\n        v_result = _mm_add_epi32(v_dst, v_stride);\n        _mm_storeu_si256((__m256i *)(&dst[v_result[0] / cols]), v_result);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c++\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256i idx_vec = _mm256_set_epi32(indices[0], indices[1], indices[2], indices[3]);\n    __m256 src_vec = _mm256_loadu_ps(src);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256 dst_vec = _mm256_loadu_ps(dst + i * 4);\n        dst_vec = _mm256_permutevar_ps(dst_vec, idx_vec);\n        _mm256_storeu_ps(dst + i * 4, dst_vec);\n    }\n\n    if (length % 4 != 0) {\n        __m256 dst_vec = _mm256_loadu_ps(dst + length / 4 * 4);\n        dst_vec = _mm256_permutevar_ps(dst_vec, idx_vec);\n        _mm256_storeu_ps(dst + length / 4 * 4, dst_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256i idx_vec = _mm256_set_epi32(indices[0], indices[1], indices[2], indices[3]);\n    __m256 src_vec = _mm256_loadu_ps(src);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256 dst_vec = _mm256_loadu_ps(dst + i * 4);\n        dst_vec = _mm256_permutevar_ps(dst_vec, idx_vec);\n        _mm256_storeu_ps(dst + i * 4, dst_vec);\n    }\n\n    if (length % 4 != 0) {\n        __m128 idx_vec = _mm_set_epi32(indices[length / 4], indices[length / 4 + 1], indices[length / 4 + 2], indices[length / 4 + 3]);\n        __m128 src_vec = _mm_loadu_ps(src + length / 4 * 4);\n\n        __m128 dst_vec = _mm_loadu_ps(dst + length / 4 * 4);\n        dst_vec = _mm_permutevar_ps(dst_vec, idx_vec);\n        _mm_storeu_ps(dst + length / 4 * 4, dst_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256i idx_vec = _mm256_set_epi32(indices[0], indices[1], indices[2], indices[3]);\n    __m256 src_vec = _mm256_loadu_ps(src);\n\n    for (size_t i = 4; i < length; i += 4) {\n        idx_vec = _mm256_add_epi32(idx_vec, _mm256_set_epi32(i, i + 1, i + 2, i + 3));\n        src_vec = _mm256_loadu_ps(&src[i]);\n\n        dst[i] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, idx_vec));\n        dst[i + 1] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, idx_vec + 4));\n        dst[i + 2] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, idx_vec + 8));\n        dst[i + 3] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, idx_vec + 12));\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - (length % 4); i < length; ++i) {\n        dst[i] = src[i];\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256i indices_vec = _mm256_loadu_si256((__m256i *)indices);\n    __m256 src_vec;\n\n    for (size_t i = 0; i < length; i += 8) {\n        src_vec = _mm256_loadu_ps(&src[i]);\n        dst[i] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec));\n        dst[i + 1] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec + 4));\n        dst[i + 2] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec + 8));\n        dst[i + 3] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec + 12));\n        dst[i + 4] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec + 16));\n        dst[i + 5] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec + 20));\n        dst[i + 6] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec + 24));\n        dst[i + 7] = _mm256_cvtss_f32(_mm256_permutevar_ps(src_vec, indices_vec + 28));\n    }\n}\n```"}
{"task_id": "SimdBench_3_AVX", "completion": "```c\nvoid indexed_access_simd(const float *src, const int *indices, float *dst, size_t length) {\n    __m256i idx = _mm256_set_epi32(indices[0], indices[1], indices[2], indices[3]);\n    __m256 src_vec = _mm256_loadu_ps(src);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256 dst_vec = _mm256_maskload_ps(dst + i * 4, idx);\n        dst_vec = _mm256_add_ps(dst_vec, src_vec);\n        _mm256_storeu_ps(dst + i * 4, dst_vec);\n    }\n\n    if (length % 4 != 0) {\n        __m128 idx1 = _mm_set_epi32(indices[length - 4], indices[length - 3], indices[length - 2], indices[length - 1]);\n        __m128 src_vec1 = _mm_loadu_ps(&src[length - 4]);\n\n        __m128 dst_vec1 = _mm_maskload_ps(dst + length - 4, idx1);\n        dst_vec1 = _mm_add_ps(dst_vec1, src_vec1);\n        _mm_storeu_ps(dst + length - 4, dst_vec1);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    const int16_t *begin = arr + start;\n    const int16_t *end_ptr = arr + end;\n\n    // Calculate the number of elements to process in each chunk\n    size_t chunk_size = (end - start) / 32; // Assuming 32 bytes per vector\n\n    // Process chunks in parallel\n    for (size_t i = 0; i < chunk_size; ++i) {\n        __m128i vec1 = _mm_loadu_si128((__m128i *)begin);\n        __m128i vec2 = _mm_loadu_si128((__m128i *)(end_ptr - 32));\n        __m128i vec3 = _mm_loadu_si128((__m128i *)(end_ptr - 64));\n\n        __m128i reversed_vec1 = _mm_shuffle_epi32(vec1, vec2);\n        __m128i reversed_vec2 = _mm_shuffle_epi32(vec2, vec3);\n\n        _mm_storeu_si128((__m128i *)begin, reversed_vec1);\n        _mm_storeu_si128((__m128i *)(end_ptr - 32), reversed_vec2);\n\n        begin += 32;\n        end_ptr -= 32;\n    }\n\n    // Handle remaining elements if there are less than 32 elements left\n    if ((end - start) % 32 != 0) {\n        __m128i vec1 = _mm_loadu_si128((__m128i *)begin);\n        __m128i vec2 = _mm_loadu_si128((__m128i *)(end_ptr - 32));\n\n        __m128i reversed_vec1 = _mm_shuffle_epi32(vec1, vec2);\n\n        _mm_storeu_si128((__m128i *)begin, reversed_vec1);\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    __m256i *ptr = (__m256i *)arr;\n    int64_t stride = 32; // Assuming each vector contains 32 elements\n\n    while (start + stride <= end) {\n        __m256i vec1 = _mm256_loadu_si256(ptr + start / stride);\n        __m256i vec2 = _mm256_loadu_si256(ptr + (end - stride + 1) / stride);\n\n        _mm256_storeu_si256(ptr + start / stride, vec2);\n        _mm256_storeu_si256(ptr + (end - stride + 1) / stride, vec1);\n\n        start += stride;\n        end -= stride;\n    }\n\n    // Handle remaining elements\n    while (start <= end) {\n        arr[start] ^= arr[end];\n        arr[end] ^= arr[start];\n        arr[start++] ^= arr[end--];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    __m256i *ptr = (__m256i *)arr;\n    __m256i *end_ptr = ptr + ((end - start) / 32);\n\n    while (ptr < end_ptr) {\n        __m256i temp = _mm256_loadu_si256(ptr);\n        __m256i reversed_temp = _mm256_permutevar_epi8(temp, _mm256_setr_epi8(0, 7, 4, 5, 2, 1, 6, 3));\n        _mm256_storeu_si256(ptr, reversed_temp);\n        ptr += 32;\n    }\n\n    // Handle remaining elements\n    int16_t *remaining_arr = arr + ((end - start) % 32);\n    int16_t *end_remaining_arr = remaining_arr + ((end - start) % 32);\n    while (remaining_arr < end_remaining_arr) {\n        *remaining_arr = *(end_remaining_arr - 1);\n        remaining_arr++;\n        end_remaining_arr--;\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    int64_t stride = 8; // Assuming 8 bytes per vector\n    int64_t num_vectors = (end - start + stride - 1) / stride;\n\n    __m256i *vec_arr = (__m256i *)arr;\n    __m256i *vec_start = vec_arr + start / stride;\n    __m256i *vec_end = vec_arr + (end + stride - 1) / stride;\n\n    for (int i = 0; i < num_vectors; ++i) {\n        __m256i vec = _mm256_loadu_si256(vec_start + i);\n        __m256i reversed_vec = _mm256_permutevar_epi8(vec, _mm256_set_epi8(7, 6, 5, 4, 3, 2, 1, 0));\n        _mm256_storeu_si256(vec_start + i, reversed_vec);\n    }\n\n    // Handle remaining elements\n    for (int i = num_vectors * stride; i <= end; ++i) {\n        arr[i] = arr[end - i];\n    }\n}\n```"}
{"task_id": "SimdBench_4_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_reverse_simd(int16_t *arr, uint64_t start, uint64_t end) {\n    __m256i *ptr = (__m256i *)arr;\n    int64_t n = end - start + 1;\n    int64_t chunk_size = 32; // Assuming each chunk size is 32 elements\n\n    if (n % chunk_size != 0) {\n        // Handle the last incomplete chunk\n        int64_t remainder = n % chunk_size;\n        for (int i = start; i < start + remainder; ++i) {\n            arr[i] = arr[end - i];\n        }\n    } else {\n        // Process complete chunks\n        for (int i = start; i < start + n / chunk_size; ++i) {\n            __m256i chunk = _mm256_loadu_si256((__m256i *)&arr[i]);\n            __m256i reversed_chunk = _mm256_permutevar_epi8(chunk, _mm256_setr_epi8(7, 6, 5, 4, 3, 2, 1, 0));\n            _mm256_storeu_si256((__m256i *)&arr[i], reversed_chunk);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c++\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t stride = dim1 * dim2;\n    const size_t slice_stride = stride * dim3;\n\n    // Calculate the starting index of the slice in the flattened tensor\n    size_t start_index = slice_idx * stride;\n\n    // Extract the slice using AVX/AVX2 intrinsics\n    __m256i *ptr = (__m256i *)(&tensor[start_index]);\n    __m256i *end_ptr = (__m256i *)((char *)&tensor[start_index] + slice_stride);\n\n    while (ptr < end_ptr) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        _mm256_storeu_si256((__m256i *)slice, vec);\n        ptr += 4;\n        slice += 4;\n    }\n\n    // Handle remaining elements if there are fewer than 4 elements left\n    size_t remaining_elements = slice_stride - (start_index % stride);\n    if (remaining_elements > 0) {\n        memcpy(slice, &tensor[start_index], remaining_elements);\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t stride = dim1 * dim2;\n    const size_t slice_start = stride * slice_idx;\n\n    __m256i *ptr = (__m256i *)tensor + slice_start;\n    __m256i *end_ptr = (__m256i *)(tensor + stride * (slice_idx + 4));\n\n    while (ptr < end_ptr) {\n        __m256i vec1 = _mm256_loadu_si256(ptr++);\n        __m256i vec2 = _mm256_loadu_si256(ptr++);\n        __m256i vec3 = _mm256_loadu_si256(ptr++);\n        __m256i vec4 = _mm256_loadu_si256(ptr++);\n\n        _mm256_storeu_si256((__m256i *)slice, vec1);\n        _mm256_storeu_si256((__m256i *)(slice + 16), vec2);\n        _mm256_storeu_si256((__m256i *)(slice + 32), vec3);\n        _mm256_storeu_si256((__m256i *)(slice + 48), vec4);\n    }\n\n    // Handle remaining elements if there are fewer than 4 elements left\n    size_t remaining_elements = stride - slice_start % stride;\n    if (remaining_elements > 0) {\n        __m256i *ptr = (__m256i *)(tensor + slice_start);\n        __m256i *end_ptr = (__m256i *)(tensor + stride * (slice_idx + 4));\n\n        while (ptr < end_ptr && remaining_elements > 0) {\n            __m256i vec = _mm256_loadu_si256(ptr++);\n            _mm256_storeu_si256((__m256i *)slice, vec);\n            --remaining_elements;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t stride = dim1 * dim2;\n    const size_t start_index = stride * slice_idx;\n\n    __m256i *ptr = (__m256i *)tensor + start_index / 4;\n    __m256i *end_ptr = (__m256i *)(tensor + stride * (slice_idx + 1));\n\n    while (ptr < end_ptr) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        _mm256_storeu_si256((__m256i *)slice, vec);\n        ptr += 4;\n        slice += 4;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c++\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t stride = dim1 * dim2;\n    const size_t start_index = slice_idx * stride;\n\n    __m256i *ptr = (__m256i *)(&tensor[start_index]);\n    __m256i *end_ptr = (__m256i *)(&tensor[start_index + stride - 4]);\n\n    while (ptr <= end_ptr) {\n        __m256i vec1 = _mm256_loadu_si256(ptr++);\n        __m256i vec2 = _mm256_loadu_si256(ptr++);\n        __m256i vec3 = _mm256_loadu_si256(ptr++);\n        __m256i vec4 = _mm256_loadu_si256(ptr++);\n\n        _mm256_storeu_si256((__m256i *)slice, vec1);\n        _mm256_storeu_si256((__m256i *)(slice + 16), vec2);\n        _mm256_storeu_si256((__m256i *)(slice + 32), vec3);\n        _mm256_storeu_si256((__m256i *)(slice + 48), vec4);\n\n        slice += 64;\n    }\n\n    // Handle remaining elements if there are less than 4 elements left\n    while (ptr < end_ptr) {\n        __m256i vec1 = _mm256_loadu_si256(ptr++);\n        _mm256_storeu_si256((__m256i *)slice, vec1);\n        slice += 16;\n    }\n}\n```"}
{"task_id": "SimdBench_5_AVX", "completion": "```c\nvoid extract_tensor_slice_simd(const uint8_t *tensor, uint8_t *slice, size_t dim1, size_t dim2, size_t dim3, size_t slice_idx) {\n    const size_t stride = dim1 * dim2;\n    const size_t start_index = slice_idx * stride;\n\n    __m256i *ptr = (__m256i *)tensor + start_index / 4;\n    __m256i *end_ptr = (__m256i *)(tensor + start_index + stride - 1);\n\n    while (ptr <= end_ptr) {\n        __m256i vec1 = _mm256_loadu_si256(ptr++);\n        __m256i vec2 = _mm256_loadu_si256(ptr++);\n        __m256i vec3 = _mm256_loadu_si256(ptr++);\n        __m256i vec4 = _mm256_loadu_si256(ptr++);\n\n        _mm256_storeu_si256((__m256i *)slice, vec1);\n        _mm256_storeu_si256((__m256i *)(slice + 16), vec2);\n        _mm256_storeu_si256((__m256i *)(slice + 32), vec3);\n        _mm256_storeu_si256((__m256i *)(slice + 48), vec4);\n    }\n\n    // Handle remaining elements if there are fewer than 4 elements left\n    while (ptr < end_ptr) {\n        __m256i vec = _mm256_loadu_si256(ptr++);\n        _mm256_storeu_si256((__m256i *)slice, vec);\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const size_t num_blocks_rows = rows / block_size;\n    const size_t num_blocks_cols = cols / block_size;\n\n    for (size_t i = 0; i < num_blocks_rows; ++i) {\n        for (size_t j = 0; j < num_blocks_cols; ++j) {\n            __m128i row_block[4];\n            __m128i col_block[4];\n\n            // Load the current row into a row block\n            for (size_t k = 0; k < block_size; ++k) {\n                row_block[k >> 2] |= _mm_loadu_si128((__m128i *)(src + i * block_size + k));\n            }\n\n            // Load the current column into a column block\n            for (size_t k = 0; k < block_size; ++k) {\n                col_block[k >> 2] |= _mm_loadu_si128((__m128i *)(src + k * cols + j * block_size));\n            }\n\n            // Transpose the blocks\n            for (size_t k = 0; k < block_size; ++k) {\n                _mm_storeu_si128((__m128i *)(dst + j * block_size + k), _mm_shuffle_epi32(row_block[k], col_block[k]));\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const size_t num_blocks = (cols + block_size - 1) / block_size;\n    const size_t block_cols = block_size;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < num_blocks; ++j) {\n            const size_t start_col = j * block_cols;\n            const size_t end_col = std::min(start_col + block_cols, cols);\n\n            __m128i *src_row = (__m128i *)(src + i * cols + start_col);\n            __m128i *dst_row = (__m128i *)(dst + i * cols + start_col);\n\n            for (size_t k = start_col; k < end_col; k += 4) {\n                __m128i vec1 = _mm_loadu_si128(src_row++);\n                __m128i vec2 = _mm_loadu_si128(src_row++);\n                __m128i vec3 = _mm_loadu_si128(src_row++);\n                __m128i vec4 = _mm_loadu_si128(src_row++);\n\n                _mm_storeu_si128(dst_row++, _mm_shuffle_epi32(vec1, vec2));\n                _mm_storeu_si128(dst_row++, _mm_shuffle_epi32(vec3, vec4));\n            }\n\n            // Handle remaining elements if block_cols is not a multiple of 4\n            for (size_t k = end_col; k < cols; ++k) {\n                dst[i * cols + k] = src[i * cols + k];\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n__attribute__((always_inline)) __forceinline void blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const size_t num_blocks = (cols + block_size - 1) / block_size;\n    const size_t num_rows_per_block = rows;\n\n    for (size_t i = 0; i < num_blocks; ++i) {\n        const size_t start_col = i * block_size;\n        const size_t end_col = std::min(start_col + block_size, cols);\n\n        for (size_t j = 0; j < num_rows_per_block; ++j) {\n            const size_t src_index = j * cols + start_col;\n            const size_t dst_index = (start_col + j) * rows + i;\n\n            _mm_storeu_si16(&dst[dst_index], _mm_loadu_si16(&src[src_index]));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const size_t num_blocks = rows / block_size;\n    const size_t remaining_rows = rows % block_size;\n\n    for (size_t i = 0; i < num_blocks; ++i) {\n        __m128i *src_row = (__m128i *)(&src[i * block_size]);\n        __m128i *dst_row = (__m128i *)(&dst[i * cols]);\n\n        for (size_t j = 0; j < cols; ++j) {\n            __m128i temp = _mm_loadu_si128(src_row + j);\n            _mm_storeu_si128(dst_row + (cols - j - 1), temp);\n        }\n    }\n\n    if (remaining_rows > 0) {\n        __m128i *src_row = (__m128i *)(&src[num_blocks * block_size]);\n        __m128i *dst_row = (__m128i *)(&dst[num_blocks * cols]);\n\n        for (size_t j = 0; j < cols; ++j) {\n            __m128i temp = _mm_loadu_si128(src_row + j);\n            _mm_storeu_si128(dst_row + (cols - j - 1), temp);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_6_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid blocked_matrix_transpose_simd(const int16_t *src, int16_t *dst, size_t rows, size_t cols, size_t block_size) {\n    const size_t num_blocks = rows / block_size;\n    const size_t num_cols_per_block = cols / block_size;\n\n    for (size_t i = 0; i < num_blocks; ++i) {\n        for (size_t j = 0; j < num_cols_per_block; ++j) {\n            __m128i row = _mm_loadu_si128((__m128i *)(&src[i * block_size + j * cols]));\n            __m128i col = _mm_loadu_si128((__m128i *)(&dst[j * block_size + i * cols]));\n\n            __m128i transposed_row = _mm_shuffle_epi32(row, row, 0b1010); // Swap elements\n            __m128i transposed_col = _mm_shuffle_epi32(col, col, 0b1010);\n\n            _mm_storeu_si128((__m128i *)(&dst[j * block_size + i * cols]), transposed_row);\n            _mm_storeu_si128((__m128i *)(&dst[i * block_size + j * cols]), transposed_col);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n__attribute__((target(\"avx2\")))\nfloat diagonal_sum_3d_simd(const float *array, size_t dim) {\n    const __m256i dim_mask = _mm256_set_epi32(dim - 1, dim - 1, dim - 1, dim - 1);\n    const __m256i dim_shift = _mm256_set_epi32(0, 4, 8, 12);\n\n    float sum = 0.0f;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        __m256i idx = _mm256_set_epi32(i, i + 1, i + 2, i + 3);\n        __m256 v = _mm256_loadu_ps(array + idx);\n        __m256 mask = _mm256_cmpeq_epi32(idx, dim_mask);\n        __m256 result = _mm256_and_ps(v, mask);\n        sum += _mm256_reduce_add_ps(result);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c++\n__attribute__((target(\"avx2\"))) float diagonal_sum_3d_simd(const float *array, size_t dim) {\n    const __m256i dim_mask = _mm256_set_epi32(dim - 1, dim - 1, dim - 1, dim - 1);\n    const __m256i zero = _mm256_setzero_ps();\n    __m256 sum = zero;\n\n    for (size_t i = 0; i < dim; i += 4) {\n        const __m256i idx = _mm256_set_epi32(i, i + 1, i + 2, i + 3);\n        const __m256 v = _mm256_loadu_ps(array + idx);\n        const __m256 mask = _mm256_cmpeq_epi32(idx, dim_mask);\n        sum = _mm256_add_ps(sum, _mm256_mul_ps(v, mask));\n    }\n\n    return _mm256_reduce_add_ps(sum, _mm256_setzero_ps());\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n__attribute__((always_inline)) __forceinline float diagonal_sum_3d_simd(const float *array, size_t dim) {\n    const int vec_size = sizeof(float) / sizeof(__m256);\n    const int num_vecs = dim / vec_size;\n\n    __m256 sum = _mm256_setzero_ps();\n    for (int i = 0; i < num_vecs; ++i) {\n        const __m256 vec = _mm256_loadu_ps(array + i * vec_size);\n        sum = _mm256_add_ps(sum, _mm256_permute_ps(vec, _MM_SHUFFLE(0, 0, 0, 0)));\n    }\n\n    float result = 0.0f;\n    for (int i = 0; i < vec_size; ++i) {\n        result += _mm256_extractf32_ps(sum, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n__attribute__((always_inline)) __target__(\"avx2\")\nstatic inline float diagonal_sum_3d_simd(const float *array, size_t dim) {\n    const int vec_size = sizeof(float) / sizeof(__m256);\n    const int num_vecs = dim / vec_size;\n\n    __m256 sum = _mm256_setzero_ps();\n    for (int i = 0; i < num_vecs; ++i) {\n        const __m256 vec = _mm256_loadu_ps(array + i * vec_size);\n        sum = _mm256_add_ps(sum, _mm256_permute_ps(vec, _MM_SHUFFLE(0, 0, 0, 0)));\n    }\n\n    // Reduce the sum to a single float\n    float result;\n    _mm_store_ss(&result, sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_7_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n__attribute__((always_inline)) __target__(\"avx2\")\nstatic inline float diagonal_sum_3d_simd(const float *array, size_t dim) {\n    const int vec_size = sizeof(float) / sizeof(__m256);\n    const int num_vecs = dim / vec_size;\n\n    __m256 sum = _mm256_setzero_ps();\n    for (int i = 0; i < num_vecs; ++i) {\n        const __m256 vec = _mm256_loadu_ps(array + i * vec_size);\n        sum = _mm256_add_ps(sum, _mm256_permute_ps(vec, _MM_SHUFFLE(0, 0, 0, 0)));\n    }\n\n    float result = 0.0f;\n    for (int i = 0; i < vec_size; ++i) {\n        result += _mm256_extractf32_ps(sum, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256i mask = _mm_set1_epi32(0); // Initialize mask with zeros\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            const double val = src[i * cols + j];\n            if (val > threshold) {\n                __m256 scaled_val = _mm_mul_ps(_mm_loadu_ps(&val), _mm_set1_ps(scale));\n                _mm_storeu_ps(&dst[i * cols + j], scaled_val);\n            } else {\n                dst[i * cols + j] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256i mask = _mm256_set1_epi32(0); // Initialize mask to zero\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            const double val = src[i * cols + j];\n            if (val > threshold) {\n                __m256 scaled_val = _mm256_mul_ps(_mm256_set1_pd(val), _mm256_set1_pd(scale));\n                _mm256_storeu_pd(dst + i * cols + j, scaled_val);\n            } else {\n                dst[i * cols + j] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c++\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256i mask = _mm256_set1_epi32(0); // Initialize mask to zero\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            const double val = src[i * cols + j];\n            if (val > threshold) {\n                __m256 scaled_val = _mm256_mul_ps(_mm256_set1_pd(val), _mm256_set1_pd(scale));\n                _mm256_storeu_pd(dst + i * cols + j, scaled_val);\n            } else {\n                dst[i * cols + j] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256i mask = _mm256_set1_epi32(0); // Initialize mask to zero\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            const __m256d val = _mm256_loadu_pd(&src[i * cols + j]);\n            const __m256d scaled_val = _mm256_cmpgt_pd(val, _mm256_set1_pd(threshold));\n            const __m256d masked_val = _mm256_blendv_pd(scaled_val, val, mask);\n            _mm256_storeu_pd(&dst[i * cols + j], masked_val);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_8_AVX", "completion": "```c\nvoid conditional_scale_simd(const double *src, double *dst, size_t rows, size_t cols, double threshold, double scale) {\n    __m256i mask = _mm256_set1_epi32(0); // Initialize mask to all zeros\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            const double val = src[i * cols + j];\n            if (val > threshold) {\n                __m256 scaled_val = _mm256_mul_ps(_mm256_set1_pd(val), _mm256_set1_pd(scale));\n                _mm256_storeu_pd(dst + i * cols + j, scaled_val);\n            } else {\n                dst[i * cols + j] = val;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m256i idx_vec = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i mask_vec = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        __m256i indices_vec = _mm256_loadu_si256((__m256i *)(&indices[i]));\n        __m256i sorted_indices_vec = _mm256_permutevar_ps(idx_vec, indices_vec);\n        __m256 sorted_src_vec = _mm256_permutevar_ps(_mm256_loadu_si256((__m256i *)(&src[i])), sorted_indices_vec);\n        _mm256_storeu_si256((__m256i *)(&dst[i]), sorted_src_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m256i idx_vec = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i mask_vec = _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1);\n\n    for (size_t i = 0; i < rows; i += 8) {\n        __m256i src_indices = _mm256_loadu_si256((__m256i *)(&indices[i]));\n        __m256i dst_indices = _mm256_shuffle_epi32(src_indices, idx_vec, mask_vec);\n\n        __m256d src_row = _mm256_loadu_pd((__m256d *)(&src[dst_indices[0] * cols]));\n        __m256d dst_row = _mm256_loadu_pd((__m256d *)(&dst[i * cols]));\n\n        _mm256_storeu_pd((__m256d *)(&dst[i * cols]), src_row);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m256i idx_vec = _mm256_set_epi32(indices[0], indices[1], indices[2], indices[3]);\n    __m256i mask_vec = _mm256_cmpgt_epi32(idx_vec, _mm256_setzero_si256());\n    __m256i permuted_idx_vec = _mm256_permutevar_epi32(idx_vec, mask_vec);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256 src_row = _mm256_loadu_ps(&src[i * cols]);\n        __m256 dst_row = _mm256_loadu_ps(&dst[i * cols]);\n\n        __m256 permuted_src_row = _mm256_permutevar_ps(src_row, permuted_idx_vec);\n        _mm256_storeu_ps(&dst[i * cols], permuted_src_row);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m256i idx_vec = _mm256_set_epi32(indices[0], indices[1], indices[2], indices[3]);\n    __m256i mask_vec = _mm256_cmpgt_epi32(idx_vec, _mm256_setzero_si256());\n    __m256i permuted_idx_vec = _mm256_permutevar_epi32(idx_vec, mask_vec);\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256 src_row = _mm256_loadu_ps(&src[i * cols]);\n        __m256 dst_row = _mm256_loadu_ps(&dst[i * cols]);\n\n        __m256 permuted_src_row = _mm256_permutevar_ps(src_row, permuted_idx_vec);\n        _mm256_storeu_ps(&dst[i * cols], permuted_src_row);\n    }\n}\n```"}
{"task_id": "SimdBench_9_AVX", "completion": "```c\nvoid reorder_matrix_rows_simd(const double *src, double *dst, size_t rows, size_t cols, const size_t *indices) {\n    __m256i idx_vec;\n    __m256d src_vec;\n    __m256d dst_vec;\n\n    for (size_t i = 0; i < rows; ++i) {\n        idx_vec = _mm256_loadu_si256((__m256i *)(&indices[i * cols]));\n        src_vec = _mm256_loadu_pd((const double *)&src[idx_vec]);\n        dst_vec = _mm256_storeu_pd((double *)&dst[i * cols], src_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i vec_scalar = _mm256_set1_epi64(scalar);\n    __m256i vec_result;\n\n    for (size_t i = 0; i < length / 8; ++i) {\n        __m256i vec_src1 = _mm256_loadu_si256((__m256i *)(&src1[i * 8]));\n        __m256i vec_src2 = _mm256_loadu_si256((__m256i *)(&src2[i * 8]));\n\n        vec_result = _mm256_add_epi64(vec_src1, vec_src2);\n        vec_result = _mm256_add_epi64(vec_result, vec_scalar);\n\n        _mm256_storeu_si256((__m256i *)(&dst[i * 8]), vec_result);\n    }\n\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i > 0; --i) {\n        dst[length - i] = (src1[length - i] + src2[length - i]) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i *src1_vec = (__m256i *)src1;\n    __m256i *src2_vec = (__m256i *)src2;\n    __m256i *dst_vec = (__m256i *)dst;\n\n    for (size_t i = 0; i < length / 8; ++i) {\n        __m256i sum = _mm256_add_epi64(*src1_vec++, *src2_vec++);\n        __m256i scaled_sum = _mm256_add_epi64(sum, _mm256_set1_epi64(scalar));\n        __m256i rounded_sum = _mm256_round_epi32(scaled_sum, _MM_FROUND_TOWARD_ZERO);\n        *dst_vec++ = _mm256_extract_epi64(rounded_sum, 0);\n    }\n\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i > 0; --i) {\n        *dst_vec++ = (*src1_vec++) + (*src2_vec++) + scalar;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i *vec_src1 = (__m256i *)src1;\n    __m256i *vec_src2 = (__m256i *)src2;\n    __m256i *vec_dst = (__m256i *)dst;\n\n    for (size_t i = 0; i < length / 8; ++i) {\n        __m256i sum = _mm256_add_epi64(*vec_src1++, *vec_src2++);\n        sum = _mm256_add_epi64(sum, _mm256_set1_epi64(scalar));\n        sum = _mm256_div_epi64(sum, _mm256_set1_epi64(2));\n        *vec_dst++ = sum;\n    }\n\n    // Handle remaining elements if length is not divisible by 8\n    for (size_t i = length % 8; i < length; ++i) {\n        *dst++ = (*src1 + *src2 + scalar) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i *vec_src1 = (__m256i *)src1;\n    __m256i *vec_src2 = (__m256i *)src2;\n    __m256i *vec_dst = (__m256i *)dst;\n\n    for (size_t i = 0; i < length / 8; ++i) {\n        __m256i sum = _mm256_add_epi64(*vec_src1++, *vec_src2++);\n        sum = _mm256_add_epi64(sum, _mm256_set1_epi64(scalar));\n        sum = _mm256_div_epi64(sum, _mm256_set1_epi64(2));\n        *vec_dst++ = sum;\n    }\n\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i > 0; --i) {\n        *dst++ = (*vec_src1++) + scalar + (*vec_src2++) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_10_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_vector_add_simd(const int64_t *src1, const int64_t *src2, int64_t *dst, int64_t scalar, size_t length) {\n    __m256i vec_scalar = _mm_set_epi64x(scalar, scalar);\n    __m256i vec_result;\n\n    for (size_t i = 0; i < length / 8; ++i) {\n        __m256i vec_src1 = _mm_loadu_si256((__m256i *)(&src1[i * 8]));\n        __m256i vec_src2 = _mm_loadu_si256((__m256i *)(&src2[i * 8]));\n\n        vec_result = _mm_add_epi64(vec_src1, vec_src2);\n        vec_result = _mm_add_epi64(vec_result, vec_scalar);\n\n        _mm_storeu_si256((__m256i *)(&dst[i * 8]), vec_result);\n    }\n\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i > 0; --i) {\n        dst[length - i] = (src1[length - i] + src2[length - i]) / 2;\n    }\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0)\n        return -1;\n\n    __m256i min_even = _mm256_set1_epi32(INT_MAX);\n    __m256i mask = _mm256_set1_epi32(0x00000002);\n\n    for(size_t i = 0; i < length; i += 4){\n        __m256i current = _mm_loadu_si256((__m256i *)(src + i));\n        __m256i even_mask = _mm_and_si256(current, mask);\n        __m256i even_values = _mm_and_si256(current, even_mask);\n        __m256i even_min = _mm256_min_epu32(min_even, even_values);\n        min_even = even_min;\n    }\n\n    int result = 0;\n    for(int i = 0; i < 4; ++i){\n        result |= _mm_extract_epi32(min_even, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_even = _mm256_setzero_si256();\n    __m256i mask = _mm256_set_epi32(0, 1, 0, 1);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i current = _mm_loadu_si256((__m256i *)(src + i));\n        __m256i even_mask = _mm_and_si256(mask, current);\n        __m256i even_values = _mm_and_si256(even_mask, current);\n        __m256i even_min = _mm256_min_epi32(min_even, even_values);\n        min_even = even_min;\n    }\n\n    int result = 0;\n    for (size_t i = 0; i < 4; ++i) {\n        result |= _mm_extract_epi32(min_even, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0)\n        return -1;\n\n    __m256i min_even = _mm256_setzero_si256();\n    __m256i mask = _mm256_set_epi32(0, 2, 4, 6);\n\n    for(size_t i = 0; i < length; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i *)(src + i));\n        __m256i even_mask = _mm256_and_si256(mask, current);\n        __m256i even_values = _mm256_and_si256(even_mask, current);\n        __m256i even_min = _mm256_min_ps(_mm256_castsi256_ps(even_values), min_even);\n        min_even = _mm256_blendv_ps(min_even, even_min, even_mask);\n    }\n\n    int result;\n    _mm_storeu_si32(&result, _mm256_castps_si32(min_even));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length) {\n    if (length == 0) {\n        return -1;\n    }\n\n    __m256i min_even = _mm256_set1_epi32(INT_MAX); // Initialize with maximum possible value\n\n    for (size_t i = 0; i < length; i += 4) {\n        const __m256i current = _mm256_loadu_si256((__m256i *)(&src[i]));\n        const __m256i even_mask = _mm256_and_si256(_mm256_cmpgt_epi32(current, _mm256_setzero_si256()), _mm256_slli_epi32(current, 1)); // Mask even numbers\n        const __m256i even_values = _mm256_and_si256(even_mask, current); // Extract even values\n        const __m256i min_even_current = _mm256_min_epi32(min_even, even_values); // Update minimum even value\n    }\n\n    int result = _mm256_extract_epi32(_mm256_min_epi32(min_even, _mm256_set1_epi32(INT_MAX)), 0); // Extract the final minimum value\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_11_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vector_even_min_simd(const int *src, size_t length){\n    if(length == 0) return -1;\n\n    __m256i min_even = _mm256_setzero_si256();\n    const __m256i mask = _mm256_set_epi32(0, 2, 4, 6);\n\n    for(size_t i = 0; i < length; i += 8) {\n        const __m256i vec = _mm256_loadu_si256((__m256i *)(src + i));\n        const __m256i even_vec = _mm256_and_si256(vec, mask);\n        const __m256i min_even_vec = _mm256_min_epi32(min_even, even_vec);\n        min_even = _mm256_blendv_epi32(min_even, min_even_vec, _mm256_cmpgt_epi32(even_vec, min_even));\n    }\n\n    int result;\n    _mm_storeu_si32(&result, _mm256_extract_epi32(min_even, 0));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i src_vec = _mm256_loadu_si256((__m256i const *)src);\n    __m256i dst_vec = _mm256_setzero_si256();\n\n    // Arithmetic right shift\n    __m256i arith_shifted = _mm256_srli_epi32(src_vec, shift);\n\n    // Logical right shift\n    __m256i logical_shifted = _mm256_srai_epi32(src_vec, shift);\n\n    // Add the results\n    __m256i sum = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n    // Store the result\n    _mm256_storeu_si256((__m256i *)dst, sum);\n\n    // Process remaining elements if necessary\n    for (size_t i = length % 4; i > 0; --i) {\n        dst[length - i] = (int)((uint32_t)(src[length - i]) >> shift) + ((uint32_t)(src[length - i]) >> shift);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i src_vec = _mm256_loadu_si256((__m256i const *)src);\n    __m256i dst_vec = _mm256_setzero_si256();\n\n    // Arithmetic right shift\n    __m256i arith_shifted = _mm256_srli_epi32(src_vec, shift);\n\n    // Logical right shift\n    __m256i logical_shifted = _mm256_srai_epi32(src_vec, shift);\n    __m256i cast_logical = _mm256_castsi256_si32(logical_shifted);\n    __m256i shifted_back = _mm256_cvtepi32_ps(cast_logical);\n    __m256i logical_shifted_rounded = _mm256_roundps(shifted_back, _MM_FROUND_TO_ZERO);\n    __m256i final_logical_shifted = _mm256_cvtss_si32(logical_shifted_rounded);\n\n    // Add the results\n    __m256i sum = _mm256_add_epi32(arith_shifted, final_logical_shifted);\n\n    _mm256_storeu_si256((__m256i *)dst, sum);\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i src_vec = _mm256_loadu_si256((__m256i const *)src);\n    __m256i dst_vec = _mm256_setzero_si256();\n\n    // Arithmetic right shift\n    __m256i arith_shifted = _mm256_srli_epi32(src_vec, shift);\n\n    // Logical right shift\n    __m256i logical_shifted = _mm256_srai_epi32(_mm256_cvtepi32_epi16(src_vec), shift);\n    logical_shifted = _mm256_cvtepi16_epi32(logical_shifted);\n\n    // Add the results\n    __m256i sum = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n    // Store the results\n    _mm256_storeu_si256((__m256i *)dst, sum);\n\n    // Process remaining elements if necessary\n    for (size_t i = length % 4; i > 0; --i) {\n        dst[length - i] = (int)((src[length - i] >> shift) | ((src[length - i] & 0xFFFF) >> shift));\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i src_vec = _mm256_loadu_si256((__m256i const *)src);\n    __m256i dst_vec = _mm256_setzero_si256();\n\n    // Arithmetic right shift\n    __m256i arith_shifted = _mm256_srli_epi32(src_vec, shift);\n\n    // Logical right shift\n    __m256i logical_shifted = _mm256_srai_epi32(src_vec, shift);\n\n    // Add the results\n    __m256i sum = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n    // Store the result\n    _mm256_storeu_si256((__m256i *)dst, sum);\n\n    // Process remaining elements if necessary\n    for (size_t i = 4; i < length; i += 4) {\n        src_vec = _mm256_loadu_si256((__m256i const *)(src + i));\n        dst_vec = _mm256_setzero_si256();\n\n        arith_shifted = _mm256_srli_epi32(src_vec, shift);\n        logical_shifted = _mm256_srai_epi32(src_vec, shift);\n        sum = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n        _mm256_storeu_si256((__m256i *)(dst + i), sum);\n    }\n}\n```"}
{"task_id": "SimdBench_12_AVX", "completion": "```c\nvoid mixed_right_shift_simd(const int *src, int *dst, uint8_t shift, size_t length) {\n    __m256i src_vec = _mm256_loadu_si256((__m256i const *)src);\n    __m256i dst_vec = _mm256_setzero_si256();\n\n    // Arithmetic right shift\n    __m256i arith_shifted = _mm256_srli_epi32(src_vec, shift);\n\n    // Logical right shift\n    __m256i logical_shifted = _mm256_srai_epi32(src_vec, shift);\n\n    // Add the results\n    __m256i sum = _mm256_add_epi32(arith_shifted, logical_shifted);\n\n    // Store the result\n    _mm256_storeu_si256((__m256i *)dst, sum);\n\n    // Process remaining elements if necessary\n    for (size_t i = 4; i < length; i += 4) {\n        src_vec = _mm_loadu_si256((__m256i const *)(src + i));\n        arith_shifted = _mm256_srli_epi32(src_vec, shift);\n        logical_shifted = _mm256_srai_epi32(src_vec, shift);\n        sum = _mm256_add_epi32(arith_shifted, logical_shifted);\n        _mm256_storeu_si256((__m256i *)(dst + i), sum);\n    }\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const __m256i *ptr = (__m256i *)matrix;\n    int64_t sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m256i vec = _mm256_loadu_si256(ptr + i * 4 + j);\n            sum += _mm256_reduce_add_epi64(vec);\n        }\n        if (cols % 4 != 0) {\n            __m128i vec = _mm_loadu_si128((const __m128i *)(ptr + i * 4 + cols / 4));\n            sum += _mm_reduce_add_epi64(_mm256_extractf128_si256(vec, 0)) +\n                   _mm_reduce_add_epi64(_mm256_extractf128_si256(vec, 1));\n        }\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const __m256i *ptr = (__m256i *)matrix;\n    int64_t sum = 0;\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256i vec = _mm256_loadu_si256(ptr++);\n        sum += _mm256_reduce_add_epi64(vec);\n    }\n\n    // Handle remaining elements if there are less than 8 elements per vector\n    for (size_t i = 0; i < rows % 8; ++i) {\n        sum += matrix[i];\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const int *end = matrix + rows * cols;\n    __m256i sum = _mm256_setzero_si256();\n    while (matrix < end) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)matrix);\n        sum = _mm256_add_epi64(sum, vec);\n        matrix += 8;\n    }\n    // Reduce the sum to a single 64-bit integer\n    int64_t result = 0;\n    for (size_t i = 0; i < 8; ++i) {\n        result += _mm256_extract_epi64(sum, i);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const int *end = matrix + rows * cols;\n    __m256i sum = _mm256_setzero_si256();\n    while (matrix < end) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)matrix);\n        sum = _mm256_add_epi64(sum, vec);\n        matrix += 8;\n    }\n    // Reduce the sum to a single 64-bit integer\n    int64_t result = 0;\n    for (size_t i = 0; i < 8; ++i) {\n        result += _mm256_extract_epi64(sum, i);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_13_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t matrix_sum_simd(const int *matrix, size_t rows, size_t cols) {\n    const int *end = matrix + rows * cols;\n    __m256i sum = _mm256_setzero_si256();\n    while (matrix < end) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)matrix);\n        sum = _mm256_add_epi64(sum, vec);\n        matrix += 8;\n    }\n    // Reduce the sum to a single 64-bit integer\n    int64_t result = _mm256_extract_epi64(_mm256_reduce_add_epi64(sum), 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_val = _mm_set1_epi8(-128); // Initialize max_val to -128 (the smallest possible 8-bit integer)\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i vec = _mm_loadu_si256((__m256i *)(src + i));\n        __m256i cmp = _mm_cmpgt_epi8(vec, max_val);\n        __m256i mask = _mm_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            size_t idx = __builtin_ctz(mask); // Find the index of the first set bit\n            if (idx >= 32) {\n                idx -= 32;\n            }\n            if (idx >= length) {\n                idx = length - 1;\n            }\n            if (idx > max_index) {\n                max_index = idx;\n            }\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_val = _mm256_set1_epi8(-128); // Initialize max_val to -128 (the smallest possible int8_t)\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i current_vals = _mm256_loadu_si256((__m256i *)(&src[i]));\n        __m256i cmp = _mm256_cmpgt_epi8(current_vals, max_val);\n        __m256i mask = _mm256_movemask_epi8(cmp);\n\n        if (mask != 0) {\n            size_t idx = _mm256_extract_epi32(mask, 0);\n            if (idx > max_index) {\n                max_index = idx;\n            }\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_val = _mm_set1_epi8(-128); // Initialize max_val to -128 (the smallest possible 8-bit integer)\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i vec = _mm_loadu_si256((__m256i *)(src + i));\n        __m256i cmp = _mm_cmpgt_epi8(vec, max_val);\n        __m256i mask = _mm_movemask_epi8(cmp);\n\n        size_t idx = __builtin_ctz(mask); // Find the first set bit in the mask\n        if (idx > max_index) {\n            max_index = idx;\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_val = _mm256_set1_epi8(-128); // Initialize max_val to -128 (the smallest possible 8-bit integer)\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i vec = _mm256_loadu_si256((__m256i *)(&src[i]));\n        __m256i cmp = _mm256_cmpgt_epi8(vec, max_val);\n        __m256i idx = _mm256_and_si256(cmp, _mm256_setr_epi32(0, 4, 8, 12));\n        __m256i mask = _mm256_movemask_epi8(idx);\n\n        if (mask != 0) {\n            size_t idx_masked = _mm256_extract_epi32(mask, 0);\n            if (idx_masked > max_index) {\n                max_index = idx_masked;\n            }\n        }\n    }\n\n    // Handle remaining elements\n    for (size_t i = length % 32; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_14_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t argmax_simd(const int8_t *src, size_t length) {\n    if (length == 0) {\n        return 0;\n    }\n\n    __m256i max_val = _mm_set1_epi8(-128); // Initialize max_val to -128 (the smallest possible int8_t)\n    size_t max_index = 0;\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i vec = _mm_loadu_si256((__m256i *)(&src[i]));\n        __m256i cmp = _mm_cmpgt_epi8(vec, max_val);\n        __m256i mask = _mm_movemask_epi8(cmp);\n\n        size_t idx = i + _mm_popcnt_u32(mask);\n        if (idx > max_index) {\n            max_index = idx;\n        }\n    }\n\n    for (; i < length; ++i) {\n        if (src[i] > src[max_index]) {\n            max_index = i;\n        }\n    }\n\n    return max_index;\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c++\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i mask_vec = _mm256_set_epi32(mask & 0xFF, mask & 0xFF00, mask & 0xFF0000, mask & 0xFF000000);\n    __m256i src1_vec;\n    __m256i src2_vec;\n\n    for (size_t i = 0; i < length; i += 4) {\n        src1_vec = _mm256_loadu_si256((__m256i*)(&src1[i]));\n        src2_vec = _mm256_loadu_si256((__m256i*)(&src2[i]));\n\n        __m256i blended_vec = _mm256_and_si256(src1_vec, mask_vec);\n        blended_vec = _mm256_or_si256(blended_vec, src2_vec);\n\n        _mm256_storeu_si256((__m256i*)(&dst[i]), blended_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c++\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i mask_vec = _mm_set_epi32(mask & 0xFF, mask & 0xFF, mask & 0xFF, mask & 0xFF);\n    __m256i src1_vec;\n    __m256i src2_vec;\n\n    for (size_t i = 0; i < length; i += 4) {\n        src1_vec = _mm_loadu_si256((__m256i*)(src1 + i));\n        src2_vec = _mm_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i blended_vec = _mm_and_si256(src1_vec, mask_vec);\n        __m256i result_vec = _mm_or_si256(blended_vec, src2_vec);\n\n        _mm_storeu_si256((__m256i*)(dst + i), result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c++\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i mask_vec = _mm256_set_epi32(mask & 0xFF, mask & 0xFF00, mask & 0xFF0000, mask & 0xFF000000);\n    __m256i src1_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src1));\n    __m256i src2_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src2));\n\n    __m256i blended_vec = _mm256_and_si256(src1_vec, mask_vec);\n    __m256i unblended_vec = _mm256_or_si256(src2_vec, mask_vec);\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(dst), blended_vec);\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(dst + 4), unblended_vec);\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c++\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i mask_vec = _mm256_set_epi32(mask & 0xFF, mask & 0xFF00, mask & 0xFF0000, mask & 0xFF000000);\n    __m256i src1_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src1));\n    __m256i src2_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(src2));\n\n    __m256i blended_vec = _mm256_and_si256(src1_vec, mask_vec);\n    __m256i result_vec = _mm256_or_si256(blended_vec, src2_vec);\n\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(dst), result_vec);\n}\n```"}
{"task_id": "SimdBench_15_AVX", "completion": "```c++\nvoid vector_blend_simd(const uint16_t* src1, const uint16_t* src2, uint32_t mask, uint16_t* dst, size_t length) {\n    __m256i mask_vec = _mm_set_epi32(mask & 0xFF, mask & 0xFF00, mask & 0xFF0000, mask & 0xFF000000);\n    __m256i src1_vec;\n    __m256i src2_vec;\n\n    for (size_t i = 0; i < length; i += 4) {\n        src1_vec = _mm_loadu_si256((__m256i*)(&src1[i]));\n        src2_vec = _mm_loadu_si256((__m256i*)(&src2[i]));\n\n        __m256i blended_vec = _mm_and_si256(src1_vec, mask_vec);\n        __m256i result_vec = _mm_or_si256(blended_vec, src2_vec);\n\n        _mm_storeu_si256((__m256i*)(&dst[i]), result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m256i sum = _mm256_setzero_si256();\n    const __m256i* end = (const __m256i*)(((size_t)src + length - 1) & ~0xF);\n\n    while (src < end) {\n        __m256i v = _mm256_loadu_si256(src);\n        sum = _mm256_add_epi32(sum, _mm256_popcnt_u32(v));\n        src += 4;\n    }\n\n    // Process remaining elements if there are less than 4 left\n    while (src < end) {\n        __m256i v = _mm256_loadu_si256(src);\n        sum = _mm256_add_epi32(sum, _mm256_popcnt_u32(v));\n        src++;\n    }\n\n    // Store the result back into the destination array\n    for (size_t i = 0; i < length; ++i) {\n        dst[i] = _mm256_extract_epi32(sum, i);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m256i sum = _mm256_setzero_si256();\n    const __m256i* end = reinterpret_cast<const __m256i*>(src + length - 4);\n\n    while (src < end) {\n        __m256i v = _mm256_loadu_si256(src);\n        sum = _mm256_add_epi32(sum, _mm256_popcnt_u32(v));\n        src += 4;\n    }\n\n    // Process remaining elements if there are less than 4 left\n    if (src < src + length) {\n        __m256i v = _mm256_loadu_si256(src);\n        sum = _mm256_add_epi32(sum, _mm256_popcnt_u32(v));\n    }\n\n    // Store the result back to the destination array\n    for (size_t i = 0; i < length; ++i) {\n        dst[i] = _mm256_extract_epi32(sum, i);\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m256i mask = _mm_set_epi32(0x0F, 0x0F, 0x0F, 0x0F); // Mask to extract the least significant 4 bits\n    __m256i sum = _mm_setzero_si256(); // Initialize sum to zero\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i vec = _mm_loadu_si256((__m256i*)(&src[i])); // Load 8 elements from source into a vector\n        vec = _mm_and_si256(vec, mask); // Extract the least significant 4 bits\n        vec = _mm_popcnt_u32(_mm_extract_epi32(vec, 0)); // Count the number of set bits\n        sum = _mm_add_epi32(sum, vec); // Add the count to the sum\n    }\n\n    for (size_t i = 8; i < length; ++i) {\n        dst[i] = _mm_popcnt_u32(src[i]); // Count the number of set bits for remaining elements\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m256i mask = _mm_set_epi32(0x000000ff, 0x0000ffff, 0x00ffffff, 0xffffffff); // Mask to isolate the least significant byte\n    __m256i sum = _mm_setzero_si256(); // Initialize sum to zero\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec = _mm_loadu_si256((__m256i*)(&src[i])); // Load four elements from the source array into a vector\n        vec = _mm_and_si256(vec, mask); // Apply the mask to isolate the least significant byte\n        vec = _mm_popcnt_u32(_mm_extract_epi32(vec, 0)); // Count the number of set bits in the least significant byte\n        sum = _mm_add_epi32(sum, vec); // Add the count to the sum\n    }\n\n    for (size_t i = 0; i < length % 4; ++i) {\n        uint32_t val = src[length - i - 1]; // Load the last remaining element from the source array\n        val &= 0xff; // Isolate the least significant byte\n        val = __builtin_popcount(val); // Count the number of set bits in the least significant byte\n        dst[length - i - 1] = val; // Store the count in the destination array\n    }\n}\n```"}
{"task_id": "SimdBench_16_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid population_count_simd(const uint32_t* src, uint32_t* dst, size_t length) {\n    __m256i sum = _mm256_setzero_si256();\n    const __m256i mask = _mm256_set_epi32(0x55555555, 0x55555555, 0x55555555, 0x55555555);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i vec = _mm_loadu_si256((__m256i*)(src + i));\n        vec = _mm_and_si256(vec, mask);\n        vec = _mm_add_epi32(_mm_sad_epu8(vec, _mm_setzero_si8()), _mm_sad_epu8(_mm_shuffle_epi8(vec, vec), _mm_setzero_si8()));\n        sum = _mm_add_epi32(sum, vec);\n    }\n\n    // Reduce the sum to a single value\n    for (int j = 0; j < 4; ++j) {\n        *dst++ = _mm_extract_epi32(sum, j);\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m256i sum;\n    __m256i max_val = _mm_set1_epi16(UINT16_MAX);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i v1 = _mm_loadu_si256((__m256i*)src1 + i);\n        __m256i v2 = _mm_loadu_si256((__m256i*)src2 + i);\n        __m256i saturated_sum = _mm_add_epi16(v1, v2);\n        saturated_sum = _mm_max_epi16(saturated_sum, max_val);\n        _mm_storeu_si256((__m256i*)dst + i, saturated_sum);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length % 4; i < length; ++i) {\n        uint16_t val1 = src1[i];\n        uint16_t val2 = src2[i];\n        uint16_t saturated_sum = val1 + val2;\n        saturated_sum = saturated_sum > UINT16_MAX ? UINT16_MAX : saturated_sum;\n        dst[i] = saturated_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i v1 = _mm_loadu_si256((__m256i*)src1 + i);\n        __m256i v2 = _mm_loadu_si256((__m256i*)src2 + i);\n        __m256i saturated_sum = _mm_adds_epu16(v1, v2);\n        saturated_sum = _mm_min_epu16(saturated_sum, max_val);\n        _mm_storeu_si256((__m256i*)dst + i, saturated_sum);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length % 8; i > 0; --i) {\n        uint16_t val1 = *src1++;\n        uint16_t val2 = *src2++;\n        uint16_t saturated_sum = val1 + val2;\n        saturated_sum = saturated_sum > UINT16_MAX ? UINT16_MAX : saturated_sum;\n        *dst++ = saturated_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m128i sum;\n    __m128i max_val = _mm_set1_epi16(UINT16_MAX);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i a = _mm_loadu_si128((__m128i*)src1 + i);\n        __m128i b = _mm_loadu_si128((__m128i*)src2 + i);\n        __m128i c = _mm_add_epi16(a, b);\n        sum = _mm_max_epi16(c, max_val);\n        _mm_storeu_si128((__m128i*)dst + i, sum);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length - (length % 4); i < length; ++i) {\n        uint16_t a = src1[i];\n        uint16_t b = src2[i];\n        uint16_t c = a + b;\n        if (c > UINT16_MAX) {\n            c = UINT16_MAX;\n        }\n        dst[i] = c;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m256i sum = _mm256_setzero_si256();\n    __m256i max_val = _mm256_set1_epi16(UINT16_MAX);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i a = _mm_loadu_si256((__m256i*)(src1 + i));\n        __m256i b = _mm_loadu_si256((__m256i*)(src2 + i));\n\n        __m256i saturated_sum = _mm_add_epi16(a, b);\n        saturated_sum = _mm_max_epi16(saturated_sum, max_val);\n\n        _mm_storeu_si256((__m256i*)(dst + i), saturated_sum);\n    }\n\n    // Handle remaining elements\n    for (size_t i = length % 4; i > 0; --i) {\n        uint16_t a = src1[i];\n        uint16_t b = src2[i];\n\n        uint16_t saturated_sum = a + b;\n        saturated_sum = saturated_sum > UINT16_MAX ? UINT16_MAX : saturated_sum;\n\n        dst[i] = saturated_sum;\n    }\n}\n```"}
{"task_id": "SimdBench_17_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid saturating_add_simd(const uint16_t* src1, const uint16_t* src2, uint16_t* dst, size_t length) {\n    __m128i* ptr_dst = (__m128i*)dst;\n    __m128i* ptr_src1 = (__m128i*)src1;\n    __m128i* ptr_src2 = (__m128i*)src2;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m128i sum = _mm_add_epi16(*ptr_src1++, *ptr_src2++);\n        __m128i saturated_sum = _mm_max_epi16(sum, _mm_set1_epi16(UINT16_MAX));\n        *ptr_dst++ = saturated_sum;\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (size_t i = length % 4; i > 0; --i) {\n        uint16_t sum = *ptr_src1++ + *ptr_src2++;\n        *ptr_dst++ = _mm_max_epi16(_mm_set1_epi16(sum), _mm_set1_epi16(UINT16_MAX));\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask to skip out-of-range elements\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                if (_mm_testz_si128(_mm_loadu_si128((__m128i*)&mask), _mm_loadu_si128((__m128i*)&A[k * m + i]))) {\n                    continue;\n                }\n                if (_mm_testz_si128(_mm_loadu_si128((__m128i*)&mask), _mm_loadu_si128((__m128i*)&B[j * n + k]))) {\n                    continue;\n                }\n                sum += _mm_cvtss_f64(_mm_add_ps(_mm_loadu_ps((__m128*)&A[k * m + i]), _mm_loadu_ps((__m128*)&B[j * n + k])));\n            }\n            _mm_storeu_si128((__m128i*)&C[j * m + i], _mm_cvtsd_ss(_mm_setzero_ps(), sum));\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c++\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256i mask = _mm256_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF,\n                                     0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                if (A[i * n + k] >= -100 && A[i * n + k] <= 100 &&\n                    B[k * p + j] >= -100 && B[k * p + j] <= 100) {\n                    __m256 a = _mm256_loadu_ps(&A[i * n + k]);\n                    __m256 b = _mm256_loadu_ps(&B[k * p + j]);\n                    __m256 c = _mm256_fmadd_ps(a, b, _mm256_setzero_ps());\n                    sum += _mm256_reduce_add_ps(c);\n                }\n            }\n            C[i * p + j] = sum;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256i mask = _mm256_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask to skip out-of-range values\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < n; ++k) {\n                if (_mm256_testz_si256(_mm256_loadu_si256((__m256i*)&A[i*n + k]), mask)) continue; // Skip if out-of-range\n                if (_mm256_testz_si256(_mm256_loadu_si256((__m256i*)&B[k*p + j]), mask)) continue; // Skip if out-of-range\n                sum += _mm256_reduce_add_ps(_mm256_fmadd_pd(_mm256_loadu_si256((__m256i*)&A[i*n + k]), _mm256_loadu_si256((__m256i*)&B[k*p + j]))); // Add element-wise product\n            }\n            _mm256_storeu_si256((__m256i*)&C[i*p + j], _mm256_setzero_ps()); // Initialize C[i][j] to zero\n            _mm256_storeu_si256((__m256i*)&C[i*p + j], _mm256_add_ps(_mm256_setzero_ps(), _mm256_set1_ps(sum))); // Store the computed sum\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask for values in the range [-100, 100]\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256d sum = _mm_setzero_pd(); // Initialize sum\n\n            for (size_t k = 0; k < n; ++k) {\n                if (_mm_test_epi32(_mm_loadu_si128((__m128i*)&A[i*n + k]), mask)) { // Check if element is in the range\n                    __m256d a = _mm_loadu_pd(&A[i*n + k]); // Load element from A\n                    __m256d b = _mm_loadu_pd(&B[k*p + j]); // Load element from B\n                    sum = _mm_add_pd(sum, _mm_mul_pd(a, b)); // Multiply and add to sum\n                }\n            }\n\n            _mm_storeu_pd(&C[i*p + j], sum); // Store result in C\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_18_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid range_matrix_mul_simd(const double* A, const double* B, double* C, size_t m, size_t n, size_t p) {\n    __m256i mask = _mm256_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF,\n                                      0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256 sum = _mm256_setzero_ps();\n            for (size_t k = 0; k < n; ++k) {\n                if (A[i * n + k] >= -100 && A[i * n + k] <= 100 &&\n                    B[k * p + j] >= -100 && B[k * p + j] <= 100) {\n                    __m256 a = _mm256_loadu_ps(&A[i * n + k]);\n                    __m256 b = _mm256_loadu_ps(&B[k * p + j]);\n                    sum = _mm256_add_ps(sum, _mm256_mul_ps(a, b));\n                }\n            }\n            _mm_storeu_ps(&C[i * p + j], sum);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i* ptr_A = (__m256i*)A;\n    __m256i* ptr_B = (__m256i*)B;\n    __m256i* ptr_C = (__m256i*)C;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t k = 0; k < dim3; ++k) {\n                __m256i sum = _mm256_loadu_si256(ptr_A + i * dim2 * dim3 + j * dim3 + k);\n                sum = _mm256_add_epi64(sum, _mm256_loadu_si256(ptr_B + i * dim2 * dim3 + j * dim3 + k));\n                _mm256_storeu_si256(ptr_C + i * dim2 * dim3 + j * dim3 + k, sum);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c++\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i* ptr_A = (__m256i*)A;\n    __m256i* ptr_B = (__m256i*)B;\n    __m256i* ptr_C = (__m256i*)C;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t k = 0; k < dim3; ++k) {\n                __m256i sum = _mm256_loadu_si256(ptr_A + i * dim2 * dim3 + j * dim3 + k);\n                sum = _mm256_add_epi64(sum, _mm256_loadu_si256(ptr_B + i * dim2 * dim3 + j * dim3 + k));\n                _mm256_storeu_si256(ptr_C + i * dim2 * dim3 + j * dim3 + k, sum);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i* ptr_A = (__m256i*)A;\n    __m256i* ptr_B = (__m256i*)B;\n    __m256i* ptr_C = (__m256i*)C;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t k = 0; k < dim3; ++k) {\n                __m256i sum = _mm256_loadu_si256(ptr_A + i * dim2 * dim3 + j * dim3 + k);\n                sum = _mm256_add_epi64(sum, _mm256_loadu_si256(ptr_B + i * dim2 * dim3 + j * dim3 + k));\n                _mm256_storeu_si256(ptr_C + i * dim2 * dim3 + j * dim3 + k, sum);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i* ptr_A = (__m256i*)A;\n    __m256i* ptr_B = (__m256i*)B;\n    __m256i* ptr_C = (__m256i*)C;\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += 8) {\n        __m256i sum = _mm256_loadu_si256(ptr_A + i);\n        __m256i add = _mm256_loadu_si256(ptr_B + i);\n        sum = _mm256_add_epi64(sum, add);\n        _mm256_storeu_si256(ptr_C + i, sum);\n    }\n}\n```"}
{"task_id": "SimdBench_19_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_add_3d_simd(const int64_t* A, const int64_t* B, int64_t* C, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i* ptr_A = (__m256i*)A;\n    __m256i* ptr_B = (__m256i*)B;\n    __m256i* ptr_C = (__m256i*)C;\n\n    for (size_t i = 0; i < dim1; ++i) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t k = 0; k < dim3; ++k) {\n                __m256i sum = _mm256_loadu_si256(ptr_A + i * dim2 * dim3 + j * dim3 + k);\n                sum = _mm256_add_epi64(sum, _mm256_loadu_si256(ptr_B + i * dim2 * dim3 + j * dim3 + k));\n                _mm256_storeu_si256(ptr_C + i * dim2 * dim3 + j * dim3 + k, sum);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256i v_src1 = _mm256_loadu_si256((__m256i*)src1);\n    __m256i v_src2 = _mm256_loadu_si256((__m256i*)src2);\n    __m256i v_dst = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256f f_src1 = _mm256_castsi256_ps(v_src1);\n        __m256f f_src2 = _mm256_castsi256_ps(v_src2);\n\n        __m256f f_sum = _mm256_add_ps(f_src1, f_src2);\n        __m256i v_rounded = _mm256_cvtps_epi32(_mm256_round_ps(f_sum, _MM_FROUND_TO_NEAREST));\n        v_dst = _mm256_or_si256(v_dst, v_rounded);\n\n        v_src1 = _mm256_slli_epi32(v_src1, 8);\n        v_src2 = _mm256_slli_epi32(v_src2, 8);\n    }\n\n    if (length % 4 > 0) {\n        __m128i v_src1 = _mm_loadu_si128((__m128i*)(&src1[length / 4 * 4]));\n        __m128i v_src2 = _mm_loadu_si128((__m128i*)(&src2[length / 4 * 4]));\n\n        __m128f f_src1 = _mm_castsi128_ps(v_src1);\n        __m128f f_src2 = _mm_castsi128_ps(v_src2);\n\n        __m128f f_sum = _mm_add_ps(f_src1, f_src2);\n        __m128i v_rounded = _mm_cvtps_epi32(_mm_round_ps(f_sum, _MM_FROUND_TO_NEAREST));\n        v_dst = _mm_or_si128(v_dst, v_rounded);\n\n        v_src1 = _mm_slli_epi32(v_src1, 8);\n        v_src2 = _mm_slli_epi32(v_src2, 8);\n    }\n\n    _mm_storeu_si256((__m256i*)dst, v_dst);\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 sum;\n    __m256 rounded_sum;\n\n    for (size_t i = 0; i < length - 7; i += 8) {\n        // Load two 32-bit floats from src1 and src2 into m256 registers\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n\n        // Add the two vectors\n        sum = _mm256_add_ps(v1, v2);\n\n        // Round each element of the sum to the nearest integer\n        rounded_sum = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT);\n\n        // Store the rounded sum back to dst\n        _mm256_storeu_ps(dst + i, rounded_sum);\n    }\n\n    // Handle remaining elements if there are less than 8 elements left\n    for (size_t i = length - (length % 8); i < length; ++i) {\n        dst[i] = static_cast<float>(roundf(src1[i] + src2[i]));\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 sum;\n    __m256 rounded_sum;\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load two 32-bit floats from src1 and src2 into m256 registers\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n\n        // Add the two vectors\n        sum = _mm256_add_ps(v1, v2);\n\n        // Round each element of the sum to the nearest integer\n        rounded_sum = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT);\n\n        // Store the rounded sum back to dst\n        _mm256_storeu_ps(dst + i, rounded_sum);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 sum;\n    __m256 rounded_sum;\n\n    for (size_t i = 0; i < length; i += 8) {\n        // Load two 32-bit floats from src1 and src2 into m256 registers\n        __m256 v1 = _mm256_loadu_ps(src1 + i);\n        __m256 v2 = _mm256_loadu_ps(src2 + i);\n\n        // Add the two vectors\n        sum = _mm256_add_ps(v1, v2);\n\n        // Round each element of the sum to the nearest integer\n        rounded_sum = _mm256_round_ps(sum, _MM_FROUND_TO_NEAREST_INT);\n\n        // Store the rounded sum back into dst\n        _mm256_storeu_ps(dst + i, rounded_sum);\n    }\n}\n```"}
{"task_id": "SimdBench_20_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_add_round_nearest_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256* ptr_src1 = (__m256*)src1;\n    __m256* ptr_src2 = (__m256*)src2;\n    __m256* ptr_dst = (__m256*)dst;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256 sum = _mm_add_ps(*ptr_src1++, *ptr_src2++);\n        __m256 rounded = _mm_round_ps(sum, _MM_FROUND_TO_NEAREST_INT);\n        *ptr_dst++ = _mm_cvtss_f32(rounded);\n    }\n\n    // Handle remaining elements if length is not divisible by 4\n    for (size_t i = length % 4; i > 0; --i) {\n        float sum = *ptr_src1++;\n        float rounded = floorf(sum + 0.5f);\n        *ptr_dst++ = rounded;\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length){\n    __m256 mask = _mm_setzero_ps();\n    __m256 abs_diff = _mm_setzero_ps();\n\n    for(size_t i = 0; i < length; i += 4){\n        __m256 v1 = _mm_loadu_ps(src1 + i);\n        __m256 v2 = _mm_loadu_ps(src2 + i);\n\n        // Calculate absolute difference\n        abs_diff = _mm_sub_ps(v1, v2);\n        abs_diff = _mm_abs_ps(abs_diff);\n\n        // Check if absolute difference is within 50\n        mask = _mm_cmpgt_ps(abs_diff, _mm_set1_ps(50));\n\n        // Round up towards positive infinity\n        __m256 rounded_up = _mm_ceil_ps(_mm_add_ps(v1, v2));\n        rounded_up = _mm_and_ps(rounded_up, mask);\n\n        // Store the result\n        _mm_storeu_ps(dst + i, rounded_up);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 mask = _mm_setzero_ps();\n    __m256 abs_diff = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256 v1 = _mm_loadu_ps(src1 + i);\n        __m256 v2 = _mm_loadu_ps(src2 + i);\n\n        // Calculate the absolute difference between v1 and v2\n        abs_diff = _mm_sub_ps(v1, v2);\n        abs_diff = _mm_abs_ps(abs_diff);\n\n        // Check if the absolute difference is within 50\n        mask = _mm_cmpgt_ps(_mm_and_ps(abs_diff, _mm_set1_ps(50)), _mm_setzero_ps());\n\n        // Round up the result towards positive infinity\n        __m256 rounded_up = _mm_ceil_ps(_mm_or_ps(mask, _mm_set1_ps(-1)));\n\n        // Store the result in the dst array\n        _mm_storeu_ps(dst + i, rounded_up);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 mask = _mm_setzero_ps();\n    __m256 abs_diff = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256 v1 = _mm_loadu_ps(src1 + i);\n        __m256 v2 = _mm_loadu_ps(src2 + i);\n\n        // Calculate the absolute difference between v1 and v2\n        abs_diff = _mm_sub_ps(v1, v2);\n        abs_diff = _mm_abs_ps(abs_diff);\n\n        // Check if the absolute difference is within 50\n        mask = _mm_cmpgt_ps(_mm_and_ps(abs_diff, _mm_set1_ps(50)), _mm_setzero_ps());\n\n        // Round up the result towards positive infinity\n        __m256 rounded = _mm_ceil_ps(v1 * v2);\n        rounded = _mm_or_ps(rounded, mask);\n\n        // Store the result in the dst array\n        _mm_storeu_ps(dst + i, rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 mask = _mm_setzero_ps();\n    __m256 abs_diff = _mm_setzero_ps();\n    __m256 result = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256 v1 = _mm_loadu_ps(src1 + i);\n        __m256 v2 = _mm_loadu_ps(src2 + i);\n\n        // Calculate the absolute difference between v1 and v2\n        abs_diff = _mm_sub_ps(v1, v2);\n        abs_diff = _mm_abs_ps(abs_diff);\n\n        // Check if the absolute difference is within 50\n        mask = _mm_cmpgt_ps(abs_diff, _mm_set1_ps(50));\n\n        // Round up the result towards positive infinity\n        result = _mm_ceil_ps(_mm_mul_ps(v1, v2));\n\n        // Store the result in the dst array\n        _mm_storeu_ps(dst + i, result);\n    }\n}\n```"}
{"task_id": "SimdBench_21_AVX", "completion": "```c\nvoid vector_mul_round_up_simd(const float* src1, const float* src2, float* dst, size_t length) {\n    __m256 mask = _mm_setzero_ps();\n    __m256 abs_diff = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256 v1 = _mm_loadu_ps(src1 + i);\n        __m256 v2 = _mm_loadu_ps(src2 + i);\n\n        // Calculate the absolute difference between v1 and v2\n        abs_diff = _mm_sub_ps(v1, v2);\n        abs_diff = _mm_abs_ps(abs_diff);\n\n        // Check if the absolute difference is within 50\n        mask = _mm_cmpgt_ps(_mm_and_ps(abs_diff, _mm_set1_ps(50)), _mm_setzero_ps());\n\n        // Round up the result towards positive infinity\n        __m256 rounded_up = _mm_ceil_ps(v1 * v2);\n        rounded_up = _mm_or_ps(rounded_up, mask);\n\n        // Store the result in the dst array\n        _mm_storeu_ps(dst + i, rounded_up);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const __m256d scalar_vec = _mm_set_pd(scalar, scalar);\n\n    for (size_t i = 0; i < m * n; i += 8) {\n        __m256d vec1 = _mm_loadu_pd(&mat[i]);\n        __m256d vec2 = _mm_mul_pd(vec1, scalar_vec);\n        _mm_storeu_pd(&dst[i], vec2);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const __m256d scalar_vec = _mm_set_pd(scalar, scalar);\n\n    for (size_t i = 0; i < m * n; i += 32) {\n        const __m256d vec = _mm_loadu_pd(&mat[i]);\n        __m256d result = _mm_mul_pd(vec, scalar_vec);\n        _mm_storeu_pd(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    const __m256d scalar_vec = _mm_set_pd(scalar);\n\n    for (size_t i = 0; i < m * n; i += 32) {\n        __m256d vec1 = _mm_loadu_pd(&mat[i]);\n        __m256d vec2 = _mm_mul_pd(vec1, scalar_vec);\n        _mm_storeu_pd(&dst[i], vec2);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm_set_pd(scalar);\n\n    for (size_t i = 0; i < m * n; i += 32) {\n        const __m256d vec = _mm_loadu_pd(&mat[i]);\n        __m256d result = _mm_mul_pd(vec, scalar_vec);\n        _mm_storeu_pd(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_22_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_scalar_mul_simd(const double* mat, double scalar, double* dst, size_t m, size_t n) {\n    __m256d scalar_vec = _mm_set_pd(scalar);\n\n    for (size_t i = 0; i < m * n; i += 32) {\n        __m256d vec1 = _mm_loadu_pd(&mat[i]);\n        __m256d vec2 = _mm_mul_pd(vec1, scalar_vec);\n        _mm_storeu_pd(&dst[i], vec2);\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; i += 4) {\n        float* row = &mat[i * n];\n        __m256 row_vec = _mm256_loadu_ps(row);\n\n        // Calculate the squared magnitude of the row vector\n        __m256 squared_magnitude = _mm256_mul_ps(row_vec, row_vec);\n        squared_magnitude = _mm256_add_ps(squared_magnitude, squared_magnitude);\n        squared_magnitude = _mm256_add_ps(squared_magnitude, squared_magnitude);\n        squared_magnitude = _mm256_add_ps(squared_magnitude, squared_magnitude);\n\n        // Divide each element in the row by the squared magnitude\n        __m256 inv_sqrt = _mm256.rsqrt_ps(squared_magnitude);\n        row_vec = _mm256_mul_ps(row_vec, inv_sqrt);\n\n        // Store the normalized row back into memory\n        _mm256_storeu_ps(row, row_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c++\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; i += 4) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 norm_squared = _mm256_mul_ps(row, row);\n        __m256 norm = _mm256_sqrt_ps(norm_squared);\n        __m256 normalized_row = _mm256_div_ps(row, norm);\n        _mm256_storeu_ps(mat + i * n, normalized_row);\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; i += 4) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 norm_squared = _mm256_mul_ps(row, row);\n        __m256 norm = _mm256_sqrt_ps(norm_squared);\n        __m256 normalized_row = _mm256_div_ps(row, norm);\n        _mm256_storeu_ps(mat + i * n, normalized_row);\n    }\n\n    // Handle remaining rows if there are fewer than 4 rows left\n    for (size_t i = m % 4; i < m; ++i) {\n        float sum = 0.0f;\n        for (size_t j = 0; j < n; ++j) {\n            sum += mat[i * n + j];\n        }\n        float norm = sqrt(sum);\n        for (size_t j = 0; j < n; ++j) {\n            mat[i * n + j] /= norm;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; i += 4) {\n        __m256 row = _mm256_loadu_ps(mat + i * n);\n        __m256 norm_squared = _mm256_mul_ps(row, row);\n        __m256 norm = _mm256_sqrt_ps(norm_squared);\n        __m256 normalized_row = _mm256_div_ps(row, norm);\n        _mm256_storeu_ps(mat + i * n, normalized_row);\n    }\n}\n```"}
{"task_id": "SimdBench_23_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_normalize_rows_simd(float* mat, size_t m, size_t n) {\n    __m256 zero = _mm256_setzero_ps();\n    __m256 one = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < m; i += 4) {\n        float* row = &mat[i * n];\n        __m256 row_vec = _mm256_loadu_ps(row);\n\n        // Calculate the squared magnitude of the row vector\n        __m256 squared_magnitude = _mm256_mul_ps(row_vec, row_vec);\n        squared_magnitude = _mm256_add_ps(squared_magnitude, squared_magnitude);\n        squared_magnitude = _mm256_add_ps(squared_magnitude, squared_magnitude);\n        squared_magnitude = _mm256_add_ps(squared_magnitude, squared_magnitude);\n\n        // Divide each element in the row by the squared magnitude\n        __m256 inv_sqrt = _mm256.rsqrt_ps(squared_magnitude);\n        row_vec = _mm256_mul_ps(row_vec, inv_sqrt);\n\n        // Store the normalized row back into the matrix\n        _mm256_storeu_ps(row, row_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n    __m256i v3 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < m; i += 4) {\n        for (size_t j = 0; j < n; j++) {\n            __m256 v4 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256 v5 = _mm256_loadu_pd(&mat2[j * p]);\n\n            __m256 v6 = _mm256_mul_ps(v4, v5);\n\n            __m256i v7 = _mm256_cvtps_epi32(v6);\n            v7 = _mm256_packus_epi32(v7, v7);\n\n            _mm256_storeu_si256((__m256i*)&dst[i * p], v7);\n        }\n    }\n\n    // Handle remaining elements if necessary\n    for (size_t i = m % 4; i < m; ++i) {\n        for (size_t j = 0; j < n; j++) {\n            __m256 v4 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256 v5 = _mm256_loadu_pd(&mat2[j * p]);\n\n            __m256 v6 = _mm256_mul_ps(v4, v5);\n\n            __m256i v7 = _mm256_cvtps_epi32(v6);\n            v7 = _mm256_packus_epi32(v7, v7);\n\n            _mm256_storeu_si256((__m256i*)&dst[i * p], v7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n    __m256i v3 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < m; i += 4) {\n        for (size_t j = 0; j < n; j++) {\n            __m256 v4 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256 v5 = _mm256_loadu_pd(&mat2[j * p]);\n\n            __m256 v6 = _mm256_mul_ps(v4, v5);\n            __m256 v7 = _mm256_round_ps(v6, _MM_FROUND_TO_NEAREST_INT);\n\n            _mm256_storeu_pd(&dst[i * p + j], v7);\n        }\n    }\n\n    // Handle remaining elements if m % 4 != 0\n    for (size_t i = m - (m % 4); i < m; i++) {\n        for (size_t j = 0; j < n; j++) {\n            __m256 v4 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256 v5 = _mm256_loadu_pd(&mat2[j * p]);\n\n            __m256 v6 = _mm256_mul_ps(v4, v5);\n            __m256 v7 = _mm256_round_ps(v6, _MM_FROUND_TO_NEAREST_INT);\n\n            _mm256_storeu_pd(&dst[i * p + j], v7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n    __m256i v3 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < m; i += 4) {\n        for (size_t j = 0; j < p; j++) {\n            __m256 v4 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256 v5 = _mm256_loadu_pd(&mat2[j * n]);\n\n            __m256 v6 = _mm256_mul_ps(v4, v5);\n            __m256 v7 = _mm256_round_ps(v6, _MM_FROUND_TO_NEAREST_INT);\n\n            _mm256_storeu_pd(&dst[i * p + j], v7);\n        }\n    }\n\n    // Handle remaining elements if there are less than 4 rows left\n    for (size_t i = m % 4; i < m; i++) {\n        for (size_t j = 0; j < p; j++) {\n            __m256 v4 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256 v5 = _mm256_loadu_pd(&mat2[j * n]);\n\n            __m256 v6 = _mm256_mul_ps(v4, v5);\n            __m256 v7 = _mm256_round_ps(v6, _MM_FROUND_TO_NEAREST_INT);\n\n            _mm256_storeu_pd(&dst[i * p + j], v7);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256i i0 = _mm256_set_epi32(0, 1, 2, 3);\n    __m256i i1 = _mm256_set_epi32(4, 5, 6, 7);\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256d sum = _mm256_setzero_pd();\n        for (size_t j = 0; j < n; ++j) {\n            __m256d v1 = _mm256_loadu_pd(&mat1[i * n + j]);\n            __m256d v2 = _mm256_loadu_pd(&mat2[j * p]);\n            __m256d v3 = _mm256_mul_pd(v1, v2);\n            sum = _mm256_add_pd(sum, v3);\n        }\n        __m256d rounded_sum = _mm256_round_pd(sum, _MM_FROUND_TO_NEAREST_INT);\n        _mm256_storeu_pd(&dst[i * p], rounded_sum);\n    }\n}\n```"}
{"task_id": "SimdBench_24_AVX", "completion": "```c\nvoid matrix_mul_round_int_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n, size_t p) {\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n    __m256i v3 = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < p; ++j) {\n            __m256 v4 = _mm256_loadu_ps(&mat1[i * n + j]);\n            __m256 v5 = _mm256_loadu_ps(&mat2[j * p]);\n\n            __m256 v6 = _mm256_mul_ps(v4, v5);\n\n            __m256i v7 = _mm256_cvtps_epi32(v6);\n            __m256i v8 = _mm256_round_ps(v7, _MM_FROUND_TO_NEAREST_INT);\n\n            _mm256_storeu_ps(&dst[i * p + j], v8);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c++\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256i mask = _mm256_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF); // Mask to keep only the lower 31 bits\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m256 v1 = _mm256_loadu_ps(&src[i * cols + j * 4]);\n            __m256 v2 = _mm256_loadu_ps(&src[i * cols + (j + 1) * 4]);\n            __m256 v3 = _mm256_loadu_ps(&src[i * cols + (j + 2) * 4]);\n            __m256 v4 = _mm256_loadu_ps(&src[i * cols + (j + 3) * 4]);\n\n            __m256 rounded_v1 = _mm256_round_ps(v1, _MM_FROUND_TO_NEAREST_INT);\n            __m256 rounded_v2 = _mm256_round_ps(v2, _MM_FROUND_TO_NEAREST_INT);\n            __m256 rounded_v3 = _mm256_round_ps(v3, _MM_FROUND_TO_NEAREST_INT);\n            __m256 rounded_v4 = _mm256_round_ps(v4, _MM_FROUND_TO_NEAREST_INT);\n\n            __m256 combined_v1 = _mm256_or_ps(rounded_v1, _mm256_andnot_ps(mask, rounded_v2));\n            __m256 combined_v2 = _mm256_or_ps(rounded_v2, _mm256_andnot_ps(mask, rounded_v3));\n            __m256 combined_v3 = _mm256_or_ps(rounded_v3, _mm256_andnot_ps(mask, rounded_v4));\n\n            _mm256_storeu_ps(&dst[j * rows + i], combined_v1);\n            _mm256_storeu_ps(&dst[(j + 1) * rows + i], combined_v2);\n            _mm256_storeu_ps(&dst[(j + 2) * rows + i], combined_v3);\n            _mm256_storeu_ps(&dst[(j + 3) * rows + i], combined_v4);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c++\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256i mask = _mm256_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF); // Mask to keep only the lower 31 bits\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m256 v = _mm256_loadu_ps(&src[i * cols + j * 4]);\n            __m256 rounded = _mm256_round_ps(v, _MM_FROUND_TO_NEAREST_INT);\n            rounded = _mm256_and_si256(rounded, mask);\n            _mm256_storeu_ps(&dst[j * rows + i], rounded);\n        }\n    }\n\n    // Handle remaining elements if cols is not divisible by 4\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = cols % 4; j < cols; ++j) {\n            float value = src[i * cols + j];\n            int rounded = static_cast<int>(std::round(value * 0.25));\n            dst[j * rows + i] = static_cast<float>(rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c++\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256i mask = _mm256_set_epi32(0x00000001, 0x00000001, 0x00000001, 0x00000001,\n                                          0x00000001, 0x00000001, 0x00000001, 0x00000001);\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols; ++j) {\n            __m256f src_vec = _mm256_loadu_ps(&src[i * cols + j]);\n            __m256f rounded_vec = _mm256_round_ps(src_vec, _MM_FROUND_TO_NEAREST_INT);\n            __m256i rounded_int_vec = _mm256_cvtps_epi32(rounded_vec);\n            __m256i shifted_int_vec = _mm256_slli_epi32(rounded_int_vec, 2);\n            __m256i masked_int_vec = _mm256_and_si256(shifted_int_vec, mask);\n            __m256f unshifted_float_vec = _mm256_cvtpi2ps(masked_int_vec);\n            _mm256_storeu_ps(&dst[j * rows + i], unshifted_float_vec);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256i mask = _mm_set_epi32(1, 0, 1, 0); // Mask to extract even and odd elements\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols / 2; ++j) {\n            __m256f src_row = _mm_loadu_ps(&src[i * cols + 2 * j]);\n            __m256f src_col = _mm_loadu_ps(&src[2 * j * cols + i]);\n\n            __m256f transposed_row = _mm_shuffle_ps(src_row, src_col, 0b0101);\n            __m256f transposed_col = _mm_shuffle_ps(src_row, src_col, 0b1010);\n\n            __m256f rounded_row = _mm_round_ps(transposed_row, _MM_FROUND_TO_NEAREST_INT);\n            __m256f rounded_col = _mm_round_ps(transposed_col, _MM_FROUND_TO_NEAREST_INT);\n\n            _mm_storeu_ps(&dst[j * rows + i], rounded_row);\n            _mm_storeu_ps(&dst[j * rows + i + cols / 2], rounded_col);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_25_AVX", "completion": "```c++\nvoid matrix_transpose_round_quarter_simd(const float* src, float* dst, size_t rows, size_t cols) {\n    const __m256i mask = _mm256_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF); // Mask to keep only the lower 31 bits\n\n    for (size_t i = 0; i < rows; ++i) {\n        for (size_t j = 0; j < cols / 4; ++j) {\n            __m256 v = _mm256_loadu_ps(&src[i * cols + j * 4]);\n            __m256 rounded = _mm256_round_ps(v, _MM_FROUND_TO_NEAREST_INT);\n            rounded = _mm256_and_si256(rounded, mask);\n            _mm256_storeu_ps(&dst[j * rows + i], rounded);\n        }\n    }\n\n    if (cols % 4 != 0) {\n        for (size_t i = 0; i < rows; ++i) {\n            __m256 v = _mm256_loadu_ps(&src[i * cols + cols - (cols % 4)]);\n            __m256 rounded = _mm256_round_ps(v, _MM_FROUND_TO_NEAREST_INT);\n            rounded = _mm256_and_si256(rounded, mask);\n            _mm256_storeu_ps(&dst[(cols / 4) * rows + i], rounded);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask to keep all bits set\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m256f t1 = _mm_loadu_ps(tensor1 + i);\n        __m256f t2 = _mm_loadu_ps(tensor2 + i);\n\n        __m256f diff = _mm_sub_ps(t1, t2);\n        __m256f rounded_diff = _mm_floor_ps(diff);\n\n        _mm_storeu_ps(dst + i, rounded_diff);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask for rounding towards zero\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m256 v1 = _mm_loadu_ps(tensor1 + i);\n        __m256 v2 = _mm_loadu_ps(tensor2 + i);\n\n        __m256 sub = _mm_sub_ps(v1, v2);\n        __m256 rounded = _mm_round_ps(sub, mask);\n\n        _mm_storeu_ps(dst + i, rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask for rounding towards zero\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m256 v1 = _mm_loadu_ps(tensor1 + i);\n        __m256 v2 = _mm_loadu_ps(tensor2 + i);\n\n        __m256 sub = _mm_sub_ps(v1, v2);\n        __m256 rounded = _mm_round_ps(sub, mask);\n\n        _mm_storeu_ps(dst + i, rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask to set all bits to 1\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m256 v1 = _mm_loadu_ps(&tensor1[i]);\n        __m256 v2 = _mm_loadu_ps(&tensor2[i]);\n\n        __m256 sub = _mm_sub_ps(v1, v2);\n\n        __m256 rounded = _mm_round_ps(sub, _MM_FROUND_TO_ZERO);\n\n        _mm_storeu_ps(&dst[i], rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_26_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor3d_sub_round_zero_simd(const float* tensor1, const float* tensor2, float* dst, size_t dim1, size_t dim2, size_t dim3) {\n    __m256* t1 = (__m256*)tensor1;\n    __m256* t2 = (__m256*)tensor2;\n    __m256* d = (__m256*)dst;\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += 8) {\n        __m256 v1 = _mm256_loadu_ps(t1 + i);\n        __m256 v2 = _mm256_loadu_ps(t2 + i);\n\n        // Perform element-wise subtraction\n        __m256 sub = _mm256_sub_ps(v1, v2);\n\n        // Round towards zero\n        __m256 rounded = _mm256_round_ps(sub, _MM_FROUND_TO_ZERO);\n\n        // Store the result\n        _mm256_storeu_ps(d + i, rounded);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    __m256d vec1, vec2, result;\n    int i;\n\n    // Load the first row of mat1 and mat2 into vectors\n    vec1 = _mm256_loadu_pd(mat1);\n    vec2 = _mm256_loadu_pd(mat2);\n\n    // Perform element-wise multiplication and store the result in dst\n    for (i = 0; i < m; i += 8) {\n        result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(dst + i * 2, result);\n        vec1 = _mm256_loadu_pd(mat1 + i * 8);\n        vec2 = _mm256_loadu_pd(mat2 + i * 8);\n    }\n\n    // Handle remaining elements if there are fewer than 8 elements per row\n    for (; i < m; ++i) {\n        dst[i * 2] = mat1[i] * mat2[i];\n        dst[i * 2 + 1] = mat1[i] * mat2[i];\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    __m256d vec1, vec2, result;\n    int i;\n\n    for (i = 0; i < m * n; i += 8) {\n        vec1 = _mm256_loadu_pd(&mat1[i]);\n        vec2 = _mm256_loadu_pd(&mat2[i]);\n        result = _mm256_mul_pd(vec1, vec2);\n        _mm256_storeu_pd(&dst[i], result);\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    __m256i i = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n    __m256i j = _mm256_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (size_t i = 0; i < m; i += 8) {\n        __m256d v1 = _mm256_loadu_pd(&mat1[i * n]);\n        __m256d v2 = _mm256_loadu_pd(&mat2[i * n]);\n\n        __m256d result = _mm256_mul_pd(v1, v2);\n        _mm256_storeu_pd(&dst[i * n], result);\n    }\n\n    // Handle remaining elements if there are less than 8 elements per row\n    for (size_t i = m % 8; i < m; ++i) {\n        for (size_t j = 0; j < n; ++j) {\n            dst[i * n + j] = mat1[i * n + j] * mat2[i * n + j];\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    __m256d *mat1_vec = (__m256d*)mat1;\n    __m256d *mat2_vec = (__m256d*)mat2;\n    __m256d *dst_vec = (__m256d*)dst;\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; ++j) {\n            __m256d vec1 = _mm256_loadu_pd(&mat1_vec[i * n + j]);\n            __m256d vec2 = _mm256_loadu_pd(&mat2_vec[i * n + j]);\n            __m256d result = _mm256_mul_pd(vec1, vec2);\n            _mm256_storeu_pd(&dst_vec[i * n + j], result);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_27_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_hadamard_product_simd(const double* mat1, const double* mat2, double* dst, size_t m, size_t n) {\n    __m256d vec1, vec2, result;\n    int i, j;\n\n    // Load the first row of mat1 into vec1\n    vec1 = _mm256_loadu_pd(mat1);\n\n    // Iterate over each column of mat2\n    for (j = 0; j < n; ++j) {\n        // Load the current column of mat2 into vec2\n        vec2 = _mm256_loadu_pd(mat2 + j * m);\n\n        // Perform element-wise multiplication and store the result in result\n        result = _mm256_mul_pd(vec1, vec2);\n\n        // Store the result back into the dst matrix\n        _mm256_storeu_pd(dst + j * m, result);\n\n        // Load the next row of mat1 into vec1\n        vec1 = _mm256_loadu_pd(mat1 + m);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m256* mat_ptr = (__m256*)mat;\n    __m256* vec_ptr = (__m256*)vec;\n    __m256* dst_ptr = (__m256*)dst;\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m256 row = _mm_loadu_ps(mat_ptr[i * n + j]);\n            __m256 col = _mm_loadu_ps(vec_ptr[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(row, col));\n        }\n        _mm_storeu_ps(dst_ptr[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m256* mat_ptr = (__m256*)mat;\n    __m256* vec_ptr = (__m256*)vec;\n    __m256* dst_ptr = (__m256*)dst;\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m256 row = _mm_loadu_ps(mat_ptr[i * n + j]);\n            __m256 col = _mm_loadu_ps(vec_ptr[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(row, col));\n        }\n        _mm_storeu_ps(dst_ptr + i, sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m256* mat_ptr = (__m256*)mat;\n    __m256* vec_ptr = (__m256*)vec;\n    __m256* dst_ptr = (__m256*)dst;\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m256 row = _mm_loadu_ps(&mat_ptr[i * n + j]);\n            __m256 col = _mm_loadu_ps(&vec_ptr[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(row, col));\n        }\n        _mm_storeu_ps(&dst_ptr[i], sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m256* mat_ptr = (__m256*)mat;\n    __m256* vec_ptr = (__m256*)vec;\n    __m256* dst_ptr = (__m256*)dst;\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m256 row = _mm_loadu_ps(mat_ptr[i * n + j]);\n            __m256 col = _mm_loadu_ps(vec_ptr[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(row, col));\n        }\n        _mm_storeu_ps(dst_ptr + i, sum);\n    }\n}\n```"}
{"task_id": "SimdBench_28_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_vector_mul_simd(const float* mat, const float* vec, float* dst, size_t m, size_t n) {\n    __m256* mat_ptr = (__m256*)mat;\n    __m256* vec_ptr = (__m256*)vec;\n    __m256* dst_ptr = (__m256*)dst;\n\n    for (size_t i = 0; i < m; ++i) {\n        __m256 sum = _mm_setzero_ps();\n        for (size_t j = 0; j < n; j += 4) {\n            __m256 row = _mm_loadu_ps(mat_ptr[i * n + j]);\n            __m256 col = _mm_loadu_ps(vec_ptr[j]);\n            sum = _mm_add_ps(sum, _mm_mul_ps(row, col));\n        }\n        _mm_storeu_ps(dst_ptr + i, sum);\n    }\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    const __m256d* ptr = (__m256d*)mat;\n\n    for (size_t i = 0; i < m; i += 8) {\n        __m256d row = _mm256_loadu_pd(ptr + i);\n        sum = _mm256_add_pd(sum, row);\n    }\n\n    // Reduce the sum to a single value\n    sum = _mm256_hadd_pd(sum, sum);\n    sum = _mm256_hadd_pd(sum, sum);\n\n    double avg;\n    _mm_store_sd(&avg, sum);\n    return avg;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c++\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    const __m256d* ptr = reinterpret_cast<const __m256d*>(mat);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n / 4; ++j) {\n            sum = _mm256_add_pd(sum, _mm256_loadu_pd(ptr + i * 4 + j));\n        }\n    }\n\n    // Handle remaining elements if n is not divisible by 4\n    for (size_t i = m * 4; i < m * 4 + n % 4; ++i) {\n        sum = _mm256_add_pd(sum, _mm256_loadu_pd(&ptr[i]));\n    }\n\n    double result = _mm256_reduce_add_pd(sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c++\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    const __m256d* ptr = reinterpret_cast<const __m256d*>(mat);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n; j += 4) {\n            __m256d vec = _mm256_loadu_pd(ptr + i * n + j);\n            sum = _mm256_add_pd(sum, vec);\n        }\n    }\n\n    double total_sum = _mm256_reduce_add_pd(sum);\n    return total_sum / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c++\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    const __m256d* ptr = reinterpret_cast<const __m256d*>(mat);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n / 4; ++j) {\n            sum = _mm256_add_pd(sum, _mm256_loadu_pd(ptr + i * 4 + j));\n        }\n        if (n % 4 != 0) {\n            sum = _mm256_add_pd(sum, _mm256_loadu_pd(ptr + i * 4 + n / 4));\n        }\n    }\n\n    double total_sum = _mm256_reduce_add_pd(sum);\n    return total_sum / (m * n);\n}\n```"}
{"task_id": "SimdBench_29_AVX", "completion": "```c++\ndouble matrix_average_simd(const double* mat, size_t m, size_t n) {\n    __m256d sum = _mm256_setzero_pd();\n    const __m256d* ptr = reinterpret_cast<const __m256d*>(mat);\n\n    for (size_t i = 0; i < m; ++i) {\n        for (size_t j = 0; j < n / 4; ++j) {\n            sum = _mm256_add_pd(sum, _mm256_loadu_pd(ptr + i * 4 + j));\n        }\n    }\n\n    // Handle remaining elements if n is not divisible by 4\n    for (size_t i = m * 4; i < m * 4 + n % 4; ++i) {\n        sum = _mm256_add_pd(sum, _mm256_loadu_pd(&ptr[i]));\n    }\n\n    double avg = _mm256_reduce_add_ps(sum, _mm256_set1_pd(1.0)) / (double)m;\n    return avg;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m256i* vec_matrix = (__m256i*)matrix;\n    __m256i* vec_directions = (__m256i*)directions;\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256i vec_row = _mm256_loadu_si256(vec_matrix + i);\n        __m256i vec_sort_dir = _mm256_loadu_si256(vec_directions + i);\n\n        // Check if the row is sorted in ascending order\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi16(vec_row, _mm256_setzero_si256())) != 0 && !vec_sort_dir[0]) {\n            return false;\n        }\n\n        // Check if the row is sorted in descending order\n        if (_mm256_movemask_epi8(_mm256_cmplt_epi16(vec_row, _mm256_setzero_si256())) != 0 && vec_sort_dir[0]) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m256i* vec_matrix = (__m256i*)matrix;\n    __m256i* vec_directions = (__m256i*)directions;\n\n    for (size_t i = 0; i < rows; i += 4) {\n        __m256i vec_row = _mm256_loadu_si256(vec_matrix + i);\n        __m256i vec_dir = _mm256_loadu_si256(vec_directions + i);\n\n        for (size_t j = 0; j < cols - 1; j++) {\n            if ((vec_dir[j] && vec_row[j] > vec_row[j + 1]) || (!vec_dir[j] && vec_row[j] < vec_row[j + 1])) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m256i mask = _mm_set1_epi32(0xFFFFFFFF); // Initialize mask to all ones\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256i row = _mm_loadu_si256((__m256i*)(&matrix[i * cols]));\n        __m256i sorted_row;\n\n        if (directions[i]) {\n            sorted_row = _mm_cmpgt_epi32(row, mask); // Compare row elements with mask\n        } else {\n            sorted_row = _mm_cmplt_epi32(row, mask); // Compare row elements with mask\n        }\n\n        if (_mm_movemask_epi8(sorted_row) != 0xFFFFFFFF) { // Check if any element is out of order\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c++\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m256i* vec_matrix = (__m256i*)matrix;\n    __m256i* vec_directions = (__m256i*)directions;\n\n    for (size_t i = 0; i < rows; i += 4) {\n        __m256i vec_row = _mm256_loadu_si256(vec_matrix + i);\n        __m256i vec_mask = _mm256_set1_epi32(1);\n\n        for (size_t j = 0; j < cols - 3; j += 4) {\n            __m256i vec_col1 = _mm256_loadu_si256(vec_matrix + i + j);\n            __m256i vec_col2 = _mm256_loadu_si256(vec_matrix + i + j + 1);\n            __m256i vec_col3 = _mm256_loadu_si256(vec_matrix + i + j + 2);\n            __m256i vec_col4 = _mm256_loadu_si256(vec_matrix + i + j + 3);\n\n            if (directions[i + j]) {\n                vec_mask = _mm256_and_si256(_mm256_cmpgt_epi32(vec_col1, vec_col2), vec_mask);\n                vec_mask = _mm256_and_si256(_mm256_cmpgt_epi32(vec_col2, vec_col3), vec_mask);\n                vec_mask = _mm256_and_si256(_mm256_cmpgt_epi32(vec_col3, vec_col4), vec_mask);\n            } else {\n                vec_mask = _mm256_and_si256(_mm256_cmplt_epi32(vec_col1, vec_col2), vec_mask);\n                vec_mask = _mm256_and_si256(_mm256_cmplt_epi32(vec_col2, vec_col3), vec_mask);\n                vec_mask = _mm256_and_si256(_mm256_cmplt_epi32(vec_col3, vec_col4), vec_mask);\n            }\n        }\n\n        if (_mm256_movemask_epi8(vec_mask) != 0xFFFFFFFF) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_30_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_sorted_verify_simd(const int* matrix, const bool* directions, size_t rows, size_t cols) {\n    __m256i* vec_matrix = (__m256i*)matrix;\n    __m256i* vec_directions = (__m256i*)directions;\n\n    for (size_t i = 0; i < rows; i += 4) {\n        __m256i vec_row = _mm256_loadu_si256(vec_matrix + i);\n\n        for (size_t j = 0; j < cols - 1; j++) {\n            __m256i vec_current = _mm256_loadu_si256(vec_row + j);\n            __m256i vec_next = _mm256_loadu_si256(vec_row + j + 1);\n\n            if (directions[i]) {\n                if (_mm256_cmpgt_epi32(vec_current, vec_next)) {\n                    return false;\n                }\n            } else {\n                if (_mm256_cmplt_epi32(vec_current, vec_next)) {\n                    return false;\n                }\n            }\n        }\n\n        vec_row += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c++\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm_set1_epi8(base); // Create a vector containing the base value\n    __m256i mask_vec = _mm_setzero_si256(); // Create a zero vector\n\n    for (size_t i = 0; i < length; i += 4) { // Process elements in chunks of 4\n        __m256i src_chunk = _mm_loadu_si256((__m256i*)(&src[i])); // Load 4 elements from src into a vector\n        __m256i rounded_chunk = _mm_round_ps(src_chunk, _MM_FROUND_TO_NEAREST_INT); // Round each element to the nearest integer\n        __m256i remainder_chunk = _mm_sub_epi16(rounded_chunk, src_chunk); // Calculate the remainder of each rounded element\n        __m256i adjusted_chunk = _mm_add_epi16(remainder_chunk, base_vec); // Add the base value to the remainder\n        __m256i result_chunk = _mm_min_epi16(adjusted_chunk, src_chunk); // Clamp the result to the original value\n        _mm_storeu_si256((__m256i*)(&dst[i]), result_chunk); // Store the result back to dst\n    }\n\n    for (size_t i = length % 4; i < length; ++i) { // Process remaining elements\n        int16_t src_val = src[i];\n        int16_t base_val = base;\n        int16_t rounded_val = (src_val + base_val / 2) / base_val * base_val;\n        if (rounded_val > src_val) {\n            rounded_val -= base_val;\n        }\n        dst[i] = rounded_val;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm_set1_epi8(base); // Create a vector containing the base value\n    __m256i mask_vec = _mm_setzero_si256(); // Create a zero vector\n\n    for (size_t i = 0; i < length; i += 32) { // Process blocks of 32 elements at a time\n        __m256i src_vec = _mm_loadu_si256((__m256i*)(&src[i])); // Load 4 elements from src into a vector\n        __m256i rounded_vec = _mm_round_ps(src_vec, _MM_FROUND_TO_NEAREST_INT); // Round the values to the nearest integer\n        __m256i aligned_vec = _mm_and_si256(rounded_vec, base_vec); // Align the rounded values to the base value\n        __m256i result_vec = _mm_or_si256(aligned_vec, mask_vec); // Ensure the result does not exceed the original value\n        _mm_storeu_si256((__m256i*)(&dst[i]), result_vec); // Store the results back to dst\n    }\n\n    for (size_t i = length % 32; i < length; ++i) { // Handle remaining elements\n        int16_t src_val = src[i];\n        int16_t aligned_val = (src_val / base) * base; // Align the value to the base value\n        if (aligned_val > src_val) {\n            aligned_val -= base; // Adjust if it exceeds the original value\n        }\n        dst[i] = aligned_val;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm_set1_epi8(base); // Create a vector containing the base value repeated 4 times\n    __m256i src_vec; // Initialize an empty vector\n\n    // Load the first 4 elements from src into src_vec\n    src_vec = _mm_loadu_si256((__m256i*)src);\n\n    // Perform the nearest multiple calculation using AVX2 intrinsics\n    __m256i rounded_src = _mm_round_ps(src_vec, _MM_FROUND_TO_NEAREST_INT); // Round each element to the nearest integer\n    __m256i nearest_multiple = _mm_mullo_epi16(rounded_src, base_vec); // Multiply by the base vector\n\n    // Store the results back to dst\n    _mm_storeu_si256((__m256i*)dst, nearest_multiple);\n\n    // Process the remaining elements if there are any left over\n    size_t remaining_elements = length % 4;\n    if (remaining_elements > 0) {\n        src_vec = _mm_loadu_si256((__m256i*)(src + 4 * (length / 4))); // Load the next 4 elements from src\n        rounded_src = _mm_round_ps(src_vec, _MM_FROUND_TO_NEAREST_INT); // Round each element to the nearest integer\n        nearest_multiple = _mm_mullo_epi16(rounded_src, base_vec); // Multiply by the base vector\n        _mm_storeu_si256((__m256i*)(dst + 4 * (length / 4)), nearest_multiple); // Store the results back to dst\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm_set1_epi8(base); // Create a vector containing the base value\n    __m256i mask_vec = _mm_setzero_si256(); // Create a zero vector\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i src_vec = _mm_loadu_si256((__m256i*)(&src[i])); // Load 4 elements from src into a vector\n        __m256i rounded_vec = _mm_round_ps(src_vec, _MM_FROUND_TO_NEAREST_INT); // Round each element to the nearest integer\n        __m256i remainder_vec = _mm_sub_epi16(rounded_vec, src_vec); // Calculate the remainder of each rounded element\n        __m256i adjusted_remainder_vec = _mm_and_si256(remainder_vec, mask_vec); // Mask out the remainder if it's greater than half the base\n        __m256i adjusted_rounded_vec = _mm_add_epi16(rounded_vec, adjusted_remainder_vec); // Adjust the rounded element by adding the remainder\n        __m256i result_vec = _mm_min_epi16(adjusted_rounded_vec, src_vec); // Clamp the result to the original value\n        _mm_storeu_si256((__m256i*)(&dst[i]), result_vec); // Store the result back to dst\n    }\n\n    // Handle remaining elements\n    for (size_t i = length % 4; i > 0; --i) {\n        int16_t src_val = src[length - i];\n        int16_t dst_val = dst[length - i];\n        int16_t remainder = abs(src_val - dst_val);\n        if (remainder > base / 2) {\n            dst_val = (src_val + base / 2) / base * base;\n        }\n        dst[length - i] = dst_val;\n    }\n}\n```"}
{"task_id": "SimdBench_31_AVX", "completion": "```c\nvoid nearest_multiple_simd(const int16_t* src, int16_t* dst, uint8_t base, size_t length) {\n    __m256i base_vec = _mm_set1_epi8(base); // Load base into a vector\n    __m256i result_vec;\n\n    for (size_t i = 0; i < length; i += 4) { // Process elements in chunks of 4\n        __m256i src_chunk = _mm_loadu_si256((__m256i*)(&src[i])); // Load 4 elements from src\n        __m256i rounded_chunk = _mm_round_ps(src_chunk, _MM_FROUND_TO_NEAREST_INT); // Round each element\n\n        // Multiply rounded_chunk by base_vec\n        __m256i multiplied_chunk = _mm_mullo_epi16(rounded_chunk, base_vec);\n\n        // Subtract base_vec from multiplied_chunk to get the nearest multiple\n        __m256i nearest_chunk = _mm_sub_epi16(multiplied_chunk, base_vec);\n\n        // Store the nearest chunk back to dst\n        _mm_storeu_si256((__m256i*)(&dst[i]), nearest_chunk);\n    }\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256 v_vector = _mm256_loadu_ps(vector); // Load the vector into a 256-bit register\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256 v_matrix_row = _mm256_loadu_ps(matrix + i * cols); // Load the current row into a 256-bit register\n        __m256 result = _mm256_cmp_ps(v_matrix_row, v_vector, _CMP_EQ_OQ); // Compare each element of the row with the vector\n        if (_mm256_movemask_ps(result) != 0) { // Check if there is at least one matching element\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256 v_vector = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        __m256 v_matrix_row = _mm256_loadu_pd(matrix + i * cols);\n        __m256 result = _mm256_cmpneq_ps(v_matrix_row, v_vector);\n        if (_mm256_movemask_epi8(result) != 0) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c++\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256 v = _mm_loadu_ps(vector); // Load the vector into a 256-bit register\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256 m = _mm_loadu_ps(matrix + i * cols); // Load a row from the matrix into a 256-bit register\n        __m256 result = _mm_cmpeq_ps(m, v); // Compare each element of the row with the vector\n        if (_mm_movemask_ps(result) != 0) { // Check if there is at least one matching element\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d v_vector = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d v_matrix = _mm256_loadu_pd(matrix + i * cols);\n        __m256d result = _mm256_cmpneq_pd(v_matrix, v_vector);\n        if (_mm256_movemask_epi8(result) != 0) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_32_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_has_row_simd(const double* matrix, const double* vector, size_t rows, size_t cols) {\n    __m256d v_vector = _mm256_loadu_pd(vector);\n    for (size_t i = 0; i < rows; ++i) {\n        __m256d v_matrix = _mm256_loadu_pd(matrix + i * cols);\n        __m256d result = _mm256_cmpneq_pd(v_matrix, v_vector);\n        if (_mm256_movemask_epi8(result) != 0) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i, j, k, l;\n\n    // Unroll the loop by 4 iterations\n    for (i = 0; i < dims[0]; i += 4) {\n        for (j = 0; j < dims[1]; j += 4) {\n            for (k = 0; k < dims[2]; k += 4) {\n                for (l = 0; l < dims[3]; l += 4) {\n                    __m256i a = _mm256_loadu_si256((__m256i*)(&A[i * 4 + j * 4 + k * 4 + l * 4]));\n                    __m256i b = _mm256_loadu_si256((__m256i*)(&B[i * 4 + j * 4 + k * 4 + l * 4]));\n\n                    __m256i diff = _mm256_sub_epi8(a, b);\n                    __m256i mask = _mm256_cmpneq_epi8(diff, _mm256_setzero_si256());\n                    __m256i sum = _mm256_add_epi8(mask, mask);\n\n                    total_count += _mm256_reduce_add_epi8(sum);\n                }\n            }\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i = 0;\n\n    // Loop through each dimension\n    for (int d = 0; d < 4; ++d) {\n        size_t stride = dims[d];\n        size_t chunk_size = 32 / sizeof(uint8_t);\n\n        while (i < stride * dims[d - 1]) {\n            __m256i a_chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(A + i));\n            __m256i b_chunk = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(B + i));\n\n            // Compare chunks using AVX2 intrinsics\n            __m256i cmp_result = _mm256_cmpeq_epi8(a_chunk, b_chunk);\n            __m256i sum = _mm256_add_epi32(_mm256_sad_epu8(cmp_result, _mm256_setzero_si256()), _mm256_setzero_si256());\n\n            // Add the sum of each chunk to the total count\n            total_count += _mm256_extract_epi32(sum, 0);\n\n            i += chunk_size;\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t batch_size = dims[3];\n    size_t element_size = dims[2] * dims[1];\n\n    // Iterate over each batch in the 4D tensor\n    for (size_t i = 0; i < batch_size; ++i) {\n        __m256i a_batch = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(A + i * element_size));\n        __m256i b_batch = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(B + i * element_size));\n\n        // Perform bitwise AND operation on corresponding elements of the batches\n        __m256i and_result = _mm256_and_si256(a_batch, b_batch);\n\n        // Count the number of set bits (equal elements)\n        __m256i count_result = _mm256_popcnt_epi8(and_result);\n\n        // Sum up the counts from all batches\n        total_count += _mm256_reduce_add_epi32(count_result);\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    size_t total_count = 0;\n    size_t i, j, k, l;\n\n    // Iterate over each dimension\n    for (i = 0; i < dims[0]; ++i) {\n        for (j = 0; j < dims[1]; ++j) {\n            for (k = 0; k < dims[2]; ++k) {\n                for (l = 0; l < dims[3]; ++l) {\n                    __m256i a = _mm256_loadu_si256((__m256i*)(&A[i * dims[1] * dims[2] * dims[3] + j * dims[2] * dims[3] + k * dims[3] + l]));\n                    __m256i b = _mm256_loadu_si256((__m256i*)(&B[i * dims[1] * dims[2] * dims[3] + j * dims[2] * dims[3] + k * dims[3] + l]));\n\n                    // Perform element-wise comparison using AVX2\n                    __m256 cmp = _mm256_cmpeq_epi8(a, b);\n\n                    // Sum up the results\n                    total_count += _mm256_popcnt_u32(_mm256_extract_epi32(cmp, 0)) +\n                                  _mm256_popcnt_u32(_mm256_extract_epi32(cmp, 1)) +\n                                  _mm256_popcnt_u32(_mm256_extract_epi32(cmp, 2)) +\n                                  _mm256_popcnt_u32(_mm256_extract_epi32(cmp, 3));\n                }\n            }\n        }\n    }\n\n    return total_count;\n}\n```"}
{"task_id": "SimdBench_33_AVX", "completion": "```c++\nsize_t tensor_4d_equal_count_simd(const uint8_t* A, const uint8_t* B, const size_t dims[4]) {\n    // Initialize the result counter\n    size_t count = 0;\n\n    // Extract the dimensions from the input array\n    size_t M = dims[0];\n    size_t N = dims[1];\n    size_t P = dims[2];\n    size_t Q = dims[3];\n\n    // Calculate the total number of elements in each dimension\n    size_t total_elements = M * N * P * Q;\n\n    // Use AVX/AVX2 intrinsics to process the data in parallel\n    __m256i* ptr_A = (__m256i*)A;\n    __m256i* ptr_B = (__m256i*)B;\n\n    // Process the data in chunks of 4 vectors at a time\n    for (size_t i = 0; i < total_elements / 4; ++i) {\n        // Load 4 vectors from A and B\n        __m256i vec_A = _mm256_loadu_si256(ptr_A + i);\n        __m256i vec_B = _mm256_loadu_si256(ptr_B + i);\n\n        // Perform element-wise comparison using AVX/AVX2 intrinsics\n        __m256i cmp_result = _mm256_cmpeq_epi8(vec_A, vec_B);\n\n        // Count the number of matching elements\n        count += _mm_popcnt_u32(_mm256_extract_epi32(cmp_result, 0)) +\n                 _mm_popcnt_u32(_mm256_extract_epi32(cmp_result, 1)) +\n                 _mm_popcnt_u32(_mm256_extract_epi32(cmp_result, 2)) +\n                 _mm_popcnt_u32(_mm256_extract_epi32(cmp_result, 3));\n    }\n\n    // Handle any remaining elements that do not fit into a chunk of 4 vectors\n    if (total_elements % 4 != 0) {\n        // Load the remaining elements from A and B\n        __m256i vec_A = _mm256_loadu_si256(ptr_A + total_elements / 4);\n        __m256i vec_B = _mm256_loadu_si256(ptr_B + total_elements / 4);\n\n        // Perform element-wise comparison using AVX/AVX2 intrinsics\n        __m256i cmp_result = _mm256_cmpeq_epi8(vec_A, vec_B);\n\n        // Count the number of matching elements\n        count += _mm_popcnt_u32(_mm256_extract_epi32(cmp_result, 0));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i x_vec = _mm256_loadu_si256((__m256i *)x);\n\n    __m256i abs_x_vec = _mm256_abs_epi64(x_vec);\n    __m256i result_vec = _mm256_mul_epi64(a_vec, abs_x_vec);\n\n    _mm256_storeu_si256((__m256i *)y, result_vec);\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i x_vec = _mm256_loadu_si256((__m256i *)x);\n\n    __m256i abs_x_vec = _mm256_abs_epi64(x_vec);\n    __m256i result_vec = _mm256_mul_epi64(a_vec, abs_x_vec);\n\n    _mm256_storeu_si256((__m256i *)y, result_vec);\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i x_vec = _mm256_loadu_si256((__m256i *)x);\n\n    __m256i abs_x_vec = _mm256_abs_epi64(x_vec);\n    __m256i result_vec = _mm256_mul_epi64(a_vec, abs_x_vec);\n\n    _mm256_storeu_si256((__m256i *)y, result_vec);\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set_epi64x(a, a, a, a);\n    __m256i x_vec = _mm256_loadu_si256((__m256i *)x);\n\n    __m256i abs_x_vec = _mm256_abs_epi64(x_vec);\n    __m256i result_vec = _mm256_mul_epi64(a_vec, abs_x_vec);\n\n    _mm256_storeu_si256((__m256i *)y, result_vec);\n}\n```"}
{"task_id": "SimdBench_34_AVX", "completion": "```c\nvoid axm_abs_simd(size_t length, const int64_t a, const int64_t *x, int64_t *y) {\n    __m256i a_vec = _mm256_set1_epi64(a);\n    __m256i x_vec, y_vec;\n\n    for (size_t i = 0; i < length; i += 8) {\n        x_vec = _mm256_loadu_si256((__m256i *)(&x[i]));\n        y_vec = _mm256_mullo_epi64(x_vec, a_vec); // Scalar-vector multiplication\n        y_vec = _mm256_abs_epi64(y_vec); // Absolute value\n        _mm256_storeu_si256((__m256i *)(&y[i]), y_vec);\n    }\n\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i < length; ++i) {\n        y[i] = abs(x[i]) * a;\n    }\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        const __m128i current_val = _mm_loadu_si128((__m128i*)(&vec[i * 4]));\n        max_val = _mm_max_epi16(max_val, current_val);\n        min_val = _mm_min_epi16(min_val, current_val);\n    }\n\n    if (length % 4 > 0) {\n        const __m128i remaining_val = _mm_loadu_si128((__m128i*)&vec[length - 4]);\n        max_val = _mm_max_epi16(max_val, remaining_val);\n        min_val = _mm_min_epi16(min_val, remaining_val);\n    }\n\n    MinMaxPair result;\n    result.min_num = _mm_extract_epi16(min_val, 0);\n    result.max_num = _mm_extract_epi16(max_val, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i max_val = _mm_set1_epi16(INT16_MIN);\n    __m128i min_val = _mm_set1_epi16(INT16_MAX);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        const __m128i current_val = _mm_loadu_si128((__m128i*)(&vec[i * 4]));\n        max_val = _mm_max_epi16(max_val, current_val);\n        min_val = _mm_min_epi16(min_val, current_val);\n    }\n\n    if (length % 4 > 0) {\n        const __m128i current_val = _mm_loadu_si128((__m128i*)(&vec[length - 4]));\n        max_val = _mm_max_epi16(max_val, current_val);\n        min_val = _mm_min_epi16(min_val, current_val);\n    }\n\n    MinMaxPair result;\n    result.min_num = _mm_extract_epi16(min_val, 0);\n    result.max_num = _mm_extract_epi16(max_val, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i v = _mm_loadu_si256((__m256i*)vec);\n    __m256i min_v = _mm_min_epi16(v, v);\n    __m256i max_v = _mm_max_epi16(v, v);\n\n    for (size_t i = 4; i < length; i += 4) {\n        v = _mm_loadu_si256((__m256i*)(vec + i));\n        min_v = _mm_min_epi16(min_v, v);\n        max_v = _mm_max_epi16(max_v, v);\n    }\n\n    __m256i last_v = _mm_loadu_si256((__m256i*)(vec + length - 4));\n    min_v = _mm_min_epi16(min_v, last_v);\n    max_v = _mm_max_epi16(max_v, last_v);\n\n    __m128i min_m = _mm_cvtepi32_epi16(_mm_min_epi16(min_v, _mm_setzero_si128()));\n    __m128i max_m = _mm_cvtepi32_epi16(_mm_max_epi16(max_v, _mm_setzero_si128()));\n\n    MinMaxPair result;\n    result.min_num = _mm_extract_epi16(min_m, 0);\n    result.max_num = _mm_extract_epi16(max_m, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m256i v = _mm_loadu_si256((__m256i*)vec);\n    __m256i min_v = _mm_min_epi16(v, v);\n    __m256i max_v = _mm_max_epi16(v, v);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i v = _mm_loadu_si256((__m256i*)(vec + i * 4));\n        __m256i min_v = _mm_min_epi16(min_v, v);\n        __m256i max_v = _mm_max_epi16(max_v, v);\n    }\n\n    __m256i min_final = _mm_min_epi16(min_v, min_v);\n    __m256i max_final = _mm_max_epi16(max_v, max_v);\n\n    int16_t min_num = _mm_cvtsi128_si32(_mm_extract_epi16(min_final, 0));\n    int16_t max_num = _mm_cvtsi128_si32(_mm_extract_epi16(max_final, 0));\n\n    return {min_num, max_num};\n}\n```"}
{"task_id": "SimdBench_35_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ntypedef struct {\n    int16_t min_num;\n    int16_t max_num;\n} MinMaxPair;\n\nMinMaxPair min_max_pair_simd(const int16_t* vec, size_t length) {\n    __m128i max_val = _mm_setzero_si128();\n    __m128i min_val = _mm_setzero_si128();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m128i current_val = _mm_loadu_si128((__m128i*)(vec + i));\n        max_val = _mm_max_epi16(max_val, current_val);\n        min_val = _mm_min_epi16(min_val, current_val);\n    }\n\n    if (length % 4 != 0) {\n        __m128i remaining_val = _mm_loadu_si128((__m128i*)(vec + length - 4));\n        max_val = _mm_max_epi16(max_val, remaining_val);\n        min_val = _mm_min_epi16(min_val, remaining_val);\n    }\n\n    MinMaxPair result;\n    result.min_num = _mm_extract_epi16(min_val, 0);\n    result.max_num = _mm_extract_epi16(max_val, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const __m256i* ptr = reinterpret_cast<const __m256i*>(matrix);\n    const __m256i* end = ptr + (rows - 1) / 4;\n\n    while (ptr != end) {\n        __m256i current_row = _mm256_loadu_si256(ptr);\n        __m256i next_row = _mm256_loadu_si256(ptr + 1);\n\n        // Check if the current row is less than or equal to the next row\n        if (_mm256_cmpgt_epi32(current_row, next_row)) {\n            return false;\n        }\n\n        ptr += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c++\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0 || cols == 0) {\n        return false;\n    }\n\n    const __m256i* ptr = reinterpret_cast<const __m256i*>(matrix);\n    const __m256i* end_ptr = ptr + (rows / 4);\n\n    while (ptr != end_ptr) {\n        __m256i current_row = _mm256_loadu_si256(ptr);\n        __m256i next_row = _mm256_loadu_si256(ptr + 1);\n\n        // Check if the current row is less than or equal to the next row\n        if (_mm256_cmpgt_epi32(current_row, next_row)) {\n            return false;\n        }\n\n        ptr += 4;\n    }\n\n    // Handle the remaining rows if there are any left over\n    size_t remaining_rows = rows % 4;\n    if (remaining_rows > 0) {\n        __m256i current_row = _mm256_loadu_si256(ptr);\n        for (size_t i = 0; i < remaining_rows; ++i) {\n            if (current_row[i] <= *(ptr + i)) {\n                return false;\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c++\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const __m256i* ptr = reinterpret_cast<const __m256i*>(matrix);\n    const __m256i* end_ptr = ptr + (rows - 1) / 4;\n\n    while (ptr != end_ptr) {\n        __m256i current_row = _mm256_loadu_si256(ptr);\n        __m256i next_row = _mm256_loadu_si256(ptr + 4);\n\n        // Check if the current row is strictly increasing\n        if (_mm256_cmpgt_epi32(current_row, next_row)) {\n            return false;\n        }\n\n        ptr += 4;\n    }\n\n    // Check the last few rows if necessary\n    for (; ptr < ptr + (rows % 4); ++ptr) {\n        if (*ptr >= *(ptr + 1)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const __m256i* ptr = reinterpret_cast<const __m256i*>(matrix);\n    const __m256i* end_ptr = ptr + (rows - 1) / 4;\n\n    while (ptr != end_ptr) {\n        __m256i current_row = _mm256_loadu_si256(ptr);\n        __m256i next_row = _mm256_loadu_si256(ptr + 1);\n\n        // Check if the current row is less than or equal to the next row\n        if (_mm256_cmpgt_epi32(current_row, next_row)) {\n            return false;\n        }\n\n        ptr += 4;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_36_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_rows_strictly_increasing_simd(const int* matrix, size_t rows, size_t cols) {\n    if (rows == 0) {\n        return false;\n    }\n\n    const __m256i* row_start = reinterpret_cast<const __m256i*>(matrix);\n    const __m256i* end = row_start + (rows - 1) / 4;\n\n    for (__m256i* current_row = row_start; current_row != end; ++current_row) {\n        __m256i prev_row = *current_row;\n        __m256i next_row = *(current_row + 1);\n\n        // Check if the current row is strictly increasing\n        if (_mm256_cmpgt_epi32(prev_row, next_row)) {\n            return false;\n        }\n    }\n\n    // Handle the remaining rows if there are any\n    for (; current_row != row_start + rows / 4; ++current_row) {\n        __m256i prev_row = *current_row;\n        __m256i next_row = *(current_row + 1);\n\n        // Check if the current row is strictly increasing\n        if (_mm256_cmpgt_epi32(prev_row, next_row)) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > min(rows, cols)) {\n        return false;\n    }\n\n    const size_t num_blocks = (rows + block_size - 1) / block_size * (cols + block_size - 1) / block_size;\n\n    __m256i mask = _mm_setzero_si256();\n    __m256i result = _mm_setzero_si256();\n\n    for (size_t i = 0; i < num_blocks; ++i) {\n        size_t row_start = i * block_size;\n        size_t col_start = i * block_size;\n\n        __m256i block1 = _mm_loadu_si256((__m256i*)(matrix + row_start * cols + col_start));\n        __m256i block2 = _mm_loadu_si256((__m256i*)(matrix + row_start * cols + col_start));\n\n        result = _mm_or_si256(result, _mm_cmpeq_epi32(block1, block2));\n    }\n\n    return _mm_movemask_epi8(result) != 0;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > std::min(rows, cols)) {\n        return false;\n    }\n\n    const size_t num_blocks = (rows + block_size - 1) / block_size * (cols + block_size - 1) / block_size;\n\n    __m256i mask = _mm_setzero_si256();\n    __m256i result = _mm_setzero_si256();\n\n    for (size_t i = 0; i < num_blocks; ++i) {\n        size_t row_start = i * block_size;\n        size_t col_start = i * block_size;\n\n        __m256i block1 = _mm_loadu_si256(reinterpret_cast<const __m256i*>(matrix + row_start * cols + col_start));\n        __m256i block2 = _mm_loadu_si256(reinterpret_cast<const __m256i*>(matrix + row_start * cols + col_start));\n\n        result = _mm_or_si256(result, _mm_cmpeq_epi32(block1, block2));\n    }\n\n    return _mm_movemask_epi8(result) != 0;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > std::min(rows, cols)) {\n        return false;\n    }\n\n    const size_t num_blocks = (rows + block_size - 1) / block_size * (cols + block_size - 1) / block_size;\n\n    __m256i mask = _mm_setzero_si256();\n    __m256i diff_mask = _mm_setzero_si256();\n\n    for (size_t i = 0; i < num_blocks; ++i) {\n        size_t row_start = i * block_size;\n        size_t col_start = i * block_size;\n\n        __m256i block1 = _mm_loadu_si256((__m256i*)(matrix + row_start * cols + col_start));\n        __m256i block2 = _mm_loadu_si256((__m256i*)(matrix + row_start * cols + col_start));\n\n        diff_mask = _mm_sub_epi32(block1, block2);\n\n        mask = _mm_or_si256(mask, diff_mask);\n    }\n\n    return _mm_movemask_epi8(mask) == 0;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c++\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > std::min(rows, cols)) {\n        return false;\n    }\n\n    const size_t num_blocks_rows = rows / block_size;\n    const size_t num_blocks_cols = cols / block_size;\n\n    __m256i mask = _mm_set1_epi32(0xFFFFFFFF);\n\n    for (size_t i = 0; i < num_blocks_rows; ++i) {\n        for (size_t j = 0; j < num_blocks_cols; ++j) {\n            const int* block_start = matrix + i * block_size * cols + j * block_size;\n            const __m256i block = _mm_loadu_si256((__m256i*)block_start);\n\n            for (size_t k = 0; k < block_size; ++k) {\n                for (size_t l = 0; l < block_size; ++l) {\n                    if (_mm_test_all_eq(block, _mm_set1_epi32(matrix[i * block_size * cols + j * block_size + k * block_size + l]))) {\n                        continue;\n                    } else {\n                        return false;\n                    }\n                }\n            }\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_37_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool matrix_blocks_equal_simd(const int* matrix, size_t block_size, size_t rows, size_t cols) {\n    if (block_size == 0 || block_size > std::min(rows, cols)) {\n        return false;\n    }\n\n    const size_t num_blocks = (rows + block_size - 1) / block_size * (cols + block_size - 1) / block_size;\n\n    __m256i mask = _mm_setzero_si256();\n    __m256i result = _mm_setzero_si256();\n\n    for (size_t i = 0; i < num_blocks; ++i) {\n        size_t row_start = i * block_size;\n        size_t col_start = i * block_size;\n\n        __m256i block1 = _mm_loadu_si256(reinterpret_cast<const __m256i*>(matrix + row_start * cols + col_start));\n        __m256i block2 = _mm_loadu_si256(reinterpret_cast<const __m256i*>(matrix + row_start * cols + col_start));\n\n        result = _mm_or_si256(result, _mm_cmpeq_epi32(block1, block2));\n    }\n\n    return _mm_movemask_epi8(result) != 0;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size == 0) {\n        return false; // Invalid input\n    }\n\n    const __m256d* v = reinterpret_cast<const __m256d*>(vec);\n    const __m256d* end = v + (length / block_size);\n\n    bool result = true;\n    while (v < end) {\n        __m256d block1 = _mm256_loadu_pd(v);\n        __m256d block2 = _mm256_loadu_pd(v + block_size - 1);\n\n        __m256d diff = _mm256_sub_pd(block1, block2);\n        __m256d abs_diff = _mm256_abs_pd(diff);\n\n        if (_mm256_movemask_pd(abs_diff) != 0) {\n            result = false;\n            break;\n        }\n\n        v += block_size;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size == 0) {\n        return false; // Illegal input\n    }\n\n    const __m256 tolerance_vec = _mm_set1_pd(tolerance);\n\n    for (size_t i = 0; i < length - block_size + 1; i += block_size) {\n        __m256 block1 = _mm_loadu_ps(vec + i);\n        __m256 block2 = _mm_loadu_ps(vec + i + block_size / 2);\n\n        __m256 diff = _mm_sub_ps(block1, block2);\n        __m256 abs_diff = _mm_abs_ps(diff);\n        __m256 max_diff = _mm_max_ps(abs_diff, tolerance_vec);\n\n        if (_mm_movemask_ps(_mm_cmpge_ps(max_diff, tolerance_vec)) != 0) {\n            return false; // Blocks are not equal\n        }\n    }\n\n    return true; // All blocks are equal\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size == 0) {\n        return false; // Illegal input\n    }\n\n    const __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask for checking equality\n\n    for (size_t i = 0; i < length - block_size + 1; i += block_size) {\n        __m256 v1 = _mm_loadu_ps(vec + i);\n        __m256 v2 = _mm_loadu_ps(vec + i);\n\n        __m256 diff = _mm_sub_ps(v1, v2);\n        __m256 abs_diff = _mm_abs_ps(diff);\n\n        __m256 threshold = _mm_set1_ps(tolerance);\n        __m256 condition = _mm_cmpgt_ps(abs_diff, threshold);\n\n        __m256 result = _mm_andnot_si256(condition, mask);\n        if (_mm_movemask_ps(result) != 0) {\n            return false; // Found unequal blocks\n        }\n    }\n\n    return true; // All blocks are equal\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c++\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size > sizeof(double)) {\n        return false;\n    }\n\n    const __m256i mask = _mm_set1_epi32(-1); // Mask to compare all elements\n\n    for (size_t i = 0; i < length - block_size + 1; i += block_size) {\n        __m256 v1 = _mm_loadu_ps(vec + i);\n        __m256 v2 = _mm_loadu_ps(vec + i);\n\n        __m256 diff = _mm_sub_ps(v1, v2);\n        __m256 abs_diff = _mm_abs_ps(diff);\n\n        __m256 threshold = _mm_set1_pd(tolerance);\n        __m256 condition = _mm_cmpgt_pd(abs_diff, threshold);\n\n        __m256 result = _mm_andnot_si256(condition, mask);\n        if (_mm_movemask_epi8(result) != 0) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_38_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool vector_block_equal_simd(const double* vec, double tolerance, size_t length, size_t block_size) {\n    if (length % block_size != 0 || block_size > sizeof(double)) {\n        return false; // Invalid input\n    }\n\n    const __m256i mask = _mm_set1_epi32(-1); // Mask to compare all elements\n\n    for (size_t i = 0; i < length - block_size + 1; i += block_size) {\n        __m256d block1 = _mm_loadu_pd(&vec[i]);\n        __m256d block2 = _mm_loadu_pd(&vec[i]);\n\n        __m256d diff = _mm_sub_pd(block1, block2);\n        __m256d abs_diff = _mm_abs_pd(diff);\n\n        __m256d threshold = _mm_set1_pd(tolerance);\n        __m256 cond = _mm_cmpgt_pd(abs_diff, threshold);\n\n        __m256i result = _mm_movemask_ps(cond);\n        if (__builtin_popcount(result) > 0) {\n            return false; // Found a mismatch\n        }\n    }\n\n    return true; // All blocks are equal\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c++\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i sum = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256 vec = _mm256_loadu_ps(matrix + i * cols);\n\n        // Calculate the square of each element in the vector\n        __m256 squared_vec = _mm256_mul_ps(vec, vec);\n\n        // Add the squared elements to the sum\n        sum = _mm256_add_ps(sum, squared_vec);\n\n        // Check if the sum exceeds the threshold\n        __m256 threshold = _mm256_loadu_ps(thresholds + i);\n        __m256 comparison = _mm256_cmpgt_ps(sum, threshold);\n\n        // Store the result in the result array\n        _mm256_storeu_si256((__m256i*)result + i, comparison);\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i sum = _mm256_setzero_ps();\n\n    for (size_t i = 0; i < rows; i += 8) {\n        __m256 v = _mm256_loadu_ps(matrix + i * cols);\n        __m256 norm = _mm256_sqrt_ps(_mm256_mul_ps(v, v));\n        __m256 comp = _mm256_cmpgt_ps(norm, _mm256_loadu_ps(thresholds + i));\n\n        mask |= comp;\n        sum = _mm256_add_ps(sum, norm);\n    }\n\n    for (size_t i = 8; i < rows; ++i) {\n        __m256 v = _mm256_loadu_ps(matrix + i * cols);\n        __m256 norm = _mm256_sqrt_ps(_mm256_mul_ps(v, v));\n        __m256 comp = _mm256_cmpgt_ps(norm, _mm256_loadu_ps(thresholds + i));\n\n        mask |= comp;\n        sum = _mm256_add_ps(sum, norm);\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        if (_mm256_testz_si256(mask, mask)) {\n            result[i] = false;\n        } else {\n            result[i] = true;\n        }\n    }\n\n    // Store the final sum of norms\n    _mm_storeu_ps(result + rows, sum);\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i threshold_mask = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < rows; i += 8) {\n        __m256 row = _mm256_loadu_ps(matrix + i * cols);\n        __m256 norm = _mm256_sqrt_ps(row);\n\n        for (size_t j = 0; j < cols; j++) {\n            threshold_mask = _mm256_or_si256(threshold_mask, _mm256_cmpgt_ps(norm, _mm256_loadu_ps(thresholds + j)));\n        }\n\n        mask = _mm256_or_si256(mask, threshold_mask);\n    }\n\n    for (size_t i = 0; i < rows; i++) {\n        result[i] = _mm256_testz_si256(mask, _mm256_set1_epi32(1));\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256d sum = _mm256_setzero_pd();\n    __m256d norm_squared = _mm256_setzero_pd();\n\n    for (size_t i = 0; i < rows; i += 8) {\n        __m256d vec = _mm256_loadu_pd(matrix + i * cols);\n\n        // Compute the squared Euclidean norm of the current row\n        norm_squared = _mm256_add_pd(norm_squared, _mm256_mul_pd(vec, vec));\n\n        // Sum up the squared norms of all rows\n        sum = _mm256_add_pd(sum, norm_squared);\n    }\n\n    // Reduce the sum of squared norms to a single value\n    double total_sum = _mm256_reduce_add_pd(sum, 0.0);\n\n    // Compare the total sum with the corresponding thresholds\n    for (size_t i = 0; i < rows; ++i) {\n        double threshold = thresholds[i];\n        result[i] = total_sum > threshold;\n    }\n}\n```"}
{"task_id": "SimdBench_39_AVX", "completion": "```c++\nvoid matrix_row_norm_compare_simd(const double* matrix, const double* thresholds, size_t rows, size_t cols, bool* result) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i threshold_mask = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < rows; ++i) {\n        __m256 row = _mm256_loadu_ps(matrix + i * cols);\n        __m256 norm = _mm256_sqrt_ps(row);\n\n        for (size_t j = 0; j < cols; ++j) {\n            threshold_mask = _mm256_or_si256(threshold_mask, _mm256_cmpgt_ps(norm, _mm256_loadu_ps(thresholds + j)));\n        }\n\n        mask = _mm256_or_si256(mask, threshold_mask);\n    }\n\n    for (size_t i = 0; i < rows; ++i) {\n        result[i] = _mm256_testz_si256(mask, _mm256_set1_epi32(1));\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c++\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i result = _mm256_or_si256(avx_A[i], avx_B[i]);\n        _mm256_storeu_si256(&avx_C[i], result);\n    }\n\n    // Handle remaining elements if length is not divisible by 4\n    for (size_t i = length % 4; i < length; ++i) {\n        *C++ = *A++ | *B++;\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i result = _mm256_or_si256(avx_A[i], avx_B[i]);\n        _mm256_storeu_si256(&avx_C[i], result);\n    }\n\n    // Handle remaining elements if length is not divisible by 4\n    for (size_t i = length % 4; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i result = _mm256_or_si256(avx_A[i], avx_B[i]);\n        _mm256_storeu_si256(&avx_C[i], result);\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (size_t i = length % 4; i > 0; --i) {\n        *C++ = (*A++) | (*B++);\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c++\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i result = _mm256_or_si256(avx_A[i], avx_B[i]);\n        _mm256_storeu_si256(avx_C + i, result);\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (size_t i = length % 4; i > 0; --i) {\n        *C++ = (*A++) | (*B++);\n    }\n}\n```"}
{"task_id": "SimdBench_40_AVX", "completion": "```c++\nvoid tensor_bitwise_or_simd(const uint32_t* A, const uint32_t* B, uint32_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i result = _mm256_or_si256(avx_A[i], avx_B[i]);\n        _mm256_storeu_si256(&avx_C[i], result);\n    }\n\n    // Handle remaining elements if length is not divisible by 4\n    for (size_t i = length % 4; i < length; ++i) {\n        C[i] = A[i] | B[i];\n    }\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c++\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sumA = _mm256_setzero_si256();\n    __m256i sumB = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(A + i));\n        __m256i b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(B + i));\n\n        sumA = _mm256_add_epi16(sumA, _mm256_mullo_epi16(a, a));\n        sumB = _mm256_add_epi16(sumB, _mm256_mullo_epi16(b, b));\n    }\n\n    // Handle remaining elements if length is not a multiple of 32\n    for (size_t i = length - (length % 32); i < length; ++i) {\n        sumA = _mm256_add_epi16(sumA, _mm256_mullo_epi16(_mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i])), _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]))));\n    }\n\n    __m256i diff = _mm256_sub_epi64(sumA, sumB);\n\n    int64_t result = 0;\n    _mm256_storeu_si128((__m128i*)&result, diff);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sumA = _mm256_setzero_si256();\n    __m256i sumB = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i diff = _mm256_sub_epi8(a, b);\n        __m256i squareDiff = _mm256_mul_epi8(diff, diff);\n\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 0));\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 1));\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 2));\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 3));\n\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 4));\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 5));\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 6));\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 7));\n    }\n\n    // Handle remaining elements if length is not a multiple of 32\n    for (size_t i = length % 32; i < length; ++i) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n\n        int64_t diff = a - b;\n        int64_t squareDiff = diff * diff;\n\n        sumA = _mm256_add_epi32(sumA, _mm256_set1_epi32(squareDiff));\n        sumB = _mm256_add_epi32(sumB, _mm256_set1_epi32(squareDiff));\n    }\n\n    int64_t totalSumA = _mm256_reduce_add_epi32(sumA, _mm256_set1_epi32(0));\n    int64_t totalSumB = _mm256_reduce_add_epi32(sumB, _mm256_set1_epi32(0));\n\n    return totalSumA - totalSumB;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sum_squares = _mm256_setzero_si256();\n    __m256i sum_products = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i *)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i *)(B + i));\n\n        __m256i square_a = _mm256_mullo_epi16(a, a);\n        __m256i square_b = _mm256_mullo_epi16(b, b);\n\n        __m256i product_ab = _mm256_mullo_epi16(a, b);\n\n        sum_squares = _mm256_add_epi64(sum_squares, _mm256_add_epi64(square_a, square_b));\n        sum_products = _mm256_add_epi64(sum_products, product_ab);\n    }\n\n    // Handle remaining elements if length is not a multiple of 32\n    for (size_t i = length % 32; i < length; ++i) {\n        int8_t a = A[i];\n        int8_t b = B[i];\n\n        sum_squares = _mm256_add_epi64(sum_squares, _mm256_add_epi64(_mm256_set1_epi64x(a * a), _mm256_set1_epi64x(b * b)));\n        sum_products = _mm256_add_epi64(sum_products, _mm256_set1_epi64x(a * b));\n    }\n\n    __m256i diff = _mm256_sub_epi64(sum_squares, sum_products);\n\n    int64_t result = 0;\n    _mm256_storeu_si128((__m128i *)&result, diff);\n    _mm256_storeu_si128((__m128i *)&result + 4, diff + 8);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sumA = _mm256_setzero_si256();\n    __m256i sumB = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i diff = _mm256_sub_epi8(a, b);\n        __m256i squareDiff = _mm256_mullo_epi16(diff, diff);\n\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 0));\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 1));\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 2));\n        sumA = _mm256_add_epi32(sumA, _mm256_extract_epi32(squareDiff, 3));\n\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 4));\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 5));\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 6));\n        sumB = _mm256_add_epi32(sumB, _mm256_extract_epi32(squareDiff, 7));\n    }\n\n    // Handle remaining elements\n    for (size_t i = length % 32; i < length; ++i) {\n        sumA = _mm256_add_epi32(sumA, _mm256_sad_epu8(_mm256_loadu_si256((__m256i*)(A + i)), _mm256_loadu_si256((__m256i*)(B + i))));\n    }\n\n    int64_t result = _mm256_reduce_add_epi32(sumA, 0LL) - _mm256_reduce_add_epi32(sumB, 0LL);\n    return result;\n}\n```"}
{"task_id": "SimdBench_41_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint64_t squarediff_simd(const int8_t * A, const int8_t * B, size_t length) {\n    __m256i sumA = _mm256_setzero_si256();\n    __m256i sumB = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < length; i += 32) {\n        __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(A + i));\n        __m256i b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(B + i));\n\n        sumA = _mm256_add_epi16(sumA, _mm256_mullo_epi16(a, a));\n        sumB = _mm256_add_epi16(sumB, _mm256_mullo_epi16(b, b));\n    }\n\n    // Sum remaining elements\n    for (size_t i = length % 32; i < length; ++i) {\n        sumA = _mm256_add_epi16(sumA, _mm256_mullo_epi16(_mm256_loadu_si256(reinterpret_cast<const __m256i*>(&A[i])), _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&B[i]))));\n    }\n\n    int64_t sumASq = _mm256_reduce_add_epi32(sumA, _MM_REDUCE_ADD_SAT);\n    int64_t sumBSq = _mm256_reduce_add_epi32(sumB, _MM_REDUCE_ADD_SAT);\n\n    int64_t sumAProd = _mm256_reduce_add_epi32(_mm256_mullo_epi16(sumA, sumB), _MM_REDUCE_ADD_SAT);\n\n    return sumASq - sumBSq - sumAProd;\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i mask = _mm256_set1_epi64u(~avx_B[i]);\n        __m256i result = _mm256_and_si256(avx_A[i], mask);\n        _mm256_store_si256(&avx_C[i], result);\n    }\n\n    // Handle remaining elements if length is not divisible by 32\n    for (size_t i = length % 32; i < length; ++i) {\n        *C++ &= ~(*B++);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i mask = _mm256_set1_epi64u(~avx_B[i]);\n        __m256i result = _mm256_and_si256(avx_A[i], mask);\n        _mm256_store_si256(&avx_C[i], result);\n    }\n\n    // Handle remaining elements if length is not divisible by 32\n    for (size_t i = length % 32; i < length; ++i) {\n        *C++ = *A++ & ~(*B++);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    __m256i mask = _mm256_set1_epi32(-1); // Create a mask with all bits set to 1\n\n    for (size_t i = 0; i < length; i += 8) { // Process in chunks of 8 elements\n        __m256i a = _mm256_loadu_si256((__m256i*)(&A[i])); // Load 8 elements from A\n        __m256i b = _mm256_loadu_si256((__m256i*)(&B[i])); // Load 8 elements from B\n        __m256i c = _mm256_loadu_si256((__m256i*)(&C[i])); // Load 8 elements from C\n\n        // Perform bitwise AND between a and the bitwise NOT of b\n        __m256i result = _mm256_andnot_si256(b, a);\n\n        // Store the result back into C\n        _mm256_storeu_si256((__m256i*)(&C[i]), result);\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i mask = _mm256_set1_epi64(0xFFFFFFFFFFFFFFFF); // Create a mask with all bits set to 1\n        __m256i negated_B = _mm256_xor_si256(_mm256_set1_epi64(0), _mm256_loadu_si256(avx_B + i)); // Negate each element of B\n        __m256i result = _mm256_and_si256(mask, negated_B); // Perform bitwise AND between mask and negated_B\n        _mm256_storeu_si256(avx_C + i, result); // Store the result back into C\n    }\n\n    // Handle remaining elements if length is not a multiple of 32\n    for (size_t i = length % 32; i < length; ++i) {\n        *avx_C[i] &= ~(*avx_B[i]); // Perform bitwise AND between the last element of B and the corresponding element of C\n    }\n}\n```"}
{"task_id": "SimdBench_42_AVX", "completion": "```c\nvoid tensor_bit_clear_simd(const uint64_t* A, const uint64_t* B, uint64_t* C, size_t length) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n    __m256i* avx_C = (__m256i*)C;\n\n    for (size_t i = 0; i < length / 32; ++i) {\n        __m256i mask = _mm256_set1_epi64u(~avx_B[i]);\n        __m256i result = _mm256_and_si256(avx_A[i], mask);\n        _mm256_store_si256(&avx_C[i], result);\n    }\n\n    // Handle remaining elements if length is not divisible by 32\n    for (size_t i = length % 32; i < length; ++i) {\n        *C++ = *A++ & ~(*B++);\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i mask_pos = _mm256_set_epi32(0x7F, 0x7F, 0x7F, 0x7F); // Mask for positive values\n    __m256i mask_neg = _mm256_set_epi32(0xFF, 0xFF, 0xFF, 0xFF); // Mask for negative values\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        // Compare A and B for positive values\n        __m256i pos_mask = _mm256_and_si256(a, mask_pos);\n        __m256i neg_mask = _mm256_and_si256(b, mask_neg);\n\n        // Combine results for positive values\n        __m256i result_pos = _mm256_or_si256(pos_mask, neg_mask);\n\n        // Compare A and B for negative values\n        pos_mask = _mm256_and_si256(a, mask_neg);\n        neg_mask = _mm256_and_si256(b, mask_pos);\n\n        // Combine results for negative values\n        __m256i result_neg = _mm256_or_si256(pos_mask, neg_mask);\n\n        // Store results back to C\n        _mm256_storeu_si256((__m256i*)(C + i), result_pos);\n        _mm256_storeu_si256((__m256i*)(C + i + 4), result_neg);\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i mask_pos = _mm256_set_epi32(0x7F, 0x7F, 0x7F, 0x7F); // Mask for positive values\n    __m256i mask_neg = _mm256_set_epi32(0xFF, 0xFF, 0xFF, 0xFF); // Mask for negative values\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        // Check if both A and B are positive\n        __m256i pos_mask = _mm256_and_si256(a, mask_pos);\n        __m256i neg_mask = _mm256_and_si256(b, mask_neg);\n\n        // Perform element-wise three-valued logic operation\n        __m256i result = _mm256_or_si256(pos_mask, neg_mask);\n\n        // Store the result back to C\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i mask_pos = _mm256_set_epi32(0x7F, 0x7F, 0x7F, 0x7F); // Mask for positive values\n    __m256i mask_neg = _mm256_set_epi32(0xFF, 0xFF, 0xFF, 0xFF); // Mask for negative values\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i pos_a = _mm256_and_si256(a, mask_pos);\n        __m256i neg_b = _mm256_and_si256(b, mask_neg);\n\n        __m256i result = _mm256_or_si256(pos_a, neg_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i mask_pos = _mm256_set_epi32(0x7F, 0x7F, 0x7F, 0x7F); // Mask for positive values\n    __m256i mask_neg = _mm256_set_epi32(0xFF, 0xFF, 0xFF, 0xFF); // Mask for negative values\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i pos_a = _mm256_and_si256(a, mask_pos);\n        __m256i neg_b = _mm256_and_si256(b, mask_neg);\n\n        __m256i result = _mm256_or_si256(pos_a, neg_b);\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_43_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_three_valued_logic_simd(const int8_t* A, const int8_t* B, int8_t* C, size_t length) {\n    __m256i mask_pos = _mm256_set_epi32(0x7F, 0x7F, 0x7F, 0x7F); // Mask for positive values\n    __m256i mask_neg = _mm256_set_epi32(0xFF, 0xFF, 0xFF, 0xFF); // Mask for negative values\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i a = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm256_loadu_si256((__m256i*)(B + i));\n\n        __m256i pos_a = _mm256_and_si256(a, mask_pos);\n        __m256i neg_b = _mm256_and_si256(b, mask_neg);\n\n        __m256i result = _mm256_or_si256(pos_a, neg_b);\n\n        _mm256_storeu_si256((__m256i*)(C + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c++\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i* a = (__m128i*)A;\n    __m128i* b = (__m128i*)B;\n    __m128i* c = (__m128i*)C;\n\n    // Calculate the midpoint index\n    size_t mid_index = length / 2;\n\n    // Loop through the array in chunks of 4 elements\n    for (size_t i = 0; i < mid_index; i += 4) {\n        __m128i a_chunk = _mm_loadu_si128(a + i);\n        __m128i b_chunk = _mm_loadu_si128(b + i);\n\n        // Perform bitwise XOR and AND operations\n        __m128i xor_result = _mm_xor_si128(a_chunk, b_chunk);\n        __m128i and_result = _mm_and_si128(a_chunk, b_chunk);\n\n        // Store the results back into C\n        _mm_storeu_si128(c + i, xor_result);\n        _mm_storeu_si128(c + i + 4, and_result);\n    }\n\n    // Handle remaining elements if the length is odd\n    if (length % 4 != 0) {\n        __m128i a_last = _mm_loadu_si128(a + mid_index);\n        __m128i b_last = _mm_loadu_si128(b + mid_index);\n\n        __m128i xor_result = _mm_xor_si128(a_last, b_last);\n        __m128i and_result = _mm_and_si128(a_last, b_last);\n\n        _mm_storeu_si128(c + mid_index, xor_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i* avx_A = (__m128i*)A;\n    __m128i* avx_B = (__m128i*)B;\n    __m128i* avx_C = (__m128i*)C;\n\n    // Calculate the midpoint\n    size_t mid = length / 2;\n\n    // Process the first half of the tensors\n    for (size_t i = 0; i < mid; ++i) {\n        __m128i a = _mm_loadu_si128(avx_A + i);\n        __m128i b = _mm_loadu_si128(avx_B + i);\n\n        // Store the value from A\n        _mm_storeu_si128(avx_C + i, a);\n\n        // Perform bitwise XOR and AND operations\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        // Store the results\n        _mm_storeu_si128(avx_C + i + mid, xor_result);\n        _mm_storeu_si128(avx_C + i + mid * 2, and_result);\n    }\n\n    // Process the second half of the tensors if necessary\n    if (length % 2 != 0) {\n        __m128i last_a = _mm_loadu_si128(avx_A + mid);\n        __m128i last_b = _mm_loadu_si128(avx_B + mid);\n\n        // Store the value from A\n        _mm_storeu_si128(avx_C + mid, last_a);\n\n        // Perform bitwise XOR and AND operations\n        __m128i xor_result = _mm_xor_si128(last_a, last_b);\n        __m128i and_result = _mm_and_si128(last_a, last_b);\n\n        // Store the results\n        _mm_storeu_si128(avx_C + mid + mid, xor_result);\n        _mm_storeu_si128(avx_C + mid + mid * 2, and_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c++\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i* a = (__m128i*)A;\n    __m128i* b = (__m128i*)B;\n    __m128i* c = (__m128i*)C;\n\n    // Calculate the midpoint index\n    size_t mid_index = length / 2;\n\n    // Process the first half of the tensors\n    for (size_t i = 0; i < mid_index; ++i) {\n        __m128i a_val = _mm_loadu_si128(a + i);\n        __m128i b_val = _mm_loadu_si128(b + i);\n\n        // Store the value from A\n        _mm_storeu_si128(c + i, a_val);\n\n        // Perform bitwise XOR and AND operations\n        __m128i xor_result = _mm_xor_si128(a_val, b_val);\n        __m128i and_result = _mm_and_si128(a_val, b_val);\n\n        // Store the results back into C\n        _mm_storeu_si128(c + i, xor_result);\n        _mm_storeu_si128(c + i + 4, and_result);\n    }\n\n    // Process the second half of the tensors if necessary\n    if (length % 2 != 0) {\n        __m128i a_val = _mm_loadu_si128(a + mid_index);\n        __m128i b_val = _mm_loadu_si128(b + mid_index);\n\n        // Store the value from A\n        _mm_storeu_si128(c + mid_index, a_val);\n\n        // Perform bitwise XOR and AND operations\n        __m128i xor_result = _mm_xor_si128(a_val, b_val);\n        __m128i and_result = _mm_and_si128(a_val, b_val);\n\n        // Store the results back into C\n        _mm_storeu_si128(c + mid_index, xor_result);\n        _mm_storeu_si128(c + mid_index + 4, and_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i* avx_A = (__m128i*)A;\n    __m128i* avx_B = (__m128i*)B;\n    __m128i* avx_C = (__m128i*)C;\n\n    size_t mid_point = length / 2;\n    size_t end_point = length - 1;\n\n    // Process the first half of the tensors\n    for (size_t i = 0; i < mid_point; ++i) {\n        __m128i a = _mm_loadu_si128(avx_A + i);\n        __m128i b = _mm_loadu_si128(avx_B + i);\n\n        // Store the value from A\n        _mm_storeu_si128(avx_C + i, a);\n\n        // Perform bitwise XOR and AND operations\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        // Store the results back into C\n        _mm_storeu_si128(avx_C + i, xor_result);\n        _mm_storeu_si128(avx_C + i, and_result);\n    }\n\n    // Process the second half of the tensors\n    for (size_t i = mid_point; i <= end_point; ++i) {\n        __m128i a = _mm_loadu_si128(avx_A + i);\n        __m128i b = _mm_loadu_si128(avx_B + i);\n\n        // Perform bitwise XOR and AND operations\n        __m128i xor_result = _mm_xor_si128(a, b);\n        __m128i and_result = _mm_and_si128(a, b);\n\n        // Store the results back into C\n        _mm_storeu_si128(avx_C + i, xor_result);\n        _mm_storeu_si128(avx_C + i, and_result);\n    }\n}\n```"}
{"task_id": "SimdBench_44_AVX", "completion": "```c\nvoid tensor_bit_op_simd(const uint16_t* A, const uint16_t* B, uint16_t* C, size_t length) {\n    __m128i* ptr_A = (__m128i*)A;\n    __m128i* ptr_B = (__m128i*)B;\n    __m128i* ptr_C = (__m128i*)C;\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m128i a = _mm_loadu_si128(ptr_A + i);\n        __m128i b = _mm_loadu_si128(ptr_B + i);\n\n        // Bitwise OR operation for elements before the midpoint\n        __m128i xor_result = _mm_or_si128(a, b);\n\n        // Bitwise AND operation for elements before the midpoint\n        __m128i and_result = _mm_and_si128(a, b);\n\n        // Store the results back into C\n        _mm_storeu_si128(ptr_C + i, xor_result);\n        _mm_storeu_si128(ptr_C + i + 4, and_result);\n    }\n\n    // Handle remaining elements if length is not a multiple of 4\n    for (size_t i = length % 4; i > 0; --i) {\n        *ptr_C++ = *ptr_A++;\n        *ptr_C++ = *ptr_B++;\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c++\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask1 = _mm256_set_epi32(0, 0, 0, 1); // Mask for cond1\n    __m256i mask2 = _mm256_set_epi32(0, 0, 1, 0); // Mask for cond2\n\n    for (size_t k = 0; k < dim3; ++k) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t i = 0; i < dim1; i += 4) {\n                __m256 a = _mm256_loadu_ps(A + i * 4);\n                __m256 b = _mm256_loadu_ps(B + i * 4);\n                __m256 c = _mm256_loadu_ps(C + i * 4);\n                __m256 d = _mm256_loadu_ps(D + i * 4);\n\n                __m256 mask1_i = _mm256_and_si256(mask1, _mm256_cmpeq_epi32(_mm256_loadu_si256(cond1 + i), _mm256_set_epi32(1)));\n                __m256 mask2_i = _mm256_and_si256(mask2, _mm256_cmpeq_epi32(_mm256_loadu_si256(cond2 + i), _mm256_set_epi32(1)));\n\n                __m256 result = _mm256_blendv_ps(a, b, mask1_i);\n                result = _mm256_blendv_ps(result, c, mask2_i);\n                result = _mm256_blendv_ps(result, d, ~mask1_i & ~mask2_i);\n\n                _mm256_storeu_ps(out + i * 4, result);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask1 = _mm256_setzero_si256();\n    __m256i mask2 = _mm256_setzero_si256();\n\n    for (size_t k = 0; k < dim3; ++k) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t i = 0; i < dim1; i += 4) {\n                __m256 a = _mm256_loadu_ps(A + i * 4);\n                __m256 b = _mm256_loadu_ps(B + i * 4);\n                __m256 c = _mm256_loadu_ps(C + i * 4);\n                __m256 d = _mm256_loadu_ps(D + i * 4);\n\n                __m256 mask1_i = _mm256_and_si256(_mm256_loadu_si256(cond1 + i), _mm256_loadu_si256(cond2 + i));\n                __m256 mask2_i = _mm256_andnot_si256(mask1_i, _mm256_set1_epi32(1));\n\n                __m256 result = _mm256_blendv_ps(a, b, mask1_i);\n                result = _mm256_blendv_ps(result, c, mask2_i);\n                result = _mm256_blendv_ps(result, d, mask2_i);\n\n                _mm256_storeu_ps(out + i * 4, result);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask1 = _mm256_set_epi32(0, 1, 0, 1); // Create a mask for cond1\n    __m256i mask2 = _mm256_set_epi32(0, 0, 1, 1); // Create a mask for cond2\n\n    for (size_t k = 0; k < dim3; ++k) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t i = 0; i < dim1; i += 4) {\n                __m256 a = _mm256_loadu_ps(A + i * 4);\n                __m256 b = _mm256_loadu_ps(B + i * 4);\n                __m256 c = _mm256_loadu_ps(C + i * 4);\n                __m256 d = _mm256_loadu_ps(D + i * 4);\n\n                __m256 mask1_i = _mm256_and_si256(mask1, _mm256_loadu_si256(cond1 + i));\n                __m256 mask2_i = _mm256_and_si256(mask2, _mm256_loadu_si256(cond2 + i));\n\n                __m256 result = _mm256_blendv_ps(a, b, mask1_i);\n                result = _mm256_blendv_ps(result, c, mask2_i);\n                result = _mm256_blendv_ps(result, d, ~mask1_i & ~mask2_i);\n\n                _mm256_storeu_ps(out + i * 4, result);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask1 = _mm256_setzero_si256();\n    __m256i mask2 = _mm256_setzero_si256();\n\n    for (size_t k = 0; k < dim3; ++k) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t i = 0; i < dim1; i += 4) {\n                __m256 a = _mm256_loadu_ps(A + i * 4);\n                __m256 b = _mm256_loadu_ps(B + i * 4);\n                __m256 c = _mm256_loadu_ps(C + i * 4);\n                __m256 d = _mm256_loadu_ps(D + i * 4);\n\n                __m256 mask_a = _mm256_and_si256(_mm256_cmpeq_epi32(cond1 + i * 4, _mm256_set1_epi32(1)), _mm256_cmpeq_epi32(cond2 + i * 4, _mm256_set1_epi32(1)));\n                __m256 mask_b = _mm256_and_si256(_mm256_cmpeq_epi32(cond1 + i * 4, _mm256_set1_epi32(1)), _mm256_cmpeq_epi32(cond2 + i * 4, _mm256_set1_epi32(0)));\n                __m256 mask_c = _mm256_and_si256(_mm256_cmpeq_epi32(cond1 + i * 4, _mm256_set1_epi32(0)), _mm256_cmpeq_epi32(cond2 + i * 4, _mm256_set1_epi32(1)));\n                __m256 mask_d = _mm256_and_si256(_mm256_cmpeq_epi32(cond1 + i * 4, _mm256_set1_epi32(0)), _mm256_cmpeq_epi32(cond2 + i * 4, _mm256_set1_epi32(0)));\n\n                __m256 result_a = _mm256_blendv_ps(a, b, mask_a);\n                __m256 result_b = _mm256_blendv_ps(b, c, mask_b);\n                __m256 result_c = _mm256_blendv_ps(c, d, mask_c);\n                __m256 result_d = _mm256_blendv_ps(d, a, mask_d);\n\n                _mm256_storeu_ps(out + i * 4, result_a);\n                _mm256_storeu_ps(out + i * 4 + 4, result_b);\n                _mm256_storeu_ps(out + i * 4 + 8, result_c);\n                _mm256_storeu_ps(out + i * 4 + 12, result_d);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_45_AVX", "completion": "```c\nvoid tensor_multi_select_simd(const float* A, const float* B, const float* C, const float* D, const bool* cond1, const bool* cond2, float* out, size_t dim1, size_t dim2, size_t dim3) {\n    __m256i mask1 = _mm256_set_epi32(0, 1, 0, 1); // Create a mask for cond1\n    __m256i mask2 = _mm256_set_epi32(0, 0, 1, 1); // Create a mask for cond2\n\n    for (size_t k = 0; k < dim3; ++k) {\n        for (size_t j = 0; j < dim2; ++j) {\n            for (size_t i = 0; i < dim1; i += 4) {\n                __m256 a = _mm256_loadu_ps(A + i * 4);\n                __m256 b = _mm256_loadu_ps(B + i * 4);\n                __m256 c = _mm256_loadu_ps(C + i * 4);\n                __m256 d = _mm256_loadu_ps(D + i * 4);\n\n                __m256 mask1_i = _mm256_and_si256(mask1, _mm256_loadu_si256(cond1 + i));\n                __m256 mask2_i = _mm256_and_si256(mask2, _mm256_loadu_si256(cond2 + i));\n\n                __m256 result = _mm256_blendv_ps(a, b, mask1_i);\n                result = _mm256_blendv_ps(result, c, mask2_i);\n                result = _mm256_blendv_ps(result, d, ~mask1_i & ~mask2_i);\n\n                _mm256_storeu_ps(out + i * 4, result);\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c++\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const __m256i* ptr_A = reinterpret_cast<const __m256i*>(A);\n    __m256i* ptr_out = reinterpret_cast<__m256i*>(out);\n\n    for (size_t i = 0; i < dim1 * dim2 * dim3; i += 4) {\n        __m256i vec_A = _mm256_loadu_si256(ptr_A++);\n        __m256i vec_out = _mm256_setzero_si256();\n\n        // Count the number of bits set to 1 in each element of vec_A\n        vec_out = _mm256_add_epi8(vec_out, _mm256_sad_epu8(vec_A, _mm256_set1_epi8(0xFF)));\n\n        // Store the result in vec_out\n        _mm256_storeu_si256(ptr_out++, vec_out);\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const int vec_size = sizeof(uint32_t) / sizeof(__m256i);\n    const int num_vecs = dim1 * dim2 * dim3 / vec_size;\n\n    __m256i* vec_A = (__m256i*)A;\n    __m256i* vec_out = (__m256i*)out;\n\n    for (int i = 0; i < num_vecs; ++i) {\n        __m256i sum = _mm256_setzero_si256();\n        for (int j = 0; j < vec_size; ++j) {\n            sum = _mm256_add_epi32(sum, _mm256_sad_epu8(_mm256_loadu_si256(&vec_A[i * vec_size + j]), _mm256_setzero_si256()));\n        }\n        _mm_storeu_si256(&vec_out[i], sum);\n    }\n\n    // Handle remaining elements\n    for (int i = num_vecs * vec_size; i < dim1 * dim2 * dim3; ++i) {\n        uint32_t value = A[i];\n        uint8_t count = 0;\n        while (value > 0) {\n            count += value & 1;\n            value >>= 1;\n        }\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c++\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const int vec_size = sizeof(uint32_t) / sizeof(__m256i);\n    const int num_vecs = dim1 * dim2 * dim3 / vec_size;\n\n    __m256i* vec_A = (__m256i*)A;\n    __m256i* vec_out = (__m256i*)out;\n\n    for (int i = 0; i < num_vecs; ++i) {\n        __m256i sum = _mm256_setzero_si256();\n        for (int j = 0; j < vec_size; ++j) {\n            sum = _mm256_add_epi32(sum, _mm256_sad_epu8(_mm256_loadu_si256(&vec_A[i * vec_size + j]), _mm256_set1_epi8(0xFF)));\n        }\n        _mm_storeu_si256(&vec_out[i], sum);\n    }\n\n    // Handle remaining elements\n    for (int i = num_vecs * vec_size; i < dim1 * dim2 * dim3; ++i) {\n        uint32_t value = A[i];\n        for (int j = 0; j < 32; ++j) {\n            if ((value >> j) & 1) {\n                out[i] |= 1 << j;\n            }\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c++\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const int vec_size = sizeof(uint32_t) / sizeof(__m256i);\n    const int num_vecs = dim1 * dim2 * dim3 / vec_size;\n\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n\n    for (int i = 0; i < num_vecs; ++i) {\n        __m256i vec_a = _mm_loadu_si256(reinterpret_cast<const __m256i*>(A + i * vec_size));\n        __m256i vec_out = _mm_sad_epu8(_mm_and_si256(vec_a, mask), _mm_setzero_si256());\n        _mm_storeu_si256(reinterpret_cast<__m256i*>(out + i * vec_size), vec_out);\n    }\n\n    // Handle remaining elements\n    for (int i = num_vecs * vec_size; i < dim1 * dim2 * dim3; ++i) {\n        uint32_t val = A[i];\n        uint8_t count = 0;\n        while (val > 0) {\n            if (val & 1) {\n                count++;\n            }\n            val >>= 1;\n        }\n        out[i] = count;\n    }\n}\n```"}
{"task_id": "SimdBench_46_AVX", "completion": "```c++\nvoid tensor_bit_count_simd(const uint32_t* A, uint8_t* out, size_t dim1, size_t dim2, size_t dim3) {\n    const int vec_size = sizeof(uint32_t) / sizeof(__m256i);\n    const int num_vecs = (dim1 * dim2 * dim3 + vec_size - 1) / vec_size;\n\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n\n    for (int i = 0; i < num_vecs; ++i) {\n        __m256i vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(A + i * vec_size));\n        __m256i count = _mm_popcnt_u32(_mm_and_si256(vec, mask));\n        _mm_storeu_si256(reinterpret_cast<__m256i*>(out + i), count);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    __m256i mask = _mm_set_epi32(0, 0, 0, -1); // Mask to set negative values to 0\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m256i a = _mm_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm_loadu_si256((__m256i*)(B + i));\n\n        // Check if the element is divisible by 3\n        __m256i is_divisible_by_3 = _mm_cmpeq_epi32(a, _mm_mul_epi32(a, _mm_set_epi32(3, 0, 0, 0)));\n\n        // Apply the condition: if divisible by 3, take absolute value, otherwise keep original\n        __m256i result = _mm_blendv_epi32(b, _mm_andnot_si256(is_divisible_by_3, a), is_divisible_by_3);\n\n        _mm_storeu_si256((__m256i*)(B + i), result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const __m256i mask = _mm_set_epi32(0, 1, 0, 1); // Mask to check divisibility by 3\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m256i a = _mm_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm_loadu_si256((__m256i*)&B[i]);\n\n        __m256i abs_a = _mm_and_si256(a, mask); // Check if divisible by 3\n        __m256i abs_b = _mm_or_si256(abs_a, _mm_sub_epi32(b, abs_a)); // Set to absolute value if divisible by 3\n\n        _mm_storeu_si256((__m256i*)&B[i], abs_b);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    const __m256i mask = _mm_set_epi32(0, 1, 0, 1); // Mask to select elements divisible by 3\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m256i a = _mm_loadu_si256((__m256i*)&A[i]);\n        __m256i b = _mm_loadu_si256((__m256i*)&B[i]);\n\n        __m256i abs_a = _mm_and_si256(a, mask); // Select elements divisible by 3\n        __m256i abs_b = _mm_or_si256(_mm_andnot_si256(mask, a), b); // Set elements not divisible by 3 to their original values\n\n        _mm_storeu_si256((__m256i*)&B[i], abs_b);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    __m256i mask = _mm_set_epi32(0, 1, 2, 3); // Mask to select elements divisible by 3\n    __m256i abs_mask = _mm_set_epi32(0, 1, 2, 3); // Mask to select elements that are not divisible by 3\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m256i a = _mm_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm_loadu_si256((__m256i*)(B + i));\n\n        // Extract elements divisible by 3\n        __m256i div_by_3 = _mm_and_si256(a, mask);\n\n        // Extract elements not divisible by 3\n        __m256i not_div_by_3 = _mm_andnot_si256(div_by_3, mask);\n\n        // Apply the condition: if divisible by 3, take absolute value, otherwise keep original\n        __m256i abs_result = _mm_or_si256(_mm_and_si256(div_by_3, _mm_cvtepi32_ps(_mm_setzero_si256())), _mm_andnot_si256(not_div_by_3, b));\n\n        // Store the result back to B\n        _mm_storeu_si256((__m256i*)(B + i), abs_result);\n    }\n}\n```"}
{"task_id": "SimdBench_47_AVX", "completion": "```c\nvoid matrix_conditional_abs_simd(const int* A, int* B, size_t rows, size_t cols) {\n    __m256i mask = _mm_set_epi32(0, 1, 0, 1); // Mask to select elements divisible by 3\n    __m256i abs_mask = _mm_set_epi32(0, 1, 0, 1); // Mask to select elements that are not divisible by 3\n\n    for (size_t i = 0; i < rows * cols; i += 4) {\n        __m256i a = _mm_loadu_si256((__m256i*)(A + i));\n        __m256i b = _mm_loadu_si256((__m256i*)(B + i));\n\n        __m256i div_by_3 = _mm_cmpeq_epi32(a, _mm_mul_epi32(_mm_slli_epi32(a, 2), mask)); // Check if divisible by 3\n        __m256i not_div_by_3 = _mm_cmpneq_epi32(div_by_3, _mm_setzero_si256()); // Check if not divisible by 3\n\n        __m256i abs_a = _mm_and_si256(div_by_3, _mm_sub_epi32(_mm_setzero_si256(), a)); // Absolute value for divisible by 3\n        __m256i abs_b = _mm_andnot_si256(not_div_by_3, b); // Absolute value for not divisible by 3\n\n        _mm_storeu_si256((__m256i*)(B + i), abs_a);\n        _mm_storeu_si256((__m256i*)(B + i + 4), abs_b);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c++\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask = _mm_set_epi32(0xF0, 0xF0, 0xF0, 0xF0); // Mask to check if the low 4 bits are all 1\n    __m256i rotate_mask = _mm_set_epi32(0xFF << rotate_bits, 0xFF << rotate_bits, 0xFF << rotate_bits, 0xFF << rotate_bits); // Rotate mask\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm_loadu_si256((__m256i*)(&src[i]));\n        __m256i dst_vec = _mm_loadu_si256((__m256i*)(&dst[i]));\n\n        __m256i low_4_bits = _mm_and_si256(src_vec, mask); // Extract the low 4 bits\n        __m256i rotated_low_4_bits = _mm_and_si256(_mm_srli_epi32(low_4_bits, rotate_bits), rotate_mask); // Rotate the low 4 bits\n        __m256i negated_low_4_bits = _mm_xor_si256(low_4_bits, rotate_mask); // Negate the low 4 bits\n\n        __m256i result = _mm_blendv_epi32(rotated_low_4_bits, negated_low_4_bits, low_4_bits); // Blend the results based on the condition\n\n        _mm_storeu_si256((__m256i*)(&dst[i]), result);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask = _mm_set_epi32(0xF0F0F0F0, 0xF0F0F0F0, 0xF0F0F0F0, 0xF0F0F0F0);\n    __m256i rotate_mask = _mm_set_epi32(0x000000FF, 0x000000FF, 0x000000FF, 0x000000FF);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256i src_vec = _mm_loadu_si256((__m256i*)(&src[i]));\n        __m256i dst_vec = _mm_loadu_si256((__m256i*)(&dst[i]));\n\n        // Extract the low 4 bits of each element\n        __m256i low_4_bits = _mm_and_si256(src_vec, mask);\n\n        // Compare the low 4 bits with 0xF0F0F0F0\n        __m256i compare_result = _mm_cmpeq_epi32(low_4_bits, mask);\n\n        // Perform the conditional bitwise operation\n        __m256i rotated_dst = _mm_blendv_epi32(dst_vec, _mm_xor_si256(src_vec, rotate_mask), compare_result);\n\n        // Store the result back to memory\n        _mm_storeu_si256((__m256i*)(&dst[i]), rotated_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask = _mm_set_epi32(0xF0, 0xF0, 0xF0, 0xF0); // Mask to check if the low 4 bits are all 1\n    __m256i rotate_mask = _mm_set_epi32(0xFF << rotate_bits, 0xFF << rotate_bits, 0xFF << rotate_bits, 0xFF << rotate_bits); // Rotate mask\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_vec = _mm_loadu_si256((__m256i*)(&src[i]));\n        __m256i dst_vec = _mm_loadu_si256((__m256i*)(&dst[i]));\n\n        __m256i low_4_bits = _mm_and_si256(src_vec, mask); // Check if the low 4 bits are all 1\n        __m256i rotated_low_4_bits = _mm_and_si256(low_4_bits, rotate_mask); // Rotate the low 4 bits\n\n        __m256i negated_low_4_bits = _mm_xor_si256(low_4_bits, rotate_mask); // Negate the low 4 bits\n\n        __m256i new_dst_vec = _mm_or_si256(dst_vec, rotated_low_4_bits); // Set the rotated low 4 bits\n        new_dst_vec = _mm_or_si256(new_dst_vec, negated_low_4_bits); // Set the negated low 4 bits\n\n        _mm_storeu_si256((__m256i*)(&dst[i]), new_dst_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask = _mm_set_epi32(0xF, 0xF, 0xF, 0xF); // Mask to check if low 4 bits are all 1\n    __m256i rotate_mask = _mm_set_epi32(0xFF << rotate_bits, 0xFF << rotate_bits, 0xFF << rotate_bits, 0xFF << rotate_bits); // Rotate mask\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256i src_chunk = _mm_loadu_si256((__m256i*)(&src[i]));\n        __m256i dst_chunk = _mm_loadu_si256((__m256i*)(&dst[i]));\n\n        __m256i low_4_bits = _mm_and_si256(src_chunk, mask); // Check if low 4 bits are all 1\n        __m256i rotated_chunk = _mm_xor_si256(_mm_andnot_si256(low_4_bits, rotate_mask), src_chunk); // Rotate or negate based on condition\n\n        _mm_storeu_si256((__m256i*)(&dst[i]), rotated_chunk);\n    }\n}\n```"}
{"task_id": "SimdBench_48_AVX", "completion": "```c\nvoid matrix_conditional_bit_rotate_simd(const uint32_t* src, uint32_t* dst, uint8_t rotate_bits, size_t length) {\n    __m256i mask = _mm_set_epi32(0xF0F0F0F0, 0xF0F0F0F0, 0xF0F0F0F0, 0xF0F0F0F0);\n    __m256i rotate_mask = _mm_set_epi32((uint32_t)(rotate_bits & 0xF), (uint32_t)((rotate_bits >> 4) & 0xF), (uint32_t)((rotate_bits >> 8) & 0xF), (uint32_t)((rotate_bits >> 12) & 0xF));\n    __m256i rotate_shift = _mm_set_epi32(0, 0, 0, 0);\n\n    for (size_t i = 0; i < length / 4; ++i) {\n        __m256i src_vec = _mm_loadu_si256((__m256i*)(&src[i * 4]));\n        __m256i dst_vec = _mm_loadu_si256((__m256i*)(&dst[i * 4]));\n\n        __m256i mask_and_src = _mm_and_si256(src_vec, mask);\n        __m256i mask_and_not_src = _mm_andnot_si256(mask, src_vec);\n        __m256i rotated_src = _mm_shuffle_epi8(mask_and_src, rotate_mask);\n        __m256i negated_src = _mm_xor_si256(rotated_src, mask_and_not_src);\n\n        __m256i rotated_dst = _mm_shuffle_epi8(dst_vec, rotate_mask);\n        __m256i negated_dst = _mm_xor_si256(rotated_dst, mask_and_not_src);\n\n        _mm_storeu_si256((__m256i*)(&dst[i * 4]), negated_dst);\n    }\n\n    if (length % 4 > 0) {\n        __m256i src_vec = _mm_loadu_si256((__m256i*)(&src[length - 4]));\n        __m256i dst_vec = _mm_loadu_si256((__m256i*)(&dst[length - 4]));\n\n        __m256i mask_and_src = _mm_and_si256(src_vec, mask);\n        __m256i mask_and_not_src = _mm_andnot_si256(mask, src_vec);\n        __m256i rotated_src = _mm_shuffle_epi8(mask_and_src, rotate_mask);\n        __m256i negated_src = _mm_xor_si256(rotated_src, mask_and_not_src);\n\n        __m256i rotated_dst = _mm_shuffle_epi8(dst_vec, rotate_mask);\n        __m256i negated_dst = _mm_xor_si256(rotated_dst, mask_and_not_src);\n\n        _mm_storeu_si256((__m256i*)(&dst[length - 4]), negated_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 epsilon_vec = _mm_set_ps(epsilon, epsilon, epsilon, epsilon);\n    __m256 zero_vec = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 b_vec = _mm_loadu_ps(&B[i]);\n\n        __m256 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm_abs_ps(diff_vec);\n\n        __m256 condition_vec = _mm_cmpgt_ps(abs_diff_vec, epsilon_vec);\n        __m256 neg_condition_vec = _mm_cmplt_ps(abs_diff_vec, -epsilon_vec);\n\n        __m256 result_vec = _mm_or_ps(condition_vec, neg_condition_vec);\n        __m256 zero_result_vec = _mm_and_ps(result_vec, zero_vec);\n\n        _mm_storeu_ps(&out[i], zero_result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 epsilon_vec = _mm_set_ps1(epsilon);\n    __m256 zero_vec = _mm_setzero_ps();\n    __m256 one_vec = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 b_vec = _mm_loadu_ps(&B[i]);\n\n        __m256 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm_abs_ps(diff_vec);\n\n        __m256 mask_pos = _mm_cmpgt_ps(abs_diff_vec, epsilon_vec);\n        __m256 mask_neg = _mm_cmplt_ps(abs_diff_vec, epsilon_vec);\n        __m256 mask_zero = _mm_cmpeq_ps(abs_diff_vec, zero_vec);\n\n        __m256 result_pos = _mm_and_ps(mask_pos, one_vec);\n        __m256 result_neg = _mm_and_ps(mask_neg, negative_vec);\n        __m256 result_zero = _mm_andnot_ps(mask_zero, zero_vec);\n\n        _mm_storeu_ps(&out[i], result_pos);\n        _mm_storeu_ps(&out[i + 4], result_neg);\n        _mm_storeu_ps(&out[i + 8], result_zero);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c++\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 epsilon_vec = _mm_set1_ps(epsilon);\n    __m256 zero_vec = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 b_vec = _mm_loadu_ps(&B[i]);\n\n        __m256 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm_abs_ps(diff_vec);\n\n        __m256 mask_pos = _mm_cmpgt_ps(abs_diff_vec, epsilon_vec);\n        __m256 mask_neg = _mm_cmplt_ps(abs_diff_vec, -epsilon_vec);\n\n        __m256 result_pos = _mm_and_ps(mask_pos, _mm_set1_ps(1));\n        __m256 result_neg = _mm_and_ps(mask_neg, _mm_set1_ps(-1));\n\n        __m256 result_zero = _mm_andnot_ps(_mm_or_ps(mask_pos, mask_neg), zero_vec);\n\n        _mm_storeu_ps(&out[i], result_pos);\n        _mm_storeu_ps(&out[i + 4], result_neg);\n        _mm_storeu_ps(&out[i + 8], result_zero);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 epsilon_vec = _mm_set1_ps(epsilon);\n    __m256 zero_vec = _mm_setzero_ps();\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 b_vec = _mm_loadu_ps(&B[i]);\n\n        __m256 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm_abs_ps(diff_vec);\n\n        __m256 condition_vec = _mm_cmpgt_ps(abs_diff_vec, epsilon_vec);\n        __m256 neg_condition_vec = _mm_cmplt_ps(abs_diff_vec, -epsilon_vec);\n\n        __m256 result_vec = _mm_or_ps(condition_vec, neg_condition_vec);\n        __m256 zero_result_vec = _mm_and_ps(result_vec, zero_vec);\n\n        _mm_storeu_ps(&out[i], zero_result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_49_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid tensor_ternary_compare_simd(const float* A, const float* B, int8_t* out, float epsilon, size_t length) {\n    __m256 epsilon_vec = _mm_set_ps1(epsilon);\n    __m256 zero_vec = _mm_setzero_ps();\n    __m256 one_vec = _mm_set1_ps(1.0f);\n\n    for (size_t i = 0; i < length; i += 8) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 b_vec = _mm_loadu_ps(&B[i]);\n\n        __m256 diff_vec = _mm_sub_ps(a_vec, b_vec);\n        __m256 abs_diff_vec = _mm_abs_ps(diff_vec);\n\n        __m256 condition_vec = _mm_cmpgt_ps(abs_diff_vec, epsilon_vec);\n        __m256 neg_condition_vec = _mm_cmplt_ps(abs_diff_vec, epsilon_vec);\n\n        __m256 result_vec = _mm_or_ps(_mm_and_ps(condition_vec, one_vec),\n                                       _mm_and_ps(neg_condition_vec, zero_vec));\n\n        _mm_storeu_ps(&out[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    __m256i max_val = _mm_set1_epi32(INT32_MAX);\n    __m256i min_val = _mm_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256f val = _mm_loadu_ps(A + i);\n        __m256i int_val = _mm_cvtps_epi32(val);\n\n        // Saturate the values\n        int_val = _mm_max_epi32(int_val, min_val);\n        int_val = _mm_min_epi32(int_val, max_val);\n\n        _mm_storeu_si32(B + i, int_val);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    __m256i max_val = _mm_set1_epi32(INT32_MAX);\n    __m256i min_val = _mm_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256f f = _mm_loadu_ps(A + i);\n        __m256i i32 = _mm_cvtps_epi32(f);\n\n        // Saturate the values\n        i32 = _mm_min_epi32(i32, max_val);\n        i32 = _mm_max_epi32(i32, min_val);\n\n        _mm_storeu_si32(B + i, i32);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    __m256i max_val = _mm_set1_epi32(INT32_MAX);\n    __m256i min_val = _mm_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256f f = _mm_loadu_ps(A + i);\n        __m256i i32 = _mm_cvtps_epi32(f);\n\n        // Saturate the values\n        i32 = _mm_min_epi32(_mm_max_epi32(i32, min_val), max_val);\n\n        _mm_storeu_si32(B + i, i32);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    __m256i max_val = _mm_set1_epi32(INT32_MAX);\n    __m256i min_val = _mm_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256f f = _mm_loadu_ps(A + i);\n        __m256i i32 = _mm_cvtps_epi32(f);\n\n        // Saturate the values\n        i32 = _mm_min_epi32(i32, max_val);\n        i32 = _mm_max_epi32(i32, min_val);\n\n        _mm_storeu_si32(B + i, i32);\n    }\n}\n```"}
{"task_id": "SimdBench_50_AVX", "completion": "```c\nvoid float_to_int_saturate_simd(const float* A, int32_t* B, size_t size) {\n    __m256i max_val = _mm_set1_epi32(INT32_MAX);\n    __m256i min_val = _mm_set1_epi32(INT32_MIN);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256f val = _mm_loadu_ps(&A[i]);\n        __m256i int_val = _mm_cvtps_epi32(val);\n\n        // Saturate the values\n        int_val = _mm_min_epu32(int_val, max_val);\n        int_val = _mm_max_epu32(int_val, min_val);\n\n        _mm_storeu_si32(&B[i], int_val);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scaled_values;\n    __m256i temp;\n\n    for (size_t i = 0; i < size; i += 4) {\n        // Load 4 int32_t values from A into a vector\n        temp = _mm_loadu_si256((__m256i*)(&A[i]));\n\n        // Scale each value by the scale factor\n        scaled_values = _mm_mul_ps(temp, _mm_set1_ps(scale));\n\n        // Store the scaled values back into B\n        _mm_storeu_si256((__m256i*)(&B[i]), scaled_values);\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c++\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scale_vec = _mm_set1_epi8(scale); // Create a vector containing the scale factor\n    __m256i* ptr_A = (__m256i*)A; // Cast the input array pointer to __m256i*\n    __m256* ptr_B = (__m256*)B; // Cast the output array pointer to __m256*\n\n    for (size_t i = 0; i < size; i += 8) { // Process 8 elements at a time\n        __m256i vec_A = _mm_loadu_si256(ptr_A + i); // Load 8 int32_t values from A into a vector\n        __m256 vec_result = _mm_mul_ps(_mm_cvtepi32_ps(vec_A), scale_vec); // Convert int32_t to float and scale\n        _mm_storeu_si256(ptr_B + i, vec_result); // Store the result back to B\n    }\n\n    // Handle remaining elements if the size is not a multiple of 8\n    for (size_t i = 8 * ((size / 8) << 3); i < size; ++i) {\n        B[i] = static_cast<float>(A[i]) * scale;\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c++\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i scale_vec = _mm256_set1_epi8(scale); // Create a vector containing the scale factor\n\n    for (size_t i = 0; i < size; i += 8) { // Process 8 elements at a time\n        const __m256i a_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(A + i)); // Load 8 int32_t values from A into a vector\n        const __m256 scaled_a_vec = _mm256_mul_ps(a_vec, scale_vec); // Multiply each element by the scale factor\n        _mm256_storeu_ps(B + i, scaled_a_vec); // Store the scaled values back into B\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c++\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    const __m256i scale_vec = _mm_set1_epi8(scale); // Create a vector containing the scale factor\n\n    for (size_t i = 0; i < size; i += 4) { // Process 4 elements at a time\n        __m256i a_vec = _mm_loadu_si256((__m256i*)(&A[i])); // Load 4 int32_t values from A into a vector\n        __m256f b_vec = _mm_mul_ps(_mm_cvtepi32_ps(a_vec), scale_vec); // Convert int32_t values to float and scale\n        _mm_storeu_ps((float*)&B[i], b_vec); // Store the scaled float values back to B\n    }\n}\n```"}
{"task_id": "SimdBench_51_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_scaled_simd(const int32_t* A, int8_t scale, float* B, size_t size) {\n    __m256i scaled_values;\n    __m256i temp;\n\n    for (size_t i = 0; i < size; i += 4) {\n        // Load 4 int32_t values from A into a vector\n        temp = _mm_loadu_si256((__m256i*)(&A[i]));\n\n        // Scale each value by the scale factor\n        scaled_values = _mm_mul_ps(temp, _mm_set1_ps(scale));\n\n        // Store the scaled values back to B\n        _mm_storeu_si256((__m256i*)(&B[i]), scaled_values);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    __m256 a, b, c;\n    __m128i temp;\n\n    // Load data into registers\n    for (size_t i = 0; i < size; i += 4) {\n        a = _mm_loadu_ps(A + i);\n        b = _mm_loadu_ps(B + i);\n\n        // Perform division and rounding\n        c = _mm_div_ps(a, b);\n        temp = _mm_cvtps_epi32(c); // Convert to integer\n\n        // Store the result back\n        _mm_storeu_si32(C + i, temp);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    __m256 a = _mm256_loadu_ps(A);\n    __m256 b = _mm256_loadu_ps(B);\n    __m256 c;\n\n    while (size >= 8) {\n        c = _mm256_div_ps(a, b); // Perform element-wise division\n        c = _mm256_round_ps(c, _MM_FROUND_TO_NEAREST_INT); // Round to nearest integer\n        _mm256_storeu_si128((__m128i*)C, c); // Store the result\n        a = _mm256_adds_ps(a, b); // Add B to A\n        b = _mm256_setzero_ps(); // Set B to zero\n        C += 8; // Move to the next 8 elements in C\n        size -= 8;\n    }\n\n    if (size > 0) {\n        a = _mm_loadu_ps(A + size - 8);\n        b = _mm_loadu_ps(B + size - 8);\n        c = _mm256_div_ps(a, b); // Perform element-wise division\n        c = _mm256_round_ps(c, _MM_FROUND_TO_NEAREST_INT); // Round to nearest integer\n        _mm_storeu_si128((__m128i*)(C + size - 8), c); // Store the result\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    __m256 a, b, c;\n    __m128i mask;\n\n    // Load data into registers\n    for (size_t i = 0; i < size; i += 4) {\n        a = _mm_loadu_ps(&A[i]);\n        b = _mm_loadu_ps(&B[i]);\n\n        // Perform division and rounding\n        c = _mm_cvtps_epi32(_mm_div_ss(a, b));\n\n        // Store the result back to memory\n        _mm_storeu_si32(&C[i], c);\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    __m256 a, b, c;\n    __m128i mask;\n\n    // Load data into registers\n    for (size_t i = 0; i < size; i += 4) {\n        a = _mm_loadu_ps(&A[i]);\n        b = _mm_loadu_ps(&B[i]);\n\n        // Perform division and rounding\n        c = _mm_div_ps(a, b);\n        mask = _mm_cmpgt_epi32(_mm_cvtps_epi32(c), _mm_set1_epi32(0xFFFFFFFF));\n        c = _mm_sub_epi32(c, mask);\n\n        // Store the result back\n        _mm_storeu_si32(&C[i], _mm_cvtsi32_ps(c));\n    }\n}\n```"}
{"task_id": "SimdBench_52_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid float_div_round_simd(const float* A, const float* B, int32_t* C, size_t size) {\n    __m256 a, b, c;\n    __m128i mask;\n\n    // Load data into registers\n    for (size_t i = 0; i < size; i += 4) {\n        a = _mm_loadu_ps(A + i);\n        b = _mm_loadu_ps(B + i);\n\n        // Perform division and rounding\n        c = _mm_div_ps(a, b);\n        mask = _mm_cmpgt_epi32(_mm_cvtps_epi32(c), _mm_set1_epi32(0xFFFFFFFF));\n        c = _mm_sub_epi32(c, mask);\n\n        // Store the result back\n        _mm_storeu_si32(C + i, _mm_cvtsi32_ps(c));\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm_set1_pd(FLT_MAX);\n    __m256d min_val = _mm_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256d a = _mm_loadu_pd(A + i);\n        __m256d b = _mm_loadu_ps(B + i);\n\n        // Check if the double value is greater than FLT_MAX\n        __m256d greater_than_max = _mm_cmpgt_pd(a, max_val);\n        // Set those values to FLT_MAX\n        __m256d saturated_a = _mm_andnot_pd(greater_than_max, a);\n\n        // Check if the double value is less than -FLT_MAX\n        __m256d less_than_min = _mm_cmplt_pd(a, min_val);\n        // Set those values to -FLT_MAX\n        __m256d saturated_b = _mm_andnot_pd(less_than_min, b);\n\n        // Convert the double values to float\n        __m256f f_a = _mm_cvtpd_ps(saturated_a);\n        __m256f f_b = _mm_cvtpd_ps(saturated_b);\n\n        // Store the results back to the destination array\n        _mm_storeu_ps(B + i, f_a);\n        _mm_storeu_ps(B + i + 4, f_b);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm_set1_pd(FLT_MAX);\n    __m256d min_val = _mm_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256d a = _mm_loadu_pd(A + i);\n        __m256d b = _mm_loadu_ps(B + i);\n\n        // Convert double to float and saturate\n        __m256f saturated_b = _mm_cvtph_ps(_mm_cvtpd_ps(a), _mm_setzero_si128());\n        saturated_b = _mm_min_ps(saturated_b, max_val);\n        saturated_b = _mm_max_ps(saturated_b, min_val);\n\n        _mm_storeu_ps(B + i, saturated_b);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c++\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm_set1_pd(FLT_MAX);\n    __m256d min_val = _mm_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256d a = _mm_loadu_pd(A + i);\n        __m256d b = _mm_loadu_ps(B + i);\n\n        // Convert doubles to floats\n        __m256f f_a = _mm_cvtpd_ps(a);\n\n        // Saturate the floats\n        __m256f saturated_f_a = _mm_max_ps(_mm_min_ps(f_a, max_val), min_val);\n\n        // Store the saturated floats back to memory\n        _mm_storeu_ps(B + i, saturated_f_a);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm_set1_pd(FLT_MAX);\n    __m256d min_val = _mm_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256d a = _mm_loadu_pd(A + i);\n        __m256d b = _mm_loadu_ps(B + i);\n\n        // Check if the double value is greater than FLT_MAX\n        __m256d greater_than_max = _mm_cmpgt_pd(a, max_val);\n        // Check if the double value is less than -FLT_MAX\n        __m256d less_than_min = _mm_cmplt_pd(a, min_val);\n\n        // Saturate the double values\n        __m256d saturated_a = _mm_blendv_pd(a, max_val, greater_than_max);\n        saturated_a = _mm_blendv_pd(saturated_a, min_val, less_than_min);\n\n        // Convert the double values to float\n        __m256 f = _mm_cvtpd_ps(saturated_a);\n\n        // Store the float values back to the destination array\n        _mm_storeu_ps(B + i, f);\n    }\n}\n```"}
{"task_id": "SimdBench_53_AVX", "completion": "```c++\nvoid double_to_float_saturate_simd(const double* A, float* B, size_t size) {\n    __m256d max_val = _mm_set1_pd(FLT_MAX);\n    __m256d min_val = _mm_set1_pd(-FLT_MAX);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256d a = _mm_loadu_pd(A + i);\n        __m256d b = _mm_loadu_ps(B + i);\n\n        // Convert double to float\n        __m256f f = _mm_cvtpd_ps(a);\n\n        // Saturate the float values\n        __m256f saturated_f = _mm_min_ps(_mm_max_ps(f, min_val), max_val);\n\n        // Store the saturated float values back to B\n        _mm_storeu_ps(B + i, saturated_f);\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n\n    for (size_t i = 0; i < size / 4; ++i) {\n        __m256i vec_A = _mm256_loadu_si256(avx_A + i);\n        __m256i vec_B = _mm256_cvtepu16_epi32(vec_A);\n        _mm256_storeu_si256(avx_B + i, vec_B);\n    }\n\n    // Handle remaining elements if size is not divisible by 4\n    for (size_t i = size % 4; i > 0; --i) {\n        *B++ = (int32_t)*A++;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n\n    for (size_t i = 0; i < size / 4; ++i) {\n        __m256i vec_A = _mm256_loadu_si256(avx_A + i);\n        __m256i vec_B = _mm256_cvtepi16_epi32(vec_A);\n        _mm256_storeu_si256(avx_B + i, vec_B);\n    }\n\n    // Handle remaining elements if size is not a multiple of 4\n    for (size_t i = size % 4; i > 0; --i) {\n        *B++ = (int32_t)*A++;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n\n    for (size_t i = 0; i < size / 4; ++i) {\n        __m256i vec_A = _mm256_loadu_si256(avx_A + i);\n        __m256i vec_B = _mm256_cvtepu16_epi32(vec_A);\n        _mm256_storeu_si256(avx_B + i, vec_B);\n    }\n\n    // Handle remaining elements if size is not divisible by 4\n    for (size_t i = size % 4; i > 0; --i) {\n        *avx_B++ = (int32_t)*avx_A++;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n\n    for (size_t i = 0; i < size / 4; ++i) {\n        __m256i vec_A = _mm256_loadu_si256(avx_A + i);\n        __m256i vec_B = _mm256_cvtepu16_epi32(vec_A);\n        _mm256_storeu_si256(avx_B + i, vec_B);\n    }\n\n    // Handle remaining elements if size is not divisible by 4\n    for (size_t i = size % 4; i > 0; --i) {\n        *B++ = (int32_t)*A++;\n    }\n}\n```"}
{"task_id": "SimdBench_54_AVX", "completion": "```c\nvoid int16_to_int32_simd(const int16_t* A, int32_t* B, size_t size) {\n    __m256i* avx_A = (__m256i*)A;\n    __m256i* avx_B = (__m256i*)B;\n\n    for (size_t i = 0; i < size / 4; ++i) {\n        __m256i vec_A = _mm256_loadu_si256(avx_A + i);\n        __m256i vec_B = _mm256_cvtepi16_epi32(vec_A);\n        _mm256_storeu_si256(avx_B + i, vec_B);\n    }\n\n    // Handle remaining elements if size is not divisible by 4\n    for (size_t i = size % 4; i > 0; --i) {\n        *B++ = (int32_t)*A++;\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c++\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm_set_ps(min_val, min_val, min_val, min_val);\n    __m256 max_val_vec = _mm_set_ps(max_val, max_val, max_val, max_val);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 c_vec = _mm_loadu_si32(&control[i]);\n\n        __m256 mask = _mm_cmpgt_epi32(c_vec, _mm_setzero_si32());\n        __m256 normalized_a_vec = _mm_andnot_ps(mask, _mm_sub_ps(a_vec, min_val_vec));\n        normalized_a_vec = _mm_or_ps(normalized_a_vec, _mm_max_ps(_mm_setzero_ps(), _mm_min_ps(normalized_a_vec, _mm_sub_ps(max_val_vec, a_vec))));\n\n        _mm_storeu_ps(&B[i], normalized_a_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c++\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm_set_ps(min_val, min_val, min_val, min_val);\n    __m256 max_val_vec = _mm_set_ps(max_val, max_val, max_val, max_val);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 c_vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&control[i]));\n\n        __m256 mask = _mm_cmpgt_epi32(c_vec, _mm_setzero_si256());\n        __m256 neg_mask = _mm_andnot_si256(mask, _mm_setzero_si256());\n\n        __m256 min_diff_vec = _mm_sub_ps(a_vec, min_val_vec);\n        __m256 max_diff_vec = _mm_sub_ps(a_vec, max_val_vec);\n\n        __m256 norm_min_diff_vec = _mm_div_ps(min_diff_vec, max_diff_vec);\n        __m256 norm_max_diff_vec = _mm_div_ps(max_diff_vec, min_diff_vec);\n\n        __m256 norm_a_vec = _mm_or_ps(norm_min_diff_vec, norm_max_diff_vec);\n\n        __m256 zero_mask = _mm_cmplt_ps(norm_a_vec, _mm_setzero_ps());\n        __m256 one_mask = _mm_cmpgt_ps(norm_a_vec, _mm_setone_ps());\n\n        __m256 zero_norm_a_vec = _mm_andnot_si256(zero_mask, norm_a_vec);\n        __m256 one_norm_a_vec = _mm_and_si256(one_mask, norm_a_vec);\n\n        __m256 zero_result_vec = _mm_andnot_si256(zero_mask, _mm_setzero_ps());\n        __m256 one_result_vec = _mm_and_si256(one_mask, _mm_setone_ps());\n\n        __m256 result_vec = _mm_or_ps(zero_result_vec, one_result_vec);\n\n        __m256 neg_result_vec = _mm_andnot_si256(neg_mask, result_vec);\n\n        _mm_storeu_ps(&B[i], neg_result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c++\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm_set_ps(min_val, min_val, min_val, min_val);\n    __m256 max_val_vec = _mm_set_ps(max_val, max_val, max_val, max_val);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 c_vec = _mm_loadu_si32(&control[i]);\n\n        __m256 mask = _mm_cmpgt_epi32(c_vec, _mm_setzero_si32());\n        __m256 normalized_a_vec = _mm_andnot_ps(mask, _mm_sub_ps(a_vec, min_val_vec));\n        normalized_a_vec = _mm_div_ps(normalized_a_vec, max_val_vec);\n\n        _mm_storeu_ps(&B[i], normalized_a_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm_set_ps(min_val, min_val, min_val, min_val);\n    __m256 max_val_vec = _mm_set_ps(max_val, max_val, max_val, max_val);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 c_vec = _mm_loadu_si32(&control[i]);\n\n        __m256 mask = _mm_cmpgt_epi32(c_vec, _mm_setzero_si32());\n        __m256 norm_a_vec = _mm_sub_ps(a_vec, min_val_vec);\n        __m256 div_a_vec = _mm_sub_ps(max_val_vec, min_val_vec);\n        __m256 result_vec = _mm_div_ps(norm_a_vec, div_a_vec);\n\n        __m256 zero_mask = _mm_cmplt_ps(result_vec, _mm_setzero_ps());\n        __m256 one_mask = _mm_cmpgt_ps(result_vec, _mm_setone_ps());\n\n        result_vec = _mm_or_ps(_mm_and_ps(zero_mask, _mm_setzero_ps()), _mm_and_ps(one_mask, _mm_setone_ps()));\n        result_vec = _mm_or_ps(result_vec, _mm_andnot_ps(mask, result_vec));\n\n        _mm_storeu_ps(&B[i], result_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_55_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid conditional_normalize_simd(const float* A, const int32_t* control, float* B, size_t size, float min_val, float max_val) {\n    __m256 min_val_vec = _mm_set_ps(min_val, min_val, min_val, min_val);\n    __m256 max_val_vec = _mm_set_ps(max_val, max_val, max_val, max_val);\n\n    for (size_t i = 0; i < size; i += 4) {\n        __m256 a_vec = _mm_loadu_ps(&A[i]);\n        __m256 c_vec = _mm_loadu_si32(&control[i]);\n\n        __m256 mask = _mm_cmpgt_epi32(c_vec, _mm_setzero_si32());\n        __m256 normalized_a_vec = _mm_andnot_ps(mask, _mm_sub_ps(a_vec, min_val_vec));\n        normalized_a_vec = _mm_div_ps(normalized_a_vec, max_val_vec);\n\n        _mm_storeu_ps(&B[i], normalized_a_vec);\n    }\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i v_A = _mm_loadu_si256((__m256i*)A);\n    __m256i v_high_16bits = _mm_srli_epi16(v_A, 16); // Extract high 16 bits\n    __m256 v_float = _mm_cvtusi2ss(_mm_castsi256_ps(v_high_16bits)); // Convert to float\n    __m256 v_sqrt = _mm_sqrt_ss(v_float); // Take square root\n    _mm_storeu_ps(B, v_sqrt); // Store result\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i v_A = _mm_loadu_si256((__m256i*)A);\n    __m256 v_B = _mm_setzero_ps();\n    \n    // Convert high 16 bits of each uint32_t value to float\n    __m256 v_high_bits = _mm_srlv_epi16(v_A, _mm_set1_epi16(0xFFFF));\n    v_high_bits = _mm_cvtepi32_ps(v_high_bits);\n    \n    // Take the square root of the float value\n    v_B = _mm_sqrt_ps(v_high_bits);\n    \n    // Store the result in B\n    _mm_storeu_ps(B, v_B);\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i v_A = _mm_loadu_si256((__m256i*)A);\n    __m256i v_high_bits = _mm_srli_epi16(v_A, 16);\n    __m256 v_float = _mm_cvtusi2ss(_mm_packus_epi16(v_high_bits, v_high_bits));\n    __m256 v_sqrt = _mm_sqrt_ps(v_float);\n    _mm_storeu_ps(B, v_sqrt);\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i v_A = _mm256_loadu_si256((__m256i*)A);\n    __m256i v_high_bits = _mm256_srli_epi16(v_A, 16); // Extract high 16 bits\n    __m256 v_float = _mm256_cvtepi32_ps(v_high_bits); // Convert to float\n    __m256 v_sqrt = _mm256_sqrt_ps(v_float); // Take square root\n    _mm256_storeu_ps(B, v_sqrt); // Store result\n}\n```"}
{"task_id": "SimdBench_56_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_to_float_sqrt_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i v_A = _mm_loadu_si256((__m256i*)A);\n    __m256 v_B = _mm_setzero_ps();\n\n    // Convert high 16 bits of each uint32_t to float\n    __m256i v_high_bits = _mm_srli_epi16(v_A, 16);\n    __m256 v_floats = _mm_cvtepi32_ps(v_high_bits);\n\n    // Take the square root of the float values\n    v_floats = _mm_sqrt_ps(v_floats);\n\n    // Store the results in B\n    _mm_storeu_ps(B, v_floats);\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i vA;\n    __m256 vB;\n\n    // Load 32-bit values from A into vA\n    vA = _mm256_loadu_si256((__m256i*)A);\n\n    // Convert each 32-bit value in vA to a float in vB\n    vB = _mm256_cvtepi32_ps(vA);\n\n    // Store the converted floats in B\n    _mm256_storeu_ps(B, vB);\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i vA;\n    __m256 vB;\n\n    for (size_t i = 0; i < size; i += 8) {\n        vA = _mm256_loadu_si256((__m256i*)(&A[i]));\n        vB = _mm256_cvtepi32_ps(vA); // Convert uint32_t to float using AVX2 intrinsic\n        _mm256_storeu_ps((__m128*)(&B[i]), vB); // Store the result back to B\n    }\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i vA = _mm256_loadu_si256((__m256i*)A); // Load 8 uint32_t values into a vector\n    __m256 vF = _mm256_cvtph_ps(vA); // Convert each uint32_t to a float\n    _mm256_storeu_ps(B, vF); // Store the converted floats into the destination array\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i vA = _mm256_loadu_si256((__m256i*)A);\n    __m256 vB = _mm256_setzero_ps();\n    \n    while (size >= 8) {\n        vB = _mm256_cvtph_ps(vA, vB);\n        vA = _mm256_loadu_si256((__m256i*)(A + 8));\n        size -= 8;\n    }\n    \n    if (size > 0) {\n        __m128 vA = _mm_loadu_si128((__m128i*)A);\n        __m128 vB = _mm256_castps_ps128(vB);\n        \n        while (size >= 4) {\n            vB = _mm_cvtph_ps(vA, vB);\n            vA = _mm_loadu_si128((__m128i*)(A + 4));\n            size -= 4;\n        }\n        \n        if (size > 0) {\n            __m8 vA = _mm_loadu_si8(A);\n            __m8 vB = _mm256_castps_ps8(vB);\n            \n            while (size >= 2) {\n                vB = _mm_cvtph_ps(vA, vB);\n                vA = _mm_loadu_si8((const char*)(A + 2));\n                size -= 2;\n            }\n            \n            if (size == 1) {\n                vB = _mm_cvtph_ps(_mm_loadu_si8((const char*)(A)), vB);\n            }\n        }\n    }\n    \n    _mm_storeu_ps(B, vB);\n}\n```"}
{"task_id": "SimdBench_57_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nvoid int_bits_to_float_simd(const uint32_t* A, float* B, size_t size) {\n    __m256i vA;\n    __m256 vB;\n\n    // Load the first 256 bits from A into vA\n    vA = _mm256_loadu_si256((__m256i*)A);\n\n    // Convert the first 256 bits of vA to floats and store them in vB\n    vB = _mm256_cvtepi32_ps(vA);\n\n    // Store the first 256 bits of vB into B\n    _mm256_storeu_ps(B, vB);\n\n    // Process the remaining elements in chunks of 256 bits\n    for (size_t i = 256; i < size; i += 256) {\n        // Load the next 256 bits from A into vA\n        vA = _mm256_loadu_si256((__m256i*)(A + i));\n\n        // Convert the next 256 bits of vA to floats and store them in vB\n        vB = _mm256_cvtepi32_ps(vA);\n\n        // Store the next 256 bits of vB into B\n        _mm256_storeu_ps(B + i, vB);\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i prev = _mm256_loadu_si256((__m256i*)A);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i result = _mm256_sub_epi32(current, prev);\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(result, _mm256_setzero_si256())) == 0xFF) {\n            // If all elements are zero, set diff[i] to 0.0f\n            _mm256_storeu_ps(diff + i, _mm256_setzero_ps());\n        } else {\n            // Otherwise, convert to float and store\n            _mm256_storeu_ps(diff + i, _mm256_cvtepi32_ps(result));\n        }\n\n        prev = current;\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i prev = _mm256_loadu_si256((__m256i*)A);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i result = _mm256_sub_epi32(current, prev);\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(result, _mm256_setzero_si256())) != 0) {\n            result = _mm256_and_ps(result, _mm256_castsi256_ps(mask));\n        }\n\n        _mm256_storeu_si256((__m256i*)(diff + i), result);\n        prev = current;\n    }\n\n    // Handle remaining elements\n    for (size_t i = size % 8; i > 0; --i) {\n        int32_t current = A[i];\n        if (!cond[i]) {\n            diff[i] = 0.0f;\n        } else {\n            diff[i] = (float)(current - A[i - 1]);\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i prev = _mm256_loadu_si256((__m256i*)A);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i diff_mask = _mm256_and_si256(mask, cond + i);\n        __m256i diff_value = _mm256_sub_epi32(current, prev);\n        __m256 diff_float = _mm256_cvtepi32_ps(diff_value);\n        __m256 zero_mask = _mm256_cmpneq_ps(diff_float, _mm256_setzero_ps());\n        __m256 result = _mm256_blendv_ps(diff_float, _mm256_setzero_ps(), zero_mask);\n        _mm256_storeu_si256((__m256i*)(diff + i), result);\n\n        prev = current;\n        mask = _mm256_slli_epi32(mask, 8);\n    }\n\n    // Handle remaining elements\n    for (size_t i = size % 8; i > 0; --i) {\n        int32_t current = A[size - i];\n        if (cond[size - i]) {\n            diff[size - i] = static_cast<float>(current - A[size - i - 1]);\n        } else {\n            diff[size - i] = 0.0f;\n        }\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i prev = _mm256_loadu_si256((__m256i*)A);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i result = _mm256_sub_epi32(current, prev);\n\n        if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(result, _mm256_setzero_si256())) != 0) {\n            result = _mm256_and_ps(result, _mm256_castsi256_ps(mask));\n        }\n\n        _mm256_storeu_si256((__m256i*)(diff + i), result);\n        prev = current;\n    }\n}\n```"}
{"task_id": "SimdBench_58_AVX", "completion": "```c\nvoid conditional_diff_simd(const int32_t* A, const bool* cond, float* diff, size_t size) {\n    __m256i mask = _mm256_setzero_si256();\n    __m256i prev = _mm256_loadu_si256((__m256i*)A);\n\n    for (size_t i = 0; i < size; i += 8) {\n        __m256i current = _mm256_loadu_si256((__m256i*)(A + i));\n        __m256i diff_mask = _mm256_cmpgt_epi32(current, prev);\n        __m256i result = _mm256_sub_ps(_mm256_castsi256_ps(diff_mask), _mm256_castsi256_ps(prev));\n\n        if (i == 0) {\n            result = _mm256_setzero_ps(result);\n        }\n\n        _mm256_storeu_si256((__m256i*)(diff + i), result);\n        prev = current;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m256i* src_vec = (__m256i*)src;\n    __m256i* dst_vec = (__m256i*)dst;\n\n    for (size_t i = 0; i < length / 8; ++i) {\n        __m256i vec = _mm256_loadu_si256(src_vec + i);\n        __m256i result = _mm256_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF,\n                                             0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n        result = _mm256_or_si256(result, vec);\n        _mm256_storeu_si256(dst_vec + i, result);\n    }\n\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i > 0; --i) {\n        *dst_vec++ = (*src_vec++) | 0xFFFFFFFF;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m256i* src_vec = (__m256i*)src;\n    __m256i* dst_vec = (__m256i*)dst;\n\n    for (size_t i = 0; i < length / 8; ++i) {\n        __m256i vec = _mm256_loadu_si256(src_vec + i);\n        __m256i result = _mm256_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF,\n                                             0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n        vec = _mm256_or_si256(vec, result);\n        _mm256_storeu_si256(dst_vec + i, vec);\n    }\n\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i < length; ++i) {\n        *dst++ = (*src++) | 0xFFFFFFFF;\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m256i v_src = _mm256_loadu_si256((__m256i*)src);\n    __m256i v_dst = _mm256_setzero_si256();\n    __m256i v_mask = _mm256_set1_epi32(0xFFFFFFFF);\n\n    while (length >= 32) {\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n        v_src = _mm256_add_epi32(v_src, _mm256_slli_epi32(v_src, 32));\n        v_dst = _mm256_or_si256(v_dst, _mm256_and_si256(_mm256_slli_epi32(v_src, 32), v_mask));\n"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m256i v_src = _mm256_loadu_si256((__m256i*)src);\n    __m256i v_dst = _mm256_setzero_si256();\n    \n    // Shift left by 32 bits to set high 32 bits to 1\n    v_dst = _mm256_sllv_epi32(v_dst, _mm256_set1_epi32(0xFFFFFFFF));\n    \n    // Store the result back to dst\n    _mm256_storeu_si256((__m256i*)dst, v_dst);\n    \n    // Process remaining elements if necessary\n    for (size_t i = 8; i < length; i += 8) {\n        v_src = _mm256_loadu_si256((__m256i*)(src + i));\n        v_dst = _mm256_setzero_si256();\n        \n        v_dst = _mm256_sllv_epi32(v_dst, _mm256_set1_epi32(0xFFFFFFFF));\n        \n        _mm256_storeu_si256((__m256i*)(dst + i), v_dst);\n    }\n}\n```"}
{"task_id": "SimdBench_59_AVX", "completion": "```c\nvoid widening_uint_simd(const uint32_t* src, uint64_t* dst, size_t length) {\n    __m256i vec_src = _mm256_loadu_si256((__m256i*)src);\n    __m256i vec_dst = _mm256_setzero_si256();\n    \n    while (length >= 8) {\n        vec_dst = _mm256_or_si256(vec_dst, _mm256_srlv_epi32(vec_src, _mm256_set1_epi32(0xFFFFFFFF)));\n        vec_src = _mm256_add_epi32(vec_src, _mm256_set1_epi32(1));\n        length -= 8;\n    }\n    \n    if (length > 0) {\n        __m128i vec_src128 = _mm_loadu_si128((__m128i*)&src[length - 8]);\n        __m128i vec_dst128 = _mm_setzero_si128();\n        \n        vec_dst128 = _mm_or_si128(vec_dst128, _mm_srlv_epi32(vec_src128, _mm_set1_epi32(0xFFFFFFFF)));\n        vec_dst128 = _mm_add_epi32(vec_dst128, _mm_set1_epi32(1));\n        \n        _mm_storeu_si128((__m128i*)dst + length - 8, vec_dst128);\n    }\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c++\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i < length; i += 8) {\n        __m256d v = _mm256_loadu_pd(&vec[index[i]]);\n        sum = _mm256_add_pd(sum, v);\n    }\n    // Handle remaining elements if length is not a multiple of 8\n    for (size_t i = length % 8; i < length; ++i) {\n        sum = _mm_add_sd(sum, _mm_load_sd(&vec[index[i]]));\n    }\n    return _mm256_extractf64x2_pd(sum, 0);\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c++\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i < length; i += 8) {\n        __m256d idx = _mm256_loadu_si256((__m256i*)(&index[i]));\n        __m256d val = _mm256_loadu_pd((__m256d*)(&vec[idx[0]]));\n        if (i + 4 < length) {\n            val = _mm256_add_pd(val, _mm256_loadu_pd((__m256d*)(&vec[idx[1]])));\n        }\n        if (i + 8 < length) {\n            val = _mm256_add_pd(val, _mm256_loadu_pd((__m256d*)(&vec[idx[2]])));\n        }\n        if (i + 12 < length) {\n            val = _mm256_add_pd(val, _mm256_loadu_pd((__m256d*)(&vec[idx[3]])));\n        }\n        sum = _mm256_add_pd(sum, val);\n    }\n    double result = _mm256_reduce_add_pd(sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    size_t i;\n\n    for (i = 0; i < length; i += 32) {\n        __m256d v = _mm256_loadu_pd(&vec[index[i]]);\n        __m256d v2 = _mm256_loadu_pd(&vec[index[i + 1]]);\n        __m256d v3 = _mm256_loadu_pd(&vec[index[i + 2]]);\n        __m256d v4 = _mm256_loadu_pd(&vec[index[i + 3]]);\n        __m256d v5 = _mm256_loadu_pd(&vec[index[i + 4]]);\n        __m256d v6 = _mm256_loadu_pd(&vec[index[i + 5]]);\n        __m256d v7 = _mm256_loadu_pd(&vec[index[i + 6]]);\n        __m256d v8 = _mm256_loadu_pd(&vec[index[i + 7]]);\n        __m256d v9 = _mm256_loadu_pd(&vec[index[i + 8]]);\n        __m256d v10 = _mm256_loadu_pd(&vec[index[i + 9]]);\n        __m256d v11 = _mm256_loadu_pd(&vec[index[i + 10]]);\n        __m256d v12 = _mm256_loadu_pd(&vec[index[i + 11]]);\n        __m256d v13 = _mm256_loadu_pd(&vec[index[i + 12]]);\n        __m256d v14 = _mm256_loadu_pd(&vec[index[i + 13]]);\n        __m256d v15 = _mm256_loadu_pd(&vec[index[i + 14]]);\n        __m256d v16 = _mm256_loadu_pd(&vec[index[i + 15]]);\n        __m256d v17 = _mm256_loadu_pd(&vec[index[i + 16]]);\n        __m256d v18 = _mm256_loadu_pd(&vec[index[i + 17]]);\n        __m256d v19 = _mm256_loadu_pd(&vec[index[i + 18]]);\n        __m256d v20 = _mm256_loadu_pd(&vec[index[i + 19]]);\n        __m256d v21 = _mm256_loadu_pd(&vec[index[i + 20]]);\n        __m256d v22 = _mm256_loadu_pd(&vec[index[i + 21]]);\n        __m256d v23 = _mm256_loadu_pd(&vec[index[i + 22]]);\n        __m256d v24 = _mm256_loadu_pd(&vec[index[i + 23]]);\n        __m256d v25 = _mm256_loadu_pd(&vec[index[i + 24]]);\n        __m256d v26 = _mm256_loadu_pd(&vec[index[i + 25]]);\n        __m256d v27 = _mm256_loadu_pd(&vec[index[i + 26]]);\n        __m256d v28 = _mm256_loadu_pd(&vec[index[i + 27]]);\n        __m256d v29 = _mm256_loadu_pd(&vec[index[i + 28]]);\n        __m256d v30 = _mm256_loadu_pd(&vec[index[i + 29]]);\n        __m256d v31 = _mm256_loadu_pd(&vec[index[i + 30]]);\n        __m256d v32 = _mm256_loadu_pd(&vec[index[i + 31]]);\n\n        sum = _mm256_add_pd(sum, v);\n        sum = _mm256_add_pd(sum, v2);\n        sum = _mm256_add_pd(sum, v3);\n        sum = _mm256_add_pd(sum, v4);\n        sum = _mm256_add_pd(sum, v5);\n        sum = _mm256_add_pd(sum, v6);\n        sum = _mm256_add_pd(sum, v7);\n        sum = _mm256_add_pd(sum, v8);\n        sum = _mm256_add_pd(sum, v9);\n        sum = _mm256_add_pd(sum, v10);\n        sum = _mm256_add_pd(sum, v11);\n        sum = _mm256_add_pd(sum, v12);\n        sum = _mm256_add_pd(sum, v13);\n        sum = _mm256_add_pd(sum, v14);\n        sum = _mm256_add_pd(sum, v15);\n        sum = _mm256_add_pd(sum, v16);\n        sum = _mm256_add_pd(sum, v17);\n        sum = _mm256_add_pd(sum, v18);\n        sum = _mm256_add_pd(sum, v19);\n        sum = _mm256_add_pd(sum, v20);\n        sum = _mm256_add_pd(sum, v21);\n        sum = _mm256_add_pd(sum, v22);\n        sum = _mm256_add_pd(sum, v23);\n        sum = _mm256_add_pd(sum, v24);\n        sum = _mm256_add_pd(sum, v25);\n        sum = _mm256_add_pd(sum, v26);\n        sum = _mm256_add_pd(sum, v27);\n        sum = _mm256_add_pd(sum, v28);\n        sum = _mm256_add_pd(sum, v29);\n        sum = _mm256_add_pd(sum, v30);\n        sum = _mm256_add_pd(sum, v31);\n    }\n\n    // Handle remaining elements if length is not divisible by 32\n    for (; i < length; ++i) {\n        sum = _mm256_add_pd(sum, _mm256_loadu_pd(&vec[index[i]]));\n    }\n\n    double result;\n    _mm256_storeu_pd(&result, sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c++\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    __m128i mask = _mm_set_epi32(0, 1, 2, 3);\n\n    for (size_t i = 0; i < length; i += 4) {\n        __m256d v = _mm256_loadu_pd(&vec[index[i]]);\n        __m256i idx = _mm256_loadu_si256((__m256i*)&index[i]);\n        __m256i mask_idx = _mm256_and_si256(idx, mask);\n        __m256d selected_v = _mm256_maskload_pd(v, mask_idx);\n        sum = _mm256_add_pd(sum, selected_v);\n    }\n\n    for (size_t i = 4; i < length; ++i) {\n        sum = _mm_add_pd(sum, _mm_loadu_pd(&vec[index[i]]));\n    }\n\n    return _mm256_extractf64x4_pd(sum, 0);\n}\n```"}
{"task_id": "SimdBench_60_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\ndouble indexed_sum_simd(const double* vec, const int16_t* index, size_t length) {\n    __m256d sum = _mm256_setzero_pd();\n    for (size_t i = 0; i < length; i += 32) {\n        __m256d v = _mm256_loadu_pd(&vec[index[i]]);\n        __m256d w = _mm256_loadu_pd(&vec[index[i + 1]]);\n        __m256d x = _mm256_loadu_pd(&vec[index[i + 2]]);\n        __m256d y = _mm256_loadu_pd(&vec[index[i + 3]]);\n        __m256d z = _mm256_loadu_pd(&vec[index[i + 4]]);\n        __m256d a = _mm256_loadu_pd(&vec[index[i + 5]]);\n        __m256d b = _mm256_loadu_pd(&vec[index[i + 6]]);\n        __m256d c = _mm256_loadu_pd(&vec[index[i + 7]]);\n        __m256d d = _mm256_loadu_pd(&vec[index[i + 8]]);\n        __m256d e = _mm256_loadu_pd(&vec[index[i + 9]]);\n        __m256d f = _mm256_loadu_pd(&vec[index[i + 10]]);\n        __m256d g = _mm256_loadu_pd(&vec[index[i + 11]]);\n        __m256d h = _mm256_loadu_pd(&vec[index[i + 12]]);\n        __m256d i = _mm256_loadu_pd(&vec[index[i + 13]]);\n        __m256d j = _mm256_loadu_pd(&vec[index[i + 14]]);\n        __m256d k = _mm256_loadu_pd(&vec[index[i + 15]]);\n        sum = _mm256_add_pd(sum, _mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(v, w), x), y), z), a));\n        sum = _mm256_add_pd(sum, _mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(b, c), d), e), f), g)));\n        sum = _mm256_add_pd(sum, _mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(_mm256_add_pd(h, i), j), k), l))));\n    }\n    for (; i < length; ++i) {\n        sum = _mm256_add_pd(sum, _mm256_loadu_pd(&vec[index[i]]));\n    }\n    double result;\n    _mm256_storeu_pd(&result, sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c++\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    // Load the first row of the input into an AVX register\n    __m256i input_row = _mm256_loadu_si256((__m256i*)input);\n\n    // Load the first row of the kernel into an AVX register\n    __m256i kernel_row = _mm256_loadu_si256((__m256i*)kernel);\n\n    // Perform element-wise multiplication and addition\n    __m256i result = _mm256_mul_epi32(input_row, kernel_row);\n    result = _mm256_add_epi32(result, _mm256_setzero_si256());\n\n    // Store the result back to the output array\n    _mm256_storeu_si256((__m256i*)output, result);\n\n    // Increment the pointers to the next rows\n    input += input_size;\n    kernel += kernel_size;\n    output += input_size - kernel_size + 1;\n\n    // Repeat the process until all rows have been processed\n    while (input < input + input_size) {\n        // Load the next row of the input into an AVX register\n        input_row = _mm256_loadu_si256((__m256i*)input);\n\n        // Load the next row of the kernel into an AVX register\n        kernel_row = _mm256_loadu_si256((__m256i*)kernel);\n\n        // Perform element-wise multiplication and addition\n        result = _mm256_mul_epi32(input_row, kernel_row);\n        result = _mm256_add_epi32(result, _mm256_setzero_si256());\n\n        // Store the result back to the output array\n        _mm256_storeu_si256((__m256i*)output, result);\n\n        // Increment the pointers to the next rows\n        input += input_size;\n        kernel += kernel_size;\n        output += input_size - kernel_size + 1;\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    // Load the first row of the input into registers\n    __m256i input_row = _mm256_loadu_si256((__m256i*)input);\n\n    // Iterate over each column of the output\n    for (size_t col = 0; col < input_size - kernel_size + 1; ++col) {\n        // Load the current column of the output into a register\n        __m256i output_col = _mm256_setzero_si256();\n\n        // Iterate over each row of the kernel\n        for (size_t row = 0; row < kernel_size; ++row) {\n            // Load the next row of the input into a register\n            __m256i input_next_row = _mm256_loadu_si256((__m256i*)(input + col + row));\n\n            // Perform element-wise multiplication between the current row of the input and the corresponding row of the kernel\n            __m256 mul_result = _mm256_mul_ps(input_row, input_next_row);\n\n            // Add the result to the current column of the output\n            output_col = _mm256_add_ps(output_col, mul_result);\n        }\n\n        // Store the updated column of the output back to memory\n        _mm256_storeu_si256((__m256i*)(output + col), output_col);\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    // Calculate the number of elements in the input and output arrays\n    size_t num_elements = input_size * input_size;\n    \n    // Initialize the output array with zeros\n    memset(output, 0, num_elements * sizeof(double));\n    \n    // Load the first element of the input array into a register\n    __m256i input_vec = _mm256_loadu_si256((__m256i*)input);\n    \n    // Loop through each row of the input array\n    for (size_t i = 0; i < input_size - kernel_size + 1; ++i) {\n        // Load the first element of the kernel array into a register\n        __m256i kernel_vec = _mm256_loadu_si256((__m256i*)kernel);\n        \n        // Perform the convolution operation on the current row of the input array\n        __m256 conv_result = _mm256_mul_ps(input_vec, kernel_vec);\n        \n        // Store the result in the corresponding position in the output array\n        _mm256_storeu_si256((__m256i*)(output + i), conv_result);\n        \n        // Load the next element of the input array into a register\n        input_vec = _mm256_add_epi32(input_vec, _mm256_set1_epi32(kernel_size));\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    // Load the first row of the input into an AVX register\n    __m256i input_row = _mm256_loadu_si256((__m256i*)input);\n\n    // Load the first row of the kernel into an AVX register\n    __m256i kernel_row = _mm256_loadu_si256((__m256i*)kernel);\n\n    // Perform element-wise multiplication between the input row and the kernel row\n    __m256i product = _mm256_mul_epi32(input_row, kernel_row);\n\n    // Sum up the products across all elements in the row\n    __m256 sum = _mm256_hadd_epi32(product, product);\n\n    // Store the result back to the output array\n    _mm256_storeu_si256((__m256i*)output, sum);\n\n    // Increment the pointers to the next rows\n    input += input_size;\n    kernel += kernel_size;\n\n    // Continue processing until all rows have been processed\n    while (input < input_end && kernel < kernel_end) {\n        // Load the next row of the input into an AVX register\n        input_row = _mm256_loadu_si256((__m256i*)input);\n\n        // Load the next row of the kernel into an AVX register\n        kernel_row = _mm256_loadu_si256((__m256i*)kernel);\n\n        // Perform element-wise multiplication between the input row and the kernel row\n        product = _mm256_mul_epi32(input_row, kernel_row);\n\n        // Sum up the products across all elements in the row\n        sum = _mm256_hadd_epi32(product, product);\n\n        // Store the result back to the output array\n        _mm256_storeu_si256((__m256i*)output, sum);\n\n        // Increment the pointers to the next rows\n        input += input_size;\n        kernel += kernel_size;\n    }\n}\n```"}
{"task_id": "SimdBench_61_AVX", "completion": "```c\nvoid simple_conv2d_simd(const double* input, const double* kernel, double * output, size_t input_size, size_t kernel_size) {\n    // Load the first row of the input into an AVX register\n    __m256i input_row = _mm256_loadu_si256((__m256i*)input);\n\n    // Iterate over each column of the output\n    for (size_t col = 0; col < input_size - kernel_size + 1; ++col) {\n        // Load the current column of the output into an AVX register\n        __m256i output_col = _mm256_setzero_si256();\n\n        // Iterate over each row of the kernel\n        for (size_t row = 0; row < kernel_size; ++row) {\n            // Load the current element of the kernel into an AVX register\n            __m256i kernel_elem = _mm256_loadu_si256((__m256i*)(kernel + row));\n\n            // Perform the dot product between the current row of the input and the current element of the kernel\n            __m256i dot_product = _mm256_mul_epi32(input_row, kernel_elem);\n\n            // Add the result to the current column of the output\n            output_col = _mm256_add_epi32(output_col, dot_product);\n        }\n\n        // Store the current column of the output back into memory\n        _mm256_storeu_si256((__m256i*)(output + col), output_col);\n    }\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int num_elements = numbers.size();\n    __m256d threshold_vec = _mm_set_pd(threshold);\n\n    // Load the first four elements into a register\n    __m256d vec1 = _mm_loadu_pd(numbers.data());\n\n    // Compare each element with the first four elements\n    __m256d diff = _mm_sub_pd(vec1, threshold_vec);\n    __m256d mask = _mm_cmpgt_pd(diff, _mm_setzero_pd());\n    __m256d result = _mm_movemask_ps(mask);\n\n    // Check if any bit is set in the mask\n    return _mm_test_all_ones(result);\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int num_elements = numbers.size();\n    __m256d threshold_vec = _mm_set_pd(threshold);\n\n    for (int i = 0; i + 7 < num_elements; i += 8) {\n        __m256d vec1 = _mm_loadu_pd(numbers.data() + i);\n        __m256d vec2 = _mm_loadu_pd(numbers.data() + i + 4);\n\n        __m256d diff = _mm_sub_pd(vec1, vec2);\n        __m256d abs_diff = _mm_abs_pd(diff);\n\n        __m256d mask = _mm_cmpgt_pd(abs_diff, threshold_vec);\n        __m256d result = _mm_movemask_ps(mask);\n\n        if (_mm_test_all_ones(result)) {\n            return true;\n        }\n    }\n\n    for (int i = 0; i < num_elements - 7; ++i) {\n        double diff = std::abs(numbers[i] - numbers[i + 7]);\n        if (diff < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int num_elements = numbers.size();\n    __m256d threshold_vec = _mm_set_pd(threshold);\n\n    // Create a mask to mark elements that are close to each other\n    __m256d mask = _mm_setzero_pd();\n\n    for (int i = 0; i + 7 < num_elements; i += 8) {\n        __m256d vec1 = _mm_loadu_pd(numbers.data() + i);\n        __m256d vec2 = _mm_loadu_pd(numbers.data() + i + 4);\n\n        // Calculate the absolute difference between pairs of elements\n        __m256d diff = _mm_sub_pd(vec1, vec2);\n\n        // Compare the differences with the threshold\n        __m256d abs_diff = _mm_abs_pd(diff);\n        __m256d cmp_result = _mm_cmpgt_pd(abs_diff, threshold_vec);\n\n        // Update the mask with the result of the comparison\n        mask = _mm_or_pd(mask, cmp_result);\n    }\n\n    // Check if there are any set bits in the mask\n    return _mm_movemask_pd(mask) != 0;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int num_elements = numbers.size();\n    __m256d threshold_vec = _mm_set_pd(threshold);\n\n    for (int i = 0; i + 7 < num_elements; i += 8) {\n        __m256d vec1 = _mm_loadu_pd(numbers.data() + i);\n        __m256d vec2 = _mm_loadu_pd(numbers.data() + i + 4);\n\n        __m256d diff = _mm_sub_pd(vec1, vec2);\n        __m256d abs_diff = _mm_abs_pd(diff);\n\n        __m256d mask = _mm_cmpne_pd(abs_diff, threshold_vec, _CMP_LT_OQ);\n        __m256d result = _mm_movemask_ps(mask);\n\n        if (_mm_test_all_ones(result)) {\n            return true;\n        }\n    }\n\n    // Check remaining elements\n    for (int i = num_elements - 7; i < num_elements; ++i) {\n        if (std::abs(numbers[i] - numbers[i - 7]) < threshold) {\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_62_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool has_close_elements_simd(const std::vector<double> & numbers, double threshold){\n    const int num_elements = numbers.size();\n    __m256 threshold_vec = _mm_set1_ps(threshold);\n    \n    for(int i = 0; i < num_elements - 7; i += 8){\n        __m256 vec1 = _mm_loadu_ps(&numbers[i]);\n        __m256 vec2 = _mm_loadu_ps(&numbers[i + 4]);\n        \n        __m256 diff = _mm_sub_ps(vec1, vec2);\n        __m256 abs_diff = _mm_abs_ps(diff);\n        \n        __m256 close_to_threshold = _mm_cmpgt_ps(abs_diff, threshold_vec);\n        __m256 all_close = _mm_and_ps(close_to_threshold, close_to_threshold);\n        \n        if(_mm_movemask_ps(all_close) != 0){\n            return true;\n        }\n    }\n    \n    // Handle remaining elements\n    for(int i = num_elements - 7; i < num_elements; ++i){\n        if(std::abs(numbers[i] - numbers[num_elements - 1]) < threshold){\n            return true;\n        }\n    }\n    \n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations){\n    __m256i sum = _mm256_setzero_si256();\n    for (int i = 0; i < operations.size(); i += 8) {\n        __m256i current_sum = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        sum = _mm256_add_epi32(sum, current_sum);\n    }\n    int result = _mm256_movemask_epi8(_mm256_cmpgt_epi32(sum, _mm256_setzero_si256()));\n    return result != 0;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nusing namespace std;\n\nbool below_zero_simd(std::vector<int> operations) {\n    __m256i sum = _mm256_setzero_si256();\n    int i = 0;\n    while (i < operations.size()) {\n        __m256i op = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        sum = _mm256_add_epi32(sum, op);\n        i += 8;\n        if (_mm256_movemask_epi8(_mm256_cmpgt_epi32(sum, _mm256_setzero_si256())) != 0) {\n            return true;\n        }\n    }\n    return false;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nusing namespace std;\n\nbool below_zero_simd(std::vector<int> operations){\n    __m256i sum = _mm256_setzero_si256();\n    int n = operations.size();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i op = _mm256_loadu_si256((__m256i*)&operations[i]);\n        sum = _mm256_add_epi32(sum, op);\n    }\n\n    // Check if any element in the sum is negative\n    __m256i mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n    __m256i result = _mm256_movemask_epi8(mask);\n\n    // If any bit in the result is set, then there's a negative value\n    return _mm_popcnt_u32(result) != 0;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations){\n    __m256i sum = _mm256_setzero_si256();\n    for (int i = 0; i < operations.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        sum = _mm256_add_epi32(sum, vec);\n    }\n    int result = _mm256_movemask_epi32(_mm256_cmpgt_epi32(sum, _mm256_setzero_si256()));\n    return result != 0;\n}\n```"}
{"task_id": "SimdBench_63_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_zero_simd(std::vector<int> operations){\n    __m256i sum = _mm256_setzero_si256();\n    for (int i = 0; i < operations.size(); i += 4) {\n        __m256i current_sum = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&operations[i]));\n        sum = _mm256_add_epi32(sum, current_sum);\n    }\n    int remaining_operations = operations.size() % 4;\n    if (remaining_operations > 0) {\n        __m128i last_sum = _mm_loadu_si128(reinterpret_cast<const __m128i*>(&operations[operations.size() - remaining_operations]));\n        sum = _mm_add_epi32(sum, _mm256_extract_epi32(last_sum, 0));\n        sum = _mm_add_epi32(sum, _mm256_extract_epi32(last_sum, 1));\n        sum = _mm_add_epi32(sum, _mm256_extract_epi32(last_sum, 2));\n        sum = _mm_add_epi32(sum, _mm256_extract_epi32(last_sum, 3));\n    }\n    return _mm_movemask_epi8(_mm_cmpgt_epi32(sum, _mm_setzero_si256())) != 0;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int num_elements = numbers.size();\n    double sum = _mm256_setzero_pd();\n    double mean = _mm256_setzero_pd();\n\n    // Calculate the sum of all elements\n    __m256i i = _mm256_set_epi32(0, 1, 2, 3);\n    for (; i != _mm256_set_epi32(num_elements / 4, num_elements / 4 + 1, num_elements / 4 + 2, num_elements / 4 + 3); ++i) {\n        __m256 v = _mm256_loadu_ps(&numbers[i]);\n        sum += _mm256_add_pd(sum, v);\n    }\n\n    // Calculate the number of elements\n    int remaining_elements = num_elements % 4;\n    if (remaining_elements > 0) {\n        __m256 v = _mm256_loadu_ps(&numbers[num_elements - remaining_elements]);\n        sum += _mm256_add_pd(sum, v);\n    }\n\n    // Calculate the mean\n    mean = _mm256_div_pd(sum, _mm256_set1_pd(static_cast<double>(num_elements)));\n\n    // Calculate the squared differences from the mean\n    __m256 squared_diffs = _mm256_sub_pd(numbers.data(), mean);\n\n    // Sum the squared differences\n    sum = _mm256_reduce_add_pd(squared_diffs);\n\n    // Calculate the MAD\n    return sqrt(sum / static_cast<double>(num_elements));\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int num_elements = numbers.size();\n    if(num_elements == 0){\n        return 0.0;\n    }\n\n    __m256d sum = _mm256_setzero_pd();\n    __m256d mean = _mm256_setzero_pd();\n\n    for(int i = 0; i < num_elements; i += 8){\n        __m256d current_number = _mm256_loadu_pd(numbers.data() + i);\n        sum = _mm256_add_pd(sum, current_number);\n        mean = _mm256_add_pd(mean, current_number);\n    }\n\n    double total_sum = _mm256_reduce_add_pd(sum);\n    double total_mean = _mm256_reduce_add_pd(mean);\n\n    __m256d squared_diffs = _mm256_sub_pd(current_number, mean);\n    __m256d squared_diffs_sum = _mm256_reduce_add_pd(squared_diffs);\n\n    double mad = std::sqrt(squared_diffs_sum / num_elements);\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int n = numbers.size();\n    if(n == 0) return 0;\n\n    // Convert std::vector to __m256d array\n    __m256d *data = reinterpret_cast<__m256d*>(numbers.data());\n    __m256d sum = _mm256_setzero_pd();\n    __m256d mean = _mm256_setzero_pd();\n\n    // Calculate sum and mean using AVX2 intrinsics\n    for(int i = 0; i < n / 8; ++i){\n        sum = _mm256_add_pd(sum, data[i]);\n        mean = _mm256_add_pd(mean, data[i]);\n    }\n\n    double total_sum = _mm256_reduce_add_ps(sum);\n    double total_mean = _mm256_reduce_add_ps(mean);\n\n    // Calculate MAD using AVX2 intrinsics\n    __m256d diff = _mm256_sub_pd(data[0], mean);\n    __m256d mad = _mm256_abs_pd(diff);\n    for(int i = 1; i < n / 8; ++i){\n        diff = _mm256_sub_pd(data[i], mean);\n        mad = _mm256_add_pd(mad, _mm256_abs_pd(diff));\n    }\n    double mad_total = _mm256_reduce_add_ps(mad);\n\n    // Return MAD as a double\n    return mad_total / n;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    if(numbers.empty()){\n        return 0.0;\n    }\n\n    // Calculate the mean of the dataset\n    double sum = std::accumulate(numbers.begin(), numbers.end(), 0.0);\n    double mean = sum / numbers.size();\n\n    // Use AVX/AVX2 intrinsics to calculate the MAD\n    __m256d mad_sum = _mm256_setzero_pd();\n    int i = 0;\n    while(i < numbers.size() - 7){\n        __m256d num = _mm256_loadu_pd(&numbers[i]);\n        __m256d diff = _mm256_sub_pd(num, _mm256_set1_pd(mean));\n        __m256d abs_diff = _mm256_abs_pd(diff);\n        mad_sum = _mm256_add_pd(mad_sum, abs_diff);\n        i += 8;\n    }\n    mad_sum = _mm256_hadd_pd(_mm256_hadd_pd(mad_sum, mad_sum), mad_sum);\n\n    // Sum up the remaining elements\n    for(; i < numbers.size(); ++i){\n        mad_sum = _mm256_add_pd(mad_sum, _mm256_set1_pd(std::abs(numbers[i] - mean)));\n    }\n\n    // Average the MAD\n    mad_sum = _mm256_div_pd(mad_sum, _mm256_set1_pd(numbers.size()));\n    mad_sum = _mm256_extractf64x2_pd(mad_sum, 0);\n\n    return mad_sum;\n}\n```"}
{"task_id": "SimdBench_64_AVX", "completion": "```c++\ndouble mean_absolute_deviation_simd(std::vector<double> numbers){\n    const int n = numbers.size();\n    if(n <= 0) return 0.0;\n\n    // Calculate the sum of all elements\n    __m256d sum = _mm256_setzero_pd();\n    for(int i = 0; i < n; i += 8){\n        __m256d vec = _mm256_loadu_pd(numbers.data() + i);\n        sum = _mm256_add_pd(sum, vec);\n    }\n    double total_sum = _mm256_reduce_add_pd(sum);\n\n    // Calculate the mean\n    double mean = total_sum / n;\n\n    // Calculate the squared differences from the mean\n    __m256d squared_diffs = _mm256_setzero_pd();\n    for(int i = 0; i < n; i += 8){\n        __m256d vec = _mm256_loadu_pd(numbers.data() + i);\n        __m256d diff = _mm256_sub_pd(vec, _mm256_set1_pd(mean));\n        squared_diffs = _mm256_add_pd(squared_diffs, _mm256_mul_pd(diff, diff));\n    }\n    double total_squared_diffs = _mm256_reduce_add_pd(squared_diffs);\n\n    // Calculate the MAD\n    double mad = sqrt(total_squared_diffs / n);\n\n    return mad;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    if(numbers.empty()) return {};\n\n    std::vector<int> result;\n    __m256i del = _mm_set_epi32(delimeter, delimeter, delimeter, delimeter);\n\n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m256i vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i next_vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i + 4]));\n\n        __m256i combined = _mm_add_epi32(vec, next_vec);\n        __m256i result_vec = _mm_shuffle_epi32(combined, combined, 0b0000'0000'0000'00'0000'00'0000'00'0000'00'00000'0000'00'0000'0000'0000'00'0000'00'0000'0000'00'0000'00'0000'00'0000'0000'0000'0000'0000'0000'00'0000'00'0000'00'0000'00'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'00'0000'00'0000'00'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'00'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'00'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'0000'"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    if(numbers.empty()) return {};\n\n    std::vector<int> result;\n    __m256i del = _mm_set_epi32(delimeter, delimeter, delimeter, delimeter);\n\n    for(size_t i = 0; i < numbers.size(); i += 4) {\n        __m256i vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i result_vec = _mm_add_epi32(vec, del);\n        _mm_storeu_si256(reinterpret_cast<__m256i*>(&result), result_vec);\n    }\n\n    // Handle remaining elements\n    size_t remaining_elements = numbers.size() % 4;\n    if(remaining_elements > 0) {\n        __m256i vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[numbers.size() - remaining_elements]));\n        __m256i result_vec = _mm_add_epi32(vec, del);\n        _mm_storeu_si256(reinterpret_cast<__m256i*>(&result), result_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    if(numbers.empty()) return {};\n\n    const int num_elements = numbers.size();\n    const int num_words = (num_elements + 7) / 8; // Round up to nearest multiple of 8\n\n    __m256i* vec_ptr = (__m256i*)malloc(num_words * sizeof(__m256i));\n    if(!vec_ptr) throw std::runtime_error(\"Memory allocation failed\");\n\n    for(int i = 0; i < num_words; ++i) {\n        vec_ptr[i] = _mm256_setzero_si256(); // Initialize each vector element to zero\n    }\n\n    int word_index = 0;\n    for(int i = 0; i < num_elements; ++i) {\n        vec_ptr[word_index >> 3][word_index & 7] = _mm256_set_epi32(\n            numbers[i], numbers[i], numbers[i], numbers[i],\n            numbers[i], numbers[i], numbers[i], numbers[i]\n        );\n\n        word_index += 8;\n    }\n\n    std::vector<int> result;\n    for(int i = 0; i < num_words; ++i) {\n        for(int j = 0; j < 8; ++j) {\n            result.push_back(_mm256_extract_epi32(vec_ptr[i][j], 0));\n        }\n    }\n\n    free(vec_ptr);\n    return result;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    if(numbers.empty()) return {};\n\n    const int num_elements = numbers.size();\n    __m256i del = _mm_set_epi32(delimeter, delimeter, delimeter, delimeter);\n\n    __m256i* ptr = (__m256i*)numbers.data();\n    __m256i* end = (__m256i*)(ptr + num_elements - 1);\n    __m256i* next_ptr = ptr;\n\n    while(next_ptr <= end){\n        _mm_storeu_si256(ptr, *next_ptr);\n        next_ptr += 4;\n        _mm_storeu_si256(ptr + 4, del);\n        ptr += 8;\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_65_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> intersperse_simd(std::vector<int> numbers, int delimeter){\n    if(numbers.empty()) return {};\n\n    std::vector<int> result;\n    __m256i del = _mm_set_epi32(delimeter, delimeter, delimeter, delimeter);\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n\n    for(size_t i = 0; i < numbers.size(); i += 4){\n        __m256i vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        __m256i masked_vec = _mm_and_si256(vec, mask);\n        __m256i shifted_vec = _mm_slli_epi32(masked_vec, 2);\n        __m256i combined_vec = _mm_or_si256(shifted_vec, vec);\n        _mm_storeu_si256(reinterpret_cast<__m256i*>(&result), combined_vec);\n    }\n\n    // Handle remaining elements\n    size_t remaining_elements = numbers.size() % 4;\n    for(size_t i = 0; i < remaining_elements; ++i){\n        result.push_back(numbers[i]);\n        result.push_back(delimeter);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    if(numbers.empty()){\n        return {sum, product};\n    }\n\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_product = _mm256_setone_si256();\n\n    for(size_t i = 0; i < numbers.size(); i += 8){\n        __m256i vec_num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        vec_sum = _mm256_add_epi16(vec_sum, vec_num);\n        vec_product = _mm256_mullo_epi16(vec_product, vec_num);\n    }\n\n    // Handle remaining elements\n    for(size_t i = numbers.size() % 8; i < numbers.size(); ++i){\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    // Reduce the results from 256-bit vectors to 128-bit vectors\n    __m128i reduced_sum = _mm256_extractf128_si128(vec_sum, 0);\n    __m128i reduced_product = _mm256_extractf128_si128(vec_product, 0);\n\n    // Sum and multiply the reduced results\n    sum += _mm_cvtsi128_si64(reduced_sum);\n    product *= _mm_cvtsi128_si64(reduced_product);\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    if(numbers.empty()){\n        return {0, 1};\n    }\n\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_product = _mm256_setone_si256();\n\n    for(size_t i = 0; i < numbers.size(); i += 8){\n        __m256i vec_num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        vec_sum = _mm256_add_epi16(vec_sum, vec_num);\n        vec_product = _mm256_mullo_epi16(vec_product, vec_num);\n    }\n\n    // Handle remaining elements\n    for(size_t i = numbers.size() % 8; i < numbers.size(); ++i){\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    sum = _mm256_reduce_add_epi16(vec_sum);\n    product = _mm256_reduce_and_si256(vec_product);\n\n    std::vector<int64_t> result = {sum, product};\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    if(numbers.empty()){\n        return {0, 1};\n    }\n\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_product = _mm256_setone_si256();\n\n    for(size_t i = 0; i < numbers.size(); i += 8){\n        __m256i vec_num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        vec_sum = _mm256_add_epi16(vec_sum, vec_num);\n        vec_product = _mm256_mullo_epi16(vec_product, vec_num);\n    }\n\n    // Handle remaining elements\n    for(size_t i = numbers.size() % 8; i < numbers.size(); ++i){\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    vec_sum = _mm256_hadd_epi16(vec_sum, vec_sum);\n    vec_product = _mm256_hadd_epi16(vec_product, vec_product);\n\n    sum += _mm256_extract_epi16(_mm256_extract_epi16(vec_sum, 0), 0);\n    sum += _mm256_extract_epi16(_mm256_extract_epi16(vec_sum, 1), 0);\n    sum += _mm256_extract_epi16(_mm256_extract_epi16(vec_sum, 2), 0);\n    sum += _mm256_extract_epi16(_mm256_extract_epi16(vec_sum, 3), 0);\n\n    product += _mm256_extract_epi16(_mm256_extract_epi16(vec_product, 0), 0);\n    product += _mm256_extract_epi16(_mm256_extract_epi16(vec_product, 1), 0);\n    product += _mm256_extract_epi16(_mm256_extract_epi16(vec_product, 2), 0);\n    product += _mm256_extract_epi16(_mm256_extract_epi16(vec_product, 3), 0);\n\n    return {sum, product};\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    if(numbers.empty()){\n        return {0, 1};\n    }\n\n    int64_t sum = 0;\n    int64_t product = 1;\n\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_product = _mm256_setone_si256();\n\n    for(size_t i = 0; i < numbers.size(); i += 8){\n        __m256i vec_num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        vec_sum = _mm256_add_epi16(vec_sum, vec_num);\n        vec_product = _mm256_mullo_epi16(vec_product, vec_num);\n    }\n\n    // Handle remaining elements\n    for(size_t i = numbers.size() % 8; i < numbers.size(); ++i){\n        sum += numbers[i];\n        product *= numbers[i];\n    }\n\n    sum = _mm256_reduce_add_epi16(vec_sum);\n    product = _mm256_reduce_and_si256(vec_product);\n\n    std::vector<int64_t> result = {sum, product};\n    return result;\n}\n```"}
{"task_id": "SimdBench_66_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int64_t> sum_product_simd(std::vector<int16_t> numbers){\n    if(numbers.empty()){\n        return {0, 1};\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i prod = _mm256_setone_si256();\n\n    for(size_t i = 0; i < numbers.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        sum = _mm256_add_epi16(sum, vec);\n        prod = _mm256_mullo_epi16(prod, vec);\n    }\n\n    int64_t sum_int = _mm256_extract_epi64(_mm256_add_epi32(sum, _mm256_slli_epi32(sum, 32)), 0);\n    int64_t prod_int = _mm256_extract_epi64(_mm256_mullo_epi32(prod, _mm256_slli_epi32(prod, 32)), 0);\n\n    std::vector<int64_t> result;\n    result.push_back(sum_int);\n    result.push_back(prod_int);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    const int num_elements = numbers.size();\n    std::vector<int> result(num_elements);\n    \n    // Load the first two elements into registers\n    __m256i max1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[0]));\n    __m256i max2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[1]));\n    \n    // Process each pair of elements in parallel\n    for(int i = 2; i < num_elements; i += 2){\n        __m256i current = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        \n        // Compare with the previous maximums\n        __m256i max1_current = _mm256_cmpgt_epi32(max1, current);\n        __m256i max2_current = _mm256_cmpgt_epi32(max2, current);\n        \n        // Update the maximums\n        max1 = _mm256_blendv_epi32(max1, max1_current, max1_current);\n        max2 = _mm256_blendv_epi32(max2, max2_current, max2_current);\n    }\n    \n    // Handle the remaining elements if there are an odd number of elements\n    if(num_elements % 2 != 0){\n        __m256i last_element = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[num_elements - 1]));\n        __m256i max1_last = _mm256_cmpgt_epi32(max1, last_element);\n        __m256i max2_last = _mm256_cmpgt_epi32(max2, last_element);\n        \n        max1 = _mm256_blendv_epi32(max1, max1_last, max1_last);\n        max2 = _mm256_blendv_epi32(max2, max2_last, max2_last);\n    }\n    \n    // Store the results back into the vector\n    for(int i = 0; i < num_elements; ++i){\n        result[i] = _mm256_extract_epi32(max1, i / 4) > _mm256_extract_epi32(max2, i / 4) ? _mm256_extract_epi32(max1, i / 4) : _mm256_extract_epi32(max2, i / 4);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size());\n    __m256i max_vector = _mm256_setzero_si256();\n    \n    for(size_t i = 0; i < numbers.size(); ++i){\n        __m256i current_vector = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        \n        // Compare current vector with max vector and update max vector if necessary\n        max_vector = _mm256_max_epi32(max_vector, current_vector);\n        \n        // Store the maximum value from each vector pair in the result vector\n        _mm_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), max_vector);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size());\n    __m256i max_vector = _mm256_setzero_si256();\n    \n    for(size_t i = 0; i < numbers.size(); ++i){\n        __m256i current_vector = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&numbers[i]));\n        \n        // Find the maximum in the current vector\n        __m256i max_current = _mm256_cmpgt_epi32(max_vector, current_vector);\n        max_vector = _mm256_blendv_epi32(max_vector, current_vector, max_current);\n        \n        // Store the maximum in the result vector\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[i]), max_vector);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size());\n    __m256i max_values = _mm256_setzero_si256();\n    int i = 0;\n    while (i + 7 < numbers.size()) {\n        __m256i current_values = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        __m256i temp_max_values = _mm256_max_epi32(max_values, current_values);\n        _mm256_storeu_si256((__m256i*)&result[i], temp_max_values);\n        i += 8;\n    }\n    // Handle remaining elements\n    for (; i < numbers.size(); ++i) {\n        result[i] = *std::max_element(result.begin() + i - 7, result.end());\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_67_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> rolling_max_simd(std::vector<int> numbers){\n    std::vector<int> result(numbers.size());\n    __m256i max = _mm256_setzero_si256();\n    \n    for(size_t i = 0; i < numbers.size(); ++i){\n        __m256i current = _mm256_loadu_si256((__m256i*)&numbers[i]);\n        \n        if(_mm256_cmpgt_epi32(max, current)){\n            max = current;\n        }\n        \n        _mm256_storeu_si256((__m256i*)&result[i], max);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    const int len = std::max(a.size(), b.size());\n    __m256i a_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0);\n\n    for(int i = 0; i < len; ++i){\n        if(i < a.size()){\n            a_vec = _mm256_or_si256(a_vec, _mm256_set_epi8(a[i] - '0'));\n        }\n        if(i < b.size()){\n            b_vec = _mm256_or_si256(b_vec, _mm256_set_epi8(b[i] - '0'));\n        }\n\n        a_vec = _mm256_xor_si256(a_vec, b_vec);\n        b_vec = _mm256_setzero_si256();\n    }\n\n    std::string result;\n    for(int i = 0; i < len; ++i){\n        result += static_cast<char>(_mm_extract_epi8(a_vec, i) + '0');\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    const int len = std::max(a.size(), b.size());\n    __m256i a_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0,\n                                        0, 0, 0, 0, 0, 0, 0, 0);\n\n    for(int i = 0; i < len; ++i){\n        if(i < a.size()){\n            a_vec = _mm256_or_si256(a_vec, _mm256_set_epi8((int)a[i]));\n        }\n        if(i < b.size()){\n            b_vec = _mm256_or_si256(b_vec, _mm256_set_epi8((int)b[i]));\n        }\n\n        a_vec = _mm256_xor_si256(a_vec, b_vec);\n        b_vec = _mm256_setzero_si256();\n    }\n\n    std::string result;\n    for(int i = 0; i < len; ++i){\n        result += static_cast<char>(_mm_extract_epi8(a_vec, i));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    const int len = std::max(a.size(), b.size());\n    __m256i a_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0);\n\n    for(int i = 0; i < len; i++){\n        if(i < a.size()){\n            a_vec = _mm256_or_si256(a_vec, _mm256_set_epi8((a[i] == '1') ? 1 : 0));\n        }\n        if(i < b.size()){\n            b_vec = _mm256_or_si256(b_vec, _mm256_set_epi8((b[i] == '1') ? 1 : 0));\n        }\n\n        a_vec = _mm256_xor_si256(a_vec, b_vec);\n        b_vec = _mm256_setzero_si256();\n    }\n\n    std::string result;\n    for(int i = 0; i < len; i++){\n        result += (char)(a_vec[i] & 1);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    const int len = std::max(a.size(), b.size());\n    __m256i a_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n\n    for(int i = 0; i < len; ++i){\n        if(i < a.size()){\n            a_vec = _mm256_or_si256(a_vec, _mm256_set_epi8((a[i] - '0') & 1));\n        }\n        if(i < b.size()){\n            b_vec = _mm256_or_si256(b_vec, _mm256_set_epi8((b[i] - '0') & 1));\n        }\n\n        a_vec = _mm256_xor_si256(a_vec, b_vec);\n        b_vec = _mm256_setzero_si256();\n    }\n\n    std::string result;\n    for(int i = 0; i < len; ++i){\n        result += (char)(a_vec[i] & 1 + '0');\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_68_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string string_xor_simd(std::string a, std::string b){\n    const int len = std::max(a.size(), b.size());\n    __m256i a_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b_vec = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0,\n                                      0, 0, 0, 0, 0, 0, 0, 0);\n\n    for(int i = 0; i < len; i++){\n        if(i < a.size()){\n            a_vec = _mm256_or_si256(a_vec, _mm256_set_epi8(a[i] - '0'));\n        }\n        if(i < b.size()){\n            b_vec = _mm256_or_si256(b_vec, _mm256_set_epi8(b[i] - '0'));\n        }\n\n        a_vec = _mm256_xor_si256(a_vec, b_vec);\n        b_vec = _mm256_setzero_si256();\n    }\n\n    std::string result;\n    for(int i = 0; i < len; i++){\n        result += static_cast<char>(_mm_extract_epi8(a_vec, i) + '0');\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const int max_len = *std::max_element(strings.begin(), strings.end(), [](const std::string& a, const std::string& b) {\n        return a.size() < b.size();\n    });\n\n    __m256i max_len_vec = _mm_set1_epi32(max_len);\n\n    __m256i* vec_ptr = (__m256i*)strings.data();\n    __m256i* end_ptr = vec_ptr + strings.size();\n\n    while (vec_ptr != end_ptr) {\n        __m256i len_vec = _mm_loadu_si256(vec_ptr);\n        __m256i cmp_vec = _mm_cmplt_epi32(len_vec, max_len_vec);\n        __m256i mask_vec = _mm_andnot_si256(cmp_vec, max_len_vec);\n        __m256i result_vec = _mm_or_si256(result_vec, mask_vec);\n        vec_ptr += 4;\n    }\n\n    int result_index = -1;\n    for (int i = 0; i < strings.size(); ++i) {\n        if (_mm_movemask_epi8(_mm_cmpgt_epi32(_mm_loadu_si256((__m256i*)&strings[i]), result_vec)) > 0) {\n            result_index = i;\n            break;\n        }\n    }\n\n    if (result_index == -1) {\n        return \"\";\n    }\n\n    return strings[result_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const int max_len = *std::max_element(strings.begin(), strings.end(), [](const std::string& a, const std::string& b) {\n        return a.size() < b.size();\n    });\n\n    __m256i max_len_vec = _mm_set1_epi32(max_len);\n    __m256i mask = _mm_cmplt_epi32(_mm_loadu_si256((__m256i*)strings.data()), max_len_vec);\n\n    __m256i result = _mm256_setzero_si256();\n\n    for (int i = 0; i < strings.size(); i += 4) {\n        __m256i str = _mm_loadu_si256((__m256i*)(strings.data() + i));\n        __m256i len = _mm_cvtepi32_ps(_mm_extract_epi32(str, 3));\n\n        __m256i cmp = _mm_cmpgt_ps(len, max_len_vec);\n        __m256i select = _mm_blendv_ps(cmp, str, result);\n\n        result = _mm_add_epi32(result, select);\n    }\n\n    __m256i final_result = _mm256_reduce_max_ps(result);\n    int final_index = _mm_cvtss_si32(final_result);\n\n    return strings[final_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const int max_len = *std::max_element(strings.begin(), strings.end(), [](const std::string& a, const std::string& b) {\n        return a.size() < b.size();\n    });\n\n    __m256i max_len_mask = _mm_set1_epi32(max_len);\n    __m256i max_len_vec = _mm_setzero_si256();\n\n    for (size_t i = 0; i < strings.size(); i += 4) {\n        __m256i str = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&strings[i]));\n        __m256i len = _mm_cvtsi32_si256(_mm_extract_epi32(str, 3));\n        __m256i cmp = _mm_cmplt_epi32(len, max_len_mask);\n        __m256i mask = _mm_and_si256(cmp, max_len_vec);\n        max_len_vec = _mm_or_si256(mask, max_len_vec);\n    }\n\n    size_t max_index = 0;\n    for (size_t i = 0; i < strings.size(); ++i) {\n        if (_mm_movemask_epi8(_mm_cmpgt_epi32(max_len_vec, _mm_loadu_si256(reinterpret_cast<const __m256i*>(&strings[i])))) {\n            max_index = i;\n        }\n    }\n\n    return strings[max_index];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings){\n    if(strings.empty()) return \"\";\n    \n    __m256i max_len = _mm_set1_epi32(0);\n    __m256i max_index = _mm_set1_epi32(-1);\n    \n    for(size_t i = 0; i < strings.size(); i += 4){\n        const __m256i *strs = reinterpret_cast<const __m256i*>(strings.data() + i);\n        \n        __m256i len = _mm256_loadu_si256(strs);\n        __m256i index = _mm256_set_epi32(i, i+1, i+2, i+3);\n        \n        __m256i cmp = _mm256_cmpgt_epi32(len, max_len);\n        max_len = _mm256_blendv_epi32(max_len, len, cmp);\n        max_index = _mm256_blendv_epi32(max_index, index, cmp);\n    }\n    \n    size_t max_len_val = _mm256_extract_epi32(max_len, 0);\n    size_t max_index_val = _mm256_extract_epi32(max_index, 0);\n    \n    for(size_t i = max_index_val + 1; i < strings.size(); ++i){\n        if(strings[i].size() > max_len_val){\n            max_len_val = strings[i].size();\n            max_index_val = i;\n        }\n    }\n    \n    return strings[max_index_val];\n}\n```"}
{"task_id": "SimdBench_69_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <string>\n\nstd::string longest_simd(const std::vector<std::string> & strings) {\n    if (strings.empty()) {\n        return \"\";\n    }\n\n    const int max_len = *std::max_element(strings.begin(), strings.end(), [](const std::string& a, const std::string& b) {\n        return a.size() < b.size();\n    });\n\n    __m256i max_len_vec = _mm_set1_epi32(max_len);\n    __m256i mask = _mm_cmpgt_epi32(_mm_loadu_si256((__m256i*)strings.data()), max_len_vec);\n\n    __m256i result = _mm_blendv_epi8(_mm_setzero_si256(), _mm_loadu_si256((__m256i*)strings.data()), mask);\n\n    __m256i max_index = _mm_slli_epi32(result, 4);\n    __m256i index_mask = _mm_cmpgt_epi32(max_index, _mm_set1_epi32(strings.size()));\n    __m256i final_index = _mm_blendv_epi8(_mm_setzero_si32(), max_index, index_mask);\n\n    int max_index_int = static_cast<int>(_mm_extract_epi32(final_index, 0));\n\n    return strings[max_index_int];\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m256i av = _mm256_set_epi64x(a >> 32, a & 0xFFFFFFFF, b >> 32, b & 0xFFFFFFFF);\n    __m256i bv = _mm256_set_epi64x(b >> 32, b & 0xFFFFFFFF, a >> 32, a & 0xFFFFFFFF);\n\n    __m256i t;\n    while (_mm256_testz_si256(av, bv)) {\n        t = _mm256_sub_epi64(av, bv);\n        av = bv;\n        bv = t;\n    }\n\n    return _mm_cvtsi256_si64(_mm256_and_si256(av, bv));\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i avx_a = _mm256_set_epi64x(a >> 32, a & 0xFFFFFFFF);\n    __m256i avx_b = _mm256_set_epi64x(b >> 32, b & 0xFFFFFFFF);\n\n    // Compute the GCD of each pair of high and low parts separately\n    __m256i gcd_high = _mm256_gcd(avx_a, avx_b);\n    __m256i gcd_low = _mm256_gcd(_mm256_srli_si256(avx_a, 32), _mm256_srli_si256(avx_b, 32));\n\n    // Combine the results from both parts\n    __m256i combined_gcd = _mm256_or_si256(gcd_high, gcd_low);\n\n    // Extract the final result\n    uint64_t result = _mm256_extract_epi64(combined_gcd, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c++\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b) {\n    __m256i a_vec = _mm256_set_epi64x(a >> 32, a & 0xFFFFFFFF);\n    __m256i b_vec = _mm256_set_epi64x(b >> 32, b & 0xFFFFFFFF);\n\n    while (_mm256_testz_si256(a_vec, a_vec)) {\n        a_vec = _mm256_sub_epi64(a_vec, b_vec);\n    }\n\n    return _mm256_extract_epi64(_mm256_srlv_epi64(a_vec, b_vec), 0);\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m256i av = _mm256_set_epi64x(a >> 32, a & 0xFFFFFFFF, b >> 32, b & 0xFFFFFFFF);\n    __m256i bv = _mm256_set_epi64x(b >> 32, b & 0xFFFFFFFF, a >> 32, a & 0xFFFFFFFF);\n\n    __m256i t;\n    while (_mm256_testz_si256(av, bv)) {\n        t = _mm256_and_si256(av, bv);\n        av = _mm256_srli_epi64(t, 1);\n        bv = _mm256_slli_epi64(t, 1);\n    }\n\n    return _mm_cvtsi256_si64(_mm256_and_si256(av, bv));\n}\n```"}
{"task_id": "SimdBench_70_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t greatest_common_divisor_simd(uint64_t a, uint64_t b){\n    __m256i a_vec = _mm256_set_epi64x(a >> 32, a & 0xFFFFFFFF);\n    __m256i b_vec = _mm256_set_epi64x(b >> 32, b & 0xFFFFFFFF);\n\n    while (_mm256_testz_si256(a_vec, b_vec)) {\n        a_vec = _mm256_sub_epi64(a_vec, b_vec);\n        b_vec = _mm256_set_epi64x(b_vec >> 32, b_vec & 0xFFFFFFFF);\n    }\n\n    return _mm256_extract_epi64(_mm256_and_si256(a_vec, b_vec), 0);\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if(numbers.size() < 2){\n        throw std::invalid_argument(\"Input vector must have at least two elements.\");\n    }\n\n    // Load the first two elements into registers\n    __m256 v1 = _mm256_loadu_ps(&numbers[0]);\n    __m256 v2 = _mm256_loadu_ps(&numbers[1]);\n\n    // Initialize variables to store the closest elements\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_element1 = 0.0f;\n    float closest_element2 = 0.0f;\n\n    // Process pairs of elements in parallel\n    for(size_t i = 0; i < numbers.size(); i += 2){\n        __m256 v3 = _mm256_loadu_ps(&numbers[i]);\n        __m256 v4 = _mm256_loadu_ps(&numbers[i + 1]);\n\n        // Compute differences between adjacent pairs\n        __m256 diff1 = _mm256_sub_ps(v3, v1);\n        __m256 diff2 = _mm256_sub_ps(v4, v2);\n\n        // Find the minimum difference and corresponding elements\n        __m256 abs_diff1 = _mm256_abs_ps(diff1);\n        __m256 abs_diff2 = _mm256_abs_ps(diff2);\n\n        __m256 min_diff_vec = _mm256_min_ps(abs_diff1, abs_diff2);\n        __m256 min_index_vec = _mm256_argmin_ps(min_diff_vec);\n\n        int min_index = _mm256_extract_epi32(min_index_vec, 0);\n        if(min_index == 0){\n            closest_element1 = v1[0];\n            closest_element2 = v3[0];\n        } else if(min_index == 1){\n            closest_element1 = v2[0];\n            closest_element2 = v4[0];\n        } else if(min_index == 2){\n            closest_element1 = v3[1];\n            closest_element2 = v4[1];\n        } else {\n            closest_element1 = v1[1];\n            closest_element2 = v3[1];\n        }\n    }\n\n    // Store the closest elements back into the input vector\n    for(size_t i = 0; i < numbers.size(); ++i){\n        if(i % 2 == 0){\n            numbers[i] = closest_element1;\n        } else {\n            numbers[i] = closest_element2;\n        }\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if(numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must have at least two elements.\");\n    }\n\n    // Convert the input vector to a packed single-precision floating-point vector\n    __m256 vec = _mm256_loadu_ps(numbers.data());\n\n    // Initialize variables to store the closest elements\n    float min_diff = FLT_MAX;\n    float smallest = 0.0f;\n    float largest = 0.0f;\n\n    // Process the first four elements in parallel\n    __m256 diff1 = _mm256_sub_ps(vec, _mm256_set1_ps(numbers[1]));\n    __m256 diff2 = _mm256_sub_ps(vec, _mm256_set1_ps(numbers[2]));\n    __m256 diff3 = _mm256_sub_ps(vec, _mm256_set1_ps(numbers[3]));\n\n    // Find the minimum difference and corresponding elements\n    __m256 abs_diff1 = _mm256_abs_ps(diff1);\n    __m256 abs_diff2 = _mm256_abs_ps(diff2);\n    __m256 abs_diff3 = _mm256_abs_ps(diff3);\n\n    __m256 min_diff1 = _mm256_min_ps(abs_diff1, abs_diff2);\n    __m256 min_diff2 = _mm256_min_ps(min_diff1, abs_diff3);\n\n    __m256 index1 = _mm256_pcmpge_ps(min_diff1, _mm256_set1_ps(min_diff));\n    __m256 index2 = _mm256_pcmpge_ps(min_diff2, _mm256_set1_ps(min_diff));\n\n    __m256 mask1 = _mm256_and_ps(index1, _mm256_set1_ps(1.0f));\n    __m256 mask2 = _mm256_and_ps(index2, _mm256_set1_ps(1.0f));\n\n    __m256 closest1 = _mm256_blendv_ps(vec, _mm256_set1_ps(numbers[1]), mask1);\n    __m256 closest2 = _mm256_blendv_ps(vec, _mm256_set1_ps(numbers[2]), mask2);\n\n    // Store the closest elements in the output vector\n    std::vector<float> result(2);\n    result[0] = _mm256_extractf32_ps(closest1, 0);\n    result[1] = _mm256_extractf32_ps(closest2, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if(numbers.size() < 2){\n        throw std::invalid_argument(\"Input vector must have at least two elements.\");\n    }\n\n    __m256 v = _mm256_loadu_ps(&numbers[0]);\n    float min_val = _mm256_cvtss_f32(_mm256_min_ps(v, v));\n    float max_val = _mm256_cvtss_f32(_mm256_max_ps(v, v));\n\n    __m256 diff = _mm256_sub_ps(v, _mm256_set1_ps(min_val));\n    __m256 abs_diff = _mm256_abs_ps(diff);\n    __m256 arg_min = _mm256_argmin_ps(abs_diff);\n\n    int index_min = _mm256_extract_epi32(arg_min, 0);\n    int index_max = _mm256_extract_epi32(arg_min, 1);\n\n    float closest_min = numbers[index_min];\n    float closest_max = numbers[index_max];\n\n    return {closest_min, closest_max};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if(numbers.size() < 2) throw std::invalid_argument(\"Input vector must have at least two elements.\");\n\n    // Sort the input vector\n    std::sort(numbers.begin(), numbers.end());\n\n    // Initialize variables to store the closest elements\n    float min_diff = std::numeric_limits<float>::max();\n    float closest_element1 = numbers[0];\n    float closest_element2 = numbers[1];\n\n    // Use AVX/AVX2 intrinsics to find the closest elements\n    __m256 v_numbers = _mm256_loadu_ps(numbers.data());\n    __m256 v_sorted_numbers = _mm256_loadu_ps(numbers.data());\n    __m256 v_min_diff = _mm256_setzero_ps();\n\n    for(size_t i = 0; i < numbers.size(); i += 8){\n        __m256 v_current_diff = _mm256_sub_ps(v_numbers, v_sorted_numbers);\n        __m256 v_abs_diff = _mm256_abs_ps(v_current_diff);\n        __m256 v_min_diff_temp = _mm256_min_ps(v_min_diff, v_abs_diff);\n\n        if(_mm256_cmp_ps(v_min_diff_temp, v_min_diff, _CMP_LT_OQ)){\n            v_min_diff = v_min_diff_temp;\n            closest_element1 = numbers[i];\n            closest_element2 = numbers[i + 1];\n        }\n    }\n\n    // Handle remaining elements\n    for(size_t i = numbers.size() % 8; i < numbers.size(); ++i){\n        float diff = numbers[i] - closest_element1;\n        if(diff < min_diff){\n            min_diff = diff;\n            closest_element2 = closest_element1;\n            closest_element1 = numbers[i];\n        } else if(diff > min_diff){\n            closest_element2 = numbers[i];\n        }\n    }\n\n    return {closest_element1, closest_element2};\n}\n```"}
{"task_id": "SimdBench_71_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> find_closest_elements_simd(std::vector<float> numbers){\n    if(numbers.size() < 2) {\n        throw std::invalid_argument(\"Input vector must have at least two elements.\");\n    }\n\n    __m256 v_numbers = _mm256_loadu_ps(numbers.data());\n    __m256 v_min = _mm256_setzero_ps();\n    __m256 v_max = _mm256_setzero_ps();\n\n    for(int i = 0; i < numbers.size(); i += 8) {\n        __m256 v_current = _mm256_loadu_ps(numbers.data() + i);\n        __m256 v_diff = _mm256_sub_ps(v_current, v_numbers);\n        __m256 v_abs_diff = _mm256_abs_ps(v_diff);\n\n        __m256 v_min_temp = _mm256_min_ps(v_min, v_abs_diff);\n        __m256 v_max_temp = _mm256_max_ps(v_max, v_abs_diff);\n\n        v_numbers = _mm256_blendv_ps(v_numbers, v_current, v_min_temp);\n        v_min = v_min_temp;\n        v_max = v_max_temp;\n    }\n\n    float min_val = _mm256_cvtss_f32(_mm256_min_ps(v_min, v_max));\n    float max_val = _mm256_cvtss_f32(_mm256_max_ps(v_min, v_max));\n\n    std::vector<float> result{min_val, max_val};\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int n = numbers.size();\n    __m256 min_val = _mm256_setzero_ps();\n    __m256 max_val = _mm256_setzero_ps();\n\n    // Find minimum and maximum values in parallel\n    for(int i = 0; i < n; i += 8) {\n        __m256 val = _mm256_loadu_ps(&numbers[i]);\n        min_val = _mm256_min_ps(min_val, val);\n        max_val = _mm256_max_ps(max_val, val);\n    }\n\n    // Calculate scale factor\n    float scale_factor = 1.0f / (_mm256_get_ps_u32(max_val) - _mm256_get_ps_u32(min_val));\n\n    // Rescale each element\n    std::vector<float> result(n);\n    for(int i = 0; i < n; i++) {\n        __m256 val = _mm256_loadu_ps(&numbers[i]);\n        __m256 scaled_val = _mm256_mul_ps(val, _mm256_set_ps(scale_factor, scale_factor, scale_factor, scale_factor));\n        _mm256_storeu_ps(&result[i], scaled_val);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int n = numbers.size();\n    if(n < 2) return numbers; // Not enough elements\n\n    __m256 min_val = _mm256_setzero_ps(); // Initialize minimum value to zero\n    __m256 max_val = _mm256_setzero_ps(); // Initialize maximum value to zero\n\n    // Load the first four elements into registers\n    __m256 vec1 = _mm256_loadu_ps(numbers.data());\n    __m256 vec2 = _mm256_loadu_ps(numbers.data() + 4);\n\n    // Find the minimum and maximum values in the first four elements\n    min_val = _mm256_min_ps(min_val, vec1);\n    max_val = _mm256_max_ps(max_val, vec1);\n    min_val = _mm256_min_ps(min_val, vec2);\n    max_val = _mm256_max_ps(max_val, vec2);\n\n    // Broadcast the minimum and maximum values to all elements\n    __m256 broadcast_min = _mm256_broadcast_ps(min_val);\n    __m256 broadcast_max = _mm256_broadcast_ps(max_val);\n\n    // Scale each element by the reciprocal of the range\n    __m256 scaled_vec1 = _mm256_div_ps(vec1, broadcast_max);\n    __m256 scaled_vec2 = _mm256_div_ps(vec2, broadcast_max);\n\n    // Store the results back to the original array\n    _mm256_storeu_ps(numbers.data(), scaled_vec1);\n    _mm256_storeu_ps(numbers.data() + 4, scaled_vec2);\n\n    // Process the remaining elements in chunks of four\n    for(int i = 8; i < n; i += 8){\n        __m256 vec3 = _mm256_loadu_ps(numbers.data() + i);\n        __m256 vec4 = _mm256_loadu_ps(numbers.data() + i + 4);\n\n        min_val = _mm256_min_ps(min_val, vec3);\n        max_val = _mm256_max_ps(max_val, vec3);\n        min_val = _mm256_min_ps(min_val, vec4);\n        max_val = _mm256_max_ps(max_val, vec4);\n\n        broadcast_min = _mm256_broadcast_ps(min_val);\n        broadcast_max = _mm256_broadcast_ps(max_val);\n\n        scaled_vec3 = _mm256_div_ps(vec3, broadcast_max);\n        scaled_vec4 = _mm256_div_ps(vec4, broadcast_max);\n\n        _mm256_storeu_ps(numbers.data() + i, scaled_vec3);\n        _mm256_storeu_ps(numbers.data() + i + 4, scaled_vec4);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int num_elements = numbers.size();\n    if(num_elements < 2){\n        throw std::invalid_argument(\"Input vector must have at least two elements.\");\n    }\n\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for(int i = 1; i < num_elements; i += 8){\n        __m256 current_min = _mm256_min_ps(_mm256_loadu_ps(&numbers[i]), min_val);\n        __m256 current_max = _mm256_max_ps(_mm256_loadu_ps(&numbers[i]), max_val);\n\n        min_val = _mm256_min_ps(current_min, min_val);\n        max_val = _mm256_max_ps(current_max, max_val);\n    }\n\n    float min_val_f = _mm256_extractf32_ps(min_val, 0);\n    float max_val_f = _mm256_extractf32_ps(max_val, 0);\n\n    __m256 scale_factor = _mm256_set1_ps(1.0 / (max_val_f - min_val_f));\n\n    for(int i = 0; i < num_elements; i++){\n        __m256 current_value = _mm256_loadu_ps(&numbers[i]);\n        __m256 scaled_value = _mm256_mul_ps(current_value, scale_factor);\n        _mm256_storeu_ps(&numbers[i], scaled_value);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int num_elements = numbers.size();\n    if(num_elements < 2){\n        throw std::invalid_argument(\"Input vector must have at least two elements.\");\n    }\n\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for(int i = 1; i < num_elements; i += 8){\n        __m256 current_min = _mm256_min_ps(_mm256_loadu_ps(&numbers[i]), min_val);\n        __m256 current_max = _mm256_max_ps(_mm256_loadu_ps(&numbers[i]), max_val);\n\n        min_val = _mm256_min_ps(current_min, min_val);\n        max_val = _mm256_max_ps(current_max, max_val);\n    }\n\n    float min_val_f = _mm256_extractf32_ps(min_val, 0);\n    float max_val_f = _mm256_extractf32_ps(max_val, 0);\n\n    __m256 scale_factor = _mm256_set1_ps(1.0 / (max_val_f - min_val_f));\n\n    __m256 result = _mm256_sub_ps(_mm256_loadu_ps(&numbers[0]), min_val);\n    result = _mm256_mul_ps(result, scale_factor);\n\n    for(int i = 1; i < num_elements; i += 8){\n        __m256 current_result = _mm256_sub_ps(_mm256_loadu_ps(&numbers[i]), min_val);\n        current_result = _mm256_mul_ps(current_result, scale_factor);\n\n        _mm256_storeu_ps(&numbers[i], current_result);\n    }\n\n    return numbers;\n}\n```"}
{"task_id": "SimdBench_72_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> rescale_to_unit_simd(std::vector<float> numbers){\n    const int n = numbers.size();\n    if(n < 2) return numbers; // Handle edge case\n\n    __m256 min_val = _mm256_set1_ps(numbers[0]);\n    __m256 max_val = _mm256_set1_ps(numbers[0]);\n\n    for(int i = 1; i < n; i += 8){\n        __m256 current_min = _mm256_min_ps(_mm256_loadu_ps(&numbers[i]), min_val);\n        __m256 current_max = _mm256_max_ps(_mm256_loadu_ps(&numbers[i]), max_val);\n\n        min_val = _mm256_min_ps(current_min, min_val);\n        max_val = _mm256_max_ps(current_max, max_val);\n    }\n\n    float min_val_f = _mm256_extractf32_ps(min_val, 0);\n    float max_val_f = _mm256_extractf32_ps(max_val, 0);\n\n    __m256 scale_factor = _mm256_set1_ps(1.0f / (max_val_f - min_val_f));\n\n    __m256 result = _mm256_sub_ps(_mm256_loadu_ps(&numbers[0]), min_val);\n    result = _mm256_mul_ps(result, scale_factor);\n\n    std::vector<float> result_vec;\n    for(int i = 0; i < n; i++){\n        result_vec.push_back(_mm256_extractf32_ps(result, i));\n    }\n\n    return result_vec;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    uint64_t mask = 0xFFFFFFFFFFFFFFFF;\n    uint64_t result = 0;\n\n    // Use AVX2 to process pairs of numbers in parallel\n    __m256i n_vec = _mm256_set_epi64u(n >> 32, n >> 24, n >> 16, n >> 8, n >> 0);\n    __m256i mask_vec = _mm256_set_epi64u(mask, mask, mask, mask, mask);\n\n    while (_mm256_movemask_epi8(_mm256_andnot_si256(n_vec, mask_vec)) != 0) {\n        __m256i div_vec = _mm256_div_epi64(n_vec, _mm256_slli_epi64(n_vec, 1));\n        __m256i remainder_vec = _mm256_mod_epi64(div_vec, n_vec);\n        __m256i min_remainder_vec = _mm256_min_epi64(remainder_vec, mask_vec);\n        result |= _mm256_extract_epi64(min_remainder_vec, 0);\n        n_vec = _mm256_sub_epi64(n_vec, div_vec);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n){\n    uint64_t mask = 0xFFFFFFFFFFFFFFFF;\n    uint64_t result = 0;\n\n    // Use AVX2 to process pairs of numbers in parallel\n    __m256i n_vec = _mm256_set_epi64u(n >> 32, n >> 24, n >> 16, n >> 8, n >> 0);\n    __m256i div_vec = _mm256_set_epi64u(1 << 31, 1 << 23, 1 << 15, 1 << 7, 1 << 0);\n\n    while (_mm256_testz_si256(div_vec, n_vec)) {\n        result |= _mm256_and_si256(n_vec, div_vec);\n        n_vec >>= 1;\n        div_vec <<= 1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c++\n__attribute__((target(\"avx2\"))) uint64_t largest_divisor_simd(uint64_t n) {\n    // Initialize the result with 1 (since 1 is always a divisor)\n    uint64_t result = 1;\n\n    // Loop through potential divisors from 2 up to sqrt(n)\n    for (uint64_t i = 2; i * i <= n; ++i) {\n        // Check if i is a divisor of n\n        if (n % i == 0) {\n            // Update the result with the smaller divisor\n            result = std::min(result, i);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c++\n__attribute__((target(\"avx2\"))) uint64_t largest_divisor_simd(uint64_t n) {\n    // Initialize the result with 1 (since every number is divisible by 1)\n    uint64_t result = 1;\n\n    // Loop through possible divisors from 2 up to the square root of n\n    for (uint64_t i = 2; i * i <= n; ++i) {\n        // If i is a divisor of n, divide n by i and update the result\n        if (n % i == 0) {\n            n /= i;\n            result = i;\n        }\n    }\n\n    // If n is greater than 1 after the loop, it means n itself is a divisor\n    if (n > 1) {\n        result = n;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_73_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t largest_divisor_simd(uint64_t n) {\n    uint64_t mask = 0xFFFFFFFFFFFFFFFF;\n    uint64_t result = 1;\n\n    // Use AVX2 to process pairs of numbers in parallel\n    __m256i n_vec = _mm256_set_epi64u(n >> 32, n >> 24, n >> 16, n >> 8, n >> 0);\n    __m256i result_vec = _mm256_set_epi64u(result, result, result, result, result);\n\n    while (_mm256_movemask_epi8(_mm256_cmplt_epi64(n_vec, result_vec)) != 0) {\n        __m256i div_mask = _mm256_cmpgt_epi64(n_vec, result_vec);\n        __m256i div_result = _mm256_and_si256(div_mask, n_vec);\n        __m256i new_result = _mm256_or_si256(result_vec, div_result);\n        result_vec = new_result;\n        n_vec = _mm256_sub_epi64(n_vec, div_result);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    int i = 2;\n\n    while (i * i <= n) {\n        if (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i++;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    int i = 2;\n\n    while (n > 1) {\n        if (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i++;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n){\n    std::vector<int> factors;\n    int i = 2;\n\n    while (n > 1) {\n        if (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i++;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    int i = 2;\n\n    while (n > 1) {\n        if (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i++;\n        }\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_74_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> factorize_simd(int n) {\n    std::vector<int> factors;\n    int i = 2;\n\n    while (i * i <= n) {\n        if (n % i == 0) {\n            factors.push_back(i);\n            n /= i;\n        } else {\n            i++;\n        }\n    }\n\n    if (n > 1) {\n        factors.push_back(n);\n    }\n\n    return factors;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    const __m256i mask = _mm256_set_epi32(0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF);\n    std::vector<float> result(l.size());\n    __m256 *result_ptr = (__m256 *)result.data();\n    __m256 *l_ptr = (__m256 *)&l[0];\n    \n    for(size_t i = 0; i < l.size(); i += 8){\n        __m256 temp = _mm256_loadu_ps(&l_ptr[i]);\n        __m256 masked_temp = _mm256_and_ps(temp, mask);\n        _mm256_storeu_ps(result_ptr + i / 8, masked_temp);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\nusing namespace std;\nusing namespace __m256i;\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    const int n = l.size();\n    vector<__m256> v(n / 8);\n    for(int i = 0; i < n; i += 8){\n        v[i / 8] = _mm256_loadu_ps(&l[i]);\n    }\n    \n    vector<__m256> result(n / 8);\n    for(int i = 0; i < n / 8; i++){\n        result[i] = _mm256_cmpgt_ps(v[i], _mm256_setzero_ps());\n    }\n    \n    vector<float> res(n);\n    for(int i = 0; i < n; i += 8){\n        res[i] = _mm256_extractf32_ps(result[i], 0);\n        res[i + 1] = _mm256_extractf32_ps(result[i], 1);\n        res[i + 2] = _mm256_extractf32_ps(result[i], 2);\n        res[i + 3] = _mm256_extractf32_ps(result[i], 3);\n        res[i + 4] = _mm256_extractf32_ps(result[i], 4);\n        res[i + 5] = _mm256_extractf32_ps(result[i], 5);\n        res[i + 6] = _mm256_extractf32_ps(result[i], 6);\n        res[i + 7] = _mm256_extractf32_ps(result[i], 7);\n    }\n    \n    return res;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    __m256 mask = _mm_setzero_ps();\n    __m256 result = _mm_setzero_ps();\n\n    for(size_t i = 0; i < l.size(); i += 8){\n        __m256 vec = _mm_loadu_ps(&l[i]);\n        mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        result = _mm_and_ps(result, mask);\n    }\n\n    std::vector<float> res;\n    for(size_t i = 0; i < l.size(); i++){\n        if(_mm_movemask_ps(mask, _mm_castps_si128(_mm_loadu_ps(&l[i]))) != 0){\n            res.push_back(l[i]);\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    __m256 mask = _mm_setzero_ps();\n    __m256 result = _mm_setzero_ps();\n\n    for(size_t i = 0; i + 8 <= l.size(); i += 8){\n        __m256 vec = _mm_loadu_ps(&l[i]);\n        __m256 pos_mask = _mm_cmpgt_ps(vec, _mm_setzero_ps());\n        mask = _mm_or_ps(mask, pos_mask);\n        result = _mm_or_ps(result, vec);\n    }\n\n    for(size_t i = l.size() - l.size() % 8; i < l.size(); ++i){\n        if(l[i] > 0){\n            result[i] = l[i];\n        }\n    }\n\n    std::vector<float> res;\n    res.reserve(l.size());\n    for(size_t i = 0; i < l.size(); ++i){\n        if(_mm_testz_ps(mask, _mm_set1_ps(1.0f))){\n            res.push_back(result[i]);\n        }\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_75_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<float> get_positive_simd(const std::vector<float> & l){\n    __m256 v = _mm256_loadu_ps(l.data());\n    __m256 mask = _mm256_cmpgt_ps(v, _mm256_setzero_ps());\n    __m256 result = _mm256_and_ps(v, mask);\n    std::vector<float> res;\n    _mm256_storeu_ps(res.data(), result);\n    return res;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    // Check if n is less than 2\n    if (n <= 1) return false;\n\n    // Check if n is divisible by 2\n    if (n % 2 == 0) return n == 2;\n\n    // Use AVX2 to check divisibility up to the square root of n\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 16, n >> 8, n);\n    __m256i sqrt_n_vec = _mm256_sqrt_ps(_mm256_cvtepi32_ps(n_vec));\n    __m256i mask_vec = _mm256_cmpgt_epi32(sqrt_n_vec, _mm256_set_epi32(0, 0, 0, 0));\n\n    for (__m256i i = _mm256_set_epi32(1, 3, 5, 7); i != _mm256_set_epi32(0, 0, 0, 0); i += 4) {\n        __m256i div_vec = _mm256_mul_epi32(i, i);\n        __m256i mod_vec = _mm256_mod_epi64(n_vec, div_vec);\n        __m256i result_vec = _mm256_and_si256(mask_vec, mod_vec);\n        if (_mm256_movemask_epi8(result_vec) != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c++\nbool is_prime_simd(int64_t n) {\n    // Check if n is less than 2\n    if (n <= 1) return false;\n\n    // Check if n is divisible by 2\n    if (n % 2 == 0) return n == 2;\n\n    // Use AVX2 to check divisibility up to the square root of n\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 16, n >> 8, n);\n    __m256i sqrt_n_vec = _mm256_sqrt_ps(_mm256_cvtepi32_ps(n_vec));\n    __m256i mask = _mm256_cmpgt_epi32(sqrt_n_vec, _mm256_setzero_si256());\n    __m256i divisors_mask = _mm256_andnot_si256(mask, _mm256_set_epi64x(3, 5, 7, 11));\n\n    // Perform division and check for remainder\n    for (__m256i divisor : divisors_mask) {\n        __m256i dividend = _mm256_set_epi64x(divisor, divisor, divisor, divisor);\n        __m256i quotient = _mm256_div_ps(_mm256_cvtepi32_ps(n_vec), dividend);\n        __m256i remainder = _mm256_sub_ps(_mm256_cvtepi32_ps(n_vec), _mm256_mul_ps(quotient, dividend));\n        if (_mm256_testz_si256(remainder, remainder)) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    // Check if n is less than 2\n    if (n <= 1) return false;\n\n    // Check if n is divisible by 2\n    if (n % 2 == 0) return false;\n\n    // Use AVX2 to check divisibility up to the square root of n\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 16, n >> 8, n);\n    __m256i sqrt_n_vec = _mm256_sqrt_ps(_mm256_set1_ps(sqrt((double)n)));\n    __m256i mask = _mm256_cmpgt_epi64(n_vec, sqrt_n_vec);\n\n    // Perform bitwise AND operation between n_vec and mask\n    __m256i result = _mm256_and_si256(mask, n_vec);\n\n    // If all bits in result are zero, then n is prime\n    return _mm256_testz_si256(result, result);\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c++\nbool is_prime_simd(int64_t n) {\n    if (n <= 1) return false;\n    if (n == 2 || n == 3) return true;\n\n    // Check divisibility by 2 and 3\n    if (n % 2 == 0 || n % 3 == 0) return false;\n\n    // Check divisibility by other primes up to sqrt(n)\n    int64_t max_divisor = static_cast<int64_t>(std::sqrt(n));\n    for (int64_t i = 5; i <= max_divisor; i += 6) {\n        if (n % i == 0 || n % (i + 2) == 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_76_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_prime_simd(int64_t n){\n    // Check if n is less than 2\n    if (n <= 1) return false;\n\n    // Check if n is divisible by 2\n    if (n % 2 == 0) return false;\n\n    // Use AVX2 to check divisibility up to the square root of n\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 16, n >> 8, n);\n    __m256i sqrt_n_vec = _mm256_sqrt_ps(_mm256_set_epi64x((int64_t)sqrt(n), (int64_t)sqrt(n)));\n    __m256i mask_vec = _mm256_cmpgt_epi64(n_vec, sqrt_n_vec);\n\n    // Count the number of set bits in the mask vector\n    int count = _mm_popcnt_u32(_mm_extract_epi32(mask_vec, 0)) +\n                _mm_popcnt_u32(_mm_extract_epi32(mask_vec, 1)) +\n                _mm_popcnt_u32(_mm_extract_epi32(mask_vec, 2)) +\n                _mm_popcnt_u32(_mm_extract_epi32(mask_vec, 3));\n\n    // If there are no set bits, then n is prime\n    return count == 0;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(const std::string & s){\n    const int len = s.length();\n    __m256i mask = _mm_setzero_si256(); // Initialize mask with zeros\n\n    for(int i = 0; i < len; i += 4){\n        __m256i vec = _mm_loadu_si256((__m256i*)&s[i]); // Load 4 bytes into a vector\n        __m256i reversed_vec = _mm_xor_si256(vec, mask); // Reverse case if character is lowercase\n        _mm_storeu_si256((__m256i*)&s[i], reversed_vec); // Store the result back into the string\n        mask = _mm_cmpgt_epi8(mask, _mm_cmplt_epi8(vec, _mm_set1_epi8('Z'))); // Update mask for next iteration\n    }\n\n    // Handle remaining characters\n    for(int i = len % 4; i < len; ++i){\n        char c = s[i];\n        if(isalpha(c)){\n            c ^= 'A' - 'a'; // Reverse case if character is lowercase\n        }\n        s[i] = c;\n    }\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(const std::string & s){\n    const int len = s.length();\n    __m256i mask = _mm_setzero_si256(); // Initialize mask with zeros\n\n    for(int i = 0; i < len; i += 4){\n        __m256i vec = _mm_loadu_si256((__m256i*)&s[i]); // Load 4 characters into a vector\n        __m256i reversed_vec = _mm_xor_si256(vec, mask); // Reverse the case of each character\n        _mm_storeu_si256((__m256i*)&s[i], reversed_vec); // Store the reversed characters back into the string\n        mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0); // Clear the mask\n    }\n\n    // Handle remaining characters if the length is not a multiple of 4\n    for(int i = len % 4; i < len; ++i){\n        char c = s[i];\n        if(isalpha(c)){\n            c ^= 0x20; // Toggle case\n        }\n        s[i] = c;\n    }\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(const std::string & s){\n    int n = s.size();\n    __m256i mask = _mm_setzero_si256();\n    __m256i result = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i word = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i reversed_word = _mm_xor_si256(word, mask);\n        result = _mm_or_si256(result, reversed_word);\n    }\n\n    for(int i = n % 8; i < n; ++i){\n        if(isalpha(s[i])){\n            result = _mm_insert_epi8(result, toupper(s[i]), i / 8);\n        } else {\n            result = _mm_insert_epi8(result, s[i], i / 8);\n        }\n    }\n\n    std::string res;\n    for(int i = 0; i < n; ++i){\n        res.push_back((char)_mm_extract_epi8(result, i));\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(const std::string & s){\n    const int len = s.length();\n    __m256i mask = _mm_setzero_si256(); // Initialize mask with zeros\n\n    for(int i = 0; i < len; i += 4){\n        __m256i vec = _mm_loadu_si256((__m256i*)&s[i]); // Load 4 characters into a vector\n        __m256i reversed_vec = _mm_xor_si256(vec, mask); // Reverse case for each character in the vector\n        _mm_storeu_si256((__m256i*)&s[i], reversed_vec); // Store the reversed vector back into the string\n        mask = _mm_cmpgt_epi8(mask, _mm_set1_epi8(0)); // Update mask to check if there are still letters left to process\n    }\n\n    while(len % 4 != 0){ // Process remaining characters\n        char c = s[len - 1];\n        if(isalpha(c)){\n            if(islower(c)) c = toupper(c);\n            else c = tolower(c);\n        }\n        s[--len] = c;\n    }\n\n    return s;\n}\n```"}
{"task_id": "SimdBench_77_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <string>\n\nstd::string solve_simd(const std::string & s){\n    const int len = s.length();\n    __m256i mask = _mm_setzero_si256();\n    __m256i result = _mm_setzero_si256();\n\n    for(int i = 0; i < len; i += 8){\n        __m256i word = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i lower_mask = _mm_cmplt_epi8(word, _mm_set1_epi8('Z'));\n        __m256i upper_mask = _mm_cmpgt_epi8(word, _mm_set1_epi8('z'));\n\n        __m256i lower_result = _mm_and_si256(lower_mask, _mm_xor_si256(word, _mm_set1_epi8(' ')));\n        __m256i upper_result = _mm_and_si256(upper_mask, _mm_xor_si256(word, _mm_set1_epi8('-')));\n\n        result = _mm_or_si256(result, lower_result);\n        result = _mm_or_si256(result, upper_result);\n    }\n\n    for(int i = len % 8; i < len; ++i){\n        if(s[i] >= 'a' && s[i] <= 'z'){\n            result = _mm_insert_epi8(result, s[i], i / 8);\n        } else if(s[i] >= 'A' && s[i] <= 'Z'){\n            result = _mm_insert_epi8(result, s[i] + 32, i / 8);\n        }\n    }\n\n    std::string res;\n    for(int i = 0; i < len; ++i){\n        res.push_back(static_cast<char>(_mm_extract_epi8(result, i)));\n    }\n\n    return res;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    if (n % 3 != 0) {\n        throw std::invalid_argument(\"Vector size must be divisible by 3\");\n    }\n\n    __m256i* ptr = (__m256i*)l.data();\n    __m256i* end = ptr + n / 3;\n\n    // Sort each group of 3 elements\n    while (ptr < end) {\n        __m256i v1 = _mm256_loadu_si256(ptr);\n        __m256i v2 = _mm256_loadu_si256(ptr + 1);\n        __m256i v3 = _mm256_loadu_si256(ptr + 2);\n\n        __m256i sorted_v1 = _mm256_sort_ps(v1, v2, v3);\n        _mm256_storeu_si256(ptr, sorted_v1);\n\n        ptr += 3;\n    }\n\n    // Copy remaining elements\n    for (; ptr < end; ++ptr) {\n        *ptr = l[*ptr];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i* ptr = (__m256i*)l.data();\n\n    // Sort every third element in parallel\n    for (int i = 0; i < n / 3; ++i) {\n        __m256i vec = _mm256_loadu_si256(ptr + i * 3);\n        __m256i sorted_vec = _mm256_sort_epi32(vec, _MM_SORT_DESCENDING);\n        _mm256_storeu_si256(ptr + i * 3, sorted_vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i* ptr = (__m256i*)l.data();\n\n    // Sort every third element in parallel\n    for (int i = 0; i < n / 3; ++i) {\n        __m256i vec = _mm256_loadu_si256(ptr + i * 3);\n        std::sort(_m256i_cast_ptr<int>(vec), _m256i_cast_ptr<int>(vec) + 3);\n        _mm256_storeu_si256(ptr + i * 3, vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    int n = l.size();\n    if (n % 3 != 0) {\n        throw std::invalid_argument(\"Vector size must be divisible by 3\");\n    }\n\n    __m256i* ptr = (__m256i*)l.data();\n    __m256i* end = ptr + n / 3;\n\n    while (ptr < end) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        __m256i sorted_vec = _mm256_sort_ps(vec, _MM_SORT_DESCENDING);\n        _mm256_storeu_si256(ptr, sorted_vec);\n        ptr += 3;\n    }\n\n    // Copy remaining elements from the last group\n    if (n % 3 == 2) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        __m256i sorted_vec = _mm256_sort_ps(vec, _MM_SORT_DESCENDING);\n        _mm256_storeu_si256(ptr, sorted_vec);\n    } else if (n % 3 == 1) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        __m256i sorted_vec = _mm256_sort_ps(vec, _MM_SORT_DESCENDING);\n        _mm256_storeu_si256(ptr, sorted_vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_78_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> sort_third_simd(std::vector<int> l){\n    __m256i *ptr = (__m256i *)l.data();\n    int n = l.size();\n\n    // Sort every third element in parallel\n    for (int i = 0; i < n / 3; ++i) {\n        __m256i vec = _mm256_loadu_si256(ptr + i * 3);\n        __m256i sorted_vec = _mm256_sort_ps(vec, _MM_SORT_DESCENDING);\n        _mm256_storeu_si256(ptr + i * 3, sorted_vec);\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    if(l.empty()) return std::numeric_limits<float>::lowest();\n\n    __m256 v = _mm256_loadu_ps(&l[0]);\n    float max_val = _mm256_extractf32_ps(v, 0);\n\n    for(size_t i = 4; i < l.size(); i += 4){\n        v = _mm256_loadu_ps(&l[i]);\n        max_val = fmaxf(max_val, _mm256_extractf32_ps(v, 0));\n    }\n\n    for(size_t i = l.size() % 4; i < l.size(); ++i){\n        max_val = fmaxf(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    if(l.empty()) return std::numeric_limits<float>::lowest();\n    \n    __m256 v = _mm_loadu_ps(&l[0]);\n    __m256 max_v = v;\n    \n    for(size_t i = 1; i < l.size(); i += 4){\n        v = _mm_loadu_ps(&l[i]);\n        max_v = _mm_max_ps(max_v, v);\n    }\n    \n    float max_val = 0.0f;\n    for(int i = 0; i < 4; ++i){\n        max_val = std::max(max_val, _mm_cvtss_f32(_mm_extract_ps(max_v, i)));\n    }\n    \n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    if(l.empty()) return std::numeric_limits<float>::lowest();\n\n    __m256 v = _mm256_loadu_ps(&l[0]);\n    float max_val = _mm256_extractf32_ps(v, 0);\n\n    for(size_t i = 4; i < l.size(); i += 4){\n        v = _mm256_loadu_ps(&l[i]);\n        max_val = fmaxf(max_val, _mm256_extractf32_ps(v, 0));\n    }\n\n    for(size_t i = l.size() % 4; i < l.size(); ++i){\n        max_val = fmaxf(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    if(l.empty()) return std::numeric_limits<float>::lowest();\n\n    __m256 v = _mm256_loadu_ps(&l[0]);\n    float max_val = _mm256_extractf32_ps(v, 0);\n\n    for(size_t i = 4; i < l.size(); i += 4){\n        v = _mm256_loadu_ps(&l[i]);\n        max_val = fmax(max_val, _mm256_extractf32_ps(v, 0));\n    }\n\n    for(size_t i = l.size() % 4; i < l.size(); ++i){\n        max_val = fmax(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_79_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nfloat max_element_simd(std::vector<float> l){\n    if(l.empty()) return std::numeric_limits<float>::lowest();\n\n    __m256 v = _mm256_loadu_ps(&l[0]);\n    float max_val = _mm256_extractf32_ps(v, 0);\n\n    for(size_t i = 4; i < l.size(); i += 4){\n        v = _mm256_loadu_ps(&l[i]);\n        max_val = fmax(max_val, _mm256_extractf32_ps(v, 0));\n    }\n\n    for(size_t i = l.size() % 4; i < l.size(); ++i){\n        max_val = fmax(max_val, l[i]);\n    }\n\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\n__attribute__((target(\"avx2\")))\nint fizz_buzz_simd(int n) {\n    int count = 0;\n    __m256i mask = _mm_set_epi32(1, 1, 1, 1);\n    __m256i num = _mm_set_epi32(0, 0, 0, 0);\n\n    for (int i = 0; i < n; i += 4) {\n        num = _mm_loadu_si256((__m256i*)&i);\n        num = _mm_and_si256(num, mask);\n        num = _mm_or_si256(num, _mm_cmpeq_epi32(num, _mm_set_epi32(7, 7, 7, 7)));\n        count += _mm_popcnt_u32(_mm_extract_epi32(num, 0)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(num, 1)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(num, 2)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(num, 3));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    __m256i mask = _mm_set_epi32(0, 0, 0, 0);\n    int count = 0;\n\n    // Loop through all numbers from 1 to n-1\n    for (int i = 1; i < n; ++i) {\n        // Check if the number is divisible by 11 or 13\n        if ((i % 11 == 0 || i % 13 == 0)) {\n            // Increment the count if the digit 7 appears in the number\n            count += _mm_popcnt_u32(i & 0xFF) + _mm_popcnt_u32((i >> 8) & 0xFF) +\n                      _mm_popcnt_u32((i >> 16) & 0xFF) + _mm_popcnt_u32((i >> 24) & 0xFF);\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    __m256i mask = _mm_set_epi32(0, 0, 0, 0);\n    int count = 0;\n\n    while (n > 0) {\n        if ((n % 11 == 0 || n % 13 == 0) && _mm_test_all_zeros(mask)) {\n            count++;\n        }\n        n--;\n        mask = _mm_add_epi32(mask, _mm_set_epi32(n / 11, n / 13, n / 11, n / 13));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n){\n    __m256i mask = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    int count = 0;\n\n    while (n > 0) {\n        if ((n % 11 == 0 || n % 13 == 0) && _mm256_testz_si256(mask, _mm256_set_epi32(n & 0xFF, n >> 8 & 0xFF, n >> 16 & 0xFF, n >> 24 & 0xFF))) {\n            count++;\n        }\n        n--;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_80_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint fizz_buzz_simd(int n) {\n    __m256i mask = _mm_set_epi32(0, 0, 0, 0);\n    int count = 0;\n\n    while (n--) {\n        if ((n % 11 == 0 || n % 13 == 0) && _mm_test_all_ones(mask)) {\n            count++;\n        }\n        mask = _mm_slli_si256(mask, 4);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    if(l.size() <= 1) return l;\n\n    // Create a temporary vector to store the sorted even indices\n    std::vector<float> temp(l.size());\n    std::copy_if(l.begin(), l.end(), temp.begin(), [](float x){ return x % 2 == 0; });\n\n    // Sort the temporary vector\n    std::sort(temp.begin(), temp.end());\n\n    // Copy the sorted elements back to the original vector\n    for(size_t i = 0; i < temp.size(); ++i){\n        l[i * 2] = temp[i];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    if(l.size() <= 1) return l;\n\n    __m256* ptr = (__m256*)l.data();\n    int n = l.size();\n\n    // Sort each chunk of 8 elements\n    for(int i = 0; i < n / 8; ++i){\n        __m256 v = _mm256_loadu_ps(ptr + i * 8);\n        __m256 sorted_v = _mm256_sort_ps(v, _MM_FROUND_TO_NEAREST_INT_PIO_2);\n        _mm256_storeu_ps(ptr + i * 8, sorted_v);\n    }\n\n    // Merge the sorted chunks\n    for(int i = 0; i < n - 7; i += 8){\n        __m256 v1 = _mm256_loadu_ps(ptr + i);\n        __m256 v2 = _mm256_loadu_ps(ptr + i + 4);\n        __m256 sorted_v = _mm256_min_ps(_mm256_max_ps(v1, v2), _mm256_set1_ps(FLT_MAX));\n        _mm256_storeu_ps(ptr + i, sorted_v);\n    }\n\n    // Handle remaining elements\n    for(int i = n - 7; i < n; ++i){\n        float min_val = FLT_MAX;\n        for(int j = i; j < n; ++j){\n            if(l[j] < min_val){\n                min_val = l[j];\n            }\n        }\n        l[i] = min_val;\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    if(l.size() <= 1) return l;\n\n    // Create two vectors: one with even indices and another with odd indices\n    std::vector<float> even_indices;\n    std::vector<float> odd_indices;\n    for(size_t i = 0; i < l.size(); ++i){\n        if(i % 2 == 0){\n            even_indices.push_back(l[i]);\n        } else {\n            odd_indices.push_back(l[i]);\n        }\n    }\n\n    // Sort the even indices vector using AVX/AVX2\n    __m256 v_even = _mm256_loadu_ps(even_indices.data());\n    __m256 sorted_v_even = _mm256_sort_ps(v_even);\n    _mm256_storeu_ps(even_indices.data(), sorted_v_even);\n\n    // Merge the sorted even indices vector back into the original vector\n    size_t j = 0;\n    for(size_t i = 0; i < l.size(); ++i){\n        if(i % 2 == 0){\n            l[i] = even_indices[j++];\n        } else {\n            l[i] = odd_indices[i / 2];\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    if(l.size() <= 1) return l;\n\n    // Create a temporary vector to hold the sorted even indices\n    std::vector<float> temp(l.size());\n    \n    // Copy the even indices from l to temp\n    for(size_t i = 0; i < l.size(); i += 2){\n        temp[i / 2] = l[i];\n    }\n\n    // Sort the temp vector\n    std::sort(temp.begin(), temp.end());\n\n    // Copy the sorted even indices back to l\n    for(size_t i = 0; i < l.size(); i += 2){\n        l[i] = temp[i / 2];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_81_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<float> sort_even_simd(std::vector<float> l){\n    if(l.size() <= 1) return l;\n\n    // Create a temporary vector to store the sorted even indices\n    std::vector<float> temp(l.size());\n    int even_index = 0;\n\n    // Copy the even-indexed elements from l to temp\n    for(int i = 0; i < l.size(); i += 2){\n        temp[even_index++] = l[i];\n    }\n\n    // Sort the temporary vector\n    std::sort(temp.begin(), temp.end());\n\n    // Copy the sorted even-indexed elements back to l\n    for(int i = 0; i < l.size(); i += 2){\n        l[i] = temp[i / 2];\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i sum = _mm256_add_epi32(_mm256_add_epi32(vec, _mm256_shuffle_epi32(vec, vec, 0b01)), _mm256_shuffle_epi32(vec, vec, 0b10));\n        __m256i diff = _mm256_sub_epi32(sum, _mm256_set1_epi32(0));\n\n        v0 = _mm256_or_si256(v0, _mm256_cmpgt_epi32(diff, _mm256_setzero_si256()));\n        v1 = _mm256_or_si256(v1, _mm256_cmpgt_epi32(diff, _mm256_set1_epi32(n)));\n        v2 = _mm256_or_si256(v2, _mm256_cmpgt_epi32(diff, _mm256_set1_epi32(-n)));\n    }\n\n    bool result = _mm256_movemask_epi8(v0) | _mm256_movemask_epi8(v1) | _mm256_movemask_epi8(v2);\n    return result != 0;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n-2; i += 3){\n        __m256i a = _mm256_loadu_si256((__m256i*)&l[i]);\n        __m256i b = _mm256_loadu_si256((__m256i*)&l[i+1]);\n        __m256i c = _mm256_loadu_si256((__m256i*)&l[i+2]);\n\n        __m256i sum = _mm256_add_epi32(_mm256_add_epi32(a, b), c);\n        __m256i mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n        __m256i result = _mm256_and_si256(mask, _mm256_xor_si256(v0, a));\n        v0 = _mm256_or_si256(v0, result);\n\n        mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n        result = _mm256_and_si256(mask, _mm256_xor_si256(v1, b));\n        v1 = _mm256_or_si256(v1, result);\n\n        mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n        result = _mm256_and_si256(mask, _mm256_xor_si256(v2, c));\n        v2 = _mm256_or_si256(v2, result);\n    }\n\n    bool found = false;\n    if(_mm256_movemask_epi8(v0) != 0 || _mm256_movemask_epi8(v1) != 0 || _mm256_movemask_epi8(v2) != 0)\n        found = true;\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n-2; i += 3){\n        __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i+1]));\n        __m256i c = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i+2]));\n\n        __m256 sum = _mm256_add_epi32(_mm256_add_epi32(a, b), c);\n        __m256 mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n        __m256 result = _mm256_and_si256(mask, _mm256_xor_si256(a, b));\n        __m256 xor_result = _mm256_xor_si256(result, c);\n\n        v0 = _mm256_or_si256(v0, xor_result);\n        v1 = _mm256_or_si256(v1, xor_result);\n        v2 = _mm256_or_si256(v2, xor_result);\n    }\n\n    __m256i final_result = _mm256_or_si256(v0, _mm256_or_si256(v1, v2));\n\n    return _mm256_movemask_epi8(final_result) != 0;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i v0 = _mm256_setzero_si256();\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n-2; i += 3){\n        __m256i a = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        __m256i b = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i+1]));\n        __m256i c = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[i+2]));\n\n        __m256i sum = _mm256_add_epi32(_mm256_add_epi32(a, b), c);\n        __m256i mask = _mm256_cmpneq_epi32(sum, v0);\n\n        v0 = _mm256_or_si256(v0, mask);\n        v1 = _mm256_or_si256(v1, mask);\n        v2 = _mm256_or_si256(v2, mask);\n    }\n\n    // Check the last two elements separately\n    if(n >= 2 && _mm256_testz_si256(v0, v0)){\n        return true;\n    }\n    if(n >= 3 && _mm256_testz_si256(v1, v1)){\n        return true;\n    }\n    if(n >= 4 && _mm256_testz_si256(v2, v2)){\n        return true;\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_82_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool triples_sum_to_zero_simd(std::vector<int> l){\n    const int n = l.size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i + 7 < n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        sum = _mm256_add_epi32(sum, vec);\n    }\n\n    for(int i = n - 7; i >= 0; i -= 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&l[i]);\n        sum = _mm256_add_epi32(sum, vec);\n    }\n\n    __m256i mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n    __m256i count = _mm256_popcnt_u32(_mm256_movemask_epi8(mask));\n\n    return count > 0;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    const int n = l.size();\n    __m256i* ptr = (__m256i*)l.data();\n\n    if (n % 8 != 0) {\n        // Handle remaining elements\n        for (int i = n - n % 8; i < n; ++i) {\n            ptr[i] = _mm_add_epi32(ptr[i], _mm_set1_epi32(1));\n        }\n    } else {\n        // Process in chunks of 8 elements\n        for (int i = 0; i < n; i += 8) {\n            ptr[i] = _mm_add_epi32(ptr[i], _mm_set1_epi32(1));\n            ptr[i + 1] = _mm_add_epi32(ptr[i + 1], _mm_set1_epi32(1));\n            ptr[i + 2] = _mm_add_epi32(ptr[i + 2], _mm_set1_epi32(1));\n            ptr[i + 3] = _mm_add_epi32(ptr[i + 3], _mm_set1_epi32(1));\n            ptr[i + 4] = _mm_add_epi32(ptr[i + 4], _mm_set1_epi32(1));\n            ptr[i + 5] = _mm_add_epi32(ptr[i + 5], _mm_set1_epi32(1));\n            ptr[i + 6] = _mm_add_epi32(ptr[i + 6], _mm_set1_epi32(1));\n            ptr[i + 7] = _mm_add_epi32(ptr[i + 7], _mm_set1_epi32(1));\n        }\n    }\n\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int n = l.size();\n    if (n <= 8) {\n        for (int i = 0; i < n; ++i) {\n            l[i]++;\n        }\n    } else {\n        __m256 v = _mm256_loadu_si256(reinterpret_cast<const __m256*>(&l[0]));\n        v = _mm256_add_epi32(v, _mm256_set1_epi32(1));\n        _mm256_storeu_si256(reinterpret_cast<__m256*>(&l[0]), v);\n        int remaining = n % 8;\n        for (int i = 0; i < remaining; ++i) {\n            l[i]++;\n        }\n    }\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l){\n    int n = l.size();\n    if (n <= 8) {\n        for (int i = 0; i < n; ++i) {\n            l[i] += 1;\n        }\n    } else {\n        __m256i *ptr = (__m256i *)l.data();\n        __m256i mask = _mm_set_epi32(0, 0, 0, -1);\n        while (n >= 32) {\n            __m256i v = _mm_loadu_si256(ptr);\n            v = _mm_add_epi32(v, mask);\n            _mm_storeu_si256(ptr, v);\n            ptr += 4;\n            n -= 32;\n        }\n        if (n >= 16) {\n            __m128i *ptr = (__m128i *)(ptr + 4);\n            __m128i mask = _mm_set_epi32(0, -1);\n            while (n >= 16) {\n                __m128i v = _mm_loadu_si128(ptr);\n                v = _mm_add_epi32(v, mask);\n                _mm_storeu_si128(ptr, v);\n                ptr += 2;\n                n -= 16;\n            }\n        }\n        if (n >= 8) {\n            __m128i *ptr = (__m128i *)(ptr + 2);\n            __m128i mask = _mm_set_epi32(-1);\n            while (n >= 8) {\n                __m128i v = _mm_loadu_si128(ptr);\n                v = _mm_add_epi32(v, mask);\n                _mm_storeu_si128(ptr, v);\n                ptr += 1;\n                n -= 8;\n            }\n        }\n        if (n >= 4) {\n            __m128i *ptr = (__m128i *)(ptr + 1);\n            __m128i mask = _mm_set_epi32(-1);\n            while (n >= 4) {\n                __m128i v = _mm_loadu_si128(ptr);\n                v = _mm_add_epi32(v, mask);\n                _mm_storeu_si128(ptr, v);\n                ptr += 1;\n                n -= 4;\n            }\n        }\n        if (n >= 2) {\n            __m128i *ptr = (__m128i *)(ptr + 1);\n            __m128i mask = _mm_set_epi32(-1);\n            while (n >= 2) {\n                __m128i v = _mm_loadu_si128(ptr);\n                v = _mm_add_epi32(v, mask);\n                _mm_storeu_si128(ptr, v);\n                ptr += 1;\n                n -= 2;\n            }\n        }\n        if (n >= 1) {\n            __m128i *ptr = (__m128i *)(ptr + 1);\n            __m128i mask = _mm_set_epi32(-1);\n            while (n >= 1) {\n                __m128i v = _mm_loadu_si128(ptr);\n                v = _mm_add_epi32(v, mask);\n                _mm_storeu_si128(ptr, v);\n                ptr += 1;\n                n -= 1;\n            }\n        }\n    }\n    return l;\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    int n = l.size();\n    if (n <= 8) {\n        // Use scalar loop for small vectors\n        std::vector<int> result(n);\n        for (int i = 0; i < n; ++i) {\n            result[i] = l[i] + 1;\n        }\n        return result;\n    } else {\n        // Use AVX/AVX2 intrinsics for larger vectors\n        __m256i *ptr = (__m256i *)l.data();\n        __m256i *end = ptr + (n / 8);\n        __m256i *result_ptr = (__m256i *)(l.data() + n);\n\n        while (ptr != end) {\n            __m256i sum = _mm256_add_epi32(*ptr, _mm256_set1_epi32(1));\n            _mm_storeu_si256(result_ptr, sum);\n            ptr += 8;\n            result_ptr += 8;\n        }\n\n        // Handle remaining elements\n        for (; ptr < end; ++ptr) {\n            *result_ptr = (*ptr) + 1;\n            ++result_ptr;\n        }\n\n        return l;\n    }\n}\n```"}
{"task_id": "SimdBench_83_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> incr_list_simd(std::vector<int> l) {\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[0]));\n    vec = _mm256_add_epi32(vec, _mm256_set1_epi32(1));\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&l[0]), vec);\n    return l;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i *v = (__m256i *)l.data();\n    __m256i sum;\n    bool found = false;\n\n    // Process each pair of elements in parallel\n    for (int i = 0; i < n - 1; i += 2) {\n        sum = _mm256_add_epi32(_mm256_loadu_si256(v + i), _mm256_loadu_si256(v + i + 1));\n        if (_mm256_movemask_epi8(_mm256_cmp_eq_epi32(sum, _mm_setzero_si256())) != 0) {\n            found = true;\n            break;\n        }\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i *a = (__m256i*)l.data();\n    __m256i *b = (__m256i*)(l.data() + n / 4);\n    __m256i *c = (__m256i*)(l.data() + 2 * n / 4);\n    __m256i *d = (__m256i*)(l.data() + 3 * n / 4);\n\n    bool found = false;\n    for (int i = 0; i < n / 4; ++i) {\n        __m256i sum = _mm_add_epi32(a[i], b[i]);\n        __m256i neg_sum = _mm_sub_epi32(sum, c[i]);\n        __m256i diff = _mm_sub_epi32(d[i], neg_sum);\n        __m256i mask = _mm_cmpneq_epi32(diff, _mm_setzero_si256());\n        found |= _mm_movemask_epi8(mask) != 0;\n    }\n\n    return found;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i v = _mm256_setzero_si256(); // Initialize a zero vector\n    for(int i = 0; i < n; i++){\n        __m256i val = _mm256_loadu_si256((__m256i*)&l[i]); // Load the current element into a vector\n        __m256i diff = _mm256_sub_epi32(v, val); // Compute the difference between the zero vector and the current element\n        __m256i mask = _mm256_cmpneq_epi32(diff, _mm256_setzero_si256()); // Create a mask indicating which elements differ from the zero vector\n        v = _mm256_or_si256(v, mask); // Update the zero vector with the differences\n    }\n    return _mm_testz_si256(v, v); // Check if all elements in the zero vector are zero\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i *vec = (__m256i *)l.data();\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < n-3; i+=4){\n        __m256i v1 = _mm256_loadu_si256(&vec[i]);\n        __m256i v2 = _mm256_loadu_si256(&vec[i+1]);\n        __m256i v3 = _mm256_loadu_si256(&vec[i+2]);\n        __m256i v4 = _mm256_loadu_si256(&vec[i+3]);\n\n        sum = _mm256_add_epi32(sum, v1);\n        sum = _mm256_add_epi32(sum, v2);\n        sum = _mm256_add_epi32(sum, v3);\n        sum = _mm256_add_epi32(sum, v4);\n\n        __m256i neg_v1 = _mm256_sub_epi32(_mm256_setzero_si256(), v1);\n        __m256i neg_v2 = _mm256_sub_epi32(_mm256_setzero_si256(), v2);\n        __m256i neg_v3 = _mm256_sub_epi32(_mm256_setzero_si256(), v3);\n        __m256i neg_v4 = _mm256_sub_epi32(_mm256_setzero_si256(), v4);\n\n        sum = _mm256_or_si256(sum, neg_v1);\n        sum = _mm256_or_si256(sum, neg_v2);\n        sum = _mm256_or_si256(sum, neg_v3);\n        sum = _mm256_or_si256(sum, neg_v4);\n\n        __m256i mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n        __m256i result = _mm256_and_si256(mask, _mm256_cmpeq_epi32(v1, v2));\n        result = _mm256_or_si256(result, _mm256_cmpeq_epi32(v1, v3));\n        result = _mm256_or_si256(result, _mm256_cmpeq_epi32(v1, v4));\n        result = _mm256_or_si256(result, _mm256_cmpeq_epi32(v2, v3));\n        result = _mm256_or_si256(result, _mm256_cmpeq_epi32(v2, v4));\n        result = _mm256_or_si256(result, _mm256_cmpeq_epi32(v3, v4));\n\n        if(_mm256_movemask_epi8(result) != 0){\n            return true;\n        }\n    }\n\n    for(int i = n-3; i < n; i++){\n        __m256i v1 = _mm256_loadu_si256(&vec[i]);\n        __m256i neg_v1 = _mm256_sub_epi32(_mm256_setzero_si256(), v1);\n        sum = _mm256_or_si256(sum, neg_v1);\n\n        __m256i mask = _mm256_cmpgt_epi32(sum, _mm256_setzero_si256());\n        __m256i result = _mm256_and_si256(mask, _mm256_cmpeq_epi32(v1, v1));\n        if(_mm256_movemask_epi8(result) != 0){\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_84_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool pairs_sum_to_zero_simd(std::vector<int> l){\n    int n = l.size();\n    __m256i v = _mm256_set_epi32(l[0], l[1], l[2], l[3]);\n    __m256i v_neg = _mm256_sub_epi32(_mm256_set_epi32(0, 0, 0, 0), v);\n    __m256i v_sum = _mm256_add_epi32(v, v_neg);\n\n    for(int i = 4; i < n; i += 4){\n        __m256i v_i = _mm256_set_epi32(l[i], l[i+1], l[i+2], l[i+3]);\n        __m256i v_i_neg = _mm256_sub_epi32(_mm256_set_epi32(0, 0, 0, 0), v_i);\n        __m256i v_i_sum = _mm256_add_epi32(v_i, v_i_neg);\n\n        __m256i mask = _mm256_cmpneq_epi32(v_sum, v_i_sum);\n        __m256i result = _mm256_and_si256(mask, _mm256_set_epi32(1, 1, 1, 1));\n        if(_mm256_movemask_epi8(result) != 0){\n            return true;\n        }\n    }\n\n    return false;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base) {\n    if (base >= 10 || base <= 1) {\n        throw std::invalid_argument(\"Base must be between 2 and 9.\");\n    }\n\n    std::vector<int8_t> digits;\n    while (x > 0) {\n        digits.push_back(x % base);\n        x /= base;\n    }\n\n    std::reverse(digits.begin(), digits.end());\n    std::string result = \"\";\n    for (int8_t digit : digits) {\n        result += static_cast<char>(digit + '0');\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if(base >= 10 || base <= 1) return \"Invalid base\";\n\n    std::vector<int> digits;\n    while(x > 0){\n        digits.push_back(x % base);\n        x /= base;\n    }\n\n    std::reverse(digits.begin(), digits.end());\n    std::string result = \"\";\n    for(auto digit : digits){\n        result += std::to_string(digit);\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if(base == 10) return std::to_string(x);\n    \n    std::string result;\n    __m256i x_vec = _mm256_set_epi64x((int64_t)x >> 32, (int64_t)x & 0xFFFFFFFF);\n    __m256i base_vec = _mm256_set_epi8(base - 1, base - 1, base - 1, base - 1, base - 1, base - 1, base - 1, base - 1);\n    \n    while(_mm256_movemask_epi8(_mm256_cmpeq_epi64(x_vec, _mm256_setzero_si256())) != 0) {\n        __m256i remainder_vec = _mm256_div_epi64(x_vec, base_vec);\n        __m256i digit_vec = _mm256_sub_epi8(_mm256_set_epi8(9, 9, 9, 9, 9, 9, 9, 9), remainder_vec);\n        __m256i digit_mask = _mm256_cmpgt_epi8(digit_vec, _mm256_setzero_si256());\n        \n        for(int i = 0; i < 8; ++i) {\n            if(digit_mask[i]) {\n                result += \"0123456789\"[digit_vec[i]];\n            }\n        }\n        \n        x_vec = _mm256_mullo_epi64(x_vec, base_vec);\n    }\n    \n    return result + std::to_string(x % base);\n}\n```"}
{"task_id": "SimdBench_85_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if(base >= 10 || base <= 1){\n        throw std::invalid_argument(\"Base must be between 2 and 9.\");\n    }\n\n    const int8_t max_digit = base - 1;\n    const int8_t num_bits = sizeof(x) * 8;\n\n    __m256i result = _mm_setzero_si256();\n    __m256i mask = _mm_set1_epi8(max_digit);\n\n    while(num_bits > 0){\n        __m256i bits = _mm_srl_epi64(_mm_loadu_si64(&x), num_bits - 1);\n        __m256i digit = _mm_and_si256(bits, mask);\n        __m256i remainder = _mm_sub_epi64(bits, digit);\n        __m256i next_digit = _mm_add_epi64(digit, remainder);\n        __m256i next_remainder = _mm_sub_epi64(next_digit, remainder);\n        __m256i next_next_digit = _mm_add_epi64(next_digit, next_remainder);\n        __m256i next_next_remainder = _mm_sub_epi64(next_next_digit, next_remainder);\n        __m256i next_next_next_digit = _mm_add_epi64(next_next_digit, next_next_remainder);\n        __m256i next_next_next_remainder = _mm_sub_epi64(next_next_next_digit, next_next_remainder);\n        __m256i next_next_next_next_digit = _mm_add_epi64(next_next_next_digit, next_next_next_remainder);\n        __m256i next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_remainder);\n        __m256i next_next_next_next_next_digit = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_add_epi64(next_next_next_next_digit, next_next_next_next_remainder);\n        __m256i next_next_next_next_next_remainder = _mm_sub_epi6"}
{"task_id": "SimdBench_85_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\nstd::string change_base_simd(int64_t x, int8_t base){\n    if(base == 10) return std::to_string(x);\n    \n    std::vector<int> digits;\n    while(x > 0){\n        digits.push_back(x % base);\n        x /= base;\n    }\n    \n    std::reverse(digits.begin(), digits.end());\n    \n    std::string result = \"\";\n    for(auto digit : digits){\n        result += std::to_string(digit);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text){\n    int n = text.length();\n    __m256i mask = _mm_set1_epi8(0);\n    __m256i result = _mm_setzero_si256();\n\n    for(int i = 0; i < n / 4; ++i){\n        __m256i word = _mm_loadu_si256((__m256i*)&text[i * 4]);\n        __m256i reversed_word = _mm_shuffle_epi8(word, word);\n\n        result = _mm_or_si256(result, _mm_cmpeq_epi8(word, reversed_word));\n    }\n\n    for(int i = n % 4; i < n; ++i){\n        char c = text[i];\n        result = _mm_or_si256(result, _mm_cmpneq_epi8(_mm_set1_epi8(c), _mm_set1_epi8(text[n - i - 1])));\n    }\n\n    return _mm_movemask_epi8(result) == 0;\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text){\n    int n = text.length();\n    __m256i vec1 = _mm_setzero_si256();\n    __m256i vec2 = _mm_setzero_si256();\n\n    for(int i = 0; i < n / 4; ++i){\n        __m256i word = _mm_loadu_si256((__m256i*)&text[i * 4]);\n        vec1 = _mm_or_si256(vec1, word);\n        vec2 = _mm_xor_si256(vec2, word);\n    }\n\n    // Handle remaining characters\n    for(int i = n % 4; i < n; ++i){\n        vec1 = _mm_or_si256(vec1, _mm_loadu_si32((int*)&text[i]));\n        vec2 = _mm_xor_si256(vec2, _mm_loadu_si32((int*)&text[i]));\n    }\n\n    // Check if all bits in vec1 are zero\n    return _mm_testz_si256(vec1, vec1);\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text){\n    int len = text.length();\n    __m256i mask = _mm_set1_epi8(0xFF);\n    __m256i result = _mm_setzero_si256();\n\n    for(int i = 0; i < len / 4; ++i){\n        __m256i word = _mm_loadu_si256((__m256i*)&text[i * 4]);\n        __m256i reversed_word = _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle_epi8(word, _mm_shuffle"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text){\n    int len = text.length();\n    if(len <= 1)\n        return true;\n\n    __m256i mask = _mm_setzero_si256();\n    __m256i left = _mm_loadu_si256((__m256i*)&text[0]);\n    __m256i right = _mm_loadu_si256((__m256i*)&text[len-1]);\n\n    while(len > 32){\n        mask = _mm_cmpeq_epi8(left, right);\n        left = _mm_loadu_si256((__m256i*)&text[32]);\n        right = _mm_loadu_si256((__m256i*)&text[len-32]);\n        len -= 32;\n    }\n\n    mask = _mm_cmpeq_epi8(left, right);\n    left = _mm_loadu_si128((__m128i*)&text[32]);\n    right = _mm_loadu_si128((__m128i*)&text[len-32]);\n    mask = _mm_and_si128(mask, _mm_cmpneq_epi8(left, right));\n\n    return _mm_testz_si128(mask, mask);\n}\n```"}
{"task_id": "SimdBench_86_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_palindrome_simd(std::string text){\n    int n = text.length();\n    __m256i vec1, vec2;\n    int i;\n\n    // Load the first half of the string into vec1\n    vec1 = _mm256_loadu_si256((__m256i*)&text[0]);\n\n    // Loop through the string in pairs\n    for(i = 0; i < n / 4; i++){\n        // Load the next pair of characters into vec2\n        vec2 = _mm256_loadu_si256((__m256i*)&text[i * 4 + 2]);\n\n        // Compare the two pairs of characters\n        __m256 cmp = _mm256_cmpeq_epi8(vec1, vec2);\n\n        // If any pair does not match, return false\n        if(_mm256_movemask_epi8(cmp) != 0){\n            return false;\n        }\n\n        // Swap the halves of the string\n        vec1 = _mm256_xor_si256(vec1, vec2);\n    }\n\n    // Check the remaining characters\n    for(; i < n; i++){\n        if(text[i] != text[n - i - 1]){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd(std::string s){\n    __m256i v = _mm256_set1_epi8((int)'a');\n    __m256i v2 = _mm256_set1_epi8((int)'z');\n    __m256i v3 = _mm256_set1_epi8((int)'A');\n    __m256i v4 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v5 = _mm256_set1_epi8((int)'e');\n    __m256i v6 = _mm256_set1_epi8((int)'f');\n    __m256i v7 = _mm256_set1_epi8((int)'g');\n    __m256i v8 = _mm256_set1_epi8((int)'h');\n    __m256i v9 = _mm256_set1_epi8((int)'i');\n    __m256i v10 = _mm256_set1_epi8((int)'j');\n    __m256i v11 = _mm256_set1_epi8((int)'k');\n    __m256i v12 = _mm256_set1_epi8((int)'l');\n    __m256i v13 = _mm256_set1_epi8((int)'m');\n    __m256i v14 = _mm256_set1_epi8((int)'n');\n    __m256i v15 = _mm256_set1_epi8((int)'o');\n    __m256i v16 = _mm256_set1_epi8((int)'p');\n    __m256i v17 = _mm256_set1_epi8((int)'q');\n    __m256i v18 = _mm256_set1_epi8((int)'r');\n    __m256i v19 = _mm256_set1_epi8((int)'s');\n    __m256i v20 = _mm256_set1_epi8((int)'t');\n    __m256i v21 = _mm256_set1_epi8((int)'u');\n    __m256i v22 = _mm256_set1_epi8((int)'v');\n    __m256i v23 = _mm256_set1_epi8((int)'w');\n    __m256i v24 = _mm256_set1_epi8((int)'x');\n    __m256i v25 = _mm256_set1_epi8((int)'y');\n    __m256i v26 = _mm256_set1_epi8((int)'z');\n\n    __m256i v27 = _mm256_set1_epi8((int)'E');\n    __m256i v28 = _mm256_set1_epi8((int)'F');\n    __m256i v29 = _mm256_set1_epi8((int)'G');\n    __m256i v30 = _mm256_set1_epi8((int)'H');\n    __m256i v31 = _mm256_set1_epi8((int)'I');\n    __m256i v32 = _mm256_set1_epi8((int)'J');\n    __m256i v33 = _mm256_set1_epi8((int)'K');\n    __m256i v34 = _mm256_set1_epi8((int)'L');\n    __m256i v35 = _mm256_set1_epi8((int)'M');\n    __m256i v36 = _mm256_set1_epi8((int)'N');\n    __m256i v37 = _mm256_set1_epi8((int)'O');\n    __m256i v38 = _mm256_set1_epi8((int)'P');\n    __m256i v39 = _mm256_set1_epi8((int)'Q');\n    __m256i v40 = _mm256_set1_epi8((int)'R');\n    __m256i v41 = _mm256_set1_epi8((int)'S');\n    __m256i v42 = _mm256_set1_epi8((int)'T');\n    __m256i v43 = _mm256_set1_epi8((int)'U');\n    __m256i v44 = _mm256_set1_epi8((int)'V');\n    __m256i v45 = _mm256_set1_epi8((int)'W');\n    __m256i v46 = _mm256_set1_epi8((int)'X');\n    __m256i v47 = _mm256_set1_epi8((int)'Y');\n    __m256i v48 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v49 = _mm256_set1_epi8((int)'e');\n    __m256i v50 = _mm256_set1_epi8((int)'f');\n    __m256i v51 = _mm256_set1_epi8((int)'g');\n    __m256i v52 = _mm256_set1_epi8((int)'h');\n    __m256i v53 = _mm256_set1_epi8((int)'i');\n    __m256i v54 = _mm256_set1_epi8((int)'j');\n    __m256i v55 = _mm256_set1_epi8((int)'k');\n    __m256i v56 = _mm256_set1_epi8((int)'l');\n    __m256i v57 = _mm256_set1_epi8((int)'m');\n    __m256i v58 = _mm256_set1_epi8((int)'n');\n    __m256i v59 = _mm256_set1_epi8((int)'o');\n    __m256i v60 = _mm256_set1_epi8((int)'p');\n    __m256i v61 = _mm256_set1_epi8((int)'q');\n    __m256i v62 = _mm256_set1_epi8((int)'r');\n    __m256i v63 = _mm256_set1_epi8((int)'s');\n    __m256i v64 = _mm256_set1_epi8((int)'t');\n    __m256i v65 = _mm256_set1_epi8((int)'u');\n    __m256i v66 = _mm256_set1_epi8((int)'v');\n    __m256i v67 = _mm256_set1_epi8((int)'w');\n    __m256i v68 = _mm256_set1_epi8((int)'x');\n    __m256i v69 = _mm256_set1_epi8((int)'y');\n    __m256i v70 = _mm256_set1_epi8((int)'z');\n\n    __m256i v71 = _mm256_set1_epi8((int)'E');\n    __m256i v72 = _mm256_set1_epi8((int)'F');\n    __m256i v73 = _mm256_set1_epi8((int)'G');\n    __m256i v74 = _mm256_set1_epi8((int)'H');\n    __m256i v75 = _mm256_set1_epi8((int)'I');\n    __m256i v76 = _mm256_set1_epi8((int)'J');\n    __m256i v77 = _mm256_set1_epi8((int)'K');\n    __m256i v78 = _mm256_set1_epi8((int)'L');\n    __m256i v79 = _mm256_set1_epi8((int)'M');\n    __m256i v80 = _mm256_set1_epi8((int)'N');\n    __m256i v81 = _mm256_set1_epi8((int)'O');\n    __m256i v82 = _mm256_set1_epi8((int)'P');\n    __m256i v83 = _mm256_set1_epi8((int)'Q');\n    __m256i v84 = _mm256_set1_epi8((int)'R');\n    __m256i v85 = _mm256_set1_epi8((int)'S');\n    __m256i v86 = _mm256_set1_epi8((int)'T');\n    __m256i v87 = _mm256_set1_epi8((int)'U');\n    __m256i v88 = _mm256_set1_epi8((int)'V');\n    __m256i v89 = _mm256_set1_epi8((int)'W');\n    __m256i v90 = _mm256_set1_epi8((int)'X');\n    __m256i v91 = _mm256_set1_epi8((int)'Y');\n    __m256i v92 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v93 = _mm256_set1_epi8((int)'e');\n    __m256i v94 = _mm256_set1_epi8((int)'f');\n    __m256i v95 = _mm256_set1_epi8((int)'g');\n    __m256i v96 = _mm256_set1_epi8((int)'h');\n    __m256i v97 = _mm256_set1_epi8((int)'i');\n    __m256i v98 = _mm256_set1_epi8((int)'j');\n    __m256i v99 = _mm256_set1_epi8((int)'k');\n    __m256i v100 = _mm256_set1_epi8((int)'l');\n    __m256i v101 = _mm256_set1_epi8((int)'m');\n    __m256i v102 = _mm256_set1_epi8((int)'n');\n    __m256i v103 = _mm256_set1_epi8((int)'o');\n    __m256i v104 = _mm256_set1_epi8((int)'p');\n    __m256i v105 = _mm256_set1_epi8((int)'q');\n    __m256i v106 = _mm256_set1_epi8((int)'r');\n    __m256i v107 = _mm256_set1_epi8((int)'S');\n    __m256i v108 = _mm256_set1_epi8((int)'T');\n    __m256i v109 = _mm256_set1_epi8((int)'U');\n    __m256i v110 = _mm256_set1_epi8((int)'V');\n    __m256i v111 = _mm256_set1_epi8((int)'W');\n    __m256i v112 = _mm256_set1_epi8((int)'X');\n    __m256i v113 = _mm256_set1_epi8((int)'Y');\n    __m256i v114 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v115 = _mm256_set1_epi8((int)'e');\n    __m256i v116 = _mm256_set1_epi8((int)'f');\n    __m256i v117 = _mm256_set1_epi8((int)'g');\n    __m256i v118 = _mm256_set1_epi8((int)'h');\n    __m256i v119 = _mm256_set1_epi8((int)'i');\n    __m256i v120 = _mm256_set1_epi8((int)'j');\n    __m256i v121 = _mm256_set1_epi8((int)'k');\n    __m256i v122 = _mm256_set1_epi8((int)'l');\n    __m256i v123 = _mm256_set1_epi8((int)'m');\n    __m256i v124 = _mm256_set1_epi8((int)'n');\n    __m256i v125 = _mm256_set1_epi8((int)'o');\n    __m256i v126 = _mm256_set1_epi8((int)'p');\n    __m256i v127 = _mm256_set1_epi8((int)'q');\n    __m256i v128 = _mm256_set1_epi8((int)'r');\n    __m256i v129 = _mm256_set1_epi8((int)'S');\n    __m256i v130 = _mm256_set1_epi8((int)'T');\n    __m256i v131 = _mm256_set1_epi8((int)'U');\n    __m256i v132 = _mm256_set1_epi8((int)'V');\n    __m256i v133 = _mm256_set1_epi8((int)'W');\n    __m256i v134 = _mm256_set1_epi8((int)'X');\n    __m256i v135 = _mm256_set1_epi8((int)'Y');\n    __m256i v136 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v137 = _mm256_set1_epi8((int)'e');\n    __m256i v138 = _mm256_set1_epi8((int)'f');\n    __m256i v139 = _mm256_set1_epi8((int)'g');\n    __m256i v140 = _mm256_set1_epi8((int)'h');\n    __m256i v141 = _mm256_set1_epi8((int)'i');\n    __m256i v142 = _mm256_set1_epi8((int)'j');\n    __m256i v143 = _mm256_set1_epi8((int)'k');\n    __m256i v144 = _mm256_set1_epi8((int)'l');\n    __m256i v145 = _mm256_set1_epi8((int)'m');\n    __m256i v146 = _mm256_set1_epi8((int)'n');\n    __m256i v147 = _mm256_set1_epi8((int)'o');\n    __m256i v148 = _mm256_set1_epi8((int)'p');\n    __m256i v149 = _mm256_set1_epi8((int)'q');\n    __m256i v150 = _mm256_set1_epi8((int)'r');\n    __m256i v151 = _mm256_set1_epi8((int)'S');\n    __m256i v152 = _mm256_set1_epi8((int)'T');\n    __m256i v153 = _mm256_set1_epi8((int)'U');\n    __m256i v154 = _mm256_set1_epi8((int)'V');\n    __m256i v155 = _mm256_set1_epi8((int)'W');\n    __m256i v156 = _mm256_set1_epi8((int)'X');\n    __m256i v157 = _mm256_set1_epi8((int)'Y');\n    __m256i v158 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v159 = _mm256_set1_epi8((int)'e');\n    __m260 = _mm256_set1_epi8((int)'f');\n    __m261 = _mm256_set1_epi8((int)'g');\n    __m262 = _mm256_set1_epi8((int)'h');\n    __m263 = _mm256_set1_epi8((int)'i');\n    __m264 = _mm256_set1_epi8((int)'j');\n    __m265 = _mm256_set1_epi8((int)'k');\n    __m266 = _mm256_set1_epi8((int)'l');\n    __m267 = _mm256_set1_epi8((int)'m');\n    __m268 = _mm256_set1_epi8((int)'n');\n    __m269 = _mm256_set1_epi8((int)'o');\n    __m270 = _mm256_set1_epi8((int)'p');\n    __m271 = _mm256_set1_epi8((int)'q');\n    __m272 = _mm256_set1_epi8((int)'r');\n    __m273 = _mm256_set1_epi8((int)'S');\n    __m274 = _mm256_set1_epi8((int)'T');\n    __m275 = _mm256_set1_epi8((int)'U');\n    __m276 = _mm256_set1_epi8((int)'V');\n    __m277 = _mm256_set1_epi8((int)'W');\n    __m278 = _mm256_set1_epi8((int)'X');\n    __m279 = _mm256_set1_epi8((int)'Y');\n    __m280 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v281 = _mm256_set1_epi8((int)'e');\n    __m282 = _mm256_set1_epi8((int)'f');\n    __m283 = _mm256_set1_epi8((int)'g');\n    __m284 = _mm256_set1_epi8((int)'h');\n    __m285 = _mm256_set1_epi8((int)'i');\n    __m286 = _mm256_set1_epi8((int)'j');\n    __m287 = _mm256_set1_epi8((int)'k');\n    __m288 = _mm256_set1_epi8((int)'l');\n    __m289 = _mm256_set1_epi8((int)'m');\n    __m290 = _mm256_set1_epi8((int)'n');\n    __m291 = _mm256_set1_epi8((int)'o');\n    __m292 = _mm256_set1_epi8((int)'p');\n    __m293 = _mm256_set1_epi8((int)'q');\n    __m294 = _mm256_set1_epi8((int)'r');\n    __m295 = _mm256_set1_epi8((int)'S');\n    __m296 = _mm256_set1_epi8((int)'T');\n    __m297 = _mm256_set1_epi8((int)'U');\n    __m298 = _mm256_set1_epi8((int)'V');\n    __m299 = _mm256_set1_epi8((int)'W');\n    __m300 = _mm256_set1_epi8((int)'X');\n    __m301 = _mm256_set1_epi8((int)'Y');\n    __m302 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v303 = _mm256_set1_epi8((int)'e');\n    __m304 = _mm256_set1_epi8((int)'f');\n    __m305 = _mm256_set1_epi8((int)'g');\n    __m306 = _mm256_set1_epi8((int)'h');\n    __m307 = _mm256_set1_epi8((int)'i');\n    __m308 = _mm256_set1_epi8((int)'j');\n    __m309 = _mm256_set1_epi8((int)'k');\n    __m310 = _mm256_set1_epi8((int)'l');\n    __m311 = _mm256_set1_epi8((int)'m');\n    __m312 = _mm256_set1_epi8((int)'n');\n    __m313 = _mm256_set1_epi8((int)'o');\n    __m314 = _mm256_set1_epi8((int)'p');\n    __m315 = _mm256_set1_epi8((int)'q');\n    __m316 = _mm256_set1_epi8((int)'r');\n    __m317 = _mm256_set1_epi8((int)'S');\n    __m318 = _mm256_set1_epi8((int)'T');\n    __m319 = _mm256_set1_epi8((int)'U');\n    __m320 = _mm256_set1_epi8((int)'V');\n    __m321 = _mm256_set1_epi8((int)'W');\n    __m322 = _mm256_set1_epi8((int)'X');\n    __m323 = _mm256_set1_epi8((int)'Y');\n    __m324 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v325 = _mm256_set1_epi8((int)'e');\n    __m326 = _mm256_set1_epi8((int)'f');\n    __m327 = _mm256_set1_epi8((int)'g');\n    __m328 = _mm256_set1_epi8((int)'h');\n    __m329 = _mm256_set1_epi8((int)'i');\n    __m330 = _mm256_set1_epi8((int)'j');\n    __m331 = _mm256_set1_epi8((int)'k');\n    __m332 = _mm256_set1_epi8((int)'l');\n    __m333 = _mm256_set1_epi8((int)'m');\n    __m334 = _mm256_set1_epi8((int)'n');\n    __m335 = _mm256_set1_epi8((int)'o');\n    __m336 = _mm256_set1_epi8((int)'p');\n    __m337 = _mm256_set1_epi8((int)'q');\n    __m338 = _mm256_set1_epi8((int)'r');\n    __m339 = _mm256_set1_epi8((int)'S');\n    __m340 = _mm256_set1_epi8((int)'T');\n    __m341 = _mm256_set1_epi8((int)'U');\n    __m342 = _mm256_set1_epi8((int)'V');\n    __m343 = _mm256_set1_epi8((int)'W');\n    __m344 = _mm256_set1_epi8((int)'X');\n    __m345 = _mm256_set1_epi8((int)'Y');\n    __m346 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v347 = _mm256_set1_epi8((int)'e');\n    __m348 = _mm256_set1_epi8((int)'f');\n    __m349 = _mm256_set1_epi8((int)'g');\n    __m350 = _mm256_set1_epi8((int)'h');\n    __m351 = _mm256_set1_epi8((int)'i');\n    __m352 = _mm256_set1_epi8((int)'j');\n    __m353 = _mm256_set1_epi8((int)'k');\n    __m354 = _mm256_set1_epi8((int)'l');\n    __m355 = _mm256_set1_epi8((int)'m');\n    __m356 = _mm256_set1_epi8((int)'n');\n    __m357 = _mm256_set1_epi8((int)'o');\n    __m358 = _mm256_set1_epi8((int)'p');\n    __m359 = _mm256_set1_epi8((int)'q');\n    __m360 = _mm256_set1_epi8((int)'r');\n    __m361 = _mm256_set1_epi8((int)'S');\n    __m362 = _mm256_set1_epi8((int)'T');\n    __m363 = _mm256_set1_epi8((int)'U');\n    __m364 = _mm256_set1_epi8((int)'V');\n    __m365 = _mm256_set1_epi8((int)'W');\n    __m366 = _mm256_set1_epi8((int)'X');\n    __m367 = _mm256_set1_epi8((int)'Y');\n    __m368 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v369 = _mm256_set1_epi8((int)'e');\n    __m370 = _mm256_set1_epi8((int)'f');\n    __m371 = _mm256_set1_epi8((int)'g');\n    __m372 = _mm256_set1_epi8((int)'h');\n    __m373 = _mm256_set1_epi8((int)'i');\n    __m374 = _mm256_set1_epi8((int)'j');\n    __m375 = _mm256_set1_epi8((int)'k');\n    __m376 = _mm256_set1_epi8((int)'l');\n    __m377 = _mm256_set1_epi8((int)'m');\n    __m378 = _mm256_set1_epi8((int)'n');\n    __m379 = _mm256_set1_epi8((int)'o');\n    __m380 = _mm256_set1_epi8((int)'p');\n    __m381 = _mm256_set1_epi8((int)'q');\n    __m382 = _mm256_set1_epi8((int)'r');\n    __m383 = _mm256_set1_epi8((int)'S');\n    __m384 = _mm256_set1_epi8((int)'T');\n    __m385 = _mm256_set1_epi8((int)'U');\n    __m386 = _mm256_set1_epi8((int)'V');\n    __m387 = _mm256_set1_epi8((int)'W');\n    __m388 = _mm256_set1_epi8((int)'X');\n    __m389 = _mm256_set1_epi8((int)'Y');\n    __m390 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v391 = _mm256_set1_epi8((int)'e');\n    __m392 = _mm256_set1_epi8((int)'f');\n    __m393 = _mm256_set1_epi8((int)'g');\n    __m394 = _mm256_set1_epi8((int)'h');\n    __m395 = _mm256_set1_epi8((int)'i');\n    __m396 = _mm256_set1_epi8((int)'j');\n    __m397 = _mm256_set1_epi8((int)'k');\n    __m398 = _mm256_set1_epi8((int)'l');\n    __m399 = _mm256_set1_epi8((int)'m');\n    __m400 = _mm256_set1_epi8((int)'n');\n    __m401 = _mm256_set1_epi8((int)'o');\n    __m402 = _mm256_set1_epi8((int)'p');\n    __m403 = _mm256_set1_epi8((int)'q');\n    __m404 = _mm256_set1_epi8((int)'r');\n    __m405 = _mm256_set1_epi8((int)'S');\n    __m406 = _mm256_set1_epi8((int)'T');\n    __m407 = _mm256_set1_epi8((int)'U');\n    __m408 = _mm256_set1_epi8((int)'V');\n    __m409 = _mm256_set1_epi8((int)'W');\n    __m410 = _mm256_set1_epi8((int)'X');\n    __m411 = _mm256_set1_epi8((int)'Y');\n    __m412 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v413 = _mm256_set1_epi8((int)'e');\n    __m414 = _mm256_set1_epi8((int)'f');\n    __m415 = _mm256_set1_epi8((int)'g');\n    __m416 = _mm256_set1_epi8((int)'h');\n    __m417 = _mm256_set1_epi8((int)'i');\n    __m418 = _mm256_set1_epi8((int)'j');\n    __m419 = _mm256_set1_epi8((int)'k');\n    __m420 = _mm256_set1_epi8((int)'l');\n    __m421 = _mm256_set1_epi8((int)'m');\n    __m422 = _mm256_set1_epi8((int)'n');\n    __m423 = _mm256_set1_epi8((int)'o');\n    __m424 = _mm256_set1_epi8((int)'p');\n    __m425 = _mm256_set1_epi8((int)'q');\n    __m426 = _mm256_set1_epi8((int)'r');\n    __m427 = _mm256_set1_epi8((int)'S');\n    __m428 = _mm256_set1_epi8((int)'T');\n    __m429 = _mm256_set1_epi8((int)'U');\n    __m430 = _mm256_set1_epi8((int)'V');\n    __m431 = _mm256_set1_epi8((int)'W');\n    __m432 = _mm256_set1_epi8((int)'X');\n    __m433 = _mm256_set1_epi8((int)'Y');\n    __m434 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v435 = _mm256_set1_epi8((int)'e');\n    __m436 = _mm256_set1_epi8((int)'f');\n    __m437 = _mm256_set1_epi8((int)'g');\n    __m438 = _mm256_set1_epi8((int)'h');\n    __m439 ="}
{"task_id": "SimdBench_87_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd(std::string s){\n    __m256i v = _mm256_set1_epi8((int)'a');\n    __m256i v2 = _mm256_set1_epi8((int)'z');\n    __m256i v3 = _mm256_set1_epi8((int)'A');\n    __m256i v4 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v5 = _mm256_set1_epi8((int)'a' + 5);\n    __m256i v6 = _mm256_set1_epi8((int)'z' - 5);\n    __m256i v7 = _mm256_set1_epi8((int)'A' + 5);\n    __m256i v8 = _mm256_set1_epi8((int)'Z' - 5);\n\n    __m256i v9 = _mm256_set1_epi8((int)'a' + 10);\n    __m256i v10 = _mm256_set1_epi8((int)'z' - 10);\n    __m256i v11 = _mm256_set1_epi8((int)'A' + 10);\n    __m256i v12 = _mm256_set1_epi8((int)'Z' - 10);\n\n    __m256i v13 = _mm256_set1_epi8((int)'a' + 15);\n    __m256i v14 = _mm256_set1_epi8((int)'z' - 15);\n    __m256i v15 = _mm256_set1_epi8((int)'A' + 15);\n    __m256i v16 = _mm256_set1_epi8((int)'Z' - 15);\n\n    __m256i v17 = _mm256_set1_epi8((int)'a' + 20);\n    __m256i v18 = _mm256_set1_epi8((int)'z' - 20);\n    __m256i v19 = _mm256_set1_epi8((int)'A' + 20);\n    __m256i v20 = _mm256_set1_epi8((int)'Z' - 20);\n\n    __m256i v21 = _mm256_set1_epi8((int)'a' + 25);\n    __m256i v22 = _mm256_set1_epi8((int)'z' - 25);\n    __m256i v23 = _mm256_set1_epi8((int)'A' + 25);\n    __m256i v24 = _mm256_set1_epi8((int)'Z' - 25);\n\n    __m256i v25 = _mm256_set1_epi8((int)'a' + 30);\n    __m256i v26 = _mm256_set1_epi8((int)'z' - 30);\n    __m256i v27 = _mm256_set1_epi8((int)'A' + 30);\n    __m256i v28 = _mm256_set1_epi8((int)'Z' - 30);\n\n    __m256i v29 = _mm256_set1_epi8((int)'a' + 35);\n    __m256i v30 = _mm256_set1_epi8((int)'z' - 35);\n    __m256i v31 = _mm256_set1_epi8((int)'A' + 35);\n    __m256i v32 = _mm256_set1_epi8((int)'Z' - 35);\n\n    __m256i v33 = _mm256_set1_epi8((int)'a' + 40);\n    __m256i v34 = _mm256_set1_epi8((int)'z' - 40);\n    __m256i v35 = _mm256_set1_epi8((int)'A' + 40);\n    __m256i v36 = _mm256_set1_epi8((int)'Z' - 40);\n\n    __m256i v37 = _mm256_set1_epi8((int)'a' + 45);\n    __m256i v38 = _mm256_set1_epi8((int)'z' - 45);\n    __m256i v39 = _mm256_set1_epi8((int)'A' + 45);\n    __m256i v40 = _mm256_set1_epi8((int)'Z' - 45);\n\n    __m256i v41 = _mm256_set1_epi8((int)'a' + 50);\n    __m256i v42 = _mm256_set1_epi8((int)'z' - 50);\n    __m256i v43 = _mm256_set1_epi8((int)'A' + 50);\n    __m256i v44 = _mm256_set1_epi8((int)'Z' - 50);\n\n    __m256i v45 = _mm256_set1_epi8((int)'a' + 55);\n    __m256i v46 = _mm256_set1_epi8((int)'z' - 55);\n    __m256i v47 = _mm256_set1_epi8((int)'A' + 55);\n    __m256i v48 = _mm256_set1_epi8((int)'Z' - 55);\n\n    __m256i v49 = _mm256_set1_epi8((int)'a' + 60);\n    __m256i v50 = _mm256_set1_epi8((int)'z' - 60);\n    __m256i v51 = _mm256_set1_epi8((int)'A' + 60);\n    __m256i v52 = _mm256_set1_epi8((int)'Z' - 60);\n\n    __m256i v53 = _mm256_set1_epi8((int)'a' + 65);\n    __m256i v54 = _mm256_set1_epi8((int)'z' - 65);\n    __m256i v55 = _mm256_set1_epi8((int)'A' + 65);\n    __m256i v56 = _mm256_set1_epi8((int)'Z' - 65);\n\n    __m256i v57 = _mm256_set1_epi8((int)'a' + 70);\n    __m256i v58 = _mm256_set1_epi8((int)'z' - 70);\n    __m256i v59 = _mm256_set1_epi8((int)'A' + 70);\n    __m256i v60 = _mm256_set1_epi8((int)'Z' - 70);\n\n    __m256i v61 = _mm256_set1_epi8((int)'a' + 75);\n    __m256i v62 = _mm256_set1_epi8((int)'z' - 75);\n    __m256i v63 = _mm256_set1_epi8((int)'A' + 75);\n    __m256i v64 = _mm256_set1_epi8((int)'Z' - 75);\n\n    __m256i v65 = _mm256_set1_epi8((int)'a' + 80);\n    __m256i v66 = _mm256_set1_epi8((int)'z' - 80);\n    __m256i v67 = _mm256_set1_epi8((int)'A' + 80);\n    __m256i v68 = _mm256_set1_epi8((int)'Z' - 80);\n\n    __m256i v69 = _mm256_set1_epi8((int)'a' + 85);\n    __m256i v70 = _mm256_set1_epi8((int)'z' - 85);\n    __m256i v71 = _mm256_set1_epi8((int)'A' + 85);\n    __m256i v72 = _mm256_set1_epi8((int)'Z' - 85);\n\n    __m256i v73 = _mm256_set1_epi8((int)'a' + 90);\n    __m256i v74 = _mm256_set1_epi8((int)'z' - 90);\n    __m256i v75 = _mm256_set1_epi8((int)'A' + 90);\n    __m256i v76 = _mm256_set1_epi8((int)'Z' - 90);\n\n    __m256i v77 = _mm256_set1_epi8((int)'a' + 95);\n    __m256i v78 = _mm256_set1_epi8((int)'z' - 95);\n    __m256i v79 = _mm256_set1_epi8((int)'A' + 95);\n    __m256i v80 = _mm256_set1_epi8((int)'Z' - 95);\n\n    __m256i v81 = _mm256_set1_epi8((int)'a' + 100);\n    __m256i v82 = _mm256_set1_epi8((int)'z' - 100);\n    __m256i v83 = _mm256_set1_epi8((int)'A' + 100);\n    __m256i v84 = _mm256_set1_epi8((int)'Z' - 100);\n\n    __m256i v85 = _mm256_set1_epi8((int)'a' + 105);\n    __m256i v86 = _mm256_set1_epi8((int)'z' - 105);\n    __m256i v87 = _mm256_set1_epi8((int)'A' + 105);\n    __m256i v88 = _mm256_set1_epi8((int)'Z' - 105);\n\n    __m256i v89 = _mm256_set1_epi8((int)'a' + 110);\n    __m256i v90 = _mm256_set1_epi8((int)'z' - 110);\n    __m256i v91 = _mm256_set1_epi8((int)'A' + 110);\n    __m256i v92 = _mm256_set1_epi8((int)'Z' - 110);\n\n    __m256i v93 = _mm256_set1_epi8((int)'a' + 115);\n    __m256i v94 = _mm256_set1_epi8((int)'z' - 115);\n    __m256i v95 = _mm256_set1_epi8((int)'A' + 115);\n    __m256i v96 = _mm256_set1_epi8((int)'Z' - 115);\n\n    __m256i v97 = _mm256_set1_epi8((int)'a' + 120);\n    __m256i v98 = _mm256_set1_epi8((int)'z' - 120);\n    __m256i v99 = _mm256_set1_epi8((int)'A' + 120);\n    __m256i v100 = _mm256_set1_epi8((int)'Z' - 120);\n\n    __m256i v101 = _mm256_set1_epi8((int)'a' + 125);\n    __m256i v102 = _mm256_set1_epi8((int)'z' - 125);\n    __m256i v103 = _mm256_set1_epi8((int)'A' + 125);\n    __m256i v104 = _mm256_set1_epi8((int)'Z' - 125);\n\n    __m256i v105 = _mm256_set1_epi8((int)'a' + 130);\n    __m256i v106 = _mm256_set1_epi8((int)'z' - 130);\n    __m256i v107 = _mm256_set1_epi8((int)'A' + 130);\n    __m256i v108 = _mm256_set1_epi8((int)'Z' - 130);\n\n    __m256i v109 = _mm256_set1_epi8((int)'a' + 135);\n    __m256i v110 = _mm256_set1_epi8((int)'z' - 135);\n    __m256i v111 = _mm256_set1_epi8((int)'A' + 135);\n    __m256i v112 = _mm256_set1_epi8((int)'Z' - 135);\n\n    __m256i v113 = _mm256_set1_epi8((int)'a' + 140);\n    __m256i v114 = _mm256_set1_epi8((int)'z' - 140);\n    __m256i v115 = _mm256_set1_epi8((int)'A' + 140);\n    __m256i v116 = _mm256_set1_epi8((int)'Z' - 140);\n\n    __m256i v117 = _mm256_set1_epi8((int)'a' + 145);\n    __m256i v118 = _mm256_set1_epi8((int)'z' - 145);\n    __m256i v119 = _mm256_set1_epi8((int)'A' + 145);\n    __m256i v120 = _mm256_set1_epi8((int)'Z' - 145);\n\n    __m256i v121 = _mm256_set1_epi8((int)'a' + 150);\n    __m256i v122 = _mm256_set1_epi8((int)'z' - 150);\n    __m256i v123 = _mm256_set1_epi8((int)'A' + 150);\n    __m256i v124 = _mm256_set1_epi8((int)'Z' - 150);\n\n    __m256i v125 = _mm256_set1_epi8((int)'a' + 155);\n    __m256i v126 = _mm256_set1_epi8((int)'z' - 155);\n    __m256i v127 = _mm256_set1_epi8((int)'A' + 155);\n    __m256i v128 = _mm256_set1_epi8((int)'Z' - 155);\n\n    __m256i v129 = _mm256_set1_epi8((int)'a' + 160);\n    __m256i v130 = _mm256_set1_epi8((int)'z' - 160);\n    __m256i v131 = _mm256_set1_epi8((int)'A' + 160);\n    __m256i v132 = _mm256_set1_epi8((int)'Z' - 160);\n\n    __m256i v133 = _mm256_set1_epi8((int)'a' + 165);\n    __m256i v134 = _mm256_set1_epi8((int)'z' - 165);\n    __m256i v135 = _mm256_set1_epi8((int)'A' + 165);\n    __m256i v136 = _mm256_set1_epi8((int)'Z' - 165);\n\n    __m256i v137 = _mm256_set1_epi8((int)'a' + 170);\n    __m256i v138 = _mm256_set1_epi8((int)'z' - 170);\n    __m256i v139 = _mm256_set1_epi8((int)'A' + 170);\n    __m256i v140 = _mm256_set1_epi8((int)'Z' - 170);\n\n    __m256i v141 = _mm256_set1_epi8((int)'a' + 175);\n    __m256i v142 = _mm256_set1_epi8((int)'z' - 175);\n    __m256i v143 = _mm256_set1_epi8((int)'A' + 175);\n    __m256i v144 = _mm256_set1_epi8((int)'Z' - 175);\n\n    __m256i v145 = _mm256_set1_epi8((int)'a' + 180);\n    __m256i v146 = _mm256_set1_epi8((int)'z' - 180);\n    __m256i v147 = _mm256_set1_epi8((int)'A' + 180);\n    __m256i v148 = _mm256_set1_epi8((int)'Z' - 180);\n\n    __m256i v149 = _mm256_set1_epi8((int)'a' + 185);\n    __m256i v150 = _mm256_set1_epi8((int)'z' - 185);\n    __m256i v151 = _mm256_set1_epi8((int)'A' + 185);\n    __m256i v152 = _mm256_set1_epi8((int)'Z' - 185);\n\n    __m256i v153 = _mm256_set1_epi8((int)'a' + 190);\n    __m256i v154 = _mm256_set1_epi8((int)'z' - 190);\n    __m256i v155 = _mm256_set1_epi8((int)'A' + 190);\n    __m256i v156 = _mm256_set1_epi8((int)'Z' - 190);\n\n    __m256i v157 = _mm256_set1_epi8((int)'a' + 195);\n    __m256i v158 = _mm256_set1_epi8((int)'z' - 195);\n    __m256i v159 = _mm256_set1_epi8((int)'A' + 195);\n    __m260 = _mm256_set1_epi8((int)'Z' - 195);\n\n    __m256i v161 = _mm256_set1_epi8((int)'a' + 200);\n    __m256i v162 = _mm256_set1_epi8((int)'z' - 200);\n    __m256i v163 = _mm256_set1_epi8((int)'A' + 200);\n    __m264 = _mm256_set1_epi8((int)'Z' - 200);\n\n    __m256i v165 = _mm256_set1_epi8((int)'a' + 205);\n    __m256i v166 = _mm256_set1_epi8((int)'z' - 205);\n    __m256i v167 = _mm256_set1_epi8((int)'A' + 205);\n    __m268 = _mm256_set1_epi8((int)'Z' - 205);\n\n    __m256i v169 = _mm256_set1_epi8((int)'a' + 210);\n    __m256i v170 = _mm256_set1_epi8((int)'z' - 210);\n    __m256i v171 = _mm256_set1_epi8((int)'A' + 210);\n    __m272 = _mm256_set1_epi8((int)'Z' - 210);\n\n    __m256i v173 = _mm256_set1_epi8((int)'a' + 215);\n    __m256i v174 = _mm256_set1_epi8((int)'z' - 215);\n    __m256i v175 = _mm256_set1_epi8((int)'A' + 215);\n    __m276 = _mm256_set1_epi8((int)'Z' - 215);\n\n    __m256i v177 = _mm256_set1_epi8((int)'a' + 220);\n    __m256i v178 = _mm256_set1_epi8((int)'z' - 220);\n    __m256i v179 = _mm256_set1_epi8((int)'A' + 220);\n    __m280 = _mm256_set1_epi8((int)'Z' - 220);\n\n    __m256i v181 = _mm256_set1_epi8((int)'a' + 225);\n    __m256i v182 = _mm256_set1_epi8((int)'z' - 225);\n    __m256i v183 = _mm256_set1_epi8((int)'A' + 225);\n    __m284 = _mm256_set1_epi8((int)'Z' - 225);\n\n    __m256i v185 = _mm256_set1_epi8((int)'a' + 230);\n    __m256i v186 = _mm256_set1_epi8((int)'z' - 230);\n    __m256i v187 = _mm256_set1_epi8((int)'A' + 230);\n    __m288 = _mm256_set1_epi8((int)'Z' - 230);\n\n    __m256i v189 = _mm256_set1_epi8((int)'a' + 240);\n    __m256i v190 = _mm256_set1_epi8((int)'z' - 240);\n    __m256i v191 = _mm256_set1_epi8((int)'A' + 240);\n    __m292 = _mm256_set1_epi8((int)'Z' - 240);\n\n    __m256i v193 = _mm256_set1_epi8((int)'a' + 245);\n    __m256i v194 = _mm256_set1_epi8((int)'z' - 245);\n    __m256i v195 = _mm256_set1_epi8((int)'A' + 245);\n    __m296 = _mm256_set1_epi8((int)'Z' - 245);\n\n    __m256i v197 = _mm256_set1_epi8((int)'a' + 250);\n    __m256i v198 = _mm256_set1_epi8((int)'z' - 250);\n    __m256i v199 = _mm256_set1_epi8((int)'A' + 250);\n    __m300 = _mm256_set1_epi8((int)'Z' - 250);\n\n    __m256i v301 = _mm256_set1_epi8((int)'a' + 255);\n    __m256i v302 = _mm256_set1_epi8((int)'z' - 255);\n    __m256i v303 = _mm256_set1_epi8((int)'A' + 255);\n    __m304 = _mm256_set1_epi8((int)'Z' - 255);\n\n    __m256i v305 = _mm256_set1_epi8((int)'a' + 260);\n    __m256i v306 = _mm256_set1_epi8((int)'z' - 260);\n    __m256i v307 = _mm256_set1_epi8((int)'A' + 260);\n    __m308 = _mm256_set1_epi8((int)'Z' - 260);\n\n    __m256i v309 = _mm256_set1_epi8((int)'a' + 265);\n    __m260 = _mm256_set1_epi8((int)'z' - 265);\n    __m261 = _mm256_set1_epi8((int)'A' + 265);\n    __m312 = _mm256_set1_epi8((int)'Z' - 265);\n\n    __m256i v313 = _mm256_set1_epi8((int)'a' + 270);\n    __m264 = _mm256_set1_epi8((int)'z' - 270);\n    __m265 = _mm256_set1_epi8((int)'A' + 270);\n    __m316 = _mm256_set1_epi8((int)'Z' - 270);\n\n    __m256i v317 = _mm256_set1_epi8((int)'a' + 275);\n    __m268 = _mm256_set1_epi8((int)'z' - 275);\n    __m269 = _mm256_set1_epi8((int)'A' + 275);\n    __m320 = _mm256_set1_epi8((int)'Z' - 275);\n\n    __m256i v321 = _mm256_set1_epi8((int)'a' + 280);\n    __m272 = _mm256_set1_epi8((int)'z' - 280);\n    __m273 = _mm256_set1_epi8((int)'A' + 280);\n    __m324 = _mm256_set1_epi8((int)'Z' - 280);\n\n    __m256i v325 = _mm256_set1_epi8((int)'a' + 285);\n    __m276 = _mm256_set1_epi8((int)'z' - 285);\n    __m277 = _mm256_set1_epi8((int)'A' + 285);\n    __m328 = _mm256_set1_epi8((int)'Z' - 285);\n\n    __m256i v329 = _mm256_set1_epi8((int)'a' + 290);\n    __m280 = _mm256_set1_epi8((int)'z' - 290);\n    __m281 = _mm256_set1_epi8((int)'A' + 290);\n    __m332 = _mm256_set1_epi8((int)'Z' - 290);\n\n    __m256i v333 = _mm256_set1_epi8((int)'a' + 295);\n    __m284 = _mm256_set1_epi8((int)'z' - 295);\n    __m285 = _mm256_set1_epi8((int)'A' + 295);\n    __m336 = _mm256_set1_epi8((int)'Z' - 295);\n\n    __m256i v337 = _mm256_set1_epi8((int)'a' + 300);\n    __m288 = _mm256_set1_epi8((int)'z' - 300);\n    __m289 = _mm256_set1_epi8((int)'A' + 300);\n    __m340 = _mm256_set1_epi8((int)'Z' - 300);\n\n    __m256i v341 = _mm256_set1_epi8((int)'a' + 305);\n    __m292 = _mm256_set1_epi8((int)'z' - 305);\n    __m293 = _mm256_set1_epi8((int)'A' + 305);\n    __m344 = _mm256_set1_epi8((int)'Z' - 305);\n\n    __m256i v345 = _mm256_set1_epi8((int)'a' + 310);\n    __m296 = _mm256_set1_epi8((int)'z' - 310);\n    __m297 = _mm256_set1_epi8((int)'A' + 310);\n    __m348 = _mm256_set1_epi8((int)'Z' - 310);\n\n    __m256i v351 = _mm256_set1_epi8((int)'a' + 315);\n    __m300 = _mm256_set1_epi8((int)'z' - 315);\n    __m301 = _mm256_set1_epi8((int)'A' + 315);\n    __m352 = _mm256_set1_epi8((int)'Z' - 315);\n\n    __m256i v353 = _mm256_set1_epi8((int)'a' + 320);\n    __m304 = _mm256_set1_epi8((int)'z' - 320);\n    __m305 = _mm256_set1_epi8((int)'A' + 320);\n    __m356 = _mm256_set1_epi8((int)'Z' - 320);\n\n    __m256i v357 = _mm256_set1_epi8((int)'a' + 325);\n    __m308 = _mm256_set1_epi8((int)'z' - 325);\n    __m310 = _mm256_set1_epi8((int)'A' + 325);\n    __m360 = _mm25"}
{"task_id": "SimdBench_87_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd(std::string s){\n    __m256i v = _mm256_set1_epi8('a');\n    __m256i v2 = _mm256_set1_epi8('z');\n    __m256i v3 = _mm256_set1_epi8('A');\n    __m256i v4 = _mm256_set1_epi8('Z');\n\n    __m256i v5 = _mm256_set1_epi8(' ');\n    __m256i v6 = _mm256_set1_epi8('.');\n    __m256i v7 = _mm256_set1_epi8(',');\n    __m256i v8 = _mm256_set1_epi8('-');\n    __m256i v9 = _mm256_set1_epi8('_');\n    __m256i va = _mm256_set1_epi8('+');\n    __m256i vb = _mm256_set1_epi8('*');\n    __m256i vc = _mm256_set1_epi8('/');\n    __m256i vd = _mm256_set1_epi8(':');\n    __m256i ve = _mm256_set1_epi8(';');\n    __m256i vf = _mm256_set1_epi8('<');\n    __m256i vg = _mm256_set1_epi8('=');\n    __m256i vh = _mm256_set1_epi8('>');\n    __m256i vi = _mm256_set1_epi8('?');\n    __m256i vj = _mm256_set1_epi8('@');\n    __m256i vk = _mm256_set1_epi8('$');\n    __m256i vl = _mm256_set1_epi8('%');\n    __m256i vm = _mm256_set1_epi8('^');\n    __m256i vn = _mm256_set1_epi8('&');\n    __m256i vo = _mm256_set1_epi8('(');\n    __m256i vp = _mm256_set1_epi8(')');\n    __m256i vq = _mm256_set1_epi8('{');\n    __m256i vr = _mm256_set1_epi8('}');\n    __m256i vs = _mm256_set1_epi8('|');\n    __m256i vt = _mm256_set1_epi8('~');\n    __m256i vu = _mm256_set1_epi8('`');\n    __m256i vv = _mm256_set1_epi8('!');\n    __m256i vw = _mm256_set1_epi8('#');\n    __m256i vx = _mm256_set1_epi8('$');\n    __m256i vy = _mm256_set1_epi8('%');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8('{');\n    __m256i vw = _mm256_set1_epi8('}');\n    __m256i vx = _mm256_set1_epi8('|');\n    __m256i vy = _mm256_set1_epi8('~');\n    __m256i vz = _mm256_set1_epi8('^');\n    __m256i vw = _mm256_set1_epi8('&');\n    __m256i vx = _mm256_set1_epi8('(');\n    __m256i vy = _mm256_set1_epi8(')');\n    __m256i vz = _mm256_set1_epi8"}
{"task_id": "SimdBench_87_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd(std::string s){\n    __m256i v = _mm256_set1_epi8((int)'a');\n    __m256i v_shifted = _mm256_set1_epi8((int)'f'); // Shifted by 5\n    __m256i v_sub = _mm256_set1_epi8((int)'a' - (int)'f'); // Difference between shifted and original characters\n\n    __m256i v_result = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < s.size(); i += 32) {\n        __m256i v_input = _mm256_loadu_si256((__m256i*)&s[i]);\n        __m256i v_output = _mm256_add_epi8(v_input, v_shifted);\n        __m256i v_result_temp = _mm256_sub_epi8(v_output, v_sub);\n        _mm256_storeu_si256((__m256i*)&v_result, v_result_temp);\n    }\n\n    std::string result;\n    for (size_t i = 0; i < s.size(); ++i) {\n        result += (char)(v_result[i] + 'a');\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_87_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string encode_shift(std::string s){\n    std::string out;\n    int i;\n    for (i=0;i<s.length();i++)\n    {\n        int w=((int)s[i]+5-(int)'a')%26+(int)'a';   \n        out=out+(char)w;\n    }\n    return out;\n}\n\ndecode_shift_simd(std::string s){\n    __m256i v = _mm256_set1_epi8((int)'a');\n    __m256i v2 = _mm256_set1_epi8((int)'z');\n    __m256i v3 = _mm256_set1_epi8((int)'A');\n    __m256i v4 = _mm256_set1_epi8((int)'Z');\n\n    __m256i v5 = _mm256_set1_epi8((int)'a' + 5);\n    __m256i v6 = _mm256_set1_epi8((int)'z' - 5);\n    __m256i v7 = _mm256_set1_epi8((int)'A' + 5);\n    __m256i v8 = _mm256_set1_epi8((int)'Z' - 5);\n\n    __m256i v9 = _mm256_set1_epi8((int)'a' + 10);\n    __m256i v10 = _mm256_set1_epi8((int)'z' - 10);\n    __m256i v11 = _mm256_set1_epi8((int)'A' + 10);\n    __m256i v12 = _mm256_set1_epi8((int)'Z' - 10);\n\n    __m256i v13 = _mm256_set1_epi8((int)'a' + 15);\n    __m256i v14 = _mm256_set1_epi8((int)'z' - 15);\n    __m256i v15 = _mm256_set1_epi8((int)'A' + 15);\n    __m256i v16 = _mm256_set1_epi8((int)'Z' - 15);\n\n    __m256i v17 = _mm256_set1_epi8((int)'a' + 20);\n    __m256i v18 = _mm256_set1_epi8((int)'z' - 20);\n    __m256i v19 = _mm256_set1_epi8((int)'A' + 20);\n    __m256i v20 = _mm256_set1_epi8((int)'Z' - 20);\n\n    __m256i v21 = _mm256_set1_epi8((int)'a' + 25);\n    __m256i v22 = _mm256_set1_epi8((int)'z' - 25);\n    __m256i v23 = _mm256_set1_epi8((int)'A' + 25);\n    __m256i v24 = _mm256_set1_epi8((int)'Z' - 25);\n\n    __m256i v25 = _mm256_set1_epi8((int)'a' + 30);\n    __m256i v26 = _mm256_set1_epi8((int)'z' - 30);\n    __m256i v27 = _mm256_set1_epi8((int)'A' + 30);\n    __m256i v28 = _mm256_set1_epi8((int)'Z' - 30);\n\n    __m256i v29 = _mm256_set1_epi8((int)'a' + 35);\n    __m256i v30 = _mm256_set1_epi8((int)'z' - 35);\n    __m256i v31 = _mm256_set1_epi8((int)'A' + 35);\n    __m256i v32 = _mm256_set1_epi8((int)'Z' - 35);\n\n    __m256i v33 = _mm256_set1_epi8((int)'a' + 40);\n    __m256i v34 = _mm256_set1_epi8((int)'z' - 40);\n    __m256i v35 = _mm256_set1_epi8((int)'A' + 40);\n    __m256i v36 = _mm256_set1_epi8((int)'Z' - 40);\n\n    __m256i v37 = _mm256_set1_epi8((int)'a' + 45);\n    __m256i v38 = _mm256_set1_epi8((int)'z' - 45);\n    __m256i v39 = _mm256_set1_epi8((int)'A' + 45);\n    __m256i v40 = _mm256_set1_epi8((int)'Z' - 45);\n\n    __m256i v41 = _mm256_set1_epi8((int)'a' + 50);\n    __m256i v42 = _mm256_set1_epi8((int)'z' - 50);\n    __m256i v43 = _mm256_set1_epi8((int)'A' + 50);\n    __m256i v44 = _mm256_set1_epi8((int)'Z' - 50);\n\n    __m256i v45 = _mm256_set1_epi8((int)'a' + 55);\n    __m256i v46 = _mm256_set1_epi8((int)'z' - 55);\n    __m256i v47 = _mm256_set1_epi8((int)'A' + 55);\n    __m256i v48 = _mm256_set1_epi8((int)'Z' - 55);\n\n    __m256i v49 = _mm256_set1_epi8((int)'a' + 60);\n    __m256i v50 = _mm256_set1_epi8((int)'z' - 60);\n    __m256i v51 = _mm256_set1_epi8((int)'A' + 60);\n    __m256i v52 = _mm256_set1_epi8((int)'Z' - 60);\n\n    __m256i v53 = _mm256_set1_epi8((int)'a' + 65);\n    __m256i v54 = _mm256_set1_epi8((int)'z' - 65);\n    __m256i v55 = _mm256_set1_epi8((int)'A' + 65);\n    __m256i v56 = _mm256_set1_epi8((int)'Z' - 65);\n\n    __m256i v57 = _mm256_set1_epi8((int)'a' + 70);\n    __m256i v58 = _mm256_set1_epi8((int)'z' - 70);\n    __m256i v59 = _mm256_set1_epi8((int)'A' + 70);\n    __m256i v60 = _mm256_set1_epi8((int)'Z' - 70);\n\n    __m256i v61 = _mm256_set1_epi8((int)'a' + 75);\n    __m256i v62 = _mm256_set1_epi8((int)'z' - 75);\n    __m256i v63 = _mm256_set1_epi8((int)'A' + 75);\n    __m256i v64 = _mm256_set1_epi8((int)'Z' - 75);\n\n    __m256i v65 = _mm256_set1_epi8((int)'a' + 80);\n    __m256i v66 = _mm256_set1_epi8((int)'z' - 80);\n    __m256i v67 = _mm256_set1_epi8((int)'A' + 80);\n    __m256i v68 = _mm256_set1_epi8((int)'Z' - 80);\n\n    __m256i v69 = _mm256_set1_epi8((int)'a' + 85);\n    __m256i v70 = _mm256_set1_epi8((int)'z' - 85);\n    __m256i v71 = _mm256_set1_epi8((int)'A' + 85);\n    __m256i v72 = _mm256_set1_epi8((int)'Z' - 85);\n\n    __m256i v73 = _mm256_set1_epi8((int)'a' + 90);\n    __m256i v74 = _mm256_set1_epi8((int)'z' - 90);\n    __m256i v75 = _mm256_set1_epi8((int)'A' + 90);\n    __m256i v76 = _mm256_set1_epi8((int)'Z' - 90);\n\n    __m256i v77 = _mm256_set1_epi8((int)'a' + 95);\n    __m256i v78 = _mm256_set1_epi8((int)'z' - 95);\n    __m256i v79 = _mm256_set1_epi8((int)'A' + 95);\n    __m256i v80 = _mm256_set1_epi8((int)'Z' - 95);\n\n    __m256i v81 = _mm256_set1_epi8((int)'a' + 100);\n    __m256i v82 = _mm256_set1_epi8((int)'z' - 100);\n    __m256i v83 = _mm256_set1_epi8((int)'A' + 100);\n    __m256i v84 = _mm256_set1_epi8((int)'Z' - 100);\n\n    __m256i v85 = _mm256_set1_epi8((int)'a' + 105);\n    __m256i v86 = _mm256_set1_epi8((int)'z' - 105);\n    __m256i v87 = _mm256_set1_epi8((int)'A' + 105);\n    __m256i v88 = _mm256_set1_epi8((int)'Z' - 105);\n\n    __m256i v89 = _mm256_set1_epi8((int)'a' + 110);\n    __m256i v90 = _mm256_set1_epi8((int)'z' - 110);\n    __m256i v91 = _mm256_set1_epi8((int)'A' + 110);\n    __m256i v92 = _mm256_set1_epi8((int)'Z' - 110);\n\n    __m256i v93 = _mm256_set1_epi8((int)'a' + 115);\n    __m256i v94 = _mm256_set1_epi8((int)'z' - 115);\n    __m256i v95 = _mm256_set1_epi8((int)'A' + 115);\n    __m256i v96 = _mm256_set1_epi8((int)'Z' - 115);\n\n    __m256i v97 = _mm256_set1_epi8((int)'a' + 120);\n    __m256i v98 = _mm256_set1_epi8((int)'z' - 120);\n    __m256i v99 = _mm256_set1_epi8((int)'A' + 120);\n    __m256i v100 = _mm256_set1_epi8((int)'Z' - 120);\n\n    __m256i v101 = _mm256_set1_epi8((int)'a' + 125);\n    __m256i v102 = _mm256_set1_epi8((int)'z' - 125);\n    __m256i v103 = _mm256_set1_epi8((int)'A' + 125);\n    __m256i v104 = _mm256_set1_epi8((int)'Z' - 125);\n\n    __m256i v105 = _mm256_set1_epi8((int)'a' + 130);\n    __m256i v106 = _mm256_set1_epi8((int)'z' - 130);\n    __m256i v107 = _mm256_set1_epi8((int)'A' + 130);\n    __m256i v108 = _mm256_set1_epi8((int)'Z' - 130);\n\n    __m256i v109 = _mm256_set1_epi8((int)'a' + 135);\n    __m256i v110 = _mm256_set1_epi8((int)'z' - 135);\n    __m256i v111 = _mm256_set1_epi8((int)'A' + 135);\n    __m256i v112 = _mm256_set1_epi8((int)'Z' - 135);\n\n    __m256i v113 = _mm256_set1_epi8((int)'a' + 140);\n    __m256i v114 = _mm256_set1_epi8((int)'z' - 140);\n    __m256i v115 = _mm256_set1_epi8((int)'A' + 140);\n    __m256i v116 = _mm256_set1_epi8((int)'Z' - 140);\n\n    __m256i v117 = _mm256_set1_epi8((int)'a' + 145);\n    __m256i v118 = _mm256_set1_epi8((int)'z' - 145);\n    __m256i v119 = _mm256_set1_epi8((int)'A' + 145);\n    __m256i v120 = _mm256_set1_epi8((int)'Z' - 145);\n\n    __m256i v121 = _mm256_set1_epi8((int)'a' + 150);\n    __m256i v122 = _mm256_set1_epi8((int)'z' - 150);\n    __m256i v123 = _mm256_set1_epi8((int)'A' + 150);\n    __m256i v124 = _mm256_set1_epi8((int)'Z' - 150);\n\n    __m256i v125 = _mm256_set1_epi8((int)'a' + 155);\n    __m256i v126 = _mm256_set1_epi8((int)'z' - 155);\n    __m256i v127 = _mm256_set1_epi8((int)'A' + 155);\n    __m256i v128 = _mm256_set1_epi8((int)'Z' - 155);\n\n    __m256i v129 = _mm256_set1_epi8((int)'a' + 160);\n    __m256i v130 = _mm256_set1_epi8((int)'z' - 160);\n    __m256i v131 = _mm256_set1_epi8((int)'A' + 160);\n    __m256i v132 = _mm256_set1_epi8((int)'Z' - 160);\n\n    __m256i v133 = _mm256_set1_epi8((int)'a' + 165);\n    __m256i v134 = _mm256_set1_epi8((int)'z' - 165);\n    __m256i v135 = _mm256_set1_epi8((int)'A' + 165);\n    __m256i v136 = _mm256_set1_epi8((int)'Z' - 165);\n\n    __m256i v137 = _mm256_set1_epi8((int)'a' + 170);\n    __m256i v138 = _mm256_set1_epi8((int)'z' - 170);\n    __m256i v139 = _mm256_set1_epi8((int)'A' + 170);\n    __m256i v140 = _mm256_set1_epi8((int)'Z' - 170);\n\n    __m256i v141 = _mm256_set1_epi8((int)'a' + 175);\n    __m256i v142 = _mm256_set1_epi8((int)'z' - 175);\n    __m256i v143 = _mm256_set1_epi8((int)'A' + 175);\n    __m256i v144 = _mm256_set1_epi8((int)'Z' - 175);\n\n    __m256i v145 = _mm256_set1_epi8((int)'a' + 180);\n    __m256i v146 = _mm256_set1_epi8((int)'z' - 180);\n    __m256i v147 = _mm256_set1_epi8((int)'A' + 180);\n    __m256i v148 = _mm256_set1_epi8((int)'Z' - 180);\n\n    __m256i v149 = _mm256_set1_epi8((int)'a' + 185);\n    __m256i v150 = _mm256_set1_epi8((int)'z' - 185);\n    __m256i v151 = _mm256_set1_epi8((int)'A' + 185);\n    __m256i v152 = _mm256_set1_epi8((int)'Z' - 185);\n\n    __m256i v153 = _mm256_set1_epi8((int)'a' + 190);\n    __m256i v154 = _mm256_set1_epi8((int)'z' - 190);\n    __m256i v155 = _mm256_set1_epi8((int)'A' + 190);\n    __m256i v156 = _mm256_set1_epi8((int)'Z' - 190);\n\n    __m256i v157 = _mm256_set1_epi8((int)'a' + 195);\n    __m256i v158 = _mm256_set1_epi8((int)'z' - 195);\n    __m256i v159 = _mm256_set1_epi8((int)'A' + 195);\n    __m260 = _mm256_set1_epi8((int)'Z' - 195);\n\n    __m256i v161 = _mm256_set1_epi8((int)'a' + 200);\n    __m256i v162 = _mm256_set1_epi8((int)'z' - 200);\n    __m256i v163 = _mm256_set1_epi8((int)'A' + 200);\n    __m264 = _mm256_set1_epi8((int)'Z' - 200);\n\n    __m256i v165 = _mm256_set1_epi8((int)'a' + 205);\n    __m256i v166 = _mm256_set1_epi8((int)'z' - 205);\n    __m256i v167 = _mm256_set1_epi8((int)'A' + 205);\n    __m268 = _mm256_set1_epi8((int)'Z' - 205);\n\n    __m256i v169 = _mm256_set1_epi8((int)'a' + 210);\n    __m256i v170 = _mm256_set1_epi8((int)'z' - 210);\n    __m256i v171 = _mm256_set1_epi8((int)'A' + 210);\n    __m272 = _mm256_set1_epi8((int)'Z' - 210);\n\n    __m256i v173 = _mm256_set1_epi8((int)'a' + 215);\n    __m256i v174 = _mm256_set1_epi8((int)'z' - 215);\n    __m256i v175 = _mm256_set1_epi8((int)'A' + 215);\n    __m276 = _mm256_set1_epi8((int)'Z' - 215);\n\n    __m256i v177 = _mm256_set1_epi8((int)'a' + 220);\n    __m256i v178 = _mm256_set1_epi8((int)'z' - 220);\n    __m256i v179 = _mm256_set1_epi8((int)'A' + 220);\n    __m280 = _mm256_set1_epi8((int)'Z' - 220);\n\n    __m256i v181 = _mm256_set1_epi8((int)'a' + 225);\n    __m256i v182 = _mm256_set1_epi8((int)'z' - 225);\n    __m256i v183 = _mm256_set1_epi8((int)'A' + 225);\n    __m284 = _mm256_set1_epi8((int)'Z' - 225);\n\n    __m256i v185 = _mm256_set1_epi8((int)'a' + 230);\n    __m256i v186 = _mm256_set1_epi8((int)'z' - 230);\n    __m256i v187 = _mm256_set1_epi8((int)'A' + 230);\n    __m288 = _mm256_set1_epi8((int)'Z' - 230);\n\n    __m256i v189 = _mm256_set1_epi8((int)'a' + 240);\n    __m256i v190 = _mm256_set1_epi8((int)'z' - 240);\n    __m256i v191 = _mm256_set1_epi8((int)'A' + 240);\n    __m292 = _mm256_set1_epi8((int)'Z' - 240);\n\n    __m256i v193 = _mm256_set1_epi8((int)'a' + 245);\n    __m256i v194 = _mm256_set1_epi8((int)'z' - 245);\n    __m256i v195 = _mm256_set1_epi8((int)'A' + 245);\n    __m296 = _mm256_set1_epi8((int)'Z' - 245);\n\n    __m256i v197 = _mm256_set1_epi8((int)'a' + 250);\n    __m256i v198 = _mm256_set1_epi8((int)'z' - 250);\n    __m256i v199 = _mm256_set1_epi8((int)'A' + 250);\n    __m300 = _mm256_set1_epi8((int)'Z' - 250);\n\n    __m256i v301 = _mm256_set1_epi8((int)'a' + 255);\n    __m256i v302 = _mm256_set1_epi8((int)'z' - 255);\n    __m256i v303 = _mm256_set1_epi8((int)'A' + 255);\n    __m304 = _mm256_set1_epi8((int)'Z' - 255);\n\n    __m256i v305 = _mm256_set1_epi8((int)'a' + 260);\n    __m256i v306 = _mm256_set1_epi8((int)'z' - 260);\n    __m256i v307 = _mm256_set1_epi8((int)'A' + 260);\n    __m308 = _mm256_set1_epi8((int)'Z' - 260);\n\n    __m256i v309 = _mm256_set1_epi8((int)'a' + 265);\n    __m260 = _mm256_set1_epi8((int)'z' - 265);\n    __m261 = _mm256_set1_epi8((int)'A' + 265);\n    __m312 = _mm256_set1_epi8((int)'Z' - 265);\n\n    __m256i v313 = _mm256_set1_epi8((int)'a' + 270);\n    __m264 = _mm256_set1_epi8((int)'z' - 270);\n    __m265 = _mm256_set1_epi8((int)'A' + 270);\n    __m316 = _mm256_set1_epi8((int)'Z' - 270);\n\n    __m256i v317 = _mm256_set1_epi8((int)'a' + 275);\n    __m268 = _mm256_set1_epi8((int)'z' - 275);\n    __m269 = _mm256_set1_epi8((int)'A' + 275);\n    __m320 = _mm256_set1_epi8((int)'Z' - 275);\n\n    __m256i v321 = _mm256_set1_epi8((int)'a' + 280);\n    __m272 = _mm256_set1_epi8((int)'z' - 280);\n    __m273 = _mm256_set1_epi8((int)'A' + 280);\n    __m324 = _mm256_set1_epi8((int)'Z' - 280);\n\n    __m256i v325 = _mm256_set1_epi8((int)'a' + 285);\n    __m276 = _mm256_set1_epi8((int)'z' - 285);\n    __m277 = _mm256_set1_epi8((int)'A' + 285);\n    __m328 = _mm256_set1_epi8((int)'Z' - 285);\n\n    __m256i v329 = _mm256_set1_epi8((int)'a' + 290);\n    __m280 = _mm256_set1_epi8((int)'z' - 290);\n    __m281 = _mm256_set1_epi8((int)'A' + 290);\n    __m332 = _mm256_set1_epi8((int)'Z' - 290);\n\n    __m256i v333 = _mm256_set1_epi8((int)'a' + 295);\n    __m284 = _mm256_set1_epi8((int)'z' - 295);\n    __m285 = _mm256_set1_epi8((int)'A' + 295);\n    __m336 = _mm256_set1_epi8((int)'Z' - 295);\n\n    __m256i v337 = _mm256_set1_epi8((int)'a' + 300);\n    __m288 = _mm256_set1_epi8((int)'z' - 300);\n    __m289 = _mm256_set1_epi8((int)'A' + 300);\n    __m340 = _mm256_set1_epi8((int)'Z' - 300);\n\n    __m256i v341 = _mm256_set1_epi8((int)'a' + 305);\n    __m292 = _mm256_set1_epi8((int)'z' - 305);\n    __m293 = _mm256_set1_epi8((int)'A' + 305);\n    __m344 = _mm256_set1_epi8((int)'Z' - 305);\n\n    __m256i v345 = _mm256_set1_epi8((int)'a' + 310);\n    __m296 = _mm256_set1_epi8((int)'z' - 310);\n    __m297 = _mm256_set1_epi8((int)'A' + 310);\n    __m348 = _mm256_set1_epi8((int)'Z' - 310);\n\n    __m256i v351 = _mm256_set1_epi8((int)'a' + 315);\n    __m300 = _mm256_set1_epi8((int)'z' - 315);\n    __m301 = _mm256_set1_epi8((int)'A' + 315);\n    __m352 = _mm256_set1_epi8((int)'Z' - 315);\n\n    __m256i v353 = _mm256_set1_epi8((int)'a' + 320);\n    __m304 = _mm256_set1_epi8((int)'z' - 320);\n    __m305 = _mm256_set1_epi8((int)'A' + 320);\n    __m356 = _mm256_set1_epi8((int)'Z' - 320);\n\n    __m256i v357 = _mm256_set1_epi8((int)'a' + 325);\n    __m308 = _mm256_set1_epi8((int)'z' - 325);\n    __m310 = _mm256_set1_epi8((int)'A' + 325);\n    __m360 = _mm25"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m256i threshold = _mm_set_epi32(t, t, t, t);\n    __m256i vec;\n    bool result = true;\n\n    for(size_t i = 0; i + 4 <= l.size(); i += 4){\n        vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        result &= (_mm_cmpgt_epi32(vec, threshold) == _mm_setzero_si256());\n    }\n\n    // Handle remaining elements\n    for(size_t i = l.size() - (l.size() % 4); i < l.size(); ++i){\n        if(l[i] >= t){\n            return false;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m256i v = _mm256_loadu_si256((__m256i*)&l[0]);\n    __m256i mask = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(t));\n    return _mm256_movemask_epi8(mask) == 0;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[0]));\n    __m256i mask = _mm256_cmpgt_epi32(v, _mm256_set1_epi32(t));\n    return _mm256_movemask_epi8(mask) == 0;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&l[0]));\n    __m256i mask = _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(t));\n    return _mm256_movemask_epi8(mask) == 0;\n}\n```"}
{"task_id": "SimdBench_88_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool below_threshold_simd(std::vector<int>l, int t){\n    __m256i threshold = _mm_set_epi32(t, t, t, t);\n    __m256i vec;\n    bool result = true;\n\n    for(size_t i = 0; i + 4 <= l.size(); i += 4){\n        vec = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&l[i]));\n        result &= _mm_testz_si256(vec, threshold);\n    }\n\n    // Handle remaining elements\n    for(size_t i = l.size() - (l.size() % 4); i < l.size(); ++i){\n        if(l[i] >= t){\n            result = false;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi32(-1);\n    __m256i stack = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i++){\n        if(brackets[i] == '<'){\n            stack = _mm_insert_epi32(stack, i, 0);\n        } else {\n            if(_mm_testz_si256(mask, stack)){\n                return false;\n            }\n            stack = _mm_extract_epi32(stack, 0);\n        }\n    }\n\n    return _mm_testz_si256(mask, stack);\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi32(0);\n    __m256i stack = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i++){\n        if(brackets[i] == '<'){\n            __m256i new_stack = _mm_insert_epi32(stack, i, 0);\n            mask = _mm_or_si256(mask, _mm_cmpgt_epi32(_mm_loadu_si256(&new_stack), _mm_setzero_si256()));\n        } else {\n            __m256i popped_index = _mm_extract_epi32(stack, 0);\n            if(popped_index >= i || popped_index < 0){\n                return false;\n            }\n            stack = _mm_shuffle_epi32(stack, stack, 0b1100);\n        }\n    }\n\n    return _mm_testz_si256(mask, mask);\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi32(-1);\n    __m256i stack = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i++){\n        if(brackets[i] == '<'){\n            stack = _mm_insert_epi32(stack, i, 0);\n        } else {\n            if(_mm_testz_si256(mask, stack)){\n                return false;\n            }\n            stack = _mm_extract_epi32(stack, 0);\n        }\n    }\n\n    return _mm_testz_si256(mask, stack);\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi8(0);\n    __m256i stack = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i++){\n        char c = brackets[i];\n        if(c == '<'){\n            stack = _mm_insert_epi8(stack, 1, 0);\n        } else if(c == '>'){\n            stack = _mm_andnot_si256(_mm_set1_epi8(1), stack);\n        }\n\n        if(_mm_testz_si256(mask, stack)){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_89_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    __m256i mask = _mm_set1_epi8(0);\n    __m256i result = _mm_setzero_si256();\n\n    for(size_t i = 0; i < brackets.size(); i += 4){\n        __m256i current = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i]));\n        __m256i next = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&brackets[i + 4]));\n\n        // Check for matching pairs\n        __m256i match = _mm_cmpeq_epi8(current, next);\n        result = _mm_or_si256(result, match);\n\n        // Update mask to track unmatched opening brackets\n        mask = _mm_andnot_si256(match, mask);\n    }\n\n    // Process remaining characters\n    for(size_t i = brackets.size() % 4; i < brackets.size(); ++i){\n        char c = brackets[i];\n        if(c == '(' && mask[0] != 0) return false;\n        else if(c == ')' && mask[3] != 0) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\nbool correct_bracketing_simd(std::string brackets){\n    __m256i mask = _mm_set1_epi32(0);\n    int n = brackets.size();\n    for(int i=0; i<n-1; i+=4){\n        __m256i vec = _mm_loadu_si256((__m256i*)&brackets[i]);\n        __m256i cmp = _mm_cmpeq_epi8(vec, _mm_set1_epi8('('));\n        mask = _mm_or_si256(mask, cmp);\n    }\n    for(int i=n-1; i>=0; i--){\n        if(brackets[i] == ')'){\n            return false;\n        }\n    }\n    return true;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi32(-1);\n    __m256i acc = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i += 4){\n        __m256i current = _mm_loadu_si256((__m256i*)&brackets[i]);\n        __m256i next = _mm_loadu_si256((__m256i*)&brackets[i+1]);\n\n        __m256i diff = _mm_sub_epi32(current, next);\n\n        acc = _mm_or_si256(acc, diff);\n    }\n\n    // Check if there are any unmatched brackets\n    return _mm_movemask_epi8(acc) != 0;\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi32(0);\n    __m256i stack = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i++){\n        char c = brackets[i];\n        if(c == '('){\n            __m256i new_stack = _mm_insert_epi32(stack, i, 0);\n            stack = new_stack;\n        } else {\n            if(_mm_testz_si256(mask, stack)){\n                return false;\n            }\n            __m256i index = _mm_extract_epi32(stack, 0);\n            mask = _mm_andnot_si256(_mm_set1_epi32(index), mask);\n            stack = _mm_sub_epi32(stack, _mm_set1_epi32(1));\n        }\n    }\n\n    return _mm_testz_si256(mask, stack);\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi32(-1);\n    __m256i result = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i += 4){\n        __m256i current = _mm_loadu_si256((__m256i*)&brackets[i]);\n        __m256i next = _mm_loadu_si256((__m256i*)&brackets[i+1]);\n\n        // Check if there's an opening bracket followed by a closing bracket\n        __m256i check = _mm_cmpeq_epi8(current, next);\n        result = _mm_or_si256(result, check);\n\n        // Shift the masks to the right\n        mask >>= 4;\n    }\n\n    // Check remaining elements\n    for(int i = n % 4; i > 0; --i){\n        char c = brackets[n-i];\n        if(c == '(' && result[0] != 0){\n            return false;\n        }\n        result[0] &= ~_mm_cmpgt_epi8(_mm_set1_epi8(c), _mm_set1_epi8(')'));\n    }\n\n    return _mm_testz_si256(mask, result);\n}\n```"}
{"task_id": "SimdBench_90_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool correct_bracketing_simd(std::string brackets){\n    const int n = brackets.size();\n    __m256i mask = _mm_set1_epi32(-1);\n    __m256i sum = _mm_setzero_si256();\n\n    for(int i = 0; i < n; i += 4){\n        __m256i current = _mm_loadu_si256((__m256i*)&brackets[i]);\n        __m256i next = _mm_loadu_si256((__m256i*)&brackets[i+1]);\n\n        __m256i diff = _mm_sub_epi32(next, current);\n        __m256i result = _mm_and_si256(diff, mask);\n\n        sum = _mm_add_epi32(sum, result);\n    }\n\n    int total_sum = _mm_extract_epi32(sum, 0) + _mm_extract_epi32(sum, 1) +\n                     _mm_extract_epi32(sum, 2) + _mm_extract_epi32(sum, 3);\n\n    return total_sum == 0;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    std::vector<float> result(n - 1);\n    \n    __m256* ptr = (__m256*)xs.data();\n    __m256* res_ptr = (__m256*)&result[0];\n    \n    for(int i = 0; i < n - 1; i += 4){\n        __m256 x = _mm256_loadu_ps(ptr + i);\n        __m256 dx = _mm256_mul_ps(x, _mm256_set1_ps(i + 1));\n        _mm256_storeu_ps(res_ptr + i, dx);\n    }\n    \n    // Handle remaining elements if there are less than 4 elements left\n    for(int i = n - 4; i < n; ++i){\n        result[i] = xs[i + 1] * (i + 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int n = xs.size();\n    std::vector<float> result(n - 1);\n    \n    __m256* ptr = (__m256*)xs.data();\n    __m256* res_ptr = (__m256*)&result[0];\n    \n    for(int i = 0; i < n - 1; i += 4){\n        __m256 x = _mm256_loadu_ps(ptr + i);\n        __m256 dx = _mm256_mul_ps(x, _mm256_set1_ps(i + 1));\n        _mm256_storeu_ps(res_ptr + i, dx);\n    }\n    \n    // Handle remaining elements if n is not a multiple of 4\n    for(int i = n - 1; i >= 0; --i){\n        result[i] = i == 0 ? 0 : xs[i] * i;\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    int n = xs.size();\n    __m256* ptr = (__m256*)xs.data();\n    __m256 result[n];\n\n    for(int i = 0; i < n - 1; i++){\n        __m256 temp = _mm256_loadu_ps(ptr + i);\n        __m256 next_temp = _mm256_loadu_ps(ptr + i + 1);\n        __m256 diff = _mm256_sub_ps(next_temp, temp);\n        _mm256_storeu_ps(result + i, diff);\n    }\n\n    // Handle remaining elements if there are less than 4 elements\n    for(int i = n - 1; i >= 0; i--){\n        result[i] = _mm256_setzero_ps();\n    }\n\n    return vector<float>(result, result + n);\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int n = xs.size();\n    std::vector<float> result(n - 1);\n    \n    __m256* ptr = (__m256*)xs.data();\n    __m256* res_ptr = (__m256*)&result[0];\n    \n    for(int i = 0; i < n - 1; i += 4){\n        __m256 x = _mm256_loadu_ps(ptr + i);\n        __m256 dx = _mm256_mul_ps(x, _mm256_set1_ps(i + 1));\n        _mm256_storeu_ps(res_ptr + i, dx);\n    }\n    \n    // Handle remaining elements if there are less than 4 elements left\n    for(int i = n - 4; i < n; ++i){\n        result[i] = xs[i + 1] * (i + 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_91_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<float> derivative_simd(std::vector<float> xs){\n    const int n = xs.size();\n    std::vector<float> result(n - 1);\n    \n    __m256* ptr = (__m256*)xs.data();\n    __m256* res_ptr = (__m256*)&result[0];\n    \n    for(int i = 0; i < n - 1; i += 4){\n        __m256 x = _mm256_loadu_ps(ptr + i);\n        __m256 dx = _mm256_mul_ps(x, _mm256_set1_ps(i + 1));\n        _mm256_storeu_ps(res_ptr + i, dx);\n    }\n    \n    // Handle remaining elements if n is not a multiple of 4\n    for(int i = n - 1 - (n % 4); i >= 0; --i){\n        result[i] = xs[i + 1] * (i + 1);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n\nint vowels_count_simd(std::string s){\n    int count = 0;\n    const char* str = s.c_str();\n    __m256i mask = _mm_set_epi8(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n    __m256i vowels = _mm_set_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U');\n\n    while (*str != '\\0') {\n        __m256i ch = _mm_loadu_si256((__m256i*)str);\n        ch &= mask;\n        ch = _mm_or_si256(ch, _mm_cmpeq_epi8(ch, vowels));\n        count += _mm_popcnt_u32(_mm_extract_epi32(ch, 0)) + _mm_popcnt_u32(_mm_extract_epi32(ch, 1)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(ch, 2)) + _mm_popcnt_u32(_mm_extract_epi32(ch, 3));\n        str += 4;\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s){\n    int count = 0;\n    __m256i mask = _mm_set1_epi8('aeiouAEIOU');\n    __m256i str = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&s[0]));\n    __m256i result = _mm_cmpeq_epi8(str, mask);\n    count += _mm_popcnt_u32(_mm_extract_epi32(result, 0));\n    count += _mm_popcnt_u32(_mm_extract_epi32(result, 1));\n    count += _mm_popcnt_u32(_mm_extract_epi32(result, 2));\n    count += _mm_popcnt_u32(_mm_extract_epi32(result, 3));\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s){\n    int count = 0;\n    __m256i mask = _mm_set1_epi8('aeiouAEIOU');\n    __m256i str = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&s[0]));\n    \n    while (__builtin_constant_p(str)) {\n        __m256i result = _mm_or_si256(_mm_cmpeq_epi8(str, mask), _mm_and_si256(_mm_cmpgt_epi8(str, _mm_set1_epi8('y')), _mm_set1_epi8(1)));\n        count += _mm_popcnt_u32(_mm_extract_epi32(result, 0));\n        count += _mm_popcnt_u32(_mm_extract_epi32(result, 1));\n        count += _mm_popcnt_u32(_mm_extract_epi32(result, 2));\n        count += _mm_popcnt_u32(_mm_extract_epi32(result, 3));\n        \n        if (__builtin_constant_p(s.substr(4))) {\n            str = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&s[4]));\n        } else {\n            break;\n        }\n    }\n    \n    return count + _mm_popcnt_u32(_mm_loadu_si256(reinterpret_cast<const __m256i*>(&s[s.size() - 1])));\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint vowels_count_simd(std::string s){\n    int count = 0;\n    __m256i mask = _mm_set_epi8(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0);\n    __m256i vowels = _mm_set_epi8('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U');\n\n    for (size_t i = 0; i + 4 <= s.size(); i += 4) {\n        __m256i word = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i result = _mm_or_si256(_mm_cmpeq_epi8(word, vowels), mask);\n        count += _mm_popcnt_u32(_mm_extract_epi32(result, 0)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(result, 1)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(result, 2)) +\n                 _mm_popcnt_u32(_mm_extract_epi32(result, 3));\n    }\n\n    // Process remaining characters\n    for (size_t i = s.size() - (s.size() % 4); i < s.size(); ++i) {\n        if (_mm_test_all_ones(_mm_cmpeq_epi8(_mm_loadu_si256((__m256i*)&s[i]), vowels))) {\n            count++;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_92_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint vowels_count_simd(std::string s){\n    int count = 0;\n    __m256i mask = _mm_set_epi8(97, 101, 105, 111, 117, 99, 104, 111, 114, 116); // ASCII values of 'a', 'e', 'i', 'o', 'u'\n    __m256i lower_mask = _mm_set_epi8(97, 101, 105, 111, 117, 99, 104, 111, 114, 116);\n    __m256i upper_mask = _mm_set_epi8(65, 69, 73, 79, 85, 97, 101, 105, 111, 114);\n\n    for(size_t i = 0; i + 4 <= s.size(); i += 4){\n        __m256i vec = _mm_loadu_si256((__m256i*)&s[i]);\n        vec &= lower_mask;\n        vec |= upper_mask;\n        vec -= mask;\n        count += _mm_popcnt_u32(_mm_extract_epi32(vec, 0)) +\n               _mm_popcnt_u32(_mm_extract_epi32(vec, 1)) +\n               _mm_popcnt_u32(_mm_extract_epi32(vec, 2)) +\n               _mm_popcnt_u32(_mm_extract_epi32(vec, 3));\n    }\n\n    for(size_t i = s.size() - (s.size() % 4); i < s.size(); ++i){\n        if(s[i] == 'a' || s[i] == 'e' || s[i] == 'i' || s[i] == 'o' || s[i] == 'u'){\n            ++count;\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_char;\n\n    for(size_t i = 0; i < s.size(); i += 4){\n        vec_char = _mm256_loadu_si256((__m256i*)&s[i]);\n        vec_sum = _mm256_add_epi8(vec_sum, _mm256_sub_epi8(_mm256_and_si256(vec_char, _mm256_set1_epi8(0x3F)), _mm256_set1_epi8(0x20)));\n    }\n\n    for(size_t i = 4; i < s.size(); ++i){\n        vec_char = _mm_loadu_si256((__m256i*)&s[i]);\n        vec_sum = _mm256_add_epi8(vec_sum, _mm256_sub_epi8(_mm256_and_si256(vec_char, _mm256_set1_epi8(0x3F)), _mm256_set1_epi8(0x20)));\n    }\n\n    sum = _mm256_reduce_add_epi8(vec_sum);\n    return sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    __m256i vec_sum = _mm256_setzero_si256();\n    __m256i vec_char;\n\n    for (size_t i = 0; i < s.size(); i += 4) {\n        vec_char = _mm256_loadu_si256((__m256i*)&s[i]);\n        vec_sum = _mm256_add_epi8(vec_sum, _mm256_and_si256(_mm256_cmpgt_epi8(vec_char, _mm256_set1_epi8('0')), _mm256_sub_epi8(vec_char, _mm256_set1_epi8('0'))));\n    }\n\n    for (size_t i = 4; i < s.size(); ++i) {\n        if (s[i] >= '0' && s[i] <= '9') {\n            sum += s[i] - '0';\n        }\n    }\n\n    return _mm256_extract_epi32(vec_sum, 0) + _mm256_extract_epi32(vec_sum, 1) +\n           _mm256_extract_epi32(vec_sum, 2) + _mm256_extract_epi32(vec_sum, 3) + sum;\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vtemp;\n\n    for(size_t i = 0; i < s.size(); i += 4){\n        vtemp = _mm256_loadu_si256((__m256i*)&s[i]);\n        vtemp = _mm256_and_si256(vtemp, _mm256_set1_epi8('A' - 'a'));\n        vtemp = _mm256_or_si256(vtemp, _mm256_set1_epi8('Z' - 'z'));\n        vtemp = _mm256_sad_epu8(vtemp, _mm256_set1_epi8(0));\n        vsum = _mm256_add_epi32(vsum, vtemp);\n    }\n\n    for(; i < s.size(); ++i){\n        if(s[i] >= 'A' && s[i] <= 'Z'){\n            sum += s[i] - 'A';\n        } else if(s[i] >= 'a' && s[i] <= 'z'){\n            sum += s[i] - 'a';\n        }\n    }\n\n    return _mm256_reduce_add_epi32(vsum, _MM_FMA3_ACCUMULATE);\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vstr = _mm256_setzero_si256();\n\n    for(int i = 0; i < s.size(); i += 4){\n        __m256i vch = _mm256_loadu_si256((__m256i*)&s[i]);\n        vstr = _mm256_or_si256(vstr, _mm256_and_si256(_mm256_cmpgt_epi8(vch, _mm256_set1_epi8('0')), _mm256_cmplt_epi8(vch, _mm256_set1_epi8('9'))));\n        vsum = _mm256_add_epi32(vsum, _mm256_cvtepi8_epi32(vstr));\n    }\n\n    for(int i = 0; i < s.size() % 4; ++i){\n        char ch = s[i];\n        if(ch >= '0' && ch <= '9'){\n            sum += ch - '0';\n        }\n    }\n\n    return _mm256_reduce_add_epi32(vsum);\n}\n```"}
{"task_id": "SimdBench_93_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint digitSum_simd(std::string s){\n    int sum = 0;\n    __m256i vsum = _mm256_setzero_si256();\n    __m256i vtemp;\n\n    for(size_t i = 0; i < s.size(); i += 4){\n        vtemp = _mm256_loadu_si256((__m256i*)&s[i]);\n        vtemp = _mm256_and_si256(vtemp, _mm256_set1_epi8('A' - 'a'));\n        vtemp = _mm256_or_si256(vtemp, _mm256_set1_epi8('Z' - 'z'));\n        vtemp = _mm256_sad_epu8(vtemp, _mm256_set1_epi8(0));\n        vsum = _mm256_add_epi32(vsum, _mm256_extract_epi32(vtemp, 0));\n        vsum = _mm256_add_epi32(vsum, _mm256_extract_epi32(vtemp, 1));\n        vsum = _mm256_add_epi32(vsum, _mm256_extract_epi32(vtemp, 2));\n        vsum = _mm256_add_epi32(vsum, _mm256_extract_epi32(vtemp, 3));\n    }\n\n    for(size_t i = s.size() % 4; i < s.size(); ++i){\n        char c = s[i];\n        if(c >= 'A' && c <= 'Z') sum += c - 'A';\n        else if(c >= 'a' && c <= 'z') sum += c - 'a';\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m256i min_even = _mm_setzero_si256();\n    int min_index = -1;\n\n    for(int i = 0; i < arr.size(); i += 8){\n        __m256i vec = _mm_loadu_si256((__m256i*)&arr[i]);\n        __m256i even_mask = _mm_cmpgt_epi32(vec, _mm_setzero_si256());\n        __m256i even_values = _mm_and_si256(even_mask, vec);\n        __m256i even_min = _mm_min_epi32(even_values, min_even);\n\n        if(_mm_movemask_epi8(even_min) != 0){\n            min_even = even_min;\n            min_index = i / 8;\n        }\n    }\n\n    std::vector<int> result;\n    if(min_index != -1){\n        result.push_back(_mm_extract_epi32(min_even, 0));\n        result.push_back(min_index);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)arr.data());\n    __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n    __m256i even_mask = _mm256_and_si256(mask, _mm256_cmpgt_epi32(vec, _mm256_set1_epi32(0)));\n    __m256i even_values = _mm256_and_si256(even_mask, vec);\n\n    int min_index = -1;\n    int min_value = INT_MAX;\n\n    for(int i = 0; i < 8; ++i){\n        __m256i current_even_values = _mm256_extract_epi32(even_values, i);\n        __m256i current_min_value = _mm256_min_epi32(current_even_values, _mm256_set1_epi32(min_value));\n        __m256i current_min_index = _mm256_cmpeq_epi32(current_even_values, current_min_value);\n        min_value = _mm256_extract_epi32(current_min_value, 0);\n        min_index = _mm256_extract_epi32(current_min_index, 0);\n    }\n\n    std::vector<int> result = {min_value, min_index};\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m256i min_even = _mm_setzero_si256();\n    __m256i min_index = _mm_setzero_si256();\n\n    int i = 0;\n    for(; i + 8 <= arr.size(); i += 8) {\n        __m256i vec = _mm_loadu_si256((__m256i*)&arr[i]);\n        __m256i mask = _mm_cmpgt_epi32(_mm_shuffle_epi32(vec, vec, _MM_SHUFFLE(0, 1, 2, 3)), _mm_setzero_si256());\n        __m256i even_mask = _mm_and_si256(mask, _mm_cmpeq_epi32(vec, _mm_cvtsi32_si256((int)_mm_extract_epi32(vec, 0))));\n        __m256i even_values = _mm_and_si256(even_mask, vec);\n        __m256i even_indices = _mm_and_si256(even_mask, _mm_setr_epi32(i, i+1, i+2, i+3));\n\n        __m256i even_min = _mm_min_epi32(even_values, min_even);\n        __m256i even_min_index = _mm_min_epi32(even_indices, min_index);\n\n        min_even = even_min;\n        min_index = even_min_index;\n    }\n\n    for(; i < arr.size(); ++i) {\n        if(arr[i] % 2 == 0 && (min_even == _mm_setzero_si256() || arr[i] < _mm_extract_epi32(min_even, 0))) {\n            min_even = _mm_set_epi32(arr[i], arr[i]);\n            min_index = _mm_set_epi32(i, i);\n        }\n    }\n\n    std::vector<int> result;\n    result.push_back(_mm_extract_epi32(min_even, 0));\n    result.push_back(_mm_extract_epi32(min_index, 0));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n    __m256i even_mask = _mm256_and_si256(mask, _mm256_cmpge_epi32(vec, _mm256_set1_epi32(0)));\n    __m256i even_values = _mm256_and_si256(even_mask, vec);\n    __m256i min_even_values = _mm256_min_epu32(even_values, _mm256_permutevar_epi32(even_values, _mm256_set_epi32(0, 7, 4, 1)));\n    __m256i min_indices = _mm256_pcmpge_epi32(min_even_values, _mm256_setzero_si256());\n    __m256i indices = _mm256_packs_epi32(_mm256_packs_epi32(min_indices, _mm256_set_epi32(0, 0)), _mm256_set_epi32(0, 0));\n    int min_index = _mm256_extract_epi32(indices, 0);\n\n    std::vector<int> result;\n    result.push_back(_mm256_extract_epi32(min_even_values, 0));\n    result.push_back(min_index);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_94_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> pluck_simd(std::vector<int> arr){\n    if(arr.empty()) return {};\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n    __m256i even_mask = _mm256_and_si256(mask, _mm256_cmpge_epi32(vec, _mm256_set1_epi32(0)));\n    __m256i even_values = _mm256_and_si256(even_mask, vec);\n\n    int min_index = -1;\n    float min_even_value = FLT_MAX;\n\n    for(int i = 0; i < 4; ++i){\n        __m256i current_values = _mm256_extract_epi32(even_values, i);\n        __m256i current_min = _mm256_min_ps(_mm256_set1_ps(min_even_value), _mm256_castsi256_ps(current_values));\n        min_even_value = _mm_cvtss_f32(current_min);\n        min_index = std::max(min_index, _mm256_extract_epi32(current_values, _mm256_argmin_ps(current_min)));\n    }\n\n    if(min_index == -1) return {};\n\n    std::vector<int> result = {static_cast<int>(min_even_value), min_index};\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst){\n    int n = lst.size();\n    __m256i* vec = (__m256i*)lst.data();\n    __m256i* end = vec + (n / 8);\n    __m256i max_count = _mm_set1_epi32(0);\n\n    while (vec != end) {\n        __m256i count = _mm_popcnt_u32(_mm_movemask_epi8(_mm_cmpeq_epi32(*vec, *end)));\n        if (_mm_cmpgt_epi32(count, max_count)) {\n            max_count = count;\n        }\n        vec += 8;\n    }\n\n    int result = -1;\n    for (int i = 0; i < n; ++i) {\n        if (lst[i] <= max_count && lst[i] > 0) {\n            result = lst[i];\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst){\n    const int N = lst.size();\n    __m256i* vec = (__m256i*)lst.data();\n    __m256i* end = vec + N / 8;\n    __m256i* max_freq_vec = (__m256i*)malloc(N * sizeof(__m256i));\n    memset(max_freq_vec, 0, N * sizeof(__m256i));\n\n    while (vec != end) {\n        __m256i count = _mm256_set_epi32(0, 0, 0, 0);\n        for (int i = 0; i < 8; ++i) {\n            count += _mm256_add_epi32(count, _mm256_loadu_si256(vec + i));\n        }\n        __m256i max_count = _mm256_max_epi32(_mm256_set_epi32(0, 0, 0, 0), count);\n        _mm256_storeu_si256(max_freq_vec, max_count);\n        vec += 8;\n    }\n\n    int max_freq = 0;\n    for (int i = 0; i < N; ++i) {\n        if (max_freq_vec[i] >= lst[i]) {\n            max_freq = std::max(max_freq, lst[i]);\n        }\n    }\n\n    free(max_freq_vec);\n    return max_freq > 0 ? max_freq : -1;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    __m256i* vec = (__m256i*)lst.data();\n    __m256i* end = vec + (n / 8);\n    __m256i* result = (__m256i*)malloc(n * sizeof(__m256i));\n\n    while (vec != end) {\n        __m256i count = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_loadu_si256(vec), _mm256_loadu_si256(vec));\n        count = _mm256_add_epi32(count, mask);\n\n        vec += 8;\n    }\n\n    // Copy results back to original array\n    for (int i = 0; i < n; ++i) {\n        result[i] = _mm256_extract_epi32(count, i);\n    }\n\n    int max_value = -1;\n    for (int i = 0; i < n; ++i) {\n        if (result[i] > 0 && result[i] >= lst[i]) {\n            max_value = std::max(max_value, lst[i]);\n        }\n    }\n\n    free(result);\n    return max_value;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst){\n    int n = lst.size();\n    __m256i* vec = (__m256i*)lst.data();\n    __m256i* end = vec + n / 8;\n    __m256i* max_freq = (__m256i*)malloc(n * sizeof(__m256i));\n    memset(max_freq, -1, n * sizeof(__m256i));\n\n    while (vec != end) {\n        __m256i count = _mm256_set_epi32(0, 0, 0, 0);\n        for (int i = 0; i < 8; ++i) {\n            count += _mm256_add_epi32(count, _mm256_extract_epi32(vec[i], 0));\n        }\n        for (int i = 0; i < 8; ++i) {\n            if (_mm256_cmpgt_epi32(_mm256_extract_epi32(count, i), _mm256_extract_epi32(vec[i], 0))) {\n                max_freq[_mm256_extract_epi32(vec[i], 0)] = _mm256_extract_epi32(count, i);\n            }\n        }\n        vec += 8;\n    }\n\n    int result = -1;\n    for (int i = 0; i < n; ++i) {\n        if (max_freq[i] >= i && max_freq[i] > result) {\n            result = max_freq[i];\n        }\n    }\n\n    free(max_freq);\n    return result;\n}\n```"}
{"task_id": "SimdBench_95_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint search_simd(std::vector<int> lst) {\n    int n = lst.size();\n    __m256i* vec = (__m256i*)lst.data();\n    __m256i* end = vec + (n / 8);\n    __m256i* result = (__m256i*)malloc(n * sizeof(__m256i));\n    __m256i* res_end = result + n;\n\n    while (vec != end) {\n        __m256i count = _mm256_set1_epi32(0);\n        __m256i mask = _mm256_cmpgt_epi32(*vec, *vec);\n\n        for (int i = 0; i < 8; ++i) {\n            count = _mm256_add_epi32(count, _mm256_and_si256(mask, *vec));\n            vec++;\n        }\n\n        for (int i = 0; i < 8; ++i) {\n            if (_mm256_movemask_epi8(_mm256_cmpeq_epi32(count, *result)) > 0) {\n                break;\n            }\n            *result = _mm256_set1_epi32(*count);\n            result++;\n        }\n    }\n\n    int max_val = -1;\n    for (int i = 0; i < n; ++i) {\n        if (*result == lst[i] && *result >= lst[i]) {\n            max_val = lst[i];\n            break;\n        }\n    }\n\n    free(result);\n    return max_val;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[0]));\n    __m256i reversed_vec = _mm256_permutevar_ps(vec, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n    __m256i diff = _mm256_sub_epi32(vec, reversed_vec);\n    __m256i mask = _mm256_cmpgt_epi32(diff, _mm256_setzero_si256());\n    __m256i sum = _mm256_reduce_add_epi32(mask, _mm256_set1_epi32(0));\n    return _mm256_movemask_epi8(sum) == 0 && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[0]));\n    __m256i reversed_vec = _mm256_permutevar_epi32(vec, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n    __m256i diff = _mm256_sub_epi32(vec, reversed_vec);\n    __m256i mask = _mm256_cmpgt_epi32(diff, _mm256_setzero_si256());\n    bool is_balanced = _mm256_movemask_epi8(mask) == 0;\n    __m256i sum = _mm256_reduce_add_epi32(_mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[0])));\n    bool is_less_than_w = _mm256_cmpgt_epi32(sum, _mm256_set1_epi32(w));\n    return is_balanced && is_less_than_w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q,int w){\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[0]));\n    __m256i reversed_vec = _mm256_permutevar_epi8(vec, _mm256_set_epi8(7, 6, 5, 4, 3, 2, 1, 0));\n    __m256i diff = _mm256_sub_epi32(vec, reversed_vec);\n    __m256i mask = _mm256_cmpgt_epi32(diff, _mm256_setzero_si256());\n    int count = _mm_popcnt_u32(_mm256_movemask_epi8(mask));\n    \n    __m256i sum = _mm256_reduce_add_epi32(vec);\n    bool is_balanced = count == 0;\n    bool can_fly = sum <= w && is_balanced;\n    \n    return can_fly;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q,int w){\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[0]));\n    __m256i reversed_vec = _mm256_permutevar_ps(vec, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n    __m256i diff = _mm256_sub_epi32(vec, reversed_vec);\n    __m256i mask = _mm256_cmpgt_epi32(diff, _mm256_setzero_si256());\n    int count = _mm_popcnt_u32(_mm_movemask_epi8(mask));\n\n    bool is_balanced = count == 0;\n    int sum = _mm_reduce_add_epi32(vec);\n\n    return is_balanced && sum <= w;\n}\n```"}
{"task_id": "SimdBench_96_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool will_it_fly_simd(std::vector<int> q, int w) {\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&q[0]));\n    __m256i reversed_vec = _mm256_permutevar_ps(vec, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n    __m256i diff = _mm256_sub_epi32(vec, reversed_vec);\n    __m256i mask = _mm256_cmpgt_epi32(diff, _mm256_setzero_si256());\n    __m256i sum = _mm256_reduce_add_epi32(mask, _mm256_setzero_si256());\n    return _mm256_movemask_epi8(sum) == 0 && sum <= w;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    __m256i* ptr = (__m256i*)arr.data();\n    __m256i* end = ptr + (n / 8);\n    int changes = 0;\n\n    while (ptr < end) {\n        __m256i vec = _mm256_loadu_si256(ptr);\n        __m256i reversed_vec = _mm256_permutevar_ps(vec, _mm256_set_epi32(7, 6, 5, 4, 3, 2, 1, 0));\n        __m256i diff = _mm256_sub_epi32(reversed_vec, vec);\n        changes += _mm256_movemask_epi8(diff);\n        ptr += 8;\n    }\n\n    // Handle remaining elements if n is odd\n    if (n % 8 != 0) {\n        __m128i last_element = _mm_loadu_si128((__m128i*)(ptr + n - 1));\n        __m128i reversed_last_element = _mm_shuffle_epi32(last_element, last_element, 0b1100);\n        __m128i diff = _mm_sub_epi32(reversed_last_element, last_element);\n        changes += _mm_movemask_epi8(diff);\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    __m256i* vec_arr = (__m256i*)arr.data();\n\n    // Process pairs of elements in parallel\n    for (int i = 0; i < n / 4; ++i) {\n        __m256i pair = _mm256_loadu_si256(vec_arr + i);\n        __m256i reversed_pair = _mm256_permutevar_ps(pair, _mm256_set_epi32(3, 2, 1, 0));\n        __m256i diff = _mm256_sub_epi32(reversed_pair, pair);\n        __m256i count = _mm256_sad_epu8(diff, _mm256_setzero_si256());\n        _mm_storeu_si128((__m128i*)(vec_arr + i), _mm256_extract_epi16(count, 0));\n    }\n\n    // Handle remaining elements if there are less than 4\n    for (int i = n % 4; i < n; ++i) {\n        int diff = abs(arr[i] - arr[n - i - 1]);\n        _mm_storeu_si128((__m128i*)&arr[i], _mm_set_epi32(diff, diff, diff, diff));\n    }\n\n    // Sum up all differences to get the total number of changes\n    int total_changes = 0;\n    for (int i = 0; i < n; ++i) {\n        total_changes += arr[i];\n    }\n\n    return total_changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    __m256i *vec_arr = (__m256i *)arr.data();\n\n    // Load two vectors at once\n    __m256i vec1 = _mm256_loadu_si256(vec_arr);\n    __m256i vec2 = _mm256_loadu_si256(vec_arr + n / 2);\n\n    // Compare the vectors\n    __m256i cmp = _mm256_cmpeq_epi32(vec1, vec2);\n\n    // Count the number of mismatches\n    int mismatch_count = _mm_popcnt_u32(_mm_movemask_epi32(cmp));\n\n    // Calculate the total number of changes needed\n    int total_changes = n - mismatch_count;\n\n    return total_changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr) {\n    int n = arr.size();\n    __m256i *vec_arr = (__m256i *)arr.data();\n    __m256i *end_vec_arr = vec_arr + (n / 8);\n    int changes = 0;\n\n    while (vec_arr != end_vec_arr) {\n        __m256i first_half = _mm256_loadu_si256(vec_arr);\n        __m256i second_half = _mm256_loadu_si256(end_vec_arr - 1);\n\n        // Compare the two halves\n        __m256i cmp_result = _mm256_cmpeq_epi32(first_half, second_half);\n\n        // Count the differences\n        changes += _mm256_popcnt_u32(_mm256_extract_epi32(cmp_result, 0)) +\n                   _mm256_popcnt_u32(_mm256_extract_epi32(cmp_result, 1)) +\n                   _mm256_popcnt_u32(_mm256_extract_epi32(cmp_result, 2)) +\n                   _mm256_popcnt_u32(_mm256_extract_epi32(cmp_result, 3));\n\n        // Move to the next pair\n        vec_arr++;\n        end_vec_arr--;\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_97_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint smallest_change_simd(std::vector<int> arr){\n    int n = arr.size();\n    __m256i *vec_arr = (__m256i*)arr.data();\n    int changes = 0;\n\n    // Process pairs of elements in parallel\n    for (int i = 0; i < n / 4; ++i) {\n        __m256i v1 = _mm256_loadu_si256(vec_arr + i);\n        __m256i v2 = _mm256_loadu_si256(vec_arr + n - 1 - i);\n\n        // Compare elements and count mismatches\n        __m256i diff = _mm256_sub_epi32(v1, v2);\n        changes += _mm256_popcnt_u32(_mm256_movemask_epi8(diff));\n    }\n\n    // Handle remaining elements if the vector size is odd\n    if (n % 4 != 0) {\n        int last_element = arr[n / 4];\n        for (int i = 0; i < n % 4; ++i) {\n            if (arr[i] != last_element) {\n                changes++;\n            }\n        }\n    }\n\n    return changes;\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int n = lst1.size();\n    int m = lst2.size();\n\n    __m256i *vec1 = (__m256i *)lst1.data();\n    __m256i *vec2 = (__m256i *)lst2.data();\n\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n / 8; ++i){\n        sum1 = _mm256_add_epi8(sum1, _mm256_loadu_si256(vec1 + i));\n        sum2 = _mm256_add_epi8(sum2, _mm256_loadu_si256(vec2 + i));\n    }\n\n    for(int i = n % 8; i < n; ++i){\n        sum1 = _mm256_add_epi8(sum1, _mm256_loadu_si256(&vec1[i]));\n        sum2 = _mm256_add_epi8(sum2, _mm256_loadu_si256(&vec2[i]));\n    }\n\n    int count1 = _mm256_extract_epi32(_mm256_sad_epu8(sum1, _mm256_setzero_si256()), 0);\n    int count2 = _mm256_extract_epi32(_mm256_sad_epu8(sum2, _mm256_setzero_si256()), 0);\n\n    if(count1 == count2){\n        return lst1;\n    } else {\n        return count1 < count2 ? lst1 : lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int n = lst1.size();\n    int m = lst2.size();\n\n    __m256i v1 = _mm256_setzero_si256();\n    __m256i v2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i str1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i str2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        v1 = _mm256_add_epi32(v1, _mm256_sub_epi32(_mm256_count_nonzero(str1), _mm256_count_nonzero(str2)));\n    }\n\n    for(int i = 8; i < n; i++){\n        __m256i str1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst1[i]));\n        __m256i str2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst2[i]));\n\n        v1 = _mm256_add_epi32(v1, _mm256_sub_epi32(_mm256_count_nonzero(str1), _mm256_count_nonzero(str2)));\n    }\n\n    int count = _mm256_extract_epi32(v1, 0);\n    if(count == m){\n        return lst1;\n    } else {\n        std::vector<std::string> result;\n        for(int i = 0; i < n; i++){\n            if(_mm256_testz_si256(v1, _mm256_set1_epi32(i))){\n                result.push_back(lst1[i]);\n            }\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int n = lst1.size();\n    int m = lst2.size();\n\n    __m256i *vec1 = (__m256i *)lst1.data();\n    __m256i *vec2 = (__m256i *)lst2.data();\n\n    __m256i count1 = _mm256_setzero_si256();\n    __m256i count2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i str1 = _mm256_loadu_si256(vec1 + i);\n        __m256i str2 = _mm256_loadu_si256(vec2 + i);\n\n        count1 = _mm256_add_epi8(count1, _mm256_sub_epi8(str1, str2));\n    }\n\n    int total_count1 = _mm256_extract_epi32(count1, 0) +\n                        _mm256_extract_epi32(count1, 1) +\n                        _mm256_extract_epi32(count1, 2) +\n                        _mm256_extract_epi32(count1, 3);\n\n    int total_count2 = _mm256_extract_epi32(count2, 0) +\n                        _mm256_extract_epi32(count2, 1) +\n                        _mm256_extract_epi32(count2, 2) +\n                        _mm256_extract_epi32(count2, 3);\n\n    if(total_count1 == total_count2){\n        return lst1;\n    } else {\n        return lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int n = lst1.size();\n    int m = lst2.size();\n\n    __m256i *vec1 = (__m256i *)lst1.data();\n    __m256i *vec2 = (__m256i *)lst2.data();\n\n    __m256i sum1 = _mm256_setzero_si256();\n    __m256i sum2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n / 8; ++i){\n        sum1 = _mm_add_epi32(sum1, _mm256_sad_epu8(_mm_loadu_si256(vec1 + i), _mm_loadu_si256(vec2 + i)));\n    }\n\n    for(int i = n % 8; i < n; ++i){\n        sum1 = _mm_add_epi32(sum1, _mm_sad_epu8(_mm_loadu_si256(vec1 + i), _mm_loadu_si256(vec2 + i)));\n    }\n\n    int count1 = _mm_cvtsi128_si32(_mm_extract_epi32(sum1, 0));\n    int count2 = _mm_cvtsi128_si32(_mm_extract_epi32(sum2, 0));\n\n    if(count1 == count2){\n        return lst1;\n    } else {\n        return count1 < count2 ? lst1 : lst2;\n    }\n}\n```"}
{"task_id": "SimdBench_98_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <algorithm>\n\nstd::vector<std::string> total_match_simd(const std::vector<std::string> & lst1, const std::vector<std::string> & lst2){\n    int n = lst1.size();\n    int m = lst2.size();\n\n    __m256i *vec1 = (__m256i *)lst1.data();\n    __m256i *vec2 = (__m256i *)lst2.data();\n\n    __m256i count1 = _mm256_setzero_si256();\n    __m256i count2 = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i str1 = _mm256_loadu_si256(vec1 + i);\n        __m256i str2 = _mm256_loadu_si256(vec2 + i);\n\n        count1 = _mm256_add_epi8(count1, _mm256_popcnt_u8(str1));\n        count2 = _mm256_add_epi8(count2, _mm256_popcnt_u8(str2));\n    }\n\n    int total_count1 = _mm256_extract_epi32(count1, 0) +\n                       _mm256_extract_epi32(count1, 1) +\n                       _mm256_extract_epi32(count1, 2) +\n                       _mm256_extract_epi32(count1, 3);\n\n    int total_count2 = _mm256_extract_epi32(count2, 0) +\n                       _mm256_extract_epi32(count2, 1) +\n                       _mm256_extract_epi32(count2, 2) +\n                       _mm256_extract_epi32(count2, 3);\n\n    if(total_count1 == total_count2){\n        return lst1;\n    } else {\n        std::vector<std::string> result;\n        for(int i = 0; i < n; i++){\n            if(_mm256_popcnt_u8(_mm256_loadu_si256(vec1 + i)) < _mm256_popcnt_u8(_mm256_loadu_si256(vec2 + i))){\n                result.push_back(lst1[i]);\n            }\n        }\n        return result;\n    }\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_simple_power_simd(uint64_t x, uint64_t n){\n    __m256i x_vec = _mm256_set_epi64x(x >> 32, x >> 24, x >> 16, x >> 8, x >> 0);\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 24, n >> 16, n >> 8, n >> 0);\n\n    __m256i result = _mm256_cmp_eq_epi64(x_vec, n_vec);\n\n    return _mm256_movemask_epi8(result) == 0xFFFFFFFF;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c++\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    // Check if n is 0 or 1\n    if (n == 0 || n == 1) return x == 1;\n\n    // Initialize variables\n    __m256i x_vec = _mm256_set_epi64x(x >> 32, x >> 24, x >> 16, x >> 8);\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 24, n >> 16, n >> 8);\n\n    // Loop through each byte of x and n\n    for (int i = 0; i < 4; ++i) {\n        __m256i x_byte = _mm256_extract_epi64(x_vec, i);\n        __m256i n_byte = _mm256_extract_epi64(n_vec, i);\n\n        // Check if n_byte is 0 or 1\n        if (_mm256_testz_si256(n_byte, n_byte)) continue;\n\n        // Check if x_byte is divisible by n_byte\n        if (!_mm256_testz_si256(_mm256_div_epi64(x_byte, n_byte), _mm256_div_epi64(x_byte, n_byte))) continue;\n\n        // If both conditions are met, return false\n        return false;\n    }\n\n    // If all conditions are met, return true\n    return true;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c++\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    // Check if n is 1, which is not a valid base for powers\n    if (n == 1) return x == 1;\n\n    // Use AVX2 instructions to check if x is a power of n\n    __m256i x_vec = _mm256_set_epi64x(x >> 32, x >> 24, x >> 16, x >> 8, x >> 0);\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 24, n >> 16, n >> 8, n >> 0);\n\n    // Perform bitwise AND operation between x_vec and n_vec\n    __m256i result = _mm256_and_si256(x_vec, n_vec);\n\n    // Check if all elements in the result vector are zero\n    bool is_power_of_n = _mm256_testz_si256(result, result);\n\n    // Return the result\n    return is_power_of_n;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c++\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    // Check if n is 1, which is the only trivial case\n    if (n == 1) return x == 1;\n\n    // Use AVX2 to check if x is a power of n\n    __m256i x_vec = _mm256_set_epi64x(x >> 32, x >> 16, x >> 8, x);\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 16, n >> 8, n);\n\n    // Compare each pair of elements from x_vec and n_vec\n    __m256i cmp_result = _mm256_cmpeq_epi64(x_vec, n_vec);\n\n    // Count the number of matching pairs\n    int count = _mm_popcnt_u32(_mm_movemask_epi8(cmp_result));\n\n    // If all pairs match, then x is a power of n\n    return count == 4;\n}\n```"}
{"task_id": "SimdBench_99_AVX", "completion": "```c++\nbool is_simple_power_simd(uint64_t x, uint64_t n) {\n    // Check if n is 1, which is the only power of 1\n    if (n == 1) return x == 1;\n\n    // Initialize variables for AVX/AVX2 operations\n    __m256i x_vec = _mm256_set_epi64x(x >> 32, x >> 24, x >> 16, x >> 8);\n    __m256i n_vec = _mm256_set_epi64x(n >> 32, n >> 24, n >> 16, n >> 8);\n\n    // Perform bitwise AND operation between x_vec and n_vec\n    __m256i result = _mm256_and_si256(x_vec, n_vec);\n\n    // Check if all bits in the result are zero\n    __m256 mask = _mm256_cmpeq_epi64_mask(result, _mm256_setzero_si256());\n    int count = _mm_popcnt_u32(mask);\n\n    // If count is 0, then x is a simple power of n\n    return count == 0;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\n// Helper function to check if a digit is prime\nbool is_prime(char digit) {\n    switch(digit) {\n        case '2': case '3': case '5': case '7':\n            return true;\n        default:\n            return false;\n    }\n}\n\n// Function to count prime hexadecimal digits in a string using AVX/AVX2\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    const char *str = num.c_str();\n    __m256i mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0); // Initialize mask with zeros\n\n    for (size_t i = 0; i < num.size(); i += 8) {\n        __m256i vec = _mm_loadu_si256((__m256i *)(&str[i]));\n        __m256i prime_mask = _mm_cmpeq_epi8(vec, _mm_set_epi8('2', '3', '5', '7', 'B', 'D', 'F', 'A')); // Compare each byte with prime digits\n        mask = _mm_or_si256(mask, prime_mask); // Update mask with prime digits found\n    }\n\n    // Count the number of set bits in the mask\n    __m256i sum = _mm_sad_epu8(mask, _mm_setzero_si256());\n    sum = _mm_add_epi32(sum, sum);\n    sum = _mm_add_epi32(sum, sum);\n    sum = _mm_add_epi32(sum, sum);\n\n    // Sum up all the results from the 8 vectors\n    for (size_t i = 0; i < 8; ++i) {\n        sum = _mm_add_epi32(sum, _mm_extract_epi32(_mm_shuffle_epi8(sum, sum), i));\n    }\n\n    return _mm_cvtsi32_si64(sum);\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\n// Helper function to check if a digit is prime\nbool is_prime(char digit) {\n    return digit == '2' || digit == '3' || digit == '5' || digit == '7';\n}\n\n// Function to count prime hexadecimal digits in a string using AVX/AVX2\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    const char *str = num.c_str();\n    __m256i mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0); // Initialize mask with zeros\n\n    for (size_t i = 0; i < num.size(); i += 8) {\n        __m256i vec = _mm_loadu_si256((__m256i *)(&str[i]));\n        __m256i prime_mask = _mm_cmpeq_epi8(vec, _mm_set_epi8('2', '3', '5', '7', 'B', 'D', 'F', 'A')); // Compare each byte with prime digits\n        count += _mm_popcnt_u32(_mm_movemask_epi8(prime_mask)); // Count the number of set bits\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num){\n    int count = 0;\n    const __m256i prime_mask = _mm256_set_epi32(2, 3, 5, 7);\n    const __m256i hex_digits = _mm256_set_epi8('0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F');\n\n    for (size_t i = 0; i < num.size(); i += 4) {\n        __m256i current_hex = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&num[i]));\n        __m256i mask = _mm256_cmpeq_epi8(current_hex, hex_digits);\n        __m256i prime_count = _mm256_andnot_si256(mask, prime_mask);\n        count += _mm256_popcnt_u32(_mm256_extract_epi32(prime_count, 0)) +\n                 _mm256_popcnt_u32(_mm256_extract_epi32(prime_count, 1)) +\n                 _mm256_popcnt_u32(_mm256_extract_epi32(prime_count, 2)) +\n                 _mm256_popcnt_u32(_mm256_extract_epi32(prime_count, 3));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint hex_key_simd(const std::string & num) {\n    const uint8_t *data = reinterpret_cast<const uint8_t*>(num.c_str());\n    int count = 0;\n    \n    // Load 8 bytes at a time into registers\n    __m256i vec1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data));\n    __m256i vec2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + 8));\n    __m256i vec3 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + 16));\n    __m256i vec4 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(data + 24));\n    \n    // Define masks for each digit\n    const __m256i mask2 = _mm256_set_epi8(0, 1, 0, 1, 0, 1, 0, 1);\n    const __m256i mask3 = _mm256_set_epi8(0, 0, 1, 1, 0, 0, 1, 1);\n    const __m256i mask5 = _mm256_set_epi8(0, 0, 0, 0, 1, 1, 1, 1);\n    const __m256i mask7 = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 1, 1);\n    const __m256i maskB = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 1);\n    const __m256i maskD = _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 1);\n    \n    // Extract individual digits from each register\n    __m256i digit2 = _mm256_and_si256(vec1, mask2);\n    __m256i digit3 = _mm256_and_si256(vec2, mask3);\n    __m256i digit5 = _mm256_and_si256(vec3, mask5);\n    __m256i digit7 = _mm256_and_si256(vec4, mask7);\n    __m256i digitB = _mm256_and_si256(vec1, maskB);\n    __m256i digitD = _mm256_and_si256(vec2, maskD);\n    \n    // Count the number of set bits in each register\n    __m256i count2 = _mm256_popcnt_u32(_mm256_extract_epi32(digit2, 0));\n    __m256i count3 = _mm256_popcnt_u32(_mm256_extract_epi32(digit3, 0));\n    __m256i count5 = _mm256_popcnt_u32(_mm256_extract_epi32(digit5, 0));\n    __m256i count7 = _mm256_popcnt_u32(_mm256_extract_epi32(digit7, 0));\n    __m256i countB = _mm256_popcnt_u32(_mm256_extract_epi32(digitB, 0));\n    __m256i countD = _mm256_popcnt_u32(_mm256_extract_epi32(digitD, 0));\n    \n    // Sum up all the counts\n    count += _mm256_extract_epi32(count2, 0) +\n             _mm256_extract_epi32(count3, 0) +\n             _mm256_extract_epi32(count5, 0) +\n             _mm256_extract_epi32(count7, 0) +\n             _mm256_extract_epi32(countB, 0) +\n             _mm256_extract_epi32(countD, 0);\n    \n    return static_cast<int>(count);\n}\n```"}
{"task_id": "SimdBench_100_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n#include <vector>\n\n// Function to check if a digit is prime\nbool is_prime(char digit) {\n    return digit == '2' || digit == '3' || digit == '5' || digit == '7';\n}\n\n// Function to count prime hexadecimal digits in a string using AVX/AVX2\nint hex_key_simd(const std::string & num) {\n    int count = 0;\n    const char *str = num.c_str();\n    __m256i mask = _mm_set_epi8(0, 0, 0, 0, 0, 0, 0, 0); // Initialize mask with zeros\n\n    while (*str != '\\0') {\n        char digit = *str++;\n        if (is_prime(digit)) {\n            mask = _mm_add_epi8(mask, _mm_set_epi8(1, 1, 1, 1, 1, 1, 1, 1)); // Set corresponding bits in mask\n        }\n    }\n\n    __m256i sum = _mm_reduce_add_epi8(mask);\n    count = _mm_extract_epi8(sum, 0);\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\nbool is_happy_simd(const std::string & s){\n    const int len = s.length();\n    if(len < 3) return false;\n\n    __m256i mask = _mm_set1_epi8(0xFF);\n    __m256i prev_char = _mm_setzero_si256();\n\n    for(int i = 0; i + 2 < len; i += 3){\n        __m256i current_chars = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i distinct_chars = _mm_andnot_si256(current_chars, prev_char);\n\n        if(_mm_movemask_epi8(distinct_chars) == 0xFFFFFFFF) return false;\n        prev_char = current_chars;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    int n = s.length();\n    if(n < 3)\n        return false;\n\n    __m256i mask = _mm_set1_epi8(0);\n    __m256i prev_char = _mm_setzero_si256();\n\n    for(int i = 0; i < n - 2; i += 4){\n        __m256i current_chars = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i next_chars = _mm_loadu_si256((__m256i*)&s[i + 1]);\n\n        // Check if all characters in current_chars are different from prev_char\n        mask = _mm_or_si256(mask, _mm_cmpeq_epi8(current_chars, prev_char));\n\n        // Update prev_char with current_chars\n        prev_char = current_chars;\n    }\n\n    // Process remaining characters\n    for(int i = n - 2; i < n; ++i){\n        char c = s[i];\n        if(c == prev_char || c == s[i + 1])\n            return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    const int len = s.length();\n    if(len < 3) return false;\n\n    __m256i mask = _mm_set1_epi8(0xFF);\n    __m256i prev_char = _mm_setzero_si256();\n\n    for(int i = 0; i + 2 < len; i += 3){\n        __m256i current_chars = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i unique_chars = _mm_andnot_si256(current_chars, prev_char);\n\n        if(_mm_movemask_epi8(unique_chars) == 0xFFFFFFFF) return false;\n        prev_char = current_chars;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_happy_simd(const std::string & s){\n    int n = s.length();\n    if(n < 3) return false;\n\n    __m256i mask = _mm_set1_epi8(0);\n    __m256i prev_char = _mm_setzero_si256();\n\n    for(int i = 0; i < n - 2; i += 4){\n        __m256i current_char = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i next_char = _mm_loadu_si256((__m256i*)&s[i + 1]);\n\n        // Check if all characters in the current chunk are distinct\n        mask = _mm_or_si256(mask, _mm_cmpeq_epi8(current_char, next_char));\n        mask = _mm_or_si256(mask, _mm_cmpeq_epi8(current_char, prev_char));\n\n        prev_char = current_char;\n    }\n\n    // Process remaining characters\n    for(int i = n - 2; i < n; ++i){\n        char current_char = s[i];\n        char next_char = s[i + 1];\n\n        // Check if all characters in the current chunk are distinct\n        mask = _mm_or_si256(mask, _mm_cmpeq_epi8(_mm_set1_epi8(current_char), next_char));\n    }\n\n    // Check if there are any repeated characters\n    return _mm_movemask_epi8(mask) == 0;\n}\n```"}
{"task_id": "SimdBench_101_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool is_happy_simd(const std::string & s){\n    const int len = s.length();\n    if(len < 3) return false;\n\n    __m256i mask = _mm_set1_epi8(0);\n    __m256i prev_char = _mm_setzero_si256();\n\n    for(int i = 0; i + 2 < len; i += 3){\n        __m256i current_chars = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i distinct_chars = _mm_andnot_si256(current_chars, prev_char);\n\n        mask = _mm_or_si256(mask, distinct_chars);\n        prev_char = current_chars;\n    }\n\n    // Check the last three characters separately\n    __m256i current_chars = _mm_loadu_si256((__m256i*)&s[len - 3]);\n    __m256i distinct_chars = _mm_andnot_si256(current_chars, prev_char);\n    mask = _mm_or_si256(mask, distinct_chars);\n\n    return _mm_movemask_epi8(mask) != 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str){\n    int length = str.length();\n    __m256i vecLength = _mm256_set1_epi32(length);\n    __m256i mask = _mm256_cmpgt_epi32(vecLength, _mm256_set1_epi32(2));\n    __m256i result = _mm256_and_si256(mask, _mm256_sub_epi32(_mm256_set1_epi32(length), _mm256_set1_epi32(1)));\n    return _mm256_movemask_epi8(result) != 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str){\n    int len = str.length();\n    if(len <= 1) return false;\n    \n    __m256i mask = _mm_set_epi32(0, 0, 0, 0);\n    __m256i count = _mm_set_epi32(0, 0, 0, 0);\n    \n    for(int i = 0; i < len; i += 4){\n        __m256i word = _mm_loadu_si256((__m256i*)&str[i]);\n        __m256i ones = _mm_cmpeq_epi8(word, _mm_set_epi8(0));\n        \n        mask = _mm_or_si256(mask, ones);\n        count = _mm_add_epi32(count, _mm_popcnt_u32(_mm_movemask_epi8(ones)));\n    }\n    \n    int totalOnes = _mm_movemask_epi8(mask);\n    int totalCount = _mm_extract_epi32(count, 0);\n    \n    return totalCount > 1 && totalOnes % 2 != 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <algorithm>\n\n// Helper function to check if a number is prime\nbool is_prime(int num) {\n    if (num <= 1) return false;\n    if (num <= 3) return true;\n    if (num % 2 == 0 || num % 3 == 0) return false;\n    for (int i = 5; i * i <= num; i += 6) {\n        if (num % i == 0 || num % (i + 2) == 0) return false;\n    }\n    return true;\n}\n\n// Function to calculate the length of a string using AVX/AVX2\nbool prime_length_simd(const std::string &str) {\n    int len = str.length();\n    if (len <= 1) return false;\n\n    // Convert the string to an array of characters\n    const char* c_str = str.c_str();\n\n    // Calculate the number of words in the string\n    int word_count = len / 4 + (len % 4 != 0);\n\n    // Use AVX/AVX2 to process the words in parallel\n    __m256i mask = _mm_set_epi32(0, 0, 0, 0);\n    for (int i = 0; i < word_count; i++) {\n        __m256i word = _mm_loadu_si256((__m256i*)(&c_str[i * 4]));\n        int word_len = _mm_popcnt_u32(_mm_extract_epi32(word, 0)) +\n                       _mm_popcnt_u32(_mm_extract_epi32(word, 1)) +\n                       _mm_popcnt_u32(_mm_extract_epi32(word, 2)) +\n                       _mm_popcnt_u32(_mm_extract_epi32(word, 3));\n        mask |= _mm_cmpgt_epi32(mask, _mm_set_epi32(word_len, word_len, word_len, word_len));\n    }\n\n    // Check if any word length is prime\n    bool is_prime_word = _mm_movemask_epi8(mask) != 0;\n\n    return is_prime_word;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str){\n    const int len = str.length();\n    __m256i vec_len = _mm256_set_epi32(len >> 0, len >> 4, len >> 8, len >> 12);\n    __m256i mask = _mm256_cmpgt_epi32(vec_len, _mm256_set_epi32(1, 1, 1, 1));\n    __m256i count = _mm256_popcnt_u32(_mm256_and_si256(mask, vec_len));\n\n    return _mm256_movemask_epi8(count) != 0;\n}\n```"}
{"task_id": "SimdBench_102_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nbool prime_length_simd(const std::string & str){\n    int len = str.length();\n    __m256i len_vec = _mm256_set_epi32(len >> 24, len >> 16, len >> 8, len);\n    __m256i mask = _mm256_cmpgt_epi32(len_vec, _mm256_setzero_si256());\n    __m256i count = _mm256_popcnt_u32(_mm256_and_si256(mask, len_vec));\n    return _mm256_movemask_epi8(count) != 0;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N){\n    __m256i sum = _mm256_setzero_si256();\n    uint64_t mask = 0xFF;\n\n    while (N > 0) {\n        __m256i digit = _mm256_and_si256(_mm256_set_epi32(N & mask), _mm256_set_epi32(1));\n        sum = _mm256_add_epi32(sum, digit);\n        N >>= 8;\n    }\n\n    std::string result = \"\";\n    for (__m256i i = sum; i != _mm256_setzero_si256(); i = _mm256_srlv_epi32(i, _mm256_set_epi32(8))) {\n        uint32_t value = _mm256_extract_epi32(i, 0);\n        result += std::bitset<32>(value).to_string();\n    }\n\n    // Remove leading zeros\n    auto it = result.begin();\n    while (*it == '0') ++it;\n    result.erase(result.begin(), it);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N){\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(1);\n    __m256i temp;\n\n    while (N > 0) {\n        temp = _mm256_loadu_si256((__m256i*)&N);\n        sum = _mm256_add_epi32(sum, _mm256_and_si256(temp, mask));\n        N >>= 8;\n    }\n\n    uint64_t result = 0;\n    _mm256_storeu_si256((__m256i*)&result, sum);\n\n    std::string binary_result = \"\";\n    while (result > 0) {\n        binary_result = std::to_string(result % 2) + binary_result;\n        result /= 2;\n    }\n\n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N){\n    __m256i sum = _mm256_setzero_si256();\n    uint64_t mask = 0xFF;\n    while (N > 0) {\n        __m256i digit = _mm256_set_epi32(N & mask, (N >> 8) & mask, (N >> 16) & mask, (N >> 24) & mask);\n        sum = _mm256_add_epi32(sum, digit);\n        N >>= 32;\n    }\n    uint64_t result = _mm256_extract_epi32(sum, 0);\n    if (result == 0) {\n        result = _mm256_extract_epi32(sum, 1);\n    }\n    if (result == 0) {\n        result = _mm256_extract_epi32(sum, 2);\n    }\n    if (result == 0) {\n        result = _mm256_extract_epi32(sum, 3);\n    }\n    std::string binary_result = \"\";\n    while (result > 0) {\n        binary_result = std::to_string(result % 2) + binary_result;\n        result /= 2;\n    }\n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N){\n    __m256i sum = _mm256_setzero_si256();\n    uint64_t mask = 0xFFFFFFFF;\n\n    while (N > 0) {\n        __m256i digit = _mm256_set_epi32(N & mask);\n        sum = _mm256_add_epi32(sum, digit);\n        N >>= 32;\n        mask <<= 32;\n    }\n\n    uint64_t result = _mm256_extract_epi32(sum, 0);\n    if (result == 0) {\n        result = _mm256_extract_epi32(sum, 1);\n    }\n    if (result == 0) {\n        result = _mm256_extract_epi32(sum, 2);\n    }\n    if (result == 0) {\n        result = _mm256_extract_epi32(sum, 3);\n    }\n\n    std::string binary_result = \"\";\n    while (result > 0) {\n        binary_result = std::to_string(result % 2) + binary_result;\n        result /= 2;\n    }\n\n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_103_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string solve_simd(uint64_t N){\n    __m256i sum = _mm256_setzero_si256();\n    uint64_t n = N;\n    \n    while(n > 0){\n        __m256i digit = _mm256_set_epi32((int)n % 10, (int)(n / 10) % 10, (int)(n / 100) % 10, (int)(n / 1000) % 10);\n        sum = _mm256_add_epi32(sum, digit);\n        n /= 10000;\n    }\n    \n    uint64_t result = _mm256_extract_epi32(_mm256_sad_epu32(sum, _mm256_setzero_si256()), 0);\n    std::string binary_result = \"\";\n    while(result > 0){\n        binary_result = std::to_string(result % 2) + binary_result;\n        result /= 2;\n    }\n    \n    return binary_result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int result = 0;\n    __m256i *ptr = (__m256i*)lst.data();\n    int n = lst.size();\n\n    // Process pairs of elements in parallel\n    for (int i = 0; i < n - 1; i += 2) {\n        __m256i even = _mm256_loadu_si256(ptr + i);\n        __m256i odd = _mm256_loadu_si256(ptr + i + 1);\n        __m256i sum = _mm256_add_epi32(even, odd);\n        result += _mm256_reduce_add_epi32(sum);\n    }\n\n    // Handle remaining single element if size is odd\n    if (n % 2 != 0) {\n        result += ptr[n - 1];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int result = 0;\n    __m256i vec1 = _mm256_loadu_si256((__m256i*)&lst[0]);\n    __m256i vec2 = _mm256_loadu_si256((__m256i*)&lst[2]);\n\n    // Add even elements at odd indices\n    __m256i even_at_odd_indices = _mm256_add_epi32(_mm256_and_si256(vec1, _mm256_set1_epi32(0)), _mm256_and_si256(vec2, _mm256_set1_epi32(0)));\n\n    // Sum up the results\n    result += _mm256_reduce_add_epi32(even_at_odd_indices);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int result = 0;\n    __m256i *ptr = (__m256i *)lst.data();\n    int n = lst.size();\n\n    // Process each 32-bit chunk in parallel\n    for (int i = 0; i < n - 1; i += 32) {\n        __m256i even_elements = _mm256_loadu_si256(ptr + i);\n        __m256i odd_indices = _mm256_set_epi32(i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9, i+10, i+11, i+12, i+13, i+14, i+15, i+16, i+17, i+18, i+19, i+20, i+21, i+22, i+23, i+24, i+25, i+26, i+27, i+28, i+29);\n        __m256i even_at_odd_indices = _mm256_and_si256(even_elements, odd_indices);\n        __m256i sum = _mm256_add_epi32(sum, even_at_odd_indices);\n    }\n\n    // Sum up the remaining elements if there are less than 32 elements left\n    for (int i = n - 1; i >= 0; --i) {\n        result += lst[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int result = 0;\n    __m256i *ptr = (__m256i *)lst.data();\n    int n = lst.size();\n\n    // Process each 32-bit chunk in parallel\n    for (int i = 0; i < n - 32; i += 32) {\n        __m256i even_elements = _mm256_loadu_si256(ptr + i);\n        __m256i even_indices = _mm256_set_epi32(i, i+1, i+2, i+3);\n        __m256i even_values = _mm256_and_si256(even_elements, even_indices);\n        __m256i even_values_masked = _mm256_and_si256(even_values, _mm256_cmpgt_epi32(even_values, _mm256_setzero_si256()));\n        __m256i even_values_summed = _mm256_add_epi32(even_values_masked, _mm256_setzero_si256());\n        result += _mm256_reduce_add_epi32(even_values_summed);\n    }\n\n    // Process remaining elements if there are less than 32 elements left\n    for (int i = n - (n % 32); i < n; ++i) {\n        result += lst[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_104_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_simd(std::vector<int> lst){\n    int result = 0;\n    __m256i vec1 = _mm256_set_epi32(lst[0], lst[2], lst[4], lst[6]);\n    __m256i vec2 = _mm256_set_epi32(lst[1], lst[3], lst[5], lst[7]);\n\n    __m256i even_elements = _mm256_and_si256(_mm256_cmpgt_epi32(vec1, _mm256_setzero_si256()), _mm256_set_epi32(1, 0, 1, 0));\n    __m256i sum = _mm256_add_epi32(even_elements, _mm256_extract_epi32(vec2, 0));\n\n    result += _mm256_reduce_add_epi32(sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const char *str = s.c_str();\n    const int len = s.length();\n    __m256i key = _mm_set_epi32(0, 1, 2, 3); // Rotate by 2*2 places\n\n    std::string result;\n    result.reserve(len);\n\n    for(int i = 0; i < len; i += 4){\n        __m256i input = _mm_loadu_si256((__m256i*)(&str[i]));\n        __m256i output = _mm_xor_si256(input, key);\n        _mm_storeu_si256((__m256i*)&result[i], output);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const char *str = s.c_str();\n    size_t len = s.length();\n\n    __m256i key = _mm_set_epi32(0x1a, 0x1b, 0x1c, 0x1d); // Rotate the alphabet by 4 positions\n\n    __m256i result;\n    for(size_t i = 0; i < len; i += 32){\n        __m256i input = _mm_loadu_si256((__m256i*)(&str[i]));\n        __m256i shifted = _mm_add_epi32(input, key);\n        __m256i masked = _mm_and_si24(shifted, _mm_set_epi8(0x61, 0x7a, 0x6e, 0x69, 0x6f, 0x72, 0x6c, 0x68));\n        __m256i unmasked = _mm_xor_si256(masked, _mm_set_epi8(0x61, 0x7a, 0x6e, 0x69, 0x6f, 0x72, 0x6c, 0x68));\n        _mm_storeu_si256((__m256i*)(&result[i]), unmasked);\n    }\n\n    std::string encrypted;\n    for(size_t i = 0; i < len; ++i){\n        encrypted.push_back(static_cast<char>(result[i]));\n    }\n\n    return encrypted;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s){\n    const char* src = s.c_str();\n    const int len = s.length();\n    __m256i key = _mm_set_epi32(0, 1, 2, 3); // Rotate the alphabet by 2 * 2 places\n\n    std::string result(len);\n    __m256i dest;\n\n    for(int i = 0; i < len; i += 4){\n        __m256i src_block = _mm_loadu_si256((__m256i*)src + i);\n        __m256i shifted_block = _mm_add_epi32(src_block, key);\n        dest = _mm_shuffle_epi8(shifted_block, shifted_block);\n        _mm_storeu_si256((__m256i*)&result[i], dest);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nstd::string encrypt_simd(const std::string & s){\n    const char* src = s.c_str();\n    const char* dst = new char[s.size()];\n    int len = s.size();\n\n    __m256i mask = _mm_set_epi32(0, 1, 2, 3);\n    __m256i shift = _mm_set_epi32(2 * 2, 4 * 2, 6 * 2, 8 * 2);\n\n    for(int i = 0; i < len; i += 32){\n        __m256i data = _mm_loadu_si256((__m256i*)src + i);\n        __m256i shifted = _mm_add_epi32(data, shift);\n        __m256i masked = _mm_and_si256(shifted, mask);\n        __m256i result = _mm_xor_si256(masked, data);\n        _mm_storeu_si256((__m256i*)dst + i, result);\n    }\n\n    std::string encrypted(dst, len);\n    delete[] dst;\n    return encrypted;\n}\n```"}
{"task_id": "SimdBench_105_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n\nstd::string encrypt_simd(const std::string & s){\n    const char *alphabet = \"abcdefghijklmnopqrstuvwxyz\";\n    const int shift = 4; // 2 * 2\n    __m256i key = _mm_set_epi8(alphabet[0], alphabet[1], alphabet[2], alphabet[3],\n                                 alphabet[4], alphabet[5], alphabet[6], alphabet[7],\n                                 alphabet[8], alphabet[9], alphabet[10], alphabet[11],\n                                 alphabet[12], alphabet[13], alphabet[14], alphabet[15]);\n\n    __m256i input = _mm_loadu_si256((__m256i*)&s[0]);\n    __m256i output;\n\n    for(int i = 0; i < s.size(); i += 16){\n        output = _mm_xor_si256(input, key);\n        input = _mm_add_epi8(output, _mm_set_epi8(shift, shift, shift, shift,\n                                                  shift, shift, shift, shift,\n                                                  shift, shift, shift, shift,\n                                                  shift, shift, shift, shift));\n    }\n\n    std::string result;\n    for(int i = 0; i < s.size(); ++i){\n        result.push_back((char)_mm_extract_epi8(output, i));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <algorithm>\n#include <cassert>\n\n// Helper functions\nstatic inline char swap_case(char c) {\n    return islower(c) ? toupper(c) : tolower(c);\n}\n\nstatic inline char replace_vowel(char c) {\n    switch (c) {\n        case 'a': return 'c';\n        case 'e': return 'g';\n        case 'i': return 'k';\n        case 'o': return 'q';\n        case 'u': return 'w';\n        default: return c;\n    }\n}\n\n// Encode the message using AVX/AVX2 intrinsics\nstd::string encode_simd(const std::string & message) {\n    const int len = message.length();\n    __m256i v_message = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[0]));\n    __m256i v_result;\n\n    // Swap case and replace vowels\n    v_result = _mm256_xor_si256(v_message, _mm256_set1_epi8(32)); // Swap case\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(97)); // Replace vowels\n\n    // Store the result back to memory\n    char result[len];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), v_result);\n\n    return std::string(result, len);\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <algorithm>\n#include <cassert>\n\n// Helper functions to swap case and replace vowels\nchar swap_case(char c) {\n    return islower(c) ? toupper(c) : tolower(c);\n}\n\nchar replace_vowel(char c) {\n    switch (c) {\n        case 'a': return 'c';\n        case 'e': return 'g';\n        case 'i': return 'k';\n        case 'o': return 'q';\n        case 'u': return 'w';\n        default: return c;\n    }\n}\n\n// Encode the message using AVX/AVX2 intrinsics\nstd::string encode_simd(const std::string & message) {\n    const int n = message.size();\n    __m256i v_message = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[0]));\n    __m256i v_result;\n\n    // Swap case and replace vowels\n    v_result = _mm256_xor_si256(v_message, _mm256_set1_epi8(32)); // Convert to uppercase\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(97)); // Convert to lowercase\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(32)); // Convert back to uppercase\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(97)); // Convert back to lowercase\n\n    // Replace vowels\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(32)); // Convert to uppercase\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(97)); // Convert to lowercase\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(32)); // Convert back to uppercase\n    v_result = _mm256_xor_si256(v_result, _mm256_set1_epi8(97)); // Convert back to lowercase\n\n    // Store the result\n    char result[n];\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(result), v_result);\n\n    // Copy the result to a string\n    std::string encoded_message(result, n);\n\n    return encoded_message;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    const char* vowels = \"aeiouAEIOU\";\n    const int vowels_len = strlen(vowels);\n    const int shift_table[5] = {1, 3, 5, 7, 9};\n\n    __m256i message_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message[0]));\n    __m256i result_vec;\n\n    for(int i = 0; i < message.size(); i += 4){\n        // Swap case\n        __m256i swapped_case = _mm256_xor_si256(message_vec, _mm256_set1_epi8(32));\n\n        // Replace vowels\n        __m256i shifted_vowels = _mm256_set1_epi8(0);\n        for(int j = 0; j < vowels_len; j++){\n            __m256i vowel_mask = _mm256_cmpeq_epi8(swapped_case, _mm256_set1_epi8(vowels[j]));\n            __m256i shifted_vowel = _mm256_shuffle_epi8(_mm256_set1_epi8(shift_table[j]), vowel_mask, 0b1010);\n            shifted_vowels = _mm256_or_si256(shifted_vowels, vowel_mask);\n        }\n        result_vec = _mm256_or_si256(result_vec, shifted_vowels);\n\n        message_vec = _mm256_add_epi8(message_vec, shifted_vowels);\n    }\n\n    std::string result;\n    _mm256_storeu_si256(reinterpret_cast<__m256i*>(&result[0]), result_vec);\n    return result;\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cstring>\n#include <algorithm>\n#include <vector>\n\nstd::string encode_simd(const std::string & message){\n    const char vowels[] = \"aeiouAEIOU\";\n    const int vowel_offset[] = {1, 3, 5, 7, 9};\n    const int num_vowels = sizeof(vowels) / sizeof(vowels[0]);\n\n    __m256i v_message = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&message));\n    __m256i v_result = _mm256_setzero_si256();\n\n    for(int i = 0; i < message.size(); i += 4){\n        __m256i v_char = _mm256_extract_epi8(v_message, i);\n        __m256i v_uppercase = _mm256_or_si256(_mm256_and_si256(v_char, _mm256_set1_epi8(0x20)), _mm256_set1_epi8(0xDF));\n\n        for(int j = 0; j < num_vowels; j++){\n            if(_mm256_testz_si256(_mm256_cmpeq_epi8(v_uppercase, _mm256_set1_epi8(vowels[j])), _mm256_set1_epi8(0))){\n                __m256i v_offset = _mm256_set1_epi8(vowel_offset[j]);\n                __m256i v_next_vowel = _mm256_add_epi8(v_uppercase, v_offset);\n                __m256i v_next_vowel_uppercase = _mm256_or_si256(_mm256_and_si256(v_next_vowel, _mm256_set1_epi8(0x20)), _mm256_set1_epi8(0xDF));\n                v_result = _mm256_or_si256(v_result, v_next_vowel_uppercase);\n                break;\n            }\n        }\n\n        v_message = _mm256_insert_epi8(v_message, _mm256_extract_epi8(v_result, i), i);\n    }\n\n    return _mm256_cvtsi256_si64(v_message);\n}\n```"}
{"task_id": "SimdBench_106_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n#include <string>\n\nstd::string encode_simd(const std::string & message){\n    const char vowels[] = \"aeiouAEIOU\";\n    const char* vowel_map = \"bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ\";\n    \n    __m256i mask = _mm_set_epi32(0x000000FF, 0x000000FF, 0x000000FF, 0x000000FF);\n    __m256i swapped_mask = _mm_set_epi32(0xFF000000, 0xFF000000, 0xFF000000, 0xFF000000);\n    __m256i swapped_vowels = _mm_set_epi32(0x00000000, 0x00000000, 0x00000000, 0x00000000);\n\n    for(int i = 0; i < message.size(); i += 4){\n        __m256i input = _mm_loadu_si256((__m256i*)&message[i]);\n        \n        // Swap case\n        __m256i swapped_input = _mm_xor_si256(input, swapped_mask);\n        \n        // Replace vowels\n        for(int j = 0; j < 4; j++){\n            int index = std::find(vowels, vowels + sizeof(vowels), swapped_input[j]) - vowels;\n            if(index != -1){\n                int new_index = (index + 2) % 10;\n                swapped_vowels[j] = *(vowel_map + new_index);\n            }\n        }\n        \n        _mm_storeu_si256((__m256i*)&message[i], swapped_vowels);\n    }\n\n    return message;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <vector>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()){\n        return false;\n    }\n\n    const size_t num_keys = dict.size();\n    __m256i mask = _mm_setzero_si256();\n\n    for(size_t i = 0; i < num_keys; i += 8){\n        __m256i key = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&dict[i]));\n        __m256i lower_mask = _mm_cmplt_epi8(key, _mm_set1_epi8('Z'));\n        __m256i upper_mask = _mm_cmpgt_epi8(key, _mm_set1_epi8('A'));\n\n        mask = _mm_or_si256(mask, lower_mask);\n        mask = _mm_or_si256(mask, upper_mask);\n    }\n\n    // Check if all bits in mask are set\n    bool all_lower = _mm_test_all_ones(mask);\n    bool all_upper = _mm_test_all_zeros(mask);\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()){\n        return false;\n    }\n\n    __m256i mask = _mm_setzero_si256();\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for(const auto& pair : dict){\n        const std::string& key = pair.first;\n        const std::string& value = pair.second;\n\n        __m256i key_bytes = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&key[0]));\n        __m256i value_bytes = _mm_loadu_si256(reinterpret_cast<const __m256i*>(&value[0]));\n\n        // Check if all bytes in the key are lowercase\n        __m256i lower_mask = _mm_cmplt_epi8(key_bytes, _mm_set1_epi8('A'));\n        __m256i upper_mask = _mm_cmpgt_epi8(key_bytes, _mm_set1_epi8('Z'));\n\n        mask |= lower_mask;\n        all_lower &= !__builtin_popcount(mask);\n\n        // Check if all bytes in the value are uppercase\n        mask |= upper_mask;\n        all_upper &= !__builtin_popcount(mask);\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    __m256i mask = _mm_setzero_si256();\n    bool all_lower = true;\n    bool all_upper = true;\n\n    for(const auto& pair : dict){\n        const char* key = pair.first.c_str();\n        const char* value = pair.second.c_str();\n\n        __m256i key_m128 = _mm_loadu_si128((__m128i*)key);\n        __m256i value_m128 = _mm_loadu_si128((__m128i*)value);\n\n        __m256i key_m256 = _mm_shuffle_epi8(key_m128, key_m128);\n        __m256i value_m256 = _mm_shuffle_epi8(value_m128, value_m128);\n\n        __m256i key_m128_lower = _mm_and_si256(key_m128, _mm_set1_epi8('a'));\n        __m256i key_m128_upper = _mm_and_si256(key_m128, _mm_set1_epi8('A'));\n\n        __m256i value_m128_lower = _mm_and_si256(value_m128, _mm_set1_epi8('a'));\n        __m256i value_m128_upper = _mm_and_si256(value_m128, _mm_set1_epi8('A'));\n\n        __m256i key_m128_not_lower = _mm_xor_si256(key_m128_lower, _mm_set1_epi8('a'));\n        __m256i key_m128_not_upper = _mm_xor_si256(key_m128_upper, _mm_set1_epi8('A'));\n\n        __m256i value_m128_not_lower = _mm_xor_si256(value_m128_lower, _mm_set1_epi8('a'));\n        __m256i value_m128_not_upper = _mm_xor_si256(value_m128_upper, _mm_set1_epi8('A'));\n\n        mask = _mm_or_si256(mask, key_m128_not_lower);\n        mask = _mm_or_si256(mask, key_m128_not_upper);\n        mask = _mm_or_si256(mask, value_m128_not_lower);\n        mask = _mm_or_si256(mask, value_m128_not_upper);\n\n        if(_mm_movemask_epi8(mask) != 0) {\n            all_lower = false;\n            all_upper = false;\n            break;\n        }\n    }\n\n    return all_lower || all_upper;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n#include <algorithm>\n\nbool check_dict_case_simd(std::map<std::string, std::string> dict){\n    if(dict.empty()) return false;\n\n    // Convert all keys to lowercase\n    std::vector<std::string> keys;\n    for(const auto& pair : dict){\n        keys.push_back(pair.first);\n    }\n    std::transform(keys.begin(), keys.end(), keys.begin(),\n                   [](const std::string& str){ return std::tolower(str); });\n\n    // Check if all keys are in lowercase\n    bool all_lowercase = std::all_of(keys.begin(), keys.end(),\n                                   [](const std::string& str){ return std::islower(str[0]); });\n\n    // Convert all keys to uppercase\n    keys.clear();\n    for(const auto& pair : dict){\n        keys.push_back(pair.first);\n    }\n    std::transform(keys.begin(), keys.end(), keys.begin(),\n                   [](const std::string& str){ return std::toupper(str); });\n\n    // Check if all keys are in uppercase\n    bool all_uppercase = std::all_of(keys.begin(), keys.end(),\n                                    [](const std::string& str){ return std::isupper(str[0]); });\n\n    return all_lowercase || all_uppercase;\n}\n```"}
{"task_id": "SimdBench_107_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <map>\n#include <string>\n#include <vector>\n\n// Helper function to check if a string is in lowercase\nbool is_lower(const std::string& str) {\n    for (char c : str) {\n        if (!islower(c)) {\n            return false;\n        }\n    }\n    return true;\n}\n\n// Helper function to check if a string is in uppercase\nbool is_upper(const std::string& str) {\n    for (char c : str) {\n        if (!isupper(c)) {\n            return false;\n        }\n    }\n    return true;\n}\n\n// Function to check if all keys in the map are either all lowercase or all uppercase\nbool check_dict_case_simd(std::map<std::string, std::string> dict) {\n    if (dict.empty()) {\n        return false;\n    }\n\n    // Get the first key to determine the case type\n    const auto& first_key = dict.begin()->first;\n    bool is_lowercase = is_lower(first_key);\n    bool is_uppercase = is_upper(first_key);\n\n    // Check if all keys have the same case type\n    for (const auto& pair : dict) {\n        if ((is_lowercase && !is_lower(pair.first)) || (is_uppercase && !is_upper(pair.first))) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if(n <= 1)\n        return primes;\n\n    // Initialize a boolean array to mark non-prime numbers\n    __m256i mask = _mm256_set1_epi32(0);\n    for(int i = 2; i * i <= n; ++i){\n        if(_mm256_testz_si256(mask, _mm256_set1_epi32(i)) == 0){\n            // Mark multiples of i as non-prime\n            int start = (i - 1) / 4 * 4 + 1;\n            int end = (n - 1) / 4 * 4 + 1;\n            for(int j = start; j < end; j += 4){\n                __m256i current = _mm256_loadu_si256((__m256i*)&primes[j]);\n                __m256i next = _mm256_add_epi32(current, _mm256_set1_epi32(i));\n                __m256i result = _mm256_andnot_si256(next, current);\n                _mm256_storeu_si256((__m256i*)&primes[j], result);\n            }\n        }\n    }\n\n    // Collect remaining primes\n    for(int i = (n - 1) / 4 * 4 + 1; i < n; ++i){\n        if(primes[i] > 0){\n            primes.push_back(primes[i]);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n){\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    // Initialize a boolean array to mark non-prime numbers\n    __m256i mask = _mm256_setzero_si256();\n    __m256i limit = _mm256_set_epi32(n - 1, n - 1, n - 1, n - 1);\n\n    for (int i = 2; i * i <= n; ++i) {\n        if (_mm256_testz_si256(mask, limit)) {\n            int start = i * i;\n            int end = std::min(start + 31, n);\n            __m256i start_mask = _mm256_set_epi32(start, start, start, start);\n            __m256i end_mask = _mm256_set_epi32(end, end, end, end);\n            __m256i step_mask = _mm256_set_epi32(i, i, i, i);\n\n            while (_mm256_testz_si256(start_mask, limit)) {\n                if (!_mm256_testz_si256(_mm256_and_si256(start_mask, mask), limit)) {\n                    primes.push_back(_mm256_extract_epi32(start_mask, 0));\n                }\n                start_mask = _mm256_add_epi32(start_mask, step_mask);\n            }\n\n            mask = _mm256_or_si256(mask, _mm256_andnot_si256(_mm256_cmplt_epi32(start_mask, limit), start_mask));\n        } else {\n            break;\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    // Initialize a mask with all bits set to 1\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    // Loop through potential divisors up to sqrt(n)\n    for (int i = 2; i * i <= n; ++i) {\n        if (_mm256_testz_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 24))) {\n            // If i is a divisor, mark multiples of i as non-prime\n            mask &= ~_mm256_slli_epi32(_mm256_set1_epi32(i), 24);\n        }\n    }\n\n    // Extract the prime numbers from the mask\n    for (int i = 2; i <= n; ++i) {\n        if (!_mm256_testz_si256(mask, _mm256_set1_epi32(i))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    // Initialize a mask with all bits set to 1\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    // Iterate through potential divisors up to sqrt(n)\n    for (int i = 2; i * i <= n; ++i) {\n        // Check if the current number is divisible by any known prime\n        if (_mm256_testz_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 24))) {\n            // If it's not divisible, mark its multiples as composite\n            mask &= ~_mm256_slli_epi32(_mm256_set1_epi32(i), 24);\n        }\n    }\n\n    // Extract the prime numbers from the mask\n    for (int i = 2; i <= n; ++i) {\n        if (_mm256_testz_si256(mask, _mm256_set1_epi32(i))) {\n            primes.push_back(i);\n        }\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_108_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> count_up_to_simd(int n) {\n    std::vector<int> primes;\n    if (n <= 1) return primes;\n\n    // Initialize a mask with all bits set to 1\n    __m256i mask = _mm256_set1_epi32(-1);\n\n    // Iterate through potential divisors up to sqrt(n)\n    for (int i = 2; i * i <= n; ++i) {\n        if (_mm256_testz_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 24))) {\n            // If i is a divisor, mark multiples of i as non-prime\n            mask = _mm256_or_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 24));\n            mask = _mm256_or_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 16));\n            mask = _mm256_or_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 8));\n            mask = _mm256_or_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 4));\n            mask = _mm256_or_si256(mask, _mm256_slli_epi32(_mm256_set1_epi32(i), 2));\n            mask = _mm256_or_si256(mask, _mm256_set1_epi32(i));\n        }\n    }\n\n    // Extract the remaining non-zero elements from the mask\n    int index = 0;\n    while (!_mm256_testz_si256(mask, mask)) {\n        if (_mm256_movemask_epi8(mask) == 0xFFFFFFFF) {\n            break;\n        }\n        int value = _mm256_extract_epi32(mask, 0);\n        primes.push_back(value);\n        mask = _mm256_srli_epi32(mask, 24);\n    }\n\n    return primes;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\nint count_upper_simd(const std::string & s){\n    int n = s.length();\n    __m256i mask = _mm_set_epi32(0x41, 0x41, 0x41, 0x41); // Mask for uppercase vowels: A, E, I, O, U\n    __m256i result = _mm_setzero_si256(); // Initialize result to zero\n\n    for(int i = 0; i < n; i += 4){\n        __m256i word = _mm_loadu_si256((__m256i*)&s[i]); // Load 4 characters at a time\n        __m256i upper_word = _mm_cmpeq_epi8(word, mask); // Compare each character with the mask\n        __m256i count = _mm_popcnt_u32(_mm_movemask_epi8(upper_word)); // Count the number of set bits\n        result = _mm_add_epi32(result, count); // Add the count to the result\n    }\n\n    return _mm_extract_epi32(result, 0); // Extract the final result from the vector\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s){\n    int result = 0;\n    const char *str = s.c_str();\n    __m256i mask = _mm_set_epi32(0x41, 0x41, 0x41, 0x41); // Mask for uppercase vowels: A, E, I, O, U\n    __m256i upper_vowels;\n\n    while (*str != '\\0') {\n        upper_vowels = _mm_cmpeq_epi8(_mm_loadu_si256((__m256i *)str), mask);\n        result += _mm_popcnt_u32(_mm_movemask_epi8(upper_vowels));\n        str += 4; // Move to the next 4 characters\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\nint count_upper_simd(const std::string & s){\n    int result = 0;\n    const char *ptr = s.c_str();\n    __m256i mask = _mm_set_epi32(0x41, 0x41, 0x41, 0x41); // Mask for uppercase vowels: A, E, I, O, U\n    __m256i upper_chars = _mm_loadu_si256((__m256i *)ptr);\n\n    while (upper_chars != _mm_setzero_si256()) {\n        __m256i lower_chars = _mm_and_si256(upper_chars, mask);\n        __m256i upper_mask = _mm_cmpgt_epi8(lower_chars, _mm_setzero_si256());\n        result += _mm_popcnt_u32(_mm_movemask_epi8(upper_mask));\n        upper_chars = _mm_srli_si256(upper_chars, 8);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <string>\n\nint count_upper_simd(const std::string & s){\n    const __m256i upper_vowels = _mm_set_epi32(0x41, 0x45, 0x49, 0x4F); // 'A', 'E', 'I', 'O'\n    int count = 0;\n\n    for(size_t i = 0; i < s.size(); i += 4){\n        __m256i word = _mm_loadu_si256((__m256i*)&s[i]);\n        __m256i mask = _mm_cmpeq_epi8(word, upper_vowels);\n        count += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_109_AVX", "completion": "```c++\nint count_upper_simd(const std::string & s){\n    int result = 0;\n    const char *str = s.c_str();\n    __m256i mask = _mm_set_epi32(0x41, 0x41, 0x41, 0x41); // Mask for uppercase vowels: A, E, I, O, U\n    __m256i upper_vowels = _mm_loadu_si256((__m256i *)str);\n    upper_vowels = _mm_and_si256(upper_vowels, mask);\n    upper_vowels = _mm_sad_epu8(upper_vowels, _mm_setzero_si256());\n    result = _mm_extract_epi32(upper_vowels, 0);\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    __m256i n_vec = _mm256_set_epi32(n, n-2, n-4, n-6, n-8, n-10, n-12, n-14);\n\n    for (__m256i i = _mm256_setzero_si256(); i != n_vec; i = _mm256_add_epi32(i, _mm256_set_epi32(2, 2, 2, 2, 2, 2, 2, 2))) {\n        int odd_even = _mm256_extract_epi32(_mm256_and_si256(i, _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1)), 0);\n        __m256i next_value = _mm256_add_epi32(i, _mm256_set_epi32(odd_even, odd_even, odd_even, odd_even, odd_even, odd_even, odd_even, odd_even));\n        _mm256_storeu_si256((__m256i*)&result[i], next_value);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    __m256i n_vec = _mm256_set_epi32(n, n-2, n-4, n-6, n-8, n-10, n-12, n-14);\n\n    for (__m256i i = _mm256_setzero_si256(); i != n_vec; i = _mm256_add_epi32(i, _mm256_set_epi32(2, 2, 2, 2, 2, 2, 2, 2))) {\n        __m256i odd_even = _mm256_and_si256(_mm256_cmpgt_epi32(i, _mm256_setzero_si256()), _mm256_set_epi32(1, 0, 1, 0, 1, 0, 1, 0));\n        __m256i next_value = _mm256_add_epi32(i, _mm256_mul_epi32(odd_even, _mm256_set_epi32(1, 1, 0, 0, 1, 1, 0, 0)));\n        _mm256_storeu_si256((__m256i*)&result[i], next_value);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    __m256i n_vec = _mm256_set_epi32(n, n-1, n-2, n-3, n-4, n-5, n-6, n-7);\n\n    for (__m256i i = n_vec; i != _mm256_setzero_si256(); i = _mm256_add_epi32(i, _mm256_set_epi32(2, 2, 2, 2, 2, 2, 2, 2))) {\n        int odd = _mm256_extract_epi32(_mm256_and_si256(i, _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1)), 0);\n        int even = _mm256_extract_epi32(_mm256_and_si256(i, _mm256_set_epi32(0, 0, 1, 1, 0, 0, 1, 1)), 0);\n        result[odd] += 2;\n        result[even] += 2;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    __m256i v_n = _mm256_set_epi32(n, n-1, n-2, n-3, n-4, n-5, n-6, n-7);\n\n    for (__m256i i = v_n; i != _mm256_setzero_si256(); i >>= 2) {\n        __m256i odd = _mm256_and_si256(i, _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1));\n        __m256i even = _mm256_xor_si256(i, odd);\n        __m256i next_odd = _mm256_add_epi32(even, _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1));\n        __m256i next_even = _mm256_sub_epi32(even, _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1));\n\n        _mm256_storeu_si256((__m256i*)&result[0], next_odd);\n        _mm256_storeu_si256((__m256i*)&result[n/2], next_even);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_110_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> make_a_pile_simd(int n){\n    std::vector<int> result(n);\n    __m256i v_n = _mm256_set_epi32(n, n-1, n-2, n-3, n-4, n-5, n-6, n-7);\n\n    for (__m256i i = v_n; i != _mm256_setzero_si256(); i >>= 2) {\n        __m256i odd_even = _mm256_and_si256(_mm256_cmpgt_epi32(i, _mm256_setzero_si256()), _mm256_set_epi32(0, 1, 0, 1, 0, 1, 0, 1));\n        __m256i next_value = _mm256_add_epi32(i, _mm256_shuffle_epi32(odd_even, odd_even, 0, 2, 1, 3, 4, 6));\n        _mm256_storeu_si256((__m256i*)&result[i], next_value);\n    }\n\n    // Handle remaining elements\n    for (int i = 0; i < n % 8; ++i) {\n        int value = result[i];\n        if (value % 2 == 0) {\n            value++;\n        } else {\n            value += 2;\n        }\n        result[i] = value;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m)\n        return \"-1\";\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count = _mm256_setzero_si256();\n\n    int64_t start = std::max(n, m);\n    int64_t end = std::min(n, m);\n\n    while(start <= end){\n        __m256i num = _mm256_set_epi64x(start, start + 1, start + 2, start + 3);\n        __m256i avg = _mm256_div_ps(_mm256_castsi256_ps(num), _mm256_set1_ps(4));\n        sum = _mm256_add_ps(sum, avg);\n        count = _mm256_add_epi32(count, _mm256_set_epi32(4));\n\n        start += 4;\n    }\n\n    __m256i final_sum = _mm256_hadd_ps(_mm256_hadd_ps(sum, sum), sum);\n    __m256i final_count = _mm256_hadd_epi32(_mm256_hadd_epi32(count, count), count);\n\n    float total_sum = _mm256_reduce_add_ps(final_sum);\n    int64_t total_count = _mm256_reduce_add_epi32(final_count);\n\n    double average = total_sum / total_count;\n\n    int64_t rounded_average = static_cast<int64_t>(average);\n\n    char binary[33];\n    snprintf(binary, sizeof(binary), \"%b\", rounded_average);\n\n    return binary;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m)\n        return \"-1\";\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count = _mm256_setzero_si256();\n\n    int64_t start = std::max(n, m);\n    int64_t end = std::min(n, m);\n\n    while(start <= end){\n        __m256i num = _mm256_set_epi64x(start, start + 1, start + 2, start + 3);\n        sum = _mm256_add_epi64(sum, num);\n        count = _mm256_add_epi64(count, _mm256_set_epi32(1, 1, 1, 1));\n        start += 4;\n    }\n\n    __m256i avg = _mm256_div_epi64(sum, count);\n    __m256i rounded_avg = _mm256_round_epi32(avg, _MM_FROUND_TO_NEAREST_TIE);\n\n    char result[32];\n    sprintf(result, \"%032b\", _mm256_extract_epi32(rounded_avg, 0));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m) {\n        return \"-1\";\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count = _mm256_setzero_si256();\n\n    int64_t start = std::max(n, 1);\n    int64_t end = std::min(m, 100); // Assuming max value for simplicity\n\n    for(int64_t i = start; i <= end; i += 8) {\n        __m256i current_sum = _mm256_loadu_si256((__m256i*)&i);\n        __m256i current_count = _mm256_set1_epi32(1);\n\n        sum = _mm256_add_epi32(sum, current_sum);\n        count = _mm256_add_epi32(count, current_count);\n    }\n\n    int64_t remaining = end - start + 1;\n    if(remaining % 8 != 0) {\n        __m256i current_sum = _mm256_loadu_si256((__m256i*)&start);\n        __m256i current_count = _mm256_set1_epi32(remaining % 8);\n\n        sum = _mm256_add_epi32(sum, current_sum);\n        count = _mm256_add_epi32(count, current_count);\n    }\n\n    int64_t total_sum = _mm256_reduce_add_epi32(sum, _MM_REDUCE_ADD_EPI32_ACCUMULATOR);\n    int64_t total_count = _mm256_reduce_add_epi32(count, _MM_REDUCE_ADD_EPI32_ACCUMULATOR);\n\n    double avg = static_cast<double>(total_sum) / total_count;\n\n    int64_t rounded_avg = static_cast<int64_t>(std::floor(avg));\n\n    std::bitset<32> bits(rounded_avg);\n    return bits.to_string();\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m)\n        return \"-1\";\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count = _mm256_setzero_si256();\n\n    int64_t start = std::max(n, m);\n    int64_t end = std::min(n, m);\n\n    while(start <= end){\n        __m256i num = _mm256_set_epi64x(start, start + 1, start + 2, start + 3);\n        __m256i avg = _mm256_div_ps(_mm256_castsi256_ps(num), _mm256_set1_ps(4));\n        sum = _mm256_add_ps(sum, avg);\n        count = _mm256_add_epi32(count, _mm256_set_epi32(4));\n\n        start += 4;\n    }\n\n    int64_t total_sum = _mm256_reduce_add_ps(sum, _mm256_set1_ps(0)).u64[0];\n    int64_t total_count = _mm256_reduce_add_epi32(count, _mm256_set1_epi32(0)).u32[0];\n\n    double avg = static_cast<double>(total_sum) / total_count;\n\n    int64_t rounded_avg = static_cast<int64_t>(std::round(avg));\n\n    return std::bitset<64>(rounded_avg).to_string().substr(0, 6);\n}\n```"}
{"task_id": "SimdBench_111_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::string rounded_avg_simd(int64_t n, int64_t m){\n    if(n > m)\n        return \"-1\";\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i count = _mm256_setzero_si256();\n\n    int64_t start = std::max(n, m);\n    int64_t end = std::min(n, m);\n\n    while(start <= end){\n        __m256i num = _mm256_set_epi64x(start, start + 1, start + 2, start + 3);\n        __m256i avg = _mm256_div_ps(_mm256_castsi256_ps(num), _mm256_set1_ps(4));\n        sum = _mm256_add_ps(sum, avg);\n        count = _mm256_add_epi32(count, _mm256_set_epi32(4));\n\n        start += 4;\n    }\n\n    __m256i final_sum = _mm256_hadd_ps(_mm256_hadd_ps(sum, sum), sum);\n    __m256i final_count = _mm256_hadd_epi32(_mm256_hadd_epi32(count, count), count);\n\n    float total_sum = _mm256_reduce_add_ps(final_sum);\n    int64_t total_count = _mm256_reduce_add_epi32(final_count);\n\n    double average = total_sum / total_count;\n\n    int64_t rounded_average = static_cast<int64_t>(average);\n\n    std::stringstream ss;\n    ss << std::bitset<64>(rounded_average).to_string().substr(64 - 16);\n\n    return ss.str();\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i mask = _mm_set_epi32(0, 1, 0, 1); // Mask to select even indices\n\n    for (int i = 0; i < n; i += 4) {\n        __m256i vec = _mm_loadu_si256((__m256i*)&result[i]); // Load 4 elements into a vector\n        __m256i even_mask = _mm_and_si256(vec, mask); // Select even indices\n        __m256i odd_mask = _mm_andnot_si256(even_mask); // Select odd indices\n\n        __m256i even_factorial = _mm_set_epi32(1, 1, 2, 6); // Precomputed factorials for even indices\n        __m256i odd_sum = _mm_set_epi32(1, 1, 3, 6); // Precomputed sums for odd indices\n\n        __m256i even_result = _mm_mul_epi32(even_factorial, even_mask); // Multiply by precomputed factorials\n        __m256i odd_result = _mm_add_epi32(odd_sum, odd_mask); // Add precomputed sums\n\n        _mm_storeu_si256((__m256i*)&result[i], even_result); // Store results back into the vector\n        _mm_storeu_si256((__m256i*)&result[i + 2], odd_result); // Store results back into the vector\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i mask = _mm_set_epi32(0, 0, 0, 1); // Mask to select even indices\n\n    for (int i = 0; i < n; ++i) {\n        if (_mm_testz_si128(mask, _mm_loadu_si128((__m128i*)&result[i]))) {\n            // Even index, calculate factorial\n            __m256i factorials = _mm_set_epi32(1, 1, 2, 6); // Precomputed factorials for even indices\n            __m256i current_factorial = _mm_loadu_si128((__m128i*)&factorials[i / 4]);\n            __m256i next_factorial = _mm_add_epi32(current_factorial, _mm_loadu_si128((__m128i*)&factorials[(i + 1) / 4]));\n            __m256i result_value = _mm_mullo_epi32(next_factorial, _mm_loadu_si128((__m128i*)&result[i]));\n            _mm_storeu_si128((__m128i*)&result[i], result_value);\n        } else {\n            // Odd index, calculate sum\n            __m256i sums = _mm_set_epi32(1, 3, 6, 10); // Precomputed sums for odd indices\n            __m256i current_sum = _mm_loadu_si128((__m128i*)&sums[i / 4]);\n            __m256i next_sum = _mm_add_epi32(current_sum, _mm_loadu_si128((__m128i*)&sums[(i + 1) / 4]));\n            __m256i result_value = _mm_add_epi32(next_sum, _mm_loadu_si128((__m128i*)&result[i]));\n            _mm_storeu_si128((__m128i*)&result[i], result_value);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i factorials[4] = {};\n    __m256i sums[4] = {};\n\n    // Initialize factorials and sums arrays with zeros\n    _mm256_storeu_si256(factorials, _mm256_setzero_si256());\n    _mm256_storeu_si256(sums, _mm256_setzero_si256());\n\n    int start = 1;\n    int end = n;\n\n    while (start <= end) {\n        int count = (end - start + 1) / 4;\n        for (int i = 0; i < count; ++i) {\n            int idx = start + i * 4;\n            __m256i num = _mm256_set_epi32(idx, idx + 1, idx + 2, idx + 3);\n            __m256i fact = _mm256_set_epi32(1, 1, 1, 1);\n\n            if (idx % 2 == 0) {\n                fact = _mm256_mullo_epi32(fact, num);\n            } else {\n                __m256i sum = _mm256_add_epi32(sum, num);\n            }\n\n            _mm256_storeu_si256(factorials + i, fact);\n            _mm256_storeu_si256(sums + i, sum);\n        }\n        start += count * 4;\n        end -= count * 4;\n    }\n\n    // Copy results back to the result vector\n    for (int i = 0; i < n; ++i) {\n        if (i % 2 == 0) {\n            result[i] = _mm256_extract_epi32(factorials[i >> 2], 0) % 10000;\n        } else {\n            result[i] = _mm256_extract_epi32(sums[i >> 2], 0);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n){\n    std::vector<int> result(n);\n    __m256i factorials[4] = {};\n    __m256i sums[4] = {};\n\n    // Initialize factorials and sums arrays with zeros\n    _mm256_storeu_si256(factorials, _mm256_setzero_si256());\n    _mm256_storeu_si256(sums, _mm256_setzero_si256());\n\n    int start = 1;\n    int end = std::min(n, 4);\n\n    // Process the first batch of elements\n    for (int i = 0; i < end; ++i) {\n        if ((i + 1) % 2 == 0) {\n            factorials[i / 4] = _mm256_mullo_epi32(factorials[i / 4], _mm256_set1_epi32(i + 1));\n        } else {\n            sums[i / 4] = _mm256_add_epi32(sums[i / 4], _mm256_set1_epi32(i + 1));\n        }\n    }\n\n    // Store the results in the result vector\n    for (int i = 0; i < end; ++i) {\n        if ((i + 1) % 2 == 0) {\n            result[start + i] = _mm256_extract_epi32(factorials[i / 4], 0);\n        } else {\n            result[start + i] = _mm256_extract_epi32(sums[i / 4], 0);\n        }\n    }\n\n    // Process the remaining elements if necessary\n    if (n > 4) {\n        int remaining_elements = n - 4;\n        __m256i remaining_factorials[remaining_elements / 4];\n        __m256i remaining_sums[remaining_elements / 4];\n\n        // Initialize remaining factorials and sums arrays with zeros\n        _mm256_storeu_si256(remaining_factorials, _mm256_setzero_si256());\n        _mm256_storeu_si256(remaining_sums, _mm256_setzero_si256());\n\n        // Process the remaining elements\n        for (int i = 0; i < remaining_elements; ++i) {\n            if ((i + 1) % 2 == 0) {\n                remaining_factorials[i / 4] = _mm256_mullo_epi32(remaining_factorials[i / 4], _mm256_set1_epi32(i + 1));\n            } else {\n                remaining_sums[i / 4] = _mm256_add_epi32(remaining_sums[i / 4], _mm256_set1_epi32(i + 1));\n            }\n        }\n\n        // Store the results in the result vector\n        for (int i = 0; i < remaining_elements; ++i) {\n            if ((i + 1) % 2 == 0) {\n                result[start + 4 + i] = _mm256_extract_epi32(remaining_factorials[i / 4], 0);\n            } else {\n                result[start + 4 + i] = _mm256_extract_epi32(remaining_sums[i / 4], 0);\n            }\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_112_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> func_simd(int n) {\n    std::vector<int> result(n);\n    __m256i factorials[4] = {};\n    __m256i sums[4] = {};\n\n    // Initialize factorials and sums arrays with zeros\n    _mm256_storeu_si256(factorials, _mm256_setzero_si256());\n    _mm256_storeu_si256(sums, _mm256_setzero_si256());\n\n    int start = 1;\n    for (int i = 0; i < n; ++i) {\n        if (i % 2 == 0) {\n            // Calculate factorial for even indices\n            __m256i current_factorial = _mm256_set1_epi32(start);\n            for (int j = 1; j <= i / 2; ++j) {\n                current_factorial = _mm256_mul_epi32(current_factorial, _mm256_set1_epi32(j));\n            }\n            _mm256_storeu_si256(factorials[i / 2], current_factorial);\n        } else {\n            // Calculate sum for odd indices\n            __m256i current_sum = _mm256_set1_epi32(start);\n            for (int j = 1; j <= i; ++j) {\n                current_sum = _mm256_add_epi32(current_sum, _mm256_set1_epi32(j));\n            }\n            _mm256_storeu_si256(sums[i / 2], current_sum);\n        }\n\n        start += 4;\n    }\n\n    // Combine results from factorials and sums\n    for (int i = 0; i < n; ++i) {\n        if (i % 2 == 0) {\n            result[i] = _mm256_extract_epi32(factorials[i / 2], 0);\n        } else {\n            result[i] = _mm256_extract_epi32(sums[i / 2], 0);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n    __m256i result = _mm_set_epi32(0, 0, 0, 0);\n\n    int i = 1;\n    while(i <= n){\n        __m256i num = _mm_cvtsi32_si256(i);\n        __m256i reversed_num = _mm_shuffle_epi8(num, num);\n        __m256i diff = _mm_sub_epi32(reversed_num, num);\n\n        __m256i even_mask = _mm_cmpeq_epi32(diff, _mm_setzero_si256());\n        __m256i odd_mask = _mm_cmpgt_epi32(diff, _mm_setzero_si256());\n\n        result = _mm_add_epi32(result, even_mask);\n        result = _mm_add_epi32(result, odd_mask);\n\n        i += 4;\n    }\n\n    int even_count = _mm_extract_epi32(result, 0);\n    int odd_count = _mm_extract_epi32(result, 1);\n\n    return {even_count, odd_count};\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2);\n    __m256i mask = _mm_set_epi32(1, 1, 1, 1); // Initialize mask with all ones\n\n    while (n > 0) {\n        int digit = n % 10;\n        if (digit % 2 == 0) {\n            result[0] += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n        } else {\n            result[1] += _mm_popcnt_u32(_mm_movemask_epi8(mask));\n        }\n        n /= 10;\n        mask >>= 1; // Shift mask right by 1 bit\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2);\n    __m256i mask = _mm_set_epi32(0, 1, 0, 1); // Mask to check if a number is even or odd\n\n    for (int i = 1; i <= n; ++i) {\n        __m256i num = _mm_set_epi32(i, i, i, i); // Convert number to 4-byte integers\n        __m256i even_mask = _mm_and_si256(num, mask); // Check if number is even\n        __m256i odd_mask = _mm_andnot_si256(even_mask, mask); // Check if number is odd\n\n        int even_count = _mm_popcnt_u32(_mm_extract_epi32(even_mask, 0)) + _mm_popcnt_u32(_mm_extract_epi32(even_mask, 1));\n        int odd_count = _mm_popcnt_u32(_mm_extract_epi32(odd_mask, 0)) + _mm_popcnt_u32(_mm_extract_epi32(odd_mask, 1));\n\n        result[0] += even_count;\n        result[1] += odd_count;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2);\n    __m256i mask = _mm_set_epi32(1, 0, 1, 0); // Mask to check if a number is even or odd\n\n    for (__m256i i = _mm_set_epi32(1, 2, 3, 4); i <= _mm_set_epi32(n / 2, n / 2 + 1, n / 2 + 2, n / 2 + 3); i += 4) {\n        __m256i even_mask = _mm_and_si256(i, mask); // Check if the number is even\n        __m256i odd_mask = _mm_andnot_si256(even_mask, mask); // Check if the number is odd\n\n        int even_count = _mm_popcnt_u32(_mm_extract_epi32(even_mask, 0)) +\n                         _mm_popcnt_u32(_mm_extract_epi32(even_mask, 1));\n        int odd_count = _mm_popcnt_u32(_mm_extract_epi32(odd_mask, 0)) +\n                        _mm_popcnt_u32(_mm_extract_epi32(odd_mask, 1));\n\n        result[0] += even_count;\n        result[1] += odd_count;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_113_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> even_odd_palindrome_simd(int n){\n    std::vector<int> result(2);\n    __m256i mask = _mm_set_epi32(0, 1, 0, 1); // Mask to check if a number is even or odd\n\n    for (int i = 1; i <= n; i++) {\n        __m256i num = _mm_set_epi32(i, i, i, i); // Convert number to 4-byte integers\n        __m256i even_mask = _mm_and_si256(num, mask); // Check if number is even\n        int even_count = _mm_popcnt_u32(_mm_extract_epi32(even_mask, 0)) + _mm_popcnt_u32(_mm_extract_epi32(even_mask, 1));\n        int odd_count = _mm_popcnt_u32(_mm_extract_epi32(~even_mask, 0)) + _mm_popcnt_u32(_mm_extract_epi32(~even_mask, 1));\n\n        result[0] += even_count;\n        result[1] += odd_count;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    int result = 0;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m256i num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i abs_num = _mm256_abs_epi32(num);\n        __m256i sign_bit = _mm256_and_si256(abs_num, _mm256_set1_epi32(0x80000000));\n        __m256i sum_with_sign = _mm256_add_epi32(sum, abs_num);\n        __m256i sum_without_sign = _mm256_sub_epi32(sum_with_sign, sign_bit);\n        __m256i sum_masked = _mm256_and_si256(sum_without_sign, mask);\n        result += _mm256_popcnt_u32(_mm256_extract_epi32(sum_masked, 0)) +\n                  _mm256_popcnt_u32(_mm256_extract_epi32(sum_masked, 1)) +\n                  _mm256_popcnt_u32(_mm256_extract_epi32(sum_masked, 2)) +\n                  _mm256_popcnt_u32(_mm256_extract_epi32(sum_masked, 3));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n) {\n    int result = 0;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m256i num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i abs_num = _mm256_abs_epi32(num);\n        __m256i sign_bit = _mm256_and_si256(abs_num, mask);\n        __m256i sum_of_digits = _mm256_add_epi32(sum, _mm256_sub_epi32(_mm256_xor_si256(sign_bit, abs_num), _mm256_and_si256(sign_bit, abs_num)));\n        sum = _mm256_add_epi32(sum, sum_of_digits);\n    }\n\n    for (size_t i = 4; i < n.size(); ++i) {\n        if (n[i] > 0) {\n            result++;\n        } else if (n[i] < 0) {\n            result += std::abs(n[i]) - 1;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    int result = 0;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x80);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m256i num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i abs_num = _mm256_andnot_si256(mask, num);\n        __m256i sign_bit = _mm256_and_si256(abs_num, mask);\n        __m256i sum_abs = _mm256_add_epi32(sum, _mm256_sub_epi32(_mm256_sad_epu8(sign_bit, _mm256_set1_epi8(0)), _mm256_set1_epi32(1)));\n        sum = _mm256_add_epi32(sum, sum_abs);\n    }\n\n    for (size_t i = 4; i < n.size(); ++i) {\n        if (n[i] > 0) {\n            result++;\n        }\n    }\n\n    return result + _mm256_extract_epi32(sum, 0);\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    int result = 0;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi8(0x80);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m256i num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i abs_num = _mm256_abs_epi8(num);\n        __m256i sign_mask = _mm256_and_si256(abs_num, mask);\n        __m256i sum_digits = _mm256_add_epi8(sum, abs_num);\n        sum_digits = _mm256_sub_epi8(sum_digits, sign_mask);\n        sum = _mm256_add_epi8(sum, sum_digits);\n    }\n\n    // Reduce the sum to a single integer\n    __m256i final_sum = _mm256_hadd_epi32(_mm256_hadd_epi32(sum, sum));\n    final_sum = _mm256_hadd_epi32(final_sum, final_sum);\n    final_sum = _mm256_extract_epi32(final_sum, 0);\n\n    return static_cast<int>(final_sum);\n}\n```"}
{"task_id": "SimdBench_114_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint count_nums_simd(std::vector<int> n){\n    int result = 0;\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(0x7FFFFFFF);\n\n    for (size_t i = 0; i < n.size(); i += 4) {\n        __m256i num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&n[i]));\n        __m256i abs_num = _mm256_abs_epi32(num);\n        __m256i sign_bit = _mm256_and_si256(abs_num, mask);\n        __m256i sum_digits = _mm256_add_epi32(sum, _mm256_sub_epi32(_mm256_sad_epu32(sign_bit, _mm256_setzero_si256()), _mm256_setzero_si256()));\n        sum = _mm256_add_epi32(sum, sum_digits);\n    }\n\n    // Reduce the sum across all lanes\n    for (size_t i = 0; i < 4; ++i) {\n        result += _mm256_extract_epi32(sum, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr) {\n    int n = arr.size();\n    if (n == 0) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i sorted_vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n\n    // Sort the first 256 bits\n    sorted_vec = _mm256_sort_epi32(sorted_vec, vec);\n\n    // Compare the sorted vector with the original vector\n    for (int i = 0; i < n - 1; ++i) {\n        if (_mm256_cmpgt_epi32(sorted_vec, _mm256_loadu_si256((__m256i*)&arr[i + 1])) {\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0)\n        return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i next_vec = _mm256_loadu_si256((__m256i*)&arr[1]);\n\n    while(true){\n        if(_mm256_cmpgt_epi32(vec, next_vec))\n            break;\n        \n        vec = _mm256_slli_epi32(vec, 1);\n        next_vec = _mm256_slli_epi32(next_vec, 1);\n\n        if(_mm256_movemask_epi8(_mm256_cmpeq_epi32(vec, next_vec)) != 0)\n            break;\n    }\n\n    return _mm256_movemask_epi8(_mm256_cmpeq_epi32(vec, next_vec)) == 0;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i sorted_vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n\n    for(int i = 0; i < n - 7; i += 8){\n        __m256i shifted_vec = _mm256_srli_epi32(vec, 1);\n        sorted_vec = _mm256_min_epi32(sorted_vec, shifted_vec);\n\n        vec = shifted_vec;\n    }\n\n    for(int i = n - 7; i < n; ++i){\n        sorted_vec = _mm256_min_epi32(sorted_vec, _mm256_slli_epi32(_mm256_loadu_si256((__m256i*)&arr[i]), 31));\n    }\n\n    for(int i = 0; i < n / 8; ++i){\n        if(_mm256_movemask_epi8(_mm256_cmpgt_epi32(sorted_vec, vec)) != 0){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i sorted_vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n\n    for(int i = 0; i < n - 7; i += 8){\n        __m256i shifted_vec = _mm256_srli_epi32(vec, 1);\n        sorted_vec = _mm256_min_epi32(sorted_vec, shifted_vec);\n\n        vec = shifted_vec;\n    }\n\n    for(int i = n - 7; i < n; ++i){\n        sorted_vec = _mm256_min_epi32(sorted_vec, _mm256_slli_epi32(_mm256_loadu_si256((__m256i*)&arr[i]), 31));\n    }\n\n    for(int i = 0; i < n; ++i){\n        if(arr[i] != sorted_vec[i]) return false;\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_115_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool move_one_ball_simd(std::vector<int> arr){\n    int n = arr.size();\n    if(n == 0) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i sorted_vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n\n    for(int i = 0; i < n - 7; i += 8){\n        __m256i shifted_vec = _mm256_srli_epi32(vec, 1);\n        sorted_vec = _mm256_min_epi32(sorted_vec, shifted_vec);\n\n        vec = shifted_vec;\n    }\n\n    for(int i = n - 7; i < n; ++i){\n        sorted_vec = _mm256_min_epi32(sorted_vec, _mm256_slli_epi32(_mm256_loadu_si256((__m256i*)&arr[i]), 31));\n    }\n\n    for(int i = 0; i < n; ++i){\n        if(arr[i] != _mm256_extract_epi32(sorted_vec, i)){\n            return false;\n        }\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    __m256i* vec1 = (__m256i*)lst1.data();\n    __m256i* vec2 = (__m256i*)lst2.data();\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_and_si256(vec1[i], _mm256_set1_epi32(1)));\n        __m256i odd_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_and_si256(vec2[i], _mm256_set1_epi32(1)));\n\n        if (_mm256_testz_si256(even_mask, odd_mask)) {\n            continue;\n        }\n\n        __m256i temp = _mm256_blendv_epi32(vec1[i], vec2[i], even_mask);\n        _mm256_storeu_si256((__m256i*)&vec1[i], temp);\n\n        break;\n    }\n\n    for (int i = n % 8; i < n; ++i) {\n        if (lst1[i] % 2 == 0) {\n            continue;\n        }\n\n        bool found_odd = false;\n        for (int j = 0; j < n; ++j) {\n            if (lst2[j] % 2 != 0 && lst2[j] != lst1[i]) {\n                lst1[i] = lst2[j];\n                found_odd = true;\n                break;\n            }\n        }\n\n        if (!found_odd) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    __m256i* vec1 = (__m256i*)lst1.data();\n    __m256i* vec2 = (__m256i*)lst2.data();\n\n    for(int i = 0; i < n / 4; ++i){\n        __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec1[i]);\n        __m256i odd_mask = _mm256_cmpgt_epi32(vec1[i], _mm256_setzero_si256());\n\n        __m256i even_sum = _mm256_add_epi32(even_mask, odd_mask);\n        if(_mm256_movemask_epi8(even_sum) == 0){\n            // If all elements in vec1 are even, we can swap with vec2\n            return \"YES\";\n        }\n    }\n\n    return \"NO\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    __m256i* vec1 = (__m256i*)lst1.data();\n    __m256i* vec2 = (__m256i*)lst2.data();\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec1[i]);\n        __m256i odd_mask = _mm256_cmpgt_epi32(vec1[i], _mm256_setzero_si256());\n        __m256i even_count = _mm256_popcnt_u32(_mm256_movemask_epi32(even_mask));\n        __m256i odd_count = _mm256_popcnt_u32(_mm256_movemask_epi32(odd_mask));\n\n        if (even_count > odd_count) {\n            // Perform an exchange\n            __m256i temp = *vec1;\n            *vec1 = *vec2;\n            *vec2 = temp;\n        }\n    }\n\n    for (int i = n % 8; i < n; ++i) {\n        if (lst1[i] % 2 != 0) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    __m256i* vec1 = (__m256i*)lst1.data();\n    __m256i* vec2 = (__m256i*)lst2.data();\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_and_si256(vec1[i], _mm256_set1_epi32(1)));\n        __m256i odd_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), _mm256_and_si256(vec2[i], _mm256_set1_epi32(1)));\n\n        if (_mm256_testz_si256(even_mask, odd_mask)) {\n            // Exchange elements\n            __m256i temp = _mm256_xor_si256(vec1[i], vec2[i]);\n            _mm256_storeu_si256(&vec1[i], temp);\n            _mm256_storeu_si256(&vec2[i], vec1[i]);\n        }\n    }\n\n    for (int i = n % 8; i < n; ++i) {\n        if (lst1[i] % 2 != 0) {\n            return \"NO\";\n        }\n    }\n\n    return \"YES\";\n}\n```"}
{"task_id": "SimdBench_116_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::string myexchange_simd(const std::vector<int>& lst1, const std::vector<int>& lst2){\n    int n = lst1.size();\n    __m256i* vec1 = (__m256i*)lst1.data();\n    __m256i* vec2 = (__m256i*)lst2.data();\n\n    for (int i = 0; i < n / 8; ++i) {\n        __m256i even_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec1[i]);\n        __m256i odd_mask = _mm256_cmpgt_epi32(vec1[i], _mm256_setzero_si256());\n\n        __m256i even_sum = _mm256_add_epi32(even_mask, _mm256_setzero_si256());\n        __m256i odd_sum = _mm256_add_epi32(odd_mask, _mm256_setzero_si256());\n\n        if (_mm256_movemask_epi8(even_sum) == 0 && _mm256_movemask_epi8(odd_sum) != 0) {\n            return \"YES\";\n        }\n    }\n\n    return \"NO\";\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    const int n = lst.size();\n    std::vector<std::string> result(n);\n    \n    // Convert each string to an integer array\n    __m256i *int_array = new __m256i[n];\n    for (int i = 0; i < n; ++i) {\n        int_array[i] = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(lst[i].c_str()));\n    }\n    \n    // Count the number of odd digits in each integer array\n    for (int i = 0; i < n; ++i) {\n        __m256i count = _mm256_setzero_si256();\n        count = _mm256_or_si256(count, _mm256_and_si256(_mm256_cmplt_epi8(_mm256_cmpgt_epi8(int_array[i], _mm256_set1_epi8('0')), _mm256_set1_epi8('9'))));\n        count = _mm256_or_si256(count, _mm256_and_si256(_mm256_cmplt_epi8(_mm256_cmpgt_epi8(int_array[i], _mm256_set1_epi8('A')), _mm256_set1_epi8('F')));\n        count = _mm256_or_si256(count, _mm256_and_si256(_mm256_cmplt_epi8(_mm256_cmpgt_epi8(int_array[i], _mm256_set1_epi8('a')), _mm256_set1_epi8('f')));\n        \n        int count_value = _mm256_extract_epi8(count, 0) + _mm256_extract_epi8(count, 1) + _mm256_extract_epi8(count, 2) + _mm256_extract_epi8(count, 3);\n        result[i] = \"the number of odd elements \" + std::to_string(count_value) + \"n the str\" + std::to_string(i+1) + \" of the \" + std::to_string(n) + \"nput.\";\n    }\n    \n    delete[] int_array;\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    std::vector<std::string> result;\n    const int n = lst.size();\n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n    __m256i count = _mm_setzero_si256();\n\n    for (int i = 0; i < n; i += 4) {\n        __m256i vec = _mm_loadu_si256((__m256i*)&lst[i]);\n        __m256i odd_mask = _mm_cmpgt_epi8(vec, _mm_setzero_si256());\n        count = _mm_add_epi32(count, _mm_popcnt_u32(_mm_movemask_epi8(odd_mask)));\n    }\n\n    for (int i = n % 4; i < n; ++i) {\n        int count = 0;\n        for (char c : lst[i]) {\n            if ((c - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n        result.push_back(\"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(i+1) + \" of the input.\");\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst) {\n    const int n = lst.size();\n    std::vector<std::string> result(n);\n\n    __m256i mask = _mm_setzero_si256(); // Initialize mask with zeros\n\n    for (int i = 0; i < n; i += 8) { // Process 8 strings at a time\n        __m256i vec = _mm_loadu_si256((__m256i*)&lst[i]); // Load 8 strings into a vector\n\n        // Count odd digits in each string\n        __m256i count = _mm_popcnt_u32(_mm_extract_epi32(vec, 0)) +\n                         _mm_popcnt_u32(_mm_extract_epi32(vec, 1)) +\n                         _mm_popcnt_u32(_mm_extract_epi32(vec, 2)) +\n                         _mm_popcnt_u32(_mm_extract_epi32(vec, 3));\n\n        // Store the count in the result vector\n        _mm_storeu_si256((__m256i*)&result[i], count);\n    }\n\n    // Handle remaining strings if there are fewer than 8\n    for (int i = n % 8; i < n; ++i) {\n        int count = 0;\n        for (char c : lst[i]) {\n            if ((c - '0') % 2 != 0) {\n                ++count;\n            }\n        }\n        result[i] = \"the number of odd elements \" + std::to_string(count) + \"n the str\" + std::to_string(i + 1) + \" of the input.\";\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    const int n = lst.size();\n    std::vector<std::string> result(n);\n    \n    __m256i mask = _mm_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF); // Mask for odd numbers\n    \n    for(int i = 0; i < n; i += 4){\n        __m256i vec = _mm_loadu_si256((__m256i*)&lst[i]);\n        \n        // Extract odd digits from each byte\n        __m256i odd_bytes = _mm_and_si256(vec, mask);\n        \n        // Count the number of odd bytes\n        __m256i count = _mm_popcnt_u32(_mm_extract_epi32(odd_bytes, 0)) +\n                        _mm_popcnt_u32(_mm_extract_epi32(odd_bytes, 1)) +\n                        _mm_popcnt_u32(_mm_extract_epi32(odd_bytes, 2)) +\n                        _mm_popcnt_u32(_mm_extract_epi32(odd_bytes, 3));\n        \n        // Store the count in the result vector\n        _mm_storeu_si256((__m256i*)&result[i], count);\n    }\n    \n    // Handle remaining elements if there are less than 4\n    for(int i = n % 4; i < n; ++i){\n        int count = 0;\n        for(char c : lst[i]){\n            if((c - '0') % 2 != 0){\n                ++count;\n            }\n        }\n        result[i] = \"the number of odd elements \" + std::to_string(count) + \" in the str\" + std::to_string(i+1) + \" of the input.\";\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_117_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n#include <string>\n\nstd::vector<std::string> odd_count_simd(const std::vector<std::string> & lst){\n    const int vec_size = 8;\n    std::vector<int> counts(lst.size());\n    __m256i mask = _mm_set_epi32(0, 1, 2, 3, 4, 5, 6, 7);\n\n    for (int i = 0; i < lst.size(); i += vec_size) {\n        __m256i vec = _mm_loadu_si256((__m256i*)&lst[i]);\n        __m256i odd_mask = _mm_cmpgt_epi8(vec, mask);\n        __m256i count = _mm_popcnt_u32(_mm_movemask_epi8(odd_mask));\n        counts[i / vec_size] = static_cast<int>(count);\n    }\n\n    std::vector<std::string> result;\n    for (int i = 0; i < lst.size(); ++i) {\n        std::stringstream ss;\n        ss << \"the number of odd elements \" << counts[i] << \"n the str\" << i + 1 << \"ng \" << i + 1 << \" of the \" << lst.size() << \"nput.\";\n        result.push_back(ss.str());\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    __m256i min_sum = _mm256_setzero_si256();\n    __m256i current_min_sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i sum = _mm256_add_epi64(current_min_sum, vec);\n        __m256i min_sum_temp = _mm256_min_epi64(sum, min_sum);\n        min_sum = _mm256_blendv_epi64(min_sum_temp, min_sum, _mm256_cmpgt_epi64(sum, min_sum));\n        current_min_sum = _mm256_min_epi64(sum, current_min_sum);\n    }\n\n    int64_t result = INT64_MAX;\n    for (size_t i = 0; i < n; i++) {\n        if (nums[i] < result) {\n            result = nums[i];\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i min_sum = _mm256_set1_epi64x(nums[0]);\n    __m256i max_sum = _mm256_set1_epi64x(nums[0]);\n\n    for (size_t i = 1; i < n; ++i) {\n        __m256i current_sum = _mm256_add_epi64(sum, _mm256_loadu_si256((__m256i*)&nums[i]));\n        __m256i new_min_sum = _mm256_min_epi64(min_sum, current_sum);\n        __m256i new_max_sum = _mm256_max_epi64(max_sum, current_sum);\n\n        sum = current_sum;\n        min_sum = new_min_sum;\n        max_sum = new_max_sum;\n    }\n\n    int64_t result = *std::max_element(min_sum.begin(), min_sum.end());\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i min_sum = _mm256_set1_epi64x(INT64_MAX);\n    \n    for (size_t i = 0; i < n; ++i) {\n        __m256i num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        sum = _mm256_add_epi64(sum, num);\n        min_sum = _mm256_min_epi64(min_sum, sum);\n        \n        // If the sum exceeds INT64_MAX, reset it to INT64_MIN\n        if (_mm256_cmpgt_epi64(sum, _mm256_set1_epi64x(INT64_MAX))) {\n            sum = _mm256_setzero_si256();\n        }\n    }\n    \n    // Reduce the result to a single value\n    int64_t min_val = _mm256_reduce_min_epi64(min_sum);\n    \n    return min_val;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums) {\n    int64_t n = nums.size();\n    if (n <= 0) return 0;\n\n    __m256i min_sum = _mm256_setzero_si256();\n    __m256i current_min = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < n; ++i) {\n        __m256i num = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&nums[i]));\n        __m256i new_min = _mm256_add_epi64(current_min, num);\n        __m256i new_min_sum = _mm256_min_epi64(min_sum, new_min);\n        min_sum = new_min_sum;\n        current_min = new_min;\n    }\n\n    // Reduce the result to a single value\n    int64_t result = _mm256_reduce_min_epi64(min_sum);\n\n    // Handle the remaining elements\n    for (size_t i = n % 8; i < n; ++i) {\n        result = std::min(result, nums[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_118_AVX", "completion": "```c++\nint64_t minSubArraySum_simd(const std::vector<int64_t>& nums){\n    const int N = nums.size();\n    __m256i sum = _mm256_setzero_si256();\n    __m256i min_sum = _mm256_set1_epi64x(INT64_MAX);\n    \n    for(int i = 0; i < N; i += 8){\n        __m256i current_sum = _mm256_loadu_si256((__m256i*)&nums[i]);\n        \n        // Add current sum to running total\n        sum = _mm256_add_epi64(sum, current_sum);\n        \n        // Update minimum sum found so far\n        min_sum = _mm256_min_epi64(min_sum, sum);\n    }\n    \n    // Reduce the remaining elements if there are less than 8 elements left\n    for(int i = N % 8; i < N; ++i){\n        sum = _mm256_add_epi64(sum, _mm256_loadu_si256((__m256i*)&nums[i]));\n        min_sum = _mm256_min_epi64(min_sum, sum);\n    }\n    \n    // Extract the minimum value from the reduced sum\n    int64_t result = _mm256_extract_epi64(_mm256_min_epi64(min_sum, _mm256_set1_epi64x(0)), 0);\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < rows; ++i) {\n        __m256i mask = _mm256_set1_epi8(1);\n        __m256i current_sum = _mm256_setzero_si256();\n\n        for (int j = 0; j < cols; ++j) {\n            if (grid[i][j] == 1) {\n                current_sum = _mm256_add_epi8(current_sum, mask);\n            }\n        }\n\n        sum = _mm256_max_epi8(sum, current_sum);\n    }\n\n    int result = 0;\n    while (_mm256_movemask_epi8(sum) != 0) {\n        __m256i mask = _mm256_set1_epi8(1);\n        __m256i current_sum = _mm256_setzero_si256();\n\n        for (int j = 0; j < cols; ++j) {\n            if ((sum >> j) & 1) {\n                current_sum = _mm256_add_epi8(current_sum, mask);\n            }\n        }\n\n        sum = _mm256_sub_epi8(sum, current_sum);\n        result++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n\n    __m256i sum_mask = _mm256_setzero_si256(); // Initialize sum mask with zeros\n    __m256i bucket_mask = _mm256_set1_epi32(capacity); // Set bucket mask to the bucket capacity\n\n    for (int i = 0; i < rows; ++i) {\n        __m256i row_sum = _mm256_setzero_si256(); // Initialize row sum with zeros\n\n        for (int j = 0; j < cols; ++j) {\n            if (grid[i][j] == 1) {\n                row_sum = _mm256_add_epi32(row_sum, bucket_mask); // Add bucket mask to row sum if water exists\n            }\n        }\n\n        sum_mask = _mm256_or_si256(sum_mask, row_sum); // Update sum mask with current row's sum\n    }\n\n    int total_sum = _mm256_reduce_add_epi32(sum_mask, _mm256_set1_epi32(0)); // Sum up all elements in the sum mask\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            if (grid[i][j] == 1) {\n                __m256i mask = _mm256_set1_epi8(1);\n                sum = _mm256_add_epi8(sum, _mm256_and_si256(mask, _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&grid[i][j]))));\n            }\n        }\n    }\n\n    int count = 0;\n    while (_mm256_movemask_epi8(_mm256_cmpgt_epi8(sum, _mm256_setzero_si256())) != 0) {\n        __m256i mask = _mm256_cmpgt_epi8(sum, _mm256_setzero_si256());\n        __m256i ones = _mm256_sad_epu8(mask, _mm256_setzero_si256());\n        count += _mm256_extract_epi32(ones, 0);\n\n        __m256i temp = _mm256_sub_epi8(sum, _mm256_and_si256(mask, _mm256_set1_epi8(1)));\n        _mm256_storeu_si256(reinterpret_cast<__m256i*>(&sum), temp);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n\n    __m256i sum_mask = _mm256_setzero_si256(); // Initialize sum mask to zero\n    __m256i min_sum = _mm256_set1_epi32(INT_MAX); // Initialize minimum sum to infinity\n\n    for (int i = 0; i < rows; ++i) {\n        __m256i current_sum = _mm256_setzero_si256(); // Initialize current sum to zero\n        for (int j = 0; j < cols; ++j) {\n            if (grid[i][j] == 1) {\n                current_sum = _mm256_add_epi32(current_sum, _mm256_set1_epi32(1)); // Add 1 to current sum if water is present\n            }\n        }\n\n        // Update sum mask with the maximum value found so far\n        sum_mask = _mm256_max_epi32(sum_mask, current_sum);\n\n        // Update minimum sum with the minimum value found so far\n        min_sum = _mm256_min_epi32(min_sum, current_sum);\n    }\n\n    // Calculate the total number of buckets needed by dividing the minimum sum by the bucket capacity\n    return _mm256_extract_epi32(_mm256_div_epi32(min_sum, _mm256_set1_epi32(capacity)), 0);\n}\n```"}
{"task_id": "SimdBench_119_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint max_fill_simd(const std::vector<std::vector<int>>& grid, int capacity) {\n    const int rows = grid.size();\n    const int cols = grid[0].size();\n\n    __m256i sum = _mm256_setzero_si256(); // Initialize sum with zeros\n\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            if (grid[i][j] == 1) {\n                __m256i mask = _mm256_set1_epi8(1); // Create a mask for the current cell\n                __m256i bucket = _mm256_set1_epi8(capacity); // Create a bucket with the specified capacity\n                __m256i result = _mm256_and_si256(mask, bucket); // Apply the mask to the bucket\n                sum = _mm256_add_epi8(sum, result); // Add the result to the sum\n            }\n        }\n    }\n\n    int total_sum = 0;\n    for (int i = 0; i < 4; ++i) { // Process the sum in chunks of 4 elements\n        total_sum += _mm256_reduce_add_epi8(sum);\n        sum = _mm256_permutevar_epi8(sum, _mm256_set_epi32(0, 1, 2, 3)); // Shuffle the sum\n    }\n\n    return total_sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t result = 0;\n    __m256i vec = _mm256_loadu_si256((__m256i*)&lst[0]);\n    __m256i mask = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i odd_mask = _mm256_and_si256(mask, vec);\n    __m256i odd_sum = _mm256_reduce_add_epi16(odd_mask);\n    result += _mm256_extract_epi64(odd_sum, 0);\n    if (lst.size() % 2 == 1) {\n        result += lst.back();\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    const int16_t* ptr = lst.data();\n    __m256i sum = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst.size(); i += 4) {\n        __m256i vec = _mm_loadu_si256((__m256i*)ptr);\n        __m256i mask = _mm_cmplt_epi16(_mm_set1_epi16(1), vec); // Check if element is odd\n        __m256i even_mask = _mm_and_si256(mask, _mm_cmpgt_epi16(_mm_set1_epi16(0), i)); // Check if position is even\n        __m256i result = _mm_and_si256(even_mask, vec); // Multiply by 2 to get even elements\n        sum = _mm_add_epi64(sum, _mm256_reduce_add_epi32(result, _MM_REDUCE_ADD_EPI32_A));\n        ptr += 4;\n    }\n\n    int64_t final_sum = _mm256_extract_epi64(sum, 0);\n    return final_sum;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t result = 0;\n    __m256i vec = _mm256_loadu_si256((__m256i*)&lst[0]);\n    __m256i mask = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i odd_elements = _mm256_and_si256(vec, mask);\n    __m256i sum = _mm256_reduce_add_epi16(odd_elements);\n    result += _mm256_extract_epi64(sum, 0);\n    if (lst.size() % 2 == 1) {\n        result += lst.back();\n    }\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst) {\n    int64_t result = 0;\n    const int16_t* ptr = lst.data();\n    __m256i vec1 = _mm256_loadu_si256((__m256i*)ptr);\n    ptr += 8;\n\n    while (ptr < lst.end()) {\n        __m256i vec2 = _mm256_loadu_si256((__m256i*)ptr);\n        ptr += 8;\n\n        // Extract odd elements from even positions\n        __m256i odd_even = _mm256_and_si256(_mm256_cmpgt_epi16(vec1, _mm256_setzero_si256()), _mm256_set1_epi16(1));\n        odd_even = _mm256_or_si256(odd_even, _mm256_and_si256(_mm256_cmpgt_epi16(vec2, _mm256_setzero_si256()), _mm256_set1_epi16(1)));\n\n        // Sum the odd elements\n        __m256i sum_odd = _mm256_reduce_add_epi16(odd_even);\n\n        // Add the sum to the result\n        result += _mm256_extract_epi64(sum_odd, 0);\n\n        vec1 = vec2;\n    }\n\n    // Handle remaining elements\n    if (lst.size() % 2 == 1) {\n        int16_t last_element = lst.back();\n        if (_mm256_testz_si256(_mm256_set1_epi16(last_element), _mm256_set1_epi16(1))) {\n            result += last_element;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_120_AVX", "completion": "```c++\nint64_t solution_simd(const std::vector<int16_t>& lst){\n    int64_t result = 0;\n    __m256i vec = _mm256_loadu_si256((__m256i*)&lst[0]);\n    __m256i mask = _mm256_set_epi16(0, 1, 0, 1, 0, 1, 0, 1);\n    __m256i odd_mask = _mm256_and_si256(mask, _mm256_cmpgt_epi16(_mm256_setzero_si256(), vec));\n    __m256i odd_elements = _mm256_and_si256(vec, odd_mask);\n    __m256i sum = _mm256_reduce_add_epi16(odd_elements);\n    result += _mm256_extract_epi64(sum, 0);\n    result += _mm256_extract_epi64(sum, 1);\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    const int n = arr.size();\n    if (n == 0 || k > n) {\n        return 0;\n    }\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i mask = _mm256_set1_epi32(99);\n\n    for (int i = 0; i < k; ++i) {\n        __m256i element = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i abs_element = _mm256_abs_epi32(element);\n        __m256i less_than_99 = _mm256_cmpgt_epi32(abs_element, mask);\n        __m256i sum_with_mask = _mm256_add_epi32(sum, element & less_than_99);\n        sum = sum_with_mask;\n    }\n\n    int result = 0;\n    _mm256_storeu_si128((__m128i*)&result, sum);\n    _mm_storeu_si128((__m128i*)&result, _mm256_extract_epi32(sum, 4));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    const int n = arr.size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < k; ++i){\n        __m256i val = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i mask = _mm256_cmpgt_epi8(val, _mm256_set1_epi8(9));\n        __m256i filtered_val = _mm256_and_si256(mask, val);\n        sum = _mm256_add_epi32(sum, filtered_val);\n    }\n\n    // Reduce the sum to a single value\n    int result = 0;\n    for(int i = 0; i < 8; ++i){\n        result += _mm256_extract_epi32(sum, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    const int n = std::min(k, static_cast<int>(arr.size()));\n    __m256i sum = _mm256_setzero_si256();\n    \n    for(int i = 0; i < n; i += 4){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i mask = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8(9));\n        vec = _mm256_and_si256(mask, vec);\n        sum = _mm256_add_epi32(sum, _mm256_sad_epu8(vec, _mm256_set1_epi8(0)));\n    }\n    \n    int result = 0;\n    for(int i = 0; i < n % 4; ++i){\n        int val = arr[i];\n        if(val >= 0 && val <= 99)\n            result += val;\n    }\n    \n    return _mm256_extract_epi32(sum, 0) + result;\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k){\n    const int n = std::min(k, static_cast<int>(arr.size()));\n    __m256i sum = _mm256_setzero_si256();\n    \n    for(int i = 0; i < n; i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i mask = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8(9));\n        vec = _mm256_and_si256(mask, vec);\n        vec = _mm256_sad_epu8(vec, _mm256_set1_epi8(0));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 0));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 1));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 2));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 3));\n    }\n    \n    int result = 0;\n    for(int i = 0; i < n % 8; ++i){\n        result += arr[i];\n    }\n    \n    return _mm256_reduce_add_epi32(sum, _MM_FMAE);\n}\n```"}
{"task_id": "SimdBench_121_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint add_elements_simd(std::vector<int> arr, int k) {\n    const int n = std::min(k, static_cast<int>(arr.size()));\n    __m256i sum = _mm256_setzero_si256();\n    \n    for (int i = 0; i < n; i += 4) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i mask = _mm256_cmpgt_epi8(vec, _mm256_set1_epi8(9));\n        vec = _mm256_and_si256(mask, vec);\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 0));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 1));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 2));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(vec, 3));\n    }\n    \n    int result = 0;\n    for (int i = 0; i < n; ++i) {\n        result += arr[i];\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&lst[0]);\n    __m256i next_vec = _mm256_loadu_si256((__m256i*)&lst[1]);\n\n    while(next_vec != _mm256_setzero_si256()){\n        if(_mm256_cmpgt_epi32(vec, next_vec)) return false;\n        vec = next_vec;\n        next_vec = _mm256_loadu_si256((__m256i*)&lst[2]);\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[0]));\n    __m256i next_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[1]));\n\n    while(next_vec != _mm256_setzero_si256()){\n        if(_mm256_cmpgt_epi32(vec, next_vec)) return false;\n        vec = next_vec;\n        next_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[2]));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1)\n        return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&lst[0]);\n    int prev_val = _mm256_extract_epi32(vec, 0);\n\n    for(int i = 1; i < lst.size(); i += 8){\n        vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        int current_val = _mm256_extract_epi32(vec, 0);\n        \n        if(current_val < prev_val)\n            return false;\n        \n        prev_val = current_val;\n    }\n\n    // Check remaining elements\n    for(int i = lst.size() - 8; i < lst.size(); ++i){\n        if(lst[i] < prev_val)\n            return false;\n        \n        prev_val = lst[i];\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&lst[0]);\n    int prev_val = lst[0];\n\n    for(int i = 1; i < lst.size(); ++i){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n\n        // Compare each pair of elements in the vector\n        __m256 cmp_result = _mm256_cmpgt_epi32(current_vec, vec);\n\n        // Check if any element is greater than the previous one\n        if(_mm256_movemask_epi8(cmp_result) != 0){\n            return false;\n        }\n\n        // Update the previous value\n        prev_val = lst[i];\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_122_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nbool is_sorted_simd(const std::vector<int> & lst){\n    if(lst.size() <= 1) return true;\n\n    __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[0]));\n    __m256i next_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[1]));\n\n    while(next_vec != _mm256_setzero_si256()){\n        if(_mm256_cmpgt_epi32(vec, next_vec)) return false;\n        vec = next_vec;\n        next_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[2]));\n    }\n\n    return true;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign = _mm256_set1_epi8(1);\n\n    for(size_t i = 0; i < arr.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sign_vec = _mm256_cmpgt_epi32(abs_vec, _mm256_setzero_si256());\n        __m256i sign_sum = _mm256_add_epi32(sign, sign_vec);\n        sum = _mm256_add_epi32(sum, abs_vec * sign_sum);\n    }\n\n    int result = 0;\n    for(int i = 0; i < 8; ++i){\n        result += _mm256_extract_epi32(sum, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign = _mm256_set1_epi8(1);\n\n    for(size_t i = 0; i < arr.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sign_vec = _mm256_cmpgt_epi32(abs_vec, _mm256_setzero_si256());\n        sum = _mm256_add_epi32(sum, _mm256_mul_epi32(sign_vec, abs_vec));\n        sign = _mm256_xor_si256(sign, sign_vec);\n    }\n\n    int result = 0;\n    for(int i = 0; i < 8; ++i){\n        result += _mm256_extract_epi32(sum, i);\n    }\n\n    if(_mm256_movemask_epi8(sign) != 0) {\n        result *= -1;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <numeric>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign = _mm256_setzero_si256();\n\n    for(size_t i = 0; i < arr.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sign_vec = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        sum = _mm256_add_epi32(sum, abs_vec);\n        sign = _mm256_xor_si256(sign, sign_vec);\n    }\n\n    int result = 0;\n    for(int i = 0; i < 8; ++i){\n        result += _mm256_extract_epi32(sum, i);\n    }\n    result *= _mm256_extract_epi32(sign, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign = _mm256_set1_epi8(1);\n\n    for(size_t i = 0; i < arr.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        __m256i abs_vec = _mm256_abs_epi32(vec);\n        __m256i sign_vec = _mm256_and_si256(_mm256_cmpgt_epi32(abs_vec, _mm256_setzero_si256()), _mm256_set1_epi8(-1));\n        sum = _mm256_add_epi32(sum, _mm256_mul_epi32(abs_vec, sign_vec));\n        sign = _mm256_mul_epi32(sign, sign_vec);\n    }\n\n    int result = 0;\n    for(int i = 0; i < 8; ++i){\n        result += _mm256_extract_epi32(sum, i);\n    }\n    result *= _mm256_extract_epi32(sign, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_123_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint prod_signs_simd(const std::vector<int> & arr){\n    if(arr.empty()) return -32768;\n\n    __m256i sum = _mm256_setzero_si256();\n    __m256i sign = _mm256_setzero_si256();\n\n    for(size_t i = 0; i < arr.size(); i += 8){\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&arr[i]));\n        sum = _mm256_add_epi32(sum, _mm256_abs_epi32(vec));\n        sign = _mm256_mul_epi32(sign, _mm256_and_si256(_mm256_cmpgt_epi32(vec, _mm256_setzero_si256()), _mm256_cmplt_epi32(vec, _mm256_setzero_si256())));\n    }\n\n    int result = 0;\n    for(int i = 0; i < 8; ++i){\n        result += _mm256_extract_epi32(sum, i);\n    }\n    result *= _mm256_extract_epi32(sign, 0);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <vector>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    std::vector<int> result(k);\n    \n    // Initialize the first row with the smallest values\n    for (int i = 0; i < N; ++i) {\n        result[0] = grid[0][i];\n        break;\n    }\n    \n    // Process the remaining rows using AVX/AVX2 intrinsics\n    for (int i = 1; i < k; ++i) {\n        __m256 v_result = _mm256_set1_epi32(result[i - 1]);\n        \n        for (int j = 0; j < N; ++j) {\n            __m256 v_grid = _mm256_loadu_si256(reinterpret_cast<const __m256*>(&grid[j]));\n            \n            __m256 v_min = _mm256_min_epu32(v_result, v_grid);\n            v_result = _mm256_max_epu32(v_result, v_min);\n        }\n        \n        result[i] = _mm256_extract_epi32(_mm256_srlv_epi32(v_result, _mm256_set1_epi32(8)), 0);\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    const int M = grid[0].size();\n    const int max_value = N * N;\n    \n    // Initialize the result vector\n    std::vector<int> result(k);\n    \n    // Create a mask to check if a cell has been visited\n    __m256i visited_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been processed\n    __m256i processed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been updated\n    __m256i updated_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been removed from the queue\n    __m256i removed_mask = _mm256_setzero_si256();\n    \n    // Create a mask to check if a cell has been added to the queue\n    __m256i added_mask = _mm256_set"}
{"task_id": "SimdBench_124_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    const int M = grid[0].size();\n\n    // Initialize the result vector\n    std::vector<int> result(k);\n\n    // Create a mask to indicate whether a cell has been visited\n    __m256i visited_mask = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path values\n    __m256i min_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path indices\n    __m256i min_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path lengths\n    __m256i min_path_lengths = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path directions\n    __m256i min_path_directions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path distances\n    __m256i min_path_distances = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path costs\n    __m256i min_path_costs = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path parents\n    __m256i min_path_parents = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path weights\n    __m256i min_path_weights = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path depths\n    __m256i min_path_depths = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path counts\n    __m256i min_path_counts = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path sums\n    __m256i min_path_sums = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path products\n    __m256i min_path_products = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path quotients\n    __m256i min_path_quotients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path remainders\n    __m256i min_path_remainders = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path powers\n    __m256i min_path_powers = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path roots\n    __m256i min_path_roots = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logs\n    __m256i min_path_logs = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exps\n    __m256i min_path_exps = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path factorials\n    __m256i min_path_factorials = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path binomial coefficients\n    __m256i min_path_binomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path permutations\n    __m256i min_path_permutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path combinations\n    __m256i min_path_combinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subfactorials\n    __m256i min_path_subfactorials = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subcombinations\n    __m256i min_path_subcombinations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subpermutations\n    __m256i min_path_subpermutations = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path subbinomial coefficients\n    __m256i min_path_subbinomial_coefficients = _mm256_setzero_si25"}
{"task_id": "SimdBench_124_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int> > & grid, int k){\n    const int N = grid.size();\n    const int M = grid[0].size();\n\n    // Initialize the result vector\n    std::vector<int> result(k);\n\n    // Create a mask to check if a cell has been visited\n    __m256i visited_mask = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path values\n    __m256i min_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path indices\n    __m256i min_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path lengths\n    __m256i min_path_lengths = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path directions\n    __m256i min_path_directions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path costs\n    __m256i min_path_costs = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path parents\n    __m256i min_path_parents = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path distances\n    __m256i min_path_distances = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path weights\n    __m256i min_path_weights = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path scores\n    __m256i min_path_scores = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path paths\n    __m256i min_path_paths = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path counts\n    __m256i min_path_counts = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path sums\n    __m256i min_path_sums = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path products\n    __m256i min_path_products = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path powers\n    __m256i min_path_powers = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path roots\n    __m256i min_path_roots = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponents\n    __m256i min_path_exponents = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithms\n    __m256i min_path_logarithms = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i min_path_hyperbolic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path exponential functions\n    __m256i min_path_exponential_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path logarithmic functions\n    __m256i min_path_logarithmic_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path trigonometric functions\n    __m256i min_path_trigonometric_functions = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path hyperbolic functions\n    __m256i"}
{"task_id": "SimdBench_124_AVX", "completion": "```c++\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> minPath_simd(const std::vector<std::vector<int>>& grid, int k) {\n    const int N = grid.size();\n    const int M = grid[0].size();\n\n    // Initialize the result vector\n    std::vector<int> result(k);\n\n    // Create a mask to check if a cell has been visited\n    __m256i visited_mask = _mm256_setzero_si256();\n\n    // Create a vector to store the minimum path values\n    __m256i min_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the indices of the minimum path values\n    __m256i min_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices = _mm256_setzero_si256();\n\n    // Create a vector to store the next possible moves\n    __m256i next_moves = _mm256_set_epi32(1, 0, -1, 0, 0, 1, 0, -1);\n\n    // Create a vector to store the current position\n    __m256i current_position = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path length\n    __m256i current_length = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    // Create a vector to store the current path values\n    __m256i current_path_values = _mm256_setzero_si256();\n\n    // Create a vector to store the current path indices\n    __m256i current_path_indices"}
{"task_id": "SimdBench_125_AVX", "completion": "```c++\nuint64_t digits_simd(uint64_t n) {\n    // Convert the input number to a vector of 64-bit integers\n    __m256i vec_n = _mm256_set_epi64u((int64_t)(n >> 56), (int64_t)(n >> 48), (int64_t)(n >> 40), (int64_t)(n >> 32),\n                                         (int64_t)(n >> 24), (int64_t)(n >> 16), (int64_t)(n >> 8), (int64_t)n);\n\n    // Extract the odd bits from each element in the vector\n    __m256i odd_bits = _mm256_and_si256(vec_n, _mm256_set_epi64u(0x55555555, 0x55555555, 0x55555555, 0x55555555,\n                                                              0x55555555, 0x55555555, 0x55555555, 0x55555555));\n\n    // Sum up the odd bits\n    __m256i sum_odd_bits = _mm256_add_epi64(_mm256_add_epi64(_mm256_add_epi64(_mm256_add_epi64(odd_bits, odd_bits), odd_bits), odd_bits));\n    __m256i sum_even_bits = _mm256_xor_si256(sum_odd_bits, vec_n);\n    __m256i result = _mm256_sub_epi64(sum_odd_bits, sum_even_bits);\n\n    // Reduce the result to a single 64-bit integer\n    uint64_t final_result = _mm256_extract_epi64(result, 0);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c++\nuint64_t digits_simd(uint64_t n) {\n    __m256i n_vec = _mm256_set_epi64x(n & 0xFFFFFFFF, (n >> 32) & 0xFFFFFFFF);\n    __m256i mask = _mm256_cmpgt_epi64(n_vec, _mm256_setzero_si256());\n    __m256i odd_mask = _mm256_and_si256(mask, _mm256_set_epi64x(1, 3));\n    __m256i product = _mm256_mullo_epi64(_mm256_slli_epi64(odd_mask, 1), odd_mask);\n    uint64_t result = _mm256_reduce_add_epi64(product);\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n){\n    __m256i n_vec = _mm256_set_epi64u(n >> 56, n >> 48, n >> 40, n >> 32,\n                                        n >> 24, n >> 16, n >> 8, n);\n    __m256i mask = _mm256_set_epi64u(0x55555555, 0x55555555, 0x55555555, 0x55555555,\n                                        0x55555555, 0x55555555, 0x55555555, 0x55555555);\n    __m256i odd_mask = _mm256_and_si256(mask, n_vec);\n    __m256i product = _mm256_mullo_epi64(odd_mask, odd_mask);\n    uint64_t result = _mm256_extract_epi64(product, 0);\n    result |= _mm256_extract_epi64(product, 1) << 32;\n    result |= _mm256_extract_epi64(product, 2) << 64;\n    result |= _mm256_extract_epi64(product, 3) << 96;\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c++\nuint64_t digits_simd(uint64_t n) {\n    // Convert the number to a vector of 64-bit integers\n    __m256i vec = _mm256_set_epi64u((int64_t)(n >> 56), (int64_t)(n >> 48), (int64_t)(n >> 40), (int64_t)(n >> 32),\n                                       (int64_t)(n >> 24), (int64_t)(n >> 16), (int64_t)(n >> 8), (int64_t)n);\n\n    // Extract the odd digits from each vector element\n    __m256i odd_digits = _mm256_and_si256(vec, _mm256_set_epi32(1, 3, 5, 7, 9, 11, 13, 15));\n\n    // Sum the odd digits\n    __m256 sum = _mm256_add_epi32(_mm256_add_epi32(_mm256_add_epi32(_mm256_add_epi32(\n        _mm256_extract_epi32(odd_digits, 0), _mm256_extract_epi32(odd_digits, 1)),\n        _mm256_extract_epi32(odd_digits, 2)), _mm256_extract_epi32(odd_digits, 3))),\n        _mm256_extract_epi32(odd_digits, 4));\n\n    // Reduce the sum to a single value\n    uint64_t result = _mm256_reduce_add_epi32(sum, 0);\n\n    // Return the result\n    return result;\n}\n```"}
{"task_id": "SimdBench_125_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t digits_simd(uint64_t n) {\n    __m256i v = _mm256_set_epi64x(n & 0xFFFFFFFF, (n >> 32) & 0xFFFFFFFF);\n    __m256i mask = _mm256_cmpgt_epi64(v, _mm256_setzero_si256());\n    __m256i result = _mm256_and_si256(mask, _mm256_slli_epi64(_mm256_set_epi64x(1, 1), 5));\n    uint64_t sum = 0;\n    for (__m256i i = result; i != _mm256_setzero_si256(); i = _mm256_add_epi64(i, _mm256_extract_epi64(i, 0))) {\n        sum += _mm256_extract_epi64(i, 0);\n    }\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <algorithm>\n#include <cmath>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const size_t n = lst.size();\n    if (n == 0) return 0;\n\n    // Convert float to int using ceil\n    __m256i ceil_ints = _mm256_setzero_si256();\n    for (size_t i = 0; i < n; i += 8) {\n        __m256f floats = _mm256_loadu_ps(&lst[i]);\n        __m256i ints = _mm256_ceil_ps(floats);\n        ceil_ints = _mm256_or_si256(ceil_ints, ints);\n    }\n\n    // Square the integers\n    __m256i squares = _mm256_mullo_epi32(ceil_ints, ceil_ints);\n\n    // Sum the squares\n    uint64_t sum = 0;\n    for (size_t i = 0; i < n; i += 8) {\n        __m256i square = _mm256_loadu_si256(&squares[i]);\n        sum += _mm256_reduce_add_epi32(square);\n    }\n\n    return sum;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst){\n    const size_t n = lst.size();\n    if (n == 0) return 0;\n\n    __m256i vsum = _mm256_setzero_si256();\n    __m256 vceil = _mm256_set1_ps(1.0f);\n\n    for (size_t i = 0; i < n; i += 8) {\n        __m256 vf = _mm256_loadu_ps(&lst[i]);\n        __m256 vc = _mm256_ceilvf(vf);\n        __m256 vsq = _mm256_mul_ps(vc, vc);\n        vsum = _mm256_add_epi64(vsum, _mm256_extract_epi64(_mm256_castps_si256(vsq), 0));\n        vsum = _mm256_add_epi64(vsum, _mm256_extract_epi64(_mm256_castps_si256(vsq), 1));\n        vsum = _mm256_add_epi64(vsum, _mm256_extract_epi64(_mm256_castps_si256(vsq), 2));\n        vsum = _mm256_add_epi64(vsum, _mm256_extract_epi64(_mm256_castps_si256(vsq), 3));\n    }\n\n    uint64_t result = 0;\n    for (size_t i = 0; i < n; ++i) {\n        result += static_cast<uint64_t>(std::ceil(lst[i]));\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    const size_t n = lst.size();\n    uint64_t result = 0;\n    \n    // Convert float to int using ceiling\n    __m256i ceil_ints = _mm256_setzero_si256();\n    for (size_t i = 0; i < n; i += 8) {\n        __m256f floats = _mm256_loadu_ps(&lst[i]);\n        __m256i ints = _mm256_ceil_ps(floats);\n        ceil_ints = _mm256_add_epi32(ceil_ints, ints);\n    }\n    \n    // Sum up the squared values\n    __m256i squared_ints = _mm256_mullo_epi32(ceil_ints, ceil_ints);\n    __m256i summed_squared_ints = _mm256_reduce_add_epi32(squared_ints);\n    result = _mm256_extract_epi64(summed_squared_ints, 0);\n    \n    // Repeat for remaining elements if necessary\n    for (size_t i = 8; i < n; ++i) {\n        result += static_cast<uint64_t>(std::ceil(lst[i]));\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    const size_t n = lst.size();\n    uint64_t result = 0;\n    \n    // Convert float array to __m256 type\n    __m256f * vec_ptr = reinterpret_cast<__m256f*>(lst.data());\n    \n    // Process each chunk of 4 floats at a time\n    for (size_t i = 0; i < n - 3; i += 4) {\n        __m256f x = _mm256_loadu_ps(vec_ptr + i);\n        __m256f y = _mm256_ceil_ps(x); // Ceiling each element\n        __m256f z = _mm256_mul_ps(y, y); // Square each element\n        __m256f w = _mm256_add_ps(z, z); // Sum squares\n        result += _mm256_reduce_add_epi64(w); // Reduce to 64-bit integer\n    }\n    \n    // Handle remaining elements if there are less than 4\n    for (size_t i = n - 3; i < n; ++i) {\n        result += static_cast<uint64_t>(_mm_cvtss_si64(_mm_ceil_ss(_mm_set1_ps(lst[i]))));\n    }\n    \n    return result;\n}\n```"}
{"task_id": "SimdBench_126_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nuint64_t sum_squares_simd(const std::vector<float> & lst) {\n    const size_t n = lst.size();\n    uint64_t result = 0;\n\n    // Convert float to int using ceil\n    __m256i ceil_ints = _mm256_setzero_si256();\n    for (size_t i = 0; i < n; i += 8) {\n        __m256f floats = _mm256_loadu_ps(&lst[i]);\n        __m256i ints = _mm256_ceil_ps(floats);\n        ceil_ints = _mm256_add_epi32(ceil_ints, ints);\n    }\n\n    // Sum up the integers\n    for (int i = 0; i < 8; ++i) {\n        result += _mm_extract_epi32(ceil_ints, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if(n <= 1) return -1;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i prev_vec = _mm256_setzero_si256();\n\n    for(int i = 1; i < n; i += 8){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i diff = _mm256_sub_epi32(current_vec, prev_vec);\n        __m256i mask = _mm256_cmpgt_epi32(diff, _mm256_setzero_si256());\n        __m256i result = _mm256_and_si256(mask, _mm256_set1_epi32(i));\n        vec = _mm256_or_si256(vec, result);\n\n        prev_vec = current_vec;\n    }\n\n    int result = -1;\n    for(int i = 0; i < n; i++){\n        if(_mm256_testz_si256(vec, _mm256_set1_epi32(i))) {\n            result = i;\n            break;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if(n <= 1) return -1;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i prev_vec = _mm256_setzero_si256();\n\n    for(int i = 1; i < n; i += 8){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Compare current element with previous element\n        __m256i cmp_result = _mm256_cmpgt_epi32(current_vec, prev_vec);\n\n        // Find the first non-zero element in the comparison result\n        __m256i mask = _mm256_movemask_epi8(cmp_result);\n        if(mask == 0xFFFFFFFF) continue; // No non-zero element found\n\n        // Extract the index from the mask\n        int idx = __builtin_ctz(mask); // Count trailing zeros\n\n        // Check if the extracted index is valid\n        if(idx >= i && idx < n) {\n            return idx;\n        }\n\n        // Update the previous vector\n        prev_vec = current_vec;\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if(n <= 1) return -1;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i prev_vec = _mm256_setzero_si256();\n\n    for(int i = 1; i < n; i += 8){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n        __m256i cmp_result = _mm256_cmpgt_epi32(current_vec, prev_vec);\n        __m256i mask = _mm256_movemask_epi8(cmp_result);\n\n        if(mask == 0xFFFFFFFFFFFFFFFFULL){\n            return i;\n        }\n\n        prev_vec = _mm256_blendv_epi32(prev_vec, current_vec, mask);\n    }\n\n    for(int i = n % 8; i < n; ++i){\n        if(arr[i] > arr[i-1]){\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if(n <= 1) return -1;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i prev_vec = _mm256_setzero_si256();\n\n    for(int i = 1; i < n; i += 8){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&arr[i]);\n\n        // Compare current element with previous element\n        __m256i cmp = _mm256_cmpgt_epi32(current_vec, prev_vec);\n\n        // Find the first non-zero element in the comparison result\n        __m256i mask = _mm256_andnot_si256(cmp, _mm256_set1_epi32(1));\n\n        // Check if all elements in the comparison result are zero\n        __m256i all_zero = _mm256_movemask_epi8(mask);\n        if(all_zero == 0xFFFFFFFF) break;\n\n        // Update the previous vector with the current vector\n        prev_vec = current_vec;\n    }\n\n    // Check the remaining elements\n    for(int i = n % 8; i < n; ++i){\n        if(arr[i] > arr[i-1]) return i;\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_127_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint can_arrange_simd(const std::vector<int> & arr){\n    int n = arr.size();\n    if(n <= 1) return -1;\n\n    __m256i vec = _mm256_loadu_si256((__m256i*)&arr[0]);\n    __m256i prev_vec = _mm256_setzero_si256();\n\n    for(int i = 0; i < n - 1; i += 8){\n        __m256i current_vec = _mm256_loadu_si256((__m256i*)&arr[i + 1]);\n        __m256i cmp_result = _mm256_cmpgt_epi32(current_vec, prev_vec);\n        __m256i mask = _mm256_movemask_epi8(cmp_result);\n\n        if(mask == 0xFFFFFFFFFFFFFFFFLL){\n            return i;\n        }\n\n        prev_vec = _mm256_blendv_epi32(prev_vec, current_vec, mask);\n    }\n\n    for(int i = n - 7; i < n; ++i){\n        if(arr[i] > arr[i - 1]){\n            return i;\n        }\n    }\n\n    return -1;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    __m256i vec = _mm256_set_epi32(lst[0], lst[1], lst[2], lst[3]);\n    __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n    __m256i pos_mask = _mm256_cmpge_epi32(vec, _mm256_setzero_si256());\n    __m256i neg_max = _mm256_and_si256(neg_mask, vec);\n    __m256i pos_min = _mm256_and_si256(pos_mask, vec);\n\n    int result[4];\n    _mm_storeu_si128((__m128i*)result, neg_max);\n    _mm_storeu_si128((__m128i*)(result + 4), pos_min);\n\n    std::vector<int> res(result, result + 4);\n    return res;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    __m256i v = _mm256_set_epi32(0, 0, 0, 0);\n    __m256i v_neg = _mm256_set_epi32(-1, -1, -1, -1);\n    __m256i v_pos = _mm256_set_epi32(1, 1, 1, 1);\n\n    for (int i = 0; i < lst.size(); i += 4) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i mask_neg = _mm256_cmpgt_epi32(vec, v_neg);\n        __m256i mask_pos = _mm256_cmpgt_epi32(v_pos, vec);\n\n        __m256i max_neg = _mm256_and_si256(mask_neg, vec);\n        __m256i min_pos = _mm256_and_si256(mask_pos, vec);\n\n        v = _mm256_max_epi32(v, max_neg);\n        v = _mm256_min_epi32(v, min_pos);\n    }\n\n    std::vector<int> result;\n    for (int i = 0; i < lst.size(); ++i) {\n        if (lst[i] < 0) {\n            result.push_back(lst[i]);\n        } else {\n            result.push_back(lst[i]);\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    __m256i max_neg = _mm256_setzero_si256();\n    __m256i min_pos = _mm256_setzero_si256();\n\n    for (size_t i = 0; i < lst.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i neg_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i pos_mask = _mm256_cmpge_epi32(vec, _mm256_setzero_si256());\n\n        max_neg = _mm256_or_si256(max_neg, _mm256_and_si256(neg_mask, vec));\n        min_pos = _mm256_or_si256(min_pos, _mm256_and_si256(pos_mask, vec));\n    }\n\n    int max_neg_val = _mm256_extract_epi32(_mm256_max_ps(max_neg), 0);\n    int min_pos_val = _mm256_extract_epi32(_mm256_min_ps(min_pos), 0);\n\n    std::vector<int> result(2);\n    result[0] = max_neg_val;\n    result[1] = min_pos_val;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst){\n    __m256i v = _mm256_set_epi32(0, 0, 0, 0);\n    int max_neg = INT_MIN;\n    int min_pos = INT_MAX;\n\n    for(int i = 0; i < lst.size(); i += 4){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i neg_mask = _mm256_cmpgt_epi32(vec, _mm256_setzero_si256());\n        __m256i pos_mask = _mm256_cmpge_epi32(vec, _mm256_setzero_si256());\n\n        __m256i neg_max = _mm256_and_si256(neg_mask, vec);\n        __m256i pos_min = _mm256_and_si256(pos_mask, vec);\n\n        v = _mm256_or_si256(v, neg_max);\n        v = _mm256_or_si256(v, pos_min);\n\n        max_neg = std::max(max_neg, _mm256_extract_epi32(_mm256_srai_epi32(neg_max, 31), 0));\n        min_pos = std::min(min_pos, _mm256_extract_epi32(_mm256_srai_epi32(pos_min, 31), 0));\n    }\n\n    // Handle remaining elements if size is not divisible by 4\n    for(int i = lst.size() % 4; i < lst.size(); ++i){\n        int val = lst[i];\n        if(val < 0)\n            max_neg = std::max(max_neg, val);\n        else if(val > 0)\n            min_pos = std::min(min_pos, val);\n    }\n\n    std::vector<int> result(2);\n    result[0] = max_neg;\n    result[1] = min_pos;\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_128_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nstd::vector<int> largest_smallest_integers_simd(const std::vector<int>& lst) {\n    __m256i vec = _mm256_set_epi32(lst[0], lst[1], lst[2], lst[3]);\n    __m256i neg_mask = _mm256_cmpgt_epi32(_mm256_setzero_si256(), vec);\n    __m256i pos_mask = _mm256_cmpge_epi32(vec, _mm256_setzero_si256());\n    \n    __m256i neg_max = _mm256_and_si256(neg_mask, vec);\n    __m256i pos_min = _mm256_and_si256(pos_mask, vec);\n    \n    __m256i max_neg = _mm256_max_epi32(neg_max, _mm256_setzero_si256());\n    __m256i min_pos = _mm256_min_epi32(pos_min, _mm256_setzero_si256());\n    \n    int result[4];\n    _mm_storeu_si128((__m128i*)result, max_neg);\n    _mm_storeu_si128((__m128i*)(result + 4), min_pos);\n    \n    std::vector<int> res(result, result + 4);\n    return res;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c++\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) {\n        return 1;\n    }\n\n    __m256i result = _mm256_set1_epi64x(1);\n    __m256i i = _mm256_set1_epi64x(n);\n\n    while (i > _mm256_set1_epi64x(1)) {\n        __m256i temp = _mm256_mul_epi64(result, i);\n        result = _mm256_add_epi64(temp, result);\n        i = _mm256_sub_epi64(i, _mm256_set1_epi64x(1));\n    }\n\n    uint64_t final_result = 1;\n    for (__m256i i = result; i != _mm256_set1_epi64x(0); i = _mm256_srl_epi64(i, 32)) {\n        final_result *= _mm256_extract_epi64(i, 0);\n    }\n    final_result *= _mm256_extract_epi64(result, 1);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) {\n        return 1;\n    }\n\n    uint64_t result = 1;\n    __m256i vec_n = _mm256_set_epi64x(n, n - 1, n - 2, n - 3);\n    __m256i vec_result = _mm256_set_epi64x(1, 1, 1, 1);\n\n    while (__builtin_popcountll(vec_n[0]) > 0) {\n        __m256i vec_product = _mm256_mullo_epi64(vec_result, vec_n);\n        vec_result = _mm256_add_epi64(vec_result, vec_product);\n        vec_n = _mm256_srlv_epi64(vec_n, _mm256_set_epi64x(1, 1, 1, 1));\n    }\n\n    result += _mm256_reduce_add_epi64(vec_result);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c++\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) {\n        return 1;\n    }\n\n    __m256i result = _mm256_set1_epi64x(1);\n    uint64_t i = 2;\n\n    while (i <= n) {\n        __m256i factor = _mm256_set1_epi64x(i);\n        result = _mm256_mullo_epi64(result, factor);\n        i += 4;\n    }\n\n    // Reduce the result to a single value\n    __m256i reduced_result = _mm256_reduce_add_epi64(result);\n    uint64_t final_result = _mm_extract_epi64(reduced_result, 0);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) {\n        return 1;\n    }\n\n    __m256i result = _mm256_set1_epi64x(1);\n    uint64_t i = 2;\n\n    while (i <= n) {\n        __m256i next_result = _mm256_mullo_epi64(result, _mm256_set1_epi64x(i));\n        result = next_result;\n        i += 4;\n    }\n\n    // Reduce the result to a single value\n    uint64_t final_result = 1;\n    for (__m256i r : _mm256_split_epi64(result, result)) {\n        final_result *= _mm_cvtsi128_si64(r);\n    }\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_129_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t special_factorial_simd(uint64_t n) {\n    if (n == 0 || n == 1) {\n        return 1;\n    }\n\n    __m256i result = _mm256_set1_epi64x(1);\n    uint64_t i = 2;\n\n    while (i <= n) {\n        __m256i factor = _mm256_set1_epi64x(i);\n        result = _mm256_mul_epi64(result, factor);\n        i += 4;\n    }\n\n    // Reduce the result to a single value\n    __m256i reduced_result = _mm256_reduce_add_epi64(result);\n    uint64_t final_result = _mm_extract_epi64(reduced_result, 0);\n\n    return final_result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int sum = 0;\n    const int n = lst.size();\n    __m256i vsum = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i v = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&lst[i]));\n        __m256i v_square = _mm256_mul_ps(v, v);\n        __m256i v_cube = _mm256_mul_ps(_mm256_mul_ps(v, v), v);\n\n        if(i % 3 == 0){\n            vsum = _mm256_add_ps(vsum, v_square);\n        }\n        if(i % 4 == 0 && i % 3 != 0){\n            vsum = _mm256_add_ps(vsum, v_cube);\n        }\n    }\n\n    for(int i = 0; i < n % 8; ++i){\n        int val = lst[i];\n        if(i % 3 == 0){\n            sum += val * val;\n        }\n        if(i % 4 == 0 && i % 3 != 0){\n            sum += val * val * val;\n        }\n    }\n\n    return _mm256_reduce_add_epi32(vsum, _MM_FMAE);\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask3 = _mm256_cmpeq_epi32(_mm256_set_epi32(i % 3), _mm256_set_epi32(0));\n        __m256i mask4 = _mm256_cmpeq_epi32(_mm256_set_epi32(i % 4), _mm256_set_epi32(0));\n\n        __m256i squared = _mm256_mul_ps(v, v);\n        __m256i cubed = _mm256_mul_ps(squared, v);\n\n        sum = _mm256_add_ps(sum, _mm256_blendv_ps(cubed, squared, mask4));\n        sum = _mm256_add_ps(sum, _mm256_blendv_ps(v, squared, mask3));\n    }\n\n    int result = 0;\n    for(int i = 0; i < n; ++i){\n        result += lst[i];\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask3 = _mm256_cmpeq_epi32(_mm256_set_epi32(3), _mm256_srli_epi32(i, 2));\n        __m256i mask4 = _mm256_cmpeq_epi32(_mm256_set_epi32(4), _mm256_srli_epi32(i, 2));\n\n        v = _mm256_blendv_epi32(v, _mm256_mul_ps(v, _mm256_set1_ps(2.0f)), mask3);\n        v = _mm256_blendv_epi32(v, _mm256_mul_ps(v, _mm256_set1_ps(3.0f)), mask4);\n\n        sum = _mm256_add_epi32(sum, v);\n    }\n\n    // Handle remaining elements\n    for(int i = n % 8; i < n; ++i){\n        sum = _mm256_add_epi32(sum, _mm256_set1_epi32(lst[i]));\n    }\n\n    int result = 0;\n    _mm_storeu_si32(reinterpret_cast<int*>(&result), sum);\n    return result;\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i v = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask3 = _mm256_cmpeq_epi32(_mm256_set_epi32(3), _mm256_srli_epi32(i, 2));\n        __m256i mask4 = _mm256_cmpeq_epi32(_mm256_set_epi32(4), _mm256_srli_epi32(i, 2));\n\n        __m256i squared = _mm256_mul_ps(v, v);\n        __m256i cubed = _mm256_mul_ps(squared, v);\n\n        __m256i result = _mm256_or_ps(mask3 * squared, mask4 * cubed);\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(result, 0));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(result, 1));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(result, 2));\n        sum = _mm256_add_epi32(sum, _mm256_extract_epi32(result, 3));\n    }\n\n    // Handle remaining elements\n    for(int i = n % 8; i < n; ++i){\n        int val = lst[i];\n        if(i % 3 == 0)\n            val *= val;\n        else if(i % 4 == 0)\n            val *= val * val;\n        sum += val;\n    }\n\n    return _mm256_reduce_add_epi32(sum);\n}\n```"}
{"task_id": "SimdBench_130_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint sum_squares_simd(const std::vector<int> & lst){\n    int n = lst.size();\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 8){\n        __m256i vec = _mm256_loadu_si256((__m256i*)&lst[i]);\n        __m256i mask3 = _mm256_cmpeq_epi32(_mm256_set_epi32(i % 3), _mm256_set_epi32(0));\n        __m256i mask4 = _mm256_cmpeq_epi32(_mm256_set_epi32(i % 4), _mm256_set_epi32(0));\n\n        vec = _mm256_mul_ps(vec, _mm256_castsi256_ps(mask3));\n        vec = _mm256_add_ps(vec, _mm256_mul_ps(vec, _mm256_castsi256_ps(mask4)));\n\n        sum = _mm256_add_ps(sum, vec);\n    }\n\n    for(int i = n % 8; i < n; ++i){\n        sum = _mm_add_ps(sum, _mm_set1_ps(lst[i] * (i % 3 == 0 ? i : i % 4 == 0 ? i * i : 1)));\n    }\n\n    return _mm256_reduce_add_ps(sum);\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    __m256i vec = _mm256_setzero_si256();\n    \n    for(int i = 0; i < nums.size(); i += 8){\n        __m256i num = _mm256_loadu_si256((__m256i*)&nums[i]);\n        \n        // Extract first and last digit\n        __m256i first_digit = _mm256_and_si256(num, _mm256_slli_epi32(num, 24));\n        __m256i last_digit = _mm256_and_si256(num, _mm256_srli_epi32(num, 8));\n        \n        // Check if first and last digit are odd\n        __m256i is_odd = _mm256_or_si256(_mm256_cmpgt_epi32(first_digit, _mm256_set1_epi32(4)), _mm256_cmpgt_epi32(last_digit, _mm256_set1_epi32(4)));\n        \n        // Count numbers that satisfy the condition\n        count += _mm256_popcnt_u32(_mm256_movemask_epi8(is_odd));\n    }\n    \n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    __m256i mask = _mm256_setzero_si256();\n    __m256i temp;\n\n    for (size_t i = 0; i < nums.size(); i += 8) {\n        __m256i vec = _mm256_loadu_si256((__m256i*)&nums[i]);\n        temp = _mm256_and_si256(vec, _mm256_set_epi32(1, 3, 5, 7, 9, 1, 3, 5));\n        mask = _mm256_or_si256(mask, _mm256_cmpgt_epi32(temp, _mm256_set_epi32(10)));\n    }\n\n    for (size_t i = 0; i < nums.size() % 8; ++i) {\n        if (abs(nums[i]) > 10 && (nums[i] % 10 == 1 || nums[i] % 10 == 3 || nums[i] % 10 == 5 || nums[i] % 10 == 7 || nums[i] % 10 == 9)) {\n            ++count;\n        }\n    }\n\n    return _mm_popcnt_u32(_mm_movemask_epi8(mask));\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    __m256i mask = _mm256_setzero_si256();\n    __m256i temp;\n\n    for(size_t i = 0; i < nums.size(); i += 8){\n        temp = _mm256_loadu_si256((__m256i*)&nums[i]);\n        mask = _mm256_cmpgt_epi32(temp, _mm256_set1_epi32(10));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 31), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 30), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 29), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 28), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 27), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 26), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 25), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 24), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 23), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 22), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 21), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 20), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 19), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 18), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 17), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 16), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 15), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 14), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 13), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 12), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 11), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 10), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 9), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 8), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 7), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 6), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 5), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 4), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 3), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 2), 1));\n        mask = _mm256_and_si256(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 1), 1));\n        mask = _mm256_add_epi32(mask, _mm256_slli_epi32(_mm256_srai_epi32(temp, 0), 1));\n        count += _mm256_popcnt_u32(mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    __m256i mask = _mm_setzero_si256();\n    __m256i num;\n\n    for (size_t i = 0; i < nums.size(); i += 8) {\n        num = _mm_loadu_si256((__m256i*)&nums[i]);\n        mask = _mm_cmpgt_epi32(num, _mm_set1_epi32(10));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b0101));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask, mask, 0b1010));\n        mask = _mm_and_si256(mask, _mm_shuffle_epi32(mask"}
{"task_id": "SimdBench_131_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nint specialFilter_simd(const std::vector<int> & nums){\n    int count = 0;\n    __m256i vec = _mm256_setzero_si256();\n    __m256i mask = _mm256_setzero_si256();\n\n    for(int i = 0; i < nums.size(); i += 8){\n        __m256i num = _mm256_loadu_si256((__m256i*)&nums[i]);\n        __m256i abs_num = _mm256_abs_epi32(num);\n        __m256i first_digit = _mm256_and_si256(abs_num, _mm256_slli_epi32(abs_num, 24));\n        __m256i last_digit = _mm256_and_si256(abs_num, _mm256_srli_epi32(abs_num, 8));\n\n        mask = _mm256_or_si256(mask, _mm256_cmpgt_epi32(first_digit, _mm256_set1_epi32(10)));\n        mask = _mm256_or_si256(mask, _mm256_cmpgt_epi32(last_digit, _mm256_set1_epi32(10)));\n\n        vec = _mm256_or_si256(vec, mask);\n    }\n\n    for(int i = 0; i < nums.size() % 8; i++){\n        int num = nums[i];\n        if(num > 10 && ((num / 10) % 2 != 0) && ((num % 10) % 2 != 0)){\n            count++;\n        }\n    }\n\n    return _mm_popcnt_u32(_mm256_extract_epi32(vec, 0)) + _mm_popcnt_u32(_mm256_extract_epi32(vec, 1)) +\n           _mm_popcnt_u32(_mm256_extract_epi32(vec, 2)) + _mm_popcnt_u32(_mm256_extract_epi32(vec, 3)) +\n           _mm_popcnt_u32(_mm256_extract_epi32(vec, 4)) + _mm_popcnt_u32(_mm256_extract_epi32(vec, 5)) +\n           _mm_popcnt_u32(_mm256_extract_epi32(vec, 6)) + _mm_popcnt_u32(_mm256_extract_epi32(vec, 7)) + count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    __m256i a = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i c = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i temp = _mm256_set_epi32(i * i - i + 1, i * i - i + 1, i * i - i + 1, i * i - i + 1,\n                                           i * i - i + 1, i * i - i + 1, i * i - i + 1, i * i - i + 1);\n        a = _mm256_add_epi32(a, temp);\n        b = _mm256_add_epi32(b, temp);\n        c = _mm256_add_epi32(c, temp);\n    }\n\n    uint64_t count = 0;\n    for (int i = 0; i < 8; i++) {\n        __m256i sum = _mm256_loadu_si256((const __m256i*)&a[i]);\n        __m256i mod = _mm256_mod_epi32(sum, _mm256_set_epi32(3));\n        __m256i mask = _mm256_cmpgt_epi32(mod, _mm256_set_epi32(1));\n        __m256i count_mask = _mm256_and_si256(mask, _mm256_set_epi32(1));\n        count += _mm256_reduce_add_epi32(count_mask);\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    __m256i a = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i c = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i ai = _mm256_loadu_si256((__m256i*)&a[i]);\n        __m256i bi = _mm256_loadu_si256((__m256i*)&b[i]);\n        __m256i ci = _mm256_loadu_si256((__m256i*)&c[i]);\n\n        __m256i sum_ab = _mm256_add_epi32(ai, bi);\n        __m256i sum_bc = _mm256_add_epi32(bi, ci);\n        __m256i sum_ca = _mm256_add_epi32(ci, ai);\n\n        __m256i mod_3_ab = _mm256_and_si256(_mm256_srai_epi32(sum_ab, 2), _mm256_set_epi32(2, 2, 2, 2));\n        __m256i mod_3_bc = _mm256_and_si256(_mm256_srai_epi32(sum_bc, 2), _mm256_set_epi32(2, 2, 2, 2));\n        __m256i mod_3_ca = _mm256_and_si256(_mm256_srai_epi32(sum_ca, 2), _mm256_set_epi32(2, 2, 2, 2));\n\n        __m256i count_ab = _mm256_cmpneq_epi32(mod_3_ab, _mm256_set_epi32(0, 0, 0, 0));\n        __m256i count_bc = _mm256_cmpneq_epi32(mod_3_bc, _mm256_set_epi32(0, 0, 0, 0));\n        __m256i count_ca = _mm256_cmpneq_epi32(mod_3_ca, _mm256_set_epi32(0, 0, 0, 0));\n\n        __m256i total_count = _mm256_add_epi32(count_ab, count_bc);\n        total_count = _mm256_add_epi32(total_count, count_ca);\n\n        _mm256_storeu_si256((__m256i*)&a[i], total_count);\n    }\n\n    uint64_t result = 0;\n    for (int i = 0; i < n; i++) {\n        result += _mm256_extract_epi32(a[i], 0);\n    }\n\n    return result / 3;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c++\nuint64_t get_max_triples_simd(uint64_t n){\n    __m256i a = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i sum = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    for(int i = 0; i < n; i++){\n        a = _mm256_add_epi32(a, _mm256_set_epi32(i * i - i + 1, i * i - i + 1, i * i - i + 1, i * i - i + 1));\n    }\n\n    for(int i = 0; i < n; i++){\n        sum = _mm256_add_epi32(sum, a);\n    }\n\n    uint64_t count = 0;\n    for(int i = 0; i < n; i++){\n        for(int j = i + 1; j < n; j++){\n            for(int k = j + 1; k < n; k++){\n                if((sum[i] + sum[j] + sum[k]) % 3 == 0){\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nuint64_t get_max_triples_simd(uint64_t n){\n    __m256i a = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i b = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i c = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < n; i += 8) {\n        __m256i ai = _mm256_loadu_si256((__m256i*)&a);\n        __m256i bi = _mm256_loadu_si256((__m256i*)&b);\n        __m256i ci = _mm256_loadu_si256((__m256i*)&c);\n\n        __m256i sum_ab = _mm256_add_epi32(ai, bi);\n        __m256i sum_bc = _mm256_add_epi32(bi, ci);\n        __m256i sum_ca = _mm256_add_epi32(ci, ai);\n\n        __m256i mod_3_ab = _mm256_and_si256(_mm256_cmpgt_epi32(sum_ab, _mm256_set_epi32(2, 2, 2, 2, 2, 2, 2, 2)), _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1));\n        __m256i mod_3_bc = _mm256_and_si256(_mm256_cmpgt_epi32(sum_bc, _mm256_set_epi32(2, 2, 2, 2, 2, 2, 2, 2)), _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1));\n        __m256i mod_3_ca = _mm256_and_si256(_mm256_cmpgt_epi32(sum_ca, _mm256_set_epi32(2, 2, 2, 2, 2, 2, 2, 2)), _mm256_set_epi32(1, 1, 1, 1, 1, 1, 1, 1));\n\n        __m256i count_ab = _mm256_sub_epi32(_mm256_set_epi32(n, n, n, n, n, n, n, n), mod_3_ab);\n        __m256i count_bc = _mm256_sub_epi32(_mm256_set_epi32(n, n, n, n, n, n, n, n), mod_3_bc);\n        __m256i count_ca = _mm256_sub_epi32(_mm256_set_epi32(n, n, n, n, n, n, n, n), mod_3_ca);\n\n        __m256i total_count = _mm256_add_epi32(count_ab, _mm256_add_epi32(count_bc, count_ca));\n\n        _mm256_storeu_si256((__m256i*)&a, sum_ab);\n        _mm256_storeu_si256((__m256i*)&b, sum_bc);\n        _mm256_storeu_si256((__m256i*)&c, sum_ca);\n    }\n\n    uint64_t result = 0;\n    for (int i = 0; i < n; i++) {\n        result += a[i];\n        result += b[i];\n        result += c[i];\n    }\n\n    return result / 3;\n}\n```"}
{"task_id": "SimdBench_132_AVX", "completion": "```c++\nuint64_t get_max_triples_simd(uint64_t n){\n    __m256i a = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n    __m256i sum = _mm256_set_epi32(0, 0, 0, 0, 0, 0, 0, 0);\n\n    for (int i = 0; i < n; ++i) {\n        int val = i * i - i + 1;\n        a = _mm256_add_epi32(a, _mm256_set_epi32(val, val, val, val));\n        sum = _mm256_add_epi32(sum, _mm256_set_epi32(val, val, val, val));\n    }\n\n    uint64_t count = 0;\n    for (int i = 0; i < n; ++i) {\n        for (int j = i + 1; j < n; ++j) {\n            for (int k = j + 1; k < n; ++k) {\n                if ((sum[i] + sum[j] + sum[k]) % 3 == 0) {\n                    count++;\n                }\n            }\n        }\n    }\n\n    return count;\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c++\nint x_or_y_simd(int n, int x, int y){\n    __m256i n_vec = _mm256_set_epi32(n, n, n, n);\n    __m256i mask = _mm256_cmpeq_epi32(n_vec, _mm256_set_epi32(0, 0, 0, 0));\n    __m256i result = _mm256_blendv_epi32(x_vec, y_vec, mask);\n\n    return _mm256_extract_epi32(result, 0);\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    __m256i n_vec = _mm256_set_epi32(n, n, n, n);\n    __m256i mask = _mm256_cmpgt_epi32(n_vec, _mm256_setzero_si256());\n    __m256i result = _mm256_blendv_epi32(x_vec, y_vec, mask);\n\n    return _mm256_extract_epi32(result, 0);\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    __m256i n_vec = _mm256_set_epi32(n, n, n, n);\n    __m256i x_vec = _mm256_set_epi32(x, x, x, x);\n    __m256i y_vec = _mm256_set_epi32(y, y, y, y);\n\n    // Check if n is a prime number\n    __m256i is_prime_mask = _mm256_cmpgt_epi32(n_vec, _mm256_setzero_si256());\n    is_prime_mask = _mm256_andnot_si256(is_prime_mask, _mm256_cmplt_epi32(n_vec, _mm256_set1_epi32(2)));\n\n    // Multiply x by is_prime_mask and y by (1 - is_prime_mask)\n    __m256i result = _mm256_mul_epi32(is_prime_mask, x_vec);\n    result = _mm256_add_epi32(result, _mm256_mul_epi32(_mm256_sub_epi32(_mm256_set1_epi32(1), is_prime_mask), y_vec));\n\n    // Extract the results from the vector\n    int result_int[4];\n    _mm_storeu_si128((__m128i*)result_int, result);\n\n    return result_int[0];\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c++\nint x_or_y_simd(int n, int x, int y) {\n    __m256i n_vec = _mm256_set_epi32(n, n, n, n);\n    __m256i x_vec = _mm256_set_epi32(x, x, x, x);\n    __m256i y_vec = _mm256_set_epi32(y, y, y, y);\n\n    // Check if n is a prime number\n    __m256i mask = _mm256_cmpgt_epi32(n_vec, _mm256_setzero_si256());\n    __m256i result = _mm256_andnot_si256(mask, x_vec);\n\n    // Return the result\n    return _mm256_extract_epi32(result, 0);\n}\n```"}
{"task_id": "SimdBench_133_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n\nint x_or_y_simd(int n, int x, int y){\n    __m256i n_vec = _mm256_set_epi32(n, n, n, n);\n    __m256i mask = _mm256_cmpgt_epi32(n_vec, _mm256_setzero_si256());\n    __m256i result = _mm256_blendv_epi32(x_vec, y_vec, mask);\n\n    return _mm256_extract_epi32(result, 0);\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <cmath>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if(lst.empty()) return 0;\n\n    const int n = lst.size();\n    __m256i mask = _mm256_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n    __m256i neg_mask = _mm256_set_epi32(0, 0, 0, 0);\n\n    uint64_t result = 0;\n    for(int i = 0; i < n; i += 4){\n        __m256f vec = _mm256_loadu_ps(&lst[i]);\n        __m256f abs_vec = _mm256_abs_ps(vec);\n        __m256i int_part = _mm256_cvtps_epi32(abs_vec);\n        __m256i odd_mask = _mm256_cmpgt_epi32(int_part, mask);\n        __m256i neg_mask_vec = _mm256_cmplt_epi32(int_part, neg_mask);\n        __m256f odd_abs_vec = _mm256_and_ps(abs_vec, odd_mask);\n        __m256f neg_odd_abs_vec = _mm256_and_ps(abs_vec, neg_mask_vec);\n        __m256f squared_odd_abs_vec = _mm256_mul_ps(odd_abs_vec, odd_abs_vec);\n        __m256f squared_neg_odd_abs_vec = _mm256_mul_ps(neg_odd_abs_vec, neg_odd_abs_vec);\n        __m256f sum = _mm256_add_ps(squared_odd_abs_vec, squared_neg_odd_abs_vec);\n        result += _mm256_reduce_add_ps(sum);\n    }\n\n    // Handle remaining elements\n    for(int i = n % 4; i < n; ++i){\n        float num = lst[i];\n        if(num > 0 && std::floor(num) == num){\n            result += num * num;\n        }\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if(lst.empty()) return 0;\n\n    const int n = lst.size();\n    __m256i mask = _mm256_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n    __m256i sum = _mm256_setzero_si256();\n\n    for(int i = 0; i < n; i += 4){\n        __m256f f = _mm256_loadu_ps(&lst[i]);\n        __m256i i_mask = _mm256_cmpgt_epi32(_mm256_cvtps_epi32(f), mask);\n        __m256i s_mask = _mm256_andnot_si256(i_mask, mask);\n\n        __m256f f_sqr = _mm256_mul_ps(f, f);\n        __m256i s_sum = _mm256_add_epi32(s_mask, _mm256_castps_si256(f_sqr));\n\n        sum = _mm256_add_epi32(sum, s_sum);\n    }\n\n    uint64_t result = 0;\n    _mm256_storeu_si64((__int64_t*)&result, sum);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if(lst.empty()) return 0;\n\n    __m256i mask = _mm256_set_epi32(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);\n    __m256i neg_mask = _mm256_set_epi32(0x00000000, 0x00000000, 0x00000000, 0x00000000);\n\n    uint64_t sum = 0;\n    for(size_t i = 0; i < lst.size(); i += 4){\n        __m256f vec = _mm256_loadu_ps(&lst[i]);\n        __m256f abs_vec = _mm256_abs_ps(vec);\n        __m256i int_vec = _mm256_cvtps_epi32(abs_vec);\n        __m256i odd_mask = _mm256_and_si256(int_vec, mask);\n        __m256i neg_masked_odd = _mm256_and_si256(odd_mask, neg_mask);\n        __m256f squared_odd = _mm256_mul_ps(neg_masked_odd, neg_masked_odd);\n        __m256 sum_vec = _mm256_add_ps(sum_vec, squared_odd);\n    }\n\n    for(size_t i = 0; i < lst.size() % 4; ++i){\n        float val = lst[i];\n        if(val > 0 && val != static_cast<int>(val)){\n            sum += val * val;\n        }\n    }\n\n    return _mm256_reduce_add_epi64(_mm256_castps_si256(sum_vec)) >> 32;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    if(lst.empty()) return 0;\n\n    const int n = lst.size();\n    __m256i v = _mm256_setzero_si256(); // Initialize a zero vector\n\n    for(int i = 0; i < n; i += 8){\n        __m256f f = _mm256_loadu_ps(&lst[i]); // Load 8 floats from the vector into a vector\n        __m256i mask = _mm256_cmpgt_epi32(_mm256_cvtps_epi32(f), _mm256_setzero_si256()); // Create a mask for positive integers\n        __m256i odd_mask = _mm256_and_si256(mask, _mm256_cmpeq_epi32(_mm256_and_si256(mask, _mm256_srli_epi32(mask, 31)), _mm256_setzero_si256())); // Create a mask for odd integers\n        __m256i even_mask = _mm256_xor_si256(odd_mask, mask); // Create a mask for even integers\n        __m256f odd_f = _mm256_mul_ps(f, odd_mask); // Multiply odd integers by themselves\n        __m256f even_f = _mm256_mul_ps(f, even_mask); // Multiply even integers by themselves\n        __m256f sum_odd = _mm256_add_ps(odd_f, even_f); // Add the sums of odd and even integers\n        v = _mm256_add_epi64(v, _mm256_castps_si256(sum_odd)); // Add the sums to the zero vector\n    }\n\n    uint64_t result = 0;\n    for(int i = 0; i < 8; ++i){\n        result += _mm256_extract_epi64(v, i);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_134_AVX", "completion": "```c++\n#include <immintrin.h>\n#include <x86intrin.h>\n#include <vector>\n\nuint64_t double_the_difference_simd(const std::vector<float> & lst){\n    uint64_t result = 0;\n    __m256i vec = _mm256_setzero_si256();\n    int i = 0;\n\n    while (i < lst.size()) {\n        float num = lst[i];\n        if (num > 0 && num == static_cast<int>(num)) {\n            __m256f fnum = _mm256_set1_ps(num);\n            __m256f square = _mm256_mul_ps(fnum, fnum);\n            __m256i mask = _mm256_cmpgt_epi32(_mm256_cvtps_epi32(square), _mm256_setzero_si256());\n            __m256i even_mask = _mm256_cmpeq_epi32(_mm256_cvtps_epi32(square), _mm256_setzero_si256());\n            __m256i odd_mask = _mm256_andnot_si256(even_mask, mask);\n            __m256i odd_square = _mm256_permutevar_ps(square, odd_mask);\n            __m256i odd_square_int = _mm256_cvtepi32_ps(odd_square);\n            result += _mm256_reduce_add_epi32(odd_square_int);\n        }\n        i++;\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[0]));\n    __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[0]));\n\n    __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n    __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n    __m256i result_vec = _mm256_cvtepi32_ps(abs_diff_vec);\n    __m256i result_ints = _mm256_extract_epi32(result_vec, 0);\n\n    std::vector<int> result(result_ints.size());\n    for(int i = 0; i < result.size(); ++i){\n        result[i] = static_cast<int>(result_ints[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    __m256i game_vec, guess_vec;\n    __m256i diff_vec;\n\n    // Load game and guess vectors into AVX registers\n    game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[0]));\n    guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[0]));\n\n    // Perform element-wise subtraction and square\n    diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n    diff_vec = _mm256_sqr_epi32(diff_vec);\n\n    // Sum up the squared differences\n    __m256 sum = _mm256_add_epi32(_mm256_add_epi32(diff_vec, _mm256_shuffle_epi32(diff_vec, diff_vec, 0b10)), _mm256_shuffle_epi32(diff_vec, diff_vec, 0b01));\n    sum = _mm256_hadd_epi32(sum, sum);\n    sum = _mm256_hadd_epi32(sum, sum);\n\n    // Convert the sum back to an integer vector\n    std::vector<int> result(n);\n    _mm_storeu_si128(reinterpret_cast<__m128i*>(&result[0]), _mm256_extract_epi32(sum, 0));\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[0]));\n    __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[0]));\n\n    __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n    __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n    __m256i result_vec = _mm256_cvtepi32_ps(abs_diff_vec);\n    __m256i result_ints = _mm256_extract_epi32(result_vec, 0);\n\n    vector<int> result(n);\n    for(int i = 0; i < n; ++i){\n        result[i] = static_cast<int>(result_ints[i]);\n    }\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\nusing namespace std;\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[0]));\n    __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[0]));\n\n    __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n    __m256i abs_diff_vec = _mm256_abs_epi32(diff_vec);\n\n    __m256i result_vec = _mm256_permutevar_epi32(abs_diff_vec, _mm256_set_epi32(0, 1, 2, 3));\n\n    std::vector<int> result(n);\n    _mm256_storeu_si128(reinterpret_cast<__m128i*>(&result[0]), result_vec);\n\n    return result;\n}\n```"}
{"task_id": "SimdBench_135_AVX", "completion": "```cpp\n#include <immintrin.h>\n#include <x86intrin.h>\n\nstd::vector<int> compare_simd(const std::vector<int>& game, const std::vector<int>& guess){\n    int n = game.size();\n    __m256i game_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&game[0]));\n    __m256i guess_vec = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(&guess[0]));\n\n    std::vector<int> diff(n);\n    for(int i = 0; i < n; i += 8){\n        __m256i diff_vec = _mm256_sub_epi32(game_vec, guess_vec);\n        _mm_storeu_si256(reinterpret_cast<__m256i*>(&diff[i]), diff_vec);\n    }\n\n    return diff;\n}\n```"}
